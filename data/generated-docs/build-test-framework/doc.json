{"html":"<h1 id=\"project-apollo-test-framework-design-document\">Project Apollo: Test Framework Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>Project Apollo is a Python-based test framework built for learning, demonstrating how modern frameworks like pytest operate under the hood. The key architectural challenge it solves is providing a flexible, extensible system for test discovery, execution, and reporting while maintaining test isolation and supporting complex features like fixtures and assertions. This document guides the implementation through four progressive milestones, from basic discovery to a full-featured CLI.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"1-context-and-problem-statement\">1. Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides the foundational understanding and motivation for the entire project, spanning all four milestones.</p>\n</blockquote>\n<h3 id=\"mental-model-the-test-conductor\">Mental Model: The Test Conductor</h3>\n<p>Imagine you are the conductor of a large, distributed orchestra. Your musicians (individual test functions) are scattered across multiple buildings (source files and modules). Your sheet music (test fixtures and setup instructions) is stored in various libraries. Your performance hall (the test execution environment) needs to be prepared before each piece and cleaned afterward. Your ultimate goal is to produce a cohesive performance report (test results) that tells the audience (developers) exactly which musicians played correctly, which made mistakes, and why.</p>\n<p>A test framework is that conductor. It must:</p>\n<ol>\n<li><strong>Discover the musicians:</strong> Find all test functions across the codebase by scanning files and recognizing naming patterns.</li>\n<li><strong>Organize the performance:</strong> Determine the order of execution, manage dependencies between tests, and ensure musicians don&#39;t interfere with each other (test isolation).</li>\n<li><strong>Provide sheet music:</strong> Supply each test with the resources (fixtures, data, mocks) it needs to perform its part.</li>\n<li><strong>Evaluate the performance:</strong> Listen to each musician (execute the test) and judge whether they played the correct notes (assertions passed) or made an error.</li>\n<li><strong>Deliver the critique:</strong> Compile a clear, actionable report detailing successes, failures, and the specific nature of any mistakes.</li>\n</ol>\n<p>This mental model of the framework as an <strong>orchestrating conductor</strong> separates the <em>what</em> (the test logic written by developers) from the <em>how</em> (the mechanics of finding, running, and evaluating tests). This separation of concerns is the core architectural principle of any effective test framework.</p>\n<h3 id=\"the-core-problem-automated-verification\">The Core Problem: Automated Verification</h3>\n<p>Writing software without automated tests is like constructing a skyscraper without blueprints or inspections. You might get it to stand, but its reliability, safety, and maintainability are unknowable. Before test frameworks, verification was a manual, ad-hoc process:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Hypothetical manual test script (the \"dark ages\")</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_user_creation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connect_to_database()  </span><span style=\"color:#6A737D\"># Manual setup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    user </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_user(</span><span style=\"color:#9ECBFF\">\"Alice\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> user.name </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"Alice\"</span><span style=\"color:#E1E4E8\">:    </span><span style=\"color:#6A737D\"># Manual assertion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"FAIL: Name mismatch\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> db.user_exists(user.id):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"FAIL: User not saved\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db.cleanup()                </span><span style=\"color:#6A737D\"># Manual teardown</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"PASS\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p>This approach is <strong>brittle, unrepeatable, and unscalable</strong>. The problems compound rapidly:</p>\n<ul>\n<li><strong>Forgetting tests:</strong> Manual tests are easily overlooked or skipped.</li>\n<li><strong>No isolation:</strong> Tests can interfere with each other (e.g., leftover database state).</li>\n<li><strong>Poor diagnostics:</strong> Error messages are inconsistent and unhelpful.</li>\n<li><strong>No aggregation:</strong> There&#39;s no unified view of what passed or failed across the codebase.</li>\n<li><strong>High maintenance:</strong> Any change to the system under test requires manually updating countless scattered validation scripts.</li>\n</ul>\n<p>The <strong>Core Problem</strong> is therefore: <em>How do we automate the repetitive, mechanical aspects of software verification so developers can focus on writing test logic?</em> An effective solution must address four fundamental requirements:</p>\n<ol>\n<li><strong>Discovery:</strong> Automatically find all test cases in a codebase without requiring manual registration.</li>\n<li><strong>Execution:</strong> Run tests in a controlled, isolated environment with predictable setup and cleanup.</li>\n<li><strong>Assertion:</strong> Provide a rich, expressive language for verifying expectations with helpful failure messages.</li>\n<li><strong>Reporting:</strong> Aggregate results into a clear, actionable summary for both humans and machines.</li>\n</ol>\n<blockquote>\n<p><strong>Key Insight:</strong> The value of a test framework is not just in running tests—it&#39;s in <em>reducing the cognitive load</em> of verification. A good framework makes writing and maintaining tests <em>easier</em> than not having them.</p>\n</blockquote>\n<h3 id=\"existing-approaches-amp-comparison\">Existing Approaches &amp; Comparison</h3>\n<p>The software ecosystem offers multiple solutions to the test automation problem, each with different philosophies and trade-offs. Understanding this design space is crucial for making informed architectural decisions for Project Apollo.</p>\n<h4 id=\"architecture-decision-foundation-approach\">Architecture Decision: Foundation Approach</h4>\n<blockquote>\n<p><strong>Decision: Build a Modern, Convention-Based Framework</strong></p>\n<ul>\n<li><strong>Context:</strong> We need to choose a foundational philosophy that guides all subsequent design decisions. The built-in <code>unittest</code> module exists, but modern Python projects overwhelmingly prefer <code>pytest</code>.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Extend <code>unittest</code>:</strong> Build upon Python&#39;s standard library module.</li>\n<li><strong>Build <code>pytest</code>-like:</strong> Create a new, convention-based framework from scratch.</li>\n<li><strong>Minimal Assertion Library:</strong> Create only a better assertion library without discovery or fixtures.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Build a <code>pytest</code>-like, convention-based framework from scratch.</li>\n<li><strong>Rationale:</strong><ol>\n<li><strong>Educational Value:</strong> Building from scratch maximizes learning about all components (discovery, fixtures, reporting), not just assertions.</li>\n<li><strong>Modern Practices:</strong> Convention-over-configuration and fixture dependency injection are industry standards that reduce boilerplate.</li>\n<li><strong>Separation of Concerns:</strong> A clean-slate design allows us to implement clean boundaries between components (Discoverer, Runner, Fixture Registry) without legacy constraints.</li>\n</ol>\n</li>\n<li><strong>Consequences:</strong><ul>\n<li><strong>Positive:</strong> We gain deep understanding of modern test framework architecture. The final product will feel familiar to <code>pytest</code> users.</li>\n<li><strong>Negative:</strong> More implementation work than extending <code>unittest</code>. We must reimplement basics that <code>unittest</code> already provides.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Approach</th>\n<th align=\"left\">Pros</th>\n<th align=\"left\">Cons</th>\n<th align=\"left\">Why Not Chosen for Apollo</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Built-in (<code>unittest</code>)</strong></td>\n<td align=\"left\"><ul><li>No dependencies</li><li>Battle-tested</li><li>Standardized xUnit pattern</li></ul></td>\n<td align=\"left\"><ul><li>Verbose (requires test classes)</li><li>Limited fixture system</li><li>No auto-discovery by default</li><li>Less expressive assertions</li></ul></td>\n<td align=\"left\">While robust, <code>unittest</code> embodies an older architectural pattern. Building on it would limit our exploration of modern design patterns like convention-based discovery and functional fixture injection.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Modern (<code>pytest</code>)</strong></td>\n<td align=\"left\"><ul><li>Convention-over-configuration</li><li>Powerful fixture system</li><li>Rich plugin ecosystem</li><li>Excellent error reporting</li></ul></td>\n<td align=\"left\"><ul><li>Complex internals (hard to learn from)</li><li>Heavy dependency graph</li><li>&quot;Magic&quot; that can be opaque</li></ul></td>\n<td align=\"left\"><strong>Our inspiration, not our base.</strong> We are <em>recreating</em> this experience from first principles to understand the &quot;magic.&quot;</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Minimal (e.g., <code>assert</code>)</strong></td>\n<td align=\"left\"><ul><li>Extremely simple</li><li>No framework overhead</li><li>Direct control</li></ul></td>\n<td align=\"left\"><ul><li>No discovery or organization</li><li>Manual setup/teardown</li><li>No aggregated reporting</li><li>Poor error messages</li></ul></td>\n<td align=\"left\">Does not solve the core problem of automation. Leaves too much mechanical work to the developer.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Project Apollo (Our Build)</strong></td>\n<td align=\"left\"><ul><li><strong>Educational transparency</strong></li><li>Modern convention-based design</li><li>Controlled complexity</li><li>All core components built from scratch</li></ul></td>\n<td align=\"left\"><ul><li>Not production-hardened</li><li>Limited feature set</li><li>No plugin system initially</li></ul></td>\n<td align=\"left\"><strong>Chosen.</strong> Provides maximum learning value by implementing all core components with clean separation, mirroring modern framework architecture.</td>\n</tr>\n</tbody></table>\n<h4 id=\"key-architectural-differentiators\">Key Architectural Differentiators</h4>\n<p>The table above highlights fundamental architectural choices that differentiate frameworks:</p>\n<ol>\n<li><p><strong>Discovery Mechanism:</strong></p>\n<ul>\n<li><strong>Registration-based (<code>unittest</code>):</strong> Tests must inherit from a base class or be explicitly registered. This creates boilerplate but is explicit.</li>\n<li><strong>Convention-based (<code>pytest</code>, Apollo):</strong> The framework scans modules for functions/classes matching patterns (e.g., <code>test_*</code>). This reduces boilerplate but requires clear naming rules.</li>\n</ul>\n</li>\n<li><p><strong>Test Organization:</strong></p>\n<ul>\n<li><strong>Class-based xUnit (<code>unittest</code>):</strong> Tests are methods within classes, allowing shared <code>setUp</code>/<code>tearDown</code> at class level.</li>\n<li><strong>Function-based with Fixtures (<code>pytest</code>, Apollo):</strong> Tests are standalone functions; shared resources are provided via parameter injection (fixtures). This is more compositional and flexible.</li>\n</ul>\n</li>\n<li><p><strong>Assertion System:</strong></p>\n<ul>\n<li><strong>Method-based (<code>unittest</code>):</strong> <code>self.assertEqual(a, b)</code> provides good error messages but verbose syntax.</li>\n<li><strong>Built-in statement (<code>assert</code>):</strong> <code>assert a == b</code> is concise but yields poor default error messages.</li>\n<li><strong>Enhanced built-in (<code>pytest</code>, Apollo):</strong> Override or wrap the <code>assert</code> statement to provide rich diffs and introspection <em>while keeping the simple syntax</em>.</li>\n</ul>\n</li>\n<li><p><strong>Execution Model:</strong></p>\n<ul>\n<li><strong>Linear (<code>unittest</code> basic):</strong> Tests run sequentially in discovery order.</li>\n<li><strong>Isolated &amp; Parallel (Apollo goal):</strong> Each test runs in its own context, enabling safe parallel execution for speed.</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p><strong>Design Principle:</strong> Convention over configuration reduces boilerplate and cognitive load. By adopting sensible defaults (e.g., <code>test_</code> prefix), we make the common case easy while allowing escape hatches for complexity.</p>\n</blockquote>\n<h3 id=\"common-pitfalls-in-understanding-test-frameworks\">Common Pitfalls in Understanding Test Frameworks</h3>\n<p>⚠️ <strong>Pitfall: Treating the Framework as a Test Runner Only</strong></p>\n<ul>\n<li><strong>Description:</strong> Thinking the framework&#39;s primary job is just to execute a list of tests you manually give it.</li>\n<li><strong>Why It&#39;s Wrong:</strong> This ignores the critical problems of <em>discovery</em> (finding all tests) and <em>isolation</em> (preventing test interference). A framework that doesn&#39;t handle these becomes a glorified script executor.</li>\n<li><strong>How to Avoid:</strong> Design components with clear responsibilities: the <strong>Discoverer</strong> finds tests, the <strong>Runner</strong> manages isolated execution, and the <strong>Reporter</strong> aggregates results. These are distinct concerns.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Overlooking Test Isolation</strong></p>\n<ul>\n<li><strong>Description:</strong> Allowing tests to share mutable state (e.g., global variables, database connections) without proper cleanup.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Tests become interdependent and non-deterministic. A passing test may depend on leftover state from a previous test, causing false passes and mysterious failures when tests run in different orders.</li>\n<li><strong>How to Avoid:</strong> Design the <strong>Runner</strong> to execute each test in a clean environment. For milestone 1, this means each test runs in its own process or with carefully managed state rollback. For milestone 3, fixtures will provide controlled, scoped sharing.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Poor Error Messages in Assertions</strong></p>\n<ul>\n<li><strong>Description:</strong> Assertions that only say &quot;Assertion failed&quot; without showing the expected vs. actual values.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Developers waste time manually printing values to debug test failures, defeating the purpose of automation.</li>\n<li><strong>How to Avoid:</strong> Design the <strong>Assertion Engine</strong> (milestone 2) to compute and display informative diffs. For <code>assert_equal(a, b)</code>, the error message should show both values, their types, and a visual diff for collections.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Neglecting Resource Cleanup</strong></p>\n<ul>\n<li><strong>Description:</strong> Setting up resources (files, network connections) in tests but not guaranteeing cleanup on test failure.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Resource leaks accumulate across test runs, eventually causing failures (e.g., &quot;too many open files&quot;).</li>\n<li><strong>How to Avoid:</strong> Design the <strong>Fixture System</strong> (milestone 3) with guaranteed teardown using context managers (<code>try/finally</code>) or generator fixtures that run cleanup code after the test.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section establishes the <em>why</em> and <em>what</em>. The following sections will detail the <em>how</em>. However, to ground these concepts, here is a minimal starter structure and a tangible example of what we&#39;re building toward.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option (for Learning)</th>\n<th align=\"left\">Advanced Option (for Robustness)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Module Discovery</strong></td>\n<td align=\"left\"><code>importlib</code> + <code>os.walk</code></td>\n<td align=\"left\"><code>pkgutil</code> with namespace package support</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Test Isolation</strong></td>\n<td align=\"left\">Subprocess execution (<code>subprocess</code>)</td>\n<td align=\"left\"><code>sys.settrace()</code> for in-process isolation</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Assertion Engine</strong></td>\n<td align=\"left\">Override <code>__assert__</code> (not possible) → Helper functions</td>\n<td align=\"left\"><code>inspect</code> module for frame introspection</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Fixture Management</strong></td>\n<td align=\"left\">Simple dict registry + <code>functools.partial</code></td>\n<td align=\"left\">Context managers + dependency graph resolution</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>CLI &amp; Reporting</strong></td>\n<td align=\"left\"><code>argparse</code> + formatted print statements</td>\n<td align=\"left\"><code>rich</code> library for colored output, <code>lxml</code> for XML</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>To keep our conceptual components separate in code, start with this project layout:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project_apollo/\n├── apollo/                    # Main package\n│   ├── __init__.py           # Public API: `from apollo import test, fixture`\n│   ├── cli.py                # CLI Parser (Milestone 4)\n│   ├── discover.py           # Discoverer (Milestone 1)\n│   ├── runner.py             # Runner (Milestone 1 &amp; 3)\n│   ├── assert.py             # Assertion Engine (Milestone 2)\n│   ├── fixtures.py           # Fixture System (Milestone 3)\n│   ├── report.py             # Reporter (Milestone 4)\n│   └── results.py            # Data models: TestCase, TestResult, etc.\n├── tests/                    # Tests for Apollo itself\n│   ├── test_discover.py\n│   ├── test_runner.py\n│   └── ...\n└── examples/                 # Example test files for demonstration\n    ├── test_math.py\n    └── test_user.py</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-core-data-models\">C. Infrastructure Starter Code: Core Data Models</h4>\n<p>These data structures are the backbone of the framework. Implement them first to give shape to your system.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># File: apollo/results.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional, Callable</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"The possible states a test result can be in.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PASSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"passed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span><span style=\"color:#6A737D\">      # Assertion failed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERRORED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"errored\"</span><span style=\"color:#6A737D\">    # Uncaught exception</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestCase</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single test to be executed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Unique identifier: module::function_name or module::ClassName::method_name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeid: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # The actual callable test function or method</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    func: Callable[[], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # The file path where this test was discovered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Line number in the file (for better error reporting)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line_no: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"The outcome of executing a single TestCase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_case: TestCase</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: TestStatus</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Human-readable message (e.g., assertion error text)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Exception object if test raised one</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exception: Optional[</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Traceback formatted as string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    traceback: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Duration in seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Ensure consistency between status and other fields.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Failed tests must have a message\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">ERRORED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.exception </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Errored tests must have an exception\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestSuite</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A collection of TestCases, often representing a module or directory.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tests: list[TestCase]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_test</span><span style=\"color:#E1E4E8\">(self, test: TestCase):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tests.append(test)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __len__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tests)</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-the-simplest-possible-runner\">D. Core Logic Skeleton: The Simplest Possible Runner</h4>\n<p>To make the abstract concrete, here is the skeleton of the most basic runner that executes tests sequentially without any isolation or fixtures. This is your starting point for Milestone 1.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># File: apollo/runner.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .results </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase, TestResult, TestStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleRunner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Executes TestCases one by one and collects TestResults.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_test</span><span style=\"color:#E1E4E8\">(self, test_case: TestCase) -> TestResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a single test function and return its result.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Steps:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        1. Create a PENDING TestResult</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        2. Start timer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        3. Change status to RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        4. Execute test_case.func() in a try/except block</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        5. Catch AssertionError -> status FAILED, store message</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        6. Catch any other Exception -> status ERRORED, store exception</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        7. If no exception -> status PASSED</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        8. Stop timer, store duration</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        9. Return TestResult</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Create a TestResult with status=PENDING for this test_case</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Record start time using time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Update the result's status to RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Wrap test_case.func() call in try/except/else</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: In except AssertionError as e: set status=FAILED, message=str(e)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: In except Exception as e: set status=ERRORED, exception=e, traceback=traceback.format_exc()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 7: In else: set status=PASSED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 8: Record end time and calculate duration, store in result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 9: Ensure result is returned in all code paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_suite</span><span style=\"color:#E1E4E8\">(self, suite: TestSuite) -> list[TestResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run all tests in a suite and return their results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> test </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> suite.tests:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call run_test for each test and append result to list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-python\">E. Language-Specific Hints: Python</h4>\n<ul>\n<li>Use <code>importlib.import_module()</code> to dynamically import test files by their file path. Remember to handle <code>ImportError</code> gracefully.</li>\n<li>The <code>inspect</code> module is your friend for analyzing function signatures (for fixture injection) and retrieving source code lines.</li>\n<li>For parallel execution (Milestone 1), <code>concurrent.futures.ProcessPoolExecutor</code> provides true isolation (separate processes) but has overhead. <code>ThreadPoolExecutor</code> is lighter but requires careful state management.</li>\n<li>To override assertion behavior, you cannot modify the <code>assert</code> statement directly. Instead, provide helper functions like <code>assert_equal()</code> that raise <code>AssertionError</code> with rich messages.</li>\n</ul>\n<h4 id=\"f-milestone-1-checkpoint\">F. Milestone 1 Checkpoint</h4>\n<p>After implementing the basic Discoverer and SimpleRunner, you should be able to run:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo.cli</span><span style=\"color:#9ECBFF\"> discover</span><span style=\"color:#9ECBFF\"> examples/</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ol>\n<li>The Discoverer scans <code>examples/</code> directory for <code>.py</code> files.</li>\n<li>It imports each module and finds functions whose names start with <code>test_</code>.</li>\n<li>It creates <code>TestCase</code> objects for each found test.</li>\n<li>The Runner executes each test function.</li>\n<li>The CLI prints a simple report:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>    Running 2 tests from examples/\n    \n    test_addition (examples/test_math.py) ... PASSED (0.001s)\n    test_failing (examples/test_math.py) ... FAILED (0.000s)\n      AssertionError: 2 != 3\n    \n    === Summary ===\n    Passed: 1, Failed: 1, Errored: 0\n    Total time: 0.002s</code></pre></div>\n\n<p><strong>Signs of Trouble:</strong></p>\n<ul>\n<li><strong>No tests found:</strong> Check that your Discoverer is correctly filtering function names and handling module imports.</li>\n<li><strong>Tests interfering:</strong> If one test&#39;s failure affects another, you haven&#39;t achieved isolation. Ensure each test runs in a fresh environment.</li>\n<li><strong>No error details:</strong> Your <code>run_test</code> method isn&#39;t catching and storing exceptions properly.</li>\n</ul>\n<h4 id=\"g-debugging-tips-early-discovery-issues\">G. Debugging Tips: Early Discovery Issues</h4>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Symptom</th>\n<th align=\"left\">Likely Cause</th>\n<th align=\"left\">How to Diagnose</th>\n<th align=\"left\">Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong><code>ModuleNotFoundError</code> during discovery</strong></td>\n<td align=\"left\">Incorrect module path conversion from file path.</td>\n<td align=\"left\">Print the calculated module name before importing.</td>\n<td align=\"left\">Use <code>importlib.util.spec_from_file_location</code> for file-based imports.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Test functions from helper modules are discovered</strong></td>\n<td align=\"left\">Discovery isn&#39;t filtering by file name pattern (e.g., <code>test_*.py</code>).</td>\n<td align=\"left\">Print all files being scanned.</td>\n<td align=\"left\">Filter files by name <em>before</em> importing: <code>if not filename.startswith(&#39;test_&#39;): continue</code></td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Class methods not discovered</strong></td>\n<td align=\"left\">Only scanning module-level functions, not class methods.</td>\n<td align=\"left\">Inspect classes in the module using <code>inspect.getmembers(module, inspect.isclass)</code>.</td>\n<td align=\"left\">Recursively inspect classes for methods with <code>test_</code> prefix.</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"2-goals-and-non-goals\">2. Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the scope and boundaries for the entire project, providing clear direction for all four milestones.</p>\n</blockquote>\n<p>Every architectural undertaking requires clear boundaries to focus effort and prevent scope creep. This section defines <strong>what the framework must achieve</strong> (Goals) and <strong>what it explicitly won&#39;t address</strong> (Non-Goals), establishing a contract that guides design decisions throughout the project. Think of this as defining the playing field: we&#39;re building a comprehensive single-player training ground for test fundamentals, not a professional stadium with every possible feature.</p>\n<h3 id=\"goals\">Goals</h3>\n<p>The primary goal of Project Apollo is to create a <strong>pedagogical test framework</strong> that demonstrates how modern testing tools work internally while providing practical utility for testing small to medium Python projects. Each goal maps directly to one of the four milestones, creating a progressive learning path from basic automation to sophisticated testing infrastructure.</p>\n<table>\n<thead>\n<tr>\n<th>Goal Category</th>\n<th>Milestone Alignment</th>\n<th>Core Capability</th>\n<th>Why This Matters</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Automatic Test Discovery &amp; Execution</strong></td>\n<td>Milestone 1</td>\n<td>The framework automatically finds test functions in code modules and runs them in controlled environments</td>\n<td>Eliminates manual test registration and ensures tests run in isolation, preventing state contamination</td>\n</tr>\n<tr>\n<td><strong>Expressive Assertion System</strong></td>\n<td>Milestone 2</td>\n<td>Provides rich comparison assertions with helpful error messages and extensible matcher architecture</td>\n<td>Moves beyond basic <code>assert</code> statements to give developers immediate insight into why tests fail</td>\n</tr>\n<tr>\n<td><strong>Fixture-Based Test Environment</strong></td>\n<td>Milestone 3</td>\n<td>Manages test setup/teardown through reusable fixtures with configurable lifetimes and dependency injection</td>\n<td>Reduces test code duplication and properly manages expensive resources like database connections</td>\n</tr>\n<tr>\n<td><strong>Comprehensive Reporting &amp; CLI</strong></td>\n<td>Milestone 4</td>\n<td>Delivers human-readable and machine-parsable test results through a configurable command-line interface</td>\n<td>Enables integration into development workflows and CI/CD pipelines with professional-grade output</td>\n</tr>\n</tbody></table>\n<p>For each goal category, we define specific functional requirements that must be satisfied:</p>\n<h4 id=\"goal-1-automatic-test-discovery-amp-execution\">Goal 1: Automatic Test Discovery &amp; Execution</h4>\n<p>The framework must automatically locate and execute test functions without requiring explicit registration lists. This involves:</p>\n<ul>\n<li><strong>Convention-based discovery</strong>: Finding functions and methods whose names start with <code>test_</code> within modules and classes</li>\n<li><strong>Module traversal</strong>: Recursively scanning directories and importing Python modules to search for tests</li>\n<li><strong>Isolated execution</strong>: Running each test in a clean environment where test failures don&#39;t propagate to subsequent tests</li>\n<li><strong>Parallel execution support</strong>: Running independent tests concurrently to reduce total suite execution time</li>\n<li><strong>Error containment</strong>: Distinguishing between test failures (assertion failures) and test errors (unexpected exceptions)</li>\n</ul>\n<blockquote>\n<p><strong>Architecture Decision Record: Convention-over-Configuration for Discovery</strong></p>\n<ul>\n<li><strong>Context</strong>: Test frameworks need a way to identify which functions are tests without requiring explicit decoration or registration.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Explicit decorators (e.g., <code>@test</code>)</li>\n<li>Inheritance from a base test class</li>\n<li>Naming convention (functions starting with <code>test_</code>)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use naming convention (<code>test_</code> prefix) for test identification</li>\n<li><strong>Rationale</strong>: <ul>\n<li><strong>Zero boilerplate</strong>: Tests look like regular functions, reducing cognitive overhead</li>\n<li><strong>Familiarity</strong>: Matches pytest&#39;s approach, which developers already know</li>\n<li><strong>Simplicity</strong>: No imports or decorators required for basic tests</li>\n<li><strong>Discoverability</strong>: Easy to scan code for tests by function name</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>Positive: Extremely low barrier to entry for writing tests</li>\n<li>Negative: Less explicit than decorators, harder to customize per-test behavior</li>\n<li>Neutral: Requires careful module inspection logic</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<h4 id=\"goal-2-expressive-assertion-system\">Goal 2: Expressive Assertion System</h4>\n<p>The framework must provide a rich set of assertion utilities that offer better diagnostics than Python&#39;s built-in <code>assert</code> statement:</p>\n<ul>\n<li><strong>Value comparisons</strong>: Deep equality checking with diff generation for mismatches</li>\n<li><strong>Boolean conditions</strong>: Truthiness/falsiness assertions with descriptive messages</li>\n<li><strong>Collection operations</strong>: Checking membership, length, and subset relationships</li>\n<li><strong>Exception verification</strong>: Ensuring specific exceptions are raised by code under test</li>\n<li><strong>Extensible matchers</strong>: Plugin system for domain-specific assertion logic with custom failure messages</li>\n</ul>\n<p><strong>Assertion Comparison Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Assertion Type</th>\n<th>Example Usage</th>\n<th>Key Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Equality</td>\n<td><code>assert_equal(actual, expected)</code></td>\n<td>Shows diff between complex structures</td>\n</tr>\n<tr>\n<td>Exception</td>\n<td><code>assert_raises(ValueError, func, arg)</code></td>\n<td>Validates exception type and optionally message</td>\n</tr>\n<tr>\n<td>Collection</td>\n<td><code>assert_contains(collection, item)</code></td>\n<td>Clear message when item missing</td>\n</tr>\n<tr>\n<td>Custom Matcher</td>\n<td><code>assert_that(value, is_even())</code></td>\n<td>Domain-specific validation with tailored messages</td>\n</tr>\n</tbody></table>\n<h4 id=\"goal-3-fixture-based-test-environment\">Goal 3: Fixture-Based Test Environment</h4>\n<p>The framework must manage test dependencies and environment setup through a fixture system:</p>\n<ul>\n<li><strong>Per-test setup/teardown</strong>: <code>setUp</code> and <code>tearDown</code> methods that run before and after each test</li>\n<li><strong>Fixture sharing</strong>: Resources created once and shared across multiple tests with configurable lifetimes</li>\n<li><strong>Scope management</strong>: Fixtures with different lifetimes (function, class, module, session)</li>\n<li><strong>Dependency injection</strong>: Automatic provision of fixtures to test functions based on parameter names</li>\n<li><strong>Cleanup guarantees</strong>: Reliable teardown even when tests fail or errors occur</li>\n</ul>\n<blockquote>\n<p><strong>Mental Model: The Test Kitchen</strong>\nThink of fixtures as a well-organized test kitchen. <code>setUp</code> is like gathering ingredients before cooking each dish (test). Shared fixtures are like expensive appliances (stand mixer, sous-vide) that you set up once and use for multiple recipes. <code>tearDown</code> is cleaning the workspace after each dish, regardless of whether it turned out perfectly. The framework acts as the kitchen manager, ensuring the right tools are available when needed and everything gets cleaned up properly.</p>\n</blockquote>\n<h4 id=\"goal-4-comprehensive-reporting-amp-cli\">Goal 4: Comprehensive Reporting &amp; CLI</h4>\n<p>The framework must provide professional output and a usable command-line interface:</p>\n<ul>\n<li><strong>Progress reporting</strong>: Real-time display of test execution with pass/fail status</li>\n<li><strong>Statistical summary</strong>: Totals for passed, failed, errored, and skipped tests with timing information</li>\n<li><strong>Flexible test selection</strong>: Filtering tests by name patterns, file patterns, or custom markers</li>\n<li><strong>Machine-readable output</strong>: JUnit XML format for CI/CD pipeline integration</li>\n<li><strong>Proper exit codes</strong>: Non-zero exit code when tests fail for scriptable automation</li>\n</ul>\n<p><strong>CLI Feature Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Example</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File pattern matching</td>\n<td><code>apollo tests/*.py</code></td>\n<td>Run tests in specific files/directories</td>\n</tr>\n<tr>\n<td>Test name filtering</td>\n<td><code>apollo -k &quot;login&quot;</code></td>\n<td>Run only tests with &quot;login&quot; in their name</td>\n</tr>\n<tr>\n<td>Output format control</td>\n<td><code>apollo --junit report.xml</code></td>\n<td>Generate XML for CI systems</td>\n</tr>\n<tr>\n<td>Verbosity levels</td>\n<td><code>apollo -v</code> / <code>apollo -vv</code></td>\n<td>Control detail level in output</td>\n</tr>\n<tr>\n<td>Exit code behavior</td>\n<td>Returns 0 if all pass, 1 otherwise</td>\n<td>Enable scripting and CI failure detection</td>\n</tr>\n</tbody></table>\n<h3 id=\"non-goals\">Non-Goals</h3>\n<p>While the goals define what we will build, non-goals explicitly state what we <strong>will not build</strong>, preventing feature creep and maintaining focus on the educational objectives. These are features commonly found in production test frameworks but excluded from this learning project.</p>\n<table>\n<thead>\n<tr>\n<th>Non-Goal Category</th>\n<th>Example Features</th>\n<th>Why Excluded</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Distributed Test Execution</strong></td>\n<td>Running tests across multiple machines/processors with smart distribution</td>\n<td>Complexity far exceeds educational scope; focuses on system architecture rather than testing fundamentals</td>\n</tr>\n<tr>\n<td><strong>Plugin Ecosystem</strong></td>\n<td>Third-party plugins for coverage, mocking, database utilities, etc.</td>\n<td>Would require stable public APIs and compatibility guarantees beyond learning objectives</td>\n</tr>\n<tr>\n<td><strong>Custom Test Language/DSL</strong></td>\n<td>Special syntax for describing tests beyond Python functions</td>\n<td>Adds unnecessary abstraction layer; Python functions are sufficient for learning</td>\n</tr>\n<tr>\n<td><strong>Test Generation/Parameterization</strong></td>\n<td>Automatically generating test cases from data tables or property-based testing</td>\n<td>Advanced feature that would distract from core framework mechanics</td>\n</tr>\n<tr>\n<td><strong>Advanced Mocking Library</strong></td>\n<td>Complex patching, spy objects, call verification beyond basic unittest.mock</td>\n<td>Mocking is a separate concern from test framework infrastructure</td>\n</tr>\n<tr>\n<td><strong>Cross-Language Support</strong></td>\n<td>Testing JavaScript, Go, or other languages from the same framework</td>\n<td>Would require language-specific parsers and executors; Python-only keeps focus</td>\n</tr>\n<tr>\n<td><strong>Interactive Debugger Integration</strong></td>\n<td>Built-in debugger, breakpoints, or REPL within test failures</td>\n<td>Adds significant UI complexity; standard debugging tools suffice</td>\n</tr>\n<tr>\n<td><strong>Test Parallelization Dependencies</strong></td>\n<td>Automatic detection of test dependencies for safe parallel execution</td>\n<td>Requires complex static analysis; manual marking is sufficient for learning</td>\n</tr>\n<tr>\n<td><strong>Custom Report Formats</strong></td>\n<td>HTML reports, dashboard integration, historical trend analysis</td>\n<td>Output formatting is extensible enough via JUnit XML; fancy reports are ancillary</td>\n</tr>\n<tr>\n<td><strong>Database Migration Testing</strong></td>\n<td>Specialized fixtures for database state management and rollbacks</td>\n<td>Domain-specific testing concern better handled by dedicated libraries</td>\n</tr>\n</tbody></table>\n<h4 id=\"important-clarifications-on-scope-boundaries\">Important Clarifications on Scope Boundaries</h4>\n<p><strong>Why No Plugin System?</strong>\nWhile a plugin architecture is common in production frameworks (pytest has over 1,000 plugins), implementing it would shift focus from <strong>how tests run</strong> to <strong>how to build extensible systems</strong>. The educational value lies in understanding test execution, not building extension points. However, the design leaves intentional <strong>extension seams</strong> where plugins could be added later without major redesign.</p>\n<p><strong>Why Only Basic Parallel Execution?</strong>\nWe implement simple parallel execution where tests run in separate processes without dependency analysis. A full production system would need:</p>\n<ul>\n<li>Dependency detection between tests</li>\n<li>Resource contention management</li>\n<li>Shared fixture coordination across processes</li>\n<li>Result aggregation with ordering guarantees</li>\n</ul>\n<p>These complexities would triple the implementation effort while teaching little about core test framework concepts.</p>\n<p><strong>Why Exclude Advanced Mocking?</strong>\nMocking libraries (like <code>unittest.mock</code>) are already comprehensive in Python. Reinventing them would:</p>\n<ol>\n<li>Duplicate existing standard library functionality</li>\n<li>Divert focus from test orchestration to object interception</li>\n<li>Require deep understanding of Python&#39;s object model and descriptor protocol</li>\n</ol>\n<p>The framework will integrate with existing mocking tools rather than replace them.</p>\n<p><strong>The &quot;Good Enough&quot; Principle</strong>\nThis framework follows the <strong>&quot;good enough for learning&quot;</strong> principle. Each feature is implemented to the minimum level needed to understand the concept, not to production-grade robustness. For example:</p>\n<ul>\n<li><strong>File system watching</strong> (rerunning tests on file changes) is excluded—it&#39;s a nice-to-have feature that doesn&#39;t teach core testing concepts</li>\n<li><strong>Test ordering control</strong> (running tests in specific sequences) is minimal—just enough to ensure fixtures work correctly</li>\n<li><strong>Customizable discovery rules</strong> (changing the <code>test_</code> prefix) is excluded—convention-over-configuration simplifies learning</li>\n</ul>\n<blockquote>\n<p><strong>Design Principle: Single Responsibility for Learning</strong>\nProject Apollo focuses exclusively on <strong>test orchestration</strong>—finding tests, running them in the right environment, and reporting results. It deliberately excludes:</p>\n<ol>\n<li><strong>Test generation</strong> (property-based testing, combinatorial testing)</li>\n<li><strong>Environment management</strong> (Docker integration, cloud testing)</li>\n<li><strong>Code analysis</strong> (coverage measurement, static analysis)</li>\n<li><strong>UI testing</strong> (browser automation, mobile testing)</li>\n</ol>\n<p>These are important concerns in real-world testing but represent separate domains of knowledge. By keeping a tight focus, we ensure learners understand the core mechanics before exploring complementary tools.</p>\n</blockquote>\n<h4 id=\"what-could-be-added-later-future-extensions\">What Could Be Added Later (Future Extensions)</h4>\n<p>While not goals for the initial implementation, these areas represent logical extensions that the architecture accommodates:</p>\n<ol>\n<li><strong>Test Markers</strong> (<code>@slow</code>, <code>@integration</code>): Simple decorator-based test categorization</li>\n<li><strong>Parameterized Tests</strong>: Running the same test with different input data sets</li>\n<li><strong>Snapshot Testing</strong>: Comparing output against stored &quot;golden&quot; references</li>\n<li><strong>Benchmark Timing</strong>: Measuring and reporting test execution performance</li>\n<li><strong>Simple Plugin Hook</strong>: A few well-defined extension points for custom reporters or discovery</li>\n</ol>\n<p>The architecture will be designed with these possible extensions in mind, but they remain explicitly out of scope for the initial implementation.</p>\n<h3 id=\"success-criteria\">Success Criteria</h3>\n<p>The framework will be considered successful when it can:</p>\n<ol>\n<li><strong>Discover and run</strong> all <code>test_</code> prefixed functions in a directory of Python modules</li>\n<li><strong>Provide helpful failure messages</strong> showing exactly what went wrong in assertions</li>\n<li><strong>Manage fixture lifecycles</strong> correctly, cleaning up resources even when tests fail</li>\n<li><strong>Produce both human-readable</strong> terminal output and <strong>machine-parsable</strong> JUnit XML</li>\n<li><strong>Run tests in parallel</strong> when requested, with isolated execution environments</li>\n<li><strong>Integrate smoothly</strong> with existing Python projects using standard tooling</li>\n</ol>\n<p>These criteria map directly to the acceptance criteria in each milestone, creating clear, measurable objectives for the implementation.</p>\n<h3 id=\"trade-offs-and-constraints\">Trade-offs and Constraints</h3>\n<p>The goals and non-goals establish several important constraints:</p>\n<table>\n<thead>\n<tr>\n<th>Constraint</th>\n<th>Implication</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Python-only tests</strong></td>\n<td>No support for testing other languages</td>\n<td>Keeps implementation focused and leverages Python&#39;s introspection capabilities</td>\n</tr>\n<tr>\n<td><strong>Minimal external dependencies</strong></td>\n<td>Only standard library where possible</td>\n<td>Reduces setup complexity and demonstrates how things work without magic</td>\n</tr>\n<tr>\n<td><strong>Educational clarity over performance</strong></td>\n<td>May sacrifice optimizations for understandable code</td>\n<td>Primary value is learning, not production speed</td>\n</tr>\n<tr>\n<td><strong>Convention over configuration</strong></td>\n<td>Less flexible than config-driven approaches</td>\n<td>Reduces cognitive load for beginners</td>\n</tr>\n<tr>\n<td><strong>Progressive complexity</strong></td>\n<td>Later milestones build on earlier ones</td>\n<td>Ensures foundational understanding before advanced features</td>\n</tr>\n</tbody></table>\n<p>These constraints intentionally shape the framework into a <strong>learning tool first</strong> and a <strong>production tool second</strong>. The architecture will prioritize educational clarity, with clear separation of concerns and minimal magic, even if this means slightly more verbose code or fewer optimizations.</p>\n<h3 id=\"common-pitfalls-in-scope-definition\">Common Pitfalls in Scope Definition</h3>\n<p>⚠️ <strong>Pitfall: Over-engineering for hypothetical use cases</strong></p>\n<ul>\n<li><strong>Description</strong>: Implementing features &quot;just in case&quot; someone might need them, adding complexity without clear value</li>\n<li><strong>Why it&#39;s wrong</strong>: Diverts effort from core learning objectives, increases maintenance burden, and makes the codebase harder to understand</li>\n<li><strong>How to avoid</strong>: Strictly adhere to the non-goals list; when tempted to add a feature, ask &quot;does this directly support one of the four milestones?&quot;</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Under-specifying edge case behavior</strong></p>\n<ul>\n<li><strong>Description</strong>: Assuming &quot;obvious&quot; behavior for error cases or boundary conditions without explicit specification</li>\n<li><strong>Why it&#39;s wrong</strong>: Leads to inconsistent implementation and bugs that are hard to fix later</li>\n<li><strong>How to avoid</strong>: For each goal, explicitly define behavior for: empty test suites, import errors, fixture teardown failures, assertion errors in setup methods, etc.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Copying existing frameworks without understanding</strong></p>\n<ul>\n<li><strong>Description</strong>: Implementing features because &quot;pytest has them&quot; without understanding why they exist</li>\n<li><strong>Why it&#39;s wrong</strong>: Misses the educational value; creates cargo-cult architecture</li>\n<li><strong>How to avoid</strong>: For each feature, document the problem it solves and alternative approaches considered</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations-table\">Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Recommended)</th>\n<th>Advanced Option (Future Consideration)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Test Discovery</strong></td>\n<td><code>importlib</code> + <code>inspect</code> module</td>\n<td>AST parsing for more sophisticated analysis</td>\n</tr>\n<tr>\n<td><strong>Parallel Execution</strong></td>\n<td><code>multiprocessing.Pool</code> with process isolation</td>\n<td><code>concurrent.futures</code> with fine-grained control</td>\n</tr>\n<tr>\n<td><strong>Assertion Diffing</strong></td>\n<td><code>difflib.SequenceMatcher</code> for strings</td>\n<td>Custom differ for nested structures</td>\n</tr>\n<tr>\n<td><strong>Fixture Management</strong></td>\n<td>Simple registry with scope caching</td>\n<td>Graph-based dependency resolution</td>\n</tr>\n<tr>\n<td><strong>CLI Parsing</strong></td>\n<td><code>argparse</code> from standard library</td>\n<td><code>click</code> for more sophisticated CLI</td>\n</tr>\n<tr>\n<td><strong>XML Generation</strong></td>\n<td>Manual XML building with <code>xml.etree.ElementTree</code></td>\n<td>Template-based generation</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n├── __init__.py              # Public API exports\n├── __main__.py             # CLI entry point (python -m apollo)\n├── cli/\n│   ├── __init__.py\n│   ├── parser.py           # CLI argument parsing\n│   └── commands.py         # CLI command implementations\n├── discovery/\n│   ├── __init__.py\n│   ├── finder.py           # Module scanning and test discovery\n│   └── filters.py          # Test filtering logic\n├── runner/\n│   ├── __init__.py\n│   ├── base.py             # Base runner classes\n│   ├── serial.py           # Serial test execution\n│   └── parallel.py         # Parallel test execution\n├── assertions/\n│   ├── __init__.py         # Public assertion functions\n│   ├── engine.py           # Core assertion logic\n│   ├── matchers.py         # Matcher base classes\n│   └── diffs.py            # Diff generation utilities\n├── fixtures/\n│   ├── __init__.py\n│   ├── registry.py         # Fixture registration and lookup\n│   ├── scopes.py           # Scope management\n│   └── injection.py        # Parameter injection logic\n├── reporting/\n│   ├── __init__.py\n│   ├── console.py          # Terminal output formatting\n│   ├── junit.py            # JUnit XML generation\n│   └── statistics.py       # Result aggregation and stats\n├── core/\n│   ├── __init__.py\n│   ├── types.py            # Core data structures (TestCase, TestResult, etc.)\n│   ├── config.py           # Configuration object\n│   └── exceptions.py       # Framework-specific exceptions\n└── utils/\n    ├── __init__.py\n    ├── introspection.py    # Helper functions for code inspection\n    └── timing.py           # Execution timing utilities</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code-core-data-types\">Infrastructure Starter Code: Core Data Types</h4>\n<p>These core types provide the foundation for the entire framework. Learners should use these exact definitions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># apollo/core/types.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Core data types for the test framework.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Callable, Any, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> tb</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Possible states of a test during execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span><span style=\"color:#6A737D\">    # Test hasn't started yet</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span><span style=\"color:#6A737D\">    # Test is currently executing</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PASSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"passed\"</span><span style=\"color:#6A737D\">      # All assertions passed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span><span style=\"color:#6A737D\">      # An assertion failed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERRORED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"errored\"</span><span style=\"color:#6A737D\">    # An unexpected exception occurred</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span><span style=\"color:#6A737D\">    # Test was skipped (not implemented yet)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestCase</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single test function to be executed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeid: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">           # Unique identifier (e.g., \"test_module.py::test_function\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    func: Callable        </span><span style=\"color:#6A737D\"># The actual test function to call</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">        # Absolute path to the file containing the test</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line_no: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">          # Line number where test function is defined</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fixtures: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Names of required fixtures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.nodeid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of executing a single test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_case: TestCase</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: TestStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">          # Failure/error message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exception: Optional[</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Exception object if test errored</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    traceback: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">        # Formatted traceback for errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#6A737D\">                  # Execution time in seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> passed</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">PASSED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> failed</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">FAILED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> errored</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">ERRORED</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestSuite</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Collection of tests to be executed together.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tests: List[TestCase]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __len__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tests)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __iter__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> iter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tests)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Fixture</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A test fixture that provides resources to tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    func: Callable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scope: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"function\"</span><span style=\"color:#6A737D\">  # \"function\", \"class\", \"module\", \"session\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Names of other fixtures needed</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-test-discovery\">Core Logic Skeleton: Test Discovery</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># apollo/discovery/finder.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Test discovery implementation.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> importlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> inspect</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Iterator, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> apollo.core.types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> discover_tests</span><span style=\"color:#E1E4E8\">(start_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"test_*.py\"</span><span style=\"color:#E1E4E8\">) -> List[TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Discover all test functions in modules under start_path.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        start_path: Directory or file to start discovery from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        pattern: Glob pattern for test files (default: \"test_*.py\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of TestCase objects representing discovered tests</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\"> Implementation Steps:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    1. Convert start_path to absolute Path object</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    2. If start_path is a file, discover tests only in that file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    3. If start_path is a directory, recursively find all Python files matching pattern</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    4. For each Python file:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       a. Import it as a module (handle import errors gracefully)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       b. Inspect the module for functions starting with \"test_\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       c. Also inspect classes in the module for methods starting with \"test_\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">       d. For each test function found, create a TestCase object</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">          - nodeid: \"file.py::function_name\" or \"file.py::ClassName::method_name\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">          - func: The actual callable test function</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">          - file_path: Absolute path to the file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">          - line_no: Line number where function is defined (use inspect.getsourcelines)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    5. Return list of all discovered TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Hint: Use importlib.import_module for importing, inspect.getmembers for inspection</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert start_path to absolute Path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Determine if it's a file or directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect all Python files to inspect</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each file, import and discover tests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _discover_in_module</span><span style=\"color:#E1E4E8\">(module, file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Discover test functions within a single module.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        module: Imported module object</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        file_path: Path to the module file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of TestCase objects found in the module</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use inspect.getmembers to get all objects in module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Filter for callable objects (functions and methods)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if name starts with \"test_\" (for functions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Also check classes for methods starting with \"test_\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each test found, create TestCase with proper nodeid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints-for-python\">Language-Specific Hints for Python</h4>\n<ol>\n<li><p><strong>Module Import Safety</strong>: Use <code>importlib.util.spec_from_file_location</code> and <code>importlib.util.module_from_spec</code> to import test modules without adding them to <code>sys.modules</code> permanently, preventing side effects between test discoveries.</p>\n</li>\n<li><p><strong>Line Number Detection</strong>: <code>inspect.getsourcelines(func)</code> returns the source lines and starting line number, but be prepared for functions defined in C extensions (which will fail).</p>\n</li>\n<li><p><strong>Path Handling</strong>: Use <code>pathlib.Path</code> objects rather than string manipulation for cross-platform compatibility.</p>\n</li>\n<li><p><strong>Parallel Execution</strong>: For Milestone 1&#39;s parallel execution, use <code>multiprocessing.Pool</code> with <code>initializer</code> to set up each worker process, passing tests via <code>map</code>. Remember to use <code>if __name__ == &quot;__main__&quot;:</code> guards.</p>\n</li>\n<li><p><strong>Exception Preservation</strong>: When tests raise exceptions in separate processes, pickle the exception info and re-raise it in the main process for proper reporting.</p>\n</li>\n</ol>\n<h4 id=\"milestone-checkpoint-goal-verification\">Milestone Checkpoint: Goal Verification</h4>\n<p>After implementing the basic discovery from Milestone 1, verify the goals with this test:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create a test file</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">cat</span><span style=\"color:#F97583\"> ></span><span style=\"color:#9ECBFF\"> test_example.py</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#9ECBFF\"> 'EOF'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_addition():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    assert 1 + 1 == 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_failure():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    assert 2 + 2 == 5  # This will fail</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">class TestCalculator:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    def test_multiplication(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        assert 3 * 3 == 9</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">EOF</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run discovery and execution</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo</span><span style=\"color:#9ECBFF\"> discover</span><span style=\"color:#9ECBFF\"> test_example.py</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Should list 3 tests found</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> test_example.py</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_example.py::test_addition ... PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_example.py::test_failure ... FAILED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_example.py::TestCalculator::test_multiplication ... PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ====== SUMMARY ======</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Tests: 3, Passed: 2, Failed: 1, Errored: 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Exit code should be 1 (since tests failed)</span></span></code></pre></div>\n\n<p><strong>Signs of Problems:</strong></p>\n<ul>\n<li><strong>No tests found</strong>: Check that discovery is looking for <code>test_</code> prefix and importing modules correctly</li>\n<li><strong>Tests run in wrong order</strong>: Ensure tests are isolated; order shouldn&#39;t matter</li>\n<li><strong>Exit code is 0 when tests fail</strong>: The CLI must return non-zero on failure for CI integration</li>\n</ul>\n<h4 id=\"debugging-tips-for-scope-related-issues\">Debugging Tips for Scope-Related Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Framework tries to test non-Python files</strong></td>\n<td>Discovery pattern too broad</td>\n<td>Add debug logging to show which files are being considered</td>\n<td>Tighten file pattern matching to <code>*.py</code> only</td>\n</tr>\n<tr>\n<td><strong>Import errors break entire test run</strong></td>\n<td>Not catching import exceptions in discovery</td>\n<td>Wrap module imports in try/except</td>\n<td>Continue discovery even if some modules fail to import</td>\n</tr>\n<tr>\n<td><strong>Tests from installed packages get discovered</strong></td>\n<td>Discovery scanning system paths</td>\n<td>Check which directories are being scanned</td>\n<td>Limit discovery to user-specified paths only</td>\n</tr>\n<tr>\n<td><strong>Fixture teardown runs multiple times</strong></td>\n<td>Incorrect scope implementation</td>\n<td>Add logging to fixture setup/teardown</td>\n<td>Ensure fixture caching by scope key</td>\n</tr>\n</tbody></table>\n<h2 id=\"3-high-level-architecture\">3. High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides the structural blueprint for the entire framework, establishing the core components and their relationships that will be developed across all four milestones.</p>\n</blockquote>\n<p>At its heart, a test framework is a <strong>Test Conductor</strong>—an orchestrator that locates test functions, provides them with necessary resources (fixtures), executes them in a controlled environment, validates their outcomes, and composes a performance report. This architectural view reveals how the framework transforms a simple command into a comprehensive test run by coordinating specialized components.</p>\n<h3 id=\"component-overview-amp-responsibilities\">Component Overview &amp; Responsibilities</h3>\n<p>Think of the framework as a factory assembly line. Raw materials (your source code) enter at one end. The <strong>Discoverer</strong> acts as the quality inspector, identifying test-shaped parts. The <strong>Runner</strong> is the robotic arm that assembles and tests each unit in isolation. The <strong>Assertion Engine</strong> is the precision measurement tool that validates each output against specifications. The <strong>Reporter</strong> is the final quality assurance station that stamps each part with a pass/fail label and generates the inspection report. The <strong>CLI Parser</strong> is the factory control panel where the operator (you) specifies which production line to run and how to format the report.</p>\n<p>Each component has a single, well-defined responsibility and communicates with others through clear data structures, minimizing coupling and enabling independent evolution. The system follows a linear dataflow pipeline architecture, where output from one stage becomes input for the next.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Key Inputs</th>\n<th>Key Outputs</th>\n<th>Data Structures It Owns</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CLI Parser</strong></td>\n<td>Translates user command-line arguments into a structured configuration that drives the entire test run.</td>\n<td>Command-line arguments (<code>sys.argv</code>)</td>\n<td><code>Configuration</code> object</td>\n<td><code>Configuration</code> (parsed arguments)</td>\n</tr>\n<tr>\n<td><strong>Discoverer</strong></td>\n<td>Scans the filesystem and Python modules to find test functions/classes based on naming conventions.</td>\n<td><code>Configuration</code> (paths, patterns), filesystem</td>\n<td><code>TestSuite</code> (collection of <code>TestCase</code> objects)</td>\n<td><code>TestCase</code> definitions (but not their execution state)</td>\n</tr>\n<tr>\n<td><strong>Runner</strong></td>\n<td>Executes test cases in a controlled, isolated environment, managing fixture lifecycle and capturing outcomes.</td>\n<td><code>TestSuite</code>, <code>Configuration</code> (parallel flags)</td>\n<td>List of <code>TestResult</code> objects</td>\n<td>Test execution state, fixture instances (during run)</td>\n</tr>\n<tr>\n<td><strong>Assertion Engine</strong></td>\n<td>Evaluates assertion conditions and produces rich, descriptive failure messages when expectations aren&#39;t met.</td>\n<td>Actual value, expected value/matcher</td>\n<td>Either passes silently or raises <code>AssertionError</code> with formatted message</td>\n<td>Comparison diffs, custom matcher state</td>\n</tr>\n<tr>\n<td><strong>Reporter</strong></td>\n<td>Collects test results, formats them for human and machine consumption, and outputs to appropriate destinations.</td>\n<td>List of <code>TestResult</code> objects, <code>Configuration</code> (output format, verbosity)</td>\n<td>Console output, JUnit XML files, exit code</td>\n<td>Aggregated statistics (totals, durations)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The pipeline architecture creates natural separation of concerns. Each component only needs to understand the data structures it receives from the previous stage and produces for the next. This makes the system testable—you can verify the Discoverer by checking if it produces the right <code>TestCase</code> objects without actually running tests.</p>\n</blockquote>\n<p><strong>CLI Parser</strong> is the system&#39;s front door. It validates user input, converts file patterns (<code>tests/**/test_*.py</code>) into concrete filesystem paths, and sets operational modes (verbose output, parallel execution). It produces a <code>Configuration</code> object that becomes the single source of truth for the entire test run.</p>\n<p><strong>Discoverer</strong> implements the <strong>convention-over-configuration</strong> principle. Instead of requiring users to register tests manually, it automatically scans directories and modules. It inspects each Python module&#39;s attributes, looking for functions whose names start with <code>test_</code> or classes following the <strong>xUnit</strong> pattern. For each discovered test, it creates a <code>TestCase</code> object containing the test&#39;s identity (nodeid), the callable function, source location, and any declared fixture dependencies.</p>\n<p><strong>Runner</strong> is the workhorse that brings tests to life. Its most critical responsibility is ensuring <strong>test isolation</strong>—each test runs in its own environment so that state leaks or failures don&#39;t affect subsequent tests. The runner coordinates with the fixture system to set up required resources before test execution and tear them down afterward. For Milestone 1&#39;s parallel execution, the runner manages a pool of worker processes/threads, distributing independent tests to maximize throughput while maintaining isolation.</p>\n<p><strong>Assertion Engine</strong> transforms vague &quot;something went wrong&quot; errors into actionable diagnostics. When an assertion fails (like <code>assert_equal(actual, expected)</code>), the engine doesn&#39;t just say &quot;False is not True&quot;—it computes a diff between actual and expected values, formats them for readability, and raises an <code>AssertionError</code> with a clear message. This component also provides the extensible <strong>Matchers API</strong> that allows users to define domain-specific assertions with custom failure messages.</p>\n<p><strong>Reporter</strong> serves two audiences: humans reading terminal output and CI/CD systems parsing machine-readable formats. It formats individual test results (pass/fail status with execution time) and aggregates summary statistics (total passed/failed/errored counts, total duration). For CI integration, it generates <strong>JUnit XML</strong> output—a standardized format understood by tools like Jenkins and GitHub Actions.</p>\n<blockquote>\n<p><strong>Architecture Decision Record: Pipeline vs. Plugin Architecture</strong></p>\n<p><strong>Decision:</strong> We chose a linear pipeline architecture over a more flexible plugin system for the core framework.</p>\n<p><strong>Context:</strong> The framework needs to coordinate multiple distinct phases (discovery, execution, reporting) with clear dependencies between them. While modern frameworks like pytest support extensive plugin ecosystems, our educational framework prioritizes simplicity and clarity of data flow.</p>\n<p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Linear Pipeline (Chosen):</strong> Components execute in fixed sequence (CLI → Discover → Run → Report) with well-defined interfaces between them.</li>\n<li><strong>Event-Driven Plugin System:</strong> Components emit events (test_discovered, test_started, test_finished) that plugins can hook into, allowing arbitrary extension points.</li>\n<li><strong>Modular Monolith:</strong> All functionality in a single orchestrator class with configurable strategy objects for each phase.</li>\n</ol>\n<p><strong>Rationale:</strong> The pipeline architecture provides the clearest mental model for learners—they can trace execution from start to finish without dealing with inversion of control. Each component has a single responsibility, making the codebase easier to understand and test. The interfaces between components (like <code>TestSuite</code> and <code>TestResult</code>) become natural learning boundaries.</p>\n<p><strong>Consequences:</strong></p>\n<ul>\n<li><strong>Positive:</strong> Straightforward to implement, debug, and reason about. Clear separation of concerns.</li>\n<li><strong>Negative:</strong> Less flexible than a plugin system—adding new hooks requires modifying the pipeline stages directly.</li>\n<li><strong>Mitigation:</strong> We keep components loosely coupled through interface-like data structures, allowing internal refactoring without breaking the overall flow.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Linear Pipeline</td>\n<td>Simple to understand, easy to debug, clear data flow</td>\n<td>Less extensible, harder to add cross-cutting concerns</td>\n<td>✅ Yes</td>\n</tr>\n<tr>\n<td>Event-Driven Plugin System</td>\n<td>Highly extensible, supports cross-cutting concerns</td>\n<td>Complex to implement and debug, inversion of control</td>\n<td>❌ No</td>\n</tr>\n<tr>\n<td>Modular Monolith</td>\n<td>Unified control flow, can optimize across phases</td>\n<td>Tight coupling, harder to test components in isolation</td>\n<td>❌ No</td>\n</tr>\n</tbody></table>\n<p>The components interact as shown in the system component diagram:</p>\n<p><img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Fsys-component.svg\" alt=\"System Component Diagram\"></p>\n<h3 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h3>\n<p>Organizing code thoughtfully from the start prevents &quot;everything in one file&quot; chaos and establishes clear boundaries. Think of the project structure as a library building: the ground floor (<code>apollo/</code>) contains public areas anyone can visit (the API), upper floors (<code>core/</code>) house specialized departments (components), and the basement (<code>internal/</code>) contains infrastructure utilities that only library staff should access.</p>\n<blockquote>\n<p><strong>Design Principle:</strong> The public API should be minimal and intuitive—users typically interact with just the test runner and assertion functions. Internal components can be complex as needed, but their complexity is hidden behind clean interfaces.</p>\n</blockquote>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo-test-framework/\n├── pyproject.toml              # Project metadata and dependencies\n├── README.md\n├── examples/                   # Example test files for demonstration\n│   ├── test_math_operations.py\n│   └── test_with_fixtures.py\n├── apollo/                     # Public API package - what users import\n│   ├── __init__.py            # Main exports: run_tests, assert_equal, fixture, etc.\n│   ├── runner.py              # Public runner interface (simple entry point)\n│   ├── assertions.py          # Public assertion functions (assert_equal, assert_raises)\n│   └── fixtures.py            # Public fixture decorators and utilities\n├── src/                        # Core implementation (internal)\n│   ├── apollo/\n│   │   ├── core/              # Core framework components\n│   │   │   ├── __init__.py\n│   │   │   ├── config.py      # Configuration dataclass and CLI parsing\n│   │   │   ├── discoverer.py  # Test discovery logic\n│   │   │   ├── runner.py      # Test execution and isolation\n│   │   │   ├── assertions/    # Assertion engine implementation\n│   │   │   │   ├── __init__.py\n│   │   │   │   ├── engine.py  # Core assertion evaluation\n│   │   │   │   ├── matchers.py # Custom matcher API\n│   │   │   │   └── diffs.py   # Diff calculation for rich error messages\n│   │   │   ├── fixtures/      # Fixture system implementation\n│   │   │   │   ├── __init__.py\n│   │   │   │   ├── registry.py # Fixture registration and lookup\n│   │   │   │   ├── scopes.py   # Scope management (function, class, module)\n│   │   │   │   └── injection.py # Parameter injection mechanism\n│   │   │   └── reporting/     # Reporting and output formatting\n│   │   │       ├── __init__.py\n│   │   │       ├── reporter.py # Console and XML reporting\n│   │   │       ├── console.py  # Terminal output formatting\n│   │   │       └── junit_xml.py # JUnit XML generation\n│   │   ├── data/              # Data structures (shared across components)\n│   │   │   ├── __init__.py\n│   │   │   ├── test_case.py   # TestCase and TestSuite definitions\n│   │   │   ├── test_result.py # TestResult and TestStatus definitions\n│   │   │   └── fixtures.py    # Fixture definition data structures\n│   │   └── utils/             # Internal utilities\n│   │       ├── __init__.py\n│   │       ├── introspection.py # Module inspection helpers\n│   │       ├── path_utils.py    # Filesystem path handling\n│   │       └── parallel.py      # Parallel execution utilities\n├── tests/                      # Framework's own test suite\n│   ├── __init__.py\n│   ├── test_discovery.py      # Tests for the discoverer\n│   ├── test_runner.py         # Tests for the runner\n│   ├── test_assertions.py     # Tests for assertions\n│   └── test_fixtures.py       # Tests for fixtures\n└── scripts/                   # Development and utility scripts\n    └── cli.py                 # Main CLI entry point (installed as 'apollo')</code></pre></div>\n\n<blockquote>\n<p><strong>Key Architectural Boundaries:</strong></p>\n<ol>\n<li><strong>Public vs. Internal:</strong> The <code>apollo/</code> directory contains the user-facing API, while <code>src/apollo/core/</code> contains implementation details. This separation allows us to evolve the internal architecture without breaking user code.</li>\n<li><strong>Data Layer:</strong> The <code>data/</code> directory houses all shared data structures (<code>TestCase</code>, <code>TestResult</code>, <code>Fixture</code>). Components pass these objects but don&#39;t own their definitions—this prevents circular dependencies.</li>\n<li><strong>Component Independence:</strong> Each subdirectory under <code>core/</code> represents a component with minimal imports from other components. They communicate through the data structures and well-defined function calls.</li>\n</ol>\n</blockquote>\n<p><strong>Module Ownership and Dependencies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Module</th>\n<th>Owns</th>\n<th>Depends On</th>\n<th>Access Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>apollo/runner.py</code></td>\n<td>Public <code>run_tests()</code> function</td>\n<td><code>core.config</code>, <code>core.discoverer</code>, <code>core.runner</code></td>\n<td>Public API</td>\n</tr>\n<tr>\n<td><code>core/config.py</code></td>\n<td><code>Configuration</code> dataclass, CLI parsing</td>\n<td>Standard library (<code>argparse</code>, <code>pathlib</code>)</td>\n<td>Internal</td>\n</tr>\n<tr>\n<td><code>core/discoverer.py</code></td>\n<td><code>discover_tests()</code> function, module scanning</td>\n<td><code>data.test_case</code>, <code>utils.introspection</code>, <code>utils.path_utils</code></td>\n<td>Internal</td>\n</tr>\n<tr>\n<td><code>core/runner.py</code></td>\n<td><code>SimpleRunner</code> class, test execution</td>\n<td><code>data.test_result</code>, <code>core.fixtures</code>, <code>utils.parallel</code></td>\n<td>Internal</td>\n</tr>\n<tr>\n<td><code>assertions/engine.py</code></td>\n<td><code>assert_equal()</code>, <code>assert_true()</code> implementations</td>\n<td><code>assertions.diffs</code></td>\n<td>Internal</td>\n</tr>\n<tr>\n<td><code>fixtures/registry.py</code></td>\n<td><code>FixtureRegistry</code> class, fixture storage</td>\n<td><code>data.fixtures</code>, <code>fixtures.scopes</code></td>\n<td>Internal</td>\n</tr>\n<tr>\n<td><code>reporting/reporter.py</code></td>\n<td><code>Reporter</code> class, result aggregation</td>\n<td><code>data.test_result</code>, <code>reporting.console</code>, <code>reporting.junit_xml</code></td>\n<td>Internal</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision Record: Flat vs. Nested Public API</strong></p>\n<p><strong>Decision:</strong> We provide a flat public API with all common functions directly importable from the <code>apollo</code> package.</p>\n<p><strong>Context:</strong> Users expect test frameworks to be convenient to use—they don&#39;t want to remember complex import paths. The trade-off is between simplicity and namespace pollution.</p>\n<p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Flat API (Chosen):</strong> <code>from apollo import run_tests, assert_equal, fixture</code></li>\n<li><strong>Namespaced API:</strong> <code>from apollo.runner import run_tests</code>, <code>from apollo.assertions import assert_equal</code></li>\n<li><strong>Module-based API:</strong> <code>import apollo; apollo.run_tests()</code>, <code>import apollo.assertions</code></li>\n</ol>\n<p><strong>Rationale:</strong> The flat API matches user expectations from popular frameworks like pytest (<code>import pytest</code>) and unittest (<code>import unittest</code>). It reduces cognitive load for beginners—they learn one import statement and get all essential functionality. We can carefully curate what&#39;s exposed in <code>apollo/__init__.py</code> to avoid true namespace pollution.</p>\n<p><strong>Consequences:</strong></p>\n<ul>\n<li><strong>Positive:</strong> Extremely user-friendly, matches industry conventions.</li>\n<li><strong>Negative:</strong> The <code>apollo</code> namespace becomes crowded if we&#39;re not disciplined.</li>\n<li><strong>Mitigation:</strong> Only expose the 10-15 most commonly used functions/classes publicly. Keep specialized utilities in their submodules.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Circular Dependencies Between Components</strong></p>\n<ul>\n<li><strong>Description:</strong> The Discoverer imports from Runner to create TestCase objects that need runner-specific information, while the Runner imports from Discoverer to understand how to execute those objects.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Circular dependencies cause import errors and make the codebase fragile and hard to reason about. They often indicate poor separation of concerns.</li>\n<li><strong>How to Avoid:</strong> Establish a clear data layer (<code>data/</code>) with plain dataclasses that have no business logic. Components should only depend on data structures, not on each other&#39;s implementation details. Use dependency injection or callback registration if components need to communicate.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Monolithic Runner Class</strong></p>\n<ul>\n<li><strong>Description:</strong> Creating a single <code>Runner</code> class that handles discovery, fixture management, parallel execution, and result collection—becoming a &quot;god object&quot; with thousands of lines.</li>\n<li><strong>Why It&#39;s Wrong:</strong> Violates single responsibility principle, makes testing difficult, and prevents reuse of components (e.g., using the fixture system without the runner).</li>\n<li><strong>How to Avoid:</strong> Decompose the runner into focused collaborators: a <code>FixtureManager</code> handles fixtures, a <code>ParallelExecutor</code> manages concurrency, a <code>TestExecutor</code> runs individual tests. The main <code>SimpleRunner</code> coordinates these helpers.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Hardcoded Path Assumptions</strong></p>\n<ul>\n<li><strong>Description:</strong> Assuming tests will always be run from the project root directory or using relative imports that break when the framework is installed as a package.</li>\n<li><strong>Why It&#39;s Wrong:</strong> The framework will fail when users run tests from different directories or install it via pip.</li>\n<li><strong>How to Avoid:</strong> Always use absolute paths derived from <code>__file__</code> or user-provided configuration. Use <code>importlib</code> for module loading instead of manipulating <code>sys.path</code> directly. Test the framework from various working directories.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section bridges the architectural design with concrete implementation, providing the scaffolding to start building.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CLI Parser</strong></td>\n<td>Python&#39;s built-in <code>argparse</code> module</td>\n<td><code>click</code> library for richer CLI experience</td>\n</tr>\n<tr>\n<td><strong>Module Discovery</strong></td>\n<td><code>importlib</code> + <code>os.walk</code> for filesystem traversal</td>\n<td><code>pathlib</code> for modern path handling, <code>pkgutil</code> for package-aware discovery</td>\n</tr>\n<tr>\n<td><strong>Test Isolation</strong></td>\n<td>Fresh <code>sys.modules</code> import for each test</td>\n<td>Subprocess execution for complete isolation (slower but more robust)</td>\n</tr>\n<tr>\n<td><strong>Parallel Execution</strong></td>\n<td><code>concurrent.futures.ThreadPoolExecutor</code> (I/O-bound)</td>\n<td><code>multiprocessing.Pool</code> for CPU-bound tests (avoids GIL)</td>\n</tr>\n<tr>\n<td><strong>Diff Generation</strong></td>\n<td><code>difflib.unified_diff</code> for text comparison</td>\n<td>Custom object diffing with <code>pprint</code> formatting for complex structures</td>\n</tr>\n<tr>\n<td><strong>XML Generation</strong></td>\n<td>Manual string building with <code>xml.etree.ElementTree</code></td>\n<td><code>xml.dom.minidom</code> for pretty-printing</td>\n</tr>\n<tr>\n<td><strong>Fixture Injection</strong></td>\n<td>Function signature inspection via <code>inspect.signature</code></td>\n<td>AST parsing for more advanced injection patterns</td>\n</tr>\n</tbody></table>\n<p>For this educational project, we recommend the <strong>Simple Option</strong> for each component to focus on core concepts rather than production optimizations.</p>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Following the architecture above, create these initial files with the following minimal content:</p>\n<p><strong>1. Create the public API (<code>apollo/__init__.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Apollo Test Framework - A simple, extensible testing framework for Python.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .runner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> run_tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .assertions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_equal,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_not_equal,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_true,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_false,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_raises,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_in,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_not_in,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .fixtures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> fixture</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__version__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"0.1.0\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__all__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"run_tests\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"assert_equal\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"assert_not_equal\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"assert_true\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"assert_false\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"assert_raises\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"assert_in\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"assert_not_in\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"fixture\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<p><strong>2. Create the data structures layer (<code>src/apollo/data/__init__.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Data structures shared across all framework components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .test_case </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase, TestSuite</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .test_result </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestResult, TestStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .fixtures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Fixture</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__all__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"TestCase\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"TestSuite\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"TestResult\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"TestStatus\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Fixture\"</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<p><strong>3. Create the main CLI entry point (<code>scripts/cli.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">#!/usr/bin/env python3</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Command-line interface for the Apollo test framework.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.apollo.core.config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> parse_cli_args</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.apollo.core.discoverer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> discover_tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.apollo.core.runner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleRunner</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.apollo.core.reporting.reporter </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Reporter</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main entry point for the Apollo CLI.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse command-line arguments using parse_cli_args()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use discover_tests() to find tests based on configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create a TestSuite from discovered tests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create a SimpleRunner and run the test suite</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create a Reporter and output results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Set appropriate exit code (0 if all tests pass, 1 otherwise)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    main()</span></span></code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p>Here&#39;s complete, working code for foundational utilities that aren&#39;t the core learning focus:</p>\n<p><strong>1. Path Utilities (<code>src/apollo/utils/path_utils.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Utilities for filesystem path handling in test discovery.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> resolve_patterns_to_paths</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    patterns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    base_dir: Union[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Path] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> List[Path]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Convert file patterns (like 'tests/**/test_*.py') to concrete file paths.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        patterns: List of glob patterns or file paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        base_dir: Base directory for relative patterns (defaults to cwd)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of absolute Path objects to existing Python files</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> base_dir </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        base_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path.cwd()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        base_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(base_dir).resolve()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resolved_paths </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> pattern </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> patterns:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If it's already an existing file, use it directly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(pattern)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            resolved_paths.append(path.resolve())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Otherwise treat as a glob pattern relative to base_dir</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> matched </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> base_dir.glob(pattern):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> matched.is_file() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> matched.suffix </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '.py'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                resolved_paths.append(matched.resolve())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Remove duplicates while preserving order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seen </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    unique_paths </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> path </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> resolved_paths:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> path </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> seen:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            seen.add(path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            unique_paths.append(path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> unique_paths</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> module_path_to_name</span><span style=\"color:#E1E4E8\">(file_path: Path, root_dir: Path) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Convert a filesystem path to a Python importable module name.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Example: /home/user/project/tests/test_math.py -> tests.test_math</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Make relative to root directory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rel_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> file_path.relative_to(root_dir)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # File is outside root directory - use its absolute path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # but replace path separators</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(file_path.parts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Remove .py extension from last part</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">][:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \".\"</span><span style=\"color:#E1E4E8\">.join(parts)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Remove .py extension and convert to dotted notation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    module_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(rel_path.with_suffix(</span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)).replace(os.sep, </span><span style=\"color:#9ECBFF\">'.'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> module_name</span></span></code></pre></div>\n\n<p><strong>2. Configuration Dataclass (<code>src/apollo/core/config.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Configuration dataclass and CLI argument parsing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> argparse</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Configuration</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Structured configuration for a test run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # File patterns to search for tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file_patterns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"test_*.py\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"*_test.py\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Directory to start discovery (defaults to current directory)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_dir: Path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">Path.cwd)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test name filters (only run tests matching these substrings)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_name_filters: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Output control</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    verbose: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quiet: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"console\"</span><span style=\"color:#6A737D\">  # \"console\" or \"junit-xml\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Execution control</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parallel: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_workers: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Reporting control</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    show_durations: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    junit_xml_path: Optional[Path] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse_cli_args</span><span style=\"color:#E1E4E8\">(args</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">) -> Configuration:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Parse command-line arguments into a Configuration object.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        args: Command-line arguments (defaults to sys.argv[1:])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Configuration object with parsed settings</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> argparse.ArgumentParser(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Apollo Test Framework - Run Python tests\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # File discovery arguments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"paths\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        nargs</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        default</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\".\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"File or directory paths to search for tests (default: current directory)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"-p\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--pattern\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        action</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"append\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        dest</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"patterns\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        default</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Glob pattern for test files (e.g., 'test_*.py'). Can be specified multiple times.\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test selection arguments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"-k\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--keyword\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        action</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"append\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        dest</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"keywords\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        default</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Only run tests matching given substring in test name. Can be specified multiple times.\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Output control arguments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"-v\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--verbose\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        action</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"store_true\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Verbose output: show more details about test execution\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"-q\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--quiet\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        action</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"store_true\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Quiet mode: only show failures and final summary\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"--junit-xml\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        dest</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"junit_xml_path\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">Path,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Path to write JUnit XML test results\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Execution control arguments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"--parallel\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        action</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"store_true\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Run tests in parallel using multiple workers\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"-n\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"--num-workers\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        type</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        dest</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"max_workers\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Number of parallel workers (default: CPU count)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Parse and convert to Configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse_args(args)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Use default patterns if none provided</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    patterns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parsed.patterns </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> parsed.patterns </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"test_*.py\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"*_test.py\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Configuration(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        file_patterns</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">patterns,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        start_dir</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">Path(parsed.paths[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]).resolve(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        test_name_filters</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">parsed.keywords,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        verbose</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">parsed.verbose,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        quiet</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">parsed.quiet,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        output_format</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"junit-xml\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> parsed.junit_xml_path </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"console\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        parallel</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">parsed.parallel,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">parsed.max_workers,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        show_durations</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">parsed.verbose,  </span><span style=\"color:#6A737D\"># Show durations in verbose mode</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        junit_xml_path</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">parsed.junit_xml_path,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>1. TestCase Data Structure (<code>src/apollo/data/test_case.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Test case representation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestCase</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Represents a single test case to be executed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Fields:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        nodeid: Unique identifier for the test (e.g., \"test_module.py::test_function\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        func: The callable test function or method</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        file_path: Absolute path to the file containing the test</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        line_no: Line number where the test is defined (approximate)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        fixtures: List of fixture names this test depends on</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeid: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    func: Callable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file_path: Path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line_no: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fixtures: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.nodeid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestSuite</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    A collection of test cases that can be executed together.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Fields:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        name: Name of the test suite (often the module or directory name)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tests: List of test cases in this suite</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tests: List[TestCase]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __len__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.tests)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_test</span><span style=\"color:#E1E4E8\">(self, test: TestCase) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add a test case to the suite.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Append the test to the tests list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> filter_tests</span><span style=\"color:#E1E4E8\">(self, keywords: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">\"TestSuite\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Return a new TestSuite containing only tests whose nodeid</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        contains any of the given keywords.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            keywords: List of substrings to match in test names</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            New TestSuite with filtered tests</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If keywords is empty, return a copy of self</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Otherwise, filter tests where any keyword in keywords</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         is a substring of test.nodeid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return a new TestSuite with the filtered tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>2. Discoverer Core Function (<code>src/apollo/core/discoverer.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Test discovery implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> inspect</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> importlib.util</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Iterator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data.test_case </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase, TestSuite</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> discover_tests</span><span style=\"color:#E1E4E8\">(start_path: Path, pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"test_*.py\"</span><span style=\"color:#E1E4E8\">) -> List[TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Discover all test functions in modules under start_path.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        start_path: Directory or file to start discovery from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        pattern: Glob pattern for test files</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of discovered TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If start_path is a file, just process that file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If start_path is a directory, recursively find all .py files matching pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each Python file, import it as a module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Inspect the module for test functions (names starting with 'test_')</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Also inspect classes for test methods (methods starting with 'test_')</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: For each test function/method, create a TestCase object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Collect all TestCase objects and return them</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _import_module_from_file</span><span style=\"color:#E1E4E8\">(file_path: Path):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Import a module from a filesystem path.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        file_path: Path to Python file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Imported module object</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate a unique module name based on file path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use importlib.util.spec_from_file_location to create a module spec</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create the module from the spec</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute the module in its own namespace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add the module to sys.modules (temporarily)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return the module object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _find_tests_in_module</span><span style=\"color:#E1E4E8\">(module) -> Iterator[TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Yield TestCase objects for test functions found in a module.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        module: Imported module object</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Yields:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        TestCase objects for each discovered test</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all members of the module using dir(module)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each member, get the actual object using getattr(module, name)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if the object is a callable function (using inspect.isfunction)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check if the function name starts with 'test_'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If so, create a TestCase with appropriate nodeid, func, etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Also check for classes and their methods (xUnit style)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Yield each discovered TestCase</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Module Import Isolation:</strong> Use <code>importlib.import_module</code> with a fresh <code>sys.modules</code> copy for each test to prevent side effects. Store the original <code>sys.modules</code>, create a copy, run the test, then restore the original.</li>\n<li><strong>Parallel Execution:</strong> Start with <code>ThreadPoolExecutor</code> for I/O-bound tests. Use <code>futures.as_completed()</code> to process results as they complete rather than waiting for all tests.</li>\n<li><strong>Path Handling:</strong> Always convert paths to <code>pathlib.Path</code> objects early and use their methods (<code>resolve()</code>, <code>relative_to()</code>, <code>parent</code>) instead of string manipulation.</li>\n<li><strong>Inspection:</strong> Use <code>inspect.signature(func).parameters</code> to detect fixture dependencies from parameter names. Remember that <code>inspect.getsourcefile()</code> can get the file path of a function.</li>\n<li><strong>Dynamic Import:</strong> When importing test modules dynamically, generate a unique module name like <code>f&quot;__apollo_test_{hash(file_path)}&quot;</code> to avoid conflicts with existing modules.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the basic file structure and starter code:</p>\n<p><strong>Verify Setup:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># From the project root directory</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> scripts/cli.py</span><span style=\"color:#79B8FF\"> --version</span><span style=\"color:#6A737D\">  # Should show 0.1.0 if imports work</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"from src.apollo.data.test_case import TestCase; print('Data structures import OK')\"</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong> No import errors. The CLI script should run (though it won&#39;t do anything yet). The data structures should be importable without circular dependency issues.</p>\n<p><strong>Signs of Trouble:</strong></p>\n<ul>\n<li><code>ModuleNotFoundError: No module named &#39;apollo&#39;</code> → Check your <code>PYTHONPATH</code> or install the package in development mode with <code>pip install -e .</code></li>\n<li><code>ImportError: cannot import name &#39;TestCase&#39; from partially initialized module</code> → Circular dependency. Ensure <code>data/__init__.py</code> only imports from completed modules.</li>\n<li><code>AttributeError: module &#39;argparse&#39; has no attribute &#39;ArgumentParser&#39;</code> → Python version issue. Use Python 3.7+.</li>\n</ul>\n<h2 id=\"4-data-model\">4. Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All four milestones, as the data model defines the foundational structures that flow through every component.</p>\n</blockquote>\n<p>At the heart of every well-architected system lies a clear, consistent data model—the vocabulary through which components communicate and the scaffolding upon which behavior is built. Think of the data model as the <strong>DNA of the test framework</strong>: just as DNA encodes the instructions for building and operating an organism, our data structures encode everything about a test—its identity, its dependencies, its outcome, and its relationship to other tests. Without a precise, well-defined data model, components would speak different languages, leading to confusion, bugs, and an architecture that&#39;s difficult to extend.</p>\n<p>This section defines the core types that represent the essential concepts in our test framework—tests, results, and fixtures—along with supporting structures that orchestrate their interactions. These data structures flow through the pipeline architecture described earlier, transforming from one form to another as the framework discovers, executes, and reports on tests.</p>\n<h3 id=\"core-types\">Core Types</h3>\n<p>The core types are the fundamental building blocks that every component understands and manipulates. They represent the irreducible concepts in the testing domain.</p>\n<h4 id=\"teststatus-the-lifecycle-state-of-a-test\">TestStatus: The Lifecycle State of a Test</h4>\n<p>Think of <code>TestStatus</code> as the <strong>traffic light system</strong> for your tests. Just as a traffic light tells drivers whether to go, slow down, or stop, the <code>TestStatus</code> tells the framework what stage a test is in and what should happen next. This enumeration captures every possible state in a test&#39;s lifecycle, from its initial discovery to its final outcome.</p>\n<table>\n<thead>\n<tr>\n<th>Value</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PENDING</code></td>\n<td>The test has been discovered but not yet scheduled for execution. This is the initial state for all discovered tests.</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td>The test is currently being executed. This state is useful for tracking concurrent execution and providing real-time feedback.</td>\n</tr>\n<tr>\n<td><code>PASSED</code></td>\n<td>The test completed execution and all assertions within it passed. This is the desired outcome.</td>\n</tr>\n<tr>\n<td><code>FAILED</code></td>\n<td>The test completed execution but one or more assertions failed. The test logic worked correctly but the expected condition wasn&#39;t met.</td>\n</tr>\n<tr>\n<td><code>ERRORED</code></td>\n<td>The test encountered an unexpected exception during execution (e.g., a syntax error, missing import, or fixture setup failure). This indicates a problem with the test itself, not a failed assertion.</td>\n</tr>\n<tr>\n<td><code>SKIPPED</code></td>\n<td>The test was intentionally skipped, either through framework directives or user configuration. Skipped tests are not executed.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The distinction between <code>FAILED</code> and <code>ERRORED</code> is crucial for effective debugging. A failure means the code under test didn&#39;t behave as expected, while an error means the test itself is broken. Separating these states helps developers quickly diagnose whether they need to fix their production code or their test code.</p>\n</blockquote>\n<p><strong>Architecture Decision Record: Three-State vs. Six-State Test Outcomes</strong></p>\n<blockquote>\n<p><strong>Decision: Use a six-state enumeration for test results</strong></p>\n<ul>\n<li><strong>Context:</strong> We need to track test outcomes with enough granularity to support detailed reporting, CI integration, and user debugging. Some frameworks (like JUnit) use a simpler three-state model (pass/fail/error), while others (like pytest) have more nuanced states.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Three states:</strong> <code>PASS</code>, <code>FAIL</code>, <code>ERROR</code> (simplest model)</li>\n<li><strong>Four states:</strong> Add <code>SKIP</code> to three-state model</li>\n<li><strong>Six states:</strong> <code>PENDING</code>, <code>RUNNING</code>, <code>PASSED</code>, <code>FAILED</code>, <code>ERRORED</code>, <code>SKIPPED</code> (full lifecycle tracking)</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use the six-state enumeration as defined above.</li>\n<li><strong>Rationale:</strong> The additional states provide valuable tracking for parallel execution (<code>RUNNING</code>), test discovery (<code>PENDING</code>), and intentional test exclusion (<code>SKIPPED</code>). This granularity enables richer reporting (e.g., showing currently executing tests) and better integration with CI systems that distinguish between skipped and pending tests. The <code>ERRORED</code>/<code>FAILED</code> distinction is particularly valuable for test maintenance.</li>\n<li><strong>Consequences:</strong> More complex state management throughout the framework, but enables more sophisticated reporting and debugging features. The state machine (shown below) becomes more elaborate but more informative.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Three states</td>\n<td>Simple to implement, easy to reason about</td>\n<td>Loses valuable diagnostic information; can&#39;t distinguish skipped from pending tests</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Four states</td>\n<td>Adds skip capability useful for conditional tests</td>\n<td>Still misses execution lifecycle states needed for parallel execution</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Six states</td>\n<td>Full lifecycle tracking, supports parallel execution reporting, clear error/failure distinction</td>\n<td>More complex implementation, more states to handle</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<p>The state transitions for a <code>TestResult</code> follow a clear lifecycle, which we can visualize as a state machine:</p>\n<p><img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Ftest-state-machine.svg\" alt=\"Test Result State Machine\"></p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PENDING</code></td>\n<td><code>test_started</code></td>\n<td><code>RUNNING</code></td>\n<td>Record start timestamp, begin test execution</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>assertion_passed</code></td>\n<td><code>RUNNING</code></td>\n<td>Continue test execution (no state change)</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>assertion_failed</code></td>\n<td><code>FAILED</code></td>\n<td>Record failure message and traceback, stop test execution</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>exception_raised</code></td>\n<td><code>ERRORED</code></td>\n<td>Record exception and traceback, stop test execution</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>test_completed</code></td>\n<td><code>PASSED</code></td>\n<td>Record end timestamp, calculate duration</td>\n</tr>\n<tr>\n<td><code>PENDING</code></td>\n<td><code>test_skipped</code></td>\n<td><code>SKIPPED</code></td>\n<td>Record skip reason, never execute test</td>\n</tr>\n<tr>\n<td>Any state</td>\n<td><code>execution_interrupted</code></td>\n<td><code>ERRORED</code></td>\n<td>Record interruption reason (e.g., keyboard interrupt)</td>\n</tr>\n</tbody></table>\n<h4 id=\"testcase-the-blueprint-of-a-test\">TestCase: The Blueprint of a Test</h4>\n<p>A <code>TestCase</code> represents a single test that can be executed. Think of it as a <strong>recipe card</strong> for a test: it contains all the information needed to prepare and execute the test, but none of the results. Just as a recipe card lists ingredients (fixtures) and instructions (the test function) without tracking whether you burned the dinner, the <code>TestCase</code> describes <em>what</em> to test, not <em>how it went</em>.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>nodeid</code></td>\n<td><code>str</code></td>\n<td>A unique identifier for the test, following a hierarchical path format (e.g., <code>&quot;tests/test_math.py::test_addition&quot;</code> or <code>&quot;tests/test_math.py::TestCalculator::test_division&quot;</code>). This serves as the test&#39;s primary key throughout the framework.</td>\n</tr>\n<tr>\n<td><code>func</code></td>\n<td><code>Callable</code></td>\n<td>The actual test function or method to be executed. This is a callable Python object that takes zero or more parameters (for fixture injection).</td>\n</tr>\n<tr>\n<td><code>file_path</code></td>\n<td><code>str</code></td>\n<td>Absolute filesystem path to the Python file containing this test. Used for module imports and error reporting.</td>\n</tr>\n<tr>\n<td><code>line_no</code></td>\n<td><code>int</code></td>\n<td>Line number in the source file where the test function is defined. Crucial for IDE integration and pinpointing test location in failure reports.</td>\n</tr>\n<tr>\n<td><code>fixtures</code></td>\n<td><code>list[str]</code></td>\n<td>Names of fixtures required by this test, extracted from the function&#39;s parameter names. This drives the dependency injection system.</td>\n</tr>\n</tbody></table>\n<p><strong>Example Walkthrough:</strong> Consider a test function defined in <code>/home/user/project/tests/test_math.py</code> at line 42:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_addition</span><span style=\"color:#E1E4E8\">(db_connection, mock_logger):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 4</span></span></code></pre></div>\n<p>The corresponding <code>TestCase</code> would be:</p>\n<ul>\n<li><code>nodeid</code>: <code>&quot;tests/test_math.py::test_addition&quot;</code></li>\n<li><code>func</code>: The <code>test_addition</code> function object</li>\n<li><code>file_path</code>: <code>&quot;/home/user/project/tests/test_math.py&quot;</code></li>\n<li><code>line_no</code>: <code>42</code></li>\n<li><code>fixtures</code>: <code>[&quot;db_connection&quot;, &quot;mock_logger&quot;]</code></li>\n</ul>\n<blockquote>\n<p><strong>Design Insight:</strong> The <code>nodeid</code> format follows a convention-over-configuration approach inspired by pytest. It&#39;s human-readable, parseable, and provides a natural hierarchy (file → class → test). This format enables powerful filtering (e.g., run all tests in <code>test_math.py</code> or all tests in <code>TestCalculator</code> class) without complex configuration.</p>\n</blockquote>\n<h4 id=\"testresult-the-outcome-of-a-test-execution\">TestResult: The Outcome of a Test Execution</h4>\n<p>While <code>TestCase</code> describes <em>what</em> to run, <code>TestResult</code> captures <em>what happened</em> when it ran. Think of it as the <strong>medical chart</strong> for a test: it records symptoms (failures), diagnoses (error types), treatment duration (execution time), and final outcome. Just as a medical chart follows a patient through their hospital stay, the <code>TestResult</code> follows a test through its execution lifecycle.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_case</code></td>\n<td><code>TestCase</code></td>\n<td>Reference to the test case that was executed. This maintains the connection between outcome and blueprint.</td>\n</tr>\n<tr>\n<td><code>status</code></td>\n<td><code>TestStatus</code></td>\n<td>The final state of the test after execution (one of <code>PASSED</code>, <code>FAILED</code>, <code>ERRORED</code>, <code>SKIPPED</code>).</td>\n</tr>\n<tr>\n<td><code>message</code></td>\n<td><code>Optional[str]</code></td>\n<td>Human-readable description of what happened, especially for failures and errors. For assertions, this includes the failure message; for errors, it might include the exception message.</td>\n</tr>\n<tr>\n<td><code>exception</code></td>\n<td><code>Optional[Exception]</code></td>\n<td>The actual exception object raised during test execution (if any). Preserved for advanced debugging and programmatic inspection.</td>\n</tr>\n<tr>\n<td><code>traceback</code></td>\n<td><code>Optional[str]</code></td>\n<td>Formatted traceback string showing the call stack at the point of failure. Essential for debugging test errors.</td>\n</tr>\n<tr>\n<td><code>duration</code></td>\n<td><code>float</code></td>\n<td>Execution time in seconds, measured with high precision (typically using <code>time.perf_counter()</code>).</td>\n</tr>\n</tbody></table>\n<p><strong>Example Scenario:</strong> When <code>test_addition</code> from our previous example fails due to <code>assert result == 5</code>, the resulting <code>TestResult</code> would contain:</p>\n<ul>\n<li><code>test_case</code>: The <code>TestCase</code> object for <code>test_addition</code></li>\n<li><code>status</code>: <code>FAILED</code></li>\n<li><code>message</code>: <code>&quot;AssertionError: Expected 5 but got 4&quot;</code></li>\n<li><code>exception</code>: The <code>AssertionError</code> instance</li>\n<li><code>traceback</code>: <code>&quot;Traceback (most recent call last):\\n  File \\&quot;/home/user/project/tests/test_math.py\\&quot;, line 44, in test_addition\\n    assert result == 5\\nAssertionError: Expected 5 but got 4&quot;</code></li>\n<li><code>duration</code>: <code>0.000142</code> (seconds)</li>\n</ul>\n<blockquote>\n<p><strong>Architecture Decision Record: Storing Exceptions vs. Serializing Them</strong></p>\n<p><strong>Decision: Store both the exception object and a formatted traceback string</strong></p>\n<ul>\n<li><strong>Context:</strong> When tests fail or error, we need to capture diagnostic information for reporting and debugging. Exceptions in Python contain valuable information but can be tricky to serialize/store long-term.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Store only exception object:</strong> Keep the live exception instance for programmatic access</li>\n<li><strong>Store only serialized form:</strong> Convert to string immediately for simplicity</li>\n<li><strong>Store both:</strong> Keep the exception object and a pre-formatted traceback string</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Store both the exception object (<code>exception</code>) and formatted traceback string (<code>traceback</code>).</li>\n<li><strong>Rationale:</strong> The exception object is valuable for advanced users who might want to inspect exception attributes programmatically (e.g., custom assertion libraries). The pre-formatted traceback string ensures we have a human-readable representation even if the exception object becomes problematic to serialize (e.g., for parallel execution where objects might need to be pickled). This dual approach provides maximum utility with reasonable complexity.</li>\n<li><strong>Consequences:</strong> Slightly more memory usage per test result, but enables both programmatic inspection and reliable reporting.</li>\n</ul>\n</blockquote>\n<h4 id=\"fixture-reusable-test-resources\">Fixture: Reusable Test Resources</h4>\n<p>Fixtures are the <strong>stage crew</strong> of the test framework. While tests are the actors performing the main show, fixtures work behind the scenes to set up the stage (setup), provide props (test data), and clean up afterward (teardown). A <code>Fixture</code> object describes one such crew member—what resources it provides, how to create them, and who depends on it.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>name</code></td>\n<td><code>str</code></td>\n<td>The unique name by which this fixture is referenced in test parameter lists. This serves as the key for dependency resolution.</td>\n</tr>\n<tr>\n<td><code>func</code></td>\n<td><code>Callable</code></td>\n<td>The fixture function that, when called, returns the fixture value. This may include setup and teardown logic (especially if implemented as a generator).</td>\n</tr>\n<tr>\n<td><code>scope</code></td>\n<td><code>str</code></td>\n<td>Determines the lifecycle and caching of the fixture. One of: <code>&quot;function&quot;</code>, <code>&quot;class&quot;</code>, <code>&quot;module&quot;</code>, <code>&quot;session&quot;</code>.</td>\n</tr>\n<tr>\n<td><code>dependencies</code></td>\n<td><code>list[str]</code></td>\n<td>Names of other fixtures that this fixture depends on, extracted from its parameter names. Forms a directed acyclic graph (DAG) of dependencies.</td>\n</tr>\n</tbody></table>\n<p><strong>Fixture Scope Behavior:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scope</th>\n<th>Creation Time</th>\n<th>Teardown Time</th>\n<th>Shared Across</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>&quot;function&quot;</code></td>\n<td>Before each test that uses it</td>\n<td>After each test that uses it</td>\n<td>No tests (fresh per test)</td>\n</tr>\n<tr>\n<td><code>&quot;class&quot;</code></td>\n<td>Before first test in a class</td>\n<td>After last test in a class</td>\n<td>All tests in the same test class</td>\n</tr>\n<tr>\n<td><code>&quot;module&quot;</code></td>\n<td>Before first test in a module</td>\n<td>After last test in a module</td>\n<td>All tests in the same module</td>\n</tr>\n<tr>\n<td><code>&quot;session&quot;</code></td>\n<td>Once at test session start</td>\n<td>After all tests complete</td>\n<td>All tests in the entire test run</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Mental Model:</strong> Think of fixture scopes as <strong>caching policies with cleanup</strong>. A function-scoped fixture is like disposable cutlery—used once and thrown away. A session-scoped fixture is like the venue itself—set up once for the entire performance, then cleaned up after everyone leaves. The scope determines the trade-off between isolation (safety) and performance (speed).</p>\n</blockquote>\n<p><strong>Example:</strong> A database connection fixture might be defined as:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> db_connection</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_expensive_database_connection()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    yield</span><span style=\"color:#E1E4E8\"> conn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.close()</span></span></code></pre></div>\n<p>The corresponding <code>Fixture</code> object would have:</p>\n<ul>\n<li><code>name</code>: <code>&quot;db_connection&quot;</code></li>\n<li><code>func</code>: The <code>db_connection</code> generator function</li>\n<li><code>scope</code>: <code>&quot;function&quot;</code> (default if not specified)</li>\n<li><code>dependencies</code>: <code>[]</code> (no parameters, so no dependencies)</li>\n</ul>\n<p>If it depended on a configuration fixture:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> db_connection</span><span style=\"color:#E1E4E8\">(config):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_connection(config.database_url)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    yield</span><span style=\"color:#E1E4E8\"> conn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.close()</span></span></code></pre></div>\n<p>Then <code>dependencies</code> would be <code>[&quot;config&quot;]</code>.</p>\n<blockquote>\n<p><strong>Architecture Decision Record: Generator-Based vs. Context Manager Fixtures</strong></p>\n<p><strong>Decision: Use generator functions for fixture lifecycle management</strong></p>\n<ul>\n<li><strong>Context:</strong> Fixtures need to support both setup and teardown logic. We need a clean way to specify &quot;do this before the test, provide this value during the test, do this after the test.&quot;</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Separate setup/teardown functions:</strong> Explicit <code>setup_fixture()</code> and <code>teardown_fixture()</code> functions</li>\n<li><strong>Context managers:</strong> Use <code>@contextmanager</code> decorator and <code>with</code> statement</li>\n<li><strong>Generator functions:</strong> Use <code>yield</code> to separate setup from teardown (pytest&#39;s approach)</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use generator functions where <code>yield</code> returns the fixture value.</li>\n<li><strong>Rationale:</strong> Generator functions provide a natural, linear flow for setup/teardown that&#39;s easy for users to understand. The code before <code>yield</code> is setup, the value after <code>yield</code> is the fixture value, and code after <code>yield</code> is teardown. This pattern also handles exceptions gracefully—if a test fails, the code after <code>yield</code> still executes (like a <code>finally</code> block). It&#39;s a proven pattern adopted by pytest that balances simplicity with power.</li>\n<li><strong>Consequences:</strong> Users must understand generator semantics. The framework must carefully handle generator lifecycle (calling <code>next()</code> to get the value, then ensuring the generator completes even if tests fail).</li>\n</ul>\n</blockquote>\n<h3 id=\"supporting-structures\">Supporting Structures</h3>\n<p>While core types represent the fundamental testing concepts, supporting structures provide the organizational scaffolding that orchestrates their interactions. These are the <strong>containers, configuration, and coordination mechanisms</strong> that allow the core types to work together effectively.</p>\n<h4 id=\"testsuite-organizing-tests-for-execution\">TestSuite: Organizing Tests for Execution</h4>\n<p>A <code>TestSuite</code> is a <strong>playlist of tests</strong>—it groups related tests together for organized execution, much like a music playlist groups songs by mood or genre. While individual tests can be executed independently, suites provide logical grouping for reporting, filtering, and parallel execution strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>name</code></td>\n<td><code>str</code></td>\n<td>Descriptive name for the suite, typically derived from the discovery path or pattern (e.g., <code>&quot;tests/&quot;</code> or <code>&quot;math tests&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>tests</code></td>\n<td><code>list[TestCase]</code></td>\n<td>Ordered collection of test cases to be executed as part of this suite. Order may reflect discovery order or be shuffled for certain testing strategies.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The <code>TestSuite</code> serves as the primary data structure passed from the Discoverer to the Runner. Its simplicity (just a name and list) belies its importance as the <strong>handoff point</strong> between discovery and execution phases. By keeping it minimal, we allow different discovery strategies (e.g., pattern-based, tag-based) to all produce the same structure for the runner to consume.</p>\n</blockquote>\n<p><strong>Suite Organization Strategies:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>How Tests Are Grouped</th>\n<th>When to Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>By module</strong></td>\n<td>All tests from one Python file in one suite</td>\n<td>Simple projects, module-level parallelism</td>\n</tr>\n<tr>\n<td><strong>By directory</strong></td>\n<td>All tests from a directory tree in one suite</td>\n<td>Large projects with hierarchical organization</td>\n</tr>\n<tr>\n<td><strong>By test type</strong></td>\n<td>Unit tests, integration tests in separate suites</td>\n<td>Mixed test types with different resource requirements</td>\n</tr>\n<tr>\n<td><strong>Dynamic grouping</strong></td>\n<td>Tests grouped by estimated execution time</td>\n<td>Optimizing parallel execution balance</td>\n</tr>\n</tbody></table>\n<h4 id=\"configuration-framework-behavior-as-data\">Configuration: Framework Behavior as Data</h4>\n<p>The <code>Configuration</code> object is the <strong>control panel</strong> for the test framework—every knob, switch, and dial that affects framework behavior is captured here. Think of it as a recipe that specifies not what to test, but <em>how</em> to test: which tests to run, how to run them, and what to do with the results.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>file_patterns</code></td>\n<td><code>list[str]</code></td>\n<td>Glob patterns for test file discovery (e.g., <code>[&quot;test_*.py&quot;, &quot;*_test.py&quot;]</code>). Defaults to framework conventions.</td>\n</tr>\n<tr>\n<td><code>start_dir</code></td>\n<td><code>Path</code></td>\n<td>Directory to start discovery from. Typically the current working directory or a user-specified path.</td>\n</tr>\n<tr>\n<td><code>test_name_filters</code></td>\n<td><code>list[str]</code></td>\n<td>Substring or regex patterns to filter test names (e.g., <code>[&quot;test_addition&quot;, &quot;.*math.*&quot;]</code>).</td>\n</tr>\n<tr>\n<td><code>verbose</code></td>\n<td><code>bool</code></td>\n<td>Enable detailed output, including test names as they run and additional diagnostic information.</td>\n</tr>\n<tr>\n<td><code>quiet</code></td>\n<td><code>bool</code></td>\n<td>Minimize output, showing only final summary or errors. Mutually exclusive with <code>verbose</code>.</td>\n</tr>\n<tr>\n<td><code>output_format</code></td>\n<td><code>str</code></td>\n<td>Format for test results: <code>&quot;terminal&quot;</code> for human-readable, <code>&quot;junit-xml&quot;</code> for CI integration.</td>\n</tr>\n<tr>\n<td><code>parallel</code></td>\n<td><code>bool</code></td>\n<td>Whether to run tests in parallel. Enables concurrent execution for speed.</td>\n</tr>\n<tr>\n<td><code>max_workers</code></td>\n<td><code>Optional[int]</code></td>\n<td>Maximum number of concurrent worker processes/threads for parallel execution. <code>None</code> means use CPU count.</td>\n</tr>\n<tr>\n<td><code>show_durations</code></td>\n<td><code>bool</code></td>\n<td>Whether to report individual test execution times in the summary.</td>\n</tr>\n<tr>\n<td><code>junit_xml_path</code></td>\n<td><code>Optional[Path]</code></td>\n<td>Filesystem path where JUnit XML report should be written (if <code>output_format</code> includes XML).</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision Record: Configuration as Immutable Data Structure</strong></p>\n<p><strong>Decision: Treat Configuration as an immutable data class after parsing</strong></p>\n<ul>\n<li><strong>Context:</strong> Configuration flows through multiple components, and we need to ensure consistency—no component should modify configuration in ways that affect other components unexpectedly.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Mutable dictionary/object:</strong> Allow components to modify configuration as needed</li>\n<li><strong>Immutable data class:</strong> Freeze configuration after CLI parsing</li>\n<li><strong>Hierarchical configuration:</strong> Separate global, project, and runtime configuration layers</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use an immutable data class (Python&#39;s <code>dataclass</code> with <code>frozen=True</code>).</li>\n<li><strong>Rationale:</strong> Immutability prevents subtle bugs where one component&#39;s configuration changes affect another component unexpectedly. It also makes the configuration easier to reason about—you can pass it anywhere without worrying about side effects. The hierarchical configuration approach is overkill for our educational framework.</li>\n<li><strong>Consequences:</strong> Components that need to extend configuration (e.g., with derived values) must create new configuration objects. This is slightly more verbose but leads to more predictable behavior.</li>\n</ul>\n</blockquote>\n<h4 id=\"supporting-types-for-fixture-system\">Supporting Types for Fixture System</h4>\n<p>The fixture system requires additional structures to manage its complexity:</p>\n<p><strong>FixtureScope Enum:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Value</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>FUNCTION</code></td>\n<td>Fixture is created and torn down for each test function (default).</td>\n</tr>\n<tr>\n<td><code>CLASS</code></td>\n<td>Fixture persists for all tests in a single test class.</td>\n</tr>\n<tr>\n<td><code>MODULE</code></td>\n<td>Fixture persists for all tests in a single module.</td>\n</tr>\n<tr>\n<td><code>SESSION</code></td>\n<td>Fixture persists for the entire test session.</td>\n</tr>\n</tbody></table>\n<p><strong>FixtureRequest: Context for Fixture Execution</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>fixture_name</code></td>\n<td><code>str</code></td>\n<td>Name of the fixture being requested.</td>\n</tr>\n<tr>\n<td><code>scope</code></td>\n<td><code>FixtureScope</code></td>\n<td>Scope at which this fixture is being instantiated.</td>\n</tr>\n<tr>\n<td><code>test_case</code></td>\n<td><code>Optional[TestCase]</code></td>\n<td>The test case that triggered this request (if any).</td>\n</tr>\n<tr>\n<td><code>cache_key</code></td>\n<td><code>tuple</code></td>\n<td>A hashable key used for caching fixtures (combines fixture name, scope, and scope-specific identifiers).</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Mental Model:</strong> Think of <code>FixtureRequest</code> as a <strong>work order</strong> for the fixture factory. When a test needs a fixture, it doesn&#39;t call the fixture function directly—instead, it creates a work order describing what&#39;s needed, and the fixture system processes it according to the fixture&#39;s scope and dependencies, possibly reusing cached results from previous work orders.</p>\n</blockquote>\n<h4 id=\"relationship-diagram\">Relationship Diagram</h4>\n<p>The relationships between these data structures form the backbone of the framework&#39;s information architecture:</p>\n<p><img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Fdata-model-diag.svg\" alt=\"Data Model Relationships\"></p>\n<p>Key relationships depicted:</p>\n<ol>\n<li><strong>Aggregation:</strong> <code>TestSuite</code> contains multiple <code>TestCase</code> objects</li>\n<li><strong>Production:</strong> Running a <code>TestCase</code> produces a <code>TestResult</code></li>\n<li><strong>Dependency:</strong> <code>TestCase</code> depends on <code>Fixture</code> objects (through the <code>fixtures</code> list)</li>\n<li><strong>Composition:</strong> <code>TestResult</code> contains a reference to its <code>TestCase</code></li>\n<li><strong>Dependency Graph:</strong> <code>Fixture</code> objects can depend on other <code>Fixture</code> objects</li>\n</ol>\n<h4 id=\"data-flow-through-the-pipeline\">Data Flow Through the Pipeline</h4>\n<p>To understand how these data structures interact, let&#39;s trace a test through the framework:</p>\n<ol>\n<li><p><strong>Discovery Phase:</strong> </p>\n<ul>\n<li>Input: <code>Configuration</code> (specifying <code>start_dir</code> and <code>file_patterns</code>)</li>\n<li>Process: <code>Discoverer</code> scans filesystem, imports modules, inspects functions</li>\n<li>Output: <code>TestSuite</code> containing <code>TestCase</code> objects</li>\n<li>Data transformation: <code>Path</code> → <code>Module</code> → <code>Callable</code> → <code>TestCase</code> → <code>TestSuite</code></li>\n</ul>\n</li>\n<li><p><strong>Fixture Resolution Phase:</strong></p>\n<ul>\n<li>Input: <code>TestCase</code> (with <code>fixtures</code> list) + <code>Fixture</code> registry</li>\n<li>Process: Resolve dependencies, check scopes, create caching keys</li>\n<li>Output: <code>FixtureRequest</code> objects for each required fixture</li>\n<li>Data transformation: <code>list[str]</code> (fixture names) → DAG of <code>FixtureRequest</code> objects</li>\n</ul>\n</li>\n<li><p><strong>Execution Phase:</strong></p>\n<ul>\n<li>Input: <code>TestCase</code> + resolved fixture values</li>\n<li>Process: Execute test function with injected fixtures</li>\n<li>Output: <code>TestResult</code> with status, duration, and error info</li>\n<li>Data transformation: <code>TestCase</code> + fixture values → execution → <code>TestResult</code></li>\n</ul>\n</li>\n<li><p><strong>Reporting Phase:</strong></p>\n<ul>\n<li>Input: Collection of <code>TestResult</code> objects + <code>Configuration</code> (for output format)</li>\n<li>Process: Format results, calculate statistics, write output</li>\n<li>Output: Human-readable report or JUnit XML</li>\n<li>Data transformation: <code>list[TestResult]</code> → aggregated statistics → formatted output</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<h4 id=\"-pitfall-mutable-default-arguments-in-data-classes\">⚠️ <strong>Pitfall: Mutable default arguments in data classes</strong></h4>\n<ul>\n<li><strong>Description:</strong> Using mutable default values like <code>fixtures=[]</code> in data class definitions.</li>\n<li><strong>Why it&#39;s wrong:</strong> In Python, default argument values are evaluated once at function/class definition time, not each time an instance is created. All instances would share the same list object, leading to cross-test contamination.</li>\n<li><strong>Fix:</strong> Use <code>field(default_factory=list)</code> from the <code>dataclasses</code> module, which creates a new list for each instance.</li>\n</ul>\n<h4 id=\"-pitfall-forgetting-to-handle-generator-completion-in-fixtures\">⚠️ <strong>Pitfall: Forgetting to handle generator completion in fixtures</strong></h4>\n<ul>\n<li><strong>Description:</strong> When a fixture uses <code>yield</code>, the framework might not properly call <code>generator.close()</code> if a test fails, leaving resources dangling.</li>\n<li><strong>Why it&#39;s wrong:</strong> The teardown code (after <code>yield</code>) won&#39;t execute, causing resource leaks (unclosed files, database connections, etc.).</li>\n<li><strong>Fix:</strong> Always wrap fixture execution in a <code>try/finally</code> block that ensures the generator is exhausted even on exceptions.</li>\n</ul>\n<h4 id=\"-pitfall-using-floats-for-duration-comparison\">⚠️ <strong>Pitfall: Using floats for duration comparison</strong></h4>\n<ul>\n<li><strong>Description:</strong> Comparing execution durations directly with <code>==</code> or threshold checks using raw floats.</li>\n<li><strong>Why it&#39;s wrong:</strong> Floating-point arithmetic has precision limitations; two durations that should be equal might differ by tiny fractions due to rounding errors.</li>\n<li><strong>Fix:</strong> Use <code>math.isclose()</code> for comparisons or store durations as <code>decimal.Decimal</code> if precise arithmetic is needed.</li>\n</ul>\n<h4 id=\"-pitfall-storing-absolute-paths-in-testcase\">⚠️ <strong>Pitfall: Storing absolute paths in TestCase</strong></h4>\n<ul>\n<li><strong>Description:</strong> Storing absolute filesystem paths in <code>TestCase.file_path</code>.</li>\n<li><strong>Why it&#39;s wrong:</strong> Tests become non-portable—the same test suite would have different <code>nodeid</code> values on different machines or CI environments.</li>\n<li><strong>Fix:</strong> Store paths relative to a project root directory, or make them absolute but be aware of the portability trade-off. For our framework, we&#39;ll store absolute paths for simplicity but provide helpers to make them relative in reports.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Classes</td>\n<td>Python&#39;s built-in <code>dataclasses</code> module</td>\n<td><code>attrs</code> library for more features and performance</td>\n</tr>\n<tr>\n<td>Enum Implementation</td>\n<td><code>enum.Enum</code> from standard library</td>\n<td><code>enum.IntEnum</code> if integer values needed for serialization</td>\n</tr>\n<tr>\n<td>Path Handling</td>\n<td><code>pathlib.Path</code> from standard library</td>\n<td><code>pathlib</code> with custom normalization for cross-platform consistency</td>\n</tr>\n<tr>\n<td>Type Hints</td>\n<td>Basic type hints (<code>List[str]</code>, <code>Optional[int]</code>)</td>\n<td>Full mypy-compatible hints with generics and protocols</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>Manual dict conversion for JSON/XML</td>\n<td><code>pydantic</code> for validation and serialization</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n  ├── __init__.py              # Public API exports\n  ├── core/                    # Core data types and interfaces\n  │   ├── __init__.py\n  │   ├── types.py            # TestStatus, TestCase, TestResult, etc.\n  │   ├── fixtures.py         # Fixture, FixtureRequest, FixtureScope\n  │   └── configuration.py    # Configuration dataclass\n  ├── discovery/              # Test discovery components\n  ├── runner/                 # Test execution components  \n  ├── assertions/             # Assertion engine\n  ├── reporting/              # Reporting components\n  └── cli/                    # CLI interface</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>core/types.py - Foundation Data Types:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Core data types for the test framework.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#79B8FF\"> __future__</span><span style=\"color:#F97583\"> import</span><span style=\"color:#E1E4E8\"> annotations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Callable, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Possible states of a test during execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PASSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"passed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERRORED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"errored\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestCase</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single test that can be executed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeid: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    func: Callable[</span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line_no: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fixtures: list[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.nodeid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Captures the outcome of executing a TestCase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_case: TestCase</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: TestStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exception: Optional[</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    traceback: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _start_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">init</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">repr</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_timer</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start timing the test execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop_timer</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop timing and calculate duration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._start_time </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.perf_counter() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._start_time</span></span></code></pre></div>\n\n<p><strong>core/fixtures.py - Fixture Foundation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Fixture-related data types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Callable, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FixtureScope</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Scope determining fixture lifecycle and caching.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FUNCTION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"function\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CLASS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"class\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MODULE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"module\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SESSION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"session\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Fixture</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a reusable test resource with setup/teardown.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    func: Callable[</span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scope: FixtureScope </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FixtureScope.</span><span style=\"color:#79B8FF\">FUNCTION</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: list[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Fixture(name=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, scope=</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.scope.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FixtureRequest</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context for creating or retrieving a fixture.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fixture_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scope: FixtureScope</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_case: Optional[TestCase] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache_key: </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_cached</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Whether this fixture should be cached based on scope.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.scope </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> FixtureScope.</span><span style=\"color:#79B8FF\">FUNCTION</span></span></code></pre></div>\n\n<p><strong>core/configuration.py - CLI Configuration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Configuration data structure parsed from CLI arguments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Configuration</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Immutable configuration for test framework behavior.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file_patterns: list[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"test_*.py\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"*_test.py\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_dir: Path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: Path.cwd())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_name_filters: list[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    verbose: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quiet: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"terminal\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parallel: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_workers: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    show_durations: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    junit_xml_path: Optional[Path] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration after initialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.verbose </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.quiet:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Cannot set both verbose and quiet modes\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.output_format </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"terminal\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"junit-xml\"</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid output format: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.output_format</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>core/types.py - TestResult state management:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... (fields and timer methods from above)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_passed</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark the test as passed successfully.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stop_timer()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">PASSED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_failed</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, exception: Optional[</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Mark the test as failed due to assertion failure.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            message: Human-readable failure description</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            exception: The AssertionError that was raised</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stop_timer()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">FAILED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.exception </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exception</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Capture traceback from exception if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_errored</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, exception: Optional[</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Mark the test as errored due to unexpected exception.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            message: Human-readable error description</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            exception: The unexpected exception that was raised</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stop_timer()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">ERRORED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.exception </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exception</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Capture full traceback for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_skipped</span><span style=\"color:#E1E4E8\">(self, reason: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark the test as skipped with a reason.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">SKIPPED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> reason</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_successful</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return True if test passed or was skipped.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (TestStatus.</span><span style=\"color:#79B8FF\">PASSED</span><span style=\"color:#E1E4E8\">, TestStatus.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ol>\n<li><p><strong>Use <code>@dataclass(frozen=True)</code> for Configuration:</strong> This makes instances immutable, preventing accidental modification after parsing.</p>\n</li>\n<li><p><strong>Leverage <code>enum.auto()</code> for TestStatus:</strong> If you don&#39;t care about the string values, use <code>auto()</code> to let Python assign values automatically.</p>\n</li>\n<li><p><strong>Import <code>annotations</code> from future:</strong> This allows forward references in type hints (e.g., <code>&#39;TestCase&#39;</code> as string) so you can reference types before they&#39;re defined.</p>\n</li>\n<li><p><strong>Use <code>field(default_factory=list)</code> not <code>field(default=[])</code>:</strong> This ensures each instance gets a new list, not a shared reference.</p>\n</li>\n<li><p><strong>Consider <code>__slots__</code> for performance:</strong> If you create thousands of TestResult objects, adding <code>__slots__</code> to the dataclass can reduce memory usage.</p>\n</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p><strong>After implementing the data model (prerequisite for all milestones):</strong></p>\n<ol>\n<li><strong>Verification Test:</strong> Create a simple test script that exercises each data type:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # test_data_model.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> apollo.core.types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestStatus, TestCase, TestResult</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> apollo.core.fixtures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Fixture, FixtureScope</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   from</span><span style=\"color:#E1E4E8\"> apollo.core.configuration </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> test_example</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Create instances</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   tc </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestCase(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       nodeid</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"test_example\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       func</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">test_example,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       file_path</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">__file__</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">       line_no</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestResult(</span><span style=\"color:#FFAB70\">test_case</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tc)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   result.start_timer()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Simulate test execution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   result.mark_passed()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   assert</span><span style=\"color:#E1E4E8\"> result.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">PASSED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   assert</span><span style=\"color:#E1E4E8\"> result.duration </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Configuration(</span><span style=\"color:#FFAB70\">verbose</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   assert</span><span style=\"color:#E1E4E8\"> config.verbose </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><p><strong>Expected Behavior:</strong> The script should run without errors and demonstrate that all data types can be instantiated correctly.</p>\n</li>\n<li><p><strong>Signs of Problems:</strong> </p>\n<ul>\n<li>Import errors: Check that your <code>apollo/</code> directory has an <code>__init__.py</code> and the module structure matches.</li>\n<li>Dataclass issues: Ensure you&#39;re using Python 3.7+ or have the dataclasses backport installed.</li>\n<li>Circular imports: Use string type hints or import inside methods if types reference each other.</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h2 id=\"5-component-design-discovery-amp-execution-milestone-1\">5. Component Design: Discovery &amp; Execution (Milestone 1)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Test Discovery &amp; Execution)</p>\n</blockquote>\n<p>The <strong>Discovery &amp; Execution</strong> subsystem forms the beating heart of the test framework — it&#39;s responsible for finding all the tests in your codebase and running them in a controlled, predictable manner. Think of this subsystem as a <strong>concert production team</strong> that first scouts for musicians (discovery), then ensures each performer gets their own soundproof practice room to avoid interfering with others (isolation), and finally schedules multiple rehearsals simultaneously to save time (parallel execution). This component transforms raw Python functions into executable test cases and orchestrates their execution while maintaining the fundamental guarantee that one test&#39;s failures don&#39;t cascade to others.</p>\n<h3 id=\"the-discoverer\">The Discoverer</h3>\n<p><strong>Mental Model: The Talent Scout</strong><br>Imagine you&#39;re organizing a massive music festival with thousands of potential performers scattered across a city. You need a methodical talent scout who can:</p>\n<ol>\n<li>Systematically search through every building (directory) </li>\n<li>Identify individuals who look like musicians (functions matching naming conventions)</li>\n<li>Create a registry with their name, location, and special requirements</li>\n<li>Ignore people who aren&#39;t performers (non-test code)</li>\n</ol>\n<p>The Discoverer serves exactly this role in our test framework. It&#39;s the component that scans your codebase, identifies test functions based on naming conventions, and creates a structured inventory (a <code>TestSuite</code>) of all tests to be executed.</p>\n<h4 id=\"architecture-decision-convention-over-configuration-for-test-discovery\">Architecture Decision: Convention Over Configuration for Test Discovery</h4>\n<blockquote>\n<p><strong>Decision: Use Naming Convention-Based Discovery</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to automatically find test functions without requiring users to manually register each test. Test frameworks must balance explicitness (knowing exactly what will run) with convenience (not having to maintain a registry).</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Explicit Registration</strong>: Users manually list tests in a configuration file or decorator registry</li>\n<li><strong>File-System Scanning</strong>: Run all <code>.py</code> files in a test directory regardless of content</li>\n<li><strong>Naming Convention</strong>: Find functions/classes whose names start with <code>test_</code> or contain <code>Test</code> prefixes</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use naming convention-based discovery (option 3) with configurable patterns</li>\n<li><strong>Rationale</strong>: This approach follows the principle of &quot;convention over configuration&quot; that has made frameworks like pytest successful. It requires zero boilerplate from users while providing predictable behavior. The explicit registration approach adds maintenance overhead, while file-system scanning could run non-test code accidentally.</li>\n<li><strong>Consequences</strong>: Tests are automatically discovered based on their names, making the framework intuitive for new users. However, this means refactoring that changes function names could accidentally exclude tests from the test suite.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Explicit Registration</td>\n<td>Clear visibility of what runs; No false positives</td>\n<td>High maintenance; Boilerplate code</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>File-System Scanning</td>\n<td>Simple implementation; No naming rules</td>\n<td>Runs non-test code; Dangerous</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Naming Convention</td>\n<td>Zero configuration; Predictable patterns; Industry standard</td>\n<td>Refactoring risks; Limited expressiveness</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"discoverer-components-and-data-flow\">Discoverer Components and Data Flow</h4>\n<p>The Discoverer consists of three logical subcomponents that work together in a pipeline:</p>\n<ol>\n<li><strong>Path Resolver</strong>: Converts file patterns (like <code>tests/unit/*.py</code>) to concrete file system paths</li>\n<li><strong>Module Loader</strong>: Safely imports Python modules from file paths without side effects</li>\n<li><strong>Test Extractor</strong>: Inspects module contents and yields <code>TestCase</code> objects for test functions</li>\n</ol>\n<p>The following table details the complete interface for the Discoverer component:</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>discover_tests</code></td>\n<td><code>start_path: Path, pattern: str = &quot;test_*.py&quot;</code></td>\n<td><code>List[TestCase]</code></td>\n<td>Main entry point: discovers all tests under <code>start_path</code> matching the pattern</td>\n</tr>\n<tr>\n<td><code>resolve_patterns_to_paths</code></td>\n<td><code>patterns: List[str], base_dir: Path</code></td>\n<td><code>List[Path]</code></td>\n<td>Converts glob patterns to absolute file paths for processing</td>\n</tr>\n<tr>\n<td><code>module_path_to_name</code></td>\n<td><code>file_path: Path, root_dir: Path</code></td>\n<td><code>str</code></td>\n<td>Converts a filesystem path to a Python importable module name</td>\n</tr>\n<tr>\n<td><code>_import_module_from_file</code></td>\n<td><code>file_path: Path</code></td>\n<td><code>module</code></td>\n<td>Internal: imports a module from a file path safely</td>\n</tr>\n<tr>\n<td><code>_find_tests_in_module</code></td>\n<td><code>module: module</code></td>\n<td><code>Iterator[TestCase]</code></td>\n<td>Internal: yields TestCase objects for test functions in the module</td>\n</tr>\n</tbody></table>\n<h4 id=\"the-discovery-algorithm\">The Discovery Algorithm</h4>\n<p>The discovery process follows a deterministic, depth-first traversal algorithm:</p>\n<ol>\n<li><p><strong>Pattern Resolution Phase</strong>:</p>\n<ol>\n<li>Receive one or more file patterns (e.g., <code>[&quot;tests/**/*.py&quot;, &quot;test_*.py&quot;]</code>)</li>\n<li>For each pattern, use <code>resolve_patterns_to_paths()</code> to expand globs to absolute file paths</li>\n<li>Filter to only <code>.py</code> files and remove duplicates</li>\n</ol>\n</li>\n<li><p><strong>Module Import Phase</strong>:</p>\n<ol>\n<li>For each <code>.py</code> file path, compute the importable module name using <code>module_path_to_name()</code></li>\n<li>Use <code>_import_module_from_file()</code> to import the module without polluting <code>sys.modules</code></li>\n<li>Handle import errors gracefully by recording them as discovery failures</li>\n</ol>\n</li>\n<li><p><strong>Test Extraction Phase</strong>:</p>\n<ol>\n<li>Call <code>_find_tests_in_module()</code> on the successfully imported module</li>\n<li>Inspect all members of the module using Python&#39;s <code>inspect</code> module</li>\n<li>For each member, apply the <strong>Test Identification Rules</strong>:<ul>\n<li><strong>Function Rule</strong>: If it&#39;s a callable function whose name starts with <code>test_</code></li>\n<li><strong>Class Rule</strong>: If it&#39;s a class whose name starts with <code>Test</code>, inspect its methods for those starting with <code>test_</code></li>\n<li><strong>Method Rule</strong>: Methods inside test classes whose names start with <code>test_</code></li>\n</ul>\n</li>\n<li>For each identified test, create a <code>TestCase</code> object with:<ul>\n<li><code>nodeid</code>: A unique identifier like <code>&quot;tests/unit/test_math.py::test_addition&quot;</code></li>\n<li><code>func</code>: The actual callable test function or method</li>\n<li><code>file_path</code>: Absolute path to the source file</li>\n<li><code>line_no</code>: Line number where the test is defined (using <code>inspect.getsourcelines()</code>)</li>\n<li><code>fixtures</code>: Initially empty list (will be populated during fixture discovery)</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p><strong>Aggregation Phase</strong>:</p>\n<ol>\n<li>Collect all <code>TestCase</code> objects into a <code>TestSuite</code></li>\n<li>Apply any test name filters provided in the <code>Configuration</code></li>\n<li>Return the final <code>TestSuite</code> to the Runner</li>\n</ol>\n</li>\n</ol>\n<h4 id=\"testcase-data-structure-details\">TestCase Data Structure Details</h4>\n<p>Each discovered test is represented as a <code>TestCase</code> object with the following complete specification:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>nodeid</code></td>\n<td><code>str</code></td>\n<td>Unique identifier using the format <code>&quot;path/to/module.py::test_function&quot;</code> or <code>&quot;path/to/module.py::TestClass::test_method&quot;</code>. This serves as the test&#39;s primary key throughout the system.</td>\n</tr>\n<tr>\n<td><code>func</code></td>\n<td><code>Callable</code></td>\n<td>The actual Python function or method to execute. This is a reference to the test implementation that will be called by the Runner.</td>\n</tr>\n<tr>\n<td><code>file_path</code></td>\n<td><code>str</code></td>\n<td>Absolute filesystem path to the Python file containing the test. Used for error reporting and source linking.</td>\n</tr>\n<tr>\n<td><code>line_no</code></td>\n<td><code>int</code></td>\n<td>Line number within <code>file_path</code> where the test function is defined. Enables IDEs to jump directly to test failures.</td>\n</tr>\n<tr>\n<td><code>fixtures</code></td>\n<td><code>List[str]</code></td>\n<td>Names of fixtures required by this test. Initially empty during discovery; populated later by analyzing function parameters against the fixture registry.</td>\n</tr>\n</tbody></table>\n<h4 id=\"concrete-walk-through-example\">Concrete Walk-Through Example</h4>\n<p>Let&#39;s trace through discovery for a simple project structure:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project/\n├── src/\n│   └── calculator.py\n└── tests/\n    ├── unit/\n    │   ├── test_math.py\n    │   └── test_calculator.py\n    └── integration/\n        └── test_integration.py</code></pre></div>\n\n<p>When the user runs <code>apollo tests/unit/*.py</code>, here&#39;s what happens:</p>\n<ol>\n<li><p><strong>Pattern Resolution</strong>: <code>resolve_patterns_to_paths()</code> expands <code>tests/unit/*.py</code> to:</p>\n<ul>\n<li><code>/absolute/path/project/tests/unit/test_math.py</code></li>\n<li><code>/absolute/path/project/tests/unit/test_calculator.py</code></li>\n</ul>\n</li>\n<li><p><strong>Module Import</strong>: Each file is imported:</p>\n<ul>\n<li><code>test_math.py</code> contains functions: <code>test_addition()</code>, <code>test_subtraction()</code>, <code>helper_function()</code></li>\n<li><code>test_calculator.py</code> contains class: <code>TestCalculator</code> with methods: <code>test_add()</code>, <code>test_multiply()</code></li>\n</ul>\n</li>\n<li><p><strong>Test Extraction</strong>:</p>\n<ul>\n<li>From <code>test_math.py</code>: Creates <code>TestCase</code> for <code>test_addition</code> and <code>test_subtraction</code> (skips <code>helper_function</code>)</li>\n<li>From <code>test_calculator.py</code>: Creates <code>TestCase</code> for <code>TestCalculator::test_add</code> and <code>TestCalculator::test_multiply</code></li>\n</ul>\n</li>\n<li><p><strong>NodeID Generation</strong>:</p>\n<ul>\n<li><code>test_addition</code> → <code>&quot;tests/unit/test_math.py::test_addition&quot;</code></li>\n<li><code>TestCalculator::test_add</code> → <code>&quot;tests/unit/test_calculator.py::TestCalculator::test_add&quot;</code></li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Fdiscovery-flowchart.svg\" alt=\"Test Discovery Flowchart\"></p>\n<h4 id=\"common-pitfalls-in-test-discovery\">Common Pitfalls in Test Discovery</h4>\n<p>⚠️ <strong>Pitfall: Import Side Effects Polluting Test State</strong><br><strong>Description</strong>: Directly importing test modules using standard <code>import</code> statements can cause module-level code (like global variable initialization) to execute during discovery, potentially affecting test execution later.<br><strong>Why It&#39;s Wrong</strong>: If a module sets <code>DATABASE_CONNECTION = create_connection()</code> at module level, this code runs during discovery, not during test execution. The connection might timeout before tests run, or worse, tests might share state.<br><strong>How to Fix</strong>: Use <code>_import_module_from_file()</code> which imports modules in a controlled way, potentially using <code>importlib</code> to load modules without executing top-level code multiple times.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Path Handling with Relative Imports</strong><br><strong>Description</strong>: When test modules use relative imports (like <code>from ..src import my_module</code>), discovery from a different directory can cause <code>ImportError</code>.<br><strong>Why It&#39;s Wrong</strong>: The framework imports modules from arbitrary locations but doesn&#39;t adjust <code>sys.path</code> to match the module&#39;s expected import context.<br><strong>How to Fix</strong>: In <code>_import_module_from_file()</code>, temporarily modify <code>sys.path</code> to include the parent directory of the module being imported, then restore it after import.</p>\n<p>⚠️ <strong>Pitfall: Test Discovery Performance with Large Codebases</strong><br><strong>Description</strong>: Recursively scanning thousands of files and importing all of them can make discovery slow.<br><strong>Why It&#39;s Wrong</strong>: Users expect near-instant feedback when running tests; a 10-second discovery phase feels sluggish.<br><strong>How to Fix</strong>: Implement caching of discovery results (hash file contents to detect changes) and parallelize file system scanning where possible.</p>\n<h3 id=\"the-runner-amp-isolation\">The Runner &amp; Isolation</h3>\n<p><strong>Mental Model: The Soundproof Practice Rooms</strong><br>Imagine our music festival where each performer needs to practice without hearing others. We build individual soundproof rooms (test isolation) where:</p>\n<ul>\n<li>Each room starts with identical basic equipment (clean environment)</li>\n<li>Performers can bring their own instruments (test fixtures)</li>\n<li>No sound leaks between rooms (no shared state)</li>\n<li>Multiple rooms can be used simultaneously (parallel execution)</li>\n<li>After each performance, the room is thoroughly cleaned (teardown)</li>\n</ul>\n<p>The Runner implements this isolation guarantee while executing tests, ensuring that tests are hermetic units that don&#39;t affect one another.</p>\n<h4 id=\"test-execution-state-machine\">Test Execution State Machine</h4>\n<p>Every test progresses through a well-defined state machine captured in the <code>TestResult.status</code> field. This &quot;traffic light system&quot; provides clear visibility into test execution:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PENDING</code></td>\n<td><code>test_started</code></td>\n<td><code>RUNNING</code></td>\n<td>Record start timestamp; Initialize fixture context</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>assertion_passed</code></td>\n<td>(remain <code>RUNNING</code>)</td>\n<td>Continue test execution</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>assertion_failed</code></td>\n<td><code>FAILED</code></td>\n<td>Capture assertion message; Stop test execution; Begin teardown</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>exception_raised</code></td>\n<td><code>ERRORED</code></td>\n<td>Capture exception and traceback; Begin teardown</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>test_completed</code></td>\n<td><code>PASSED</code></td>\n<td>Record end timestamp; Begin teardown</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>test_skipped</code></td>\n<td><code>SKIPPED</code></td>\n<td>Record skip reason; Begin teardown</td>\n</tr>\n<tr>\n<td><code>FAILED</code></td>\n<td><code>teardown_complete</code></td>\n<td><code>FAILED</code></td>\n<td>Finalize duration calculation</td>\n</tr>\n<tr>\n<td><code>ERRORED</code></td>\n<td><code>teardown_complete</code></td>\n<td><code>ERRORED</code></td>\n<td>Finalize duration calculation</td>\n</tr>\n<tr>\n<td><code>PASSED</code></td>\n<td><code>teardown_complete</code></td>\n<td><code>PASSED</code></td>\n<td>Finalize duration calculation</td>\n</tr>\n<tr>\n<td><code>SKIPPED</code></td>\n<td><code>teardown_complete</code></td>\n<td><code>SKIPPED</code></td>\n<td>Finalize duration calculation</td>\n</tr>\n</tbody></table>\n<p>This state machine ensures that every test follows the same lifecycle regardless of outcome, and that teardown always runs (except in catastrophic failures).</p>\n<p><img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Ftest-state-machine.svg\" alt=\"Test Result State Machine\"></p>\n<h4 id=\"architecture-decision-process-based-parallel-execution\">Architecture Decision: Process-Based Parallel Execution</h4>\n<blockquote>\n<p><strong>Decision: Use Multiprocessing for Parallel Test Execution</strong></p>\n<ul>\n<li><strong>Context</strong>: To reduce total test suite execution time, we want to run independent tests concurrently. Python&#39;s Global Interpreter Lock (GIL) limits true parallelism with threads for CPU-bound tests.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Threading</strong>: Use Python&#39;s <code>threading</code> module for concurrency</li>\n<li><strong>Subprocesses</strong>: Launch separate Python processes via <code>subprocess</code></li>\n<li><strong>Multiprocessing</strong>: Use <code>multiprocessing</code> module with process pools</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use <code>multiprocessing.Pool</code> for parallel execution (option 3)</li>\n<li><strong>Rationale</strong>: Multiprocessing bypasses the GIL, providing true parallelism for CPU-bound tests. It offers better isolation than threading (each test gets its own memory space) and is more efficient than subprocesses (avokes Python interpreter startup overhead). The <code>multiprocessing</code> module provides high-level abstractions like <code>Pool.map</code> that simplify implementation.</li>\n<li><strong>Consequences</strong>: Tests run in parallel by default when <code>--parallel</code> is specified. However, process startup has overhead, so very short test suites might run slower. Also, tests that rely on shared resources (database, files) need explicit coordination.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Threading</td>\n<td>Low overhead; Shared memory</td>\n<td>GIL limits CPU parallelism; State leakage risk</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Subprocesses</td>\n<td>Maximum isolation; No GIL issues</td>\n<td>High overhead; Complex communication</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Multiprocessing</td>\n<td>Good isolation; Bypasses GIL; Standard library</td>\n<td>Pickling requirements; Memory duplication</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"runner-components-and-interfaces\">Runner Components and Interfaces</h4>\n<p>The Runner component has two primary public interfaces as defined in the naming conventions:</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SimpleRunner.run_test</code></td>\n<td><code>test_case: TestCase</code></td>\n<td><code>TestResult</code></td>\n<td>Executes a single test in isolation, handling setup, execution, and teardown</td>\n</tr>\n<tr>\n<td><code>SimpleRunner.run_suite</code></td>\n<td><code>suite: TestSuite</code></td>\n<td><code>List[TestResult]</code></td>\n<td>Runs all tests in a suite, optionally in parallel, and collects results</td>\n</tr>\n</tbody></table>\n<h4 id=\"test-execution-algorithm\">Test Execution Algorithm</h4>\n<p>When <code>SimpleRunner.run_test()</code> is called for a single test, it follows this precise sequence:</p>\n<ol>\n<li><p><strong>Initialization</strong>:</p>\n<ol>\n<li>Create a fresh <code>TestResult</code> object with <code>status=PENDING</code>, <code>test_case</code> set, and start time recorded</li>\n<li>Change status to <code>RUNNING</code></li>\n<li>Create a clean execution namespace (dictionary) for the test</li>\n</ol>\n</li>\n<li><p><strong>Fixture Setup</strong>:</p>\n<ol>\n<li>Analyze <code>test_case.fixtures</code> to determine required fixtures</li>\n<li>For each fixture, retrieve or create it via the Fixture Registry (detailed in Milestone 3)</li>\n<li>Inject fixture values into the execution namespace</li>\n</ol>\n</li>\n<li><p><strong>Test Execution</strong>:</p>\n<ol>\n<li>Extract the test function from <code>test_case.func</code></li>\n<li>Call the function with arguments extracted from the execution namespace (matching parameter names to fixture names)</li>\n<li>Wrap the call in a try-except block to catch any exceptions</li>\n<li>If the Assertion Engine raises an <code>AssertionError</code>, transition to <code>FAILED</code> state and capture the error message</li>\n<li>If any other exception occurs, transition to <code>ERRORED</code> state and capture the exception with traceback</li>\n<li>If the function returns normally, transition to <code>PASSED</code> state</li>\n</ol>\n</li>\n<li><p><strong>Fixture Teardown</strong>:</p>\n<ol>\n<li>Regardless of test outcome, execute fixture teardown procedures in reverse dependency order</li>\n<li>For function-scoped fixtures, destroy them immediately</li>\n<li>For higher-scoped fixtures, decrement reference counts</li>\n</ol>\n</li>\n<li><p><strong>Finalization</strong>:</p>\n<ol>\n<li>Record end timestamp and calculate <code>duration</code></li>\n<li>Ensure <code>TestResult</code> has appropriate fields set based on outcome</li>\n<li>Return the completed <code>TestResult</code></li>\n</ol>\n</li>\n</ol>\n<h4 id=\"parallel-execution-implementation\">Parallel Execution Implementation</h4>\n<p>For <code>SimpleRunner.run_suite()</code> with parallel execution enabled, the algorithm is:</p>\n<ol>\n<li><p><strong>Partitioning Phase</strong>:</p>\n<ol>\n<li>Analyze the test suite to identify independent tests (no shared fixture dependencies at module or session scope)</li>\n<li>Create partitions of tests that can run concurrently</li>\n<li>For tests with shared state dependencies, group them to run sequentially</li>\n</ol>\n</li>\n<li><p><strong>Worker Preparation</strong>:</p>\n<ol>\n<li>Create a <code>multiprocessing.Pool</code> with <code>max_workers</code> from configuration (defaults to CPU count)</li>\n<li>Serialize test cases (must be picklable) along with necessary fixture definitions</li>\n<li>Define a worker function that can run a single test in isolation</li>\n</ol>\n</li>\n<li><p><strong>Execution Phase</strong>:</p>\n<ol>\n<li>Use <code>Pool.map()</code> or <code>Pool.imap_unordered()</code> to distribute tests across workers</li>\n<li>Each worker:<ul>\n<li>Re-imports necessary modules (fresh interpreter state)</li>\n<li>Recreates fixture context from serialized data</li>\n<li>Runs the test using the same algorithm as <code>run_test()</code></li>\n<li>Serializes the <code>TestResult</code> and returns it</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p><strong>Aggregation Phase</strong>:</p>\n<ol>\n<li>Collect all <code>TestResult</code> objects from workers</li>\n<li>Merge them maintaining original test order if required</li>\n<li>Handle any worker failures (mark corresponding tests as <code>ERRORED</code>)</li>\n</ol>\n</li>\n</ol>\n<h4 id=\"testresult-data-structure-details\">TestResult Data Structure Details</h4>\n<p>The <code>TestResult</code> object serves as the &quot;medical chart&quot; for each test execution, containing complete diagnostic information:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>test_case</code></td>\n<td><code>TestCase</code></td>\n<td>Reference to the test that was executed. This provides context for the result.</td>\n</tr>\n<tr>\n<td><code>status</code></td>\n<td><code>TestStatus</code></td>\n<td>One of: <code>PENDING</code>, <code>RUNNING</code>, <code>PASSED</code>, <code>FAILED</code>, <code>ERRORED</code>, <code>SKIPPED</code>. The definitive outcome of the test.</td>\n</tr>\n<tr>\n<td><code>message</code></td>\n<td><code>Optional[str]</code></td>\n<td>Human-readable message describing the failure or skip reason. For assertions, contains the formatted diff.</td>\n</tr>\n<tr>\n<td><code>exception</code></td>\n<td><code>Optional[Exception]</code></td>\n<td>The actual exception object if the test errored (not for assertion failures).</td>\n</tr>\n<tr>\n<td><code>traceback</code></td>\n<td><code>Optional[str]</code></td>\n<td>Formatted traceback string for debugging errors. Captures the full call stack.</td>\n</tr>\n<tr>\n<td><code>duration</code></td>\n<td><code>float</code></td>\n<td>Execution time in seconds with millisecond precision. Measured from just before fixture setup to after teardown.</td>\n</tr>\n</tbody></table>\n<h4 id=\"concrete-walk-through-parallel-execution-scenario\">Concrete Walk-Through: Parallel Execution Scenario</h4>\n<p>Consider a test suite with 4 independent tests (A, B, C, D) and 2 workers:</p>\n<ol>\n<li><strong>Sequential Execution</strong> (without parallel):</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   Time: 0s - Start Test A (takes 3s)\n   Time: 3s - Start Test B (takes 2s)  \n   Time: 5s - Start Test C (takes 4s)\n   Time: 9s - Start Test D (takes 1s)\n   Time: 10s - All tests complete</code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Parallel Execution</strong> (with 2 workers):</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   Worker 1: Time 0-3s: Test A\n   Worker 2: Time 0-2s: Test B\n   Worker 1: Time 3-7s: Test C  (after A finishes)\n   Worker 2: Time 2-3s: Test D  (after B finishes)\n   Time: 7s - All tests complete (30% faster!)</code></pre></div>\n\n<p>The speedup isn&#39;t perfectly linear due to:</p>\n<ul>\n<li>Process creation overhead</li>\n<li>Tests with different durations</li>\n<li>Potential resource contention</li>\n</ul>\n<p><img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Ftest-execution-seq.svg\" alt=\"Test Execution Sequence\"></p>\n<h4 id=\"common-pitfalls-in-test-execution\">Common Pitfalls in Test Execution</h4>\n<p>⚠️ <strong>Pitfall: Incomplete Isolation Between Tests</strong><br><strong>Description</strong>: Tests accidentally share state through module-level variables, class attributes, or mutable default arguments.<br><strong>Why It&#39;s Wrong</strong>: Test A sets <code>config.DEBUG = True</code>, then Test B sees this changed value and behaves differently. This leads to flaky tests that pass or fail depending on execution order.<br><strong>How to Fix</strong>: The Runner must ensure each test runs in a truly clean environment. For parallel execution, use separate processes. For sequential, carefully reset module state between tests.</p>\n<p>⚠️ <strong>Pitfall: Fixture Teardown Skipped on Test Failure</strong><br><strong>Description</strong>: If a test fails with an exception, and the teardown code isn&#39;t in a <code>finally</code> block, resources (database connections, temporary files) might leak.<br><strong>Why It&#39;s Wrong</strong>: Resource leaks accumulate across test runs, eventually causing &quot;too many open files&quot; errors or database connection limits.<br><strong>How to Fix</strong>: Structure the execution algorithm so fixture teardown runs in a <code>finally</code> block after test execution, regardless of test outcome.</p>\n<p>⚠️ <strong>Pitfall: Deadlocks in Parallel Execution</strong><br><strong>Description</strong>: Tests that use shared resources (like a test database) might deadlock when run concurrently.<br><strong>Why It&#39;s Wrong</strong>: Two tests both try to acquire locks on the same database table in different orders, causing them to wait forever for each other.<br><strong>How to Fix</strong>: Provide clear documentation about parallel-safe test writing. Optionally, detect tests that use known shared resources and run them sequentially.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Module Import</td>\n<td><code>importlib.util.spec_from_file_location()</code></td>\n<td>AST parsing without execution for faster discovery</td>\n</tr>\n<tr>\n<td>Parallel Execution</td>\n<td><code>multiprocessing.Pool</code> with <code>pickle</code></td>\n<td><code>concurrent.futures.ProcessPoolExecutor</code> with custom serialization</td>\n</tr>\n<tr>\n<td>Test Isolation</td>\n<td>Fresh import per test process</td>\n<td>Module reloading with <code>importlib.reload()</code></td>\n</tr>\n<tr>\n<td>File Pattern Matching</td>\n<td><code>glob.glob()</code> with recursion</td>\n<td><code>pathlib.rglob()</code> with caching</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n├── __init__.py\n├── __main__.py              # CLI entry point\n├── cli.py                   # CLI parsing (Milestone 4)\n├── discovery/\n│   ├── __init__.py\n│   ├── discoverer.py        # Main discovery logic\n│   └── path_resolver.py     # Pattern → path conversion\n├── runner/\n│   ├── __init__.py\n│   ├── base_runner.py       # Abstract runner interface\n│   ├── simple_runner.py     # Sequential runner implementation\n│   └── parallel_runner.py   # Parallel runner implementation\n├── models/\n│   ├── __init__.py\n│   ├── test_case.py         # TestCase dataclass\n│   ├── test_result.py       # TestResult dataclass\n│   └── test_suite.py        # TestSuite container\n└── utils/\n    ├── __init__.py\n    ├── import_utils.py      # Safe module import\n    └── introspection.py     # Function/class inspection helpers</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>File: <code>apollo/utils/import_utils.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> importlib.util</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ModuleType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> import_module_from_file</span><span style=\"color:#E1E4E8\">(file_path: Path) -> Optional[ModuleType]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Safely import a Python module from a filesystem path.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This function imports the module without adding it to sys.modules</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    under its normal name, preventing import side effects from polluting</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    the discovery process.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        file_path: Absolute path to the Python file to import</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        The imported module object, or None if import failed</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate a unique module name to avoid collisions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    module_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"__apollo_discovery_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">file_path.stem</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{hash</span><span style=\"color:#E1E4E8\">(file_path)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Remove any existing module with this name to ensure freshness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> module_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> sys.modules:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        del</span><span style=\"color:#E1E4E8\"> sys.modules[module_name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create module spec from file location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        spec </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> importlib.util.spec_from_file_location(module_name, file_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> spec </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> spec.loader </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create and execute the module</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        module </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> importlib.util.module_from_spec(spec)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Temporarily add parent directory to sys.path for relative imports</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parent_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(file_path.parent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        original_sys_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sys.path.copy()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> parent_dir </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> sys.path:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sys.path.insert(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, parent_dir)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            spec.loader.exec_module(module)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Restore original sys.path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sys.path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> original_sys_path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> module</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Import failed - could be syntax error, missing dependencies, etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clean up to avoid leaving half-loaded modules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> module_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> sys.modules:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            del</span><span style=\"color:#E1E4E8\"> sys.modules[module_name]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>File: <code>apollo/discovery/discoverer.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Iterator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> inspect</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models.test_case </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..utils.import_utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> import_module_from_file</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> discover_tests</span><span style=\"color:#E1E4E8\">(start_path: Path, pattern: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"test_*.py\"</span><span style=\"color:#E1E4E8\">) -> List[TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Discover all test functions in modules under start_path matching the pattern.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This is the main entry point for test discovery. It follows the algorithm:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    1. Resolve file patterns to concrete file paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    2. Import each Python module safely</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    3. Extract test functions from each module</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    4. Return a list of TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        start_path: Directory to start discovery from</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        pattern: Glob pattern for matching test files (default: \"test_*.py\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of TestCase objects representing discovered tests</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use resolve_patterns_to_paths() to convert pattern to list of file paths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each file path, call import_module_from_file() to import the module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each successfully imported module, call _find_tests_in_module()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Collect all TestCase objects into a list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply any additional filtering (by test name patterns if provided)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return the final list of TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _find_tests_in_module</span><span style=\"color:#E1E4E8\">(module) -> Iterator[TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Yield TestCase objects for test functions found in a module.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This function inspects all members of a module and identifies:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Functions whose names start with \"test_\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Methods inside classes whose names start with \"Test\" and method names start with \"test_\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        module: The imported Python module to inspect</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Yields:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        TestCase objects for each discovered test</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use inspect.getmembers() to get all members of the module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each member, check if it's a function with name starting with \"test_\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For function tests: create TestCase with nodeid format \"module.py::function_name\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each member, check if it's a class with name starting with \"Test\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For test classes: inspect methods, find those starting with \"test_\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: For method tests: create TestCase with nodeid format \"module.py::ClassName::method_name\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: For each test, capture file_path and line_no using inspect.getfile() and inspect.getsourcelines()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Set fixtures list to empty (will be populated later during fixture discovery)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Yield the TestCase object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>File: <code>apollo/runner/simple_runner.py</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models.test_case </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models.test_result </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestResult, TestStatus</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleRunner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Sequential test runner that executes tests one at a time with isolation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This runner ensures each test runs in a clean environment and that</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    fixture teardown always occurs, even if the test fails.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_test</span><span style=\"color:#E1E4E8\">(self, test_case: TestCase) -> TestResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a single test and return its result.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        The execution follows this sequence:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        1. Create TestResult with PENDING status</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        2. Setup required fixtures</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        3. Execute test function</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        4. Teardown fixtures (always runs, even on failure)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        5. Finalize TestResult with outcome</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            test_case: The test to execute</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TestResult containing the outcome and diagnostic information</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create TestResult object with test_case reference and status=PENDING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Record start time using time.monotonic()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Change status to RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Setup fixtures (for Milestone 1, this is just record that it would happen)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Wrap test execution in try-except block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: In try: call test_case.func() and on success set status=PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: In except AssertionError: set status=FAILED and capture error message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: In except Exception: set status=ERRORED and capture exception with traceback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: In finally: execute fixture teardown (for Milestone 1, just record)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Record end time and calculate duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Return the completed TestResult</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_suite</span><span style=\"color:#E1E4E8\">(self, suite) -> List[TestResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Run all tests in a suite and return their results.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        For Milestone 1, this runs tests sequentially. In later milestones,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        this will handle parallel execution when configured.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            suite: TestSuite containing tests to run</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of TestResult objects in the same order as tests in the suite</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize empty list for results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each test_case in suite.tests:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3:   Call self.run_test(test_case) and append result to list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return the list of results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-python\">E. Language-Specific Hints (Python)</h4>\n<ol>\n<li><p><strong>Use <code>inspect</code> module for reflection</strong>: <code>inspect.isfunction()</code>, <code>inspect.getmembers()</code>, and <code>inspect.getsourcelines()</code> are essential for discovering test functions and capturing their metadata.</p>\n</li>\n<li><p><strong>Handle imports carefully</strong>: Use <code>importlib</code> instead of <code>__import__</code> for more control over the import process. Remember to clean up <code>sys.modules</code> to avoid state leakage between tests.</p>\n</li>\n<li><p><strong>For parallel execution</strong>: Use <code>multiprocessing.Pool</code> with <code>initializer</code> to set up each worker process. Make sure test cases and fixtures are picklable (implement <code>__reduce__</code> if needed).</p>\n</li>\n<li><p><strong>Isolation technique</strong>: The simplest isolation for sequential execution is to run each test in its own directory using <code>os.chdir()</code> to a temporary directory, then change back after the test.</p>\n</li>\n<li><p><strong>Timing precision</strong>: Use <code>time.monotonic()</code> instead of <code>time.time()</code> for measuring test durations, as it&#39;s not affected by system clock changes.</p>\n</li>\n</ol>\n<h4 id=\"f-milestone-1-checkpoint\">F. Milestone 1 Checkpoint</h4>\n<p>After implementing the Discovery &amp; Execution components, you should be able to run:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create a simple test file</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">cat</span><span style=\"color:#F97583\"> ></span><span style=\"color:#9ECBFF\"> test_example.py</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#9ECBFF\"> 'EOF'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_addition():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    assert 1 + 1 == 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_subtraction():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    assert 5 - 3 == 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def helper_function():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    return \"not a test\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">EOF</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run the framework on this test file</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo</span><span style=\"color:#9ECBFF\"> test_example.py</span></span></code></pre></div>\n\n<p><strong>Expected Output</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Discovering tests...\nFound 2 tests in 1 file\n\nRunning tests...\ntest_example.py::test_addition ... PASSED (0.001s)\ntest_example.py::test_subtraction ... PASSED (0.001s)\n\nSummary:\n2 passed, 0 failed, 0 errored, 0 skipped in 0.002s</code></pre></div>\n\n<p><strong>Verification Checklist</strong>:</p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> Both <code>test_addition</code> and <code>test_subtraction</code> are discovered and executed</li>\n<li><input disabled=\"\" type=\"checkbox\"> <code>helper_function</code> is NOT discovered as a test (doesn&#39;t start with <code>test_</code>)</li>\n<li><input disabled=\"\" type=\"checkbox\"> Each test shows PASSED status with execution time</li>\n<li><input disabled=\"\" type=\"checkbox\"> Tests run sequentially (one after another)</li>\n<li><input disabled=\"\" type=\"checkbox\"> Summary shows correct counts and total time</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No tests discovered</td>\n<td>Incorrect file pattern or working directory</td>\n<td>Print resolved file paths before import</td>\n<td>Use absolute paths; Check <code>start_path</code> is correct</td>\n</tr>\n<tr>\n<td>Tests discovered but not executed</td>\n<td>Test functions not callable or have parameters</td>\n<td>Check <code>inspect.iscallable()</code> on discovered functions</td>\n<td>Ensure test functions have no required parameters (yet)</td>\n</tr>\n<tr>\n<td>Tests interfering with each other</td>\n<td>Module-level state shared between tests</td>\n<td>Add print statements showing module variable values</td>\n<td>Implement proper isolation: reload modules or use fresh processes</td>\n</tr>\n<tr>\n<td>Import errors during discovery</td>\n<td>Missing dependencies or syntax errors</td>\n<td>Catch and log import exceptions in <code>import_module_from_file</code></td>\n<td>Report import errors as discovery failures, continue with other files</td>\n</tr>\n<tr>\n<td>Parallel execution hangs</td>\n<td>Deadlock in shared resources or pickling errors</td>\n<td>Add timeout to worker processes; Check picklability of fixtures</td>\n<td>Ensure tests don&#39;t use shared resources; Make fixtures picklable</td>\n</tr>\n</tbody></table>\n<h2 id=\"6-component-design-assertions-amp-matchers-milestone-2\">6. Component Design: Assertions &amp; Matchers (Milestone 2)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (Assertions &amp; Matchers)</p>\n</blockquote>\n<p>The <strong>Assertion Engine</strong> and <strong>Matchers API</strong> form the diagnostic core of the test framework—the system that transforms vague test failures into precise, actionable insights. While discovery finds tests and execution runs them, the assertion system determines whether tests actually pass or fail by comparing expected outcomes against actual results. This component embodies the framework&#39;s intelligence, moving beyond simple true/false checks to provide rich contextual information when things go wrong.</p>\n<p>Think of the assertion system as a <strong>Laboratory Microscope with Comparison Lenses</strong>. A basic microscope (simple <code>assert</code> statements) can tell you &quot;something looks wrong.&quot; Our framework provides specialized comparison lenses—some for comparing lists, others for checking exceptions, others for custom patterns—that not only identify discrepancies but also highlight them in high resolution with side-by-side comparisons, difference highlighting, and domain-specific diagnostics. The <strong>Assertion Engine</strong> is the microscope&#39;s optical system, while the <strong>Matchers API</strong> is the interchangeable lens kit that users can customize for their specific testing needs.</p>\n<h3 id=\"assertion-engine\">Assertion Engine</h3>\n<p>The Assertion Engine serves as the centralized evaluation and reporting system for all verification operations. Its primary responsibility is to transform predicate evaluations into structured failure information when expectations aren&#39;t met. Unlike Python&#39;s built-in <code>assert</code> statement (which provides minimal context like line numbers but lacks semantic understanding), the Assertion Engine understands the <em>intent</em> behind each check and can generate detailed diagnostic messages.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: The value of an assertion isn&#39;t just in determining pass/fail—it&#39;s in providing enough diagnostic information that a developer can understand <em>why</em> the failure occurred without needing to add debug prints or step through code. A good assertion failure message should be 80% of the debugging work.</p>\n</blockquote>\n<h4 id=\"mental-model-the-comparison-inspector\">Mental Model: The Comparison Inspector</h4>\n<p>Imagine an <strong>Evidence Examiner</strong> who compares two artifacts. When they find a mismatch, they don&#39;t just say &quot;these don&#39;t match&quot;—they produce a detailed report showing: &quot;At position 3, expected &#39;blue&#39; but found &#39;green&#39;; here&#39;s a side-by-side comparison with differences highlighted; here&#39;s the context around the mismatch; here are the statistical properties that differ.&quot; The Assertion Engine is this examiner, specialized in comparing different data types with appropriate comparison strategies for each.</p>\n<h4 id=\"core-design-principles\">Core Design Principles</h4>\n<ol>\n<li><strong>Semantic Understanding Over Syntactic Checking</strong>: The engine understands what &quot;equal&quot; means for different data types—structural equality for collections, approximate equality for floats, case-insensitive equality for strings in certain contexts.</li>\n<li><strong>Failure Messages as Primary Output</strong>: The error message is the main deliverable, designed to be read by humans debugging test failures.</li>\n<li><strong>Type-Adaptive Comparison Strategies</strong>: Different data types get different comparison algorithms optimized for that type&#39;s characteristics.</li>\n<li><strong>Minimal Performance Overhead in Success Paths</strong>: When assertions pass, they should add minimal overhead. Expensive operations (like deep diffing) only occur on failures.</li>\n</ol>\n<h4 id=\"architecture-decision-record-exception-based-failure-signaling\">Architecture Decision Record: Exception-Based Failure Signaling</h4>\n<blockquote>\n<p><strong>Decision: Use Python&#39;s Exception Mechanism for Assertion Failures</strong></p>\n<ul>\n<li><strong>Context</strong>: When an assertion fails, the framework needs to: (1) halt test execution at that point, (2) capture the failure reason, (3) continue running subsequent tests. We need a mechanism that naturally supports these requirements within Python&#39;s execution model.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Return Boolean + Side Channel</strong>: Each assertion returns <code>True</code>/<code>False</code> and writes failure details to a shared mutable state. The test runner checks the boolean and then reads the failure details.</li>\n<li><strong>Custom Control Flow</strong>: Assertions set a &quot;test failed&quot; flag and the test runner periodically checks this flag to decide whether to continue test execution.</li>\n<li><strong>Exception Raising</strong>: Assertions raise a custom <code>AssertionError</code> (or subclass) with failure details attached.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use exception raising with a custom <code>AssertionError</code> subclass that carries rich failure information.</li>\n<li><strong>Rationale</strong>:<ul>\n<li>Exceptions provide natural control flow interruption—when an assertion fails, execution immediately stops at that point, preventing &quot;zombie&quot; test code from running after a failure.</li>\n<li>Python&#39;s exception system is optimized and familiar to all Python developers.</li>\n<li>Exception objects can carry arbitrary data (like expected/actual values, diff results, custom messages) which can be extracted by the test runner for reporting.</li>\n<li>Exception tracebacks automatically provide file/line context (though we enrich this with semantic information).</li>\n<li>The approach aligns with existing Python testing conventions (<code>unittest</code>, <code>pytest</code>) and developer expectations.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>Tests must be written to expect assertions might raise exceptions (they already do with built-in <code>assert</code>).</li>\n<li>The test runner needs proper exception handling to catch assertion failures and continue to next tests.</li>\n<li>We get &quot;fail-fast&quot; behavior within a single test for free.</li>\n<li>Stack traces will include our assertion function calls, which we need to clean up in reports to avoid noise.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Return Boolean + Side Channel</td>\n<td>Explicit control flow; no exception overhead</td>\n<td>Requires manual propagation of failure state; easy to forget checks; test code continues after failure</td>\n<td>Makes tests verbose; violates principle of least surprise</td>\n</tr>\n<tr>\n<td>Custom Control Flow</td>\n<td>Could optimize for specific patterns; no stack unwinding</td>\n<td>Complex to implement; breaks standard Python idioms; hard to debug</td>\n<td>Reinvents control flow poorly; high maintenance cost</td>\n</tr>\n<tr>\n<td>Exception Raising</td>\n<td>Natural Python idiom; automatic control flow; carries rich data</td>\n<td>Slight performance overhead; stack traces need cleaning</td>\n<td><strong>CHOSEN</strong>: Standard, predictable, and powerful</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-structures\">Data Structures</h4>\n<p>The Assertion Engine introduces several core data structures:</p>\n<table>\n<thead>\n<tr>\n<th>Type Name</th>\n<th>Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AssertionFailure</code></td>\n<td><code>message: str</code>, <code>expected: Any</code>, <code>actual: Any</code>, <code>diff: Optional[str]</code>, <code>hint: Optional[str]</code>, <code>assertion_type: str</code></td>\n<td>Container for all information about a failed assertion. This is <em>not</em> an exception itself but data packaged within an exception.</td>\n</tr>\n<tr>\n<td><code>ComparisonContext</code></td>\n<td><code>tolerance: float</code>, <code>ignore_case: bool</code>, <code>ignore_whitespace: bool</code>, <code>unordered: bool</code></td>\n<td>Configuration for how comparisons should be performed, allowing customization per assertion call.</td>\n</tr>\n<tr>\n<td><code>DiffResult</code></td>\n<td><code>summary: str</code>, <code>unified_diff: Optional[str]</code>, <code>context_lines: int</code></td>\n<td>Structured result of diff computation between expected and actual values.</td>\n</tr>\n</tbody></table>\n<p>The relationship between these structures and the existing <code>TestResult</code> is critical:</p>\n<ol>\n<li>When an assertion fails, it creates an <code>AssertionFailure</code> with diagnostic details.</li>\n<li>This is wrapped in a custom <code>AssertionError</code> subclass (e.g., <code>TestFrameworkAssertionError</code>).</li>\n<li>The test runner catches this exception, extracts the <code>AssertionFailure</code> data, and stores it in <code>TestResult.message</code> and <code>TestResult.exception</code>.</li>\n<li>The reporter formats this information appropriately for output.</li>\n</ol>\n<h4 id=\"assertion-evaluation-algorithm\">Assertion Evaluation Algorithm</h4>\n<p>The flowchart <img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Fassertion-eval-flow.svg\" alt=\"Assertion Evaluation Flow\"> shows the high-level process. Here&#39;s the detailed algorithm for <code>assert_equal(actual, expected, msg=None, **kwargs)</code>:</p>\n<ol>\n<li><p><strong>Parse Comparison Context</strong>: Extract comparison options from <code>kwargs</code> (like <code>tolerance=0.001</code>) and create a <code>ComparisonContext</code> object.</p>\n</li>\n<li><p><strong>Type Inspection</strong>: Determine the types of <code>actual</code> and <code>expected</code>. The engine maintains a registry of type-specific comparison handlers:</p>\n<ul>\n<li>Basic types (int, str, bool): Use Python&#39;s <code>==</code> but with optional transformations (case-insensitive, etc.)</li>\n<li>Floating point numbers: Use relative/absolute tolerance comparison (like <code>math.isclose()</code>)</li>\n<li>Collections (list, tuple, dict, set): Use structural comparison with configurable ordering</li>\n<li>NumPy arrays/pandas DataFrames: Specialized handlers (could be added via Matchers API)</li>\n<li>Custom objects: Try <code>__eq__</code>, fall back to <code>repr()</code> comparison</li>\n</ul>\n</li>\n<li><p><strong>Equality Check</strong>: Apply the appropriate comparison algorithm based on types and context:</p>\n<ul>\n<li>For floats: <code>abs(a-b) &lt;= max(rel_tol*max(abs(a), abs(b)), abs_tol)</code></li>\n<li>For sequences with <code>unordered=True</code>: Convert to multisets or sorted lists</li>\n<li>For dicts: Compare keys then recursively compare values</li>\n</ul>\n</li>\n<li><p><strong>If Match Passes</strong>: Return <code>None</code> immediately (success path—minimal overhead).</p>\n</li>\n<li><p><strong>If Match Fails</strong>: Enter diagnostic generation phase:\na. <strong>Compute Diff</strong>: Based on types:</p>\n<ul>\n<li>Strings: Generate unified diff with 3 lines of context</li>\n<li>Lists/Tuples: Highlight first differing element with index</li>\n<li>Dicts: Show missing/extra keys and differing values</li>\n<li>Mixed types: Show type mismatch clearly</li>\n</ul>\n<p>b. <strong>Format Message</strong>: Assemble a human-readable message with:</p>\n<ul>\n<li>Custom message (if provided by user)</li>\n<li>Expected and actual values (with intelligent truncation for large objects)</li>\n<li>Diff output (if computed)</li>\n<li>Type information (especially helpful when <code>1 != 1.0</code>)</li>\n<li>Hint based on common mistakes (e.g., &quot;Did you forget to call the function?&quot;)</li>\n</ul>\n<p>c. <strong>Raise Exception</strong>: Instantiate <code>TestFrameworkAssertionError</code> with the <code>AssertionFailure</code> data and raise it.</p>\n</li>\n</ol>\n<h4 id=\"core-assertion-suite\">Core Assertion Suite</h4>\n<p>The framework provides these essential assertions (all following the pattern above):</p>\n<table>\n<thead>\n<tr>\n<th>Assertion Method</th>\n<th>Signature</th>\n<th>Purpose</th>\n<th>Special Comparison Logic</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>assert_equal</code></td>\n<td><code>(actual, expected, msg=None, **kwargs)</code></td>\n<td>General equality with type-aware comparison</td>\n<td>Float tolerance, unordered collections, case/whitespace insensitivity</td>\n</tr>\n<tr>\n<td><code>assert_not_equal</code></td>\n<td><code>(actual, unexpected, msg=None, **kwargs)</code></td>\n<td>Inverse of assert_equal</td>\n<td>Same comparison logic, fails if values <em>are</em> equal</td>\n</tr>\n<tr>\n<td><code>assert_true</code></td>\n<td><code>(condition, msg=None)</code></td>\n<td>Verify truthiness</td>\n<td>Uses <code>bool()</code> conversion; suggests common falsy values in message</td>\n</tr>\n<tr>\n<td><code>assert_false</code></td>\n<td><code>(condition, msg=None)</code></td>\n<td>Verify falsiness</td>\n<td>Inverse of assert_true</td>\n</tr>\n<tr>\n<td><code>assert_is</code></td>\n<td><code>(actual, expected, msg=None)</code></td>\n<td>Identity check (<code>is</code> operator)</td>\n<td>Uses <code>id()</code> for objects; shows object details in failure</td>\n</tr>\n<tr>\n<td><code>assert_is_not</code></td>\n<td><code>(actual, unexpected, msg=None)</code></td>\n<td>Inverse identity check</td>\n<td></td>\n</tr>\n<tr>\n<td><code>assert_is_none</code></td>\n<td><code>(value, msg=None)</code></td>\n<td>Special case for <code>None</code></td>\n<td>Clear message when value is not None</td>\n</tr>\n<tr>\n<td><code>assert_is_not_none</code></td>\n<td><code>(value, msg=None)</code></td>\n<td>Verify non-None</td>\n<td>Shows actual value in failure</td>\n</tr>\n<tr>\n<td><code>assert_in</code></td>\n<td><code>(item, container, msg=None)</code></td>\n<td>Membership test</td>\n<td>For strings, shows surrounding context; for dicts, checks keys</td>\n</tr>\n<tr>\n<td><code>assert_not_in</code></td>\n<td><code>(item, container, msg=None)</code></td>\n<td>Inverse membership</td>\n<td></td>\n</tr>\n<tr>\n<td><code>assert_is_instance</code></td>\n<td><code>(obj, cls, msg=None)</code></td>\n<td>Type checking</td>\n<td>Shows actual type and MRO in failure</td>\n</tr>\n<tr>\n<td><code>assert_not_is_instance</code></td>\n<td><code>(obj, cls, msg=None)</code></td>\n<td>Inverse type check</td>\n<td></td>\n</tr>\n<tr>\n<td><code>assert_almost_equal</code></td>\n<td><code>(actual, expected, places=7, msg=None, **kwargs)</code></td>\n<td>Numeric approximation</td>\n<td>Decimal place rounding; auto-switches to significant figures for very small numbers</td>\n</tr>\n<tr>\n<td><code>assert_not_almost_equal</code></td>\n<td><code>(actual, unexpected, places=7, msg=None, **kwargs)</code></td>\n<td>Inverse numeric check</td>\n<td></td>\n</tr>\n<tr>\n<td><code>assert_greater</code></td>\n<td><code>(actual, expected, msg=None)</code></td>\n<td>Ordered comparison &gt;</td>\n<td>Works with any comparable types</td>\n</tr>\n<tr>\n<td><code>assert_greater_equal</code></td>\n<td><code>(actual, expected, msg=None)</code></td>\n<td>Ordered comparison &gt;=</td>\n<td></td>\n</tr>\n<tr>\n<td><code>assert_less</code></td>\n<td><code>(actual, expected, msg=None)</code></td>\n<td>Ordered comparison &lt;</td>\n<td></td>\n</tr>\n<tr>\n<td><code>assert_less_equal</code></td>\n<td><code>(actual, expected, msg=None)</code></td>\n<td>Ordered comparison &lt;=</td>\n<td></td>\n</tr>\n<tr>\n<td><code>assert_regex</code></td>\n<td><code>(text, regex, msg=None)</code></td>\n<td>Pattern matching</td>\n<td>Shows match failure location; extracts and shows matched groups on success</td>\n</tr>\n<tr>\n<td><code>assert_not_regex</code></td>\n<td><code>(text, regex, msg=None)</code></td>\n<td>Inverse pattern match</td>\n<td></td>\n</tr>\n</tbody></table>\n<h4 id=\"collection-assertions\">Collection Assertions</h4>\n<p>Specialized assertions for collection types provide more semantic feedback:</p>\n<table>\n<thead>\n<tr>\n<th>Assertion Method</th>\n<th>Signature</th>\n<th>Purpose</th>\n<th>Special Features</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>assert_length</code></td>\n<td><code>(collection, expected_len, msg=None)</code></td>\n<td>Verify collection size</td>\n<td>Works with any <code>len()</code>-able object; suggests common off-by-one errors</td>\n</tr>\n<tr>\n<td><code>assert_count_equal</code></td>\n<td><code>(actual, expected, msg=None)</code></td>\n<td>Compare ignoring order</td>\n<td>Counts occurrences of each element; shows frequency mismatches</td>\n</tr>\n<tr>\n<td><code>assert_list_equal</code></td>\n<td><code>(actual, expected, msg=None, **kwargs)</code></td>\n<td>Ordered list equality</td>\n<td>Element-by-element diff with indices</td>\n</tr>\n<tr>\n<td><code>assert_dict_equal</code></td>\n<td><code>(actual, expected, msg=None, **kwargs)</code></td>\n<td>Dictionary equality</td>\n<td>Key-by-key comparison; shows nested differences</td>\n</tr>\n<tr>\n<td><code>assert_set_equal</code></td>\n<td><code>(actual, expected, msg=None)</code></td>\n<td>Set equality</td>\n<td>Shows symmetric difference</td>\n</tr>\n<tr>\n<td><code>assert_contains_all</code></td>\n<td><code>(container, items, msg=None)</code></td>\n<td>Superset check</td>\n<td>Shows which items are missing</td>\n</tr>\n<tr>\n<td><code>assert_contains_any</code></td>\n<td><code>(container, items, msg=None)</code></td>\n<td>At least one match</td>\n<td>Shows all attempted items</td>\n</tr>\n<tr>\n<td><code>assert_contains_none</code></td>\n<td><code>(container, items, msg=None)</code></td>\n<td>No matches allowed</td>\n<td>Shows which items were unexpectedly found</td>\n</tr>\n<tr>\n<td><code>assert_sorted</code></td>\n<td><code>(collection, key=None, reverse=False, msg=None)</code></td>\n<td>Verify sorted order</td>\n<td>Shows first out-of-order elements</td>\n</tr>\n</tbody></table>\n<h4 id=\"exception-assertions\">Exception Assertions</h4>\n<p>Testing exception behavior requires special handling because we&#39;re asserting that a <em>side effect</em> (exception raising) occurs:</p>\n<p><strong>Mental Model: The Exception Catcher</strong> — Imagine setting up a specialized net to catch only specific types of flying objects (exceptions). The assertion verifies that: (1) the right type of object gets caught, (2) it has the expected properties (message, attributes), and (3) nothing else gets caught unexpectedly.</p>\n<p><strong>Architecture Decision Record: Context Manager vs Function Wrapper for Exception Testing</strong></p>\n<blockquote>\n<p><strong>Decision: Use Context Manager Pattern for Exception Assertions</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to verify that calling a function raises an expected exception. Two common patterns exist: decorator/wrapper that calls the function, or context manager that wraps a block of code.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Function Wrapper</strong>: <code>assert_raises(exception_type, func, *args, **kwargs)</code> calls the function and checks if it raises.</li>\n<li><strong>Context Manager</strong>: <code>with assert_raises(exception_type):</code> wraps a block where the exception should occur.</li>\n<li><strong>Hybrid Approach</strong>: Support both patterns through a single flexible API.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement as a context manager that can also be used as a function decorator/caller for backward compatibility.</li>\n<li><strong>Rationale</strong>:<ul>\n<li>Context manager pattern is more Pythonic and readable for multi-line exception testing.</li>\n<li>It naturally handles the case where the exception comes from code other than a single function call.</li>\n<li>The context manager can capture and expose the exception object for further inspection (message, attributes).</li>\n<li>We can add a <code>.__call__()</code> method to support the function wrapper pattern for simple cases.</li>\n<li>This aligns with <code>pytest.raises</code> and modern Python testing practices.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>Slightly more complex implementation (context manager protocol).</li>\n<li>Provides better diagnostic information (can show which line in the block actually raised).</li>\n<li>Allows checking exception properties after it&#39;s caught.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Function Wrapper</td>\n<td>Simple implementation; single responsibility</td>\n<td>Can&#39;t test multi-line blocks; limited inspection of caught exception</td>\n<td>Too restrictive for real-world testing</td>\n</tr>\n<tr>\n<td>Context Manager</td>\n<td>Flexible; captures exception for inspection; Pythonic</td>\n<td>More complex to implement; two ways to use could confuse</td>\n<td><strong>CHOSEN</strong>: Industry standard; maximal flexibility</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Backward compatible; covers all use cases</td>\n<td>Most complex; ambiguous API</td>\n<td>Complexity not justified for learning framework</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation Algorithm for <code>assert_raises</code></strong>:</p>\n<ol>\n<li><p><strong>Context Manager Entry</strong>: When <code>with assert_raises(ExpectedException) as cm:</code> is entered, store expected exception type and any match criteria.</p>\n</li>\n<li><p><strong>Block Execution</strong>: Execute the indented block within a try-except.</p>\n</li>\n<li><p><strong>Exception Monitoring</strong>:</p>\n<ul>\n<li>If no exception is raised: FAIL → &quot;Expected exception ExpectedException but no exception was raised&quot;</li>\n<li>If wrong exception type is raised: FAIL → &quot;Expected exception ExpectedException but AnotherException was raised: message...&quot;</li>\n<li>If correct exception type is raised: PASS to step 4</li>\n</ul>\n</li>\n<li><p><strong>Exception Validation</strong>: If additional criteria provided (regex for message, predicate function), validate caught exception:</p>\n<ul>\n<li>If message regex: check exception message matches pattern</li>\n<li>If predicate: call <code>predicate(exc)</code> must return True</li>\n<li>If any validation fails: FAIL with specific reason</li>\n</ul>\n</li>\n<li><p><strong>Exception Storage</strong>: Store caught exception in context manager object for further inspection (<code>.value</code> attribute).</p>\n</li>\n<li><p><strong>Context Manager Exit</strong>: Clean up (nothing needed normally).</p>\n</li>\n</ol>\n<p><strong>Additional Exception Assertions</strong>:</p>\n<ul>\n<li><code>assert_raises_regex</code>: Combines type and message pattern matching</li>\n<li><code>assert_warns</code>: Similar pattern for warning capture</li>\n<li><code>assert_no_raises</code>: Context manager that fails if <em>any</em> exception is raised (useful for &quot;this shouldn&#39;t crash&quot; tests)</li>\n</ul>\n<h4 id=\"common-pitfalls-in-assertion-engine-implementation\">Common Pitfalls in Assertion Engine Implementation</h4>\n<p>⚠️ <strong>Pitfall: Unhelpful Error Messages for Complex Objects</strong></p>\n<ul>\n<li><strong>Description</strong>: Showing <code>assert_equal(complex_obj1, complex_obj2)</code> with just <code>&lt;object at 0x...&gt; != &lt;object at 0x...&gt;</code>.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Developer must add debug prints or use a debugger to see what actually differed.</li>\n<li><strong>Fix</strong>: Implement recursive <code>__repr__</code> inspection or use <code>pprint.pformat()</code> for nested structures. For custom objects, try to access <code>__dict__</code> or <code>__slots__</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Float Comparison with Exact Equality</strong></p>\n<ul>\n<li><strong>Description</strong>: <code>assert_equal(0.1 + 0.2, 0.3)</code> fails due to binary floating-point representation.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Mathematical equality ≠ binary representation equality for floats.</li>\n<li><strong>Fix</strong>: Use relative/absolute tolerance (<code>math.isclose()</code> logic). Provide <code>tolerance</code> parameter and sensible defaults (like <code>rel_tol=1e-9, abs_tol=0.0</code>).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Exception Context Loss</strong></p>\n<ul>\n<li><strong>Description</strong>: When <code>assert_raises</code> catches an exception, the original traceback is replaced with the assertion&#39;s traceback.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Developer can&#39;t see where in their code the exception actually originated.</li>\n<li><strong>Fix</strong>: Use <code>raise ... from None</code> pattern or store original traceback and include it in failure output.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Infinite Recursion in Nested Comparison</strong></p>\n<ul>\n<li><strong>Description</strong>: Comparing objects that reference each other (circular references) causes infinite recursion.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Crashes the test runner with recursion depth exceeded.</li>\n<li><strong>Fix</strong>: Maintain a <code>visiting</code> set of object IDs already being compared. Skip or handle circular references specially.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Mutation During Comparison</strong></p>\n<ul>\n<li><strong>Description</strong>: Assertion code inadvertently mutates the objects being compared (e.g., sorting in place for unordered comparison).</li>\n<li><strong>Why It&#39;s Wrong</strong>: Changes test behavior; makes tests non-idempotent.</li>\n<li><strong>Fix</strong>: Always work on copies. Use <code>sorted()</code> not <code>.sort()</code>, <code>copy.deepcopy()</code> for nested structures.</li>\n</ul>\n<h3 id=\"matchers-api\">Matchers API</h3>\n<p>While the Assertion Engine provides a comprehensive set of built-in assertions, real-world testing often requires domain-specific checks that aren&#39;t covered by generic equality or membership tests. The <strong>Matchers API</strong> provides an extensible, composable system for defining custom assertion logic with tailored failure messages.</p>\n<p><strong>Mental Model: The Rulebook Builder</strong> — Imagine you&#39;re creating a rulebook for verifying architectural blueprints. The built-in assertions are like standard measuring tools (rulers, protractors). The Matchers API lets you define custom verification rules: &quot;Windows must comprise 20-30% of wall surface&quot; or &quot;All doors must open inward in public spaces.&quot; These domain-specific rules can be combined (&quot;and&quot; / &quot;or&quot; / &quot;not&quot;) and provide specific failure messages (&quot;Window area 15% is below required 20%&quot;).</p>\n<h4 id=\"design-philosophy\">Design Philosophy</h4>\n<ol>\n<li><strong>Composability</strong>: Matchers can be combined using logical operators (<code>&amp;</code> for AND, <code>|</code> for OR, <code>~</code> for NOT) to build complex conditions.</li>\n<li><strong>Descriptive Failure Messages</strong>: Each matcher knows how to describe what it expected versus what it got, in domain-specific terms.</li>\n<li><strong>Reusability</strong>: Define once, use across multiple tests and even multiple projects.</li>\n<li><strong>Readability</strong>: Matcher-based assertions read like natural language specifications.</li>\n</ol>\n<h4 id=\"architecture-decision-record-object-oriented-matchers-vs-function-based-matchers\">Architecture Decision Record: Object-Oriented Matchers vs Function-Based Matchers</h4>\n<blockquote>\n<p><strong>Decision: Use Object-Oriented Matcher Classes with <code>__matches__</code> Protocol</strong></p>\n<ul>\n<li><strong>Context</strong>: We need an extensible system where users can define custom matching logic. Two approaches exist: class-based matchers with a standard method, or function-based matchers with a registration system.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Function-Based</strong>: Users define <code>def is_even(x): return x % 2 == 0</code> and register it. Assertion is <code>assert_that(value, is_even)</code>.</li>\n<li><strong>Class-Based</strong>: Users define <code>class EvenMatcher:</code> with a <code>matches(actual)</code> method and <code>describe()</code> method.</li>\n<li><strong>Hybrid Protocol</strong>: Support both through single-dispatch or adapter pattern.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use class-based matchers with a defined protocol (<code>__matches__</code> and <code>__describe__</code> methods).</li>\n<li><strong>Rationale</strong>:<ul>\n<li>Classes naturally encapsulate both the matching logic <em>and</em> the description logic.</li>\n<li>Classes support stateful matchers (e.g., <code>GreaterThan(threshold)</code> where threshold is stored).</li>\n<li>Operator overloading (<code>&amp;</code>, <code>|</code>, <code>~</code>) is cleaner with classes.</li>\n<li>Inheritance provides a clear way to extend base matcher functionality.</li>\n<li>The pattern is well-established in libraries like Hamcrest (Java) and its Python ports.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>Slightly more verbose than simple functions.</li>\n<li>Provides better structure for complex matchers.</li>\n<li>Enables matcher composition out of the box.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Function-Based</td>\n<td>Simple; familiar; minimal syntax</td>\n<td>Hard to compose; limited description capability</td>\n<td>Too limited for serious use</td>\n</tr>\n<tr>\n<td>Class-Based</td>\n<td>Full-featured; composable; extensible</td>\n<td>More boilerplate; steeper learning curve</td>\n<td><strong>CHOSEN</strong>: Professional-grade flexibility</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Maximum flexibility; accommodates both styles</td>\n<td>Complex implementation; ambiguous best practices</td>\n<td>Complexity not justified for learning</td>\n</tr>\n</tbody></table>\n<h4 id=\"core-matcher-protocol\">Core Matcher Protocol</h4>\n<p>All matchers implement this simple protocol:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Signature</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>__matches__</code></td>\n<td><code>(self, actual: Any) -&gt; bool</code></td>\n<td>Boolean</td>\n<td>Core matching logic. Return <code>True</code> if actual value satisfies the matcher&#39;s condition.</td>\n</tr>\n<tr>\n<td><code>__describe__</code></td>\n<td><code>(self) -&gt; str</code></td>\n<td>String</td>\n<td>Description of what the matcher expects. Used in failure messages.</td>\n</tr>\n<tr>\n<td><code>__describe_mismatch__</code></td>\n<td><code>(self, actual: Any) -&gt; str</code></td>\n<td>String</td>\n<td>Optional. More specific description of why <code>actual</code> didn&#39;t match.</td>\n</tr>\n<tr>\n<td><code>__and__</code></td>\n<td><code>(self, other) -&gt; AllOf</code></td>\n<td>Matcher</td>\n<td>Operator overload for <code>&amp;</code> (AND composition).</td>\n</tr>\n<tr>\n<td><code>__or__</code></td>\n<td><code>(self, other) -&gt; AnyOf</code></td>\n<td>Matcher</td>\n<td>Operator overload for `</td>\n</tr>\n<tr>\n<td><code>__invert__</code></td>\n<td><code>(self) -&gt; Not</code></td>\n<td>Matcher</td>\n<td>Operator overload for <code>~</code> (NOT composition).</td>\n</tr>\n</tbody></table>\n<p>The <code>assert_that(actual, matcher, msg=None)</code> function is the bridge between matchers and the assertion engine:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Conceptual implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_that</span><span style=\"color:#E1E4E8\">(actual, matcher, msg</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> matcher.__matches__(actual):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Build failure message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        description </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> matcher.__describe__()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mismatch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (matcher.__describe_mismatch__(actual) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(matcher, </span><span style=\"color:#9ECBFF\">'__describe_mismatch__'</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    else</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"was </span><span style=\"color:#79B8FF\">{repr</span><span style=\"color:#E1E4E8\">(actual)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        full_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Expected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">description</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        full_message </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Got: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">mismatch</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> msg:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            full_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">msg</span><span style=\"color:#79B8FF\">}\\n{</span><span style=\"color:#E1E4E8\">full_message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> AssertionError</span><span style=\"color:#E1E4E8\">(full_message)</span></span></code></pre></div>\n\n<h4 id=\"built-in-matcher-library\">Built-in Matcher Library</h4>\n<p>The framework provides a comprehensive set of built-in matchers:</p>\n<table>\n<thead>\n<tr>\n<th>Matcher Class</th>\n<th>Constructor</th>\n<th>Purpose</th>\n<th>Example Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>EqualTo</code></td>\n<td><code>(expected)</code></td>\n<td>Value equality</td>\n<td><code>assert_that(5, EqualTo(5))</code></td>\n</tr>\n<tr>\n<td><code>IsInstanceOf</code></td>\n<td><code>(type_or_types)</code></td>\n<td>Type checking</td>\n<td><code>assert_that(obj, IsInstanceOf(MyClass))</code></td>\n</tr>\n<tr>\n<td><code>Contains</code></td>\n<td><code>(item)</code></td>\n<td>Membership</td>\n<td><code>assert_that([1,2,3], Contains(2))</code></td>\n</tr>\n<tr>\n<td><code>HasLength</code></td>\n<td><code>(length)</code></td>\n<td>Collection size</td>\n<td><code>assert_that(&quot;abc&quot;, HasLength(3))</code></td>\n</tr>\n<tr>\n<td><code>GreaterThan</code></td>\n<td><code>(threshold)</code></td>\n<td>Numeric &gt;</td>\n<td><code>assert_that(10, GreaterThan(5))</code></td>\n</tr>\n<tr>\n<td><code>LessThan</code></td>\n<td><code>(threshold)</code></td>\n<td>Numeric &lt;</td>\n<td><code>assert_that(3, LessThan(5))</code></td>\n</tr>\n<tr>\n<td><code>CloseTo</code></td>\n<td><code>(expected, tolerance)</code></td>\n<td>Approximate equality</td>\n<td><code>assert_that(3.001, CloseTo(3.0, 0.01))</code></td>\n</tr>\n<tr>\n<td><code>MatchesRegex</code></td>\n<td><code>(pattern)</code></td>\n<td>Pattern matching</td>\n<td><code>assert_that(&quot;hello&quot;, MatchesRegex(r&quot;h.*o&quot;))</code></td>\n</tr>\n<tr>\n<td><code>HasAttribute</code></td>\n<td><code>(name, value_matcher=None)</code></td>\n<td>Object attribute</td>\n<td><code>assert_that(obj, HasAttribute(&quot;size&quot;, GreaterThan(0)))</code></td>\n</tr>\n<tr>\n<td><code>HasProperty</code></td>\n<td><code>(name, value_matcher=None)</code></td>\n<td>Property/getter</td>\n<td><code>assert_that(obj, HasProperty(&quot;area&quot;))</code></td>\n</tr>\n<tr>\n<td><code>Is</code></td>\n<td><code>(expected)</code></td>\n<td>Identity (<code>is</code>)</td>\n<td><code>assert_that(x, Is(None))</code></td>\n</tr>\n<tr>\n<td><code>IsNone</code></td>\n<td><code>()</code></td>\n<td>Specialized for None</td>\n<td><code>assert_that(result, IsNone())</code></td>\n</tr>\n<tr>\n<td><code>IsTrue</code></td>\n<td><code>()</code></td>\n<td>Truthiness</td>\n<td><code>assert_that(flag, IsTrue())</code></td>\n</tr>\n<tr>\n<td><code>IsFalse</code></td>\n<td><code>()</code></td>\n<td>Falsiness</td>\n<td><code>assert_that(flag, IsFalse())</code></td>\n</tr>\n<tr>\n<td><code>IsEmpty</code></td>\n<td><code>()</code></td>\n<td>Empty collection</td>\n<td><code>assert_that([], IsEmpty())</code></td>\n</tr>\n<tr>\n<td><code>IsNotEmpty</code></td>\n<td><code>()</code></td>\n<td>Non-empty collection</td>\n<td><code>assert_that([1], IsNotEmpty())</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"matcher-composition\">Matcher Composition</h4>\n<p>The real power emerges when matchers are combined:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Conceptual examples (actual API would use matcher objects)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(response, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HasAttribute(</span><span style=\"color:#9ECBFF\">\"status_code\"</span><span style=\"color:#E1E4E8\">, EqualTo(</span><span style=\"color:#79B8FF\">200</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">&#x26;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HasAttribute(</span><span style=\"color:#9ECBFF\">\"headers\"</span><span style=\"color:#E1E4E8\">, Contains(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">&#x26;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HasAttribute(</span><span style=\"color:#9ECBFF\">\"json\"</span><span style=\"color:#E1E4E8\">, HasAttribute(</span><span style=\"color:#9ECBFF\">\"success\"</span><span style=\"color:#E1E4E8\">, IsTrue())))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IsInstanceOf(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x26;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    (GreaterThan(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\"> EqualTo(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)))  </span><span style=\"color:#6A737D\"># Positive or -1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(string,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MatchesRegex(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">^\\d</span><span style=\"color:#F97583\">{3}</span><span style=\"color:#DBEDFF\">-</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#DBEDFF\">-</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{4}</span><span style=\"color:#79B8FF\">$</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">&#x26;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    ~</span><span style=\"color:#E1E4E8\">Contains(</span><span style=\"color:#9ECBFF\">\"000-00-0000\"</span><span style=\"color:#E1E4E8\">))  </span><span style=\"color:#6A737D\"># Valid SSN but not the dummy one</span></span></code></pre></div>\n\n<p><strong>Composition Matchers</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Composition Class</th>\n<th>Creates</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AllOf</code></td>\n<td><code>matcher1 &amp; matcher2 &amp; ...</code></td>\n<td>Logical AND; all matchers must match</td>\n</tr>\n<tr>\n<td><code>AnyOf</code></td>\n<td>`matcher1</td>\n<td>matcher2</td>\n</tr>\n<tr>\n<td><code>Not</code></td>\n<td><code>~matcher</code></td>\n<td>Logical NOT; inverts the matcher</td>\n</tr>\n<tr>\n<td><code>IsAny</code></td>\n<td><code>AnyOf(AlwaysTrue())</code></td>\n<td>Matches anything (useful as default)</td>\n</tr>\n<tr>\n<td><code>IsNothing</code></td>\n<td><code>AllOf(AlwaysFalse())</code></td>\n<td>Matches nothing (useful in combinations)</td>\n</tr>\n</tbody></table>\n<h4 id=\"custom-matcher-creation\">Custom Matcher Creation</h4>\n<p>Users create custom matchers by subclassing <code>BaseMatcher</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Conceptual example</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IsEven</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseMatcher</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __matches__</span><span style=\"color:#E1E4E8\">(self, actual):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(actual, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> actual </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"an even integer\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe_mismatch__</span><span style=\"color:#E1E4E8\">(self, actual):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(actual, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"was a </span><span style=\"color:#79B8FF\">{type</span><span style=\"color:#E1E4E8\">(actual).</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">{repr</span><span style=\"color:#E1E4E8\">(actual)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"was odd (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, IsEven())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, IsEven() </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\"> GreaterThan(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span></code></pre></div>\n\n<p><strong>Stateful Matcher Example</strong> (more realistic):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HasStatusCode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseMatcher</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, expected_status, allowed_errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected_status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_status</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.allowed_errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> allowed_errors </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __matches__</span><span style=\"color:#E1E4E8\">(self, actual):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(actual, </span><span style=\"color:#9ECBFF\">'status_code'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (actual.status_code </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.expected_status </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                actual.status_code </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.allowed_errors)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        desc </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"response with status </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.expected_status</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.allowed_errors:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            desc </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\" or allowed errors </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.allowed_errors</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> desc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe_mismatch__</span><span style=\"color:#E1E4E8\">(self, actual):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(actual, </span><span style=\"color:#9ECBFF\">'status_code'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"object has no 'status_code' attribute\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"had status </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual.status_code</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage in API tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(response, HasStatusCode(</span><span style=\"color:#79B8FF\">200</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(error_response, HasStatusCode(</span><span style=\"color:#79B8FF\">200</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">allowed_errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">503</span><span style=\"color:#E1E4E8\">]))</span></span></code></pre></div>\n\n<h4 id=\"integration-with-assertion-engine\">Integration with Assertion Engine</h4>\n<p>Matchers integrate seamlessly with the core assertion system:</p>\n<ol>\n<li><p><strong>Failure Message Integration</strong>: When <code>assert_that</code> fails, it creates an <code>AssertionFailure</code> with:</p>\n<ul>\n<li><code>expected</code>: The matcher&#39;s <code>__describe__()</code> output</li>\n<li><code>actual</code>: Either the raw value or matcher&#39;s <code>__describe_mismatch__()</code> output</li>\n<li><code>hint</code>: Possible fixes based on matcher type</li>\n</ul>\n</li>\n<li><p><strong>Type Registry</strong>: Matchers can register themselves as handlers for specific types in the Assertion Engine&#39;s comparison registry. For example, a <code>NumpyArrayCloseTo</code> matcher could register itself for <code>numpy.ndarray</code> type, making <code>assert_equal(numpy_array1, numpy_array2)</code> automatically use approximate comparison.</p>\n</li>\n<li><p><strong>Negation Support</strong>: The <code>assert_not_that(actual, matcher)</code> function provides the inverse check, using the <code>Not</code> composition matcher internally.</p>\n</li>\n</ol>\n<h4 id=\"common-pitfalls-in-matchers-api-implementation\">Common Pitfalls in Matchers API Implementation</h4>\n<p>⚠️ <strong>Pitfall: Matcher Side Effects in <code>__describe__</code></strong></p>\n<ul>\n<li><strong>Description</strong>: The <code>__describe__()</code> method should be pure (no side effects), but developers might compute expensive descriptions or modify state.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Description might be called even when test passes (in some implementations), causing performance issues or state corruption.</li>\n<li><strong>Fix</strong>: Make <code>__describe__()</code> a pure method. Compute expensive descriptions lazily or cache them. Document clearly that it should be side-effect free.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Incorrect Operator Precedence</strong></p>\n<ul>\n<li><strong>Description</strong>: <code>matcher1 &amp; matcher2 | matcher3</code> doesn&#39;t group as expected due to Python&#39;s operator precedence.</li>\n<li><strong>Why It&#39;s Wrong</strong>: <code>&amp;</code> has higher precedence than <code>|</code>, so this evaluates as <code>(matcher1 &amp; matcher2) | matcher3</code>, not <code>matcher1 &amp; (matcher2 | matcher3)</code>.</li>\n<li><strong>Fix</strong>: Use parentheses explicitly or implement <code>__rand__</code>, <code>__ror__</code> to try to handle gracefully. Document the precedence clearly.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Matcher Mutating the Actual Value</strong></p>\n<ul>\n<li><strong>Description</strong>: Matcher&#39;s <code>__matches__()</code> method modifies the actual value being tested.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Changes test state unexpectedly; makes tests non-deterministic.</li>\n<li><strong>Fix</strong>: Never modify <code>actual</code>. Document as requirement. Use defensive copies if needed.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Overly Complex Matcher Composition</strong></p>\n<ul>\n<li><strong>Description</strong>: Chaining too many matchers creates incomprehensible failure messages like &quot;Expected: (A and (B or (C and not D)))...&quot;</li>\n<li><strong>Why It&#39;s Wrong</strong>: Debugging becomes as hard as the original problem.</li>\n<li><strong>Fix</strong>: Encourage creating named composite matchers for common patterns. Simplify output formatting for nested compositions.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Matcher Not Handling <code>None</code> Gracefully</strong></p>\n<ul>\n<li><strong>Description</strong>: <code>HasAttribute(&quot;name&quot;)(None)</code> raises <code>AttributeError</code> instead of returning <code>False</code>.</li>\n<li><strong>Why It&#39;s Wrong</strong>: Matchers should handle all inputs gracefully, returning <code>False</code> for non-matching values, not raising exceptions.</li>\n<li><strong>Fix</strong>: Use <code>getattr()</code> with default, <code>isinstance()</code> checks, or try-except in <code>__matches__()</code>.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Comparison Engine</td>\n<td>Python&#39;s <code>==</code> with <code>repr()</code> fallback</td>\n<td>Type-registry with specialized comparators for numpy/pandas/dataclasses</td>\n</tr>\n<tr>\n<td>Diff Generation</td>\n<td><code>difflib.unified_diff</code> for strings only</td>\n<td>Recursive diff with similarity scoring for nested structures</td>\n</tr>\n<tr>\n<td>Float Comparison</td>\n<td><code>abs(a-b) &lt; tolerance</code></td>\n<td><code>math.isclose()</code> with relative/absolute tolerance and ULPs</td>\n</tr>\n<tr>\n<td>Matcher Protocol</td>\n<td>Simple <code>matches()</code> method</td>\n<td>Full protocol with <code>describe_to()</code> and <code>describe_mismatch()</code> like Hamcrest</td>\n</tr>\n<tr>\n<td>Exception Testing</td>\n<td>Function wrapper <code>assert_raises()</code></td>\n<td>Context manager with exception inspection and regex matching</td>\n</tr>\n</tbody></table>\n<p>For a learning framework, start with the Simple options and evolve toward Advanced as needed.</p>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n├── __init__.py           # Public API: assert_equal, assert_that, etc.\n├── assertions/\n│   ├── __init__.py       # Re-export core assertions\n│   ├── engine.py         # AssertionEngine class, AssertionFailure\n│   ├── core.py           # assert_equal, assert_true, assert_raises, etc.\n│   ├── collections.py    # Collection-specific assertions\n│   ├── exceptions.py     # Exception-related assertions\n│   └── numeric.py        # Numeric comparison assertions\n├── matchers/\n│   ├── __init__.py       # BaseMatcher, assert_that, built-in matchers\n│   ├── base.py           # BaseMatcher, AllOf, AnyOf, Not\n│   ├── builtins.py       # EqualTo, GreaterThan, Contains, etc.\n│   ├── collections.py    # Collection matchers\n│   ├── numeric.py        # Numeric matchers  \n│   └── objects.py        # Object/attribute matchers\n└── internal/\n    ├── diff.py           # Diff computation utilities\n    └── comparison.py     # Type-specific comparison logic</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong><code>apollo/internal/diff.py</code></strong> (complete working code):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Diff computation utilities for assertion messages.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> difflib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pprint</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> compute_unified_diff</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actual: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fromfile: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"expected\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tofile: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"actual\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    n: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compute unified diff between two strings.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        expected: Expected string value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        actual: Actual string value  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        fromfile: Label for expected in diff header</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tofile: Label for actual in diff header</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        n: Number of context lines</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Unified diff string or empty string if strings are identical</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> expected </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> actual:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected.splitlines(</span><span style=\"color:#FFAB70\">keepends</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actual_lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual.splitlines(</span><span style=\"color:#FFAB70\">keepends</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> difflib.unified_diff(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_lines, actual_lines,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        fromfile</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">fromfile, </span><span style=\"color:#FFAB70\">tofile</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tofile,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        n</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">n, </span><span style=\"color:#FFAB70\">lineterm</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">''</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">.join(diff)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> format_truncated</span><span style=\"color:#E1E4E8\">(value: Any, max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Format value with intelligent truncation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        value: Any value to format</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        max_length: Maximum string length before truncation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Formatted string, possibly truncated with ellipsis</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        formatted </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pprint.pformat(value, </span><span style=\"color:#FFAB70\">width</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">70</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">depth</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        formatted </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> repr</span><span style=\"color:#E1E4E8\">(value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(formatted) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> max_length:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Truncate at the last whitespace before max_length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        truncated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> formatted[:max_length]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_space </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> truncated.rfind(</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> last_space </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> max_length </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># Reasonable break point</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            truncated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> truncated[:last_space]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">truncated</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">... (length: </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(formatted)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> formatted</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> diff_dicts</span><span style=\"color:#E1E4E8\">(expected: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">, actual: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compute differences between two dictionaries.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        expected: Expected dictionary</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        actual: Actual dictionary</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of difference descriptions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    differences </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keys only in expected</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    missing_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(expected.keys()) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(actual.keys())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> missing_keys:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        differences.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Missing keys: </span><span style=\"color:#79B8FF\">{sorted</span><span style=\"color:#E1E4E8\">(missing_keys)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Keys only in actual  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extra_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(actual.keys()) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(expected.keys())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> extra_keys:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        differences.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Extra keys: </span><span style=\"color:#79B8FF\">{sorted</span><span style=\"color:#E1E4E8\">(extra_keys)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Common keys with different values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    common_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(expected.keys()) </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(actual.keys())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(common_keys):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> expected[key] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> actual[key]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exp_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> format_truncated(expected[key], </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            act_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> format_truncated(actual[key], </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            differences.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Key '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">': expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">exp_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">act_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> differences</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> diff_lists</span><span style=\"color:#E1E4E8\">(expected: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, actual: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">) -> Optional[Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, Any, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Find first difference between two lists.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        expected: Expected list</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        actual: Actual list</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (index, expected_value, actual_value) or None if identical</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    min_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(expected), </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(actual))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(min_len):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> expected[i] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> actual[i]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> i, expected[i], actual[i]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(actual):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Different lengths but all elements up to min_len match</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> min_len, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expected[min_len] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(expected) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> min_len </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"&#x3C;end of list>\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            actual[min_len] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(actual) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> min_len </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"&#x3C;end of list>\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<p><strong><code>apollo/internal/comparison.py</code></strong> (complete working code):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Type-specific comparison logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, Callable, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComparisonResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of a comparison operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"equal\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DIFFERENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"different\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TYPE_MISMATCH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"type_mismatch\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    APPROXIMATELY_EQUAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"approximately_equal\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> compare_floats</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rel_tol: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    abs_tol: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> ComparisonResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compare two floats with tolerance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Uses same logic as math.isclose() but returns detailed result.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        a: First float</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        b: Second float</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        rel_tol: Relative tolerance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        abs_tol: Absolute tolerance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ComparisonResult indicating equality level</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> math.isnan(a) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> math.isnan(b):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ComparisonResult.</span><span style=\"color:#79B8FF\">EQUAL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> math.isinf(a) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> math.isinf(b) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> b:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ComparisonResult.</span><span style=\"color:#79B8FF\">EQUAL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diff </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> abs</span><span style=\"color:#E1E4E8\">(a </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> b)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(rel_tol </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">abs</span><span style=\"color:#E1E4E8\">(a), </span><span style=\"color:#79B8FF\">abs</span><span style=\"color:#E1E4E8\">(b)), abs_tol)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> diff </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> threshold:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (ComparisonResult.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> diff </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\"> ComparisonResult.</span><span style=\"color:#79B8FF\">APPROXIMATELY_EQUAL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ComparisonResult.</span><span style=\"color:#79B8FF\">DIFFERENT</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> are_similar_types</span><span style=\"color:#E1E4E8\">(a: Any, b: Any) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if two values have similar/convertible types.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Useful for detecting 1 vs 1.0, \"1\" vs 1, etc.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        a: First value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        b: Second value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        True if types are similar (int/float, str/bytes, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    type_a, type_b </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> type</span><span style=\"color:#E1E4E8\">(a), </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">(b)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Same type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> type_a </span><span style=\"color:#F97583\">is</span><span style=\"color:#E1E4E8\"> type_b:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Numeric types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numeric_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">complex</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(a, numeric_types) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(b, numeric_types):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # String/bytes types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    text_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bytearray</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(a, text_types) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(b, text_types):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Collection types with similar interfaces</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    list_like </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">range</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(a, list_like) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(b, list_like):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dict_like </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">,)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(a, dict_like) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(b, dict_like):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    set_like </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">frozenset</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(a, set_like) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(b, set_like):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TypeComparatorRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Registry of type-specific comparison functions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._comparators: Dict[</span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">, Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._default_comparator </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> lambda</span><span style=\"color:#E1E4E8\"> a, b: ComparisonResult.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> b </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> ComparisonResult.</span><span style=\"color:#79B8FF\">DIFFERENT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register</span><span style=\"color:#E1E4E8\">(self, type_obj: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">, comparator: Callable):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a comparator for a specific type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._comparators[type_obj] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> comparator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_comparator</span><span style=\"color:#E1E4E8\">(self, value: Any) -> Callable:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get appropriate comparator for a value's type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check exact type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        comparator </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._comparators.get(</span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">(value))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> comparator:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> comparator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check parent classes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> type</span><span style=\"color:#E1E4E8\">(value).</span><span style=\"color:#79B8FF\">__mro__</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._comparators:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._comparators[</span><span style=\"color:#79B8FF\">cls</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Default</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._default_comparator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare</span><span style=\"color:#E1E4E8\">(self, a: Any, b: Any, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> ComparisonResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare two values using registered comparators.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> type</span><span style=\"color:#E1E4E8\">(a) </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> type</span><span style=\"color:#E1E4E8\">(b) </span><span style=\"color:#F97583\">and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> are_similar_types(a, b):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ComparisonResult.</span><span style=\"color:#79B8FF\">TYPE_MISMATCH</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        comparator </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_comparator(a)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> comparator(a, b, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global registry instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">comparator_registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeComparatorRegistry()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Register built-in comparators</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">comparator_registry.register(</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    lambda</span><span style=\"color:#E1E4E8\"> a, b, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kw: compare_floats(a, b, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kw.get(</span><span style=\"color:#9ECBFF\">'rel_tol'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1e-9</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kw.get(</span><span style=\"color:#9ECBFF\">'abs_tol'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">comparator_registry.register(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    lambda</span><span style=\"color:#E1E4E8\"> a, b, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kw: compare_floats(</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(a), </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(b), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kw.get(</span><span style=\"color:#9ECBFF\">'rel_tol'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1e-9</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        kw.get(</span><span style=\"color:#9ECBFF\">'abs_tol'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">)) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> kw.get(</span><span style=\"color:#9ECBFF\">'tolerance'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        (ComparisonResult.</span><span style=\"color:#79B8FF\">EQUAL</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> a </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> b </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> ComparisonResult.</span><span style=\"color:#79B8FF\">DIFFERENT</span><span style=\"color:#E1E4E8\">))</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong><code>apollo/assertions/engine.py</code></strong> (skeleton with TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Assertion engine core logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..internal.diff </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> compute_unified_diff, format_truncated, diff_dicts, diff_lists</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..internal.comparison </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ComparisonResult, comparator_registry</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AssertionType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Type of assertion for categorization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALITY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"equality\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TRUTHINESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"truthiness\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXCEPTION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"exception\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPARISON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"comparison\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MEMBERSHIP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"membership\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TYPE_CHECK</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"type_check\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COLLECTION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"collection\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CUSTOM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"custom\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AssertionFailure</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Detailed information about a failed assertion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actual: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diff: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hint: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assertion_type: AssertionType </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AssertionType.</span><span style=\"color:#79B8FF\">CUSTOM</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Format the failure for display.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build a readable message with sections:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Custom message (if provided)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Expected vs Actual</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Diff (if available)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Hint (if available)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use clear separators and indentation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> AssertionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Custom assertion error with rich failure information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, failure: AssertionFailure):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> failure</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(failure))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AssertionEngine</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Central engine for evaluating assertions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> assert_equal</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        actual: Any,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected: Any,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        msg: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tolerance: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rel_tol: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        abs_tol: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ignore_case: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ignore_whitespace: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        unordered: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Assert that actual equals expected with optional tolerance.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            actual: The actual value obtained</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            expected: The expected value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            msg: Optional custom failure message</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            tolerance: Absolute tolerance for numeric comparison (shortcut)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            rel_tol: Relative tolerance for numeric comparison</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            abs_tol: Absolute tolerance for numeric comparison</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ignore_case: For strings, compare case-insensitively</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ignore_whitespace: For strings, normalize whitespace</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            unordered: For collections, ignore order of elements</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            AssertionError: If values don't match with descriptive message</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Normalize inputs based on flags</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If ignore_case and strings: convert both to lowercase</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If ignore_whitespace and strings: normalize whitespace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If unordered and sequences: consider sorted versions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Determine comparison strategy based on types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Use comparator_registry.compare() with tolerance parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle tolerance parameter (convert to abs_tol)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check comparison result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If EQUAL or APPROXIMATELY_EQUAL: return None (success)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If DIFFERENT or TYPE_MISMATCH: proceed to failure handling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate failure details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Compute diff based on types (strings, lists, dicts, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Create hint based on common mistakes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Format expected/actual with truncation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Build AssertionFailure object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Include custom message if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Set assertion_type to AssertionType.EQUALITY</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Raise AssertionError with the failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> assert_true</span><span style=\"color:#E1E4E8\">(self, condition: Any, msg: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Assert that condition is truthy.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            condition: Value to check for truthiness</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            msg: Optional custom failure message</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            AssertionError: If condition is falsy</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Convert condition to bool using bool()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: If falsy, build failure message showing actual value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Include common falsy values in hint (0, '', [], None, False)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> assert_raises</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_exception: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        *</span><span style=\"color:#E1E4E8\">args,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        **</span><span style=\"color:#E1E4E8\">kwargs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager to assert that code raises an exception.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Can be used as:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            with assert_raises(ValueError):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                int(\"not a number\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Or as function call (legacy):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            assert_raises(ValueError, int, \"not a number\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            expected_exception: Exception type expected to be raised</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            *args: If provided, call the first arg as function with remaining args</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            **kwargs: May include 'msg' for custom message or 'match' for regex</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Context manager or raises AssertionError</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: If args provided, use legacy function-call mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - First arg is callable, rest are args to call it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call it and check for exception</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Otherwise, return context manager instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - The context manager should implement __enter__ and __exit__</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - __exit__ should check if correct exception was raised</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Store exception in context manager for inspection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Support 'match' regex parameter to check exception message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_failure_message</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_desc: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        actual_desc: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        diff: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hint: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        custom_msg: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Format a consistent failure message.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            expected_desc: Description of expected value/condition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            actual_desc: Description of actual value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            diff: Optional diff output</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            hint: Optional hint for debugging</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            custom_msg: Optional custom message from user</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Formatted failure message</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Build message with clear sections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Start with custom_msg if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Expected: expected_desc</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Actual: actual_desc  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Diff if available (with separator)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Hint if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use consistent indentation and line breaks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_diff</span><span style=\"color:#E1E4E8\">(self, expected: Any, actual: Any) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute appropriate diff based on value types.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            expected: Expected value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            actual: Actual value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Diff string or None if not applicable</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Handle different types:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Strings: use compute_unified_diff()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Lists: show first differing element with index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Dicts: use diff_dicts() and format results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Other: try repr() and string diff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global engine instance for convenience</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">engine </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AssertionEngine()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Public API functions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_equal</span><span style=\"color:#E1E4E8\">(actual: Any, expected: Any, msg: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Public API for assert_equal.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> engine.assert_equal(actual, expected, msg, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_true</span><span style=\"color:#E1E4E8\">(condition: Any, msg: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Public API for assert_true.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> engine.assert_true(condition, msg)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_raises</span><span style=\"color:#E1E4E8\">(expected_exception: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Public API for assert_raises.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> engine.assert_raises(expected_exception, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span></code></pre></div>\n\n<p><strong><code>apollo/matchers/base.py</code></strong> (skeleton with TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Base classes for matcher API.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseMatcher</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract base class for all matchers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __matches__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if actual value matches the condition.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            actual: The value to test</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if actual satisfies the matcher's condition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Describe what the matcher expects.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            String description of expected condition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe_mismatch__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Describe why actual value didn't match.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Override for more specific mismatch messages.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            actual: The actual value that didn't match</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            String description of mismatch</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"was </span><span style=\"color:#79B8FF\">{repr</span><span style=\"color:#E1E4E8\">(actual)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __and__</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'BaseMatcher'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'AllOf'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Operator overload for &#x26; (AND).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Return AllOf matcher containing self and other</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __or__</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'BaseMatcher'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'AnyOf'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Operator overload for | (OR).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return AnyOf matcher containing self and other</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __invert__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'Not'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Operator overload for ~ (NOT).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return Not matcher wrapping self</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __rand__</span><span style=\"color:#E1E4E8\">(self, other: Any) -> </span><span style=\"color:#9ECBFF\">'AllOf'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reverse AND operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle case where other is not a matcher</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Try to convert other to a matcher (e.g., EqualTo(other))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Then return AllOf of converted and self</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __ror__</span><span style=\"color:#E1E4E8\">(self, other: Any) -> </span><span style=\"color:#9ECBFF\">'AnyOf'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reverse OR operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Similar to __rand__ but for OR</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AllOf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseMatcher</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Matcher that requires ALL sub-matchers to match.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">matchers: BaseMatcher):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store matchers in instance variable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __matches__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Check that ALL matchers match actual</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Return False as soon as one fails</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Keep track of which failed for better messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Describe as \"all of: [matcher1, matcher2, ...]\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle single matcher case specially</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe_mismatch__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Provide detailed mismatch showing which sub-matcher failed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Test each matcher</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - For first failing matcher, include its mismatch description</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AnyOf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseMatcher</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Matcher that requires ANY sub-matcher to match.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">matchers: BaseMatcher):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Store matchers in instance variable  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __matches__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Check if ANY matcher matches actual</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Return True as soon as one matches</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Describe as \"any of: [matcher1, matcher2, ...]\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe_mismatch__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Show that NONE of the matchers matched</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Include mismatch from each matcher (could be long)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Consider truncation for many matchers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Not</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseMatcher</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Matcher that inverts another matcher.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, matcher: BaseMatcher):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Store wrapped matcher</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __matches__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Return NOT of wrapped matcher's match result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 16: Describe as \"not: [description of wrapped matcher]\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe_mismatch__</span><span style=\"color:#E1E4E8\">(self, actual: Any) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 17: Special case: if NOT fails, the wrapped matcher matched</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Return something like \"unexpectedly matched: [description]\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_that</span><span style=\"color:#E1E4E8\">(actual: Any, matcher: BaseMatcher, msg: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Assert that actual value satisfies the matcher.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        actual: Value to test</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        matcher: Matcher defining the condition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        msg: Optional custom failure message</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        AssertionError: If actual doesn't match matcher</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 18: Check if matcher matches actual</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 19: If not, get descriptions from matcher</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 20: Build failure message using matcher.__describe__() and </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #          matcher.__describe_mismatch__()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 21: Include custom message if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 22: Raise AssertionError with formatted message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints-python\">Language-Specific Hints (Python)</h4>\n<ul>\n<li><strong>Float Comparisons</strong>: Use <code>math.isclose()</code> for approximate equality with <code>rel_tol</code> and <code>abs_tol</code> parameters. For very small numbers near zero, absolute tolerance is crucial.</li>\n<li><strong>Exception Context Managers</strong>: Implement <code>__enter__</code> returning <code>self</code> and <code>__exit__(self, exc_type, exc_val, exc_tb)</code> that returns <code>True</code> if exception was handled (suppressed), <code>False</code> to propagate.</li>\n<li><strong>Operator Overloading</strong>: Matchers use <code>__and__</code>, <code>__or__</code>, <code>__invert__</code> for composition. Also implement <code>__rand__</code> and <code>__ror__</code> to handle <code>value &amp; matcher</code> syntax.</li>\n<li><strong>Recursion Protection</strong>: Use <code>sys.getrecursionlimit()</code> and <code>functools.lru_cache</code> or manual visited-sets when comparing nested structures.</li>\n<li><strong>Type Inspection</strong>: <code>isinstance()</code> is more flexible than <code>type() ==</code> for inheritance. Use <code>typing.get_args()</code> and <code>typing.get_origin()</code> for generic type checking.</li>\n<li><strong>String Formatting</strong>: Use f-strings with <code>!r</code> for <code>repr()</code>: <code>f&quot;Expected {expected!r}, got {actual!r}&quot;</code>.</li>\n<li><strong>Diff Computation</strong>: <code>difflib.unified_diff()</code> is good for strings; for nested structures, implement recursive diff with path tracking.</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing Milestone 2, you should be able to run:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create a test file test_assertions.py</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo.cli</span><span style=\"color:#9ECBFF\"> test_assertions.py</span></span></code></pre></div>\n\n<p><strong>Expected Behavior</strong>:</p>\n<ul>\n<li>Tests using <code>assert_equal()</code>, <code>assert_true()</code>, <code>assert_raises()</code> should pass or fail with descriptive messages.</li>\n<li>Collection assertions should show element-by-element differences.</li>\n<li>Float comparisons with tolerance should work correctly.</li>\n<li>Matcher API: <code>assert_that(value, EqualTo(5))</code> should work.</li>\n<li>Custom matchers should be creatable and composable.</li>\n</ul>\n<p><strong>Verification Tests</strong> (create in <code>test_assertions.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># These should PASS</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_equal(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_equal(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tolerance</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_true(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">with</span><span style=\"color:#E1E4E8\"> assert_raises(</span><span style=\"color:#79B8FF\">ValueError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not a number\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, EqualTo(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># These should FAIL with good messages</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_equal([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">,</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])  </span><span style=\"color:#6A737D\"># Should show element 2: 3 != 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_equal({</span><span style=\"color:#9ECBFF\">\"a\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">}, {</span><span style=\"color:#9ECBFF\">\"a\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">})  </span><span style=\"color:#6A737D\"># Should show key 'a' difference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_equal(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should fail without tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">assert_that(</span><span style=\"color:#9ECBFF\">\"hello\"</span><span style=\"color:#E1E4E8\">, Contains(</span><span style=\"color:#9ECBFF\">\"z\"</span><span style=\"color:#E1E4E8\">))  </span><span style=\"color:#6A737D\"># Should say \"expected to contain 'z'\"</span></span></code></pre></div>\n\n<p><strong>Signs Something is Wrong</strong>:</p>\n<ul>\n<li>Failure messages show only <code>&lt;object at 0x...&gt;</code> → Missing <code>repr()</code> or formatting logic.</li>\n<li>Float tests always fail → Tolerance not applied or wrong default.</li>\n<li>Matcher composition <code>matcher1 &amp; matcher2</code> errors → Operator overloading not implemented.</li>\n<li>Exception assertions don&#39;t catch exceptions → Context manager <code>__exit__</code> logic incorrect.</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Assertion passes but prints error message anyway</td>\n<td>Message generation happens before checking equality</td>\n<td>Add debug print to see if message generation has side effects or is called unconditionally</td>\n<td>Generate messages lazily, only on failure</td>\n</tr>\n<tr>\n<td>Float comparison inconsistent</td>\n<td>Wrong tolerance type (absolute vs relative) or wrong defaults</td>\n<td>Print the actual diff: <code>abs(a-b)</code> and tolerance threshold</td>\n<td>Use <code>math.isclose()</code> logic with both rel_tol and abs_tol</td>\n</tr>\n<tr>\n<td>Matcher <code>&amp;</code> operator doesn&#39;t work with non-matcher on left</td>\n<td>Missing <code>__rand__</code> implementation</td>\n<td>Try <code>5 &amp; EqualTo(5)</code> vs <code>EqualTo(5) &amp; 5</code></td>\n<td>Implement <code>__rand__</code> that converts left operand to matcher</td>\n</tr>\n<tr>\n<td>Circular structure comparison crashes</td>\n<td>Infinite recursion in nested comparison</td>\n<td>Add print statement showing path being compared</td>\n<td>Maintain set of <code>id()</code> of objects already being compared</td>\n</tr>\n<tr>\n<td>Exception context lost in <code>assert_raises</code></td>\n<td>Using <code>raise ... from None</code> incorrectly</td>\n<td>Check traceback - does it show test code or assertion code?</td>\n<td>Preserve original traceback in exception chaining</td>\n</tr>\n<tr>\n<td>Diff output is huge for large objects</td>\n<td>No truncation logic</td>\n<td>Print size of diff string</td>\n<td>Implement intelligent truncation in <code>format_truncated()</code></td>\n</tr>\n<tr>\n<td>Matcher describes itself with object address</td>\n<td>Missing <code>__describe__</code> implementation or using default <code>repr</code></td>\n<td>Check if matcher class implements <code>__describe__</code></td>\n<td>Implement <code>__describe__</code> returning meaningful string</td>\n</tr>\n<tr>\n<td><code>assert_that(None, HasAttribute(&quot;x&quot;))</code> crashes</td>\n<td>Matcher assumes attribute exists without checking</td>\n<td>Add try-except in <code>__matches__</code> or check with <code>hasattr()</code></td>\n<td>Use <code>getattr(actual, name, None)</code> with sentinel</td>\n</tr>\n</tbody></table>\n<h2 id=\"7-component-design-fixtures-amp-setupteardown-milestone-3\">7. Component Design: Fixtures &amp; Setup/Teardown (Milestone 3)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (Fixtures &amp; Setup/Teardown)</p>\n</blockquote>\n<p>The <strong>Fixture System</strong> transforms the test framework from a simple sequence runner into a sophisticated resource management platform. While the runner handles the <em>execution</em> of tests and the assertion engine handles the <em>verification</em>, the fixture system handles the <em>preparation</em> and <em>cleanup</em>—creating the necessary environment for tests to run while ensuring no test pollutes another&#39;s environment. This milestone introduces one of the most powerful patterns in modern testing: dependency injection with automatic resource lifecycle management.</p>\n<p>Think of fixtures as the <strong>&quot;stage crew&quot;</strong> for your test performance. Before the actors (tests) take the stage, the crew sets up the scenery, props, and lighting. After each scene, they reset everything for the next performance, and after the entire play, they strike the set and return the theater to its original state. The fixture system is this crew, working behind the scenes to create, maintain, and dismantle test environments according to a precise schedule defined by <strong>scope</strong>.</p>\n<h3 id=\"fixture-system-architecture\">Fixture System Architecture</h3>\n<p>The fixture architecture centers on two core challenges: <strong>lifecycle management</strong> (when to create and destroy resources) and <strong>dependency resolution</strong> (how to satisfy a test&#39;s resource requirements). The system must track resources, cache them appropriately based on scope, ensure cleanup even when tests fail, and resolve complex dependency graphs where one fixture depends on another.</p>\n<blockquote>\n<p><strong>Mental Model: The Resource Pool Manager</strong></p>\n<p>Imagine a hotel with different room types (scopes). A &quot;function-scoped&quot; room is cleaned after every guest (test). A &quot;module-scoped&quot; suite is cleaned only after all guests from a particular group (test module) have left. A &quot;session-scoped&quot; banquet hall is set up once for the entire event (test run) and cleaned at the very end. The fixture registry is the hotel manager who tracks which rooms are occupied, when to clean them, and which guests need which amenities (dependencies).</p>\n</blockquote>\n<h4 id=\"core-components\">Core Components</h4>\n<p>The fixture system comprises three primary components working together:</p>\n<ol>\n<li><strong>Fixture Registry</strong>: A central catalog of all available fixtures, mapping fixture names to their factory functions and metadata. It&#39;s the system&#39;s &quot;phone book&quot; for resource lookup.</li>\n<li><strong>Fixture Cache</strong>: A scope-aware storage that holds created fixture instances, preventing expensive re-creation. It&#39;s the system&#39;s &quot;warehouse&quot; for ready-to-use resources.</li>\n<li><strong>Fixture Lifecycle Manager</strong>: The orchestration engine that sequences creation, caching, injection, and teardown based on scope boundaries. It&#39;s the system&#39;s &quot;conductor&quot; for resource lifecycle events.</li>\n</ol>\n<p>These components collaborate through a well-defined data flow: when a test requests a fixture, the lifecycle manager checks the cache; if not present, it creates the fixture (resolving its dependencies recursively), stores it in the cache, injects it into the test, and schedules its eventual teardown at the appropriate scope boundary.</p>\n<h4 id=\"data-structures\">Data Structures</h4>\n<p>The fixture system extends our existing data model with several new types:</p>\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Fixture</code></td>\n<td>Class/Struct</td>\n<td>Metadata and factory for a single fixture.</td>\n</tr>\n<tr>\n<td><code>FixtureScope</code></td>\n<td>Enum</td>\n<td>Defines the lifetime of a fixture instance. Values: <code>FUNCTION</code>, <code>CLASS</code>, <code>MODULE</code>, <code>SESSION</code>.</td>\n</tr>\n<tr>\n<td><code>FixtureRequest</code></td>\n<td>Class/Struct</td>\n<td>A request for a specific fixture instance, containing the context needed to create/cache it.</td>\n</tr>\n<tr>\n<td><code>FixtureCacheKey</code></td>\n<td>Tuple</td>\n<td>A unique identifier for a cached fixture instance, typically <code>(fixture_name, scope, scope_id)</code></td>\n</tr>\n</tbody></table>\n<p><strong>Fixture Definition Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>name</code></td>\n<td><code>str</code></td>\n<td>The unique identifier used to request this fixture (matches test parameter name).</td>\n</tr>\n<tr>\n<td><code>func</code></td>\n<td><code>Callable</code></td>\n<td>The factory function that creates the fixture value. May be a generator for teardown.</td>\n</tr>\n<tr>\n<td><code>scope</code></td>\n<td><code>FixtureScope</code></td>\n<td>The lifetime scope: <code>FUNCTION</code> (per test), <code>CLASS</code> (per test class), <code>MODULE</code> (per module), <code>SESSION</code> (entire run).</td>\n</tr>\n<tr>\n<td><code>dependencies</code></td>\n<td><code>List[str]</code></td>\n<td>Names of other fixtures this fixture requires, resolved before this fixture&#39;s <code>func</code> is called.</td>\n</tr>\n</tbody></table>\n<p><strong>FixtureRequest Definition Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>fixture_name</code></td>\n<td><code>str</code></td>\n<td>Name of the fixture being requested.</td>\n</tr>\n<tr>\n<td><code>scope</code></td>\n<td><code>FixtureScope</code></td>\n<td>The scope at which this fixture is being requested.</td>\n</tr>\n<tr>\n<td><code>test_case</code></td>\n<td><code>Optional[TestCase]</code></td>\n<td>The test case that triggered this request (for <code>FUNCTION</code> and <code>CLASS</code> scoping).</td>\n</tr>\n<tr>\n<td><code>cache_key</code></td>\n<td><code>Tuple</code></td>\n<td>Computed key used to store/retrieve the fixture from cache: <code>(fixture_name, scope, scope_id)</code> where <code>scope_id</code> is derived from the test/module.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Generator-Based Fixtures for Cleanup</strong></p>\n<p><strong>Context</strong>: Fixtures often represent resources requiring cleanup (database connections, temporary files). We need a reliable mechanism to execute teardown logic regardless of test outcome.</p>\n<p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Separate <code>setup()</code>/<code>teardown()</code> functions</strong>: Require users to define two separate functions, manually paired by name.</li>\n<li><strong>Class-based fixtures with <code>__enter__</code>/<code>__exit__</code></strong>: Use context managers; requires wrapping fixture usage in <code>with</code> statements.</li>\n<li><strong>Generator functions</strong>: Fixture function <code>yield</code>s the value; code after <code>yield</code> is teardown.</li>\n</ol>\n<p><strong>Decision</strong>: Use generator functions where the fixture function contains a <code>yield</code> statement.</p>\n<p><strong>Rationale</strong>: Generators provide a natural, linear flow: setup code before <code>yield</code>, teardown after. The framework can guarantee teardown execution by exhausting the generator (calling <code>next()</code> after the test). This pattern is intuitive (users write setup and teardown in one place) and widely adopted (pytest uses it).</p>\n<p><strong>Consequences</strong>:</p>\n<ul>\n<li><strong>Enables</strong>: Automatic cleanup even if test fails; simple syntax.</li>\n<li><strong>Trade-offs</strong>: Slight complexity in framework to detect and handle generators; users must remember to <code>yield</code> exactly once.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Separate functions</td>\n<td>Simple to implement; explicit.</td>\n<td>Error-prone pairing; twice the boilerplate.</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Context managers</td>\n<td>Python-native pattern; strong guarantees.</td>\n<td>Requires <code>with</code> in tests; breaks dependency injection.</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Generator functions</td>\n<td>Single function; automatic cleanup; intuitive.</td>\n<td>Framework must handle generator lifecycle.</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"lifecycle-management-by-scope\">Lifecycle Management by Scope</h4>\n<p>Each scope defines a different caching and cleanup strategy. The lifecycle manager tracks &quot;scope boundaries&quot; (when all tests of a certain scope complete) to trigger teardown.</p>\n<p><strong>Scope Behavior Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scope</th>\n<th>Creation Trigger</th>\n<th>Cached Until</th>\n<th>Typical Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>FUNCTION</code></td>\n<td>Before each test that needs it.</td>\n<td>After that test completes.</td>\n<td>Fresh data for each test; no cross-test contamination.</td>\n</tr>\n<tr>\n<td><code>CLASS</code></td>\n<td>Before first test in a test class that needs it.</td>\n<td>After last test in that class completes.</td>\n<td>Shared setup for all tests in a class (e.g., browser instance).</td>\n</tr>\n<tr>\n<td><code>MODULE</code></td>\n<td>Before first test in a module that needs it.</td>\n<td>After last test in that module completes.</td>\n<td>Expensive setup shared across a file (e.g., database schema).</td>\n</tr>\n<tr>\n<td><code>SESSION</code></td>\n<td>Once at start of test run.</td>\n<td>After all tests complete.</td>\n<td>Global resources (e.g., Docker container, API server).</td>\n</tr>\n</tbody></table>\n<p>The sequence diagram <code>![Fixture Scope Lifecycle Sequence](/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Ffixture-scope-seq.svg)</code> illustrates the timing difference between <code>FUNCTION</code> and <code>MODULE</code> scopes across three tests.</p>\n<p><strong>Algorithm: Fixture Resolution and Creation</strong></p>\n<p>When a test requires a fixture, the lifecycle manager executes this recursive algorithm:</p>\n<ol>\n<li><strong>Receive Request</strong>: The manager receives a <code>FixtureRequest</code> for a fixture name within a specific scope context.</li>\n<li><strong>Check Cache</strong>: Compute the <code>cache_key</code> for this request. If the fixture exists in the cache for this key, return the cached instance.</li>\n<li><strong>Resolve Dependencies</strong>: For each fixture name in <code>Fixture.dependencies</code>, recursively call this algorithm (starting at step 1) to obtain the dependency values.</li>\n<li><strong>Invoke Factory</strong>: Call the fixture&#39;s <code>func</code> with the resolved dependencies as arguments.</li>\n<li><strong>Handle Generator</strong>:\na. If <code>func</code> is a generator (contains <code>yield</code>), advance it once to get the fixture value. Store the generator object for later teardown.\nb. Otherwise, use the return value directly.</li>\n<li><strong>Cache Instance</strong>: Store the fixture value in the cache under its <code>cache_key</code>. For generator fixtures, also store the generator for later cleanup.</li>\n<li><strong>Return Value</strong>: Provide the fixture value to the requester (test or dependent fixture).</li>\n</ol>\n<p><strong>Algorithm: Scope Teardown</strong></p>\n<p>When a scope boundary is reached (e.g., after a test for <code>FUNCTION</code> scope, after a module&#39;s last test for <code>MODULE</code> scope), the lifecycle manager:</p>\n<ol>\n<li><strong>Identify Fixtures to Teardown</strong>: Find all cached fixtures where <code>scope</code> matches the ending boundary and <code>scope_id</code> matches the ending context.</li>\n<li><strong>Reverse Order</strong>: Process fixtures in reverse dependency order (dependents before their dependencies) to avoid tearing down a needed resource prematurely.</li>\n<li><strong>Execute Teardown</strong>:\na. For generator fixtures: call <code>next()</code> on the stored generator object (which executes code after the <code>yield</code>).\nb. For non-generator fixtures: if they have a <code>__close__</code> or similar, call it (optional).</li>\n<li><strong>Evict from Cache</strong>: Remove the fixture from the cache.</li>\n</ol>\n<h4 id=\"common-pitfalls\">Common Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Forgetting to Handle Generator Exhaustion</strong></p>\n<ul>\n<li><strong>Description</strong>: When a generator fixture yields, the framework must call <code>next()</code> again to execute teardown code. Forgetting this leaves resources open.</li>\n<li><strong>Why it&#39;s wrong</strong>: Database connections stay open, temporary files aren&#39;t deleted, causing resource leaks and potentially affecting other tests.</li>\n<li><strong>Fix</strong>: Always check if the fixture function is a generator (using <code>inspect.isgeneratorfunction</code>). When caching the fixture value, store the generator object and register it for teardown at the appropriate scope boundary.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Circular Dependencies</strong></p>\n<ul>\n<li><strong>Description</strong>: Fixture A depends on B, and B depends on A (directly or indirectly). The resolution algorithm enters infinite recursion.</li>\n<li><strong>Why it&#39;s wrong</strong>: The framework crashes with a recursion error or hangs, preventing any tests from running.</li>\n<li><strong>Fix</strong>: Detect cycles during fixture registration or resolution. Maintain a set of &quot;currently resolving&quot; fixtures; if a fixture is requested while already in this set, raise a clear error indicating the circular dependency path.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Scope Leak (Fixture Used Beyond Its Lifetime)</strong></p>\n<ul>\n<li><strong>Description</strong>: A test holds a reference to a fixture value after the fixture has been torn down (e.g., storing a module-scoped database connection in a global variable accessed by a later session-scoped test).</li>\n<li><strong>Why it&#39;s wrong</strong>: The test may operate on a closed/resource, causing cryptic errors far from the actual problem.</li>\n<li><strong>Fix</strong>: The framework cannot fully prevent this, but it can emit warnings when fixture values are used after their scope ends. Document clearly that fixtures should not be stored beyond the test that receives them.</li>\n</ul>\n<h3 id=\"dependency-injection-mechanism\">Dependency Injection Mechanism</h3>\n<p>Dependency injection is the magic that automatically provides fixtures to test functions. Instead of manually calling setup functions, tests simply declare what they need as parameters, and the framework fulfills those requests. This mechanism transforms test functions from isolated procedures into declarative specifications of their required context.</p>\n<blockquote>\n<p><strong>Mental Model: The Restaurant Order</strong></p>\n<p>A test function is like a customer ordering a meal. The parameters (<code>def test_checkout(shopping_cart, payment_gateway)</code>) are the order items. The fixture system is the kitchen and waitstaff that prepare and deliver those items. The customer doesn&#39;t need to know how the kitchen creates the <code>shopping_cart</code> or where the <code>payment_gateway</code> comes from—they just receive what they asked for.</p>\n</blockquote>\n<h4 id=\"parameter-inspection\">Parameter Inspection</h4>\n<p>The injection process begins when the framework inspects a test function&#39;s signature to determine its requirements.</p>\n<p><strong>Algorithm: Parameter Extraction</strong></p>\n<ol>\n<li><strong>Get Signature</strong>: Use <code>inspect.signature(test_func)</code> to obtain a <code>Signature</code> object.</li>\n<li><strong>Analyze Parameters</strong>: Iterate through <code>signature.parameters</code>. For each parameter:\na. <strong>Skip Special Parameters</strong>: Ignore <code>self</code> (for methods), <code>cls</code>, or parameters with special meanings (e.g., <code>*args</code>, <code>**kwargs</code> unless specifically supported).\nb. <strong>Record Fixture Name</strong>: The parameter&#39;s name is the fixture name to inject (e.g., <code>database</code>).</li>\n<li><strong>Store Requirements</strong>: Attach the list of fixture names to the <code>TestCase</code> object (in the <code>fixtures</code> field).</li>\n</ol>\n<p>This process occurs during <strong>test discovery</strong> (Milestone 1) when <code>TestCase</code> objects are created. The <code>TestCase</code> thus carries both <em>what</em> to run (<code>func</code>) and <em>what it needs</em> (<code>fixtures</code>).</p>\n<h4 id=\"injection-process\">Injection Process</h4>\n<p>During test execution, the runner requests the fixture lifecycle manager to provide values for each required fixture name. The manager executes the resolution algorithm described earlier, returning a dictionary of <code>{fixture_name: value}</code>. The runner then calls the test function with these values as keyword arguments.</p>\n<p><strong>Algorithm: Test Execution with Injection</strong></p>\n<ol>\n<li><strong>Pre-execution</strong>: For a given <code>TestCase</code>, the runner extracts its <code>fixtures</code> list.</li>\n<li><strong>Request Fixtures</strong>: For each fixture name, create a <code>FixtureRequest</code> with appropriate scope (based on fixture definition and test context) and send to the lifecycle manager.</li>\n<li><strong>Receive Values</strong>: Obtain a dictionary mapping fixture names to their instantiated values.</li>\n<li><strong>Call Test Function</strong>: Execute <code>test_func(**fixture_values)</code>.</li>\n<li><strong>Post-execution</strong>: After test completes (success or failure), notify the lifecycle manager that the test&#39;s scope (e.g., <code>FUNCTION</code>) is ending, triggering teardown of appropriate fixtures.</li>\n</ol>\n<p>This process is illustrated in the sequence diagram <code>![Test Execution Sequence](/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Ftest-execution-seq.svg)</code>.</p>\n<blockquote>\n<p><strong>Architecture Decision: Implicit vs. Explicit Fixture Registration</strong></p>\n<p><strong>Context</strong>: How does the framework know about available fixtures? Should users explicitly register them, or should the framework discover them automatically?</p>\n<p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Explicit Registration</strong>: Users call a decorator or function to register fixtures in a central location.</li>\n<li><strong>Implicit Discovery</strong>: The framework scans modules for functions decorated with <code>@fixture</code> (or named with a convention like <code>fixture_*</code>).</li>\n</ol>\n<p><strong>Decision</strong>: Use implicit discovery via a decorator (e.g., <code>@apollo.fixture</code>).</p>\n<p><strong>Rationale</strong>: Implicit discovery aligns with the &quot;convention-over-configuration&quot; philosophy of the overall framework, reducing boilerplate. A decorator is lightweight, clearly marks fixture functions, and can attach metadata (like <code>scope</code>). It also mirrors successful patterns in pytest.</p>\n<p><strong>Consequences</strong>:</p>\n<ul>\n<li><strong>Enables</strong>: Clean, declarative fixture definition near where it&#39;s used.</li>\n<li><strong>Trade-offs</strong>: Requires module imports during discovery to find decorators; slightly more complex discovery logic.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Explicit Registration</td>\n<td>Clear central catalog; no import side effects.</td>\n<td>More boilerplate; fixtures distant from usage.</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Implicit Discovery (decorator)</td>\n<td>Minimal boilerplate; fixtures colocated with tests.</td>\n<td>Requires importing modules; decorator must be processed.</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"integration-with-setupteardown-hooks\">Integration with Setup/Teardown Hooks</h4>\n<p>The fixture system coexists with classic xUnit-style <code>setUp</code>/<code>tearDown</code> methods. They serve different purposes: fixtures are for <strong>resource provisioning</strong> (providing values to tests), while hooks are for <strong>environment setup</strong> (actions performed before/after tests). The framework executes them in a defined order:</p>\n<ol>\n<li><strong>Module-level fixtures</strong> (scope=<code>MODULE</code>) setup</li>\n<li><strong>Class-level fixtures</strong> (scope=<code>CLASS</code>) setup</li>\n<li><strong><code>setUpClass</code></strong> method (if present)</li>\n<li><strong>Function-level fixtures</strong> (scope=<code>FUNCTION</code>) setup</li>\n<li><strong><code>setUp</code></strong> method (if present)</li>\n<li><strong>Test execution</strong> (with injected fixtures)</li>\n<li><strong><code>tearDown</code></strong> method (if present)</li>\n<li><strong>Function-level fixtures</strong> teardown</li>\n<li><strong><code>tearDownClass</code></strong> method (if present)</li>\n<li><strong>Class-level fixtures</strong> teardown</li>\n<li><strong>Module-level fixtures</strong> teardown</li>\n</ol>\n<p>This order ensures that fixtures are available during <code>setUp</code> and <code>tearDown</code> if needed, and that cleanup happens in reverse order of setup.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Fixture Discovery</td>\n<td>Scan module <code>__dict__</code> for functions with <code>_is_fixture</code> attribute set by decorator.</td>\n<td>Use <code>inspect.getmembers()</code> with predicate checking for decorator metadata.</td>\n</tr>\n<tr>\n<td>Generator Detection</td>\n<td><code>inspect.isgeneratorfunction(func)</code> before calling.</td>\n<td>Also handle async generators (<code>inspect.isasyncgenfunction</code>).</td>\n</tr>\n<tr>\n<td>Cache Storage</td>\n<td>Nested dictionaries: <code>cache[scope][scope_id][fixture_name] = value</code>.</td>\n<td>Use weak references for cached values to avoid preventing garbage collection.</td>\n</tr>\n<tr>\n<td>Scope Identification</td>\n<td>For <code>MODULE</code>: use <code>test_case.file_path</code>. For <code>CLASS</code>: use class name.</td>\n<td>Create a hierarchical scope tree to manage nested scopes (e.g., session → module → class → function).</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n├── __init__.py              # Public API: fixture decorator, hooks\n├── runner.py                # Enhanced runner with fixture support\n├── fixtures/                # Fixture subsystem\n│   ├── __init__.py\n│   ├── registry.py          # FixtureRegistry: stores fixture definitions\n│   ├── cache.py             # FixtureCache: scope-aware caching\n│   ├── lifecycle.py         # FixtureLifecycleManager: creation/teardown orchestration\n│   └── scopemanager.py      # ScopeManager: tracks scope boundaries\n├── discovery.py             # Enhanced to detect fixtures and parameter requirements\n└── hooks.py                 # setUp/tearDown hook execution</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<p>Here&#39;s a complete, ready-to-use implementation of the fixture decorator and registry. Place this in <code>apollo/fixtures/registry.py</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> inspect</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Callable, Dict, List, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FixtureScope</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Lifetime scope for a fixture.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FUNCTION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"function\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CLASS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"class\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MODULE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"module\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SESSION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"session\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Fixture</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Metadata and factory for a fixture.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    func: Callable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scope: FixtureScope </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FixtureScope.</span><span style=\"color:#79B8FF\">FUNCTION</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FixtureRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Central registry for fixture definitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._fixtures: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Fixture] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._discovered </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register</span><span style=\"color:#E1E4E8\">(self, fixture: Fixture) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a fixture definition.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> fixture.name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._fixtures:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Fixture '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">fixture.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' already registered\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._fixtures[fixture.name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fixture</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Fixture:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get a fixture definition by name.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._fixtures:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> KeyError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Fixture '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._fixtures[name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scan_module</span><span style=\"color:#E1E4E8\">(self, module) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scan a module for fixture decorators and register them.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attr_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> dir</span><span style=\"color:#E1E4E8\">(module):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            attr </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(module, attr_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> callable</span><span style=\"color:#E1E4E8\">(attr) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(attr, </span><span style=\"color:#9ECBFF\">'_is_apollo_fixture'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Extract metadata stored by the decorator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                fixture_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(attr, </span><span style=\"color:#9ECBFF\">'_fixture_name'</span><span style=\"color:#E1E4E8\">, attr_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                scope </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(attr, </span><span style=\"color:#9ECBFF\">'_fixture_scope'</span><span style=\"color:#E1E4E8\">, FixtureScope.</span><span style=\"color:#79B8FF\">FUNCTION</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                dependencies </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(attr, </span><span style=\"color:#9ECBFF\">'_fixture_dependencies'</span><span style=\"color:#E1E4E8\">, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                fixture </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Fixture(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">fixture_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    func</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">attr,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    scope</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">scope,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    dependencies</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">dependencies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.register(fixture)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Public decorator API</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> fixture</span><span style=\"color:#E1E4E8\">(func</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">, scope: Union[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, FixtureScope] </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"function\"</span><span style=\"color:#E1E4E8\">, name: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Decorator to mark a function as a fixture.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        func: The fixture function (can be a generator).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        scope: One of \"function\", \"class\", \"module\", \"session\".</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        name: Optional name for the fixture (defaults to function name).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(scope, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scope </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FixtureScope(scope.lower())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> decorator</span><span style=\"color:#E1E4E8\">(f):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        f._is_apollo_fixture </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        f._fixture_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> f.</span><span style=\"color:#79B8FF\">__name__</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        f._fixture_scope </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scope</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Dependency extraction happens later during registration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        f._fixture_dependencies </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> f</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> func </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> decorator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> decorator(func)</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code:</strong></p>\n<p>Here&#39;s the skeleton for the lifecycle manager, the heart of the fixture system. Place in <code>apollo/fixtures/lifecycle.py</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> inspect</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, Generator, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .registry </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> FixtureRegistry, FixtureScope, Fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FixtureLifecycleManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates fixture creation, caching, and teardown.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, registry: FixtureRegistry):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> registry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cache: Dict[Tuple, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># (name, scope, scope_id) -> value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._generators: Dict[Tuple, Generator] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># For teardown</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._active_requests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()  </span><span style=\"color:#6A737D\"># For cycle detection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_fixture_value</span><span style=\"color:#E1E4E8\">(self, fixture_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, test_case: Optional[TestCase] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Get the value of a fixture for a given test context.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            fixture_name: Name of the fixture to resolve.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            test_case: The test case requesting the fixture (for scope context).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            The fixture value.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            RuntimeError: If circular dependency detected.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            KeyError: If fixture not found.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine scope context from test_case and fixture definition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Get fixture definition from registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Determine actual scope (fixture's scope, but test_case provides context)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Compute scope_id based on scope (e.g., module path for MODULE, test class name for CLASS)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create cache key: (fixture_name, scope, scope_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check cache; if found, return cached value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for circular dependency: if fixture_name in self._active_requests,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   raise RuntimeError with the dependency path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add fixture_name to self._active_requests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Resolve dependencies recursively:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   For each dep_name in fixture.dependencies:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #       dep_value = self.get_fixture_value(dep_name, test_case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Collect dep_values in a dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Invoke fixture factory with dependencies as kwargs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If inspect.isgeneratorfunction(fixture.func): handle specially</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call fixture.func(**dep_values)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If generator: get first yield value; store generator in self._generators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Else: use return value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Store value in cache with cache key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Remove fixture_name from self._active_requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return the fixture value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> teardown_scope</span><span style=\"color:#E1E4E8\">(self, scope: FixtureScope, scope_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Teardown all fixtures of a given scope and scope_id.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            scope: The scope level to teardown.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            scope_id: The specific context identifier.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Collect all cache keys for this (scope, scope_id)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Note: Need to iterate through self._cache and self._generators</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each fixture in reverse dependency order (implement dependency tracking):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If fixture has generator: call next(generator, None) to execute teardown code</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Remove from self._generators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Remove from self._cache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Optional: call close() or similar on value if it exists</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>inspect.signature(func).parameters</code> to get parameter names and defaults.</li>\n<li>Use <code>inspect.isgeneratorfunction(func)</code> to detect generator fixtures before calling them.</li>\n<li>Use <code>functools.lru_cache</code> on scope ID computation functions for performance.</li>\n<li>For safe generator teardown: <code>try: next(gen) \\ except StopIteration: pass</code>.</li>\n<li>Store fixture dependencies by analyzing the fixture function&#39;s parameters during registration (enhance the decorator to extract them).</li>\n</ul>\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the fixture system, verify with this test structure:</p>\n<p>Create <code>test_fixtures.py</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> apollo</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@apollo.fixture</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">scope</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"module\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> database</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Setting up database\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    yield</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"connected\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Tearing down database\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@apollo.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> user</span><span style=\"color:#E1E4E8\">(database):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Alice\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"db\"</span><span style=\"color:#E1E4E8\">: database}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_user_creation</span><span style=\"color:#E1E4E8\">(user):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> user[</span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"Alice\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> user[</span><span style=\"color:#9ECBFF\">\"db\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"connected\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_database_available</span><span style=\"color:#E1E4E8\">(database):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> database[</span><span style=\"color:#9ECBFF\">\"connected\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p>Run with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo</span><span style=\"color:#9ECBFF\"> test_fixtures.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_fixtures.py::test_user_creation\nSetting up database\nPASSED\ntest_fixtures.py::test_database_available\nPASSED\nTearing down database\n\n2 passed in 0.01s</code></pre></div>\n\n<p><strong>Verification:</strong></p>\n<ul>\n<li>Database setup appears only once (module scope).</li>\n<li>Both tests pass and receive their fixtures.</li>\n<li>Database teardown happens after both tests complete.</li>\n<li>Try adding a third test without the fixture parameter to ensure non-fixture tests still work.</li>\n</ul>\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Fixture &#39;x&#39; not found&quot; error</td>\n<td>Fixture not registered or name mismatch.</td>\n<td>Check decorator is applied; check fixture name vs. test parameter name.</td>\n<td>Ensure <code>@apollo.fixture</code> decorator is used; parameter name matches fixture function name.</td>\n</tr>\n<tr>\n<td>Database not cleaned up after tests</td>\n<td>Generator teardown not called.</td>\n<td>Add print statements before/after yield; see if teardown prints appear.</td>\n<td>Ensure <code>teardown_scope</code> is called at appropriate times and generators are advanced.</td>\n</tr>\n<tr>\n<td>Tests hang during fixture resolution</td>\n<td>Circular dependency.</td>\n<td>Print <code>self._active_requests</code> during resolution.</td>\n<td>Implement cycle detection; rework fixture dependencies to remove cycle.</td>\n</tr>\n<tr>\n<td>Fixture created multiple times per module</td>\n<td>Wrong <code>scope_id</code> computation.</td>\n<td>Print <code>scope_id</code> for each fixture request.</td>\n<td>Ensure <code>scope_id</code> for MODULE scope uses <code>test_case.file_path</code> consistently.</td>\n</tr>\n<tr>\n<td><code>setUp</code> method cannot access fixtures</td>\n<td>Fixtures injected after <code>setUp</code> runs.</td>\n<td>Check execution order.</td>\n<td>Document that fixtures are injected into test method, not <code>setUp</code>. Use class-level fixtures instead.</td>\n</tr>\n</tbody></table>\n<h2 id=\"8-component-design-reporting-amp-cli-milestone-4\">8. Component Design: Reporting &amp; CLI (Milestone 4)</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 (Reporting &amp; CLI)</p>\n</blockquote>\n<p>The <strong>Reporting &amp; CLI</strong> subsystem transforms the test framework from a collection of internal components into a usable tool that developers interact with daily. This subsystem provides the <strong>control panel</strong> (CLI) for configuring test runs and the <strong>performance review</strong> (Reporter) that delivers actionable feedback on test outcomes. While earlier milestones focused on the framework&#39;s internal machinery—finding tests, executing them, verifying conditions, and managing resources—this milestone delivers the user-facing interface that makes the framework practical and professional. The CLI must be intuitive yet powerful, accepting various patterns and filters, while the Reporter must produce clear human-readable output and machine-readable formats compatible with CI/CD pipelines.</p>\n<h3 id=\"mental-model-the-control-panel-and-performance-review\">Mental Model: The Control Panel and Performance Review</h3>\n<p>Imagine the test framework as a sophisticated laboratory testing facility. The <strong>CLI Parser</strong> is the <strong>control panel</strong> where technicians input experiment parameters: which samples to test (file patterns), what specific tests to run (name filters), how detailed the report should be (verbosity), and where to send the results (output formats). The panel has dials, switches, and input fields that translate human intentions into precise machine instructions.</p>\n<p>The <strong>Reporter</strong> is the <strong>performance review</strong> system that analyzes each test&#39;s execution. It acts as both a quality inspector who examines each test result in detail and a statistician who compiles aggregate metrics. For human consumption, it produces a clear summary report with pass/fail indicators, execution times, and helpful diagnostic information. For machine integration, it generates standardized <strong>JUnit XML</strong> reports—think of these as laboratory certification documents that can be filed with regulatory systems (CI servers). This dual-output capability ensures the framework works both for interactive development and automated build pipelines.</p>\n<h3 id=\"cli-parser\">CLI Parser</h3>\n<p>The <strong>CLI Parser</strong> serves as the framework&#39;s primary entry point, translating command-line arguments into a structured <code>Configuration</code> object that drives the entire test execution pipeline. Its design follows the principle of <strong>convention-over-configuration</strong> by providing sensible defaults while allowing extensive customization through flags.</p>\n<h4 id=\"architecture-decision-command-line-interface-design\">Architecture Decision: Command-Line Interface Design</h4>\n<blockquote>\n<p><strong>Decision: argparse-Based CLI with Rich Pattern Matching</strong></p>\n<ul>\n<li><strong>Context</strong>: The framework needs to accept various input patterns (files, directories, test names) with flexible filtering while maintaining compatibility with shell globbing and developer expectations from tools like pytest.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Simple positional arguments only</strong> – Accept only file/directory paths without flags</li>\n<li><strong>argparse with custom pattern resolution</strong> – Use Python&#39;s standard <code>argparse</code> with our pattern resolution logic</li>\n<li><strong>Click library</strong> – Use a third-party CLI framework for more sophisticated features</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use <strong>argparse with custom pattern resolution</strong> (Option 2)</li>\n<li><strong>Rationale</strong>: <code>argparse</code> is Python&#39;s standard library solution, requiring no external dependencies. It provides sufficient flexibility for our needs: flag parsing, help generation, and type validation. The custom pattern resolution layer allows us to implement the exact file matching semantics needed (supporting glob patterns, recursive directory scanning, and Python module name conversion) without being constrained by argparse&#39;s limited path handling.</li>\n<li><strong>Consequences</strong>: We avoid third-party dependencies, keep the framework lightweight, and maintain full control over pattern resolution logic. The downside is implementing pattern resolution ourselves, but this is straightforward and aligns with the educational goals of understanding how test discovery works under the hood.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple positional arguments</td>\n<td>Easy to implement, no flags to learn</td>\n<td>Limited expressiveness, can&#39;t combine filters</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>argparse + custom resolution</td>\n<td>Standard library, full control, extensible</td>\n<td>Need to implement pattern logic ourselves</td>\n<td>✅</td>\n</tr>\n<tr>\n<td>Click library</td>\n<td>Rich features, beautiful help, decorator-based</td>\n<td>External dependency, overkill for our needs</td>\n<td>❌</td>\n</tr>\n</tbody></table>\n<h4 id=\"core-responsibilities\">Core Responsibilities</h4>\n<p>The CLI Parser has three primary responsibilities:</p>\n<ol>\n<li><strong>Argument Parsing</strong>: Parse command-line arguments into a structured <code>Configuration</code> object with validated fields</li>\n<li><strong>Pattern Resolution</strong>: Convert file patterns (like <code>tests/*.py</code> or <code>**/test_*.py</code>) into concrete filesystem paths</li>\n<li><strong>Default Application</strong>: Apply sensible defaults when arguments are omitted (e.g., default to current directory)</li>\n</ol>\n<h4 id=\"configuration-data-structure\">Configuration Data Structure</h4>\n<p>The <code>Configuration</code> object (defined in the Data Model section) serves as the complete specification for a test run. The CLI Parser&#39;s job is to populate this object from command-line input:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Default Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>file_patterns</code></td>\n<td><code>list[str]</code></td>\n<td>File/directory patterns to search for tests</td>\n<td><code>[&quot;.&quot;]</code> (current directory)</td>\n</tr>\n<tr>\n<td><code>start_dir</code></td>\n<td><code>Path</code></td>\n<td>Base directory for relative pattern resolution</td>\n<td>Current working directory</td>\n</tr>\n<tr>\n<td><code>test_name_filters</code></td>\n<td><code>list[str]</code></td>\n<td>Substring patterns to match against test names (nodeids)</td>\n<td><code>[]</code> (all tests)</td>\n</tr>\n<tr>\n<td><code>verbose</code></td>\n<td><code>bool</code></td>\n<td>Show detailed output including passing tests</td>\n<td><code>False</code></td>\n</tr>\n<tr>\n<td><code>quiet</code></td>\n<td><code>bool</code></td>\n<td>Show only summary, suppress individual test output</td>\n<td><code>False</code></td>\n</tr>\n<tr>\n<td><code>output_format</code></td>\n<td><code>str</code></td>\n<td>Output format: &quot;console&quot;, &quot;junit-xml&quot;, or &quot;both&quot;</td>\n<td><code>&quot;console&quot;</code></td>\n</tr>\n<tr>\n<td><code>parallel</code></td>\n<td><code>bool</code></td>\n<td>Enable parallel test execution</td>\n<td><code>False</code></td>\n</tr>\n<tr>\n<td><code>max_workers</code></td>\n<td><code>Optional[int]</code></td>\n<td>Maximum number of parallel workers (None = auto)</td>\n<td><code>None</code></td>\n</tr>\n<tr>\n<td><code>show_durations</code></td>\n<td><code>bool</code></td>\n<td>Show execution time for each test</td>\n<td><code>False</code></td>\n</tr>\n<tr>\n<td><code>junit_xml_path</code></td>\n<td><code>Optional[Path]</code></td>\n<td>Path to write JUnit XML report</td>\n<td><code>None</code> (stdout if &quot;junit-xml&quot; format)</td>\n</tr>\n</tbody></table>\n<h4 id=\"command-line-interface-specification\">Command-Line Interface Specification</h4>\n<p>The CLI supports the following flags and arguments:</p>\n<table>\n<thead>\n<tr>\n<th>Flag/Argument</th>\n<th>Description</th>\n<th>Example</th>\n<th>Maps to Configuration Field</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>positional <code>patterns</code></td>\n<td>File/directory/glob patterns to search</td>\n<td><code>tests/ test_*.py</code></td>\n<td><code>file_patterns</code></td>\n</tr>\n<tr>\n<td><code>-k</code> <code>--filter</code></td>\n<td>Filter tests by name substring</td>\n<td><code>-k &quot;test_login&quot;</code></td>\n<td><code>test_name_filters</code></td>\n</tr>\n<tr>\n<td><code>-v</code> <code>--verbose</code></td>\n<td>Verbose output</td>\n<td><code>-v</code></td>\n<td><code>verbose=True</code></td>\n</tr>\n<tr>\n<td><code>-q</code> <code>--quiet</code></td>\n<td>Quiet mode (summary only)</td>\n<td><code>-q</code></td>\n<td><code>quiet=True</code></td>\n</tr>\n<tr>\n<td><code>--junit-xml</code></td>\n<td>Generate JUnit XML output</td>\n<td><code>--junit-xml=results.xml</code></td>\n<td><code>output_format=&quot;junit-xml&quot;</code>, <code>junit_xml_path=Path(...)</code></td>\n</tr>\n<tr>\n<td><code>--parallel</code></td>\n<td>Run tests in parallel</td>\n<td><code>--parallel</code></td>\n<td><code>parallel=True</code></td>\n</tr>\n<tr>\n<td><code>-n</code> <code>--num-workers</code></td>\n<td>Number of parallel workers</td>\n<td><code>-n 4</code></td>\n<td><code>max_workers=4</code></td>\n</tr>\n<tr>\n<td><code>--durations</code></td>\n<td>Show test execution times</td>\n<td><code>--durations</code></td>\n<td><code>show_durations=True</code></td>\n</tr>\n<tr>\n<td><code>--collect-only</code></td>\n<td>Collect tests but don&#39;t execute</td>\n<td>N/A</td>\n<td>Special mode (not in Configuration)</td>\n</tr>\n<tr>\n<td><code>--version</code></td>\n<td>Show framework version</td>\n<td><code>--version</code></td>\n<td>N/A</td>\n</tr>\n<tr>\n<td><code>-h</code> <code>--help</code></td>\n<td>Show help message</td>\n<td><code>-h</code></td>\n<td>N/A</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The <code>-k</code> filter uses substring matching rather than regular expressions initially, following the principle of progressive disclosure of complexity. Advanced regex matching could be added later as an extension.</p>\n</blockquote>\n<h4 id=\"algorithm-parsing-and-configuration-building\">Algorithm: Parsing and Configuration Building</h4>\n<p>When a user runs <code>python -m apollo tests/ -v -k &quot;login&quot;</code>, the following sequence occurs:</p>\n<ol>\n<li><p><strong>Raw Argument Parsing</strong>: The <code>parse_cli_args()</code> function receives <code>[&quot;tests/&quot;, &quot;-v&quot;, &quot;-k&quot;, &quot;login&quot;]</code> and uses <code>argparse</code> to:</p>\n<ul>\n<li>Validate flag combinations (e.g., <code>-v</code> and <code>-q</code> are mutually exclusive)</li>\n<li>Convert string values to appropriate types</li>\n<li>Handle <code>--help</code> and <code>--version</code> specially (print and exit)</li>\n</ul>\n</li>\n<li><p><strong>Pattern Resolution</strong>: For each pattern in <code>file_patterns</code>:</p>\n<ul>\n<li>If pattern is a directory (or <code>.</code>), recursively find all <code>*.py</code> files</li>\n<li>If pattern contains <code>*</code> or <code>**</code>, expand using <code>glob.glob()</code> with <code>recursive=True</code></li>\n<li>Convert relative paths to absolute relative to <code>start_dir</code></li>\n<li>Filter out non-Python files and directories</li>\n</ul>\n</li>\n<li><p><strong>Configuration Assembly</strong>: Create and return a <code>Configuration</code> object with:</p>\n<ul>\n<li>Resolved file patterns</li>\n<li>Applied defaults for unspecified fields</li>\n<li>Validated constraints (e.g., <code>max_workers</code> must be positive if specified)</li>\n</ul>\n</li>\n<li><p><strong>Special Mode Handling</strong>: If <code>--collect-only</code> is specified, the framework will discover tests, display them, and exit without execution.</p>\n</li>\n</ol>\n<h4 id=\"common-pitfalls\">Common Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Inconsistent Pattern Resolution Across Platforms</strong></p>\n<ul>\n<li><strong>Description</strong>: Using naive string matching for glob patterns that behaves differently on Windows vs Linux due to path separator differences (<code>\\</code> vs <code>/</code>).</li>\n<li><strong>Why it&#39;s wrong</strong>: Tests might be discovered on one OS but not another, causing CI failures.</li>\n<li><strong>How to fix</strong>: Use <code>pathlib.Path</code> objects throughout and <code>pathlib.Path.rglob()</code> for recursive pattern matching, which handles platform differences automatically.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Overly Permissive Default Patterns</strong></p>\n<ul>\n<li><strong>Description</strong>: Defaulting to <code>**/*.py</code> might scan virtual environments, build directories, or other non-test code.</li>\n<li><strong>Why it&#39;s wrong</strong>: Slow discovery, potential import errors from non-module files, and accidental test collection.</li>\n<li><strong>How to fix</strong>: Default to <code>.</code> (current directory) only, and let users explicitly specify broader patterns. Implement smart exclusion of common non-test directories (like <code>__pycache__</code>, <code>.git</code>, <code>venv/*</code>) in the discovery phase.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Missing Exit Code on Help/Version</strong></p>\n<ul>\n<li><strong>Description</strong>: Forgetting to exit after printing help or version information, causing the framework to proceed with test discovery.</li>\n<li><strong>Why it&#39;s wrong</strong>: User expects immediate exit; instead they see unexpected test output.</li>\n<li><strong>How to fix</strong>: Call <code>sys.exit(0)</code> immediately after printing help or version information in <code>parse_cli_args()</code>.</li>\n</ul>\n<h3 id=\"reporter\">Reporter</h3>\n<p>The <strong>Reporter</strong> component transforms raw <code>TestResult</code> objects into human-readable summaries and machine-readable reports. It serves as the framework&#39;s communication channel back to the user, providing both immediate feedback during development and structured data for continuous integration systems.</p>\n<h4 id=\"architecture-decision-dual-format-reporting-strategy\">Architecture Decision: Dual-Format Reporting Strategy</h4>\n<blockquote>\n<p><strong>Decision: Separate Formatters with Shared Statistics Calculation</strong></p>\n<ul>\n<li><strong>Context</strong>: The framework needs to output both human-friendly console reports and machine-readable JUnit XML, each with different formatting requirements but sharing the same underlying statistics.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Monolithic reporter with format branches</strong> – Single class with <code>format_console()</code>, <code>format_junit()</code> methods</li>\n<li><strong>Strategy pattern with separate formatters</strong> – Base <code>Formatter</code> interface with <code>ConsoleFormatter</code> and <code>JUnitFormatter</code> implementations</li>\n<li><strong>Visitor pattern over TestResults</strong> – Formatters visit each result to build output incrementally</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use <strong>Strategy pattern with separate formatters</strong> (Option 2)</li>\n<li><strong>Rationale</strong>: The Strategy pattern cleanly separates concerns—each formatter focuses entirely on its output format without being polluted by unrelated logic. The shared statistics calculation can be extracted to a separate <code>StatisticsCollector</code> that both formatters use. This design is more maintainable and extensible than branching conditionals and avoids the complexity overhead of the Visitor pattern for our relatively simple data structure.</li>\n<li><strong>Consequences</strong>: We get clean, testable formatter classes that can be extended independently (e.g., adding a JSON formatter later). The trade-off is slightly more boilerplate code to wire up the formatters.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Monolithic with branches</td>\n<td>Simple, all logic in one place</td>\n<td>Hard to test, violates SRP, hard to extend</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Strategy pattern</td>\n<td>Clean separation, extensible, testable</td>\n<td>More classes to manage</td>\n<td>✅</td>\n</tr>\n<tr>\n<td>Visitor pattern</td>\n<td>Elegant for complex hierarchies, open/closed</td>\n<td>Over-engineered for our needs, more complex</td>\n<td>❌</td>\n</tr>\n</tbody></table>\n<h4 id=\"core-responsibilities\">Core Responsibilities</h4>\n<p>The Reporter has four primary responsibilities:</p>\n<ol>\n<li><strong>Statistics Calculation</strong>: Aggregate test results to compute totals, success rates, and timing summaries</li>\n<li><strong>Console Formatting</strong>: Generate human-readable output with colors, indentation, and appropriate verbosity levels</li>\n<li><strong>JUnit XML Generation</strong>: Produce standardized XML reports compatible with CI/CD tools</li>\n<li><strong>Output Routing</strong>: Write reports to appropriate destinations (stdout, files, or both)</li>\n</ol>\n<h4 id=\"reporter-internal-architecture\">Reporter Internal Architecture</h4>\n<p>The Reporter comprises several collaborating classes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Reporter (Coordinator)\n├── StatisticsCollector (accumulates results)\n├── ConsoleFormatter (human output)\n├── JUnitFormatter (XML output)\n└── OutputWriter (handles file/stdout)</code></pre></div>\n\n<h5 id=\"statisticscollector\">StatisticsCollector</h5>\n<p>The <code>StatisticsCollector</code> processes a stream of <code>TestResult</code> objects and maintains running statistics:</p>\n<table>\n<thead>\n<tr>\n<th>Statistic</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>total</code></td>\n<td><code>int</code></td>\n<td>Total tests discovered</td>\n</tr>\n<tr>\n<td><code>passed</code></td>\n<td><code>int</code></td>\n<td>Tests with status <code>PASSED</code></td>\n</tr>\n<tr>\n<td><code>failed</code></td>\n<td><code>int</code></td>\n<td>Tests with status <code>FAILED</code> (assertion failures)</td>\n</tr>\n<tr>\n<td><code>errored</code></td>\n<td><code>int</code></td>\n<td>Tests with status <code>ERRORED</code> (unexpected exceptions)</td>\n</tr>\n<tr>\n<td><code>skipped</code></td>\n<td><code>int</code></td>\n<td>Tests with status <code>SKIPPED</code></td>\n</tr>\n<tr>\n<td><code>total_time</code></td>\n<td><code>float</code></td>\n<td>Sum of all test durations in seconds</td>\n</tr>\n<tr>\n<td><code>start_time</code></td>\n<td><code>datetime</code></td>\n<td>When test run began</td>\n</tr>\n<tr>\n<td><code>end_time</code></td>\n<td><code>datetime</code></td>\n<td>When test run completed</td>\n</tr>\n<tr>\n<td><code>by_module</code></td>\n<td><code>Dict[str, ModuleStats]</code></td>\n<td>Statistics grouped by module</td>\n</tr>\n<tr>\n<td><code>slowest_tests</code></td>\n<td><code>List[Tuple[str, float]]</code></td>\n<td>List of (test_name, duration) for slowest tests</td>\n</tr>\n</tbody></table>\n<h5 id=\"consoleformatter\">ConsoleFormatter</h5>\n<p>The <code>ConsoleFormatter</code> produces terminal output with these visual elements:</p>\n<ol>\n<li><strong>Progress Indicator</strong>: For non-verbose mode, show <code>.</code> for pass, <code>F</code> for fail, <code>E</code> for error, <code>s</code> for skip</li>\n<li><strong>Verbose Output</strong>: For <code>-v</code> flag, show each test name with status icon and duration</li>\n<li><strong>Failure Details</strong>: After summary, show detailed tracebacks and assertion messages for failed/errored tests</li>\n<li><strong>Summary Section</strong>: Clear totals with color-coded status counts</li>\n<li><strong>Slow Tests</strong>: If <code>--durations</code> flag, list slowest tests</li>\n</ol>\n<p><strong>Status Icons and Colors</strong> (using ANSI escape codes):</p>\n<ul>\n<li>✅ PASSED (green)</li>\n<li>❌ FAILED (red)</li>\n<li>⚠️ ERRORED (yellow/red)</li>\n<li>⏭️ SKIPPED (blue)</li>\n<li>⏳ RUNNING (cyan, for live progress)</li>\n</ul>\n<h5 id=\"junitformatter\">JUnitFormatter</h5>\n<p>The <code>JUnitFormatter</code> generates XML following the <a href=\"https://llg.cubic.org/docs/junit/\">JUnit XML schema</a>. Key elements:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">xml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">&#x3C;</span><span style=\"color:#85E89D\">testsuites</span><span style=\"color:#B392F0\"> name</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"apollo\"</span><span style=\"color:#B392F0\"> tests</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#B392F0\"> failures</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"2\"</span><span style=\"color:#B392F0\"> errors</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#B392F0\"> skipped</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"3\"</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"1.234\"</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  &#x3C;</span><span style=\"color:#85E89D\">testsuite</span><span style=\"color:#B392F0\"> name</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"test_login.py\"</span><span style=\"color:#B392F0\"> tests</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"10\"</span><span style=\"color:#B392F0\"> failures</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#B392F0\"> errors</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#B392F0\"> skipped</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"0.456\"</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    &#x3C;</span><span style=\"color:#85E89D\">testcase</span><span style=\"color:#B392F0\"> name</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"test_valid_login\"</span><span style=\"color:#B392F0\"> classname</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"test_login\"</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"0.123\"</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">      &#x3C;!-- If passed: no child elements --></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    &#x3C;/</span><span style=\"color:#85E89D\">testcase</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    &#x3C;</span><span style=\"color:#85E89D\">testcase</span><span style=\"color:#B392F0\"> name</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"test_invalid_password\"</span><span style=\"color:#B392F0\"> classname</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"test_login\"</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"0.045\"</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      &#x3C;</span><span style=\"color:#85E89D\">failure</span><span style=\"color:#B392F0\"> message</span><span style=\"color:#E1E4E8\">=</span><span style=\"color:#9ECBFF\">\"AssertionError: Expected status 200, got 401\"</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        &#x3C;![CDATA[Traceback (most recent call last):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  File \"test_login.py\", line 42, in test_invalid_password</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    assert response.status_code == 200</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">AssertionError: Expected status 200, got 401]]></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      &#x3C;/</span><span style=\"color:#85E89D\">failure</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    &#x3C;/</span><span style=\"color:#85E89D\">testcase</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  &#x3C;/</span><span style=\"color:#85E89D\">testsuite</span><span style=\"color:#E1E4E8\">></span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">&#x3C;/</span><span style=\"color:#85E89D\">testsuites</span><span style=\"color:#E1E4E8\">></span></span></code></pre></div>\n\n<p>The JUnit formatter must handle:</p>\n<ul>\n<li><strong>XML escaping</strong>: Properly escape <code>&lt;</code>, <code>&gt;</code>, <code>&amp;</code> in messages and tracebacks</li>\n<li><strong>CDATA sections</strong>: Wrap tracebacks in CDATA to avoid XML parsing issues</li>\n<li><strong>Attribute calculation</strong>: Compute aggregate statistics for testsuites</li>\n<li><strong>Timestamp formatting</strong>: ISO 8601 format for timestamps</li>\n</ul>\n<h4 id=\"algorithm-reporting-pipeline\">Algorithm: Reporting Pipeline</h4>\n<p>When the Runner completes execution, it passes all <code>TestResult</code> objects to the Reporter:</p>\n<ol>\n<li><strong>Statistics Collection</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   for result in test_results:\n       collector.add_result(result)\n   collector.finalize()  # Compute percentages, find slowest tests</code></pre></div>\n\n<ol start=\"2\">\n<li><p><strong>Format Selection</strong> (based on <code>Configuration.output_format</code>):</p>\n<ul>\n<li><strong>&quot;console&quot;</strong>: Use only <code>ConsoleFormatter</code></li>\n<li><strong>&quot;junit-xml&quot;</strong>: Use only <code>JUnitFormatter</code> </li>\n<li><strong>&quot;both&quot;</strong>: Use both formatters sequentially</li>\n</ul>\n</li>\n<li><p><strong>Formatting Process</strong>:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   if output_format includes &quot;console&quot;:\n       console_output = console_formatter.format(collector.stats, test_results)\n       output_writer.write_console(console_output)\n   \n   if output_format includes &quot;junit-xml&quot;:\n       xml_output = junit_formatter.format(collector.stats, test_results)\n       output_writer.write_file(config.junit_xml_path, xml_output)</code></pre></div>\n\n<ol start=\"4\">\n<li><strong>Exit Code Determination</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   if collector.stats.failed &gt; 0 or collector.stats.errored &gt; 0:\n       sys.exit(1)  # Non-zero exit code for CI\n   else:\n       sys.exit(0)  # Success</code></pre></div>\n\n<h4 id=\"state-machine-report-generation-states\">State Machine: Report Generation States</h4>\n<p>The Reporter progresses through distinct states during its lifecycle:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>IDLE</td>\n<td><code>receive_results(test_results)</code></td>\n<td>COLLECTING</td>\n<td>Initialize StatisticsCollector, start timer</td>\n</tr>\n<tr>\n<td>COLLECTING</td>\n<td><code>add_result(result)</code> for each result</td>\n<td>COLLECTING</td>\n<td>Update statistics, store result</td>\n</tr>\n<tr>\n<td>COLLECTING</td>\n<td><code>last_result_added()</code></td>\n<td>FORMATTING</td>\n<td>Finalize statistics, compute aggregates</td>\n</tr>\n<tr>\n<td>FORMATTING</td>\n<td><code>format_complete()</code></td>\n<td>OUTPUTTING</td>\n<td>Generate formatted strings</td>\n</tr>\n<tr>\n<td>OUTPUTTING</td>\n<td><code>write_complete()</code></td>\n<td>DONE</td>\n<td>Set exit code, clean up resources</td>\n</tr>\n<tr>\n<td>DONE</td>\n<td>(terminal state)</td>\n<td>DONE</td>\n<td>None</td>\n</tr>\n</tbody></table>\n<h4 id=\"concrete-walk-through-example\">Concrete Walk-Through Example</h4>\n<p>Consider running 5 tests with mixed outcomes:</p>\n<ol>\n<li><p><strong>Command</strong>: <code>python -m apollo test_a.py test_b.py -v</code></p>\n</li>\n<li><p><strong>Execution</strong>: Tests run, producing 5 <code>TestResult</code> objects</p>\n</li>\n<li><p><strong>Statistics Collection</strong>:</p>\n<ul>\n<li><code>test_login.py::test_valid_login</code>: PASSED, 0.12s</li>\n<li><code>test_login.py::test_invalid_password</code>: FAILED (assertion), 0.05s</li>\n<li><code>test_login.py::test_network_error</code>: ERRORED (timeout), 1.5s</li>\n<li><code>test_user.py::test_create_user</code>: PASSED, 0.08s</li>\n<li><code>test_user.py::test_delete_user</code>: SKIPPED (decorator), 0.0s</li>\n</ul>\n<p>Statistics: total=5, passed=2, failed=1, errored=1, skipped=1, total_time=1.75s</p>\n</li>\n<li><p><strong>Console Output</strong> (verbose mode):</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   test_login.py::test_valid_login ✅ 0.12s\n   test_login.py::test_invalid_password ❌ 0.05s\n   test_login.py::test_network_error ⚠️ 1.50s\n   test_user.py::test_create_user ✅ 0.08s\n   test_user.py::test_delete_user ⏭️ 0.00s (skip reason: feature disabled)\n   \n   ============================= FAILURES =============================\n   test_login.py::test_invalid_password\n   AssertionError: Expected status 200, got 401\n   &gt; test_login.py:42\n   \n   ============================= ERRORS =============================\n   test_login.py::test_network_error\n   TimeoutError: Connection timed out after 1.5s\n   &gt; test_login.py:67\n   \n   ============================= SUMMARY =============================\n   5 tests run in 1.75s\n   ✅ 2 passed | ❌ 1 failed | ⚠️ 1 errored | ⏭️ 1 skipped\n   \n   Exit code: 1 (failure)</code></pre></div>\n\n<ol start=\"5\">\n<li><strong>JUnit XML</strong> (if <code>--junit-xml=results.xml</code>):<ul>\n<li>File <code>results.xml</code> created with proper XML structure</li>\n<li>Contains all test cases with their outcomes</li>\n<li>Tracebacks included in <code>&lt;failure&gt;</code> and <code>&lt;error&gt;</code> elements</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"common-pitfalls\">Common Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Incomplete XML Escaping</strong></p>\n<ul>\n<li><strong>Description</strong>: Failing to escape special XML characters (<code>&lt;</code>, <code>&gt;</code>, <code>&amp;</code>, <code>&quot;</code>, <code>&#39;</code>) in test messages and tracebacks.</li>\n<li><strong>Why it&#39;s wrong</strong>: Invalid XML that breaks CI tools parsing the report, potentially causing false positive builds.</li>\n<li><strong>How to fix</strong>: Use <code>xml.sax.saxutils.escape()</code> for attribute values and wrap multiline tracebacks in <code>&lt;![CDATA[...]]&gt;</code> sections.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Color Codes on Non-TTY Output</strong></p>\n<ul>\n<li><strong>Description</strong>: Emitting ANSI color codes when output is redirected to a file or pipe.</li>\n<li><strong>Why it&#39;s wrong</strong>: Files contain unreadable escape sequences like <code>\\033[32m</code>, breaking downstream processing.</li>\n<li><strong>How to fix</strong>: Check <code>sys.stdout.isatty()</code> before using colors, or provide a <code>--no-color</code> flag that disables them.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Incorrect Exit Code for Mixed Results</strong></p>\n<ul>\n<li><strong>Description</strong>: Returning exit code 0 when tests fail but others pass, or vice versa.</li>\n<li><strong>Why it&#39;s wrong</strong>: CI systems rely on exit codes to detect build failures; wrong codes cause false passes/failures.</li>\n<li><strong>How to fix</strong>: Exit with code 1 if ANY test has status <code>FAILED</code> or <code>ERRORED</code>. Skipped tests don&#39;t affect exit code.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Time Calculation Floating-Point Errors</strong></p>\n<ul>\n<li><strong>Description</strong>: Using float arithmetic for duration sums causing tiny rounding errors.</li>\n<li><strong>Why it&#39;s wrong</strong>: Displaying &quot;Total time: 1.0000000002s&quot; looks unprofessional and may confuse users.</li>\n<li><strong>How to fix</strong>: Round to millisecond precision (3 decimal places) for display, using <code>round(duration, 3)</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Memory Bloat from Storing All Results</strong></p>\n<ul>\n<li><strong>Description</strong>: Keeping all <code>TestResult</code> objects with full tracebacks in memory for large test suites.</li>\n<li><strong>Why it&#39;s wrong</strong>: Memory usage grows with test count, potentially causing OOM errors.</li>\n<li><strong>How to fix</strong>: For console output, stream results as they complete. For JUnit XML, accumulate only necessary data (not full Python objects) or write incrementally to a file.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CLI Parsing</td>\n<td><code>argparse</code> (standard library)</td>\n<td><code>click</code> (third-party, richer features)</td>\n</tr>\n<tr>\n<td>Pattern Resolution</td>\n<td><code>pathlib.Path.glob()</code> + custom recursion</td>\n<td><code>pathlib.Path.rglob()</code> with ignore patterns</td>\n</tr>\n<tr>\n<td>Color Output</td>\n<td>ANSI escape codes + <code>sys.stdout.isatty()</code> check</td>\n<td><code>colorama</code> (cross-platform color support)</td>\n</tr>\n<tr>\n<td>XML Generation</td>\n<td><code>xml.etree.ElementTree</code></td>\n<td><code>lxml</code> (more features, better performance)</td>\n</tr>\n<tr>\n<td>Progress Display</td>\n<td>Simple dots/chars printed inline</td>\n<td><code>tqdm</code> or custom progress bar with threading</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n├── __main__.py              # CLI entry point: `python -m apollo`\n├── cli/\n│   ├── __init__.py\n│   ├── parser.py            # parse_cli_args(), Configuration class\n│   ├── resolver.py          # resolve_patterns_to_paths(), pattern utilities\n│   └── help.py              # Help text generation\n├── reporting/\n│   ├── __init__.py\n│   ├── reporter.py          # Reporter coordinator class\n│   ├── statistics.py        # StatisticsCollector class\n│   ├── console_formatter.py # ConsoleFormatter class\n│   ├── junit_formatter.py   # JUnitFormatter class\n│   ├── writer.py            # OutputWriter (handles stdout/file)\n│   └── colors.py            # Color constants and helpers\n└── api.py                   # Public API (if needed for programmatic use)</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>File: <code>apollo/cli/resolver.py</code></strong> (Complete implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Pattern resolution utilities for converting user input to file paths.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Iterable</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> resolve_patterns_to_paths</span><span style=\"color:#E1E4E8\">(patterns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], base_dir: Path) -> List[Path]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Convert file patterns to concrete Python file paths.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        patterns: List of glob patterns or directory paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        base_dir: Base directory for relative pattern resolution</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of Path objects to Python files matching patterns</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resolved_paths </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> pattern </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> patterns:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert to Path relative to base_dir</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pattern_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (base_dir </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> pattern).resolve()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle directory pattern (e.g., \"tests/\")</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> pattern_path.is_dir():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Recursively find all .py files in directory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> py_file </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> pattern_path.rglob(</span><span style=\"color:#9ECBFF\">\"*.py\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> _is_test_file(py_file):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    resolved_paths.add(py_file)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle glob pattern (e.g., \"**/test_*.py\")</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#9ECBFF\"> \"*\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(pattern):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Need to handle glob relative to base_dir</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> match </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> base_dir.glob(pattern):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> match.is_file() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> match.suffix </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \".py\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> _is_test_file(match):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    resolved_paths.add(match.resolve())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle explicit file path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> pattern_path.is_file() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> pattern_path.suffix </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \".py\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> _is_test_file(pattern_path):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                resolved_paths.add(pattern_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Pattern doesn't match anything</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sys.stderr.write(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Warning: Pattern '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">pattern</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' matched no files</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sort for consistent ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(resolved_paths)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _is_test_file</span><span style=\"color:#E1E4E8\">(path: Path) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if a file should be considered for test discovery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Skip common non-test directories</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exclude_parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"__pycache__\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\".git\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\".pytest_cache\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"venv\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"env\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> part </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> path.parts:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> part </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> exclude_parts:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Skip hidden files</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> path.name.startswith(</span><span style=\"color:#9ECBFF\">\".\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>File: <code>apollo/reporting/colors.py</code></strong> (Complete implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">ANSI color codes and color-aware output utilities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ANSI escape codes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RESET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[0m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">BOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[1m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[31m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">GREEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[32m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">YELLOW</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[33m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">BLUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[34m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAGENTA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[35m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">CYAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[36m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">WHITE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[37m\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">GRAY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\033</span><span style=\"color:#9ECBFF\">[90m\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Status colors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">PASS_COLOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> GREEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">FAIL_COLOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> RED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">ERROR_COLOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> YELLOW</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SKIP_COLOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> BLUE</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RUNNING_COLOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> CYAN</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> should_use_color</span><span style=\"color:#E1E4E8\">() -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Determine if we should output color codes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check if output is a terminal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> sys.stdout.isatty():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for NO_COLOR environment variable (standard)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> os.environ.get(</span><span style=\"color:#9ECBFF\">\"NO_COLOR\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for specific terminal capabilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    term </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> os.environ.get(</span><span style=\"color:#9ECBFF\">\"TERM\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> term </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"dumb\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> colorize</span><span style=\"color:#E1E4E8\">(text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, color_code: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Wrap text in color codes if colors are enabled.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> should_use_color():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">color_code</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">text</span><span style=\"color:#79B8FF\">}{RESET}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> text</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Status icons with color</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">PASS_ICON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> colorize(</span><span style=\"color:#9ECBFF\">\"✓\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">PASS_COLOR</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> should_use_color() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \".\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">FAIL_ICON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> colorize(</span><span style=\"color:#9ECBFF\">\"✗\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">FAIL_COLOR</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> should_use_color() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"F\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">ERROR_ICON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> colorize(</span><span style=\"color:#9ECBFF\">\"⚠\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">ERROR_COLOR</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> should_use_color() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"E\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SKIP_ICON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> colorize(</span><span style=\"color:#9ECBFF\">\"⏭\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">SKIP_COLOR</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> should_use_color() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"s\"</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>File: <code>apollo/cli/parser.py</code></strong> (TODO skeleton)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Command-line argument parsing for the test framework.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> argparse</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Configuration</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse_cli_args</span><span style=\"color:#E1E4E8\">(args: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Configuration:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Parse command-line arguments into a Configuration object.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        args: Argument list (defaults to sys.argv[1:])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Configuration object with parsed settings</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        SystemExit: If --help or --version is requested</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> args </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        args </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sys.argv[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> argparse.ArgumentParser(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        prog</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"apollo\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Apollo Test Framework - A pytest-inspired test runner\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        epilog</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"See https://github.com/example/apollo for more information.\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add positional argument for file/directory patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Argument name: \"patterns\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - nargs: \"*\" (zero or more)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - default: [\".\"] (current directory)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - help: \"File/directory/glob patterns to search for tests\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add -k/--filter flag for test name filtering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Action: \"append\" (can be used multiple times)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Only run tests matching substring pattern\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add -v/--verbose flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Action: \"store_true\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Verbose output (show passing tests)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add -q/--quiet flag (mutually exclusive with -v)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Action: \"store_true\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Quiet mode (summary only)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add --junit-xml flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Type: str (file path)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Generate JUnit XML report to specified file\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add --parallel flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Action: \"store_true\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Run tests in parallel\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Add -n/--num-workers flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Type: int</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Number of parallel workers (default: auto)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Add --durations flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Action: \"store_true\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Show test execution durations\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Add --collect-only flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Action: \"store_true\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Help: \"Collect tests but don't execute\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Add --version flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Action: \"version\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   - Version: import metadata and get version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Parse arguments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   parsed = parser.parse_args(args)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Validate mutually exclusive flags (-v and -q can't both be True)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Build Configuration object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   config = Configuration(</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       file_patterns=parsed.patterns,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       start_dir=Path.cwd(),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       test_name_filters=parsed.filter or [],</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       verbose=parsed.verbose,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       quiet=parsed.quiet,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       output_format=\"junit-xml\" if parsed.junit_xml else \"console\",</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       parallel=parsed.parallel,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       max_workers=parsed.num_workers,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       show_durations=parsed.durations,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #       junit_xml_path=Path(parsed.junit_xml) if parsed.junit_xml else None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Return configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    #   return config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>File: <code>apollo/reporting/statistics.py</code></strong> (TODO skeleton)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Statistics collection for test results.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestResult, TestStatus</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModuleStats</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Statistics for a single module.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    passed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errored: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    skipped: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestRunStatistics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete statistics for a test run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    passed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failed: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errored: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    skipped: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_time: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    by_module: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ModuleStats] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    slowest_tests: List[Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> success_rate</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate success rate as percentage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.total </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.passed </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.total) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StatisticsCollector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Collects and aggregates test result statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestRunStatistics()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._results: List[TestResult] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._module_cache: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ModuleStats] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_result</span><span style=\"color:#E1E4E8\">(self, result: TestResult) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Add a test result to statistics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            result: TestResult to incorporate into statistics</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Append result to internal _results list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update overall counts based on result.status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Use match/case or if/elif for TestStatus enum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract module name from result.test_case.nodeid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Format: \"path/to/module.py::test_name\" → module = \"path/to/module.py\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Get or create ModuleStats for this module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   If not in _module_cache, create new ModuleStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update module statistics (counts and time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add to total_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Track for slowest tests list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Keep only top 10 slowest tests by duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> finalize</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Finalize statistics after all results added.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Computes derived statistics and prepares for reporting.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set end_time to current time if not already set</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Sort slowest_tests by duration (descending)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Populate stats.by_module from _module_cache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate any other derived statistics needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Mark statistics as finalized (optional flag)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_summary_string</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate a human-readable summary string.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Formatted summary like \"5 tests, 3 passed, 1 failed, 1 skipped\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format summary string with counts and optional percentage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>File: <code>apollo/reporting/console_formatter.py</code></strong> (TODO skeleton)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Console formatter for human-readable test output.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestResult</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .statistics </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestRunStatistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .colors </span><span style=\"color:#F97583\">import</span><span style=\"color:#F97583\"> *</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConsoleFormatter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Formats test results for console output.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, verbose: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, show_durations: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.verbose </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> verbose</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.show_durations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> show_durations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> format</span><span style=\"color:#E1E4E8\">(self, stats: TestRunStatistics, results: List[TestResult]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Format complete test run results for console output.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stats: Aggregated statistics</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            results: Individual test results</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Formatted string ready for printing</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If not verbose, generate progress indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   For each result: </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     PASSED → \".\" or PASS_ICON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     FAILED → \"F\" or FAIL_ICON  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     ERRORED → \"E\" or ERROR_ICON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #     SKIPPED → \"s\" or SKIP_ICON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Join into single line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If verbose, generate detailed test listing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   For each result: format as \"nodeid [status_icon] [duration]\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Include skip reason if test was skipped</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate failures section</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Filter results for FAILED status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   For each: format traceback and assertion message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Include file:line information</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate errors section  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Filter results for ERRORED status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   For each: format exception traceback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Distinguish from assertion failures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate summary section</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Show total tests, time, and counts by status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Format with colors/icons if enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Include success percentage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If show_durations, add slowest tests list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Show top N slowest tests with durations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Join all parts with appropriate section headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Use separator lines like \"=\" * 60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(output_parts)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_test_result</span><span style=\"color:#E1E4E8\">(self, result: TestResult) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Format a single test result line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement based on verbosity and show_durations settings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_failure_details</span><span style=\"color:#E1E4E8\">(self, result: TestResult) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Format detailed failure information for a test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract and format assertion message, traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>File: <code>apollo/reporting/junit_formatter.py</code></strong> (TODO skeleton)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">JUnit XML formatter for CI/CD integration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> xml.etree.ElementTree </span><span style=\"color:#F97583\">as</span><span style=\"color:#79B8FF\"> ET</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> xml.dom </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> minidom</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestResult</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .statistics </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestRunStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JUnitFormatter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Formats test results as JUnit XML.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> format</span><span style=\"color:#E1E4E8\">(self, stats: TestRunStatistics, results: List[TestResult]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Format test results as JUnit XML string.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stats: Aggregated statistics</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            results: Individual test results</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            JUnit XML as string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create root &#x3C;testsuites> element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add attributes: name=\"apollo\", tests, failures, errors, skipped, time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Group results by module (file)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   results_by_module = group_results_by_file(results)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each module, create &#x3C;testsuite> element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add attributes: name (filename), tests, failures, errors, skipped, time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each test in module, create &#x3C;testcase> element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add attributes: name (test function name), classname (module), time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For failed tests, add &#x3C;failure> child element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add message attribute with assertion error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add traceback as element text (wrap in CDATA)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: For errored tests, add &#x3C;error> child element  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add message attribute with exception type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add traceback as element text (wrap in CDATA)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: For skipped tests, add &#x3C;skipped> child element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Add message attribute with skip reason if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Convert ElementTree to pretty-printed XML string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Use minidom for pretty printing or custom formatting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Return XML string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _escape_xml</span><span style=\"color:#E1E4E8\">(self, text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Escape text for safe inclusion in XML.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            text: Text to escape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Escaped text safe for XML attributes</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Replace &#x26;, &#x3C;, >, \", ' with XML entities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_cdata</span><span style=\"color:#E1E4E8\">(self, text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Wrap text in CDATA section if it contains special characters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            text: Text to wrap</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            CDATA-wrapped text or escaped text</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if text contains ]]> (illegal in CDATA)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   If yes, split and wrap multiple CDATA sections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   Otherwise, wrap in &#x3C;![CDATA[ ... ]]></span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ol>\n<li><p><strong>argparse Tips</strong>:</p>\n<ul>\n<li>Use <code>argparse.ArgumentParser(add_help=False)</code> if you want custom help formatting</li>\n<li>Mutually exclusive groups: <code>parser.add_mutually_exclusive_group()</code> for <code>-v</code> and <code>-q</code></li>\n<li>Subparsers can be used for advanced commands but aren&#39;t needed for our simple CLI</li>\n</ul>\n</li>\n<li><p><strong>Path Handling</strong>:</p>\n<ul>\n<li>Always use <code>pathlib.Path</code> over <code>os.path</code> for cleaner, more object-oriented code</li>\n<li><code>Path.resolve()</code> converts to absolute paths and resolves symlinks</li>\n<li><code>Path.rglob(&quot;*.py&quot;)</code> recursively finds Python files</li>\n</ul>\n</li>\n<li><p><strong>XML Generation</strong>:</p>\n<ul>\n<li><code>xml.etree.ElementTree</code> is in the standard library and sufficient for our needs</li>\n<li>Use <code>ET.SubElement()</code> to build the tree structure</li>\n<li>For pretty printing, use <code>xml.dom.minidom.parseString()</code> then <code>toprettyxml()</code></li>\n</ul>\n</li>\n<li><p><strong>Timing</strong>:</p>\n<ul>\n<li>Use <code>time.perf_counter()</code> for high-resolution timing, not <code>time.time()</code></li>\n<li>Store start time early in the process and calculate duration at the end</li>\n</ul>\n</li>\n<li><p><strong>Exit Codes</strong>:</p>\n<ul>\n<li><code>sys.exit(0)</code> for success, <code>sys.exit(1)</code> for failures</li>\n<li>Consider using different codes for different failure types (e.g., 2 for configuration errors) if you want to be fancy</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p><strong>To verify Milestone 4 implementation</strong>:</p>\n<ol>\n<li><strong>Run the basic test</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   python -m apollo --version</code></pre></div>\n<p>   <em>Expected</em>: Shows version information and exits with code 0.</p>\n<ol start=\"2\">\n<li><strong>Test discovery and simple run</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   python -m apollo tests/sample_tests.py -v</code></pre></div>\n<p>   <em>Expected</em>: Shows each test with ✅/❌ icons, summary, and appropriate exit code.</p>\n<ol start=\"3\">\n<li><strong>Test filtering</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   python -m apollo tests/ -k &quot;login&quot; --durations</code></pre></div>\n<p>   <em>Expected</em>: Runs only tests with &quot;login&quot; in their name, shows durations for slowest tests.</p>\n<ol start=\"4\">\n<li><strong>JUnit XML output</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   python -m apollo tests/ --junit-xml=test-results.xml\n   cat test-results.xml</code></pre></div>\n<p>   <em>Expected</em>: Creates well-formed XML file with proper escaping and CDATA sections.</p>\n<ol start=\"5\">\n<li><strong>Exit code verification</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   python -m apollo tests/failing_tests.py\n   echo $?  # On Linux/Mac</code></pre></div>\n<p>   <em>Expected</em>: Exit code 1 if any tests fail or error.</p>\n<p><strong>Signs something is wrong</strong>:</p>\n<ul>\n<li>Tests run but no output appears (check <code>-v</code> flag or default verbosity)</li>\n<li>XML file contains unescaped <code>&lt;</code> or <code>&amp;</code> characters (breaks CI parsers)</li>\n<li>Exit code is 0 when tests fail (CI would incorrectly pass)</li>\n<li>Color codes appear in redirected output (check <code>isatty()</code>)</li>\n<li>Pattern <code>**/*.py</code> includes virtual environment files (need better filtering)</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No tests found when pattern should match</td>\n<td>Incorrect pattern resolution</td>\n<td>Print resolved paths after <code>resolve_patterns_to_paths()</code></td>\n<td>Debug pattern matching logic, check <code>Path.glob()</code> vs <code>Path.rglob()</code></td>\n</tr>\n<tr>\n<td>XML file fails to parse in CI</td>\n<td>Missing XML escaping or CDATA</td>\n<td>Validate XML with <code>xmllint --noout file.xml</code></td>\n<td>Add proper escaping for <code>&lt;</code>, <code>&gt;</code>, <code>&amp;</code> and wrap tracebacks in CDATA</td>\n</tr>\n<tr>\n<td>Colors don&#39;t appear in terminal</td>\n<td><code>isatty()</code> check failing or NO_COLOR env var</td>\n<td>Print <code>sys.stdout.isatty()</code> and check <code>os.environ.get(&quot;NO_COLOR&quot;)</code></td>\n<td>Fix color detection logic or add <code>--color=always/auto/never</code> flag</td>\n</tr>\n<tr>\n<td>Exit code always 0</td>\n<td>Forgot to set exit code based on failures</td>\n<td>Add debug print of failure count before <code>sys.exit()</code></td>\n<td>Set <code>sys.exit(1)</code> if any failures or errors</td>\n</tr>\n<tr>\n<td>JUnit report missing timestamps</td>\n<td>Forgot to add <code>time</code> attribute to testcase</td>\n<td>Check generated XML for <code>time=&quot;...&quot;</code> attributes</td>\n<td>Ensure duration is recorded and formatted to 3 decimal places</td>\n</tr>\n<tr>\n<td>Verbose and quiet flags both work</td>\n<td>Mutually exclusive group not set up</td>\n<td>Check argparse mutual exclusion setup</td>\n<td>Use <code>add_mutually_exclusive_group()</code> for <code>-v</code> and <code>-q</code></td>\n</tr>\n<tr>\n<td>Pattern with <code>**</code> doesn&#39;t recurse</td>\n<td>Using <code>glob()</code> instead of <code>rglob()</code></td>\n<td>Test pattern resolution with simple test script</td>\n<td>Use <code>Path.rglob()</code> for recursive matching or <code>glob.glob(&quot;**/*.py&quot;, recursive=True)</code></td>\n</tr>\n</tbody></table>\n<h2 id=\"9-interactions-and-data-flow\">9. Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All four milestones, as this section describes the end-to-end flow through all components.</p>\n</blockquote>\n<p>The <strong>Interactions and Data Flow</strong> section reveals how the test framework&#39;s components collaborate like a well-orchestrated symphony. Imagine a <strong>music recording studio</strong> where the producer (CLI) receives the project brief (user command), gathers musicians (Discoverer), sets up instruments (Fixture System), records each track in isolation (Runner), reviews the recordings with a critical ear (Assertion Engine), and finally produces the album with track listings and performance notes (Reporter). This section traces the complete journey from user command to final report, showing how data transforms as it moves through the framework&#39;s <strong>pipeline architecture</strong>.</p>\n<h3 id=\"main-execution-sequence\">Main Execution Sequence</h3>\n<p>The framework follows a <strong>linear pipeline architecture</strong> where each component processes input and passes output to the next. This design ensures clear separation of concerns and makes the system easy to reason about. Let&#39;s trace through the most common happy path: a user runs <code>apollo tests/ --verbose</code> to execute all tests in a directory with detailed output.</p>\n<h4 id=\"phase-1-cli-invocation-and-configuration\">Phase 1: CLI Invocation and Configuration</h4>\n<blockquote>\n<p><strong>Mental Model:</strong> The <strong>Control Panel Activation</strong> - The user interacts with the framework&#39;s control panel (CLI), dialing in settings that configure the entire test run.</p>\n</blockquote>\n<ol>\n<li><p><strong>User Command Entry</strong>: The user types a command into their terminal, such as <code>apollo tests/ --verbose --parallel</code>. This command is received by the operating system and passed to the Python interpreter running the framework&#39;s entry point.</p>\n</li>\n<li><p><strong>Argument Parsing</strong>: The <code>parse_cli_args()</code> function ingests the raw command-line arguments and transforms them into a structured <code>Configuration</code> object. This process involves:</p>\n<ul>\n<li>Validating that mutually exclusive flags aren&#39;t used together (e.g., <code>--verbose</code> and <code>--quiet</code>)</li>\n<li>Resolving relative paths to absolute paths based on the current working directory</li>\n<li>Setting default values for unspecified options (e.g., default <code>output_format</code> is <code>&quot;console&quot;</code>)</li>\n<li>Normalizing file patterns (ensuring <code>*.py</code> suffix where needed)</li>\n</ul>\n</li>\n<li><p><strong>Configuration Validation</strong>: The framework performs basic sanity checks on the <code>Configuration</code>:</p>\n<ul>\n<li>Verifying that specified directories exist and are readable</li>\n<li>Ensuring <code>max_workers</code> for parallel execution is a positive integer</li>\n<li>Checking that the JUnit XML output path (if specified) is writable</li>\n</ul>\n</li>\n<li><p><strong>Early Exit Conditions</strong>: If the configuration includes help (<code>--help</code>) or version (<code>--version</code>) flags, the CLI prints the requested information and exits immediately without proceeding to discovery.</p>\n</li>\n</ol>\n<p>At this point, the abstract user intent has been transformed into a concrete, structured <code>Configuration</code> object that serves as the <strong>work order</strong> for the entire test run.</p>\n<h4 id=\"phase-2-test-discovery-and-suite-assembly\">Phase 2: Test Discovery and Suite Assembly</h4>\n<blockquote>\n<p><strong>Mental Model:</strong> The <strong>Casting Call</strong> - The Discoverer acts as a casting director, scanning through the codebase to find all eligible &quot;performers&quot; (test functions) and organizing them into a &quot;playlist&quot; (TestSuite).</p>\n</blockquote>\n<ol start=\"5\">\n<li><p><strong>Pattern Resolution</strong>: The framework calls <code>resolve_patterns_to_paths()</code> to convert the user&#39;s file patterns (like <code>tests/</code>, <code>test_*.py</code>) into concrete filesystem paths. This function:</p>\n<ul>\n<li>Recursively walks directories if specified</li>\n<li>Filters files to only include Python modules (<code>.py</code> files)</li>\n<li>Respects any <code>.gitignore</code> or similar exclusion patterns</li>\n<li>Returns a list of <code>Path</code> objects ready for import</li>\n</ul>\n</li>\n<li><p><strong>Module Import and Inspection</strong>: For each resolved file path, the Discoverer:</p>\n<ul>\n<li>Converts the filesystem path to an importable module name using <code>module_path_to_name()</code></li>\n<li>Dynamically imports the module using <code>_import_module_from_file()</code>, which handles the Python import machinery</li>\n<li>Scans the imported module for test functions with <code>_find_tests_in_module()</code></li>\n<li>For each discovered test function, creates a <code>TestCase</code> object with:<ul>\n<li><code>nodeid</code>: A unique identifier like <code>&quot;tests/test_math.py::test_addition&quot;</code></li>\n<li><code>func</code>: A reference to the actual test function</li>\n<li><code>file_path</code> and <code>line_no</code>: Source location for error reporting</li>\n<li><code>fixtures</code>: Initially empty list (populated during fixture scanning)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Fixture Registration</strong>: Concurrently with test discovery, the framework scans each module for fixture definitions using <code>FixtureRegistry.scan_module()</code>. Each <code>@fixture</code> decorator registers a <code>Fixture</code> object in the global <code>FixtureRegistry</code>, recording its name, scope, dependencies, and the function that creates it.</p>\n</li>\n<li><p><strong>Suite Construction</strong>: All discovered <code>TestCase</code> objects are grouped into a <code>TestSuite</code> object. The suite acts as a <strong>playlist of tests</strong> that maintains the execution order (typically the order of discovery, though this can be configured).</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Architecture Decision: Pipeline vs. Callback Architecture</strong></p>\n<p><strong>Context:</strong> We need to coordinate test discovery, fixture registration, and test execution. Two patterns emerged: a linear pipeline (each component completes before the next begins) vs. a callback architecture (components register callbacks that fire at specific events).</p>\n<p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Linear Pipeline:</strong> Discover all tests → Register all fixtures → Execute all tests → Report all results</li>\n<li><strong>Callback/Event-Driven:</strong> Components subscribe to events (module_loaded, test_found, fixture_registered) and react accordingly</li>\n<li><strong>Hybrid Approach:</strong> Pipeline for main flow with event hooks for extensions</li>\n</ol>\n<p><strong>Decision:</strong> Linear Pipeline Architecture</p>\n<p><strong>Rationale:</strong></p>\n<ul>\n<li>Simplicity and predictability: The flow is easy to trace and debug</li>\n<li>Natural progression: Each phase has clear preconditions (e.g., fixtures must be registered before tests that depend on them can run)</li>\n<li>Error handling: If discovery fails, we can fail fast without partially executing tests</li>\n<li>Learning value: A linear pipeline is easier for learners to understand and implement</li>\n</ul>\n<p><strong>Consequences:</strong></p>\n<ul>\n<li>The framework loads all modules into memory before any test runs (higher memory usage)</li>\n<li>No streaming or incremental processing of very large test suites</li>\n<li>Clear separation between phases makes parallelization straightforward</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Linear Pipeline</td>\n<td>Simple, predictable, easy to debug</td>\n<td>All tests loaded into memory at once</td>\n<td>✅</td>\n</tr>\n<tr>\n<td>Callback/Event-Driven</td>\n<td>Flexible, incremental, memory efficient</td>\n<td>Complex to implement and debug</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Balance of flexibility and simplicity</td>\n<td>Adds complexity without clear benefit for this project</td>\n<td>❌</td>\n</tr>\n</tbody></table>\n</blockquote>\n<h4 id=\"phase-3-fixture-preparation-and-test-execution\">Phase 3: Fixture Preparation and Test Execution</h4>\n<blockquote>\n<p><strong>Mental Model:</strong> The <strong>Stage Crew Setup</strong> - Before each &quot;performance&quot; (test), the stage crew (Fixture System) prepares the set, props, and lighting according to the &quot;stage directions&quot; (fixture dependencies).</p>\n</blockquote>\n<ol start=\"9\">\n<li><p><strong>Fixture Dependency Resolution</strong>: For each <code>TestCase</code> in the <code>TestSuite</code>, the framework analyzes the test function&#39;s parameters to determine which fixtures it requires. This is done by:</p>\n<ul>\n<li>Inspecting the function signature using Python&#39;s <code>inspect</code> module</li>\n<li>Matching parameter names against registered fixture names in the <code>FixtureRegistry</code></li>\n<li>Building a dependency graph if fixtures themselves depend on other fixtures</li>\n<li>Detecting and reporting circular dependencies with a clear error message</li>\n</ul>\n</li>\n<li><p><strong>Fixture Value Calculation</strong>: The <code>FixtureLifecycleManager</code> computes or retrieves cached values for each required fixture based on scope:</p>\n<ul>\n<li>For <code>FUNCTION</code> scope: Create a new value for each test</li>\n<li>For <code>CLASS</code> scope: Create once per test class and reuse for tests in the same class</li>\n<li>For <code>MODULE</code> scope: Create once per module and reuse for all tests in that module</li>\n<li>For <code>SESSION</code> scope: Create once for the entire test run and reuse everywhere</li>\n</ul>\n<p>The manager uses a <code>FixtureRequest</code> object as a <strong>work order</strong> for each fixture creation, containing the fixture name, scope, and a cache key derived from the scope context.</p>\n</li>\n<li><p><strong>Test Isolation Setup</strong>: The Runner prepares an isolated environment for each test:</p>\n<ul>\n<li>Creates a fresh dictionary for the test&#39;s local namespace</li>\n<li>Sets up exception handling to catch any unanticipated errors</li>\n<li>Starts a timer to measure execution duration</li>\n<li>Updates the test&#39;s status from <code>PENDING</code> to <code>RUNNING</code></li>\n</ul>\n</li>\n<li><p><strong>Test Execution with Fixture Injection</strong>: The Runner calls the test function with the prepared fixture values as arguments. This is the core <strong>dependency injection mechanism</strong>:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">    # Simplified conceptual view (not actual code)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fixture_values </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {name: manager.get_fixture_value(name, test_case) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> required_fixtures}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_function(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">fixture_values)</span></span></code></pre></div>\n\n<ol start=\"13\">\n<li><p><strong>Assertion Evaluation</strong>: During test execution, when an assertion function like <code>assert_equal()</code> is called:</p>\n<ul>\n<li>The Assertion Engine compares actual vs. expected values</li>\n<li>If they match, execution continues silently</li>\n<li>If they differ, the engine constructs a detailed <code>AssertionFailure</code> object with diff information and raises an <code>AssertionError</code> with this rich context</li>\n<li>Custom matchers are evaluated by calling their <code>__matches__()</code> method and, on failure, <code>__describe_mismatch__()</code> for a tailored error message</li>\n</ul>\n</li>\n<li><p><strong>Exception Handling</strong>: The Runner wraps test execution in a try-except block to distinguish between:</p>\n<ul>\n<li><strong>Test Failure:</strong> An <code>AssertionError</code> was raised (expected failure)</li>\n<li><strong>Test Error:</strong> Any other exception was raised (unexpected error)</li>\n<li><strong>Test Success:</strong> No exceptions were raised</li>\n</ul>\n</li>\n<li><p><strong>Result Recording</strong>: After test execution completes (whether successfully or with failure/error), the Runner:</p>\n<ul>\n<li>Stops the timer and calculates duration</li>\n<li>Creates a <code>TestResult</code> object with:<ul>\n<li><code>status</code>: <code>PASSED</code>, <code>FAILED</code>, or <code>ERRORED</code></li>\n<li><code>message</code>: A human-readable description of what went wrong (if applicable)</li>\n<li><code>exception</code> and <code>traceback</code>: For debugging failures and errors</li>\n<li><code>duration</code>: Execution time in seconds</li>\n</ul>\n</li>\n<li>Passes the <code>TestResult</code> to the <code>StatisticsCollector</code></li>\n</ul>\n</li>\n<li><p><strong>Fixture Teardown</strong>: After each test (or at appropriate <strong>scope boundaries</strong>), the <code>FixtureLifecycleManager</code> tears down fixtures:</p>\n<ul>\n<li>For generator-based fixtures (using <code>yield</code>), it resumes the generator to execute teardown code</li>\n<li>For regular fixtures, it calls any registered finalizers</li>\n<li>It removes cached values for fixtures whose scope has ended</li>\n<li>It handles teardown errors gracefully by logging them but continuing with other cleanup</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p><strong>Concrete Walk-Through: Testing a Database Connection</strong></p>\n<p>Let&#39;s trace a concrete example: A test function <code>test_user_count(db_connection)</code> depends on a <code>db_connection</code> fixture with <code>MODULE</code> scope.</p>\n<ol>\n<li><strong>Discovery:</strong> The Discoverer finds <code>test_user_count</code> and notes it requires <code>db_connection</code> parameter</li>\n<li><strong>Fixture Registration:</strong> The <code>@fixture(scope=&quot;module&quot;)</code> decorator registers <code>db_connection</code> in the registry</li>\n<li><strong>Dependency Resolution:</strong> The framework matches the parameter name <code>db_connection</code> to the fixture</li>\n<li><strong>Fixture Creation (First Test in Module):</strong> Since it&#39;s <code>MODULE</code> scope and no cached value exists, <code>FixtureLifecycleManager</code>:<ul>\n<li>Creates a <code>FixtureRequest</code> with scope <code>MODULE</code> and cache key derived from module name</li>\n<li>Calls the <code>db_connection()</code> fixture function</li>\n<li>Caches the returned connection object</li>\n</ul>\n</li>\n<li><strong>Test Execution:</strong> The Runner calls <code>test_user_count(db_connection=connection_object)</code></li>\n<li><strong>Assertion:</strong> Inside the test, <code>assert_equal(connection.query(&quot;SELECT COUNT(*) FROM users&quot;), 5)</code> passes</li>\n<li><strong>Result Recording:</strong> <code>TestResult</code> with status <code>PASSED</code> is created</li>\n<li><strong>Fixture Teardown (After Last Test in Module):</strong> When all tests in the module complete:<ul>\n<li>The manager resumes the <code>db_connection</code> generator (or calls finalizer)</li>\n<li>The connection is closed</li>\n<li>The cached value is removed</li>\n</ul>\n</li>\n</ol>\n</blockquote>\n<h4 id=\"phase-4-result-collection-and-reporting\">Phase 4: Result Collection and Reporting</h4>\n<blockquote>\n<p><strong>Mental Model:</strong> The <strong>Performance Review</strong> - After all performances are complete, the reviewer (Reporter) analyzes each recording, writes critique notes, calculates statistics, and publishes the review in multiple formats (print program, digital album, XML for archives).</p>\n</blockquote>\n<ol start=\"17\">\n<li><p><strong>Statistics Aggregation</strong>: As each <code>TestResult</code> is produced, the <code>StatisticsCollector</code>:</p>\n<ul>\n<li>Increments counters for the appropriate status (<code>passed</code>, <code>failed</code>, <code>errored</code>, <code>skipped</code>)</li>\n<li>Records execution time and tracks the slowest tests</li>\n<li>Groups results by module for detailed reporting</li>\n<li>Maintains overall start and end timestamps</li>\n</ul>\n</li>\n<li><p><strong>Format Selection</strong>: Based on the <code>Configuration.output_format</code>, the framework selects an appropriate formatter:</p>\n<ul>\n<li><code>&quot;console&quot;</code>: Uses <code>ConsoleFormatter</code> for human-readable terminal output</li>\n<li><code>&quot;junit&quot;</code>: Uses <code>JUnitFormatter</code> for CI/CD pipeline integration</li>\n<li><code>&quot;both&quot;</code>: Generates both formats and writes to appropriate destinations</li>\n</ul>\n</li>\n<li><p><strong>Output Generation</strong>: The selected formatter processes all results:</p>\n<ul>\n<li><strong>Console Output:</strong> Creates a visually appealing report with colors (if terminal supports it), progress indicators, and a summary table</li>\n<li><strong>JUnit XML:</strong> Generates standards-compliant XML with proper escaping, CDATA sections for output, and timing information</li>\n</ul>\n</li>\n<li><p><strong>Output Delivery</strong>: The formatted output is written to:</p>\n<ul>\n<li>stdout for console output</li>\n<li>The specified file path for JUnit XML</li>\n<li>Both if configured</li>\n</ul>\n</li>\n<li><p><strong>Exit Code Determination</strong>: Finally, the framework sets the process exit code:</p>\n<ul>\n<li><code>0</code>: All tests passed (or were skipped)</li>\n<li><code>1</code>: Any tests failed or errored\nThis allows CI/CD systems to automatically detect test suite failures.</li>\n</ul>\n</li>\n</ol>\n<p>The entire sequence, from CLI invocation to exit, can be visualized in the following diagram:</p>\n<p><img src=\"/api/project/build-test-framework/architecture-doc/asset?path=diagrams%2Fcli-workflow.svg\" alt=\"CLI Workflow\"></p>\n<h3 id=\"data-flow-between-components\">Data Flow Between Components</h3>\n<p>The framework&#39;s components communicate exclusively through well-defined data structures, creating a <strong>contract-based interface</strong> between each stage of the pipeline. This table summarizes the key data transformations as information flows through the system:</p>\n<table>\n<thead>\n<tr>\n<th>Source Component</th>\n<th>Destination Component</th>\n<th>Data Structure</th>\n<th>Purpose</th>\n<th>Key Transformations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>User Terminal</strong></td>\n<td><strong>CLI Parser</strong></td>\n<td>Raw command-line arguments (<code>List[str]</code>)</td>\n<td>User intent in raw form</td>\n<td>None (input)</td>\n</tr>\n<tr>\n<td><strong>CLI Parser</strong></td>\n<td><strong>Discoverer</strong></td>\n<td><code>Configuration</code> object</td>\n<td>Structured settings for the test run</td>\n<td>Raw args → validated, normalized configuration</td>\n</tr>\n<tr>\n<td><strong>Discoverer</strong></td>\n<td><strong>FixtureRegistry</strong></td>\n<td>Module objects + fixture decorators</td>\n<td>Register available fixtures</td>\n<td>Module inspection → <code>Fixture</code> objects</td>\n</tr>\n<tr>\n<td><strong>Discoverer</strong></td>\n<td><strong>Runner</strong></td>\n<td><code>TestSuite</code> containing <code>TestCase</code> objects</td>\n<td>Collection of tests to execute</td>\n<td>Module files → discovered functions → <code>TestCase</code> objects</td>\n</tr>\n<tr>\n<td><strong>FixtureRegistry</strong></td>\n<td><strong>FixtureLifecycleManager</strong></td>\n<td><code>Fixture</code> definitions + dependency graphs</td>\n<td>Knowledge of how to create fixture values</td>\n<td>Registered fixtures → dependency-resolved graph</td>\n</tr>\n<tr>\n<td><strong>Runner</strong></td>\n<td><strong>FixtureLifecycleManager</strong></td>\n<td><code>FixtureRequest</code> objects</td>\n<td>Instructions for specific fixture instantiation</td>\n<td>Test context + fixture name → cacheable request</td>\n</tr>\n<tr>\n<td><strong>FixtureLifecycleManager</strong></td>\n<td><strong>Runner</strong></td>\n<td>Fixture values (any type)</td>\n<td>Resources to inject into tests</td>\n<td>Fixture functions + caching → concrete values</td>\n</tr>\n<tr>\n<td><strong>Test Function</strong></td>\n<td><strong>Assertion Engine</strong></td>\n<td>Actual/expected values + comparison context</td>\n<td>Verification of test conditions</td>\n<td>Test state → comparison operation</td>\n</tr>\n<tr>\n<td><strong>Assertion Engine</strong></td>\n<td><strong>Test Function</strong> (via exception)</td>\n<td><code>AssertionFailure</code> object</td>\n<td>Detailed failure information when assertion fails</td>\n<td>Value comparison → diff calculation → error message</td>\n</tr>\n<tr>\n<td><strong>Runner</strong></td>\n<td><strong>StatisticsCollector</strong></td>\n<td><code>TestResult</code> objects</td>\n<td>Record of test execution outcome</td>\n<td>Test execution + timing → status categorization</td>\n</tr>\n<tr>\n<td><strong>StatisticsCollector</strong></td>\n<td><strong>Reporter</strong></td>\n<td><code>TestRunStatistics</code> + <code>List[TestResult]</code></td>\n<td>Complete results dataset for formatting</td>\n<td>Individual results → aggregated statistics</td>\n</tr>\n<tr>\n<td><strong>Reporter</strong></td>\n<td><strong>User/CI System</strong></td>\n<td>Formatted strings (console/XML)</td>\n<td>Human/machine-readable test report</td>\n<td>Statistics + results → formatted output</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-structure-lifecycle-transformations\">Data Structure Lifecycle Transformations</h4>\n<p>Each major data structure undergoes specific transformations as it flows through the pipeline:</p>\n<p><strong>1. <code>Configuration</code> Evolution:</strong></p>\n<ul>\n<li><strong>Initial State:</strong> Created by <code>parse_cli_args()</code> with user-provided values</li>\n<li><strong>Transformation 1:</strong> Paths are resolved from relative to absolute</li>\n<li><strong>Transformation 2:</strong> Default values are filled in for unspecified options</li>\n<li><strong>Final State:</strong> Complete, validated configuration used by all downstream components</li>\n</ul>\n<p><strong>2. <code>TestCase</code> Enrichment:</strong></p>\n<ul>\n<li><strong>Discovery Phase:</strong> Contains basic metadata (name, function reference, location)</li>\n<li><strong>Fixture Analysis Phase:</strong> <code>fixtures</code> list is populated with required fixture names</li>\n<li><strong>Execution Phase:</strong> Serves as the blueprint for test execution but isn&#39;t modified during execution</li>\n</ul>\n<p><strong>3. <code>TestResult</code> Creation Pipeline:</strong></p>\n<ol>\n<li><strong>Pre-execution:</strong> Runner creates a skeleton <code>TestResult</code> with status <code>PENDING</code></li>\n<li><strong>During execution:</strong> Status updates to <code>RUNNING</code>, timer starts</li>\n<li><strong>Post-execution:</strong> Status set to <code>PASSED</code>/<code>FAILED</code>/<code>ERRORED</code>, duration calculated, error details captured</li>\n<li><strong>Reporting phase:</strong> Incorporated into statistics and formatted for output</li>\n</ol>\n<p><strong>4. <code>FixtureRequest</code> as Cache Key:</strong>\nThe <code>FixtureRequest</code> serves as a <strong>cache key</strong> for fixture values, with its fields carefully chosen to ensure proper scoping:</p>\n<ul>\n<li><code>fixture_name</code>: Which fixture to create</li>\n<li><code>scope</code>: Determines caching lifetime</li>\n<li><code>test_case</code> (or derived <code>cache_key</code>): Provides context for scoping (e.g., module name for MODULE scope)</li>\n<li>The tuple <code>(fixture_name, scope, cache_key)</code> uniquely identifies a fixture instance in the cache</li>\n</ul>\n<h4 id=\"parallel-execution-data-flow\">Parallel Execution Data Flow</h4>\n<p>When parallel execution is enabled (<code>--parallel</code>), the data flow becomes more complex but follows the same conceptual pipeline:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">mermaid</span><pre class=\"arch-pre shiki-highlighted\"><code>graph TD\n    A[Discoverer] --&gt; B[TestSuite]\n    B --&gt; C[Task Queue]\n    C --&gt; D1[Worker Process 1]\n    C --&gt; D2[Worker Process 2]\n    C --&gt; D3[...]\n    D1 --&gt; E1[Local FixtureManager]\n    D2 --&gt; E2[Local FixtureManager]\n    D1 --&gt; F1[Test Execution]\n    D2 --&gt; F2[Test Execution]\n    F1 --&gt; G1[TestResult]\n    F2 --&gt; G2[TestResult]\n    G1 --&gt; H[Result Queue]\n    G2 --&gt; H\n    H --&gt; I[StatisticsCollector]</code></pre></div>\n\n<p>Key adaptations for parallel flow:</p>\n<ul>\n<li><strong>Shared FixtureRegistry:</strong> Read-only access for all workers (fixture definitions are immutable after discovery)</li>\n<li><strong>Per-Worker FixtureLifecycleManager:</strong> Each worker maintains its own cache to avoid synchronization overhead</li>\n<li><strong>Result Queue:</strong> Workers send <code>TestResult</code> objects back to the main process for aggregation</li>\n<li><strong>Scope Coordination:</strong> SESSION-scoped fixtures require special coordination (created once and shared/serialized)</li>\n</ul>\n<blockquote>\n<p><strong>Architecture Decision: Multi-process vs. Multi-thread Parallelism</strong></p>\n<p><strong>Context:</strong> We need to run tests in parallel to reduce total execution time. Python&#39;s Global Interpreter Lock (GIL) limits true parallelism with threads for CPU-bound tasks.</p>\n<p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>Multi-process (ProcessPoolExecutor):</strong> True parallelism, isolated memory spaces</li>\n<li><strong>Multi-thread (ThreadPoolExecutor):</strong> Shared memory, limited by GIL for CPU work</li>\n<li><strong>Async/Coroutine-based:</strong> Single-threaded concurrency good for I/O-bound tests</li>\n</ol>\n<p><strong>Decision:</strong> Multi-process Parallelism</p>\n<p><strong>Rationale:</strong></p>\n<ul>\n<li><strong>Test Isolation Guarantee:</strong> Processes provide stronger isolation than threads (separate memory spaces)</li>\n<li><strong>True Parallelism:</strong> Bypasses GIL limitations for CPU-bound test code</li>\n<li><strong>Failure Containment:</strong> A crashing test in one process won&#39;t bring down the entire test runner</li>\n<li><strong>Simplicity:</strong> <code>concurrent.futures.ProcessPoolExecutor</code> provides a clean API</li>\n</ul>\n<p><strong>Consequences:</strong></p>\n<ul>\n<li>Higher memory usage (each process loads its own copy of modules)</li>\n<li>Fixture values must be serializable to pass between processes</li>\n<li>More complex setup/teardown for process-scoped resources</li>\n<li>Inter-process communication overhead for result collection</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-process</td>\n<td>True parallelism, strong isolation</td>\n<td>Memory overhead, serialization requirements</td>\n<td>✅</td>\n</tr>\n<tr>\n<td>Multi-thread</td>\n<td>Shared memory, low overhead</td>\n<td>GIL limits CPU parallelism, weaker isolation</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Async/Coroutine</td>\n<td>Efficient for I/O-bound tests</td>\n<td>Complex, requires async test functions</td>\n<td>❌</td>\n</tr>\n</tbody></table>\n</blockquote>\n<h4 id=\"error-propagation-through-the-pipeline\">Error Propagation Through the Pipeline</h4>\n<p>The framework maintains clear error boundaries between components, with each component responsible for handling its own class of errors and converting them into appropriate <code>TestResult</code> statuses:</p>\n<table>\n<thead>\n<tr>\n<th>Error Origin</th>\n<th>Detection Point</th>\n<th>Error Type</th>\n<th>Result Status</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CLI Parsing</strong></td>\n<td><code>parse_cli_args()</code></td>\n<td>Invalid arguments</td>\n<td>System exit with error message</td>\n<td>Don&#39;t proceed; print usage</td>\n</tr>\n<tr>\n<td><strong>Module Import</strong></td>\n<td><code>_import_module_from_file()</code></td>\n<td>ImportError, SyntaxError</td>\n<td><code>ERRORED</code> for all tests in that module</td>\n<td>Skip the module, continue with others</td>\n</tr>\n<tr>\n<td><strong>Test Execution</strong></td>\n<td>Runner&#39;s exception handler</td>\n<td>AssertionError</td>\n<td><code>FAILED</code></td>\n<td>Continue to next test</td>\n</tr>\n<tr>\n<td><strong>Test Execution</strong></td>\n<td>Runner&#39;s exception handler</td>\n<td>Any other Exception</td>\n<td><code>ERRORED</code></td>\n<td>Continue to next test</td>\n</tr>\n<tr>\n<td><strong>Fixture Creation</strong></td>\n<td><code>FixtureLifecycleManager.get_fixture_value()</code></td>\n<td>Exception in fixture function</td>\n<td><code>ERRORED</code> for all tests using that fixture</td>\n<td>Mark dependent tests as errored</td>\n</tr>\n<tr>\n<td><strong>Fixture Teardown</strong></td>\n<td><code>FixtureLifecycleManager.teardown_scope()</code></td>\n<td>Exception during cleanup</td>\n<td>Log warning</td>\n<td>Continue teardown for other fixtures</td>\n</tr>\n<tr>\n<td><strong>Output Writing</strong></td>\n<td><code>OutputWriter.write()</code></td>\n<td>IOError, PermissionError</td>\n<td>Log error to stderr</td>\n<td>Attempt to write to alternative location</td>\n</tr>\n</tbody></table>\n<p>This error handling strategy ensures that:</p>\n<ol>\n<li><strong>Fail-fast for configuration errors:</strong> Invalid CLI arguments stop immediately</li>\n<li><strong>Graceful degradation for test errors:</strong> One failing test doesn&#39;t stop the entire suite</li>\n<li><strong>Isolation preservation:</strong> Errors in one module don&#39;t affect tests in other modules</li>\n<li><strong>Clean reporting:</strong> All errors are captured and reported in the final output</li>\n</ol>\n<h4 id=\"data-flow-example-complete-trace\">Data Flow Example: Complete Trace</h4>\n<p>Let&#39;s trace a complete example with concrete data transformations:</p>\n<p><strong>User Command:</strong> <code>apollo tests/math/ --verbose --junit-xml=results.xml</code></p>\n<table>\n<thead>\n<tr>\n<th>Step</th>\n<th>Component</th>\n<th>Input Data</th>\n<th>Output Data</th>\n<th>Transformation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>CLI Parser</td>\n<td><code>[&quot;tests/math/&quot;, &quot;--verbose&quot;, &quot;--junit-xml=results.xml&quot;]</code></td>\n<td><code>Configuration(file_patterns=[&quot;tests/math/&quot;], verbose=True, junit_xml_path=Path(&quot;results.xml&quot;), ...)</code></td>\n<td>String parsing → structured object</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Discoverer</td>\n<td><code>Configuration</code> + directory scan</td>\n<td><code>TestSuite(tests=[TestCase(nodeid=&quot;math/test_arithmetic.py::test_addition&quot;, ...), ...])</code></td>\n<td>File pattern → module imports → test discovery</td>\n</tr>\n<tr>\n<td>3</td>\n<td>FixtureRegistry</td>\n<td>Module objects from discovery</td>\n<td>Registered: <code>db_connection</code> (MODULE scope), <code>temp_dir</code> (FUNCTION scope)</td>\n<td>Decorator scanning → fixture registration</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Runner</td>\n<td><code>TestSuite</code> + <code>FixtureRegistry</code></td>\n<td>For each test: calls test function with fixture values</td>\n<td>Parameter matching → fixture value injection</td>\n</tr>\n<tr>\n<td>5</td>\n<td>Assertion Engine</td>\n<td><code>assert_equal(calculate(2,2), 5)</code> → <code>actual=4, expected=5</code></td>\n<td><code>AssertionFailure(message=&quot;Expected 5 but got 4&quot;, expected=5, actual=4, diff=&quot;-4\\n+5&quot;)</code></td>\n<td>Value comparison → diff generation</td>\n</tr>\n<tr>\n<td>6</td>\n<td>Runner</td>\n<td>Test execution outcome</td>\n<td><code>TestResult(status=FAILED, message=&quot;Expected 5 but got 4&quot;, duration=0.12, ...)</code></td>\n<td>Exception capture → result packaging</td>\n</tr>\n<tr>\n<td>7</td>\n<td>StatisticsCollector</td>\n<td>Stream of <code>TestResult</code> objects</td>\n<td><code>TestRunStatistics(total=10, passed=8, failed=1, errored=1, total_time=2.45, ...)</code></td>\n<td>Aggregation → statistics calculation</td>\n</tr>\n<tr>\n<td>8</td>\n<td>Reporter</td>\n<td><code>TestRunStatistics</code> + all <code>TestResult</code> objects</td>\n<td>Console output + <code>results.xml</code> file</td>\n<td>Formatting → rendering</td>\n</tr>\n<tr>\n<td>9</td>\n<td>System</td>\n<td>Exit code determination</td>\n<td>Process exits with code <code>1</code> (because tests failed)</td>\n<td>Status aggregation → exit code</td>\n</tr>\n</tbody></table>\n<p>This data flow creates a <strong>predictable, debuggable pipeline</strong> where each component has clear responsibilities and well-defined interfaces. The separation allows for independent development and testing of each component, which is particularly valuable for an educational project where learners implement one milestone at a time.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Pipeline Coordination</strong></td>\n<td>Linear procedural code in <code>main()</code></td>\n<td><code>asyncio</code> event loop with async/await</td>\n</tr>\n<tr>\n<td><strong>Parallel Execution</strong></td>\n<td><code>concurrent.futures.ProcessPoolExecutor</code></td>\n<td>Custom process management with <code>multiprocessing</code></td>\n</tr>\n<tr>\n<td><strong>Data Serialization</strong></td>\n<td><code>pickle</code> for inter-process communication</td>\n<td><code>marshal</code> or custom serialization for performance</td>\n</tr>\n<tr>\n<td><strong>Dependency Resolution</strong></td>\n<td>Recursive depth-first search</td>\n<td>Topological sort for detecting cycles earlier</td>\n</tr>\n<tr>\n<td><strong>Result Aggregation</strong></td>\n<td>In-memory list collection</td>\n<td>Streaming to disk for very large test suites</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Add the following files to implement the pipeline coordination:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n  ├── __main__.py              # CLI entry point with main pipeline\n  ├── pipeline.py              # Pipeline coordination logic\n  ├── orchestrator.py          # High-level test run orchestration\n  └── utils/\n      └── parallel.py          # Parallel execution utilities</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>File: <code>apollo/pipeline.py</code></strong> - Complete pipeline coordinator:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Pipeline coordination for the test framework.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Implements the linear pipeline architecture connecting all components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Configuration, parse_cli_args</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .discovery </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> discover_tests, TestSuite, TestCase</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .fixtures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> FixtureRegistry, FixtureLifecycleManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .runner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleRunner</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .reporting </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> StatisticsCollector, ConsoleFormatter, JUnitFormatter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .utils.parallel </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParallelRunner</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates the complete test execution pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config: Optional[Configuration] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suite: Optional[TestSuite] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fixture_registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FixtureRegistry()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fixture_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FixtureLifecycleManager(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.fixture_registry)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats_collector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StatisticsCollector()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.end_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_from_cli</span><span style=\"color:#E1E4E8\">(self, args: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute the complete pipeline from CLI arguments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Phase 1: Parse configuration</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parse_cli_args(args)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Phase 2: Discover tests and fixtures</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.suite </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._discover_tests()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Phase 3: Execute tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            results </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._execute_tests()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Phase 4: Report results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exit_code </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._report_results(results)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> exit_code</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> KeyboardInterrupt</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n\\n</span><span style=\"color:#9ECBFF\">Test run interrupted by user\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">file</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">sys.stderr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 130</span><span style=\"color:#6A737D\">  # Standard interrupt exit code</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Fatal error in test pipeline: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">file</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">sys.stderr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.verbose:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                traceback.print_exc()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _discover_tests</span><span style=\"color:#E1E4E8\">(self) -> TestSuite:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Discover all tests and fixtures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Configuration not set\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Discovering tests in </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.start_dir</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Discover test functions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        test_cases: List[TestCase] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> file_path </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.file_patterns:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            discovered </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> discover_tests(file_path, </span><span style=\"color:#FFAB70\">pattern</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"test_*.py\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            test_cases.extend(discovered)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Scan for fixtures in the same modules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # (Implementation detail: we need to track which modules were scanned)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> TestSuite(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tests</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">test_cases)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _execute_tests</span><span style=\"color:#E1E4E8\">(self) -> List[TestResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all tests in the suite.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">or</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suite:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Pipeline not properly initialized\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.parallel:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            runner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ParallelRunner(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.max_workers,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                fixture_manager</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.fixture_manager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            runner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleRunner(</span><span style=\"color:#FFAB70\">fixture_manager</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.fixture_manager)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> runner.run_suite(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.suite)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.end_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _report_results</span><span style=\"color:#E1E4E8\">(self, results: List[TestResult]) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate and output reports, return exit code.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Configuration not set\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add all results to statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> results:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.stats_collector.add_result(result)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats_collector.finalize()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Generate console output</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.output_format </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"console\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"both\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            console_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ConsoleFormatter.format(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.stats_collector.stats, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                verbose</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.verbose,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                show_durations</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.show_durations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(console_output)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Generate JUnit XML</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.output_format </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"junit\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"both\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.junit_xml_path:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            xml_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JUnitFormatter.format(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.stats_collector.stats,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                suite_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Apollo Test Run\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.config.junit_xml_path.write_text(xml_output)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.verbose:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">JUnit XML written to: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.junit_xml_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Determine exit code</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stats_collector.stats.failed </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stats_collector.stats.errored </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main entry point for the CLI.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestPipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exit_code </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pipeline.run_from_cli(sys.argv[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sys.exit(exit_code)</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>File: <code>apollo/orchestrator.py</code></strong> - High-level orchestration with detailed TODOs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">High-level test orchestration with detailed step-by-step implementation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestOrchestrator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates test discovery, fixture setup, execution, and reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_test_run</span><span style=\"color:#E1E4E8\">(self, config: Configuration) -> TestRunStatistics:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a complete test run from configuration to statistics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            config: The configuration for this test run</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Complete statistics about the test run</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> Implementation Steps:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        1.  Validate the configuration (check paths exist, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        2.  Initialize the StatisticsCollector and record start_time</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        3.  Call resolve_patterns_to_paths() to convert patterns to concrete file paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        4.  For each file path:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            a. Convert to module name using module_path_to_name()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            b. Import the module using _import_module_from_file()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            c. Scan for fixtures with FixtureRegistry.scan_module()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            d. Discover tests with _find_tests_in_module()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        5.  Build TestSuite from all discovered TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        6.  Analyze each TestCase's parameters to determine fixture dependencies</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        7.  Initialize appropriate Runner (SimpleRunner or ParallelRunner)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        8.  Execute the suite with runner.run_suite(), collecting TestResult objects</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        9.  For each TestResult, add to StatisticsCollector</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        10. Finalize statistics (calculate durations, slowest tests, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        11. Return the TestRunStatistics object</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize StatisticsCollector and record start_time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Convert patterns to paths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Process each module (discover tests and fixtures)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Build TestSuite</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Analyze fixture dependencies for each test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Initialize appropriate Runner</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Execute suite and collect results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Add results to statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Finalize statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Return statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _discover_tests_and_fixtures</span><span style=\"color:#E1E4E8\">(self, file_paths: List[Path]) -> Tuple[List[TestCase], FixtureRegistry]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Discover all tests and fixtures from the given file paths.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> Implementation Steps:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        1. Create empty FixtureRegistry</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        2. Create empty list for TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        3. For each file_path in file_paths:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            a. Convert to module name (strip .py, replace / with .)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            b. Try to import module; on ImportError, skip with warning</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            c. Call registry.scan_module(module) to find fixtures</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            d. Call _find_tests_in_module(module) to find tests</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            e. Extend test_cases list with discovered tests</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        4. Return (test_cases, registry)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create FixtureRegistry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create empty test_cases list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Process each file path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _analyze_fixture_dependencies</span><span style=\"color:#E1E4E8\">(self, test_case: TestCase, registry: FixtureRegistry) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze a test case's parameters and update its fixtures list.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> Implementation Steps:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        1. Use inspect.signature() to get test function parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        2. For each parameter name:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            a. Check if registry.get(name) returns a fixture</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            b. If yes, add fixture name to test_case.fixtures list</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        3. Handle edge cases:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Parameters with default values (ignore - they're not fixtures)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - *args and **kwargs parameters (ignore)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Parameters that don't match any fixture (could be error or intentional)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get function signature</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check each parameter against registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update test_case.fixtures list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<p><strong>Python-Specific Implementation Tips:</strong></p>\n<ol>\n<li><strong>Module Import:</strong> Use <code>importlib.util.spec_from_file_location()</code> and <code>importlib.util.module_from_spec()</code> for clean dynamic imports.</li>\n<li><strong>Parallel Execution:</strong> Use <code>concurrent.futures.ProcessPoolExecutor</code> with <code>max_workers=None</code> to use all CPU cores.</li>\n<li><strong>Fixture Serialization:</strong> For parallel execution, fixtures must be picklable. Use <code>__getstate__</code> and <code>__setstate__</code> for custom serialization.</li>\n<li><strong>Error Isolation:</strong> Use <code>sys.excepthook</code> to catch unhandled exceptions in worker processes.</li>\n<li><strong>Resource Cleanup:</strong> Use <code>atexit</code> handlers or context managers to ensure cleanup even on interruption.</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the pipeline coordination, verify with:</p>\n<p><strong>Command:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo</span><span style=\"color:#9ECBFF\"> tests/examples/</span><span style=\"color:#79B8FF\"> --verbose</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Discovering tests in /path/to/tests/examples/...\nFound 3 test(s) in 1 module(s)\n\nRunning tests...\ntest_addition (math.test_arithmetic) ... PASSED (0.002s)\ntest_subtraction (math.test_arithmetic) ... FAILED (0.001s)\ntest_multiplication (math.test_arithmetic) ... PASSED (0.003s)\n\nFAILURES:\ntest_subtraction (math.test_arithmetic)\n  AssertionError: Expected 1 but got -1\n  File &quot;tests/examples/math/test_arithmetic.py&quot;, line 15\n  assert_equal(calculate(1, 2), 1)\n\nSummary:\n3 tests, 2 passed, 1 failed, 0 errored, 0 skipped in 0.015s</code></pre></div>\n\n<p><strong>Verification Checklist:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> All phases execute in correct order (discovery → execution → reporting)</li>\n<li><input disabled=\"\" type=\"checkbox\"> Test isolation: Failure in one test doesn&#39;t affect others</li>\n<li><input disabled=\"\" type=\"checkbox\"> Timing is captured and displayed</li>\n<li><input disabled=\"\" type=\"checkbox\"> Exit code is 1 when tests fail</li>\n<li><input disabled=\"\" type=\"checkbox\"> Verbose flag produces detailed output</li>\n</ul>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tests run but fixtures aren&#39;t injected</strong></td>\n<td>Parameter name doesn&#39;t match fixture name</td>\n<td>Add debug logging to <code>_analyze_fixture_dependencies()</code></td>\n<td>Ensure parameter names exactly match fixture names</td>\n</tr>\n<tr>\n<td><strong>Parallel tests hang indefinitely</strong></td>\n<td>Deadlock in shared resource</td>\n<td>Add timeout to <code>ProcessPoolExecutor.submit()</code></td>\n<td>Use <code>concurrent.futures.as_completed()</code> with timeout</td>\n</tr>\n<tr>\n<td><strong>Fixture teardown runs multiple times</strong></td>\n<td>Incorrect scope caching</td>\n<td>Log cache hits/misses in <code>FixtureLifecycleManager</code></td>\n<td>Ensure cache key includes proper scope context</td>\n</tr>\n<tr>\n<td><strong>Import errors skip entire module</strong></td>\n<td>Exception not caught during discovery</td>\n<td>Wrap <code>_import_module_from_file()</code> in try-except</td>\n<td>Catch <code>ImportError</code> and log warning instead of crashing</td>\n</tr>\n<tr>\n<td><strong>JUnit XML contains invalid characters</strong></td>\n<td>Test output has control characters</td>\n<td>Check <code>_escape_xml()</code> function</td>\n<td>Add filtering for non-printable characters</td>\n</tr>\n<tr>\n<td><strong>Exit code is 0 when tests failed</strong></td>\n<td>Exit code logic error</td>\n<td>Add debug print of statistics before exit</td>\n<td>Ensure exit code checks both <code>failed</code> and <code>errored</code> counts</td>\n</tr>\n</tbody></table>\n<h2 id=\"10-error-handling-and-edge-cases\">10. Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All four milestones, as error handling must be designed into every component from discovery through reporting.</p>\n</blockquote>\n<p>Every complex system must anticipate and gracefully handle failure conditions, and a test framework is no exception. The <strong>Error Handling and Edge Cases</strong> design defines how the framework responds to unexpected situations—from test failures (which are expected) to system-level errors that could crash the entire test run. The mental model here is that of a <strong>resilient conductor</strong>: when a musician (test) plays a wrong note (assertion failure), the conductor notes it and continues; when an instrument breaks (fixture error), the conductor isolates the problem and proceeds with other musicians; when the stage collapses (system error), the conductor safely evacuates and provides a clear incident report.</p>\n<p>This section catalogs the failure modes the framework will encounter and establishes clear recovery strategies and reporting formats for each. A well-designed error handling system ensures that tests can fail without taking down the entire test suite, provides actionable diagnostic information, and maintains the integrity of test isolation.</p>\n<h3 id=\"common-failure-modes\">Common Failure Modes</h3>\n<p>The framework must distinguish between different categories of errors, as each requires different handling and reporting. The following table categorizes the primary failure modes:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Category</th>\n<th>Description</th>\n<th>Typical Triggers</th>\n<th>Expected Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Test Failures</strong></td>\n<td>Expected assertion failures within test logic</td>\n<td><code>assert_equal(actual, expected)</code> when values differ, <code>assert_true(condition)</code> when condition is False</td>\n<td>Common (part of normal test execution)</td>\n</tr>\n<tr>\n<td><strong>Test Errors</strong></td>\n<td>Unexpected exceptions during test execution</td>\n<td>Missing imports, syntax errors in test code, runtime exceptions (e.g., <code>KeyError</code>, <code>AttributeError</code>)</td>\n<td>Occasional (developer mistakes)</td>\n</tr>\n<tr>\n<td><strong>Discovery Errors</strong></td>\n<td>Problems during test discovery phase</td>\n<td>Invalid module paths, import errors in test files, malformed test functions</td>\n<td>Occasional during development</td>\n</tr>\n<tr>\n<td><strong>Fixture Lifecycle Errors</strong></td>\n<td>Problems during fixture setup, execution, or teardown</td>\n<td>Setup exceptions, teardown exceptions, circular dependencies, scope violations</td>\n<td>Infrequent but critical</td>\n</tr>\n<tr>\n<td><strong>System-Level Errors</strong></td>\n<td>Framework or environmental failures</td>\n<td>Memory exhaustion, disk full, signal interruptions (Ctrl+C), internal framework bugs</td>\n<td>Rare</td>\n</tr>\n</tbody></table>\n<h4 id=\"test-failures-vs-test-errors\">Test Failures vs. Test Errors</h4>\n<p>A critical architectural distinction is between <strong>test failures</strong> (expected) and <strong>test errors</strong> (unexpected):</p>\n<ul>\n<li><p><strong>Test Failure</strong>: Occurs when a test&#39;s assertion does not hold true. This is a <em>normal outcome</em>—the test framework is doing its job by detecting that expected behavior differs from actual behavior. The framework must capture the assertion details (expected vs. actual) and continue executing other tests.</p>\n</li>\n<li><p><strong>Test Error</strong>: Occurs when an unanticipated exception is raised during test execution (outside of assertion checks). This indicates a problem with the test code itself or its dependencies (e.g., <code>NameError</code>, <code>ImportError</code>, <code>TypeError</code>). The framework must capture the exception and traceback, mark the test as errored, and continue with other tests.</p>\n</li>\n</ul>\n<blockquote>\n<p><strong>Architecture Decision: Distinguishing Test Failures from Errors</strong></p>\n<ul>\n<li><strong>Context</strong>: The framework must report different outcomes for assertion failures (expected) versus unexpected exceptions (errors) to give developers clear signals about what went wrong.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Single outcome type</strong>: Treat all non-passing tests as &quot;failed&quot; regardless of cause.</li>\n<li><strong>Separate failure and error statuses</strong>: Distinguish between assertion failures (FAILED) and unexpected exceptions (ERRORED).</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Separate failure and error statuses (<code>TestStatus.FAILED</code> vs <code>TestStatus.ERRORED</code>).</li>\n<li><strong>Rationale</strong>: This distinction provides crucial diagnostic information. A failure indicates the code under test didn&#39;t meet expectations; an error indicates the test itself is broken. CI systems often treat errors and failures differently (e.g., blocking merge vs. warning).</li>\n<li><strong>Consequences</strong>: The <code>TestResult</code> structure must capture both assertion failures (via <code>AssertionFailure</code>) and unexpected exceptions (via <code>exception</code> and <code>traceback</code> fields). Reporting must visually distinguish between failures and errors.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single outcome type</td>\n<td>Simpler implementation, fewer statuses to manage</td>\n<td>Loses diagnostic information, harder to debug test problems</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Separate statuses</td>\n<td>Clearer diagnostics, aligns with industry practice (pytest, unittest)</td>\n<td>Slightly more complex status handling</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"discovery-errors\">Discovery Errors</h4>\n<p>Discovery errors occur when the framework cannot successfully scan for or load tests. These are particularly problematic because they prevent tests from even being attempted. Common scenarios include:</p>\n<ol>\n<li><strong>Invalid file patterns</strong>: User provides patterns that match no files</li>\n<li><strong>Import errors</strong>: Test modules have syntax errors or missing dependencies</li>\n<li><strong>Permission errors</strong>: Cannot read test files due to filesystem permissions</li>\n<li><strong>Circular imports</strong>: Test modules import each other creating import loops</li>\n</ol>\n<p>The framework must decide whether to treat discovery errors as fatal (stop execution) or recoverable (skip problematic modules and continue).</p>\n<h4 id=\"fixture-lifecycle-errors\">Fixture Lifecycle Errors</h4>\n<p>The fixture system introduces several new failure modes due to its complex lifecycle management:</p>\n<ol>\n<li><strong>Setup failures</strong>: Exception during fixture function execution</li>\n<li><strong>Teardown failures</strong>: Exception during fixture cleanup (even when test passed)</li>\n<li><strong>Circular dependencies</strong>: Fixture A depends on B, B depends on A (directly or indirectly)</li>\n<li><strong>Scope violations</strong>: Test attempts to use a fixture with incompatible scope</li>\n<li><strong>Generator fixture misuse</strong>: Fixture yields multiple times or doesn&#39;t yield at all</li>\n</ol>\n<p>Fixture errors are particularly tricky because they can affect multiple tests and may leave resources in an inconsistent state if not handled properly.</p>\n<h4 id=\"system-level-errors\">System-Level Errors</h4>\n<p>These are catastrophic failures that threaten the entire test run:</p>\n<ol>\n<li><strong>Memory exhaustion</strong>: Test suite or fixture consumes all available memory</li>\n<li><strong>Disk full</strong>: Cannot write test reports or temporary files</li>\n<li><strong>Signal interruptions</strong>: User presses Ctrl+C (SIGINT) or process receives SIGTERM</li>\n<li><strong>Framework bugs</strong>: Internal errors in the test framework itself</li>\n<li><strong>Worker process crashes</strong>: In parallel execution, a worker process dies unexpectedly</li>\n</ol>\n<blockquote>\n<p><strong>Key Insight</strong>: The framework must maintain the <strong>fail-fast vs. fail-safe</strong> balance. For test-level issues (failures, errors), continue running other tests (fail-safe). For system-level issues, terminate gracefully with clear diagnostics (fail-fast for catastrophic issues).</p>\n</blockquote>\n<h3 id=\"recovery-amp-reporting-strategy\">Recovery &amp; Reporting Strategy</h3>\n<p>For each failure category, the framework must implement a specific recovery strategy and ensure error information is captured and reported clearly. The following table defines the behavior for each failure mode:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Point</th>\n<th>Recovery Strategy</th>\n<th>Reporting Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Assertion Failure</strong></td>\n<td>During test execution in <code>assert_*</code> functions</td>\n<td>Continue test execution to completion (test marked as FAILED), then continue with next test</td>\n<td>Include <code>AssertionFailure</code> details: message, expected/actual values, diff, hint</td>\n</tr>\n<tr>\n<td><strong>Test Error (Unexpected Exception)</strong></td>\n<td>During test execution (outside assertions)</td>\n<td>Catch exception, mark test as ERRORED, continue with next test</td>\n<td>Include exception type, message, and full traceback in <code>TestResult</code></td>\n</tr>\n<tr>\n<td><strong>Discovery Import Error</strong></td>\n<td>During <code>_import_module_from_file()</code> in discovery</td>\n<td>Skip problematic module, log warning, continue discovering other modules</td>\n<td>Report in summary as &quot;discovery errors&quot; with count and module names</td>\n</tr>\n<tr>\n<td><strong>Fixture Setup Failure</strong></td>\n<td>During <code>FixtureLifecycleManager.get_fixture_value()</code></td>\n<td>Mark all dependent tests as ERRORED, continue with tests not depending on failed fixture</td>\n<td>Include fixture name and setup error in test result message</td>\n</tr>\n<tr>\n<td><strong>Fixture Teardown Failure</strong></td>\n<td>During <code>FixtureLifecycleManager.teardown_scope()</code></td>\n<td>Log error but don&#39;t affect test outcomes (test already completed), attempt to teardown other fixtures</td>\n<td>Report in summary as &quot;teardown errors&quot; with fixture names and errors</td>\n</tr>\n<tr>\n<td><strong>Circular Dependency</strong></td>\n<td>During fixture registration or dependency resolution in <code>FixtureRegistry</code></td>\n<td>Raise error during discovery phase, prevent test execution</td>\n<td>Fail fast with clear error message showing dependency cycle</td>\n</tr>\n<tr>\n<td><strong>Parallel Worker Crash</strong></td>\n<td>During <code>SimpleRunner.run_suite()</code> with parallel execution</td>\n<td>Mark tests assigned to crashed worker as ERRORED, continue with remaining tests</td>\n<td>Report worker crash in summary with affected test count</td>\n</tr>\n<tr>\n<td><strong>User Interruption (Ctrl+C)</strong></td>\n<td>Signal handler or keyboard interrupt in main execution loop</td>\n<td>Stop test execution gracefully, run pending fixture teardowns, exit with non-zero code</td>\n<td>Report &quot;interrupted by user&quot; in summary with partial results</td>\n</tr>\n<tr>\n<td><strong>Report Generation Error</strong></td>\n<td>During <code>JUnitFormatter.format()</code> or file writing</td>\n<td>Fall back to console output only, exit with error code</td>\n<td>Print error to stderr about report failure</td>\n</tr>\n</tbody></table>\n<h4 id=\"error-state-transitions\">Error State Transitions</h4>\n<p>The <code>TestResult</code> state machine from previous sections must handle error transitions. Below is the complete state transition table including error conditions:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PENDING</code></td>\n<td><code>test_started</code></td>\n<td><code>RUNNING</code></td>\n<td>Record start time, initialize fixtures</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>assertion_passed</code></td>\n<td><code>RUNNING</code></td>\n<td>Continue test execution (no state change)</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>assertion_failed</code></td>\n<td><code>FAILED</code></td>\n<td>Capture <code>AssertionFailure</code>, stop test execution, run test teardown</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>exception_raised</code> (non-assertion)</td>\n<td><code>ERRORED</code></td>\n<td>Capture exception and traceback, stop test execution, run test teardown</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>test_completed</code></td>\n<td><code>PASSED</code></td>\n<td>Record end time, calculate duration, run test teardown</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>test_skipped</code></td>\n<td><code>SKIPPED</code></td>\n<td>Record skip reason, run test teardown</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>fixture_setup_failed</code></td>\n<td><code>ERRORED</code></td>\n<td>Capture fixture error, stop test execution, attempt fixture teardown</td>\n</tr>\n<tr>\n<td>Any state</td>\n<td><code>interrupted</code></td>\n<td><code>ERRORED</code></td>\n<td>Mark as interrupted, attempt cleanup if possible</td>\n</tr>\n</tbody></table>\n<h4 id=\"error-message-design-principles\">Error Message Design Principles</h4>\n<p>Clear error messages are critical for developer productivity. The framework follows these principles:</p>\n<ol>\n<li><strong>Actionable</strong>: Tell the developer what went wrong and suggest possible fixes</li>\n<li><strong>Contextual</strong>: Include relevant context (test name, fixture name, values)</li>\n<li><strong>Hierarchical</strong>: Show root cause first, then supporting details</li>\n<li><strong>Consistent</strong>: Use consistent formatting and terminology across error types</li>\n<li><strong>Non-overwhelming</strong>: Provide essential information without excessive verbosity</li>\n</ol>\n<p>For example, an assertion failure message should show:</p>\n<ul>\n<li>What assertion failed (e.g., &quot;assert_equal failed&quot;)</li>\n<li>Expected and actual values (formatted for readability)</li>\n<li>A diff highlighting differences (for strings/collections)</li>\n<li>Additional context like variable names when available</li>\n</ul>\n<h4 id=\"fixture-error-handling-deep-dive\">Fixture Error Handling Deep Dive</h4>\n<p>Fixture errors require special handling due to their potential to affect multiple tests. Consider this scenario: a module-scoped fixture fails during setup. The framework must:</p>\n<ol>\n<li>Detect the exception in <code>FixtureLifecycleManager.get_fixture_value()</code></li>\n<li>Mark the fixture as &quot;failed&quot; in the cache (store the exception)</li>\n<li>For each test that depends on this fixture:<ul>\n<li>Immediately mark the test as <code>ERRORED</code> without executing it</li>\n<li>Include a message indicating which fixture failed and why</li>\n</ul>\n</li>\n<li>Continue executing tests that don&#39;t depend on the failed fixture</li>\n<li>Skip teardown for the failed fixture (it never successfully setup)</li>\n</ol>\n<blockquote>\n<p><strong>Architecture Decision: Fixture Teardown Error Handling</strong></p>\n<ul>\n<li><strong>Context</strong>: Fixture teardown failures occur after test execution completes, potentially leaving resources leaked.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Ignore teardown errors</strong>: Log but don&#39;t affect test outcomes</li>\n<li><strong>Treat as test errors</strong>: Mark associated tests as failed/errored</li>\n<li><strong>Propagate as separate error category</strong>: Track separately from test outcomes</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Log teardown errors but don&#39;t affect test outcomes (Option 1).</li>\n<li><strong>Rationale</strong>: Teardown happens after test verification, so test results should reflect the code-under-test behavior, not cleanup issues. However, teardown failures must be reported to prevent resource leaks.</li>\n<li><strong>Consequences</strong>: The framework needs separate tracking for teardown errors in the <code>TestRunStatistics</code>, and must ensure teardown exceptions don&#39;t crash the framework.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ignore teardown errors</td>\n<td>Clean separation of concerns, test outcomes reflect code behavior</td>\n<td>Resource leaks might go unnoticed, inconsistent state between tests</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Treat as test errors</td>\n<td>Encourages proper cleanup, visible in test results</td>\n<td>Punishes tests for cleanup issues unrelated to code under test</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>Propagate as separate category</td>\n<td>Clear visibility without affecting test outcomes</td>\n<td>More complex reporting, additional data structure needed</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h4 id=\"parallel-execution-error-handling\">Parallel Execution Error Handling</h4>\n<p>When tests run in parallel, error handling becomes more complex:</p>\n<ol>\n<li><strong>Worker process isolation</strong>: Each worker runs in its own process to ensure test isolation</li>\n<li><strong>Error propagation</strong>: Worker errors must be communicated back to the main process</li>\n<li><strong>Resource cleanup</strong>: Failed workers must not leak resources or orphan processes</li>\n</ol>\n<p>The parallel execution strategy uses the following error handling approach:</p>\n<ol>\n<li>Each worker process catches exceptions and returns structured error information</li>\n<li>The main process collects worker results, mapping errors back to originating tests</li>\n<li>If a worker process crashes unexpectedly (e.g., segmentation fault), the main process detects the crash via broken pipe or timeout and marks all tests assigned to that worker as <code>ERRORED</code></li>\n<li>Worker processes use <code>atexit</code> handlers to attempt cleanup on abnormal exit</li>\n</ol>\n<h4 id=\"common-pitfalls-in-error-handling\">Common Pitfalls in Error Handling</h4>\n<p>⚠️ <strong>Pitfall: Swallowing exceptions in fixture teardown</strong></p>\n<ul>\n<li><strong>Description</strong>: Using a bare <code>except:</code> clause in fixture teardown that catches and ignores all exceptions</li>\n<li><strong>Why it&#39;s wrong</strong>: Critical errors (KeyboardInterrupt, SystemExit) get swallowed, preventing proper shutdown</li>\n<li><strong>How to fix</strong>: Catch specific exception types (<code>Exception</code>) or re-raise critical exceptions:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">  try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      yield</span><span style=\"color:#E1E4E8\"> resource</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          cleanup(resource)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:  </span><span style=\"color:#6A737D\"># Don't catch KeyboardInterrupt, SystemExit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          log_teardown_error(e)</span></span></code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Not isolating test errors in parallel execution</strong></p>\n<ul>\n<li><strong>Description</strong>: When one test causes a worker process to crash, all tests in that worker fail</li>\n<li><strong>Why it&#39;s wrong</strong>: Loss of test isolation, one buggy test hides other test results</li>\n<li><strong>How to fix</strong>: Implement per-test exception boundaries within workers, or run each test in a subprocess</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Incomplete error information in assertion failures</strong></p>\n<ul>\n<li><strong>Description</strong>: Assertion messages like &quot;assertion failed&quot; without showing values</li>\n<li><strong>Why it&#39;s wrong</strong>: Developer must add debugging prints to understand failure</li>\n<li><strong>How to fix</strong>: Always include expected and actual values in assertion failure messages</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Discovery errors stopping entire test run</strong></p>\n<ul>\n<li><strong>Description</strong>: One malformed test file prevents discovery of all other tests</li>\n<li><strong>Why it&#39;s wrong</strong>: Reduces developer productivity, fails fast for wrong reason</li>\n<li><strong>How to fix</strong>: Continue discovery after import errors, report problematic modules separately</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations-table\">Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Exception handling</td>\n<td>Python built-in <code>try/except/finally</code></td>\n<td>Structured error types with error codes</td>\n</tr>\n<tr>\n<td>Parallel error handling</td>\n<td><code>multiprocessing.Pool</code> with error callbacks</td>\n<td><code>concurrent.futures</code> with detailed error propagation</td>\n</tr>\n<tr>\n<td>Signal handling</td>\n<td>Simple <code>signal.signal()</code> for SIGINT</td>\n<td><code>signal</code> module with proper signal masking in workers</td>\n</tr>\n<tr>\n<td>Error serialization</td>\n<td>Pickle for worker-process communication</td>\n<td>Custom serialization with error type preservation</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n  framework/\n    errors.py                    # Error type definitions and utilities\n    discovery/\n      discoverer.py              # Discovery with error handling\n      errors.py                  # Discovery-specific errors\n    runner/\n      simple_runner.py           # Test runner with error handling\n      parallel_runner.py         # Parallel runner with worker error handling\n      errors.py                  # Runner-specific errors\n    fixtures/\n      lifecycle_manager.py       # Fixture error handling\n      errors.py                  # Fixture-specific errors\n    reporting/\n      formatters.py              # Error message formatting\n      statistics.py              # Error statistics collection</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete error type definitions (<code>framework/errors.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Framework error types and utilities.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ApolloError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all framework errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DiscoveryError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ApolloError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error during test discovery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, file_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.file_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> file_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FixtureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ApolloError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error related to fixtures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, fixture_name: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fixture_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fixture_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircularDependencyError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">FixtureError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Circular dependency detected in fixtures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ScopeViolationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">FixtureError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Fixture scope violation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FormattedError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Structured error information for reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    type_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    traceback: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context: Optional[</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert to dictionary for serialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.type_name,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.message,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"traceback\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.traceback,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"context\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.context </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> capture_exception</span><span style=\"color:#E1E4E8\">(exc: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> FormattedError:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Capture an exception into a FormattedError.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exc_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> type</span><span style=\"color:#E1E4E8\">(exc).</span><span style=\"color:#79B8FF\">__name__</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exc_msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(exc) </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> repr</span><span style=\"color:#E1E4E8\">(exc)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get traceback if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tb </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> sys.exc_info()[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tb </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">.join(traceback.format_exception(</span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">(exc), exc, exc.</span><span style=\"color:#79B8FF\">__traceback__</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Extract context from exception attributes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> attr </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"fixture_name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"file_path\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"test_name\"</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(exc, attr):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            context[attr] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(exc, attr)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> FormattedError(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        type_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">exc_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        message</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">exc_msg,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        traceback</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tb,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        context</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Error-handling test runner skeleton (<code>runner/simple_runner.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Test runner with comprehensive error handling.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> capture_exception</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase, TestResult, TestStatus</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RunContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context for test execution with error tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_case: TestCase</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error: Optional[</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assertion_failure: Optional[</span><span style=\"color:#79B8FF\">AssertionError</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fixtures_loaded: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleRunner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Execute tests with proper error isolation and handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_test</span><span style=\"color:#E1E4E8\">(self, test_case: TestCase) -> TestResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a single test and return its result with error handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Initialize RunContext with test_case and start_time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Set TestResult status to RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Try to load required fixtures using FixtureLifecycleManager</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - If fixture setup fails: catch FixtureError, set status to ERRORED</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Store fixture names in context.fixtures_loaded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: If fixtures loaded successfully, execute test function</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Wrap in try-except to catch AssertionError (test failure)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Wrap in another try-except to catch other Exception (test error)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - If KeyboardInterrupt: re-raise to allow framework shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: After test execution (success or error), run teardown</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Call FixtureLifecycleManager.teardown_scope for function-scoped fixtures</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Catch and log teardown errors (don't affect test status)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: Determine final TestStatus:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - If assertion_failure: FAILED</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - If error (non-assertion): ERRORED</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - If no exceptions: PASSED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 7: Build TestResult with appropriate message and traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 8: Calculate duration and return TestResult</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement the above steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_suite</span><span style=\"color:#E1E4E8\">(self, suite: TestSuite) -> List[TestResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Run all tests in a suite and return their results.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Initialize empty list for results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: For each test in suite.tests:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Call run_test(test_case)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - Append result to results list</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - If KeyboardInterrupt caught: break loop, run cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Return results list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Ensure tests run in isolation (no shared state between iterations)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement the above steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Parallel runner error handling skeleton (<code>runner/parallel_runner.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Parallel test runner with worker error handling.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> multiprocessing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .simple_runner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleRunner, TestResult</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestSuite</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParallelRunner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Run tests in parallel processes with error isolation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_suite</span><span style=\"color:#E1E4E8\">(self, suite: TestSuite, max_workers: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[TestResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Run tests in parallel using worker processes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Split test suite into chunks for each worker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Create multiprocessing.Pool with max_workers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Define worker function that runs tests and returns (results, errors)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Use pool.map_async to distribute work</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 5: Set up timeout and handle worker crashes:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            - If worker dies: mark its tests as ERRORED with \"worker crashed\" message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 6: Collect results from all workers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 7: Handle KeyboardInterrupt: terminate pool, collect partial results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 8: Combine all results and return</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement the above steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _worker_function</span><span style=\"color:#E1E4E8\">(self, test_chunk: List[TestCase]) -> Tuple[List[TestResult], List[</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Worker process function - runs tests and returns results.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 1: Create SimpleRunner instance inside worker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 2: Run each test, collecting results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 3: Catch all exceptions and serialize for return to main process</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\"> 4: Return (results, errors) tuple</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement the above steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<ul>\n<li><strong>Exception chaining</strong>: Use <code>raise NewError from original_error</code> to preserve exception context in Python 3</li>\n<li><strong>Signal handling</strong>: Use <code>signal.signal(signal.SIGINT, handler)</code> but be careful in multi-threaded contexts</li>\n<li><strong>Multiprocessing errors</strong>: Worker processes should use <code>sys.exit(1)</code> on critical errors, not raise exceptions</li>\n<li><strong>Traceback capture</strong>: Use <code>traceback.format_exception()</code> for full traceback strings</li>\n<li><strong>Resource cleanup</strong>: Use <code>contextlib.ExitStack</code> for managing multiple resources with cleanup</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Test passes but fixture not cleaned up&quot;</td>\n<td>Exception in teardown swallowed</td>\n<td>Add debug logging in fixture finally blocks</td>\n<td>Ensure teardown errors are logged</td>\n</tr>\n<tr>\n<td>&quot;Worker process hangs forever&quot;</td>\n<td>Deadlock in shared resources or fixture</td>\n<td>Check for locks not released in fixtures</td>\n<td>Use timeout in <code>pool.map_async</code>, avoid shared state</td>\n</tr>\n<tr>\n<td>&quot;KeyboardInterrupt doesn&#39;t stop tests&quot;</td>\n<td>Signal handler not propagating to workers</td>\n<td>Check signal handling in worker processes</td>\n<td>Use <code>pool.terminate()</code> on interrupt</td>\n</tr>\n<tr>\n<td>&quot;Error message shows &lt;class &#39;__main__.MyError&#39;&gt;&quot;</td>\n<td>Error class not pickleable in parallel mode</td>\n<td>Check if error class is defined at module level</td>\n<td>Define error classes in importable modules</td>\n</tr>\n<tr>\n<td>&quot;Fixture circular dependency not detected&quot;</td>\n<td>Dependency resolution doesn&#39;t detect cycles</td>\n<td>Add dependency graph validation</td>\n<td>Implement cycle detection using DFS</td>\n</tr>\n</tbody></table>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing error handling, verify with this test scenario:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create test file with various errors</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">cat</span><span style=\"color:#F97583\"> ></span><span style=\"color:#9ECBFF\"> test_errors.py</span><span style=\"color:#F97583\"> &#x3C;&#x3C;</span><span style=\"color:#9ECBFF\"> 'EOF'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import sys</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_normal():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    assert 1 == 1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_failure():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    assert 1 == 2  # Should be FAILED</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_error():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    raise ValueError(\"Something went wrong\")  # Should be ERRORED</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def test_fixture_error(boom_fixture):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    pass  # Never reached if fixture fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">EOF</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run with error handling</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo.cli</span><span style=\"color:#9ECBFF\"> test_errors.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected output</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_errors.py::test_normal ... PASSED\ntest_errors.py::test_failure ... FAILED\ntest_errors.py::test_error ... ERRORED\ntest_errors.py::test_fixture_error ... ERRORED (Fixture 'boom_fixture' setup failed: ...)\n\n=== Summary ===\n4 tests, 1 passed, 1 failed, 2 errored in 0.12s</code></pre></div>\n\n<p><strong>Signs of problems</strong>:</p>\n<ul>\n<li>Tests stop after first failure/error (missing error isolation)</li>\n<li>No distinction between FAILED and ERRORED statuses</li>\n<li>No error messages or tracebacks in output</li>\n<li>Framework crashes when test raises exception</li>\n</ul>\n<h2 id=\"11-testing-strategy\">11. Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All four milestones, as testing the framework is an ongoing concern that evolves with each milestone.</p>\n</blockquote>\n<p>Testing a test framework presents a unique <strong>meta-problem</strong>: we must build a system that verifies other code while simultaneously verifying that our verification system itself works correctly. This section addresses this recursive challenge with a layered strategy combining <strong>self-validation</strong> (using the framework to test itself), <strong>property-based verification</strong> (ensuring mathematical correctness of assertions), and <strong>golden master testing</strong> (validating output formats against known-good references). We also provide concrete milestone checkpoints that serve as integration tests for each development phase, ensuring the framework meets its acceptance criteria.</p>\n<h3 id=\"testing-the-framework\">Testing the Framework</h3>\n<h4 id=\"self-validation-the-framework-as-its-first-user\">Self-Validation: The Framework as Its First User</h4>\n<blockquote>\n<p><strong>Mental Model: &quot;Eating Your Own Dog Food&quot;</strong> – The most powerful validation of a test framework comes from using it to test itself. This approach creates a virtuous cycle: as we add features to the framework, we immediately write tests for those features using the framework itself. Any bugs in the framework will be caught by the framework&#39;s own tests, creating immediate feedback.</p>\n</blockquote>\n<p><strong>Self-validation strategy</strong> involves organizing the framework&#39;s codebase with clear separation between the <strong>framework implementation</strong> (<code>src/apollo/</code>) and <strong>framework tests</strong> (<code>tests/</code>). The test suite for the framework should mirror its architecture:</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>What It Tests</th>\n<th>Example Test Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Discovery Tests</strong></td>\n<td>Test the <code>Discoverer</code> component&#39;s ability to find test functions and classes</td>\n<td><code>test_discover_tests_finds_test_functions</code>, <code>test_discover_tests_handles_nested_modules</code>, <code>test_discover_tests_respects_patterns</code></td>\n</tr>\n<tr>\n<td><strong>Runner Tests</strong></td>\n<td>Test the <code>Runner</code>&#39;s execution logic and isolation mechanisms</td>\n<td><code>test_run_test_returns_test_result</code>, <code>test_test_isolation_state_reset</code>, <code>test_parallel_execution_no_race_conditions</code></td>\n</tr>\n<tr>\n<td><strong>Assertion Tests</strong></td>\n<td>Test the <code>Assertion Engine</code>&#39;s correctness and error messages</td>\n<td><code>test_assert_equal_passes_when_equal</code>, <code>test_assert_equal_fails_with_diff</code>, <code>test_assert_raises_captures_exception</code></td>\n</tr>\n<tr>\n<td><strong>Fixture Tests</strong></td>\n<td>Test the <code>FixtureRegistry</code> and <code>FixtureLifecycleManager</code></td>\n<td><code>test_fixture_scope_function_creates_per_test</code>, <code>test_fixture_dependency_injection</code>, <code>test_fixture_teardown_on_error</code></td>\n</tr>\n<tr>\n<td><strong>CLI &amp; Reporting Tests</strong></td>\n<td>Test the <code>CLI Parser</code> and <code>Reporter</code> components</td>\n<td><code>test_cli_parses_file_patterns</code>, <code>test_reporter_generates_junit_xml</code>, <code>test_exit_code_on_failure</code></td>\n</tr>\n</tbody></table>\n<p><strong>Implementation approach:</strong> The framework&#39;s test suite should be runnable by the framework itself (self-hosting) as well as by other test runners (like <code>pytest</code>) for comparison. This dual approach provides a <strong>cross-validation</strong> mechanism: if both the framework and an established runner like <code>pytest</code> pass the same test suite, we gain confidence in correctness.</p>\n<blockquote>\n<p><strong>ADR: Self-Hosting vs. External Test Runner</strong></p>\n<ul>\n<li><strong>Context:</strong> We need to verify the framework&#39;s implementation but cannot trust it completely during development.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Pure self-hosting:</strong> Only use the framework to test itself</li>\n<li><strong>External runner only:</strong> Use <code>pytest</code> to test the framework</li>\n<li><strong>Dual approach:</strong> Use both, with framework tests written to be compatible with both runners</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Dual approach (Option 3)</li>\n<li><strong>Rationale:</strong> Pure self-hosting creates a circular dependency where bugs can hide (a bug in assertion logic might cause assertion tests to incorrectly pass). External-only testing doesn&#39;t exercise the framework&#39;s own discovery/execution path. The dual approach provides cross-validation: if <code>pytest</code> and our framework both pass the same test, confidence is high. It also allows us to compare output formats.</li>\n<li><strong>Consequences:</strong> Requires writing tests that are compatible with both runners (minimal use of framework-specific decorators), adds some complexity to test setup, but provides strongest validation.</li>\n</ul>\n</blockquote>\n<h4 id=\"property-based-testing-for-assertions\">Property-Based Testing for Assertions</h4>\n<blockquote>\n<p><strong>Mental Model: &quot;Fuzzing the Assertion Engine&quot;</strong> – Property-based testing treats the assertion engine as a mathematical function with certain invariants that must always hold, regardless of input. We generate thousands of random inputs to verify these invariants systematically.</p>\n</blockquote>\n<p>The assertion engine&#39;s correctness can be verified through <strong>property-based tests</strong> using a library like <code>hypothesis</code>. These tests define logical properties that should hold for all inputs:</p>\n<table>\n<thead>\n<tr>\n<th>Property</th>\n<th>Description</th>\n<th>Example Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Reflexivity</strong></td>\n<td><code>assert_equal(x, x)</code> should always pass</td>\n<td>Generate random Python values, verify assertion passes</td>\n</tr>\n<tr>\n<td><strong>Symmetry</strong></td>\n<td>If <code>assert_equal(a, b)</code> passes, then <code>assert_equal(b, a)</code> should also pass</td>\n<td>Generate random pairs where equality holds</td>\n</tr>\n<tr>\n<td><strong>Transitivity</strong></td>\n<td>If <code>assert_equal(a, b)</code> and <code>assert_equal(b, c)</code> pass, then <code>assert_equal(a, c)</code> should pass</td>\n<td>Generate chains of equal values</td>\n</tr>\n<tr>\n<td><strong>Exception Propagation</strong></td>\n<td><code>assert_raises(ExceptionType, func)</code> should fail if <code>func</code> doesn&#39;t raise <code>ExceptionType</code></td>\n<td>Generate random functions and exception types</td>\n</tr>\n<tr>\n<td><strong>Float Tolerance</strong></td>\n<td><code>assert_equal(a, b, tolerance=0.1)</code> should pass when <code>abs(a - b) &lt;= 0.1</code></td>\n<td>Generate float pairs within and outside tolerance</td>\n</tr>\n</tbody></table>\n<p><strong>Algorithm for property-based assertion testing:</strong></p>\n<ol>\n<li>Define a hypothesis strategy that generates relevant test data (integers, strings, lists, custom objects)</li>\n<li>For each property, write a test that uses <code>@given</code> decorator to generate many input examples</li>\n<li>In the test body, execute the assertion with generated inputs</li>\n<li>Verify the assertion behaves correctly (either passes or raises appropriate <code>AssertionError</code>)</li>\n<li>For failure cases, also verify the error message contains the expected information (actual vs expected)</li>\n</ol>\n<p>This approach systematically uncovers edge cases like <code>NaN != NaN</code>, <code>-0.0 == 0.0</code>, or recursive data structures that might cause infinite recursion in diff generation.</p>\n<h4 id=\"golden-master-tests-for-output\">Golden Master Tests for Output</h4>\n<blockquote>\n<p><strong>Mental Model: &quot;Snapshot Testing for Reports&quot;</strong> – Golden master testing captures known-good output (the &quot;golden master&quot;) and compares future test runs against this reference. This ensures the reporting format remains stable and compatible.</p>\n</blockquote>\n<p>The reporting system, particularly JUnit XML generation, must produce consistent, well-formed output that integrates with CI/CD tools. <strong>Golden master testing</strong> validates this by:</p>\n<ol>\n<li><strong>Capturing reference output:</strong> Run a known test suite and save the console output and JUnit XML to <code>tests/golden_masters/</code></li>\n<li><strong>Comparing during test runs:</strong> In the test suite, run the same test suite again and compare output character-by-character (with allowances for timing differences and paths)</li>\n<li><strong>Updating references:</strong> Provide a script to update golden masters when intentional changes are made to output format</li>\n</ol>\n<p><strong>Key areas for golden master testing:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Output Type</th>\n<th>What to Verify</th>\n<th>Comparison Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Console Output</strong></td>\n<td>Formatting, colors, indentation, progress indicators</td>\n<td>Normalize timing (replace <code>0.123s</code> with <code>{duration}</code>), ignore ANSI color codes in comparison</td>\n</tr>\n<tr>\n<td><strong>JUnit XML</strong></td>\n<td>XML structure, attribute values, escaping, CDATA sections</td>\n<td>Parse both as XML trees and compare structurally; allow differences in <code>timestamp</code> and <code>time</code> attributes</td>\n</tr>\n<tr>\n<td><strong>Exit Codes</strong></td>\n<td>Process return values for success/failure scenarios</td>\n<td>Exact integer comparison</td>\n</tr>\n<tr>\n<td><strong>Error Messages</strong></td>\n<td>Format of assertion failures and exception traces</td>\n<td>Normalize file paths and line numbers to placeholders</td>\n</tr>\n</tbody></table>\n<p><strong>Implementation workflow for golden master updates:</strong></p>\n<ol>\n<li>When making intentional changes to output format, run <code>python -m tests.update_golden_masters</code></li>\n<li>This re-generates all golden master files from the current implementation</li>\n<li>Review the diff to ensure only expected changes appear</li>\n<li>Commit the updated golden masters alongside code changes</li>\n</ol>\n<blockquote>\n<p><strong>ADR: Exact vs. Fuzzy Golden Master Comparison</strong></p>\n<ul>\n<li><strong>Context:</strong> Test output contains variable data (timings, file paths, process IDs) that change between runs.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Exact string comparison:</strong> Requires perfectly reproducible runs (fixed timestamps, paths)</li>\n<li><strong>Normalized comparison:</strong> Replace variable parts with placeholders before comparing</li>\n<li><strong>Structural comparison:</strong> Parse output (e.g., XML) and compare logical structure ignoring variable attributes</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Hybrid approach - normalized comparison for console output, structural comparison for XML</li>\n<li><strong>Rationale:</strong> Console output benefits from simple normalization (regex replacement of timings/paths). XML benefits from structural comparison which ignores attribute order and whitespace differences while still validating required fields and hierarchy.</li>\n<li><strong>Consequences:</strong> Requires implementing both normalization patterns and XML comparison logic, but provides robust validation without fragile exact matching.</li>\n</ul>\n</blockquote>\n<h4 id=\"common-pitfalls-in-testing-the-framework\">Common Pitfalls in Testing the Framework</h4>\n<p>⚠️ <strong>Pitfall: Recursive Test Discovery Loop</strong></p>\n<ul>\n<li><strong>Description:</strong> When the framework&#39;s test suite uses the framework itself for discovery, and the discovery logic has a bug that causes it to recursively discover the test suite&#39;s test files infinitely.</li>\n<li><strong>Why it&#39;s wrong:</strong> Causes infinite recursion, stack overflow, or extremely slow test runs.</li>\n<li><strong>How to avoid:</strong> In the framework&#39;s own test configuration, explicitly limit discovery paths to avoid re-scanning the framework source code. Use a dedicated <code>tests/</code> directory separate from <code>src/</code>.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Assertion Tests That Don&#39;t Actually Test Assertions</strong></p>\n<ul>\n<li><strong>Description:</strong> Writing <code>assert_equal(2, 1+1)</code> in a test for the <code>assert_equal</code> function - this tests Python&#39;s addition, not the assertion logic.</li>\n<li><strong>Why it&#39;s wrong:</strong> The test passes even if <code>assert_equal</code> is completely broken (e.g., always passes).</li>\n<li><strong>How to avoid:</strong> Test assertion functions by verifying they <strong>raise</strong> <code>AssertionError</code> when they should fail. Use <code>assert_raises</code> (from the framework or unittest) to check that <code>assert_equal(1, 2)</code> raises.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Golden Master Tests That Are Too Brittle</strong></p>\n<ul>\n<li><strong>Description:</strong> Golden tests fail due to insignificant differences (millisecond timing variations, temporary file path differences).</li>\n<li><strong>Why it&#39;s wrong:</strong> Creates noise, causes tests to fail randomly, reduces trust in test suite.</li>\n<li><strong>How to avoid:</strong> Implement robust normalization: replace timings with placeholders, use relative paths, sort dictionary outputs, ignore whitespace differences in XML comparison.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Fixture Tests That Don&#39;t Clean Up Properly</strong></p>\n<ul>\n<li><strong>Description:</strong> Tests for the fixture system leave resources (files, database connections) open, causing subsequent tests to fail.</li>\n<li><strong>Why it&#39;s wrong:</strong> Tests aren&#39;t isolated, causing flaky failures that depend on execution order.</li>\n<li><strong>How to avoid:</strong> Each fixture test should run in a subprocess, or should explicitly verify teardown occurred. Use context managers even in tests to ensure cleanup.</li>\n</ul>\n<h3 id=\"milestone-checkpoints\">Milestone Checkpoints</h3>\n<p>Each milestone includes concrete verification steps that serve as <strong>integration tests</strong> for that milestone&#39;s functionality. These checkpoints assume the framework is invoked as <code>apollo</code> from the command line after installation.</p>\n<h4 id=\"milestone-1-test-discovery-amp-execution\">Milestone 1: Test Discovery &amp; Execution</h4>\n<p><strong>Test File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_milestone1/\n├── test_simple.py\n├── test_isolated.py\n└── nested/\n    └── test_nested.py</code></pre></div>\n\n<p><strong>Example Test File (<code>test_simple.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test_simple.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_addition</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_subtraction</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_failure</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#6A737D\">  # This will fail</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> some_helper</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 42</span><span style=\"color:#6A737D\">  # Not a test - shouldn't be discovered</span></span></code></pre></div>\n\n<p><strong>Verification Commands and Expected Output:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Expected Output</th>\n<th>What to Verify</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>apollo discover test_milestone1/</code></td>\n<td>Lists 3 test functions: <code>test_addition</code>, <code>test_subtraction</code>, <code>test_failure</code> from <code>test_simple.py</code> and any from other test files</td>\n<td>Discovery finds test-prefixed functions, ignores non-test functions</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone1/</code></td>\n<td>Runs 3 tests, shows 2 passed, 1 failed, with test names and failure message for <code>test_failure</code></td>\n<td>Runner executes tests, captures failures, continues execution after failure</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone1/ --parallel</code></td>\n<td>Same results as above, but runs tests concurrently (may complete faster)</td>\n<td>Parallel execution works, tests remain isolated</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone1/test_simple.py::test_addition</code></td>\n<td>Runs only <code>test_addition</code>, shows 1 passed</td>\n<td>Test selection by file and name works</td>\n</tr>\n</tbody></table>\n<p><strong>Success Criteria:</strong></p>\n<ul>\n<li>All 3 tests are discovered and executed</li>\n<li><code>test_addition</code> and <code>test_subtraction</code> pass</li>\n<li><code>test_failure</code> fails with a clear message showing <code>4 != 5</code> or similar</li>\n<li>The helper function <code>some_helper</code> is NOT discovered as a test</li>\n<li>Execution continues after <code>test_failure</code> (doesn&#39;t stop the whole run)</li>\n<li>Parallel execution completes (may show different order but same results)</li>\n</ul>\n<h4 id=\"milestone-2-assertions-amp-matchers\">Milestone 2: Assertions &amp; Matchers</h4>\n<p><strong>Test File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_milestone2/\n├── test_assertions.py\n└── test_matchers.py</code></pre></div>\n\n<p><strong>Example Test File (<code>test_assertions.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test_assertions.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> apollo </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> assert_equal, assert_true, assert_raises</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_basic_assertions</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_equal(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_equal([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_true(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_assert_equal_failure</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This should fail with helpful message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_equal([</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">], [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_assert_raises</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> assert_raises(</span><span style=\"color:#79B8FF\">ValueError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not_a_number\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_assert_raises_failure</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This should fail because no exception raised</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> assert_raises(</span><span style=\"color:#79B8FF\">ValueError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"42\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Verification Commands and Expected Output:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Expected Output</th>\n<th>What to Verify</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>apollo run test_milestone2/test_assertions.py</code></td>\n<td>3 tests pass, 1 test fails (<code>test_assert_equal_failure</code>) with diff showing lists differ at index 2, 1 test fails (<code>test_assert_raises_failure</code>) with message about expected exception</td>\n<td><code>assert_equal</code> shows diff, <code>assert_raises</code> catches missing exceptions</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone2/test_assertions.py::test_assert_equal_failure -v</code></td>\n<td>Detailed output showing: <code>Expected: [1, 2, 3]</code>, <code>Actual: [1, 2, 4]</code>, diff highlighting position 2</td>\n<td>Failure messages are helpful and show comparison</td>\n</tr>\n</tbody></table>\n<p><strong>Custom Matcher Test (<code>test_matchers.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test_matchers.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> apollo </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> assert_that</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> apollo.matchers </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Matcher</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IsEven</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Matcher</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __matches__</span><span style=\"color:#E1E4E8\">(self, actual):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> actual </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"an even number\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> __describe_mismatch__</span><span style=\"color:#E1E4E8\">(self, actual):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> is odd\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_custom_matcher</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_that(</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, IsEven())</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Next line should fail with \"Expected an even number, but 3 is odd\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert_that(</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, IsEven())</span></span></code></pre></div>\n\n<p><strong>Verification Command:</strong></p>\n<ul>\n<li><code>apollo run test_milestone2/test_matchers.py</code> - Should show 1 pass, 1 fail with custom message &quot;Expected an even number, but 3 is odd&quot;</li>\n</ul>\n<p><strong>Success Criteria:</strong></p>\n<ul>\n<li>Basic assertions (<code>assert_equal</code>, <code>assert_true</code>) work correctly</li>\n<li><code>assert_equal</code> on lists shows diff highlighting difference</li>\n<li><code>assert_raises</code> passes when exception raised, fails with clear message when not</li>\n<li>Custom matchers work with <code>assert_that</code> and produce domain-specific messages</li>\n</ul>\n<h4 id=\"milestone-3-fixtures-amp-setupteardown\">Milestone 3: Fixtures &amp; Setup/Teardown</h4>\n<p><strong>Test File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_milestone3/\n├── test_fixtures.py\n├── test_setup_teardown.py\n└── conftest.py</code></pre></div>\n\n<p><strong>Example Files:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># conftest.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tempfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> apollo </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> fixture</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@fixture</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">scope</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"function\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> temp_file</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    f </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tempfile.NamedTemporaryFile(</span><span style=\"color:#FFAB70\">delete</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    yield</span><span style=\"color:#E1E4E8\"> f.name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    os.unlink(f.name)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@fixture</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">scope</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"module\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> shared_data</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_fixtures.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_uses_fixture</span><span style=\"color:#E1E4E8\">(temp_file):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # temp_file is automatically injected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> os.path.exists(temp_file)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(temp_file, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        f.write(</span><span style=\"color:#9ECBFF\">\"test\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # File should be deleted after test by fixture teardown</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_shared_fixture</span><span style=\"color:#E1E4E8\">(shared_data):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shared_data[</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> shared_data[</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # First test to use it</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_shared_again</span><span style=\"color:#E1E4E8\">(shared_data):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # If scope=\"module\", this should see counter=1 from previous test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # If scope=\"function\", this should see counter=0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # We'll test module scope</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> shared_data[</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_setup_teardown.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> apollo </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestExample</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">TestCase</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setUp</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tearDown</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.data) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Verify test ran</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_setup_works</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data.append(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<p><strong>Verification Commands and Expected Output:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Expected Output</th>\n<th>What to Verify</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>apollo run test_milestone3/test_fixtures.py -v</code></td>\n<td>All 3 tests pass. Shows fixture setup/teardown in verbose output. Temp file is created and deleted.</td>\n<td>Fixture injection works, function-scoped fixture recreated per test, module-scoped fixture shared</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone3/test_setup_teardown.py</code></td>\n<td>Test passes. If <code>tearDown</code> assertion fails, test fails (verifying <code>tearDown</code> runs even after test failure in some implementations)</td>\n<td><code>setUp</code> runs before test, <code>tearDown</code> runs after</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone3/ --setup-show</code> (if implemented)</td>\n<td>Shows fixture setup/teardown hierarchy</td>\n<td>Verbose fixture debugging works</td>\n</tr>\n</tbody></table>\n<p><strong>Success Criteria:</strong></p>\n<ul>\n<li>Tests receive fixture values via parameter names</li>\n<li>Function-scoped fixtures created fresh for each test</li>\n<li>Module-scoped fixtures created once and shared across tests in same module</li>\n<li>Fixtures clean up resources (temp file deleted)</li>\n<li><code>setUp</code> and <code>tearDown</code> methods called for test classes</li>\n<li>Circular dependency detection (if attempted) gives clear error</li>\n</ul>\n<h4 id=\"milestone-4-reporting-amp-cli\">Milestone 4: Reporting &amp; CLI</h4>\n<p><strong>Test File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_milestone4/\n├── test_reporting.py\n└── test_cli.py</code></pre></div>\n\n<p><strong>Example Mixed-Result Test File:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># test_reporting.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_pass</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_fail</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Intentional failure\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_error</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Unexpected error\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_slow</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    time.sleep(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>Verification Commands and Expected Output:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Expected Output</th>\n<th>What to Verify</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>apollo run test_milestone4/</code></td>\n<td>Shows 4 tests: 1 pass, 1 fail, 1 error, 1 pass. Summary: &quot;4 tests, 1 passed, 1 failed, 1 error, 1 passed&quot; with total time. Exit code 1.</td>\n<td>Basic reporting shows status per test, summary statistics, non-zero exit on failure</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone4/ --verbose</code></td>\n<td>Shows detailed output for each test including traceback for error and failure</td>\n<td>Verbose flag shows more details</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone4/ --junit-xml=results.xml</code></td>\n<td>Creates <code>results.xml</code> with 4 testcases, each with correct status and timing. Exit code 1.</td>\n<td>JUnit XML generated, can be parsed by CI tools</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone4/ --quiet</code></td>\n<td>Shows only summary line, no per-test output</td>\n<td>Quiet flag suppresses details</td>\n</tr>\n<tr>\n<td><code>apollo run test_milestone4/ --show-durations</code></td>\n<td>Shows timing for each test, slowest test highlighted</td>\n<td>Duration reporting works</td>\n</tr>\n<tr>\n<td><code>apollo --help</code></td>\n<td>Shows usage instructions, all command-line options</td>\n<td>CLI help is comprehensive</td>\n</tr>\n</tbody></table>\n<p><strong>Success Criteria:</strong></p>\n<ul>\n<li>Console output clearly distinguishes passed/failed/errored tests (colors if supported)</li>\n<li>Summary statistics are accurate (counts, total time)</li>\n<li>Exit code is 0 when all tests pass, non-zero when any fail or error</li>\n<li>JUnit XML is well-formed, contains all required attributes (<code>name</code>, <code>status</code>, <code>time</code>), escapes special characters</li>\n<li>Command-line flags work as documented (verbose, quiet, junit-xml, show-durations)</li>\n<li>Help text is clear and complete</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Self-Testing</strong></td>\n<td>Use Python&#39;s <code>unittest</code> for framework tests</td>\n<td>Use the framework itself + <code>pytest</code> for cross-validation</td>\n</tr>\n<tr>\n<td><strong>Property-Based Testing</strong></td>\n<td><code>hypothesis</code> library for generating test cases</td>\n<td>Custom generators for framework-specific types</td>\n</tr>\n<tr>\n<td><strong>Golden Master Testing</strong></td>\n<td>Simple file comparison with regex normalization</td>\n<td>Structural XML comparison using <code>xml.etree.ElementTree</code></td>\n</tr>\n<tr>\n<td><strong>Test Isolation</strong></td>\n<td><code>subprocess</code> module to run framework in separate process</td>\n<td>In-process isolation with careful cleanup</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure-for-tests\">Recommended File/Module Structure for Tests</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/                    # Framework source\ntests/                    # Framework tests\n├── __init__.py\n├── conftest.py           # Shared fixtures for testing the framework\n├── test_discovery.py     # Tests for Milestone 1\n├── test_assertions.py    # Tests for Milestone 2  \n├── test_fixtures.py      # Tests for Milestone 3\n├── test_cli.py           # Tests for Milestone 4\n├── property_based/       # Property-based tests\n│   ├── test_assertion_properties.py\n│   └── strategies.py     # Custom hypothesis strategies\n├── golden_masters/       # Reference outputs\n│   ├── console_output.txt\n│   ├── junit_output.xml\n│   └── update.py         # Script to update golden masters\n├── test_files/           # Test files used by the tests\n│   ├── milestone1/\n│   ├── milestone2/\n│   └── ...\n└── integration/          # Integration tests\n    ├── test_self_host.py # Framework tests itself\n    └── test_examples.py  # Example user tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code-for-golden-master-testing\">Infrastructure Starter Code for Golden Master Testing</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/golden_masters/__init__.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Golden master testing utilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> xml.etree.ElementTree </span><span style=\"color:#F97583\">as</span><span style=\"color:#79B8FF\"> ET</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">GOLDEN_MASTER_DIR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Path(</span><span style=\"color:#79B8FF\">__file__</span><span style=\"color:#E1E4E8\">).parent</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> normalize_output</span><span style=\"color:#E1E4E8\">(output: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Normalize variable parts of test output for comparison.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Replace timings like \"0.123s\" with \"{duration}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.sub(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">+</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{3}</span><span style=\"color:#DBEDFF\">s</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">{duration}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, output)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Replace file paths with placeholders</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.sub(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">/tmp/</span><span style=\"color:#79B8FF\">[a-zA-Z0-9_]</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">{temp_file}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, output)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Remove ANSI color codes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.sub(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#85E89D;font-weight:bold\">\\x1b\\[</span><span style=\"color:#79B8FF\">[0-9;]</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">m</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">, output)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> output</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> compare_xml</span><span style=\"color:#E1E4E8\">(actual: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> list[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compare two XML strings structurally, ignoring whitespace and order.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> normalize_xml</span><span style=\"color:#E1E4E8\">(xml_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">ET</span><span style=\"color:#E1E4E8\">.Element:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse and normalize XML for comparison.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        root </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> ET</span><span style=\"color:#E1E4E8\">.fromstring(xml_str)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort attributes for consistent comparison</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> el </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> root.iter():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> el.attrib:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Sort attributes alphabetically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                el.attrib </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {k: el.attrib[k] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(el.attrib)}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> root</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actual_root </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalize_xml(actual)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_root </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalize_xml(expected)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    differences </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Simple comparison - in practice, use more sophisticated XML diff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> ET</span><span style=\"color:#E1E4E8\">.tostring(actual_root) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> ET</span><span style=\"color:#E1E4E8\">.tostring(expected_root):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        differences.append(</span><span style=\"color:#9ECBFF\">\"XML structure differs\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> differences</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_golden_master</span><span style=\"color:#E1E4E8\">(name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load a golden master file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> GOLDEN_MASTER_DIR</span><span style=\"color:#F97583\"> /</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">.txt\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> path.read_text()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> save_golden_master</span><span style=\"color:#E1E4E8\">(name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Save content as a golden master.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> GOLDEN_MASTER_DIR</span><span style=\"color:#F97583\"> /</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">.txt\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path.write_text(content)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-for-self-testing\">Core Logic Skeleton for Self-Testing</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/test_self_host.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Test that the framework can run its own test suite.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> subprocess</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_framework_can_run_itself</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test that Apollo can discover and run its own test suite.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get the path to the tests directory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tests_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(</span><span style=\"color:#79B8FF\">__file__</span><span style=\"color:#E1E4E8\">).parent</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Import the framework's main entry point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use discover_tests to find tests in tests/test_discovery.py</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run the discovered tests using SimpleRunner</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify all tests pass (or at least no new failures)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compare with running same tests via pytest for cross-validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_cli_interface</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test the CLI end-to-end.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use subprocess.run to execute \"python -m apollo.cli --help\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify exit code is 0 and help text contains expected options</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run a known test file via CLI</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Capture output and verify it matches expected format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify exit code is correct based on test results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"property-based-testing-skeleton\">Property-Based Testing Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/property_based/test_assertion_properties.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Property-based tests for assertion engine.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> hypothesis </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> given, strategies </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> st, assume</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Import the assertion functions from the framework</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># from apollo.assertions import assert_equal, AssertionError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@given</span><span style=\"color:#E1E4E8\">(st.integers(), st.integers())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_assert_equal_reflexive</span><span style=\"color:#E1E4E8\">(a: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, b: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"assert_equal(x, x) should always pass.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For all integers a, assert_equal(a, a) should not raise</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Use try/except to catch AssertionError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # If AssertionError is raised, that's a test failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@given</span><span style=\"color:#E1E4E8\">(st.lists(st.integers()))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_assert_equal_list_self_comparison</span><span style=\"color:#E1E4E8\">(lst: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"assert_equal on a list compared to itself should pass.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: assert_equal(lst, lst) should always pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This tests that the comparison handles nested structures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@given</span><span style=\"color:#E1E4E8\">(st.floats(</span><span style=\"color:#FFAB70\">allow_nan</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">), st.floats(</span><span style=\"color:#FFAB70\">allow_nan</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_assert_equal_with_tolerance</span><span style=\"color:#E1E4E8\">(a: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, b: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"assert_equal with tolerance should pass when values are close.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If abs(a - b) &#x3C;= 0.1, then assert_equal(a, b, tolerance=0.1) should pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Otherwise, it should raise AssertionError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Use hypothesis assume to filter test cases</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-implementation\">Milestone Checkpoint Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/integration/test_milestone_checkpoints.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Integration tests for each milestone checkpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> subprocess</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tempfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> textwrap</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_milestone1_checkpoint</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify Milestone 1 acceptance criteria.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create a temporary test file</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_code </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> textwrap.dedent(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        def test_pass():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            assert 1 + 1 == 2</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        def test_fail():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            assert 2 * 2 == 5</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> tempfile.NamedTemporaryFile(</span><span style=\"color:#FFAB70\">mode</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">suffix</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'.py'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">delete</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        f.write(test_code)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        test_file </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f.name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Run the test file using the framework</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Capture output and exit code</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify: 2 tests discovered, 1 passed, 1 failed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify: Execution continues after failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify: Non-test functions are not discovered</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.unlink(test_file)</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips-for-framework-tests\">Debugging Tips for Framework Tests</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tests pass when run with pytest but fail with framework</td>\n<td>Framework bug in discovery or execution</td>\n<td>Run with <code>--verbose</code> in both runners, compare which tests are found and their execution order</td>\n<td>Check <code>discover_tests</code> logic for missed tests or incorrect test case creation</td>\n</tr>\n<tr>\n<td>Property-based tests find edge case failures</td>\n<td>Assertion logic doesn&#39;t handle special values (NaN, inf, -0.0)</td>\n<td>Add debug print in assertion to see the failing values</td>\n<td>Add special handling for edge cases in comparison logic</td>\n</tr>\n<tr>\n<td>Golden master tests fail on CI but pass locally</td>\n<td>Path or timing differences</td>\n<td>Compare normalized output, check what differs</td>\n<td>Improve normalization to handle CI environment (e.g., container paths)</td>\n</tr>\n<tr>\n<td>Fixture tests leak resources affecting subsequent tests</td>\n<td>Teardown not called or not cleaning completely</td>\n<td>Add logging to fixture teardown, check if it&#39;s called</td>\n<td>Ensure <code>yield</code> fixture properly cleans up in finally block</td>\n</tr>\n<tr>\n<td>Self-test causes infinite recursion</td>\n<td>Discovery scanning framework source code</td>\n<td>Add debug prints to see what paths are being scanned</td>\n<td>Limit discovery to test directories only in framework&#39;s own tests</td>\n</tr>\n</tbody></table>\n<h4 id=\"language-specific-hints-for-python\">Language-Specific Hints for Python</h4>\n<ul>\n<li>Use <code>sys.path.insert(0, str(project_root))</code> in test setup to ensure framework modules are importable</li>\n<li>For subprocess testing, use <code>subprocess.run(capture_output=True, text=True)</code> to capture stdout/stderr</li>\n<li>Use <code>tempfile.TemporaryDirectory()</code> context manager for tests that need temporary files</li>\n<li>For mocking time in duration tests, use <code>unittest.mock.patch(&#39;time.time&#39;, side_effect=[start, end])</code></li>\n<li>When comparing floats in tests, use <code>pytest.approx()</code> instead of exact equality to avoid precision issues</li>\n</ul>\n<h2 id=\"12-debugging-guide\">12. Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All four milestones, as debugging strategies are essential throughout development.</p>\n</blockquote>\n<p>Even the most meticulously designed system will encounter bugs during implementation. This guide serves as your <strong>troubleshooting playbook</strong>, helping you diagnose and fix common issues that arise when building a test framework. Think of debugging as <strong>forensic analysis</strong>—you&#39;re presented with symptoms (unexpected test behavior, crashes, or incorrect outputs), and must trace them back to their root cause in the code. This section provides a systematic approach to investigation, starting with a catalog of known issues and their fixes, followed by proactive techniques for uncovering hidden problems.</p>\n<h3 id=\"common-bugs-amp-fixes\">Common Bugs &amp; Fixes</h3>\n<p>The following table documents common symptoms you might encounter during development, organized by the subsystem where they typically originate. Each entry follows a <strong>symptom → cause → investigation → fix</strong> pattern, transforming vague problems into concrete action plans.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tests pass but fixtures aren&#39;t cleaned up</strong> (e.g., database connections remain open, temporary files persist)</td>\n<td><strong>Fixture teardown not triggered:</strong> The <code>FixtureLifecycleManager.teardown_scope()</code> method isn&#39;t being called at the appropriate <strong>scope boundary</strong>, or the fixture function isn&#39;t structured as a generator (using <code>yield</code>) when it should be.</td>\n<td>1. Add debug logging to <code>teardown_scope()</code> to see if/when it&#39;s called.<br>2. Check if fixture functions use <code>yield</code> for function-scoped cleanup.<br>3. Verify that <code>FixtureLifecycleManager._generators</code> dictionary is being populated and cleared.</td>\n<td>1. Ensure runner calls <code>teardown_scope()</code> after all tests in a scope complete.<br>2. Convert <code>@fixture</code> functions to use <code>yield</code> pattern: <code>setup; yield resource; cleanup</code>.<br>3. In <code>get_fixture_value()</code>, store yielded generators in <code>_generators</code> and ensure finalization.</td>\n</tr>\n<tr>\n<td><strong>Fixture value reused incorrectly across tests</strong> (state leaks between tests)</td>\n<td><strong>Incorrect caching key:</strong> The <code>FixtureRequest.cache_key</code> doesn&#39;t include all necessary scope identifiers (e.g., test class name for class scope). Different tests mistakenly receive the same cached fixture instance.</td>\n<td>1. Print <code>cache_key</code> for each fixture request during test execution.<br>2. Check if <code>_cache</code> dictionary keys differ between tests that should have separate instances.<br>3. Verify <code>FixtureRequest</code> creation includes correct <code>scope_id</code> based on scope.</td>\n<td>1. Update <code>cache_key</code> calculation to include: <code>(fixture_name, scope, scope_id)</code> where <code>scope_id</code> is module name for MODULE scope, class name for CLASS scope, etc.<br>2. Ensure <code>FixtureLifecycleManager</code> creates unique <code>scope_id</code> values for each scope level.</td>\n</tr>\n<tr>\n<td><strong>Circular dependency error crashes discovery</strong></td>\n<td><strong>Dependency graph cycle:</strong> Fixture A depends on B, B depends on C, and C depends on A (directly or indirectly). The framework attempts infinite recursion when resolving dependencies.</td>\n<td>1. Check <code>Fixture.dependencies</code> lists for cycles.<br>2. Add cycle detection in <code>FixtureRegistry.get()</code> or <code>FixtureLifecycleManager.get_fixture_value()</code>.<br>3. Look for <code>RecursionError</code> in stack trace.</td>\n<td>1. Implement topological sort or cycle detection before fixture resolution.<br>2. Maintain <code>_active_requests</code> set in <code>FixtureLifecycleManager</code> to detect circular requests during resolution and raise a clear error.</td>\n</tr>\n<tr>\n<td><strong>Assertion failure messages show unhelpful <code>True != False</code></strong></td>\n<td><strong>Generic assertion implementation:</strong> Using Python&#39;s built-in <code>assert</code> statement or a simple <code>assert condition, msg</code> without detailed diff generation. The framework isn&#39;t intercepting the assertion to provide rich context.</td>\n<td>1. Check if test uses framework&#39;s <code>assert_equal()</code> or just Python&#39;s <code>assert</code>.<br>2. Verify that assertion functions raise a custom exception (not plain <code>AssertionError</code>).<br>3. Look for missing <code>expected</code>/<code>actual</code> values in error output.</td>\n<td>1. Ensure all assertion functions (<code>assert_equal</code>, <code>assert_true</code>, etc.) construct an <code>AssertionFailure</code> with detailed context before raising.<br>2. Implement <code>__str__</code> method on <code>AssertionFailure</code> that formats expected vs actual with diff.<br>3. Use <code>inspect</code> module to capture the assertion call&#39;s arguments for better messages.</td>\n</tr>\n<tr>\n<td><strong>Test discovery misses functions in nested classes</strong></td>\n<td><strong>Shallow traversal:</strong> The <code>_find_tests_in_module()</code> function only examines top-level module attributes, not recursively searching within class definitions.</td>\n<td>1. Print all objects discovered during module scan.<br>2. Check if classes with <code>test_</code> methods are being examined.<br>3. Verify the discovery convention includes class methods.</td>\n<td>1. Implement recursive inspection: for each class in module, inspect its methods for <code>test_</code> prefix.<br>2. Update <code>TestCase</code> creation to include class name in <code>nodeid</code> (e.g., <code>TestClass.test_method</code>).<br>3. Consider supporting unittest-style test classes as well as plain functions.</td>\n</tr>\n<tr>\n<td><strong>Parallel tests interfere with each other (shared state)</strong></td>\n<td><strong>Lack of process isolation:</strong> When using <code>multiprocessing</code> for parallel execution, tests share module-level or global state because modules are imported in parent process and inherited by children.</td>\n<td>1. Check if test modifies a module-level variable and another test sees the change.<br>2. Verify each test runs in a separate process with fresh interpreter state.<br>3. Look for <code>pickle</code> errors when passing complex objects to worker processes.</td>\n<td>1. Ensure <code>_worker_function</code> re-imports modules in child process rather than inheriting parent&#39;s loaded modules.<br>2. Use <code>multiprocessing</code> with <code>&#39;spawn&#39;</code> start method (forces clean state).<br>3. Pass only serializable data (via <code>pickle</code>) between processes; reconstruct test execution context in worker.</td>\n</tr>\n<tr>\n<td><strong>JUnit XML contains invalid characters or malformed XML</strong></td>\n<td><strong>Missing XML escaping:</strong> Special characters (<code>&lt;</code>, <code>&gt;</code>, <code>&amp;</code>, <code>&quot;</code>, <code>&#39;</code>) in test names, error messages, or output aren&#39;t escaped before inclusion in XML.</td>\n<td>1. Examine generated XML with a validator or XML parser.<br>2. Look for parser errors mentioning &quot;unescaped&quot; or &quot;malformed&quot;.<br>3. Check if <code>_escape_xml()</code> function is called on all text fields.</td>\n<td>1. Implement robust <code>_escape_xml()</code> that replaces <code>&amp;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&quot;</code>, <code>&#39;</code> with corresponding entities.<br>2. Use CDATA sections (<code>_create_cdata()</code>) for stack traces or multiline output that may contain special characters.<br>3. Ensure <code>JUnitFormatter</code> escapes all string fields before XML generation.</td>\n</tr>\n<tr>\n<td><strong><code>assert_raises</code> doesn&#39;t catch exception or catches wrong one</strong></td>\n<td><strong>Exception context management:</strong> The <code>assert_raises</code> context manager isn&#39;t properly catching and comparing exception types, or it&#39;s swallowing exceptions incorrectly.</td>\n<td>1. Check if exception is raised but not caught by the context manager.<br>2. Verify exception type comparison uses <code>isinstance()</code> not exact type equality.<br>3. Look for exceptions propagating outside the test function.</td>\n<td>1. Implement <code>assert_raises</code> as a context manager that: catches exception, stores it, compares type, re-raises if mismatch.<br>2. Use <code>try/except</code> block in context manager&#39;s <code>__exit__</code> method.<br>3. Include exception message in assertion failure if provided.</td>\n</tr>\n<tr>\n<td><strong>Verbose output shows duplicate test names or missing tests</strong></td>\n<td><strong>Incorrect <code>nodeid</code> generation:</strong> The <code>TestCase.nodeid</code> field isn&#39;t unique across tests, or discovery is scanning the same module multiple times due to symlinks or duplicate paths.</td>\n<td>1. Print all discovered <code>nodeid</code> values.<br>2. Check if <code>resolve_patterns_to_paths()</code> returns duplicate absolute paths.<br>3. Verify <code>module_path_to_name()</code> produces consistent module names.</td>\n<td>1. Ensure <code>nodeid</code> includes full module path + function/class/method name (e.g., <code>tests/test_math.py::TestCalc::test_add</code>).<br>2. Normalize paths and deduplicate in discovery phase.<br>3. Use Python&#39;s <code>inspect.getsourcefile()</code> to get canonical file path.</td>\n</tr>\n<tr>\n<td><strong>Test status incorrectly reported as ERROR instead of FAILED</strong></td>\n<td><strong>Misclassification of exceptions:</strong> The runner catches all exceptions and marks tests as <code>ERRORED</code>, not distinguishing between assertion failures (FAILED) and unexpected errors.</td>\n<td>1. Check runner&#39;s exception handling logic in <code>SimpleRunner.run_test()</code>.<br>2. Verify if assertion functions raise a distinguishable exception type (e.g., <code>AssertionFailure</code> subclass).<br>3. Look for <code>TestResult.status</code> assignment logic.</td>\n<td>1. In <code>run_test</code>, catch <code>AssertionFailure</code> (or similar) separately from other exceptions.<br>2. Set <code>TestResult.status</code> to <code>FAILED</code> for assertion failures, <code>ERRORED</code> for other exceptions.<br>3. Store the exception in <code>TestResult.exception</code> for both cases.</td>\n</tr>\n<tr>\n<td><strong><code>--quiet</code> flag still shows progress dots</strong></td>\n<td><strong>Output logic bypasses verbosity check:</strong> The reporter&#39;s output methods aren&#39;t respecting the <code>Configuration.quiet</code> flag, or progress indicators are printed directly from runner.</td>\n<td>1. Check where progress dots are printed (likely in runner or reporter).<br>2. Verify <code>Configuration</code> is passed to all components that output.<br>3. Look for <code>if not config.quiet:</code> guards missing.</td>\n<td>1. Centralize output through <code>OutputWriter</code> component that checks <code>quiet</code> and <code>verbose</code> flags.<br>2. Move progress indicator logic to reporter, not runner.<br>3. Add conditionals around all print statements that should respect quiet mode.</td>\n</tr>\n<tr>\n<td><strong>Float comparisons fail due to tiny precision differences</strong></td>\n<td><strong>Exact equality for floats:</strong> <code>assert_equal</code> uses <code>==</code> for float comparison instead of approximate equality with tolerance.</td>\n<td>1. Check <code>assert_equal</code> implementation for float handling.<br>2. Test with values like <code>0.1 + 0.2</code> (which isn&#39;t exactly <code>0.3</code>).<br>3. Look for missing <code>tolerance</code> parameter propagation.</td>\n<td>1. Add <code>tolerance</code> parameter to <code>assert_equal</code> (default <code>1e-9</code>).<br>2. Implement approximate comparison: <code>abs(actual - expected) &lt;= tolerance</code>.<br>3. Create specialized <code>assert_almost_equal</code> function with relative/absolute tolerance options.</td>\n</tr>\n<tr>\n<td><strong>Fixture dependency injection fails with <code>TypeError</code></strong></td>\n<td><strong>Signature inspection error:</strong> The framework fails to match parameter names in test function signature with available fixture names, or tries to pass incorrect arguments.</td>\n<td>1. Check error traceback for <code>inspect.signature</code> or <code>getargspec</code> failures.<br>2. Print test function parameters and available fixtures.<br>3. Verify <code>FixtureLifecycleManager.get_fixture_value()</code> returns appropriate values.</td>\n<td>1. Use <code>inspect.signature(test_func).parameters</code> to get parameter names safely.<br>2. Match parameter names to fixture names in registry (case-sensitive).<br>3. Handle optional parameters/default values gracefully (skip if fixture not found).</td>\n</tr>\n<tr>\n<td><strong>Test execution hangs indefinitely (deadlock)</strong></td>\n<td><strong>Resource contention in parallel mode:</strong> Multiple tests or fixtures acquire locks/resources in conflicting order, or a fixture generator doesn&#39;t yield.</td>\n<td>1. Check if deadlock occurs only with <code>--parallel</code> flag.<br>2. Look for <code>@fixture</code> functions without <code>yield</code> (blocking).<br>3. Check for shared resources (files, ports) without proper locking.</td>\n<td>1. Ensure fixture generators always yield (use <code>contextlib</code>&#39;s <code>@contextmanager</code> as pattern).<br>2. Implement timeout mechanism for test execution.<br>3. Use resource pools or random port assignment for shared resources in parallel tests.</td>\n</tr>\n</tbody></table>\n<h3 id=\"debugging-techniques\">Debugging Techniques</h3>\n<p>When the symptom isn&#39;t in the table above, or you need to understand the system&#39;s internal state, employ these systematic debugging techniques. Think of yourself as a <strong>framework archeologist</strong>, uncovering the layers of execution to find where the behavior diverges from expectations.</p>\n<h4 id=\"1-strategic-logging-and-introspection\">1. Strategic Logging and Introspection</h4>\n<p>The most powerful technique is adding temporary logging at key architectural boundaries. Create a <strong>debug mode</strong> that can be enabled via environment variable or command-line flag, then instrument the framework&#39;s components to log their inputs, outputs, and state transitions.</p>\n<blockquote>\n<p><strong>Debugging Principle:</strong> Log data transformations, not just function entries. The most valuable logs show what data enters a component, how it&#39;s transformed, and what leaves.</p>\n</blockquote>\n<p><strong>What to log:</strong></p>\n<ul>\n<li><strong>Discovery phase:</strong> Log each file examined, module imported, and test function found with its <code>nodeid</code>.</li>\n<li><strong>Fixture lifecycle:</strong> Log fixture creation, caching (<code>cache_key</code>), and teardown events with scope information.</li>\n<li><strong>Test execution:</strong> Log test start/end, status transitions, and any exceptions caught.</li>\n<li><strong>Assertion evaluation:</strong> Log the <code>actual</code>/<code>expected</code> values and comparison results.</li>\n<li><strong>Parallel execution:</strong> Log worker process creation, test distribution, and result collection.</li>\n</ul>\n<p><strong>Implementation approach:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># In a debug module</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEBUG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> os.environ.get(</span><span style=\"color:#9ECBFF\">'APOLLO_DEBUG'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'0'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '1'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> debug_log</span><span style=\"color:#E1E4E8\">(component, message, data</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> DEBUG</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.strftime(</span><span style=\"color:#9ECBFF\">\"%H:%M:%S\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"[</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">] [</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">component</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">file</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">sys.stderr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"    Data: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">data</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">file</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">sys.stderr)</span></span></code></pre></div>\n\n<p>Use this in components:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># In SimpleRunner.run_test()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">debug_log(</span><span style=\"color:#9ECBFF\">\"Runner\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Starting test </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test_case.nodeid</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#6A737D\"> # ... execute test</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug_log(</span><span style=\"color:#9ECBFF\">\"Runner\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Test completed with status </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.status</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug_log(</span><span style=\"color:#9ECBFF\">\"Runner\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Test raised exception: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"2-interactive-debugging-with-pdb\">2. Interactive Debugging with PDB</h4>\n<p>For complex issues where logging isn&#39;t enough, use Python&#39;s built-in debugger (<code>pdb</code>) to pause execution and inspect state interactively. This is especially useful for understanding <strong>control flow</strong> and <strong>variable state</strong> at specific moments.</p>\n<p><strong>Techniques:</strong></p>\n<ul>\n<li><strong>Post-mortem debugging:</strong> When a test fails unexpectedly, add <code>import pdb; pdb.post_mortem()</code> in the exception handler to examine the traceback.</li>\n<li><strong>Breakpoint decorator:</strong> Create a decorator that sets a breakpoint when a specific condition is met:</li>\n</ul>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">  def</span><span style=\"color:#B392F0\"> breakpoint_on</span><span style=\"color:#E1E4E8\">(condition):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      def</span><span style=\"color:#B392F0\"> decorator</span><span style=\"color:#E1E4E8\">(func):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">          def</span><span style=\"color:#B392F0\"> wrapper</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">              if</span><span style=\"color:#E1E4E8\"> condition(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                  import</span><span style=\"color:#E1E4E8\"> pdb; pdb.set_trace()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">              return</span><span style=\"color:#E1E4E8\"> func(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">          return</span><span style=\"color:#E1E4E8\"> wrapper</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">      return</span><span style=\"color:#E1E4E8\"> decorator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">  # Use to break when fixture name contains \"db\"</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  @breakpoint_on</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> name, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">_: </span><span style=\"color:#9ECBFF\">\"db\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  def</span><span style=\"color:#B392F0\"> get_fixture_value</span><span style=\"color:#E1E4E8\">(self, fixture_name, test_case):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">      # ...</span></span></code></pre></div>\n<ul>\n<li><strong>Remote debugging:</strong> For parallel execution issues, use <code>rconsole</code> or <code>remote-pdb</code> to connect to worker processes.</li>\n</ul>\n<h4 id=\"3-state-inspection-via-test-hooks\">3. State Inspection via Test Hooks</h4>\n<p>Add special <strong>diagnostic hooks</strong> that tests can use to report framework internal state. For example, create a <code>debug_fixtures()</code> function that tests can call to see which fixtures are currently cached and their values.</p>\n<p><strong>Example diagnostic utility:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># In framework's public API</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_framework_state</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Return a snapshot of framework internal state for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'active_fixtures'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">(lifecycle_manager._cache.keys()),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'pending_teardowns'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">(lifecycle_manager._generators.keys()),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'test_queue_size'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(test_queue) </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(runner, </span><span style=\"color:#9ECBFF\">'test_queue'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> state</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test can use it:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_debug_fixtures</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> apollo.get_framework_state()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Active fixtures: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">state[</span><span style=\"color:#9ECBFF\">'active_fixtures'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Continue with test</span></span></code></pre></div>\n\n<h4 id=\"4-minimal-reproduction-creation\">4. Minimal Reproduction Creation</h4>\n<p>When encountering a bug, immediately create a <strong>minimal reproduction case</strong>—the smallest possible test file that demonstrates the issue. This isolates the problem from your larger codebase and makes debugging exponentially easier.</p>\n<p><strong>Process:</strong></p>\n<ol>\n<li>Start with the failing test in your project</li>\n<li>Remove all unrelated imports and setup</li>\n<li>Strip the test down to the essential lines that trigger the bug</li>\n<li>If it&#39;s a fixture issue, create a minimal fixture and test that shows the problem</li>\n<li>Save this as <code>reproduce_bug.py</code> and use it for focused debugging</li>\n</ol>\n<p><strong>Example minimal reproduction:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># reproduce_parallel_bug.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> apollo</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@apollo.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> shared</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_one</span><span style=\"color:#E1E4E8\">(shared):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shared[</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> shared[</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Fails if shared across processes</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_two</span><span style=\"color:#E1E4E8\">(shared):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shared[</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> shared[</span><span style=\"color:#9ECBFF\">\"counter\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Should be fresh copy</span></span></code></pre></div>\n\n<h4 id=\"5-component-isolation-testing\">5. Component Isolation Testing</h4>\n<p>Test each component in isolation by creating <strong>unit tests that mock adjacent components</strong>. This helps determine whether a bug originates in the component being tested or in its dependencies.</p>\n<p><strong>Strategy:</strong></p>\n<ul>\n<li><strong>Mock the Discoverer</strong> when testing the Runner</li>\n<li><strong>Mock the Runner</strong> when testing the Reporter  </li>\n<li><strong>Use fake fixtures</strong> when testing fixture dependency resolution</li>\n<li><strong>Create in-memory implementations</strong> of file system operations for discovery tests</li>\n</ul>\n<p><strong>Example isolated test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_runner_with_mocked_fixtures</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test runner handles fixture injection errors gracefully.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mock_fixture_registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Mock(</span><span style=\"color:#FFAB70\">spec</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">FixtureRegistry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mock_fixture_registry.get.side_effect </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> KeyError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"fixture not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleRunner(</span><span style=\"color:#FFAB70\">fixture_registry</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">mock_fixture_registry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_case </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TestCase(</span><span style=\"color:#FFAB70\">nodeid</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"test\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">func</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> runner.run_test(test_case)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TestStatus.</span><span style=\"color:#79B8FF\">ERRORED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#9ECBFF\"> \"fixture not found\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(result.exception)</span></span></code></pre></div>\n\n<h4 id=\"6-visualization-of-data-flow\">6. Visualization of Data Flow</h4>\n<p>For complex bugs involving multiple components, <strong>draw the data flow</strong> on paper or a whiteboard. Trace a test execution through:</p>\n<ol>\n<li><strong>CLI arguments</strong> → <code>Configuration</code> object</li>\n<li><strong>File patterns</strong> → concrete paths via <code>resolve_patterns_to_paths()</code></li>\n<li><strong>Module discovery</strong> → <code>TestCase</code> objects</li>\n<li><strong>Fixture resolution</strong> → dependency graph resolution</li>\n<li><strong>Test execution</strong> → <code>TestResult</code> creation</li>\n<li><strong>Result collection</strong> → statistics and reporting</li>\n</ol>\n<p>Mark where actual behavior diverges from expected behavior. This often reveals missing transformations or incorrect assumptions about data formats.</p>\n<h4 id=\"7-binary-search-debugging\">7. Binary Search Debugging</h4>\n<p>For particularly elusive bugs, use a <strong>binary search approach</strong> through the codebase:</p>\n<ol>\n<li>Add a check halfway through the execution pipeline (e.g., after discovery but before fixture setup)</li>\n<li>Verify state is correct at that point</li>\n<li>If bug occurs before that point, check halfway through the first half</li>\n<li>If bug occurs after that point, check halfway through the second half</li>\n<li>Repeat until you isolate the exact location</li>\n</ol>\n<p>This is especially effective for <strong>intermittent bugs</strong> or those with complex triggering conditions.</p>\n<h4 id=\"8-version-comparison\">8. Version Comparison</h4>\n<p>If a feature worked in an earlier milestone but now fails, use <strong>git diff</strong> to compare the working version with the current version. Focus on changes to the component exhibiting the bug.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Compare current with last known good commit</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">git</span><span style=\"color:#9ECBFF\"> diff</span><span style=\"color:#9ECBFF\"> LAST_GOOD_COMMIT</span><span style=\"color:#79B8FF\"> --</span><span style=\"color:#9ECBFF\"> component/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Or use git bisect to automatically find the breaking commit</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">git</span><span style=\"color:#9ECBFF\"> bisect</span><span style=\"color:#9ECBFF\"> start</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">git</span><span style=\"color:#9ECBFF\"> bisect</span><span style=\"color:#9ECBFF\"> bad</span><span style=\"color:#6A737D\">  # Current commit is broken</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">git</span><span style=\"color:#9ECBFF\"> bisect</span><span style=\"color:#9ECBFF\"> good</span><span style=\"color:#9ECBFF\"> LAST_GOOD_COMMIT</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Git will guide you through testing intermediate commits</span></span></code></pre></div>\n\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While the main body focuses on conceptual debugging approaches, this section provides concrete code and tools to implement those strategies.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Debug Logging</td>\n<td>Custom <code>debug_log()</code> function with environment variable toggle</td>\n<td>Structured logging with <code>logging</code> module, configurable levels, and file/stream handlers</td>\n</tr>\n<tr>\n<td>Interactive Debugging</td>\n<td>Python&#39;s built-in <code>pdb</code> with manual breakpoints</td>\n<td><code>ipdb</code> for IPython-enhanced debugging, or <code>debugpy</code> for VS Code remote debugging</td>\n</tr>\n<tr>\n<td>State Inspection</td>\n<td>Manual print statements in strategic locations</td>\n<td><code>inspect</code> module for runtime introspection of call stacks and object graphs</td>\n</tr>\n<tr>\n<td>Test Isolation</td>\n<td><code>unittest.mock</code> for mocking dependencies</td>\n<td>Property-based testing with <code>hypothesis</code> to generate edge cases</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Manual drawing or ASCII diagrams</td>\n<td>Graphviz for generating dependency graphs, or custom HTML reports</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<p>Add debugging utilities in a dedicated module:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n  __init__.py\n  cli.py\n  discover.py\n  runner.py\n  fixtures.py\n  assertions.py\n  reporter.py\n  debug/                    # ← New debugging module\n    __init__.py\n    logger.py              # Debug logging utilities\n    inspector.py           # State inspection functions  \n    hooks.py               # Debug hooks for tests\n    visualizer.py          # Data flow visualization (optional)\n  utils/\n    __init__.py</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p>Here&#39;s complete, ready-to-use debug logging infrastructure:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># apollo/debug/logger.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global debug state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEBUG_ENABLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> os.environ.get(</span><span style=\"color:#9ECBFF\">\"APOLLO_DEBUG\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"0\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"1\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEBUG_LEVEL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(os.environ.get(</span><span style=\"color:#9ECBFF\">\"APOLLO_DEBUG_LEVEL\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEBUG_COMPONENTS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> os.environ.get(</span><span style=\"color:#9ECBFF\">\"APOLLO_DEBUG_COMPONENTS\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">).split(</span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DebugLogger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Structured debug logger for the framework.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, component: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.component </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.thread_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.get_ident()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log a debug message if debugging is enabled.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> DEBUG_ENABLED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> level </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> DEBUG_LEVEL</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> DEBUG_COMPONENTS</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.component </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> DEBUG_COMPONENTS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.strftime(</span><span style=\"color:#9ECBFF\">\"%H:%M:%S\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"T</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.thread_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> threading.active_count() </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"[</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">] [</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">thread_info</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">] [</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.component</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">              file</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">sys.stderr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Pretty-print data structures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            import</span><span style=\"color:#E1E4E8\"> pprint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            formatted </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pprint.pformat(data, </span><span style=\"color:#FFAB70\">width</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">compact</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> formatted.splitlines():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"    </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">file</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">sys.stderr)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scope</span><span style=\"color:#E1E4E8\">(self, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">context):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for timing and logging operation scope.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.log(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"START </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, context)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.log(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"ERROR </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.log(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"END </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">duration</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s)\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Convenience function</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_logger</span><span style=\"color:#E1E4E8\">(component: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> DebugLogger:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> DebugLogger(component)</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p>Here are diagnostic functions to add to key components:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># apollo/debug/inspector.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..fixtures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> FixtureLifecycleManager, FixtureRegistry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..runner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleRunner</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> inspect_fixture_state</span><span style=\"color:#E1E4E8\">(lifecycle_manager: FixtureLifecycleManager) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Return current state of fixture system for debugging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Dictionary with keys:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - cached_fixtures: List of (cache_key, value_type) tuples</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - active_generators: List of fixture names with pending teardown</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - dependency_graph: Adjacency list of fixture dependencies</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract cache keys and value types from lifecycle_manager._cache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract active generators from lifecycle_manager._generators  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Build dependency graph from fixture registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Format into a readable dictionary structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Include scope information for each cached fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> diagnose_test_failure</span><span style=\"color:#E1E4E8\">(test_result, runner: SimpleRunner) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Analyze a test failure and provide diagnostic information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Dictionary with potential causes and suggestions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if failure is assertion vs exception</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If assertion, extract expected/actual values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check fixture dependencies for the test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Look for common patterns (e.g., float comparison, missing imports)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return structured diagnosis with suggested fixes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># apollo/runner.py - Add diagnostic method to SimpleRunner</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleRunner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # ... existing code ...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_execution_context</span><span style=\"color:#E1E4E8\">(self, test_case) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Return the execution context for a test (for debugging).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This shows what fixtures will be injected, their values,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        and other runtime context.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get test function signature parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each parameter, check if matching fixture exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For existing fixtures, get their current cached value or None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return dictionary mapping param -> (fixture_name, has_value, value_type)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Use <code>inspect.signature()</code></strong> for safe parameter inspection instead of <code>inspect.getargspec()</code> (deprecated).</li>\n<li><strong>For parallel debugging</strong>, set <code>multiprocessing</code> start method to <code>&#39;spawn&#39;</code> for cleaner process isolation: <code>multiprocessing.set_start_method(&#39;spawn&#39;, force=True)</code>.</li>\n<li><strong>To catch pickling errors</strong> in parallel execution, wrap test execution in try-except and log the exact object that fails serialization.</li>\n<li><strong>Use <code>sys.settrace()</code></strong> or <code>threading.settrace()</code> for low-level execution tracing in single-threaded debugging scenarios.</li>\n<li><strong>For memory leaks</strong>, use <code>tracemalloc</code> to track fixture objects that aren&#39;t being garbage collected after teardown.</li>\n</ul>\n<h4 id=\"f-debugging-tips\">F. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong><code>pickle.PickleError</code> in parallel mode</strong></td>\n<td>Test case or fixture contains non-pickleable objects (e.g., lambda functions, database connections).</td>\n<td>1. Check which object is being pickled when error occurs.<br>2. Add <code>__reduce__</code> method to custom objects.<br>3. Use <code>pickle.dumps()</code> to test serialization.</td>\n<td>1. Make test dependencies pickleable.<br>2. Recreate non-pickleable objects in worker processes.<br>3. Use <code>multiprocessing.Queue</code> for sending results instead of return values.</td>\n</tr>\n<tr>\n<td><strong>Tests pass locally but fail in CI</strong></td>\n<td>Environment differences: Python version, working directory, missing dependencies, or timing issues.</td>\n<td>1. Compare Python versions (<code>sys.version</code>).<br>2. Check current working directory.<br>3. Look for absolute paths in tests.<br>4. Add longer timeouts for slower CI environments.</td>\n<td>1. Use relative paths and <code>pathlib.Path</code>.<br>2. Isolate environment-specific code behind abstractions.<br>3. Add CI-specific configuration or skips.</td>\n</tr>\n<tr>\n<td><strong>Fixture teardown called multiple times</strong></td>\n<td><code>teardown_scope()</code> invoked redundantly due to incorrect scope boundary detection.</td>\n<td>1. Add log to <code>teardown_scope()</code> showing caller.<br>2. Check if multiple tests trigger same scope teardown.<br>3. Verify scope counting logic.</td>\n<td>1. Implement reference counting for scope teardown.<br>2. Use <code>weakref.finalize</code> for automatic cleanup.<br>3. Track active tests per scope and only teardown when last test completes.</td>\n</tr>\n</tbody></table>\n<h4 id=\"g-milestone-checkpoint-for-debugging\">G. Milestone Checkpoint for Debugging</h4>\n<p>After implementing the debugging utilities, verify they work:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test debug logging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">APOLLO_DEBUG</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">1</span><span style=\"color:#B392F0\"> python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> apollo</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> tests/example.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should show debug output like:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># [14:30:25] [Discover] Scanning module tests.example</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># [14:30:25] [Discover] Found test test_addition</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test fixture inspection</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import apollo</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from apollo.debug import inspect_fixture_state</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">@apollo.fixture</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">def my_fixture():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    return {'data': 42}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Get lifecycle manager (might need to access internal)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Call inspect_fixture_state() and print result</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test minimal reproduction creation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Create a simple test that demonstrates a bug, then run:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">APOLLO_DEBUG</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">1</span><span style=\"color:#E1E4E8\"> APOLLO_DEBUG_COMPONENTS</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">fixtures</span><span style=\"color:#B392F0\"> python</span><span style=\"color:#9ECBFF\"> reproduce_bug.py</span></span></code></pre></div>\n\n<p>The debugging system should help you quickly identify issues during development and provide clear insights into the framework&#39;s internal state. As you encounter new bugs, add them to the Common Bugs table with their solutions to build institutional knowledge.</p>\n<h2 id=\"13-future-extensions\">13. Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section explores potential enhancements that build upon all four completed milestones, extending the framework&#39;s capabilities beyond the core educational goals.</p>\n</blockquote>\n<p>The architecture of Project Apollo has been designed with <strong>extensibility</strong> as a core principle. The component-based pipeline, clear separation of concerns, and well-defined data structures create a solid foundation upon which advanced features can be built. This section explores several <strong>future extensions</strong> that could transform the framework from a learning tool into a more powerful, production-ready system. Each idea is presented with a <strong>mental model</strong>, a description of how it would integrate with the existing architecture, and an analysis of what changes would be required.</p>\n<h3 id=\"ideas-for-extension\">Ideas for Extension</h3>\n<p>The following extensions represent natural evolutions of the test framework, addressing common needs in real-world testing workflows. They are listed in approximate order of increasing complexity and architectural impact.</p>\n<hr>\n<h4 id=\"1-plugin-system-the-modular-toolbox\">1. Plugin System: The Modular Toolbox</h4>\n<p><strong>Mental Model: The App Store for Test Features</strong>\nThink of the plugin system as an <strong>app store</strong> or <strong>modular toolbox</strong> that allows third-party developers to add new capabilities without modifying the framework&#39;s core. Just as power tools can be attached to a common drill base, plugins can hook into the framework&#39;s lifecycle to add new reporters, discovery mechanisms, assertion matchers, or fixture types.</p>\n<p><strong>Description:</strong>\nA plugin system would allow developers to extend the framework&#39;s behavior at defined <strong>extension points</strong>. Plugins would be discoverable (e.g., via entry points in <code>setup.py</code> or <code>pyproject.toml</code>) and would integrate with the main pipeline. Key extension points would include:</p>\n<ul>\n<li><strong>Discovery Hooks</strong>: Modify how tests are discovered (e.g., scanning non-Python files).</li>\n<li><strong>Test Execution Wrappers</strong>: Add pre/post-processing for each test run (e.g., resource monitoring).</li>\n<li><strong>Reporting Sinks</strong>: Add new output formats beyond console and JUnit XML.</li>\n<li><strong>CLI Command Addition</strong>: Add entirely new subcommands to the CLI (e.g., <code>apollo bench</code> for benchmarking).</li>\n<li><strong>Fixture Injection Overrides</strong>: Provide alternative resolution strategies for fixture dependencies.</li>\n</ul>\n<p><strong>Architectural Accommodation:</strong>\nThe current architecture is a <strong>linear pipeline</strong>, which is simple but not inherently pluggable. To accommodate plugins, the framework would need to evolve into a <strong>publish-subscribe</strong> or <strong>middleware</strong> model. The <code>Runner</code>, <code>Discoverer</code>, and <code>Reporter</code> would need to expose <strong>hook registries</strong> where plugins can register callbacks.</p>\n<blockquote>\n<p><strong>Decision: Plugin Architecture Pattern</strong></p>\n<ul>\n<li><strong>Context</strong>: The framework needs to be extended by users without forking the core codebase.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Subclassing</strong>: Require users to subclass core components and override methods.</li>\n<li><strong>Middleware Pipeline</strong>: Define a chain of processors for each major operation (discovery, execution, reporting).</li>\n<li><strong>Event Bus</strong>: Emit events at key lifecycle points (e.g., <code>test_discovered</code>, <code>test_started</code>, <code>test_finished</code>) and allow plugins to subscribe.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Adopt a hybrid <strong>event bus + middleware</strong> model. Simple hooks use events; complex transformations use middleware interfaces.</li>\n<li><strong>Rationale</strong>: Events are great for side effects (logging, monitoring). Middleware is better for transformational operations (modifying the test suite, intercepting fixture values). The hybrid model provides flexibility without overcomplicating simple use cases.</li>\n<li><strong>Consequences</strong>: Increases framework complexity significantly. Requires careful design of event/middleware interfaces to remain stable across versions. Enables a rich ecosystem but adds maintenance burden.</li>\n</ul>\n</blockquote>\n<p><strong>Plugin System Options Comparison:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Viability with Current Architecture</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Subclassing</td>\n<td>Simple to implement, familiar OOP pattern</td>\n<td>Tight coupling, requires deep knowledge of internals, hard to compose multiple extensions</td>\n<td>High – minimal changes, but poor user experience</td>\n</tr>\n<tr>\n<td>Middleware Pipeline</td>\n<td>Powerful, composable, clear data flow</td>\n<td>More complex to implement, can become a &quot;tower of middleware&quot;</td>\n<td>Medium – requires refactoring core components into processor chains</td>\n</tr>\n<tr>\n<td>Event Bus</td>\n<td>Loose coupling, easy to add side-effect plugins</td>\n<td>Harder to modify core data (events are typically immutable), can lead to event spaghetti</td>\n<td>Medium – requires adding an event emitter to key lifecycle methods</td>\n</tr>\n<tr>\n<td>Hybrid Model</td>\n<td>Best of both worlds, flexible for different extension types</td>\n<td>Most complex to design and implement</td>\n<td>Low – significant architectural redesign required</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Introduce a <code>PluginRegistry</code> component that loads plugins at startup.</li>\n<li>Define a set of standard event types (e.g., <code>TestDiscoveryEvent</code>, <code>TestExecutionEvent</code>) and emit them at appropriate points.</li>\n<li>Refactor the <code>Runner</code> and <code>Discoverer</code> to use injectable middleware components.</li>\n<li>Update the <code>CLI Parser</code> to accept <code>--plugin</code> arguments and load plugins from specified paths.</li>\n</ol>\n<hr>\n<h4 id=\"2-custom-markers-and-filtering-the-labeling-system\">2. Custom Markers and Filtering: The Labeling System</h4>\n<p><strong>Mental Model: Color-Coded Sticky Notes</strong>\nImagine being able to place <strong>color-coded sticky notes</strong> on tests: red for &quot;slow&quot;, green for &quot;integration&quot;, yellow for &quot;flaky&quot;. The test conductor can then filter which tests to run based on these labels, just as you might pull only the &quot;urgent&quot; notes from a board.</p>\n<p><strong>Description:</strong>\nMarkers are metadata annotations attached to test functions or classes that allow for <strong>categorization</strong> and <strong>selective execution</strong>. Users could mark tests as <code>@slow</code>, <code>@integration</code>, <code>@database</code>, etc. The CLI would gain options to run only tests with certain markers (<code>-m integration</code>), exclude tests with markers (<code>-m &quot;not slow&quot;</code>), or combine markers with boolean logic.</p>\n<p><strong>Architectural Accommodation:</strong>\nThe current <code>TestCase</code> data structure already has a <code>fixtures</code> list; adding a <code>markers: set[str]</code> field would be trivial. The main challenge is in <strong>discovery</strong> (reading decorators) and <strong>filtering</strong>. The <code>Discoverer</code> would need to inspect decorators and populate the <code>markers</code> field. The <code>CLI Parser</code> would need to accept marker expressions, and the <code>Runner</code> (or a new <code>Filter</code> component) would need to filter the <code>TestSuite</code> based on those expressions before execution.</p>\n<p><strong>Marker Implementation Options:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Integration Path</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Decorator-based (<code>@mark(&#39;slow&#39;)</code>)</td>\n<td>Pythonic, explicit, easy to parse</td>\n<td>Requires modifying test code, cannot mark third-party tests</td>\n<td>Extend <code>_find_tests_in_module</code> to check for <code>__markers__</code> attribute</td>\n</tr>\n<tr>\n<td>Docstring parsing (<code># marker: slow</code>)</td>\n<td>No runtime overhead, works with any function</td>\n<td>Fragile, depends on comment format, harder to parse</td>\n<td>Add docstring parsing in discovery phase</td>\n</tr>\n<tr>\n<td>External configuration file</td>\n<td>Centralized management, no code modification</td>\n<td>Disconnected from test code, maintenance burden</td>\n<td>Create a YAML/TOML mapping of test names to markers, merge during discovery</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Add <code>markers: Set[str]</code> field to <code>TestCase</code>.</li>\n<li>Extend <code>_find_tests_in_module</code> to detect marker decorators and populate the field.</li>\n<li>Add <code>-m/--marker</code> and <code>-k/--keyword</code> (for name substring filtering) options to <code>CLI Parser</code>.</li>\n<li>Create a <code>TestFilter</code> component that takes a <code>TestSuite</code> and a filter expression, returning a filtered <code>TestSuite</code>.</li>\n<li>Integrate the <code>TestFilter</code> between <code>Discoverer</code> and <code>Runner</code>.</li>\n</ol>\n<hr>\n<h4 id=\"3-parameterized-tests-the-test-factory\">3. Parameterized Tests: The Test Factory</h4>\n<p><strong>Mental Model: The Cookie Cutter</strong>\nA parameterized test is like a <strong>cookie cutter</strong> – a single test definition (the cutter) that can produce multiple test instances (cookies) with different inputs (dough shapes). This avoids copy-pasting test code for each input variant.</p>\n<p><strong>Description:</strong>\nParameterization allows a single test function to be run multiple times with different input arguments. For example, <code>@parametrize(&#39;input,expected&#39;, [(1, 2), (3, 4)])</code> would generate two test cases. Each generated test should appear as a separate entry in discovery and reporting (with distinct <code>nodeid</code> values). Parameters can be sourced from lists, functions, or external files.</p>\n<p><strong>Architectural Accommodation:</strong>\nThis feature heavily impacts the <strong>discovery</strong> and <strong>test case identity</strong> layers. The <code>Discoverer</code> would need to recognize parameterization decorators and generate multiple <code>TestCase</code> objects from a single function. Each generated <code>TestCase</code> must have a unique <code>nodeid</code> (e.g., <code>test_foo[1-2]</code>) and must encapsulate the parameter values. The <code>Runner</code> must then call the original function with the bound parameters.</p>\n<p><strong>Parameterization Strategy ADR:</strong></p>\n<blockquote>\n<p><strong>Decision: Generation Time vs. Runtime Parameterization</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to decide when parameter values are bound to test functions.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Generation Time</strong>: During discovery, create a separate <code>TestCase</code> for each parameter set. Each <code>TestCase</code> has a closure that binds the parameters.</li>\n<li><strong>Runtime</strong>: During execution, the runner reads the parameter set and calls the test function with those values dynamically.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: <strong>Generation Time</strong>.</li>\n<li><strong>Rationale</strong>: Generation time creates clear separation of test instances, making reporting, filtering, and parallel execution straightforward. Each parameterized variant is a first-class test with its own result. Runtime parameterization would require the runner to handle loops, complicating result collection and making it harder to re-run a specific failing variant.</li>\n<li><strong>Consequences</strong>: Increases memory usage (many <code>TestCase</code> objects). Requires careful design of <code>nodeid</code> to uniquely identify each variant. Enables fine-grained control over test execution.</li>\n</ul>\n</blockquote>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Define a <code>@parametrize</code> decorator that stores parameter sets in a <code>__param_sets__</code> attribute on the function.</li>\n<li>Extend <code>_find_tests_in_module</code> to detect this attribute and generate multiple <code>TestCase</code> instances.</li>\n<li>Augment <code>TestCase</code> with a <code>parameters: Dict[str, Any]</code> field to store the bound values.</li>\n<li>Modify <code>SimpleRunner.run_test</code> to inject parameters into the test function call (either as keyword arguments or via <code>functools.partial</code>).</li>\n<li>Update the <code>Reporter</code> to display parameter values in failure messages.</li>\n</ol>\n<hr>\n<h4 id=\"4-test-doubles-and-mocking-integration-the-understudy-system\">4. Test Doubles and Mocking Integration: The Understudy System</h4>\n<p><strong>Mental Model: Hollywood Stunt Doubles</strong>\nMocks and stubs are like <strong>stunt doubles</strong> for your code&#39;s dependencies. They stand in for real objects during testing, allowing you to simulate specific behaviors (like throwing an error) without triggering real side effects (like charging a credit card).</p>\n<p><strong>Description:</strong>\nIntegrate a mocking library (e.g., <code>unittest.mock</code> pattern) directly into the framework. Provide convenient APIs to patch objects, create mock instances, and make assertions about how they were called. This could include automatic cleanup of patches after each test (integrating with the fixture system) and specialized assertion matchers for mock calls.</p>\n<p><strong>Architectural Accommodation:</strong>\nThe current architecture has no built-in mocking support. Integration would primarily involve providing a <strong>convenient API</strong> and ensuring <strong>proper cleanup</strong>. The fixture system&#39;s teardown mechanisms could be leveraged to automatically undo patches. A new <code>MockFixture</code> could be added to the standard library. The <code>Assertion Engine</code> could be extended with matchers like <code>assert_called_once_with</code>.</p>\n<p><strong>Mocking Integration Approaches:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Implementation Path</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Bundled Wrapper</strong>: Provide thin wrappers around <code>unittest.mock</code></td>\n<td>Leverages standard library, robust, well-tested</td>\n<td>Adds dependency on stdlib mocking concepts, less innovation</td>\n<td>Add <code>apollo.mock</code> module that re-exports and extends <code>unittest.mock</code></td>\n</tr>\n<tr>\n<td><strong>Native Implementation</strong>: Build a custom mocking system</td>\n<td>Full control, can optimize for framework idioms</td>\n<td>Huge effort, likely buggy, reinvents the wheel</td>\n<td>Create <code>Mock</code>, <code>MagicMock</code>, <code>patch</code> classes from scratch (not recommended)</td>\n</tr>\n<tr>\n<td><strong>Plugin-Based</strong>: Allow mocking libraries to integrate via plugin system</td>\n<td>Keeps core lean, encourages ecosystem diversity</td>\n<td>Users must install separate packages, inconsistent APIs</td>\n<td>Define mocking extension points in the plugin system</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Add an <code>apollo.mock</code> module that imports and extends <code>unittest.mock</code>.</li>\n<li>Create a <code>mock_fixture</code> helper that returns a Mock and ensures reset after test.</li>\n<li>Integrate automatic patching cleanup into the test isolation mechanism (ensure <code>patch.stopall()</code> is called after each test).</li>\n<li>Add <code>assert_called_with</code>-style matchers to the <code>Assertion Engine</code>.</li>\n</ol>\n<hr>\n<h4 id=\"5-parallel-test-execution-with-fixture-scope-awareness\">5. Parallel Test Execution with Fixture Scope Awareness</h4>\n<p><strong>Mental Model: The Assembly Line with Shared Workstations</strong>\nRunning tests in parallel is like an <strong>assembly line</strong> with multiple workers. Some workstations (fixtures) are personal (function-scoped), some are shared by a team (class-scoped), and some are factory-wide (session-scoped). The line manager must schedule tasks so workers don&#39;t conflict over shared resources.</p>\n<p><strong>Description:</strong>\nMilestone 1 includes basic parallel execution, but it assumes all tests are independent. In reality, tests that share <strong>fixtures with scope above FUNCTION</strong> (CLASS, MODULE, SESSION) cannot run truly concurrently if they use the same fixture instance. The framework needs to <strong>detect fixture dependencies</strong> and <strong>schedule tests</strong> to avoid conflicts—either by running them sequentially or by creating duplicate fixture instances.</p>\n<p><strong>Architectural Accommodation:</strong>\nThe current <code>Fixture</code> model already includes <code>scope</code> and <code>dependencies</code>. The <code>Runner</code> would need to analyze the fixture graph for each test and determine which tests can run in parallel without conflicting. This requires a <strong>scheduling algorithm</strong> that groups tests by their required fixture instances.</p>\n<p><strong>Scheduling Algorithm Options:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Dependency-Based Grouping</strong>: Group tests that share non-FUNCTION scoped fixtures</td>\n<td>Maximizes parallelism within groups, avoids conflicts</td>\n<td>Requires graph analysis, may create many small groups</td>\n<td>Medium – must compute fixture instance keys for each test</td>\n</tr>\n<tr>\n<td><strong>Sequential by Scope</strong>: Run all tests of a CLASS scope together, then next CLASS, etc.</td>\n<td>Simple to implement, guarantees no conflicts</td>\n<td>May serialize too much, reducing parallelism</td>\n<td>Low – just sort tests by their highest non-FUNCTION scope</td>\n</tr>\n<tr>\n<td><strong>Fixture Instance Locking</strong>: Lock fixture instances during use, queue tests waiting for same instance</td>\n<td>Fine-grained, can achieve high parallelism</td>\n<td>Complex deadlock avoidance, requires runtime locking</td>\n<td>High – needs a lock manager and wait queues</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Enhance <code>TestCase</code> to include a <code>required_fixture_instances: Set[Tuple[str, str]]</code> field, where the tuple is <code>(fixture_name, scope_id)</code>.</li>\n<li>Modify discovery to compute these instance keys based on fixture scope and test location.</li>\n<li>Implement a <code>Scheduler</code> component that takes a <code>TestSuite</code> and partitions it into runnable batches where tests in the same batch do not share non-FUNCTION fixture instances.</li>\n<li>Update the <code>ParallelRunner</code> to use the <code>Scheduler</code>&#39;s batches instead of naive chunking.</li>\n</ol>\n<hr>\n<h4 id=\"6-snapshot-testing-the-golden-master\">6. Snapshot Testing: The Golden Master</h4>\n<p><strong>Mental Model: The Photographic Evidence</strong>\nSnapshot testing is like taking a <strong>photograph</strong> of your code&#39;s output the first time it runs. On subsequent runs, you compare new output to the stored photograph. If they differ, you either found a bug (and update the snapshot) or detected an intentional change (and accept the new snapshot).</p>\n<p><strong>Description:</strong>\nSnapshot testing automatically captures the serialized output of a function (e.g., JSON, HTML, text) and stores it in a file. Future test runs compare the new output to the stored snapshot. The framework would provide an <code>assert_match_snapshot(actual)</code> assertion that loads the appropriate snapshot file, compares, and shows a diff on mismatch. It would also include a CLI command to update snapshots (<code>--update-snapshots</code>).</p>\n<p><strong>Architectural Accommodation:</strong>\nThis extension fits neatly into the <strong>Assertion Engine</strong> and <strong>CLI</strong>. A new <code>SnapshotMatcher</code> could be added to the matchers API. The main complexity is <strong>snapshot storage</strong> (file management, version control friendliness) and <strong>diff presentation</strong>. The existing <code>DiffResult</code> infrastructure could be reused for comparison.</p>\n<p><strong>Snapshot Storage Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Single file per test</strong>: Each test has a <code>.snap</code> file with its snapshots</td>\n<td>Easy to manage, clear mapping</td>\n<td>Many files, may clutter project</td>\n<td>Preferred for clarity and git diff readability</td>\n</tr>\n<tr>\n<td><strong>Centralized snapshot registry</strong>: One file mapping test IDs to snapshots</td>\n<td>Centralized, easy to prune</td>\n<td>Large file, merge conflicts likely, harder to read diffs</td>\n<td>Not recommended</td>\n</tr>\n<tr>\n<td><strong>Inline snapshots</strong>: Store snapshot as a string literal in the test file itself</td>\n<td>No external files, self-contained</td>\n<td>Bloats test files, harder to update programmatically</td>\n<td>Possible but requires AST manipulation</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Create a <code>SnapshotStore</code> component that manages reading/writing snapshot files, keyed by test <code>nodeid</code>.</li>\n<li>Add a <code>snapshot</code> matcher that implements <code>__matches__</code> by comparing actual to stored snapshot.</li>\n<li>Extend the CLI with <code>--update-snapshots</code> flag that writes new snapshots instead of comparing.</li>\n<li>Integrate snapshot cleanup into the <code>Reporter</code> (e.g., list unused snapshots).</li>\n</ol>\n<hr>\n<h4 id=\"7-property-based-testing-the-fuzzing-lab\">7. Property-Based Testing: The Fuzzing Lab</h4>\n<p><strong>Mental Model: The Stress-Testing Machine</strong>\nProperty-based testing is like a <strong>stress-testing machine</strong> that feeds your code random inputs, checking that certain properties always hold. Instead of testing specific examples, you define rules like &quot;for any list, sorting it twice should equal sorting it once&quot; and let the machine try to break the rule.</p>\n<p><strong>Description:</strong>\nIntegrate property-based testing à la Hypothesis or QuickCheck. Users would write test functions that accept parameters, and the framework would automatically generate many random inputs (including edge cases) to validate the property. The framework would need to provide <strong>generators</strong> for common types, <strong>shrinking</strong> of failing cases to minimal examples, and <strong>replay</strong> of failing seeds.</p>\n<p><strong>Architectural Accommodation:</strong>\nThis is a major extension that would essentially add a new <strong>test generation engine</strong>. Property-based tests would be discovered as a special test type. The <code>Runner</code> would need to invoke the property-based engine, which would run multiple iterations, handle failures, and report the minimal counterexample. The existing <code>Assertion Engine</code> could still be used for property checks.</p>\n<p><strong>Property-Based Integration Depth:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Depth</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Feasibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Thin Wrapper</strong>: Integrate Hypothesis as an external dependency</td>\n<td>Powerful, mature library, less work</td>\n<td>Adds heavy dependency, less control</td>\n<td>High – provide <code>@apollo.given</code> decorator that delegates to Hypothesis</td>\n</tr>\n<tr>\n<td><strong>Native Lightweight Engine</strong>: Build a simple generator/shrinker for core types</td>\n<td>No external dependencies, full control</td>\n<td>Limited generator variety, poor shrinking, bug-prone</td>\n<td>Medium – significant development effort, likely inferior to Hypothesis</td>\n</tr>\n<tr>\n<td><strong>Plugin</strong>: Make property-based testing a plugin</td>\n<td>Keeps core lean, allows multiple backends</td>\n<td>Fragmentation, users must install plugin</td>\n<td>Medium – depends on plugin system being implemented first</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Add a <code>@given</code> decorator that marks a test as property-based and attaches generator strategies.</li>\n<li>Create a <code>PropertyBasedRunner</code> that extends <code>SimpleRunner</code> to handle generation, iteration, and shrinking.</li>\n<li>Extend <code>TestResult</code> to include counterexample data and seed for reproduction.</li>\n<li>Update the <code>Reporter</code> to display shrunk counterexamples and seeds.</li>\n</ol>\n<hr>\n<h4 id=\"8-code-coverage-integration-the-coverage-map\">8. Code Coverage Integration: The Coverage Map</h4>\n<p><strong>Mental Model: The Heat Map of Execution</strong>\nCode coverage tools generate a <strong>heat map</strong> showing which parts of your codebase were &quot;touched&quot; during test execution. Integrating coverage gives immediate feedback on test thoroughness and helps identify untested corners.</p>\n<p><strong>Description:</strong>\nIntegrate with coverage measurement tools (e.g., <code>coverage.py</code>) to automatically collect and report coverage data during test runs. The CLI could add <code>--coverage</code> to enable collection, and the reporter could print a summary or generate HTML reports. Coverage could be measured per test, per module, or for the entire suite.</p>\n<p><strong>Architectural Accommodation:</strong>\nCoverage measurement is a <strong>cross-cutting concern</strong> that touches test execution. The <code>Runner</code> would need to start coverage before running tests and stop it after. The <code>Reporter</code> would need to format coverage data. The main challenge is <strong>correctly measuring coverage in parallel execution</strong> (requires coverage workers or merging coverage data from multiple processes).</p>\n<p><strong>Coverage Collection Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Implementation Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Subprocess Wrapping</strong>: Run the entire test process under coverage</td>\n<td>Simple, works with any runner</td>\n<td>Cannot measure coverage per test, hard to integrate with parallel workers</td>\n<td>Use <code>coverage run -m apollo.cli</code> – trivial but limited</td>\n</tr>\n<tr>\n<td><strong>In-Process API</strong>: Use coverage&#39;s Python API to start/stop around each test</td>\n<td>Fine-grained, can attribute coverage to specific tests</td>\n<td>Overhead per test, complex with parallelism</td>\n<td>Integrate with <code>SimpleRunner.run_test</code>, use <code>coverage.Coverage</code> instance</td>\n</tr>\n<tr>\n<td><strong>Separate Coverage Workers</strong>: Each parallel worker collects its own coverage, merge at end</td>\n<td>Works with parallelism, detailed data</td>\n<td>Complex merging, data duplication</td>\n<td>Use <code>FixtureLifecycleManager</code>-like merging of coverage data structures</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Add <code>coverage_enabled</code> and <code>coverage_config</code> fields to <code>Configuration</code>.</li>\n<li>Modify <code>SimpleRunner</code> to start/stop coverage measurement per test or per suite.</li>\n<li>Create a <code>CoverageMerger</code> component for parallel runs.</li>\n<li>Extend <code>Reporter</code> to output coverage summary and optionally generate HTML reports.</li>\n</ol>\n<hr>\n<h4 id=\"9-test-retries-and-flaky-test-detection-the-reliability-engineer\">9. Test Retries and Flaky Test Detection: The Reliability Engineer</h4>\n<p><strong>Mental Model: The Intermittent Fault Detector</strong>\nFlaky tests are like <strong>intermittent electrical faults</strong> – they sometimes fail, sometimes pass under identical conditions. A retry system acts as a <strong>reliability engineer</strong> that re-runs failing tests to see if the failure persists, automatically marking transient failures as &quot;flaky&quot; and persistent failures as true failures.</p>\n<p><strong>Description:</strong>\nAdd options to automatically retry failing tests a configurable number of times (<code>--retries=3</code>). If a test passes on a retry, it could be marked as <strong>flaky</strong> in the report. The framework could also track flakiness statistics over time and provide warnings. Advanced features could include <strong>detecting flaky patterns</strong> (e.g., failures only on certain times of day) and <strong>quarantining</strong> flaky tests.</p>\n<p><strong>Architectural Accommodation:</strong>\nRetries are a <strong>runner-level concern</strong>. The <code>SimpleRunner.run_test</code> would need to be wrapped in a retry loop. Results aggregation would need to handle the &quot;flaky&quot; status (perhaps a new <code>TestStatus.FLAKY</code>). The <code>Reporter</code> would need to report flaky tests separately.</p>\n<p><strong>Retry Implementation Considerations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Consideration</th>\n<th>Options</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>When to retry</strong></td>\n<td>Only on test failure (not error), or on both failure and error?</td>\n<td>Retry only on assertion failures (<code>FAILED</code>), not on Python errors (<code>ERRORED</code>), as errors often indicate setup issues that won&#39;t fix themselves.</td>\n</tr>\n<tr>\n<td><strong>Isolation between retries</strong></td>\n<td>Complete reset (re-import modules, re-create fixtures) vs. lightweight (keep fixture instances)</td>\n<td>Complete reset to eliminate state leakage as cause of flakiness. Use <code>importlib.reload</code> and fixture re-creation.</td>\n</tr>\n<tr>\n<td><strong>Reporting</strong></td>\n<td>Show all attempts, or only final result?</td>\n<td>Show final result but log retry attempts in verbose mode. Add <code>--show-retries</code> flag.</td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Add <code>retries</code> and <code>retry_delay</code> fields to <code>Configuration</code>.</li>\n<li>Create a <code>RetryingRunner</code> wrapper that delegates to <code>SimpleRunner</code> and implements the retry loop.</li>\n<li>Add <code>TestStatus.FLAKY</code> and adjust reporting logic.</li>\n<li>Extend <code>TestResult</code> to include <code>attempts: List[TestResult]</code> for history.</li>\n</ol>\n<hr>\n<h4 id=\"10-interactive-debugger-and-pdb-integration-the-surgical-suite\">10. Interactive Debugger and PDB Integration: The Surgical Suite</h4>\n<p><strong>Mental Model: The Surgical Suite with Live Monitoring</strong>\nWhen a test fails, you often want to <strong>operate</strong> on the failing code immediately. Interactive debugging integration is like a <strong>surgical suite</strong> where you can pause execution at the point of failure, inspect variables, and step through code, all without leaving the test framework environment.</p>\n<p><strong>Description:</strong>\nAdd a <code>--pdb</code> flag that drops into the Python debugger (<code>pdb</code>) on test failure or error. More advanced features could include <strong>post-mortem debugging</strong> (automatically enter debugger after exception), <strong>breakpoint decorators</strong> (<code>@breakpoint</code>), and integration with IPython&#39;s richer debugger. The framework could also provide <strong>debugging fixtures</strong> that capture system state (logs, memory) on failure.</p>\n<p><strong>Architectural Accommodation:</strong>\nDebugging hooks would be integrated into the <strong>error handling</strong> layer. When a test raises an exception, the framework would intercept it and launch the debugger before proceeding to the next test. This requires careful handling of <strong>test isolation</strong> – the debugger should not leave global state modified for subsequent tests.</p>\n<p><strong>Debugger Integration Options:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Simple PDB</strong>: Call <code>pdb.post_mortem()</code> on any exception</td>\n<td>Standard library, no dependencies</td>\n<td>Basic interface, no IDE integration</td>\n<td>Wrap <code>run_test</code> in try/except and call post_mortem</td>\n</tr>\n<tr>\n<td><strong>IPython Debugger</strong>: Use <code>ipdb</code> if available, fallback to <code>pdb</code></td>\n<td>Richer features, better UX</td>\n<td>Requires IPython optional dependency</td>\n<td>Check for <code>ipdb</code> import, use <code>ipdb.post_mortem</code></td>\n</tr>\n<tr>\n<td><strong>Debugger Hooks</strong>: Provide hooks for IDE debuggers (e.g., VS Code)</td>\n<td>Seamless IDE integration</td>\n<td>Complex, requires emitting debugger protocol</td>\n<td>Set <code>PYTHONBREAKPOINT</code> environment variable or use <code>sys.breakpointhook</code></td>\n</tr>\n</tbody></table>\n<p><strong>Required Changes:</strong></p>\n<ol>\n<li>Add <code>pdb</code> flag to <code>Configuration</code>.</li>\n<li>Modify <code>SimpleRunner.run_test</code> to catch exceptions and invoke <code>pdb.post_mortem()</code> when flag is set.</li>\n<li>Ensure debugger sessions don&#39;t break test isolation (warn user about state contamination).</li>\n<li>Optionally add <code>--pdb-failures</code> (debug only assertion failures) and <code>--pdb-errors</code> (debug all errors) flags.</li>\n</ol>\n<hr>\n<h3 id=\"summary-of-architectural-impact\">Summary of Architectural Impact</h3>\n<p>The table below summarizes the estimated impact of each extension on the existing components:</p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Components Most Affected</th>\n<th>Architectural Changes Required</th>\n<th>Effort Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plugin System</td>\n<td>All components, new PluginRegistry</td>\n<td>Major redesign to event/middleware model</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Custom Markers</td>\n<td>Discoverer, CLI Parser, new Filter</td>\n<td>Moderate – new fields and filtering logic</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Parameterized Tests</td>\n<td>Discoverer, TestCase, Runner</td>\n<td>Moderate – generation logic and nodeid changes</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Mocking Integration</td>\n<td>Assertion Engine, Fixture System</td>\n<td>Low – API layer addition</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Parallel + Fixture Awareness</td>\n<td>Runner, Discoverer, new Scheduler</td>\n<td>High – scheduling algorithm and dependency analysis</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Snapshot Testing</td>\n<td>Assertion Engine, CLI, new SnapshotStore</td>\n<td>Medium – file I/O and diff integration</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Property-Based Testing</td>\n<td>Runner, Discoverer, new Generator engine</td>\n<td>High – entirely new execution model</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Code Coverage</td>\n<td>Runner, Reporter, new CoverageMerger</td>\n<td>Medium – coverage API integration and merging</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Test Retries</td>\n<td>Runner, TestResult, Reporter</td>\n<td>Low – retry loop and status additions</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Interactive Debugger</td>\n<td>Runner, Error Handling</td>\n<td>Low – exception interception</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight:</strong> The architecture&#39;s clear separation of concerns allows many extensions to be developed <strong>in isolation</strong>. For example, snapshot testing primarily extends the Assertion Engine, while markers extend Discovery and Filtering. This modularity means you can pick and choose which extensions to implement based on need, without rewriting the entire framework.</p>\n</blockquote>\n<hr>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>While the full implementation of these extensions is beyond the scope of the core milestones, here is guidance for starting on the most feasible ones.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Recommended Libraries/Tools</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plugin System</td>\n<td><code>importlib.metadata</code> (Python 3.8+) for entry point discovery</td>\n<td>Use <code>entry_points</code> in <code>pyproject.toml</code> for plugin registration</td>\n</tr>\n<tr>\n<td>Mocking Integration</td>\n<td><code>unittest.mock</code> (standard library)</td>\n<td>Wrap, don&#39;t rewrite. Provide <code>apollo.mock</code> module.</td>\n</tr>\n<tr>\n<td>Snapshot Testing</td>\n<td><code>difflib</code> for diffing, <code>json</code>/<code>yaml</code> for serialization</td>\n<td>Consider <code>syrupy</code> or <code>snapshottest</code> for inspiration</td>\n</tr>\n<tr>\n<td>Property-Based Testing</td>\n<td><code>hypothesis</code> as optional dependency</td>\n<td>Integrate via plugin or optional extra: <code>apollo[property]</code></td>\n</tr>\n<tr>\n<td>Code Coverage</td>\n<td><code>coverage.py</code> as optional dependency</td>\n<td>Use its public API: <code>coverage.Coverage()</code></td>\n</tr>\n<tr>\n<td>Interactive Debugger</td>\n<td><code>pdb</code> (standard library)</td>\n<td>For advanced users, recommend <code>ipdb</code></td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File/Module Structure for Extensions:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>apollo/\n├── core/                 # Existing core components\n├── extensions/           # New directory for official extensions\n│   ├── __init__.py\n│   ├── markers.py       # Custom markers implementation\n│   ├── parameterize.py  # Parameterized tests\n│   ├── snapshots.py     # Snapshot testing\n│   ├── retries.py       # Test retries\n│   └── debugging.py     # PDB integration\n├── plugins/              # Plugin infrastructure (if implemented)\n│   ├── __init__.py\n│   ├── registry.py\n│   └── events.py\n└── cli.py               # Extend CLI with new flags</code></pre></div>\n\n<p><strong>Core Logic Skeleton for Custom Markers:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># In extensions/markers.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> functools</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> mark</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">marker_names: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Decorator to add markers to a test function or class.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> decorator</span><span style=\"color:#E1E4E8\">(obj):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(obj, </span><span style=\"color:#9ECBFF\">'__markers__'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            obj.__markers__ </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        obj.__markers__.update(marker_names)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> obj</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> decorator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_markers</span><span style=\"color:#E1E4E8\">(obj) -> Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Retrieve markers from a test function or class.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(obj, </span><span style=\"color:#9ECBFF\">'__markers__'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># In core/discoverer.py, augment _find_tests_in_module:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: After identifying a test callable, check for __markers__ attribute</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create TestCase with markers=set(obj.__markers__) if exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For test classes, inherit class markers to methods (union of class and method markers)</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton for Test Retries:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># In extensions/retries.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.runner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleRunner</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TestCase, TestResult, TestStatus</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryingRunner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_runner: SimpleRunner, max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">, delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_runner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> base_runner</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_retries</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_test</span><span style=\"color:#E1E4E8\">(self, test_case: TestCase) -> TestResult:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attempts: List[TestResult] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):  </span><span style=\"color:#6A737D\"># +1 for initial attempt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.base_runner.run_test(test_case)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            attempts.append(result)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If test passed, break loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If delay > 0 and not last attempt, sleep(delay)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If test failed (not errored) and attempts remain, continue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If test errored or no attempts remain, break</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Determine final status: </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If any attempt passed -> FLAKY (if different from first) or PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Else -> use last result's status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create final TestResult with aggregated attempts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints for Python:</strong></p>\n<ul>\n<li>Use <code>functools.wraps</code> when creating decorators to preserve function metadata.</li>\n<li>For plugin discovery, leverage <code>importlib.metadata.entry_points()</code> (Python ≥3.8) or the backport <code>importlib_metadata</code>.</li>\n<li>When implementing parallel fixture-aware scheduling, consider using <code>networkx</code> for graph analysis of fixture dependencies.</li>\n<li>For snapshot testing, use <code>json.dumps(obj, indent=2, sort_keys=True)</code> for deterministic serialization.</li>\n<li>Use <code>contextlib.ExitStack</code> to manage multiple patch objects in mocking integration.</li>\n</ul>\n<p><strong>Debugging Tips for Extension Development:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Markers not discovered on test functions</td>\n<td>Decorator not setting <code>__markers__</code> attribute</td>\n<td>Print <code>dir(test_func)</code> after decoration</td>\n<td>Ensure <code>mark</code> decorator correctly sets attribute</td>\n</tr>\n<tr>\n<td>Parameterized tests all have same nodeid</td>\n<td>Not generating unique IDs per parameter set</td>\n<td>Check <code>TestCase.nodeid</code> for each generated test</td>\n<td>Include parameter values in nodeid (e.g., <code>test_foo[input=1]</code>)</td>\n</tr>\n<tr>\n<td>Snapshot comparison always fails due to whitespace</td>\n<td>Serialization not normalized</td>\n<td>Compare raw strings with <code>repr()</code></td>\n<td>Use <code>json.dumps</code> with consistent formatting or strip whitespace</td>\n</tr>\n<tr>\n<td>Retries cause fixture teardown between attempts</td>\n<td>Runner creating new fixture context each attempt</td>\n<td>Check <code>FixtureLifecycleManager.get_fixture_value</code> logs</td>\n<td>Cache fixture instances across retries for same test</td>\n</tr>\n<tr>\n<td>Coverage data missing in parallel runs</td>\n<td>Each worker starts its own coverage, not merged</td>\n<td>Check coverage files in temp directories</td>\n<td>Implement <code>CoverageMerger</code> that combines <code>.coverage</code> files</td>\n</tr>\n</tbody></table>\n<p><strong>Next Steps for Learners:</strong>\nChoose one extension that aligns with your interests and implement it as a <strong>capstone project</strong>. Start by forking the completed Milestone 4 codebase. Implement the extension in the <code>extensions/</code> directory, and ensure it integrates smoothly with the existing components. Write tests for your extension using the framework itself (self-validation). This deepens your understanding of both test framework design and practical Python engineering.</p>\n<h2 id=\"14-glossary\">14. Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All four milestones, as this section defines terminology used throughout the entire design document.</p>\n</blockquote>\n<p>Clear terminology establishes a <strong>shared vocabulary</strong> that prevents confusion and ensures precise communication about the framework&#39;s architecture. This glossary defines all key terms, concepts, and component names used throughout this design document, providing a single reference point for understanding the system&#39;s language.</p>\n<h3 id=\"term-definitions\">Term Definitions</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>First Introduced In</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Actionable Error Messages</strong></td>\n<td>Error messages that not only describe what went wrong but also suggest potential fixes or next steps for the developer.</td>\n<td>Section 10: Error Handling and Edge Cases</td>\n</tr>\n<tr>\n<td><strong>Assertion Engine</strong></td>\n<td>The core component responsible for evaluating assertion conditions, comparing values, and generating helpful failure messages with diffs.</td>\n<td>Section 3: High-Level Architecture</td>\n</tr>\n<tr>\n<td><strong>AssertionFailure</strong></td>\n<td>A structured data type representing a failed assertion, containing the expected and actual values, a formatted message, and optional diff information.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Binary Search Debugging</strong></td>\n<td>A debugging technique where you repeatedly test halfway points in the execution flow to isolate the source of a bug.</td>\n<td>Section 12: Debugging Guide</td>\n</tr>\n<tr>\n<td><strong>CDATA Section</strong></td>\n<td>An XML construct (Character Data) that marks text content as not containing markup, allowing inclusion of special characters without escaping.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Circular Dependency</strong></td>\n<td>A problematic situation in the fixture system where fixture A depends on fixture B, and fixture B (directly or indirectly) depends on fixture A.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>Code Coverage Integration</strong></td>\n<td>A potential future extension that tracks which lines of source code are executed during test runs to measure test thoroughness.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>ComparisonContext</strong></td>\n<td>A configuration object that controls how equality comparisons are performed, with settings like tolerance, case sensitivity, and ordering.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Comparison Inspector</strong></td>\n<td>The mental model for the assertion engine as an examiner who compares artifacts and produces detailed comparison reports.</td>\n<td>Section 6: Component Design: Assertions &amp; Matchers</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>The primary data structure representing all CLI arguments and runtime settings, serving as the <strong>control panel</strong> for test execution.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>ConsoleFormatter</strong></td>\n<td>The component responsible for formatting test results into human-readable output for terminal display, including colors and progress indicators.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Control Panel</strong></td>\n<td>The mental model for the CLI as an interface providing knobs, switches, and dials to configure test execution behavior.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Convention-over-Configuration</strong></td>\n<td>A design paradigm where sensible defaults are used instead of requiring explicit configuration, reducing boilerplate code.</td>\n<td>Section 1: Context and Problem Statement</td>\n</tr>\n<tr>\n<td><strong>Cross-Validation</strong></td>\n<td>A testing strategy that verifies system behavior using multiple independent methods to increase confidence in correctness.</td>\n<td>Section 11: Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>Custom Markers</strong></td>\n<td>A potential future extension allowing metadata annotations on tests for categorization, filtering, and special handling.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>DebugLogger</strong></td>\n<td>A utility component for structured debug logging that helps trace execution flow and diagnose issues within framework components.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>DiffResult</strong></td>\n<td>A structured representation of differences between expected and actual values, containing both summary information and formatted diffs.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>DNA of the Test Framework</strong></td>\n<td>The mental model portraying data structures like <code>TestCase</code>, <code>TestResult</code>, and <code>Fixture</code> as fundamental instructions encoding framework behavior.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Error Isolation</strong></td>\n<td>The design principle that errors in one test should not affect the execution or reporting of other tests in the same suite.</td>\n<td>Section 10: Error Handling and Edge Cases</td>\n</tr>\n<tr>\n<td><strong>Evidence Examiner</strong></td>\n<td>The mental model for detailed comparison operations that produce side-by-side highlighting of differences between values.</td>\n<td>Section 6: Component Design: Assertions &amp; Matchers</td>\n</tr>\n<tr>\n<td><strong>Exit Code</strong></td>\n<td>The numeric value returned by a process to indicate success (0) or failure (non-zero), used by CI systems to detect test failures.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Fail-Fast vs. Fail-Safe</strong></td>\n<td>A design tradeoff between stopping test execution immediately on critical errors vs. continuing to collect as many results as possible.</td>\n<td>Section 10: Error Handling and Edge Cases</td>\n</tr>\n<tr>\n<td><strong>Fixture</strong></td>\n<td>A reusable resource or setup/teardown logic that provides test dependencies, managed by the framework with controlled lifecycle and scope.</td>\n<td>Section 3: High-Level Architecture</td>\n</tr>\n<tr>\n<td><strong>Fixture-Aware Parallel Execution</strong></td>\n<td>A potential future extension where parallel test scheduling accounts for shared fixture instances to prevent conflicts.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>FixtureLifecycleManager</strong></td>\n<td>The component responsible for creating, caching, and tearing down fixture instances according to their scope and dependency relationships.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>FixtureRegistry</strong></td>\n<td>The central registry that stores fixture definitions discovered through decorator scanning, mapping fixture names to their functions.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>FixtureRequest</strong></td>\n<td>A <strong>work order</strong> data structure representing a request for a specific fixture value, containing scope information and caching keys.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>FixtureScope</strong></td>\n<td>An enumeration defining the lifetime boundaries for fixtures: <code>FUNCTION</code>, <code>CLASS</code>, <code>MODULE</code>, and <code>SESSION</code>.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Fixture System</strong></td>\n<td>The subsystem responsible for managing test dependencies through setup/teardown hooks and dependency injection.</td>\n<td>Section 3: High-Level Architecture</td>\n</tr>\n<tr>\n<td><strong>Forensic Analysis</strong></td>\n<td>A debugging approach that treats symptoms as clues to trace back through execution flow and identify root causes.</td>\n<td>Section 12: Debugging Guide</td>\n</tr>\n<tr>\n<td><strong>FormattedError</strong></td>\n<td>A structured representation of an exception captured during test execution, containing type, message, traceback, and context.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Given</strong></td>\n<td>A potential future extension decorator for property-based testing that generates parameter values from strategies.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>God Object</strong></td>\n<td>An anti-pattern where a single class or component handles too many responsibilities, making the system difficult to maintain.</td>\n<td>Section 3: High-Level Architecture</td>\n</tr>\n<tr>\n<td><strong>Golden Master Testing</strong></td>\n<td>A testing approach that compares system output against known-good reference files, useful for verifying formatted output.</td>\n<td>Section 11: Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>Hierarchical Error Reporting</strong></td>\n<td>An error presentation strategy that shows the root cause first followed by supporting details and context.</td>\n<td>Section 10: Error Handling and Edge Cases</td>\n</tr>\n<tr>\n<td><strong>Integration Tests</strong></td>\n<td>Tests that verify multiple components working together, as opposed to unit tests that test components in isolation.</td>\n<td>Section 11: Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>Interactive Debugger Integration</strong></td>\n<td>A potential future extension that automatically drops into a debugger when tests fail, allowing immediate inspection.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Isolation</strong></td>\n<td>The property ensuring tests do not interfere with each other&#39;s state, achieved through independent execution environments.</td>\n<td>Section 5: Component Design: Discovery &amp; Execution</td>\n</tr>\n<tr>\n<td><strong>JUnit XML</strong></td>\n<td>A standardized XML format for test results used by CI/CD systems like Jenkins and GitHub Actions for reporting and analytics.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>JUnitFormatter</strong></td>\n<td>The component responsible for converting test results into JUnit XML format for CI system consumption.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Matchers API</strong></td>\n<td>An extensible API allowing users to define custom assertion predicates with tailored failure messages through the <strong>Rulebook Builder</strong> pattern.</td>\n<td>Section 6: Component Design: Assertions &amp; Matchers</td>\n</tr>\n<tr>\n<td><strong>Medical Chart</strong></td>\n<td>The mental model for <code>TestResult</code> as a detailed record of test execution, including status, duration, and diagnostic information.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Minimal Reproduction Case</strong></td>\n<td>The smallest possible test case that demonstrates a bug, essential for effective debugging and issue reporting.</td>\n<td>Section 12: Debugging Guide</td>\n</tr>\n<tr>\n<td><strong>ModuleStats</strong></td>\n<td>Statistics collected for a specific module, including counts of passed, failed, errored, and skipped tests with execution time.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Mutually Exclusive</strong></td>\n<td>A property of command-line flags that cannot be used together (e.g., <code>--verbose</code> and <code>--quiet</code>).</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Normalization</strong></td>\n<td>The process of replacing variable parts of test output (like timestamps or IDs) with placeholders for consistent comparison.</td>\n<td>Section 11: Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>OutputWriter</strong></td>\n<td>The component responsible for writing formatted output to stdout or files, handling encoding and stream management.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Parameterized Tests</strong></td>\n<td>A potential future extension where a single test definition is executed multiple times with different input values.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Pattern Resolution</strong></td>\n<td>The process of converting file/directory/glob patterns (like <code>tests/*.py</code>) to concrete file system paths.</td>\n<td>Section 5: Component Design: Discovery &amp; Execution</td>\n</tr>\n<tr>\n<td><strong>Performance Review</strong></td>\n<td>The mental model for the Reporter as a system that analyzes test outcomes and produces comprehensive reports.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Pipeline Architecture</strong></td>\n<td>The architectural pattern where components are arranged in a linear sequence, with output of one becoming input to the next.</td>\n<td>Section 3: High-Level Architecture</td>\n</tr>\n<tr>\n<td><strong>Playlist of Tests</strong></td>\n<td>The mental model for <code>TestSuite</code> as an organized collection of tests ready for execution.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Plugin Registry</strong></td>\n<td>A potential future extension component that manages plugin registration and event handling for extensibility.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Plugin System</strong></td>\n<td>A potential future extension providing a modular mechanism for third-party enhancements and integrations.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Post-Mortem Debug</strong></td>\n<td>A potential future extension function that enters a debugger after an exception occurs during test execution.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Progress Indicator</strong></td>\n<td>Visual feedback during test execution (like dots, letters, or progress bars) that shows test completion status.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Property-Based Testing</strong></td>\n<td>A testing methodology that generates random inputs to verify system invariants, as opposed to example-based testing.</td>\n<td>Section 11: Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>Recipe Card</strong></td>\n<td>The mental model for <code>TestCase</code> as a complete set of instructions for executing a single test.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Resilient Conductor</strong></td>\n<td>The mental model for the framework as an orchestra conductor that handles musician errors gracefully without crashing the entire performance.</td>\n<td>Section 10: Error Handling and Edge Cases</td>\n</tr>\n<tr>\n<td><strong>Resource Pool Manager</strong></td>\n<td>The mental model for the fixture system as a manager of scoped resources that are allocated and released on demand.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>RetryingRunner</strong></td>\n<td>A potential future extension component that automatically retries failing tests to handle flaky test scenarios.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Rulebook Builder</strong></td>\n<td>The mental model for the Matchers API as a system for creating custom verification rules for domain-specific checks.</td>\n<td>Section 6: Component Design: Assertions &amp; Matchers</td>\n</tr>\n<tr>\n<td><strong>RunContext</strong></td>\n<td>A data structure capturing the execution context of a test, including timing, loaded fixtures, and any errors.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Scope Boundary</strong></td>\n<td>The point in test execution when all tests of a certain scope complete, triggering teardown of fixtures at that scope.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>Scope Leak</strong></td>\n<td>A problematic situation where a test holds a reference to a fixture value after its teardown, potentially causing issues.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>Self-Validation</strong></td>\n<td>The practice of using the test framework to test itself, creating a virtuous cycle of quality assurance.</td>\n<td>Section 11: Testing Strategy</td>\n</tr>\n<tr>\n<td><strong>SnapshotStore</strong></td>\n<td>A potential future extension component that manages storage and retrieval of snapshot references for comparison.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Snapshot Testing</strong></td>\n<td>A potential future extension approach where actual output is automatically compared against stored reference snapshots.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Stage Crew</strong></td>\n<td>The mental model for fixtures as behind-the-scenes support that sets up the stage before tests and cleans up afterward.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>StatisticsCollector</strong></td>\n<td>The component responsible for accumulating test results and computing aggregate statistics like pass rates and timing.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Strategy Pattern</strong></td>\n<td>A design pattern defining a family of algorithms, encapsulating each one, and making them interchangeable.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n<tr>\n<td><strong>Test Conductor</strong></td>\n<td>The overarching mental model for the entire framework as an orchestra conductor coordinating tests, fixtures, and reporting.</td>\n<td>Section 1: Context and Problem Statement</td>\n</tr>\n<tr>\n<td><strong>Test Doubles</strong></td>\n<td>A potential future extension category including mocks, stubs, and fakes that replace real dependencies during testing.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Test Execution</strong></td>\n<td>The process of running test functions, evaluating assertions, and recording outcomes within isolated environments.</td>\n<td>Section 5: Component Design: Discovery &amp; Execution</td>\n</tr>\n<tr>\n<td><strong>Test Filter</strong></td>\n<td>A data structure representing filtering criteria for tests, including name patterns and marker requirements.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Test Isolation</strong></td>\n<td>The architectural guarantee that tests run in independent environments without shared state, preventing interference.</td>\n<td>Section 5: Component Design: Discovery &amp; Execution</td>\n</tr>\n<tr>\n<td><strong>Test Retries</strong></td>\n<td>A potential future extension feature that automatically re-executes failing tests a specified number of times.</td>\n<td>Section 13: Future Extensions</td>\n</tr>\n<tr>\n<td><strong>Test Run Statistics</strong></td>\n<td>Comprehensive aggregate data about a complete test run, including totals, timing, and per-module breakdowns.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>TestDiscovery</strong></td>\n<td>The process of automatically scanning modules and identifying test functions based on naming conventions.</td>\n<td>Section 5: Component Design: Discovery &amp; Execution</td>\n</tr>\n<tr>\n<td><strong>TestCase</strong></td>\n<td>The <strong>recipe card</strong> data structure representing a single test to execute, containing function reference, metadata, and dependencies.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>TestResult</strong></td>\n<td>The <strong>medical chart</strong> data structure capturing the outcome of executing a test, including status, message, and timing.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>TestStatus</strong></td>\n<td>An enumeration representing the possible states of a test: <code>PENDING</code>, <code>RUNNING</code>, <code>PASSED</code>, <code>FAILED</code>, <code>ERRORED</code>, <code>SKIPPED</code>.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>TestSuite</strong></td>\n<td>The <strong>playlist</strong> data structure representing a collection of <code>TestCase</code> objects organized for execution.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>The Restaurant Order</strong></td>\n<td>The mental model for test parameters as order items that are fulfilled by the fixture kitchen before test execution.</td>\n<td>Section 7: Component Design: Fixtures &amp; Setup/Teardown</td>\n</tr>\n<tr>\n<td><strong>Traffic Light System</strong></td>\n<td>The mental model for <code>TestStatus</code> values as indicators of test state (green for PASSED, red for FAILED, etc.).</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>Troubleshooting Playbook</strong></td>\n<td>A structured guide for diagnosing and fixing common issues, organized by symptoms and solutions.</td>\n<td>Section 12: Debugging Guide</td>\n</tr>\n<tr>\n<td><strong>TypeComparatorRegistry</strong></td>\n<td>A registry mapping Python types to specialized comparison functions for handling complex equality checks.</td>\n<td>Section 6: Component Design: Assertions &amp; Matchers</td>\n</tr>\n<tr>\n<td><strong>Work Order</strong></td>\n<td>The mental model for <code>FixtureRequest</code> as an instruction sheet for the fixture system to create a specific fixture value.</td>\n<td>Section 4: Data Model</td>\n</tr>\n<tr>\n<td><strong>xUnit</strong></td>\n<td>The family of testing frameworks following a class-based pattern with setup/teardown methods (e.g., unittest, JUnit).</td>\n<td>Section 1: Context and Problem Statement</td>\n</tr>\n<tr>\n<td><strong>XML Escaping</strong></td>\n<td>The process of replacing special XML characters (<code>&lt;</code>, <code>&gt;</code>, <code>&amp;</code>, <code>&quot;</code>, <code>&#39;</code>) with their corresponding entity references.</td>\n<td>Section 8: Component Design: Reporting &amp; CLI</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>For a glossary section, the implementation guidance focuses on how to maintain and evolve this terminology as the framework develops.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Terminology Management</strong></td>\n<td>Maintain a single glossary markdown file</td>\n<td>Use a documentation generator with term extraction</td>\n</tr>\n<tr>\n<td><strong>Cross-Reference Links</strong></td>\n<td>Manual section number references</td>\n<td>Automated link generation using documentation tooling</td>\n</tr>\n<tr>\n<td><strong>Term Validation</strong></td>\n<td>Manual review during code reviews</td>\n<td>Linter that checks for undefined terms in documentation</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-apollo/\n├── docs/\n│   ├── design-doc.md          # Main design document\n│   └── glossary.md            # Standalone glossary file (this section)\n├── src/\n│   └── apollo/\n│       ├── __init__.py        # Public API exports\n│       ├── cli.py             # CLI interface\n│       ├── discover.py        # Test discovery\n│       ├── runner.py          # Test execution\n│       ├── assertions.py      # Assertion engine\n│       ├── fixtures.py        # Fixture system\n│       ├── reporting.py       # Result formatting\n│       └── types.py           # Core data types (TestCase, TestResult, etc.)\n└── tests/\n    └── test_apollo.py         # Self-tests for the framework</code></pre></div>\n\n<h4 id=\"c-glossary-maintenance-script\">C. Glossary Maintenance Script</h4>\n<p>While the glossary itself is documentation, you can create a simple script to help maintain consistency:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">glossary_validator.py - Simple script to validate terminology usage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Set</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Load glossary terms from a text file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_glossary_terms</span><span style=\"color:#E1E4E8\">(glossary_path: Path) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load terms and their definitions from a markdown glossary file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    terms </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    term_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#85E89D;font-weight:bold\">\\*\\*</span><span style=\"color:#79B8FF\">(.</span><span style=\"color:#F97583\">*?</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#85E89D;font-weight:bold\">\\*\\*</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Matches **Term** in markdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(glossary_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">encoding</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f.readlines()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, line </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(lines):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            match </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> term_pattern.search(line)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> match </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> \"**\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> \"|\"</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> line:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Simple extraction from markdown table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line.split(</span><span style=\"color:#9ECBFF\">'|'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(parts) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    term </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].strip(</span><span style=\"color:#9ECBFF\">'* '</span><span style=\"color:#E1E4E8\">).strip()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> term </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(term) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        terms[term] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Store line number</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> terms</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> find_undefined_terms</span><span style=\"color:#E1E4E8\">(source_dir: Path, glossary_terms: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Scan source code for potential undefined terms.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    undefined </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">[A-Z][a-zA-Z]</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Matches CapitalizedTerms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> py_file </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> source_dir.rglob(</span><span style=\"color:#9ECBFF\">\"*.py\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(py_file, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">encoding</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f.read()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Find capitalized multi-word terms that might need definition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> match </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> source_pattern.finditer(content):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                term </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> match.group(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> (term </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> glossary_terms </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    len</span><span style=\"color:#E1E4E8\">(term) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    term[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].isupper() </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    not</span><span style=\"color:#E1E4E8\"> term.isupper()):  </span><span style=\"color:#6A737D\"># Exclude ALL_CAPS constants</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> term </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> undefined:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        undefined[term] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    undefined[term].append(</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(py_file))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> undefined</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Simple validation example</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    glossary_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(</span><span style=\"color:#9ECBFF\">\"docs/glossary.md\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(</span><span style=\"color:#9ECBFF\">\"src/apollo\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> glossary_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        terms </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> load_glossary_terms(glossary_path)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Loaded </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(terms)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> terms from glossary\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        undefined </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> find_undefined_terms(source_dir, </span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">(terms.keys()))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> undefined:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Potential undefined terms found in source:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> term, files </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(undefined.items()):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">term</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">', '</span><span style=\"color:#E1E4E8\">.join(files[:</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"All capitalized terms appear to be defined in glossary.\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"d-documentation-standards\">D. Documentation Standards</h4>\n<p>When adding new terms to the glossary:</p>\n<ol>\n<li><strong>Add entry immediately</strong> when introducing a new concept in the design document</li>\n<li><strong>Use consistent formatting</strong>: bold term, clear definition, section reference</li>\n<li><strong>Update alphabetically</strong>: Maintain alphabetical order for easy lookup</li>\n<li><strong>Cross-reference</strong>: Link between related terms in definitions</li>\n<li><strong>Review periodically</strong>: Schedule glossary reviews as the framework evolves</li>\n</ol>\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<ul>\n<li><strong>Python docstrings</strong>: Use glossary terms consistently in module and function docstrings</li>\n<li><strong>Type hints</strong>: Reference glossary types like <code>TestCase</code> and <code>TestResult</code> in type annotations</li>\n<li><strong>Error messages</strong>: Use terminology from the glossary in user-facing error messages for consistency</li>\n<li><strong>API documentation</strong>: When generating API docs, link back to glossary definitions for key concepts</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After completing each milestone, verify that all new terminology introduced in that milestone has been added to the glossary:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run the glossary validator to check for undefined terms</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> glossary_validator.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output example:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Loaded 45 terms from glossary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># All capitalized terms appear to be defined in glossary.</span></span></code></pre></div>\n\n<h4 id=\"g-debugging-terminology-issues\">G. Debugging Terminology Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Confusion about a term&#39;s meaning</td>\n<td>Term not defined or ambiguous definition</td>\n<td>Search glossary for term; check if multiple definitions exist</td>\n<td>Add or clarify definition in glossary; ensure single source of truth</td>\n</tr>\n<tr>\n<td>Inconsistent usage in code</td>\n<td>Different names for same concept</td>\n<td>Search codebase for synonyms; check variable/function names</td>\n<td>Standardize on glossary term; rename variables/functions</td>\n</tr>\n<tr>\n<td>Missing documentation for new feature</td>\n<td>Forgot to add terms to glossary</td>\n<td>Compare new feature documentation against glossary</td>\n<td>Add all new terms introduced by the feature</td>\n</tr>\n</tbody></table>\n<hr>\n","toc":[{"level":1,"text":"Project Apollo: Test Framework Design Document","id":"project-apollo-test-framework-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"1. Context and Problem Statement","id":"1-context-and-problem-statement"},{"level":3,"text":"Mental Model: The Test Conductor","id":"mental-model-the-test-conductor"},{"level":3,"text":"The Core Problem: Automated Verification","id":"the-core-problem-automated-verification"},{"level":3,"text":"Existing Approaches &amp; Comparison","id":"existing-approaches-amp-comparison"},{"level":4,"text":"Architecture Decision: Foundation Approach","id":"architecture-decision-foundation-approach"},{"level":4,"text":"Key Architectural Differentiators","id":"key-architectural-differentiators"},{"level":3,"text":"Common Pitfalls in Understanding Test Frameworks","id":"common-pitfalls-in-understanding-test-frameworks"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code: Core Data Models","id":"c-infrastructure-starter-code-core-data-models"},{"level":4,"text":"D. Core Logic Skeleton: The Simplest Possible Runner","id":"d-core-logic-skeleton-the-simplest-possible-runner"},{"level":4,"text":"E. Language-Specific Hints: Python","id":"e-language-specific-hints-python"},{"level":4,"text":"F. Milestone 1 Checkpoint","id":"f-milestone-1-checkpoint"},{"level":4,"text":"G. Debugging Tips: Early Discovery Issues","id":"g-debugging-tips-early-discovery-issues"},{"level":2,"text":"2. Goals and Non-Goals","id":"2-goals-and-non-goals"},{"level":3,"text":"Goals","id":"goals"},{"level":4,"text":"Goal 1: Automatic Test Discovery &amp; Execution","id":"goal-1-automatic-test-discovery-amp-execution"},{"level":4,"text":"Goal 2: Expressive Assertion System","id":"goal-2-expressive-assertion-system"},{"level":4,"text":"Goal 3: Fixture-Based Test Environment","id":"goal-3-fixture-based-test-environment"},{"level":4,"text":"Goal 4: Comprehensive Reporting &amp; CLI","id":"goal-4-comprehensive-reporting-amp-cli"},{"level":3,"text":"Non-Goals","id":"non-goals"},{"level":4,"text":"Important Clarifications on Scope Boundaries","id":"important-clarifications-on-scope-boundaries"},{"level":4,"text":"What Could Be Added Later (Future Extensions)","id":"what-could-be-added-later-future-extensions"},{"level":3,"text":"Success Criteria","id":"success-criteria"},{"level":3,"text":"Trade-offs and Constraints","id":"trade-offs-and-constraints"},{"level":3,"text":"Common Pitfalls in Scope Definition","id":"common-pitfalls-in-scope-definition"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations Table","id":"technology-recommendations-table"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code: Core Data Types","id":"infrastructure-starter-code-core-data-types"},{"level":4,"text":"Core Logic Skeleton: Test Discovery","id":"core-logic-skeleton-test-discovery"},{"level":4,"text":"Language-Specific Hints for Python","id":"language-specific-hints-for-python"},{"level":4,"text":"Milestone Checkpoint: Goal Verification","id":"milestone-checkpoint-goal-verification"},{"level":4,"text":"Debugging Tips for Scope-Related Issues","id":"debugging-tips-for-scope-related-issues"},{"level":2,"text":"3. High-Level Architecture","id":"3-high-level-architecture"},{"level":3,"text":"Component Overview &amp; Responsibilities","id":"component-overview-amp-responsibilities"},{"level":3,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":2,"text":"4. Data Model","id":"4-data-model"},{"level":3,"text":"Core Types","id":"core-types"},{"level":4,"text":"TestStatus: The Lifecycle State of a Test","id":"teststatus-the-lifecycle-state-of-a-test"},{"level":4,"text":"TestCase: The Blueprint of a Test","id":"testcase-the-blueprint-of-a-test"},{"level":4,"text":"TestResult: The Outcome of a Test Execution","id":"testresult-the-outcome-of-a-test-execution"},{"level":4,"text":"Fixture: Reusable Test Resources","id":"fixture-reusable-test-resources"},{"level":3,"text":"Supporting Structures","id":"supporting-structures"},{"level":4,"text":"TestSuite: Organizing Tests for Execution","id":"testsuite-organizing-tests-for-execution"},{"level":4,"text":"Configuration: Framework Behavior as Data","id":"configuration-framework-behavior-as-data"},{"level":4,"text":"Supporting Types for Fixture System","id":"supporting-types-for-fixture-system"},{"level":4,"text":"Relationship Diagram","id":"relationship-diagram"},{"level":4,"text":"Data Flow Through the Pipeline","id":"data-flow-through-the-pipeline"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":4,"text":"⚠️ Pitfall: Mutable default arguments in data classes","id":"-pitfall-mutable-default-arguments-in-data-classes"},{"level":4,"text":"⚠️ Pitfall: Forgetting to handle generator completion in fixtures","id":"-pitfall-forgetting-to-handle-generator-completion-in-fixtures"},{"level":4,"text":"⚠️ Pitfall: Using floats for duration comparison","id":"-pitfall-using-floats-for-duration-comparison"},{"level":4,"text":"⚠️ Pitfall: Storing absolute paths in TestCase","id":"-pitfall-storing-absolute-paths-in-testcase"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":2,"text":"5. Component Design: Discovery &amp; Execution (Milestone 1)","id":"5-component-design-discovery-amp-execution-milestone-1"},{"level":3,"text":"The Discoverer","id":"the-discoverer"},{"level":4,"text":"Architecture Decision: Convention Over Configuration for Test Discovery","id":"architecture-decision-convention-over-configuration-for-test-discovery"},{"level":4,"text":"Discoverer Components and Data Flow","id":"discoverer-components-and-data-flow"},{"level":4,"text":"The Discovery Algorithm","id":"the-discovery-algorithm"},{"level":4,"text":"TestCase Data Structure Details","id":"testcase-data-structure-details"},{"level":4,"text":"Concrete Walk-Through Example","id":"concrete-walk-through-example"},{"level":4,"text":"Common Pitfalls in Test Discovery","id":"common-pitfalls-in-test-discovery"},{"level":3,"text":"The Runner &amp; Isolation","id":"the-runner-amp-isolation"},{"level":4,"text":"Test Execution State Machine","id":"test-execution-state-machine"},{"level":4,"text":"Architecture Decision: Process-Based Parallel Execution","id":"architecture-decision-process-based-parallel-execution"},{"level":4,"text":"Runner Components and Interfaces","id":"runner-components-and-interfaces"},{"level":4,"text":"Test Execution Algorithm","id":"test-execution-algorithm"},{"level":4,"text":"Parallel Execution Implementation","id":"parallel-execution-implementation"},{"level":4,"text":"TestResult Data Structure Details","id":"testresult-data-structure-details"},{"level":4,"text":"Concrete Walk-Through: Parallel Execution Scenario","id":"concrete-walk-through-parallel-execution-scenario"},{"level":4,"text":"Common Pitfalls in Test Execution","id":"common-pitfalls-in-test-execution"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints (Python)","id":"e-language-specific-hints-python"},{"level":4,"text":"F. Milestone 1 Checkpoint","id":"f-milestone-1-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"6. Component Design: Assertions &amp; Matchers (Milestone 2)","id":"6-component-design-assertions-amp-matchers-milestone-2"},{"level":3,"text":"Assertion Engine","id":"assertion-engine"},{"level":4,"text":"Mental Model: The Comparison Inspector","id":"mental-model-the-comparison-inspector"},{"level":4,"text":"Core Design Principles","id":"core-design-principles"},{"level":4,"text":"Architecture Decision Record: Exception-Based Failure Signaling","id":"architecture-decision-record-exception-based-failure-signaling"},{"level":4,"text":"Data Structures","id":"data-structures"},{"level":4,"text":"Assertion Evaluation Algorithm","id":"assertion-evaluation-algorithm"},{"level":4,"text":"Core Assertion Suite","id":"core-assertion-suite"},{"level":4,"text":"Collection Assertions","id":"collection-assertions"},{"level":4,"text":"Exception Assertions","id":"exception-assertions"},{"level":4,"text":"Common Pitfalls in Assertion Engine Implementation","id":"common-pitfalls-in-assertion-engine-implementation"},{"level":3,"text":"Matchers API","id":"matchers-api"},{"level":4,"text":"Design Philosophy","id":"design-philosophy"},{"level":4,"text":"Architecture Decision Record: Object-Oriented Matchers vs Function-Based Matchers","id":"architecture-decision-record-object-oriented-matchers-vs-function-based-matchers"},{"level":4,"text":"Core Matcher Protocol","id":"core-matcher-protocol"},{"level":4,"text":"Built-in Matcher Library","id":"built-in-matcher-library"},{"level":4,"text":"Matcher Composition","id":"matcher-composition"},{"level":4,"text":"Custom Matcher Creation","id":"custom-matcher-creation"},{"level":4,"text":"Integration with Assertion Engine","id":"integration-with-assertion-engine"},{"level":4,"text":"Common Pitfalls in Matchers API Implementation","id":"common-pitfalls-in-matchers-api-implementation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints (Python)","id":"language-specific-hints-python"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"7. Component Design: Fixtures &amp; Setup/Teardown (Milestone 3)","id":"7-component-design-fixtures-amp-setupteardown-milestone-3"},{"level":3,"text":"Fixture System Architecture","id":"fixture-system-architecture"},{"level":4,"text":"Core Components","id":"core-components"},{"level":4,"text":"Data Structures","id":"data-structures"},{"level":4,"text":"Lifecycle Management by Scope","id":"lifecycle-management-by-scope"},{"level":4,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Dependency Injection Mechanism","id":"dependency-injection-mechanism"},{"level":4,"text":"Parameter Inspection","id":"parameter-inspection"},{"level":4,"text":"Injection Process","id":"injection-process"},{"level":4,"text":"Integration with Setup/Teardown Hooks","id":"integration-with-setupteardown-hooks"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"8. Component Design: Reporting &amp; CLI (Milestone 4)","id":"8-component-design-reporting-amp-cli-milestone-4"},{"level":3,"text":"Mental Model: The Control Panel and Performance Review","id":"mental-model-the-control-panel-and-performance-review"},{"level":3,"text":"CLI Parser","id":"cli-parser"},{"level":4,"text":"Architecture Decision: Command-Line Interface Design","id":"architecture-decision-command-line-interface-design"},{"level":4,"text":"Core Responsibilities","id":"core-responsibilities"},{"level":4,"text":"Configuration Data Structure","id":"configuration-data-structure"},{"level":4,"text":"Command-Line Interface Specification","id":"command-line-interface-specification"},{"level":4,"text":"Algorithm: Parsing and Configuration Building","id":"algorithm-parsing-and-configuration-building"},{"level":4,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Reporter","id":"reporter"},{"level":4,"text":"Architecture Decision: Dual-Format Reporting Strategy","id":"architecture-decision-dual-format-reporting-strategy"},{"level":4,"text":"Core Responsibilities","id":"core-responsibilities"},{"level":4,"text":"Reporter Internal Architecture","id":"reporter-internal-architecture"},{"level":5,"text":"StatisticsCollector","id":"statisticscollector"},{"level":5,"text":"ConsoleFormatter","id":"consoleformatter"},{"level":5,"text":"JUnitFormatter","id":"junitformatter"},{"level":4,"text":"Algorithm: Reporting Pipeline","id":"algorithm-reporting-pipeline"},{"level":4,"text":"State Machine: Report Generation States","id":"state-machine-report-generation-states"},{"level":4,"text":"Concrete Walk-Through Example","id":"concrete-walk-through-example"},{"level":4,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"9. Interactions and Data Flow","id":"9-interactions-and-data-flow"},{"level":3,"text":"Main Execution Sequence","id":"main-execution-sequence"},{"level":4,"text":"Phase 1: CLI Invocation and Configuration","id":"phase-1-cli-invocation-and-configuration"},{"level":4,"text":"Phase 2: Test Discovery and Suite Assembly","id":"phase-2-test-discovery-and-suite-assembly"},{"level":4,"text":"Phase 3: Fixture Preparation and Test Execution","id":"phase-3-fixture-preparation-and-test-execution"},{"level":4,"text":"Phase 4: Result Collection and Reporting","id":"phase-4-result-collection-and-reporting"},{"level":3,"text":"Data Flow Between Components","id":"data-flow-between-components"},{"level":4,"text":"Data Structure Lifecycle Transformations","id":"data-structure-lifecycle-transformations"},{"level":4,"text":"Parallel Execution Data Flow","id":"parallel-execution-data-flow"},{"level":4,"text":"Error Propagation Through the Pipeline","id":"error-propagation-through-the-pipeline"},{"level":4,"text":"Data Flow Example: Complete Trace","id":"data-flow-example-complete-trace"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"10. Error Handling and Edge Cases","id":"10-error-handling-and-edge-cases"},{"level":3,"text":"Common Failure Modes","id":"common-failure-modes"},{"level":4,"text":"Test Failures vs. Test Errors","id":"test-failures-vs-test-errors"},{"level":4,"text":"Discovery Errors","id":"discovery-errors"},{"level":4,"text":"Fixture Lifecycle Errors","id":"fixture-lifecycle-errors"},{"level":4,"text":"System-Level Errors","id":"system-level-errors"},{"level":3,"text":"Recovery &amp; Reporting Strategy","id":"recovery-amp-reporting-strategy"},{"level":4,"text":"Error State Transitions","id":"error-state-transitions"},{"level":4,"text":"Error Message Design Principles","id":"error-message-design-principles"},{"level":4,"text":"Fixture Error Handling Deep Dive","id":"fixture-error-handling-deep-dive"},{"level":4,"text":"Parallel Execution Error Handling","id":"parallel-execution-error-handling"},{"level":4,"text":"Common Pitfalls in Error Handling","id":"common-pitfalls-in-error-handling"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations Table","id":"technology-recommendations-table"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"11. Testing Strategy","id":"11-testing-strategy"},{"level":3,"text":"Testing the Framework","id":"testing-the-framework"},{"level":4,"text":"Self-Validation: The Framework as Its First User","id":"self-validation-the-framework-as-its-first-user"},{"level":4,"text":"Property-Based Testing for Assertions","id":"property-based-testing-for-assertions"},{"level":4,"text":"Golden Master Tests for Output","id":"golden-master-tests-for-output"},{"level":4,"text":"Common Pitfalls in Testing the Framework","id":"common-pitfalls-in-testing-the-framework"},{"level":3,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Milestone 1: Test Discovery &amp; Execution","id":"milestone-1-test-discovery-amp-execution"},{"level":4,"text":"Milestone 2: Assertions &amp; Matchers","id":"milestone-2-assertions-amp-matchers"},{"level":4,"text":"Milestone 3: Fixtures &amp; Setup/Teardown","id":"milestone-3-fixtures-amp-setupteardown"},{"level":4,"text":"Milestone 4: Reporting &amp; CLI","id":"milestone-4-reporting-amp-cli"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File/Module Structure for Tests","id":"recommended-filemodule-structure-for-tests"},{"level":4,"text":"Infrastructure Starter Code for Golden Master Testing","id":"infrastructure-starter-code-for-golden-master-testing"},{"level":4,"text":"Core Logic Skeleton for Self-Testing","id":"core-logic-skeleton-for-self-testing"},{"level":4,"text":"Property-Based Testing Skeleton","id":"property-based-testing-skeleton"},{"level":4,"text":"Milestone Checkpoint Implementation","id":"milestone-checkpoint-implementation"},{"level":4,"text":"Debugging Tips for Framework Tests","id":"debugging-tips-for-framework-tests"},{"level":4,"text":"Language-Specific Hints for Python","id":"language-specific-hints-for-python"},{"level":2,"text":"12. Debugging Guide","id":"12-debugging-guide"},{"level":3,"text":"Common Bugs &amp; Fixes","id":"common-bugs-amp-fixes"},{"level":3,"text":"Debugging Techniques","id":"debugging-techniques"},{"level":4,"text":"1. Strategic Logging and Introspection","id":"1-strategic-logging-and-introspection"},{"level":4,"text":"2. Interactive Debugging with PDB","id":"2-interactive-debugging-with-pdb"},{"level":4,"text":"3. State Inspection via Test Hooks","id":"3-state-inspection-via-test-hooks"},{"level":4,"text":"4. Minimal Reproduction Creation","id":"4-minimal-reproduction-creation"},{"level":4,"text":"5. Component Isolation Testing","id":"5-component-isolation-testing"},{"level":4,"text":"6. Visualization of Data Flow","id":"6-visualization-of-data-flow"},{"level":4,"text":"7. Binary Search Debugging","id":"7-binary-search-debugging"},{"level":4,"text":"8. Version Comparison","id":"8-version-comparison"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Debugging Tips","id":"f-debugging-tips"},{"level":4,"text":"G. Milestone Checkpoint for Debugging","id":"g-milestone-checkpoint-for-debugging"},{"level":2,"text":"13. Future Extensions","id":"13-future-extensions"},{"level":3,"text":"Ideas for Extension","id":"ideas-for-extension"},{"level":4,"text":"1. Plugin System: The Modular Toolbox","id":"1-plugin-system-the-modular-toolbox"},{"level":4,"text":"2. Custom Markers and Filtering: The Labeling System","id":"2-custom-markers-and-filtering-the-labeling-system"},{"level":4,"text":"3. Parameterized Tests: The Test Factory","id":"3-parameterized-tests-the-test-factory"},{"level":4,"text":"4. Test Doubles and Mocking Integration: The Understudy System","id":"4-test-doubles-and-mocking-integration-the-understudy-system"},{"level":4,"text":"5. Parallel Test Execution with Fixture Scope Awareness","id":"5-parallel-test-execution-with-fixture-scope-awareness"},{"level":4,"text":"6. Snapshot Testing: The Golden Master","id":"6-snapshot-testing-the-golden-master"},{"level":4,"text":"7. Property-Based Testing: The Fuzzing Lab","id":"7-property-based-testing-the-fuzzing-lab"},{"level":4,"text":"8. Code Coverage Integration: The Coverage Map","id":"8-code-coverage-integration-the-coverage-map"},{"level":4,"text":"9. Test Retries and Flaky Test Detection: The Reliability Engineer","id":"9-test-retries-and-flaky-test-detection-the-reliability-engineer"},{"level":4,"text":"10. Interactive Debugger and PDB Integration: The Surgical Suite","id":"10-interactive-debugger-and-pdb-integration-the-surgical-suite"},{"level":3,"text":"Summary of Architectural Impact","id":"summary-of-architectural-impact"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"14. Glossary","id":"14-glossary"},{"level":3,"text":"Term Definitions","id":"term-definitions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Glossary Maintenance Script","id":"c-glossary-maintenance-script"},{"level":4,"text":"D. Documentation Standards","id":"d-documentation-standards"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Terminology Issues","id":"g-debugging-terminology-issues"}],"title":"Project Apollo: Test Framework Design Document","markdown":"# Project Apollo: Test Framework Design Document\n\n\n## Overview\n\nProject Apollo is a Python-based test framework built for learning, demonstrating how modern frameworks like pytest operate under the hood. The key architectural challenge it solves is providing a flexible, extensible system for test discovery, execution, and reporting while maintaining test isolation and supporting complex features like fixtures and assertions. This document guides the implementation through four progressive milestones, from basic discovery to a full-featured CLI.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## 1. Context and Problem Statement\n\n> **Milestone(s):** This section provides the foundational understanding and motivation for the entire project, spanning all four milestones.\n\n### Mental Model: The Test Conductor\n\nImagine you are the conductor of a large, distributed orchestra. Your musicians (individual test functions) are scattered across multiple buildings (source files and modules). Your sheet music (test fixtures and setup instructions) is stored in various libraries. Your performance hall (the test execution environment) needs to be prepared before each piece and cleaned afterward. Your ultimate goal is to produce a cohesive performance report (test results) that tells the audience (developers) exactly which musicians played correctly, which made mistakes, and why.\n\nA test framework is that conductor. It must:\n\n1.  **Discover the musicians:** Find all test functions across the codebase by scanning files and recognizing naming patterns.\n2.  **Organize the performance:** Determine the order of execution, manage dependencies between tests, and ensure musicians don't interfere with each other (test isolation).\n3.  **Provide sheet music:** Supply each test with the resources (fixtures, data, mocks) it needs to perform its part.\n4.  **Evaluate the performance:** Listen to each musician (execute the test) and judge whether they played the correct notes (assertions passed) or made an error.\n5.  **Deliver the critique:** Compile a clear, actionable report detailing successes, failures, and the specific nature of any mistakes.\n\nThis mental model of the framework as an **orchestrating conductor** separates the *what* (the test logic written by developers) from the *how* (the mechanics of finding, running, and evaluating tests). This separation of concerns is the core architectural principle of any effective test framework.\n\n### The Core Problem: Automated Verification\n\nWriting software without automated tests is like constructing a skyscraper without blueprints or inspections. You might get it to stand, but its reliability, safety, and maintainability are unknowable. Before test frameworks, verification was a manual, ad-hoc process:\n\n```python\n# Hypothetical manual test script (the \"dark ages\")\ndef test_user_creation():\n    db = connect_to_database()  # Manual setup\n    user = create_user(\"Alice\")\n    if user.name != \"Alice\":    # Manual assertion\n        print(\"FAIL: Name mismatch\")\n        return\n    if not db.user_exists(user.id):\n        print(\"FAIL: User not saved\")\n        return\n    db.cleanup()                # Manual teardown\n    print(\"PASS\")\n```\n\nThis approach is **brittle, unrepeatable, and unscalable**. The problems compound rapidly:\n- **Forgetting tests:** Manual tests are easily overlooked or skipped.\n- **No isolation:** Tests can interfere with each other (e.g., leftover database state).\n- **Poor diagnostics:** Error messages are inconsistent and unhelpful.\n- **No aggregation:** There's no unified view of what passed or failed across the codebase.\n- **High maintenance:** Any change to the system under test requires manually updating countless scattered validation scripts.\n\nThe **Core Problem** is therefore: *How do we automate the repetitive, mechanical aspects of software verification so developers can focus on writing test logic?* An effective solution must address four fundamental requirements:\n\n1.  **Discovery:** Automatically find all test cases in a codebase without requiring manual registration.\n2.  **Execution:** Run tests in a controlled, isolated environment with predictable setup and cleanup.\n3.  **Assertion:** Provide a rich, expressive language for verifying expectations with helpful failure messages.\n4.  **Reporting:** Aggregate results into a clear, actionable summary for both humans and machines.\n\n> **Key Insight:** The value of a test framework is not just in running tests—it's in *reducing the cognitive load* of verification. A good framework makes writing and maintaining tests *easier* than not having them.\n\n### Existing Approaches & Comparison\n\nThe software ecosystem offers multiple solutions to the test automation problem, each with different philosophies and trade-offs. Understanding this design space is crucial for making informed architectural decisions for Project Apollo.\n\n#### Architecture Decision: Foundation Approach\n\n> **Decision: Build a Modern, Convention-Based Framework**\n> - **Context:** We need to choose a foundational philosophy that guides all subsequent design decisions. The built-in `unittest` module exists, but modern Python projects overwhelmingly prefer `pytest`.\n> - **Options Considered:**\n>   1.  **Extend `unittest`:** Build upon Python's standard library module.\n>   2.  **Build `pytest`-like:** Create a new, convention-based framework from scratch.\n>   3.  **Minimal Assertion Library:** Create only a better assertion library without discovery or fixtures.\n> - **Decision:** Build a `pytest`-like, convention-based framework from scratch.\n> - **Rationale:**\n>   1.  **Educational Value:** Building from scratch maximizes learning about all components (discovery, fixtures, reporting), not just assertions.\n>   2.  **Modern Practices:** Convention-over-configuration and fixture dependency injection are industry standards that reduce boilerplate.\n>   3.  **Separation of Concerns:** A clean-slate design allows us to implement clean boundaries between components (Discoverer, Runner, Fixture Registry) without legacy constraints.\n> - **Consequences:**\n>   - **Positive:** We gain deep understanding of modern test framework architecture. The final product will feel familiar to `pytest` users.\n>   - **Negative:** More implementation work than extending `unittest`. We must reimplement basics that `unittest` already provides.\n\n| Approach | Pros | Cons | Why Not Chosen for Apollo |\n| :--- | :--- | :--- | :--- |\n| **Built-in (`unittest`)** | <ul><li>No dependencies</li><li>Battle-tested</li><li>Standardized xUnit pattern</li></ul> | <ul><li>Verbose (requires test classes)</li><li>Limited fixture system</li><li>No auto-discovery by default</li><li>Less expressive assertions</li></ul> | While robust, `unittest` embodies an older architectural pattern. Building on it would limit our exploration of modern design patterns like convention-based discovery and functional fixture injection. |\n| **Modern (`pytest`)** | <ul><li>Convention-over-configuration</li><li>Powerful fixture system</li><li>Rich plugin ecosystem</li><li>Excellent error reporting</li></ul> | <ul><li>Complex internals (hard to learn from)</li><li>Heavy dependency graph</li><li>\"Magic\" that can be opaque</li></ul> | **Our inspiration, not our base.** We are *recreating* this experience from first principles to understand the \"magic.\" |\n| **Minimal (e.g., `assert`)** | <ul><li>Extremely simple</li><li>No framework overhead</li><li>Direct control</li></ul> | <ul><li>No discovery or organization</li><li>Manual setup/teardown</li><li>No aggregated reporting</li><li>Poor error messages</li></ul> | Does not solve the core problem of automation. Leaves too much mechanical work to the developer. |\n| **Project Apollo (Our Build)** | <ul><li>**Educational transparency**</li><li>Modern convention-based design</li><li>Controlled complexity</li><li>All core components built from scratch</li></ul> | <ul><li>Not production-hardened</li><li>Limited feature set</li><li>No plugin system initially</li></ul> | **Chosen.** Provides maximum learning value by implementing all core components with clean separation, mirroring modern framework architecture. |\n\n#### Key Architectural Differentiators\n\nThe table above highlights fundamental architectural choices that differentiate frameworks:\n\n1.  **Discovery Mechanism:**\n    - **Registration-based (`unittest`):** Tests must inherit from a base class or be explicitly registered. This creates boilerplate but is explicit.\n    - **Convention-based (`pytest`, Apollo):** The framework scans modules for functions/classes matching patterns (e.g., `test_*`). This reduces boilerplate but requires clear naming rules.\n\n2.  **Test Organization:**\n    - **Class-based xUnit (`unittest`):** Tests are methods within classes, allowing shared `setUp`/`tearDown` at class level.\n    - **Function-based with Fixtures (`pytest`, Apollo):** Tests are standalone functions; shared resources are provided via parameter injection (fixtures). This is more compositional and flexible.\n\n3.  **Assertion System:**\n    - **Method-based (`unittest`):** `self.assertEqual(a, b)` provides good error messages but verbose syntax.\n    - **Built-in statement (`assert`):** `assert a == b` is concise but yields poor default error messages.\n    - **Enhanced built-in (`pytest`, Apollo):** Override or wrap the `assert` statement to provide rich diffs and introspection *while keeping the simple syntax*.\n\n4.  **Execution Model:**\n    - **Linear (`unittest` basic):** Tests run sequentially in discovery order.\n    - **Isolated & Parallel (Apollo goal):** Each test runs in its own context, enabling safe parallel execution for speed.\n\n> **Design Principle:** Convention over configuration reduces boilerplate and cognitive load. By adopting sensible defaults (e.g., `test_` prefix), we make the common case easy while allowing escape hatches for complexity.\n\n### Common Pitfalls in Understanding Test Frameworks\n\n⚠️ **Pitfall: Treating the Framework as a Test Runner Only**\n- **Description:** Thinking the framework's primary job is just to execute a list of tests you manually give it.\n- **Why It's Wrong:** This ignores the critical problems of *discovery* (finding all tests) and *isolation* (preventing test interference). A framework that doesn't handle these becomes a glorified script executor.\n- **How to Avoid:** Design components with clear responsibilities: the **Discoverer** finds tests, the **Runner** manages isolated execution, and the **Reporter** aggregates results. These are distinct concerns.\n\n⚠️ **Pitfall: Overlooking Test Isolation**\n- **Description:** Allowing tests to share mutable state (e.g., global variables, database connections) without proper cleanup.\n- **Why It's Wrong:** Tests become interdependent and non-deterministic. A passing test may depend on leftover state from a previous test, causing false passes and mysterious failures when tests run in different orders.\n- **How to Avoid:** Design the **Runner** to execute each test in a clean environment. For milestone 1, this means each test runs in its own process or with carefully managed state rollback. For milestone 3, fixtures will provide controlled, scoped sharing.\n\n⚠️ **Pitfall: Poor Error Messages in Assertions**\n- **Description:** Assertions that only say \"Assertion failed\" without showing the expected vs. actual values.\n- **Why It's Wrong:** Developers waste time manually printing values to debug test failures, defeating the purpose of automation.\n- **How to Avoid:** Design the **Assertion Engine** (milestone 2) to compute and display informative diffs. For `assert_equal(a, b)`, the error message should show both values, their types, and a visual diff for collections.\n\n⚠️ **Pitfall: Neglecting Resource Cleanup**\n- **Description:** Setting up resources (files, network connections) in tests but not guaranteeing cleanup on test failure.\n- **Why It's Wrong:** Resource leaks accumulate across test runs, eventually causing failures (e.g., \"too many open files\").\n- **How to Avoid:** Design the **Fixture System** (milestone 3) with guaranteed teardown using context managers (`try/finally`) or generator fixtures that run cleanup code after the test.\n\n### Implementation Guidance\n\nThis section establishes the *why* and *what*. The following sections will detail the *how*. However, to ground these concepts, here is a minimal starter structure and a tangible example of what we're building toward.\n\n#### A. Technology Recommendations Table\n| Component | Simple Option (for Learning) | Advanced Option (for Robustness) |\n| :--- | :--- | :--- |\n| **Module Discovery** | `importlib` + `os.walk` | `pkgutil` with namespace package support |\n| **Test Isolation** | Subprocess execution (`subprocess`) | `sys.settrace()` for in-process isolation |\n| **Assertion Engine** | Override `__assert__` (not possible) → Helper functions | `inspect` module for frame introspection |\n| **Fixture Management** | Simple dict registry + `functools.partial` | Context managers + dependency graph resolution |\n| **CLI & Reporting** | `argparse` + formatted print statements | `rich` library for colored output, `lxml` for XML |\n\n#### B. Recommended File/Module Structure\nTo keep our conceptual components separate in code, start with this project layout:\n```\nproject_apollo/\n├── apollo/                    # Main package\n│   ├── __init__.py           # Public API: `from apollo import test, fixture`\n│   ├── cli.py                # CLI Parser (Milestone 4)\n│   ├── discover.py           # Discoverer (Milestone 1)\n│   ├── runner.py             # Runner (Milestone 1 & 3)\n│   ├── assert.py             # Assertion Engine (Milestone 2)\n│   ├── fixtures.py           # Fixture System (Milestone 3)\n│   ├── report.py             # Reporter (Milestone 4)\n│   └── results.py            # Data models: TestCase, TestResult, etc.\n├── tests/                    # Tests for Apollo itself\n│   ├── test_discover.py\n│   ├── test_runner.py\n│   └── ...\n└── examples/                 # Example test files for demonstration\n    ├── test_math.py\n    └── test_user.py\n```\n\n#### C. Infrastructure Starter Code: Core Data Models\nThese data structures are the backbone of the framework. Implement them first to give shape to your system.\n\n```python\n# File: apollo/results.py\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\nfrom typing import Any, Optional, Callable\n\nclass TestStatus(Enum):\n    \"\"\"The possible states a test result can be in.\"\"\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    PASSED = \"passed\"\n    FAILED = \"failed\"      # Assertion failed\n    ERRORED = \"errored\"    # Uncaught exception\n    SKIPPED = \"skipped\"\n\n@dataclass\nclass TestCase:\n    \"\"\"Represents a single test to be executed.\"\"\"\n    # Unique identifier: module::function_name or module::ClassName::method_name\n    nodeid: str\n    # The actual callable test function or method\n    func: Callable[[], None]\n    # The file path where this test was discovered\n    file_path: str\n    # Line number in the file (for better error reporting)\n    line_no: int\n\n@dataclass\nclass TestResult:\n    \"\"\"The outcome of executing a single TestCase.\"\"\"\n    test_case: TestCase\n    status: TestStatus\n    # Human-readable message (e.g., assertion error text)\n    message: Optional[str] = None\n    # Exception object if test raised one\n    exception: Optional[Exception] = None\n    # Traceback formatted as string\n    traceback: Optional[str] = None\n    # Duration in seconds\n    duration: float = 0.0\n\n    def __post_init__(self):\n        \"\"\"Ensure consistency between status and other fields.\"\"\"\n        if self.status == TestStatus.FAILED:\n            assert self.message is not None, \"Failed tests must have a message\"\n        if self.status == TestStatus.ERRORED:\n            assert self.exception is not None, \"Errored tests must have an exception\"\n\n@dataclass\nclass TestSuite:\n    \"\"\"A collection of TestCases, often representing a module or directory.\"\"\"\n    name: str\n    tests: list[TestCase]\n\n    def add_test(self, test: TestCase):\n        self.tests.append(test)\n\n    def __len__(self):\n        return len(self.tests)\n```\n\n#### D. Core Logic Skeleton: The Simplest Possible Runner\nTo make the abstract concrete, here is the skeleton of the most basic runner that executes tests sequentially without any isolation or fixtures. This is your starting point for Milestone 1.\n\n```python\n# File: apollo/runner.py\nfrom .results import TestCase, TestResult, TestStatus\nimport traceback\n\nclass SimpleRunner:\n    \"\"\"Executes TestCases one by one and collects TestResults.\"\"\"\n    \n    def run_test(self, test_case: TestCase) -> TestResult:\n        \"\"\"\n        Execute a single test function and return its result.\n        \n        Steps:\n        1. Create a PENDING TestResult\n        2. Start timer\n        3. Change status to RUNNING\n        4. Execute test_case.func() in a try/except block\n        5. Catch AssertionError -> status FAILED, store message\n        6. Catch any other Exception -> status ERRORED, store exception\n        7. If no exception -> status PASSED\n        8. Stop timer, store duration\n        9. Return TestResult\n        \n        TODO 1: Create a TestResult with status=PENDING for this test_case\n        TODO 2: Record start time using time.perf_counter()\n        TODO 3: Update the result's status to RUNNING\n        TODO 4: Wrap test_case.func() call in try/except/else\n        TODO 5: In except AssertionError as e: set status=FAILED, message=str(e)\n        TODO 6: In except Exception as e: set status=ERRORED, exception=e, traceback=traceback.format_exc()\n        TODO 7: In else: set status=PASSED\n        TODO 8: Record end time and calculate duration, store in result\n        TODO 9: Ensure result is returned in all code paths\n        \"\"\"\n        pass\n    \n    def run_suite(self, suite: TestSuite) -> list[TestResult]:\n        \"\"\"Run all tests in a suite and return their results.\"\"\"\n        results = []\n        for test in suite.tests:\n            # TODO: Call run_test for each test and append result to list\n            pass\n        return results\n```\n\n#### E. Language-Specific Hints: Python\n- Use `importlib.import_module()` to dynamically import test files by their file path. Remember to handle `ImportError` gracefully.\n- The `inspect` module is your friend for analyzing function signatures (for fixture injection) and retrieving source code lines.\n- For parallel execution (Milestone 1), `concurrent.futures.ProcessPoolExecutor` provides true isolation (separate processes) but has overhead. `ThreadPoolExecutor` is lighter but requires careful state management.\n- To override assertion behavior, you cannot modify the `assert` statement directly. Instead, provide helper functions like `assert_equal()` that raise `AssertionError` with rich messages.\n\n#### F. Milestone 1 Checkpoint\nAfter implementing the basic Discoverer and SimpleRunner, you should be able to run:\n\n```bash\n$ python -m apollo.cli discover examples/\n```\n\n**Expected Behavior:**\n1.  The Discoverer scans `examples/` directory for `.py` files.\n2.  It imports each module and finds functions whose names start with `test_`.\n3.  It creates `TestCase` objects for each found test.\n4.  The Runner executes each test function.\n5.  The CLI prints a simple report:\n    ```\n    Running 2 tests from examples/\n    \n    test_addition (examples/test_math.py) ... PASSED (0.001s)\n    test_failing (examples/test_math.py) ... FAILED (0.000s)\n      AssertionError: 2 != 3\n    \n    === Summary ===\n    Passed: 1, Failed: 1, Errored: 0\n    Total time: 0.002s\n    ```\n\n**Signs of Trouble:**\n- **No tests found:** Check that your Discoverer is correctly filtering function names and handling module imports.\n- **Tests interfering:** If one test's failure affects another, you haven't achieved isolation. Ensure each test runs in a fresh environment.\n- **No error details:** Your `run_test` method isn't catching and storing exceptions properly.\n\n#### G. Debugging Tips: Early Discovery Issues\n| Symptom | Likely Cause | How to Diagnose | Fix |\n| :--- | :--- | :--- | :--- |\n| **`ModuleNotFoundError` during discovery** | Incorrect module path conversion from file path. | Print the calculated module name before importing. | Use `importlib.util.spec_from_file_location` for file-based imports. |\n| **Test functions from helper modules are discovered** | Discovery isn't filtering by file name pattern (e.g., `test_*.py`). | Print all files being scanned. | Filter files by name *before* importing: `if not filename.startswith('test_'): continue` |\n| **Class methods not discovered** | Only scanning module-level functions, not class methods. | Inspect classes in the module using `inspect.getmembers(module, inspect.isclass)`. | Recursively inspect classes for methods with `test_` prefix. |\n\n---\n\n\n## 2. Goals and Non-Goals\n\n> **Milestone(s):** This section establishes the scope and boundaries for the entire project, providing clear direction for all four milestones.\n\nEvery architectural undertaking requires clear boundaries to focus effort and prevent scope creep. This section defines **what the framework must achieve** (Goals) and **what it explicitly won't address** (Non-Goals), establishing a contract that guides design decisions throughout the project. Think of this as defining the playing field: we're building a comprehensive single-player training ground for test fundamentals, not a professional stadium with every possible feature.\n\n### Goals\n\nThe primary goal of Project Apollo is to create a **pedagogical test framework** that demonstrates how modern testing tools work internally while providing practical utility for testing small to medium Python projects. Each goal maps directly to one of the four milestones, creating a progressive learning path from basic automation to sophisticated testing infrastructure.\n\n| Goal Category | Milestone Alignment | Core Capability | Why This Matters |\n|---------------|---------------------|-----------------|------------------|\n| **Automatic Test Discovery & Execution** | Milestone 1 | The framework automatically finds test functions in code modules and runs them in controlled environments | Eliminates manual test registration and ensures tests run in isolation, preventing state contamination |\n| **Expressive Assertion System** | Milestone 2 | Provides rich comparison assertions with helpful error messages and extensible matcher architecture | Moves beyond basic `assert` statements to give developers immediate insight into why tests fail |\n| **Fixture-Based Test Environment** | Milestone 3 | Manages test setup/teardown through reusable fixtures with configurable lifetimes and dependency injection | Reduces test code duplication and properly manages expensive resources like database connections |\n| **Comprehensive Reporting & CLI** | Milestone 4 | Delivers human-readable and machine-parsable test results through a configurable command-line interface | Enables integration into development workflows and CI/CD pipelines with professional-grade output |\n\nFor each goal category, we define specific functional requirements that must be satisfied:\n\n#### Goal 1: Automatic Test Discovery & Execution\nThe framework must automatically locate and execute test functions without requiring explicit registration lists. This involves:\n\n- **Convention-based discovery**: Finding functions and methods whose names start with `test_` within modules and classes\n- **Module traversal**: Recursively scanning directories and importing Python modules to search for tests\n- **Isolated execution**: Running each test in a clean environment where test failures don't propagate to subsequent tests\n- **Parallel execution support**: Running independent tests concurrently to reduce total suite execution time\n- **Error containment**: Distinguishing between test failures (assertion failures) and test errors (unexpected exceptions)\n\n> **Architecture Decision Record: Convention-over-Configuration for Discovery**\n> - **Context**: Test frameworks need a way to identify which functions are tests without requiring explicit decoration or registration.\n> - **Options Considered**:\n>   1. Explicit decorators (e.g., `@test`)\n>   2. Inheritance from a base test class\n>   3. Naming convention (functions starting with `test_`)\n> - **Decision**: Use naming convention (`test_` prefix) for test identification\n> - **Rationale**: \n>   - **Zero boilerplate**: Tests look like regular functions, reducing cognitive overhead\n>   - **Familiarity**: Matches pytest's approach, which developers already know\n>   - **Simplicity**: No imports or decorators required for basic tests\n>   - **Discoverability**: Easy to scan code for tests by function name\n> - **Consequences**:\n>   - Positive: Extremely low barrier to entry for writing tests\n>   - Negative: Less explicit than decorators, harder to customize per-test behavior\n>   - Neutral: Requires careful module inspection logic\n\n#### Goal 2: Expressive Assertion System\nThe framework must provide a rich set of assertion utilities that offer better diagnostics than Python's built-in `assert` statement:\n\n- **Value comparisons**: Deep equality checking with diff generation for mismatches\n- **Boolean conditions**: Truthiness/falsiness assertions with descriptive messages\n- **Collection operations**: Checking membership, length, and subset relationships\n- **Exception verification**: Ensuring specific exceptions are raised by code under test\n- **Extensible matchers**: Plugin system for domain-specific assertion logic with custom failure messages\n\n**Assertion Comparison Table:**\n| Assertion Type | Example Usage | Key Benefit |\n|----------------|---------------|-------------|\n| Equality | `assert_equal(actual, expected)` | Shows diff between complex structures |\n| Exception | `assert_raises(ValueError, func, arg)` | Validates exception type and optionally message |\n| Collection | `assert_contains(collection, item)` | Clear message when item missing |\n| Custom Matcher | `assert_that(value, is_even())` | Domain-specific validation with tailored messages |\n\n#### Goal 3: Fixture-Based Test Environment\nThe framework must manage test dependencies and environment setup through a fixture system:\n\n- **Per-test setup/teardown**: `setUp` and `tearDown` methods that run before and after each test\n- **Fixture sharing**: Resources created once and shared across multiple tests with configurable lifetimes\n- **Scope management**: Fixtures with different lifetimes (function, class, module, session)\n- **Dependency injection**: Automatic provision of fixtures to test functions based on parameter names\n- **Cleanup guarantees**: Reliable teardown even when tests fail or errors occur\n\n> **Mental Model: The Test Kitchen**\n> Think of fixtures as a well-organized test kitchen. `setUp` is like gathering ingredients before cooking each dish (test). Shared fixtures are like expensive appliances (stand mixer, sous-vide) that you set up once and use for multiple recipes. `tearDown` is cleaning the workspace after each dish, regardless of whether it turned out perfectly. The framework acts as the kitchen manager, ensuring the right tools are available when needed and everything gets cleaned up properly.\n\n#### Goal 4: Comprehensive Reporting & CLI\nThe framework must provide professional output and a usable command-line interface:\n\n- **Progress reporting**: Real-time display of test execution with pass/fail status\n- **Statistical summary**: Totals for passed, failed, errored, and skipped tests with timing information\n- **Flexible test selection**: Filtering tests by name patterns, file patterns, or custom markers\n- **Machine-readable output**: JUnit XML format for CI/CD pipeline integration\n- **Proper exit codes**: Non-zero exit code when tests fail for scriptable automation\n\n**CLI Feature Requirements:**\n| Feature | Example | Purpose |\n|---------|---------|---------|\n| File pattern matching | `apollo tests/*.py` | Run tests in specific files/directories |\n| Test name filtering | `apollo -k \"login\"` | Run only tests with \"login\" in their name |\n| Output format control | `apollo --junit report.xml` | Generate XML for CI systems |\n| Verbosity levels | `apollo -v` / `apollo -vv` | Control detail level in output |\n| Exit code behavior | Returns 0 if all pass, 1 otherwise | Enable scripting and CI failure detection |\n\n### Non-Goals\n\nWhile the goals define what we will build, non-goals explicitly state what we **will not build**, preventing feature creep and maintaining focus on the educational objectives. These are features commonly found in production test frameworks but excluded from this learning project.\n\n| Non-Goal Category | Example Features | Why Excluded |\n|-------------------|------------------|--------------|\n| **Distributed Test Execution** | Running tests across multiple machines/processors with smart distribution | Complexity far exceeds educational scope; focuses on system architecture rather than testing fundamentals |\n| **Plugin Ecosystem** | Third-party plugins for coverage, mocking, database utilities, etc. | Would require stable public APIs and compatibility guarantees beyond learning objectives |\n| **Custom Test Language/DSL** | Special syntax for describing tests beyond Python functions | Adds unnecessary abstraction layer; Python functions are sufficient for learning |\n| **Test Generation/Parameterization** | Automatically generating test cases from data tables or property-based testing | Advanced feature that would distract from core framework mechanics |\n| **Advanced Mocking Library** | Complex patching, spy objects, call verification beyond basic unittest.mock | Mocking is a separate concern from test framework infrastructure |\n| **Cross-Language Support** | Testing JavaScript, Go, or other languages from the same framework | Would require language-specific parsers and executors; Python-only keeps focus |\n| **Interactive Debugger Integration** | Built-in debugger, breakpoints, or REPL within test failures | Adds significant UI complexity; standard debugging tools suffice |\n| **Test Parallelization Dependencies** | Automatic detection of test dependencies for safe parallel execution | Requires complex static analysis; manual marking is sufficient for learning |\n| **Custom Report Formats** | HTML reports, dashboard integration, historical trend analysis | Output formatting is extensible enough via JUnit XML; fancy reports are ancillary |\n| **Database Migration Testing** | Specialized fixtures for database state management and rollbacks | Domain-specific testing concern better handled by dedicated libraries |\n\n#### Important Clarifications on Scope Boundaries\n\n**Why No Plugin System?**\nWhile a plugin architecture is common in production frameworks (pytest has over 1,000 plugins), implementing it would shift focus from **how tests run** to **how to build extensible systems**. The educational value lies in understanding test execution, not building extension points. However, the design leaves intentional **extension seams** where plugins could be added later without major redesign.\n\n**Why Only Basic Parallel Execution?**\nWe implement simple parallel execution where tests run in separate processes without dependency analysis. A full production system would need:\n- Dependency detection between tests\n- Resource contention management\n- Shared fixture coordination across processes\n- Result aggregation with ordering guarantees\n\nThese complexities would triple the implementation effort while teaching little about core test framework concepts.\n\n**Why Exclude Advanced Mocking?**\nMocking libraries (like `unittest.mock`) are already comprehensive in Python. Reinventing them would:\n1. Duplicate existing standard library functionality\n2. Divert focus from test orchestration to object interception\n3. Require deep understanding of Python's object model and descriptor protocol\n\nThe framework will integrate with existing mocking tools rather than replace them.\n\n**The \"Good Enough\" Principle**\nThis framework follows the **\"good enough for learning\"** principle. Each feature is implemented to the minimum level needed to understand the concept, not to production-grade robustness. For example:\n\n- **File system watching** (rerunning tests on file changes) is excluded—it's a nice-to-have feature that doesn't teach core testing concepts\n- **Test ordering control** (running tests in specific sequences) is minimal—just enough to ensure fixtures work correctly\n- **Customizable discovery rules** (changing the `test_` prefix) is excluded—convention-over-configuration simplifies learning\n\n> **Design Principle: Single Responsibility for Learning**\n> Project Apollo focuses exclusively on **test orchestration**—finding tests, running them in the right environment, and reporting results. It deliberately excludes:\n> 1. **Test generation** (property-based testing, combinatorial testing)\n> 2. **Environment management** (Docker integration, cloud testing)\n> 3. **Code analysis** (coverage measurement, static analysis)\n> 4. **UI testing** (browser automation, mobile testing)\n>\n> These are important concerns in real-world testing but represent separate domains of knowledge. By keeping a tight focus, we ensure learners understand the core mechanics before exploring complementary tools.\n\n#### What Could Be Added Later (Future Extensions)\nWhile not goals for the initial implementation, these areas represent logical extensions that the architecture accommodates:\n\n1. **Test Markers** (`@slow`, `@integration`): Simple decorator-based test categorization\n2. **Parameterized Tests**: Running the same test with different input data sets\n3. **Snapshot Testing**: Comparing output against stored \"golden\" references\n4. **Benchmark Timing**: Measuring and reporting test execution performance\n5. **Simple Plugin Hook**: A few well-defined extension points for custom reporters or discovery\n\nThe architecture will be designed with these possible extensions in mind, but they remain explicitly out of scope for the initial implementation.\n\n### Success Criteria\n\nThe framework will be considered successful when it can:\n\n1. **Discover and run** all `test_` prefixed functions in a directory of Python modules\n2. **Provide helpful failure messages** showing exactly what went wrong in assertions\n3. **Manage fixture lifecycles** correctly, cleaning up resources even when tests fail\n4. **Produce both human-readable** terminal output and **machine-parsable** JUnit XML\n5. **Run tests in parallel** when requested, with isolated execution environments\n6. **Integrate smoothly** with existing Python projects using standard tooling\n\nThese criteria map directly to the acceptance criteria in each milestone, creating clear, measurable objectives for the implementation.\n\n### Trade-offs and Constraints\n\nThe goals and non-goals establish several important constraints:\n\n| Constraint | Implication | Rationale |\n|------------|-------------|-----------|\n| **Python-only tests** | No support for testing other languages | Keeps implementation focused and leverages Python's introspection capabilities |\n| **Minimal external dependencies** | Only standard library where possible | Reduces setup complexity and demonstrates how things work without magic |\n| **Educational clarity over performance** | May sacrifice optimizations for understandable code | Primary value is learning, not production speed |\n| **Convention over configuration** | Less flexible than config-driven approaches | Reduces cognitive load for beginners |\n| **Progressive complexity** | Later milestones build on earlier ones | Ensures foundational understanding before advanced features |\n\nThese constraints intentionally shape the framework into a **learning tool first** and a **production tool second**. The architecture will prioritize educational clarity, with clear separation of concerns and minimal magic, even if this means slightly more verbose code or fewer optimizations.\n\n### Common Pitfalls in Scope Definition\n\n⚠️ **Pitfall: Over-engineering for hypothetical use cases**\n- **Description**: Implementing features \"just in case\" someone might need them, adding complexity without clear value\n- **Why it's wrong**: Diverts effort from core learning objectives, increases maintenance burden, and makes the codebase harder to understand\n- **How to avoid**: Strictly adhere to the non-goals list; when tempted to add a feature, ask \"does this directly support one of the four milestones?\"\n\n⚠️ **Pitfall: Under-specifying edge case behavior**\n- **Description**: Assuming \"obvious\" behavior for error cases or boundary conditions without explicit specification\n- **Why it's wrong**: Leads to inconsistent implementation and bugs that are hard to fix later\n- **How to avoid**: For each goal, explicitly define behavior for: empty test suites, import errors, fixture teardown failures, assertion errors in setup methods, etc.\n\n⚠️ **Pitfall: Copying existing frameworks without understanding**\n- **Description**: Implementing features because \"pytest has them\" without understanding why they exist\n- **Why it's wrong**: Misses the educational value; creates cargo-cult architecture\n- **How to avoid**: For each feature, document the problem it solves and alternative approaches considered\n\n### Implementation Guidance\n\n#### Technology Recommendations Table\n\n| Component | Simple Option (Recommended) | Advanced Option (Future Consideration) |\n|-----------|----------------------------|---------------------------------------|\n| **Test Discovery** | `importlib` + `inspect` module | AST parsing for more sophisticated analysis |\n| **Parallel Execution** | `multiprocessing.Pool` with process isolation | `concurrent.futures` with fine-grained control |\n| **Assertion Diffing** | `difflib.SequenceMatcher` for strings | Custom differ for nested structures |\n| **Fixture Management** | Simple registry with scope caching | Graph-based dependency resolution |\n| **CLI Parsing** | `argparse` from standard library | `click` for more sophisticated CLI |\n| **XML Generation** | Manual XML building with `xml.etree.ElementTree` | Template-based generation |\n\n#### Recommended File/Module Structure\n\n```\napollo/\n├── __init__.py              # Public API exports\n├── __main__.py             # CLI entry point (python -m apollo)\n├── cli/\n│   ├── __init__.py\n│   ├── parser.py           # CLI argument parsing\n│   └── commands.py         # CLI command implementations\n├── discovery/\n│   ├── __init__.py\n│   ├── finder.py           # Module scanning and test discovery\n│   └── filters.py          # Test filtering logic\n├── runner/\n│   ├── __init__.py\n│   ├── base.py             # Base runner classes\n│   ├── serial.py           # Serial test execution\n│   └── parallel.py         # Parallel test execution\n├── assertions/\n│   ├── __init__.py         # Public assertion functions\n│   ├── engine.py           # Core assertion logic\n│   ├── matchers.py         # Matcher base classes\n│   └── diffs.py            # Diff generation utilities\n├── fixtures/\n│   ├── __init__.py\n│   ├── registry.py         # Fixture registration and lookup\n│   ├── scopes.py           # Scope management\n│   └── injection.py        # Parameter injection logic\n├── reporting/\n│   ├── __init__.py\n│   ├── console.py          # Terminal output formatting\n│   ├── junit.py            # JUnit XML generation\n│   └── statistics.py       # Result aggregation and stats\n├── core/\n│   ├── __init__.py\n│   ├── types.py            # Core data structures (TestCase, TestResult, etc.)\n│   ├── config.py           # Configuration object\n│   └── exceptions.py       # Framework-specific exceptions\n└── utils/\n    ├── __init__.py\n    ├── introspection.py    # Helper functions for code inspection\n    └── timing.py           # Execution timing utilities\n```\n\n#### Infrastructure Starter Code: Core Data Types\n\nThese core types provide the foundation for the entire framework. Learners should use these exact definitions:\n\n```python\n# apollo/core/types.py\n\"\"\"Core data types for the test framework.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Optional, Callable, Any, List\nimport traceback as tb\n\nclass TestStatus(Enum):\n    \"\"\"Possible states of a test during execution.\"\"\"\n    PENDING = \"pending\"    # Test hasn't started yet\n    RUNNING = \"running\"    # Test is currently executing\n    PASSED = \"passed\"      # All assertions passed\n    FAILED = \"failed\"      # An assertion failed\n    ERRORED = \"errored\"    # An unexpected exception occurred\n    SKIPPED = \"skipped\"    # Test was skipped (not implemented yet)\n\n@dataclass\nclass TestCase:\n    \"\"\"Represents a single test function to be executed.\"\"\"\n    nodeid: str           # Unique identifier (e.g., \"test_module.py::test_function\")\n    func: Callable        # The actual test function to call\n    file_path: str        # Absolute path to the file containing the test\n    line_no: int          # Line number where test function is defined\n    fixtures: List[str] = field(default_factory=list)  # Names of required fixtures\n    \n    def __str__(self) -> str:\n        return self.nodeid\n\n@dataclass\nclass TestResult:\n    \"\"\"Result of executing a single test.\"\"\"\n    test_case: TestCase\n    status: TestStatus\n    message: Optional[str] = None          # Failure/error message\n    exception: Optional[Exception] = None  # Exception object if test errored\n    traceback: Optional[str] = None        # Formatted traceback for errors\n    duration: float = 0.0                  # Execution time in seconds\n    \n    def passed(self) -> bool:\n        return self.status == TestStatus.PASSED\n    \n    def failed(self) -> bool:\n        return self.status == TestStatus.FAILED\n    \n    def errored(self) -> bool:\n        return self.status == TestStatus.ERRORED\n\n@dataclass\nclass TestSuite:\n    \"\"\"Collection of tests to be executed together.\"\"\"\n    name: str\n    tests: List[TestCase]\n    \n    def __len__(self) -> int:\n        return len(self.tests)\n    \n    def __iter__(self):\n        return iter(self.tests)\n\n@dataclass  \nclass Fixture:\n    \"\"\"A test fixture that provides resources to tests.\"\"\"\n    name: str\n    func: Callable\n    scope: str = \"function\"  # \"function\", \"class\", \"module\", \"session\"\n    dependencies: List[str] = field(default_factory=list)  # Names of other fixtures needed\n```\n\n#### Core Logic Skeleton: Test Discovery\n\n```python\n# apollo/discovery/finder.py\n\"\"\"Test discovery implementation.\"\"\"\n\nimport os\nimport sys\nimport importlib\nimport inspect\nfrom pathlib import Path\nfrom typing import List, Iterator, Optional\n\nfrom apollo.core.types import TestCase\n\n\ndef discover_tests(start_path: str, pattern: str = \"test_*.py\") -> List[TestCase]:\n    \"\"\"\n    Discover all test functions in modules under start_path.\n    \n    Args:\n        start_path: Directory or file to start discovery from\n        pattern: Glob pattern for test files (default: \"test_*.py\")\n    \n    Returns:\n        List of TestCase objects representing discovered tests\n    \n    TODO Implementation Steps:\n    1. Convert start_path to absolute Path object\n    2. If start_path is a file, discover tests only in that file\n    3. If start_path is a directory, recursively find all Python files matching pattern\n    4. For each Python file:\n       a. Import it as a module (handle import errors gracefully)\n       b. Inspect the module for functions starting with \"test_\"\n       c. Also inspect classes in the module for methods starting with \"test_\"\n       d. For each test function found, create a TestCase object\n          - nodeid: \"file.py::function_name\" or \"file.py::ClassName::method_name\"\n          - func: The actual callable test function\n          - file_path: Absolute path to the file\n          - line_no: Line number where function is defined (use inspect.getsourcelines)\n    5. Return list of all discovered TestCase objects\n    \n    Hint: Use importlib.import_module for importing, inspect.getmembers for inspection\n    \"\"\"\n    # TODO 1: Convert start_path to absolute Path\n    # TODO 2: Determine if it's a file or directory\n    # TODO 3: Collect all Python files to inspect\n    # TODO 4: For each file, import and discover tests\n    # TODO 5: Return list of TestCase objects\n    pass\n\n\ndef _discover_in_module(module, file_path: str) -> List[TestCase]:\n    \"\"\"\n    Discover test functions within a single module.\n    \n    Args:\n        module: Imported module object\n        file_path: Path to the module file\n    \n    Returns:\n        List of TestCase objects found in the module\n    \"\"\"\n    # TODO 1: Use inspect.getmembers to get all objects in module\n    # TODO 2: Filter for callable objects (functions and methods)\n    # TODO 3: Check if name starts with \"test_\" (for functions)\n    # TODO 4: Also check classes for methods starting with \"test_\"\n    # TODO 5: For each test found, create TestCase with proper nodeid\n    pass\n```\n\n#### Language-Specific Hints for Python\n\n1. **Module Import Safety**: Use `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` to import test modules without adding them to `sys.modules` permanently, preventing side effects between test discoveries.\n\n2. **Line Number Detection**: `inspect.getsourcelines(func)` returns the source lines and starting line number, but be prepared for functions defined in C extensions (which will fail).\n\n3. **Path Handling**: Use `pathlib.Path` objects rather than string manipulation for cross-platform compatibility.\n\n4. **Parallel Execution**: For Milestone 1's parallel execution, use `multiprocessing.Pool` with `initializer` to set up each worker process, passing tests via `map`. Remember to use `if __name__ == \"__main__\":` guards.\n\n5. **Exception Preservation**: When tests raise exceptions in separate processes, pickle the exception info and re-raise it in the main process for proper reporting.\n\n#### Milestone Checkpoint: Goal Verification\n\nAfter implementing the basic discovery from Milestone 1, verify the goals with this test:\n\n```bash\n# Create a test file\ncat > test_example.py << 'EOF'\ndef test_addition():\n    assert 1 + 1 == 2\n\ndef test_failure():\n    assert 2 + 2 == 5  # This will fail\n\nclass TestCalculator:\n    def test_multiplication(self):\n        assert 3 * 3 == 9\nEOF\n\n# Run discovery and execution\npython -m apollo discover test_example.py\n# Expected: Should list 3 tests found\n\npython -m apollo run test_example.py\n# Expected: Should show:\n# test_example.py::test_addition ... PASSED\n# test_example.py::test_failure ... FAILED\n# test_example.py::TestCalculator::test_multiplication ... PASSED\n# \n# ====== SUMMARY ======\n# Tests: 3, Passed: 2, Failed: 1, Errored: 0\n# \n# Exit code should be 1 (since tests failed)\n```\n\n**Signs of Problems:**\n- **No tests found**: Check that discovery is looking for `test_` prefix and importing modules correctly\n- **Tests run in wrong order**: Ensure tests are isolated; order shouldn't matter\n- **Exit code is 0 when tests fail**: The CLI must return non-zero on failure for CI integration\n\n#### Debugging Tips for Scope-Related Issues\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Framework tries to test non-Python files** | Discovery pattern too broad | Add debug logging to show which files are being considered | Tighten file pattern matching to `*.py` only |\n| **Import errors break entire test run** | Not catching import exceptions in discovery | Wrap module imports in try/except | Continue discovery even if some modules fail to import |\n| **Tests from installed packages get discovered** | Discovery scanning system paths | Check which directories are being scanned | Limit discovery to user-specified paths only |\n| **Fixture teardown runs multiple times** | Incorrect scope implementation | Add logging to fixture setup/teardown | Ensure fixture caching by scope key |\n\n\n## 3. High-Level Architecture\n\n> **Milestone(s):** This section provides the structural blueprint for the entire framework, establishing the core components and their relationships that will be developed across all four milestones.\n\nAt its heart, a test framework is a **Test Conductor**—an orchestrator that locates test functions, provides them with necessary resources (fixtures), executes them in a controlled environment, validates their outcomes, and composes a performance report. This architectural view reveals how the framework transforms a simple command into a comprehensive test run by coordinating specialized components.\n\n### Component Overview & Responsibilities\n\nThink of the framework as a factory assembly line. Raw materials (your source code) enter at one end. The **Discoverer** acts as the quality inspector, identifying test-shaped parts. The **Runner** is the robotic arm that assembles and tests each unit in isolation. The **Assertion Engine** is the precision measurement tool that validates each output against specifications. The **Reporter** is the final quality assurance station that stamps each part with a pass/fail label and generates the inspection report. The **CLI Parser** is the factory control panel where the operator (you) specifies which production line to run and how to format the report.\n\nEach component has a single, well-defined responsibility and communicates with others through clear data structures, minimizing coupling and enabling independent evolution. The system follows a linear dataflow pipeline architecture, where output from one stage becomes input for the next.\n\n| Component | Primary Responsibility | Key Inputs | Key Outputs | Data Structures It Owns |\n|-----------|-----------------------|------------|-------------|--------------------------|\n| **CLI Parser** | Translates user command-line arguments into a structured configuration that drives the entire test run. | Command-line arguments (`sys.argv`) | `Configuration` object | `Configuration` (parsed arguments) |\n| **Discoverer** | Scans the filesystem and Python modules to find test functions/classes based on naming conventions. | `Configuration` (paths, patterns), filesystem | `TestSuite` (collection of `TestCase` objects) | `TestCase` definitions (but not their execution state) |\n| **Runner** | Executes test cases in a controlled, isolated environment, managing fixture lifecycle and capturing outcomes. | `TestSuite`, `Configuration` (parallel flags) | List of `TestResult` objects | Test execution state, fixture instances (during run) |\n| **Assertion Engine** | Evaluates assertion conditions and produces rich, descriptive failure messages when expectations aren't met. | Actual value, expected value/matcher | Either passes silently or raises `AssertionError` with formatted message | Comparison diffs, custom matcher state |\n| **Reporter** | Collects test results, formats them for human and machine consumption, and outputs to appropriate destinations. | List of `TestResult` objects, `Configuration` (output format, verbosity) | Console output, JUnit XML files, exit code | Aggregated statistics (totals, durations) |\n\n> **Design Insight:** The pipeline architecture creates natural separation of concerns. Each component only needs to understand the data structures it receives from the previous stage and produces for the next. This makes the system testable—you can verify the Discoverer by checking if it produces the right `TestCase` objects without actually running tests.\n\n**CLI Parser** is the system's front door. It validates user input, converts file patterns (`tests/**/test_*.py`) into concrete filesystem paths, and sets operational modes (verbose output, parallel execution). It produces a `Configuration` object that becomes the single source of truth for the entire test run.\n\n**Discoverer** implements the **convention-over-configuration** principle. Instead of requiring users to register tests manually, it automatically scans directories and modules. It inspects each Python module's attributes, looking for functions whose names start with `test_` or classes following the **xUnit** pattern. For each discovered test, it creates a `TestCase` object containing the test's identity (nodeid), the callable function, source location, and any declared fixture dependencies.\n\n**Runner** is the workhorse that brings tests to life. Its most critical responsibility is ensuring **test isolation**—each test runs in its own environment so that state leaks or failures don't affect subsequent tests. The runner coordinates with the fixture system to set up required resources before test execution and tear them down afterward. For Milestone 1's parallel execution, the runner manages a pool of worker processes/threads, distributing independent tests to maximize throughput while maintaining isolation.\n\n**Assertion Engine** transforms vague \"something went wrong\" errors into actionable diagnostics. When an assertion fails (like `assert_equal(actual, expected)`), the engine doesn't just say \"False is not True\"—it computes a diff between actual and expected values, formats them for readability, and raises an `AssertionError` with a clear message. This component also provides the extensible **Matchers API** that allows users to define domain-specific assertions with custom failure messages.\n\n**Reporter** serves two audiences: humans reading terminal output and CI/CD systems parsing machine-readable formats. It formats individual test results (pass/fail status with execution time) and aggregates summary statistics (total passed/failed/errored counts, total duration). For CI integration, it generates **JUnit XML** output—a standardized format understood by tools like Jenkins and GitHub Actions.\n\n> **Architecture Decision Record: Pipeline vs. Plugin Architecture**\n>\n> **Decision:** We chose a linear pipeline architecture over a more flexible plugin system for the core framework.\n>\n> **Context:** The framework needs to coordinate multiple distinct phases (discovery, execution, reporting) with clear dependencies between them. While modern frameworks like pytest support extensive plugin ecosystems, our educational framework prioritizes simplicity and clarity of data flow.\n>\n> **Options Considered:**\n> 1. **Linear Pipeline (Chosen):** Components execute in fixed sequence (CLI → Discover → Run → Report) with well-defined interfaces between them.\n> 2. **Event-Driven Plugin System:** Components emit events (test_discovered, test_started, test_finished) that plugins can hook into, allowing arbitrary extension points.\n> 3. **Modular Monolith:** All functionality in a single orchestrator class with configurable strategy objects for each phase.\n>\n> **Rationale:** The pipeline architecture provides the clearest mental model for learners—they can trace execution from start to finish without dealing with inversion of control. Each component has a single responsibility, making the codebase easier to understand and test. The interfaces between components (like `TestSuite` and `TestResult`) become natural learning boundaries.\n>\n> **Consequences:**\n> - **Positive:** Straightforward to implement, debug, and reason about. Clear separation of concerns.\n> - **Negative:** Less flexible than a plugin system—adding new hooks requires modifying the pipeline stages directly.\n> - **Mitigation:** We keep components loosely coupled through interface-like data structures, allowing internal refactoring without breaking the overall flow.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Linear Pipeline | Simple to understand, easy to debug, clear data flow | Less extensible, harder to add cross-cutting concerns | ✅ Yes |\n| Event-Driven Plugin System | Highly extensible, supports cross-cutting concerns | Complex to implement and debug, inversion of control | ❌ No |\n| Modular Monolith | Unified control flow, can optimize across phases | Tight coupling, harder to test components in isolation | ❌ No |\n\nThe components interact as shown in the system component diagram:\n\n![System Component Diagram](./diagrams/sys-component.svg)\n\n### Recommended File/Module Structure\n\nOrganizing code thoughtfully from the start prevents \"everything in one file\" chaos and establishes clear boundaries. Think of the project structure as a library building: the ground floor (`apollo/`) contains public areas anyone can visit (the API), upper floors (`core/`) house specialized departments (components), and the basement (`internal/`) contains infrastructure utilities that only library staff should access.\n\n> **Design Principle:** The public API should be minimal and intuitive—users typically interact with just the test runner and assertion functions. Internal components can be complex as needed, but their complexity is hidden behind clean interfaces.\n\n```\napollo-test-framework/\n├── pyproject.toml              # Project metadata and dependencies\n├── README.md\n├── examples/                   # Example test files for demonstration\n│   ├── test_math_operations.py\n│   └── test_with_fixtures.py\n├── apollo/                     # Public API package - what users import\n│   ├── __init__.py            # Main exports: run_tests, assert_equal, fixture, etc.\n│   ├── runner.py              # Public runner interface (simple entry point)\n│   ├── assertions.py          # Public assertion functions (assert_equal, assert_raises)\n│   └── fixtures.py            # Public fixture decorators and utilities\n├── src/                        # Core implementation (internal)\n│   ├── apollo/\n│   │   ├── core/              # Core framework components\n│   │   │   ├── __init__.py\n│   │   │   ├── config.py      # Configuration dataclass and CLI parsing\n│   │   │   ├── discoverer.py  # Test discovery logic\n│   │   │   ├── runner.py      # Test execution and isolation\n│   │   │   ├── assertions/    # Assertion engine implementation\n│   │   │   │   ├── __init__.py\n│   │   │   │   ├── engine.py  # Core assertion evaluation\n│   │   │   │   ├── matchers.py # Custom matcher API\n│   │   │   │   └── diffs.py   # Diff calculation for rich error messages\n│   │   │   ├── fixtures/      # Fixture system implementation\n│   │   │   │   ├── __init__.py\n│   │   │   │   ├── registry.py # Fixture registration and lookup\n│   │   │   │   ├── scopes.py   # Scope management (function, class, module)\n│   │   │   │   └── injection.py # Parameter injection mechanism\n│   │   │   └── reporting/     # Reporting and output formatting\n│   │   │       ├── __init__.py\n│   │   │       ├── reporter.py # Console and XML reporting\n│   │   │       ├── console.py  # Terminal output formatting\n│   │   │       └── junit_xml.py # JUnit XML generation\n│   │   ├── data/              # Data structures (shared across components)\n│   │   │   ├── __init__.py\n│   │   │   ├── test_case.py   # TestCase and TestSuite definitions\n│   │   │   ├── test_result.py # TestResult and TestStatus definitions\n│   │   │   └── fixtures.py    # Fixture definition data structures\n│   │   └── utils/             # Internal utilities\n│   │       ├── __init__.py\n│   │       ├── introspection.py # Module inspection helpers\n│   │       ├── path_utils.py    # Filesystem path handling\n│   │       └── parallel.py      # Parallel execution utilities\n├── tests/                      # Framework's own test suite\n│   ├── __init__.py\n│   ├── test_discovery.py      # Tests for the discoverer\n│   ├── test_runner.py         # Tests for the runner\n│   ├── test_assertions.py     # Tests for assertions\n│   └── test_fixtures.py       # Tests for fixtures\n└── scripts/                   # Development and utility scripts\n    └── cli.py                 # Main CLI entry point (installed as 'apollo')\n```\n\n> **Key Architectural Boundaries:**\n> 1. **Public vs. Internal:** The `apollo/` directory contains the user-facing API, while `src/apollo/core/` contains implementation details. This separation allows us to evolve the internal architecture without breaking user code.\n> 2. **Data Layer:** The `data/` directory houses all shared data structures (`TestCase`, `TestResult`, `Fixture`). Components pass these objects but don't own their definitions—this prevents circular dependencies.\n> 3. **Component Independence:** Each subdirectory under `core/` represents a component with minimal imports from other components. They communicate through the data structures and well-defined function calls.\n\n**Module Ownership and Dependencies:**\n\n| Module | Owns | Depends On | Access Level |\n|--------|------|------------|--------------|\n| `apollo/runner.py` | Public `run_tests()` function | `core.config`, `core.discoverer`, `core.runner` | Public API |\n| `core/config.py` | `Configuration` dataclass, CLI parsing | Standard library (`argparse`, `pathlib`) | Internal |\n| `core/discoverer.py` | `discover_tests()` function, module scanning | `data.test_case`, `utils.introspection`, `utils.path_utils` | Internal |\n| `core/runner.py` | `SimpleRunner` class, test execution | `data.test_result`, `core.fixtures`, `utils.parallel` | Internal |\n| `assertions/engine.py` | `assert_equal()`, `assert_true()` implementations | `assertions.diffs` | Internal |\n| `fixtures/registry.py` | `FixtureRegistry` class, fixture storage | `data.fixtures`, `fixtures.scopes` | Internal |\n| `reporting/reporter.py` | `Reporter` class, result aggregation | `data.test_result`, `reporting.console`, `reporting.junit_xml` | Internal |\n\n> **Architecture Decision Record: Flat vs. Nested Public API**\n>\n> **Decision:** We provide a flat public API with all common functions directly importable from the `apollo` package.\n>\n> **Context:** Users expect test frameworks to be convenient to use—they don't want to remember complex import paths. The trade-off is between simplicity and namespace pollution.\n>\n> **Options Considered:**\n> 1. **Flat API (Chosen):** `from apollo import run_tests, assert_equal, fixture`\n> 2. **Namespaced API:** `from apollo.runner import run_tests`, `from apollo.assertions import assert_equal`\n> 3. **Module-based API:** `import apollo; apollo.run_tests()`, `import apollo.assertions`\n>\n> **Rationale:** The flat API matches user expectations from popular frameworks like pytest (`import pytest`) and unittest (`import unittest`). It reduces cognitive load for beginners—they learn one import statement and get all essential functionality. We can carefully curate what's exposed in `apollo/__init__.py` to avoid true namespace pollution.\n>\n> **Consequences:**\n> - **Positive:** Extremely user-friendly, matches industry conventions.\n> - **Negative:** The `apollo` namespace becomes crowded if we're not disciplined.\n> - **Mitigation:** Only expose the 10-15 most commonly used functions/classes publicly. Keep specialized utilities in their submodules.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Circular Dependencies Between Components**\n- **Description:** The Discoverer imports from Runner to create TestCase objects that need runner-specific information, while the Runner imports from Discoverer to understand how to execute those objects.\n- **Why It's Wrong:** Circular dependencies cause import errors and make the codebase fragile and hard to reason about. They often indicate poor separation of concerns.\n- **How to Avoid:** Establish a clear data layer (`data/`) with plain dataclasses that have no business logic. Components should only depend on data structures, not on each other's implementation details. Use dependency injection or callback registration if components need to communicate.\n\n⚠️ **Pitfall: Monolithic Runner Class**\n- **Description:** Creating a single `Runner` class that handles discovery, fixture management, parallel execution, and result collection—becoming a \"god object\" with thousands of lines.\n- **Why It's Wrong:** Violates single responsibility principle, makes testing difficult, and prevents reuse of components (e.g., using the fixture system without the runner).\n- **How to Avoid:** Decompose the runner into focused collaborators: a `FixtureManager` handles fixtures, a `ParallelExecutor` manages concurrency, a `TestExecutor` runs individual tests. The main `SimpleRunner` coordinates these helpers.\n\n⚠️ **Pitfall: Hardcoded Path Assumptions**\n- **Description:** Assuming tests will always be run from the project root directory or using relative imports that break when the framework is installed as a package.\n- **Why It's Wrong:** The framework will fail when users run tests from different directories or install it via pip.\n- **How to Avoid:** Always use absolute paths derived from `__file__` or user-provided configuration. Use `importlib` for module loading instead of manipulating `sys.path` directly. Test the framework from various working directories.\n\n### Implementation Guidance\n\nThis section bridges the architectural design with concrete implementation, providing the scaffolding to start building.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|--------------|-----------------|\n| **CLI Parser** | Python's built-in `argparse` module | `click` library for richer CLI experience |\n| **Module Discovery** | `importlib` + `os.walk` for filesystem traversal | `pathlib` for modern path handling, `pkgutil` for package-aware discovery |\n| **Test Isolation** | Fresh `sys.modules` import for each test | Subprocess execution for complete isolation (slower but more robust) |\n| **Parallel Execution** | `concurrent.futures.ThreadPoolExecutor` (I/O-bound) | `multiprocessing.Pool` for CPU-bound tests (avoids GIL) |\n| **Diff Generation** | `difflib.unified_diff` for text comparison | Custom object diffing with `pprint` formatting for complex structures |\n| **XML Generation** | Manual string building with `xml.etree.ElementTree` | `xml.dom.minidom` for pretty-printing |\n| **Fixture Injection** | Function signature inspection via `inspect.signature` | AST parsing for more advanced injection patterns |\n\nFor this educational project, we recommend the **Simple Option** for each component to focus on core concepts rather than production optimizations.\n\n#### B. Recommended File/Module Structure\n\nFollowing the architecture above, create these initial files with the following minimal content:\n\n**1. Create the public API (`apollo/__init__.py`):**\n```python\n\"\"\"\nApollo Test Framework - A simple, extensible testing framework for Python.\n\"\"\"\n\nfrom .runner import run_tests\nfrom .assertions import (\n    assert_equal,\n    assert_not_equal,\n    assert_true,\n    assert_false,\n    assert_raises,\n    assert_in,\n    assert_not_in,\n)\nfrom .fixtures import fixture\n\n__version__ = \"0.1.0\"\n__all__ = [\n    \"run_tests\",\n    \"assert_equal\",\n    \"assert_not_equal\",\n    \"assert_true\", \n    \"assert_false\",\n    \"assert_raises\",\n    \"assert_in\",\n    \"assert_not_in\",\n    \"fixture\",\n]\n```\n\n**2. Create the data structures layer (`src/apollo/data/__init__.py`):**\n```python\n\"\"\"\nData structures shared across all framework components.\n\"\"\"\n\nfrom .test_case import TestCase, TestSuite\nfrom .test_result import TestResult, TestStatus\nfrom .fixtures import Fixture\n\n__all__ = [\"TestCase\", \"TestSuite\", \"TestResult\", \"TestStatus\", \"Fixture\"]\n```\n\n**3. Create the main CLI entry point (`scripts/cli.py`):**\n```python\n#!/usr/bin/env python3\n\"\"\"\nCommand-line interface for the Apollo test framework.\n\"\"\"\n\nimport sys\nfrom src.apollo.core.config import parse_cli_args\nfrom src.apollo.core.discoverer import discover_tests\nfrom src.apollo.core.runner import SimpleRunner\nfrom src.apollo.core.reporting.reporter import Reporter\n\ndef main():\n    \"\"\"Main entry point for the Apollo CLI.\"\"\"\n    # TODO 1: Parse command-line arguments using parse_cli_args()\n    # TODO 2: Use discover_tests() to find tests based on configuration\n    # TODO 3: Create a TestSuite from discovered tests\n    # TODO 4: Create a SimpleRunner and run the test suite\n    # TODO 5: Create a Reporter and output results\n    # TODO 6: Set appropriate exit code (0 if all tests pass, 1 otherwise)\n    pass\n\nif __name__ == \"__main__\":\n    main()\n```\n\n#### C. Infrastructure Starter Code\n\nHere's complete, working code for foundational utilities that aren't the core learning focus:\n\n**1. Path Utilities (`src/apollo/utils/path_utils.py`):**\n```python\n\"\"\"\nUtilities for filesystem path handling in test discovery.\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import List, Union\n\ndef resolve_patterns_to_paths(\n    patterns: List[str], \n    base_dir: Union[str, Path] = None\n) -> List[Path]:\n    \"\"\"\n    Convert file patterns (like 'tests/**/test_*.py') to concrete file paths.\n    \n    Args:\n        patterns: List of glob patterns or file paths\n        base_dir: Base directory for relative patterns (defaults to cwd)\n    \n    Returns:\n        List of absolute Path objects to existing Python files\n    \"\"\"\n    if base_dir is None:\n        base_dir = Path.cwd()\n    else:\n        base_dir = Path(base_dir).resolve()\n    \n    resolved_paths = []\n    for pattern in patterns:\n        # If it's already an existing file, use it directly\n        path = Path(pattern)\n        if path.exists():\n            resolved_paths.append(path.resolve())\n            continue\n            \n        # Otherwise treat as a glob pattern relative to base_dir\n        for matched in base_dir.glob(pattern):\n            if matched.is_file() and matched.suffix == '.py':\n                resolved_paths.append(matched.resolve())\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_paths = []\n    for path in resolved_paths:\n        if path not in seen:\n            seen.add(path)\n            unique_paths.append(path)\n    \n    return unique_paths\n\ndef module_path_to_name(file_path: Path, root_dir: Path) -> str:\n    \"\"\"\n    Convert a filesystem path to a Python importable module name.\n    \n    Example: /home/user/project/tests/test_math.py -> tests.test_math\n    \"\"\"\n    # Make relative to root directory\n    try:\n        rel_path = file_path.relative_to(root_dir)\n    except ValueError:\n        # File is outside root directory - use its absolute path\n        # but replace path separators\n        parts = list(file_path.parts)\n        # Remove .py extension from last part\n        parts[-1] = parts[-1][:-3]\n        return \".\".join(parts)\n    \n    # Remove .py extension and convert to dotted notation\n    module_name = str(rel_path.with_suffix('')).replace(os.sep, '.')\n    return module_name\n```\n\n**2. Configuration Dataclass (`src/apollo/core/config.py`):**\n```python\n\"\"\"\nConfiguration dataclass and CLI argument parsing.\n\"\"\"\n\nimport argparse\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import List, Optional\n\n@dataclass\nclass Configuration:\n    \"\"\"Structured configuration for a test run.\"\"\"\n    # File patterns to search for tests\n    file_patterns: List[str] = field(default_factory=lambda: [\"test_*.py\", \"*_test.py\"])\n    \n    # Directory to start discovery (defaults to current directory)\n    start_dir: Path = field(default_factory=Path.cwd)\n    \n    # Test name filters (only run tests matching these substrings)\n    test_name_filters: List[str] = field(default_factory=list)\n    \n    # Output control\n    verbose: bool = False\n    quiet: bool = False\n    output_format: str = \"console\"  # \"console\" or \"junit-xml\"\n    \n    # Execution control\n    parallel: bool = False\n    max_workers: Optional[int] = None\n    \n    # Reporting control\n    show_durations: bool = False\n    junit_xml_path: Optional[Path] = None\n\ndef parse_cli_args(args=None) -> Configuration:\n    \"\"\"\n    Parse command-line arguments into a Configuration object.\n    \n    Args:\n        args: Command-line arguments (defaults to sys.argv[1:])\n    \n    Returns:\n        Configuration object with parsed settings\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Apollo Test Framework - Run Python tests\"\n    )\n    \n    # File discovery arguments\n    parser.add_argument(\n        \"paths\",\n        nargs=\"*\",\n        default=[\".\"],\n        help=\"File or directory paths to search for tests (default: current directory)\"\n    )\n    parser.add_argument(\n        \"-p\", \"--pattern\",\n        action=\"append\",\n        dest=\"patterns\",\n        default=[],\n        help=\"Glob pattern for test files (e.g., 'test_*.py'). Can be specified multiple times.\"\n    )\n    \n    # Test selection arguments\n    parser.add_argument(\n        \"-k\", \"--keyword\",\n        action=\"append\",\n        dest=\"keywords\",\n        default=[],\n        help=\"Only run tests matching given substring in test name. Can be specified multiple times.\"\n    )\n    \n    # Output control arguments\n    parser.add_argument(\n        \"-v\", \"--verbose\",\n        action=\"store_true\",\n        help=\"Verbose output: show more details about test execution\"\n    )\n    parser.add_argument(\n        \"-q\", \"--quiet\",\n        action=\"store_true\",\n        help=\"Quiet mode: only show failures and final summary\"\n    )\n    parser.add_argument(\n        \"--junit-xml\",\n        dest=\"junit_xml_path\",\n        type=Path,\n        help=\"Path to write JUnit XML test results\"\n    )\n    \n    # Execution control arguments\n    parser.add_argument(\n        \"--parallel\",\n        action=\"store_true\",\n        help=\"Run tests in parallel using multiple workers\"\n    )\n    parser.add_argument(\n        \"-n\", \"--num-workers\",\n        type=int,\n        dest=\"max_workers\",\n        help=\"Number of parallel workers (default: CPU count)\"\n    )\n    \n    # Parse and convert to Configuration\n    parsed = parser.parse_args(args)\n    \n    # Use default patterns if none provided\n    patterns = parsed.patterns if parsed.patterns else [\"test_*.py\", \"*_test.py\"]\n    \n    return Configuration(\n        file_patterns=patterns,\n        start_dir=Path(parsed.paths[0]).resolve(),\n        test_name_filters=parsed.keywords,\n        verbose=parsed.verbose,\n        quiet=parsed.quiet,\n        output_format=\"junit-xml\" if parsed.junit_xml_path else \"console\",\n        parallel=parsed.parallel,\n        max_workers=parsed.max_workers,\n        show_durations=parsed.verbose,  # Show durations in verbose mode\n        junit_xml_path=parsed.junit_xml_path,\n    )\n```\n\n#### D. Core Logic Skeleton Code\n\n**1. TestCase Data Structure (`src/apollo/data/test_case.py`):**\n```python\n\"\"\"\nTest case representation.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Callable, List, Optional\nfrom pathlib import Path\n\n@dataclass\nclass TestCase:\n    \"\"\"\n    Represents a single test case to be executed.\n    \n    Fields:\n        nodeid: Unique identifier for the test (e.g., \"test_module.py::test_function\")\n        func: The callable test function or method\n        file_path: Absolute path to the file containing the test\n        line_no: Line number where the test is defined (approximate)\n        fixtures: List of fixture names this test depends on\n    \"\"\"\n    nodeid: str\n    func: Callable\n    file_path: Path\n    line_no: int = 0\n    fixtures: List[str] = field(default_factory=list)\n    \n    def __str__(self) -> str:\n        return self.nodeid\n\n@dataclass\nclass TestSuite:\n    \"\"\"\n    A collection of test cases that can be executed together.\n    \n    Fields:\n        name: Name of the test suite (often the module or directory name)\n        tests: List of test cases in this suite\n    \"\"\"\n    name: str\n    tests: List[TestCase]\n    \n    def __len__(self) -> int:\n        return len(self.tests)\n    \n    def add_test(self, test: TestCase) -> None:\n        \"\"\"Add a test case to the suite.\"\"\"\n        # TODO 1: Append the test to the tests list\n        pass\n    \n    def filter_tests(self, keywords: List[str]) -> \"TestSuite\":\n        \"\"\"\n        Return a new TestSuite containing only tests whose nodeid\n        contains any of the given keywords.\n        \n        Args:\n            keywords: List of substrings to match in test names\n            \n        Returns:\n            New TestSuite with filtered tests\n        \"\"\"\n        # TODO 1: If keywords is empty, return a copy of self\n        # TODO 2: Otherwise, filter tests where any keyword in keywords\n        #         is a substring of test.nodeid\n        # TODO 3: Return a new TestSuite with the filtered tests\n        pass\n```\n\n**2. Discoverer Core Function (`src/apollo/core/discoverer.py`):**\n```python\n\"\"\"\nTest discovery implementation.\n\"\"\"\n\nimport inspect\nimport importlib.util\nimport sys\nfrom pathlib import Path\nfrom typing import List, Iterator\nfrom ..data.test_case import TestCase, TestSuite\n\ndef discover_tests(start_path: Path, pattern: str = \"test_*.py\") -> List[TestCase]:\n    \"\"\"\n    Discover all test functions in modules under start_path.\n    \n    Args:\n        start_path: Directory or file to start discovery from\n        pattern: Glob pattern for test files\n    \n    Returns:\n        List of discovered TestCase objects\n    \"\"\"\n    # TODO 1: If start_path is a file, just process that file\n    # TODO 2: If start_path is a directory, recursively find all .py files matching pattern\n    # TODO 3: For each Python file, import it as a module\n    # TODO 4: Inspect the module for test functions (names starting with 'test_')\n    # TODO 5: Also inspect classes for test methods (methods starting with 'test_')\n    # TODO 6: For each test function/method, create a TestCase object\n    # TODO 7: Collect all TestCase objects and return them\n    pass\n\ndef _import_module_from_file(file_path: Path):\n    \"\"\"\n    Import a module from a filesystem path.\n    \n    Args:\n        file_path: Path to Python file\n        \n    Returns:\n        Imported module object\n    \"\"\"\n    # TODO 1: Generate a unique module name based on file path\n    # TODO 2: Use importlib.util.spec_from_file_location to create a module spec\n    # TODO 3: Create the module from the spec\n    # TODO 4: Execute the module in its own namespace\n    # TODO 5: Add the module to sys.modules (temporarily)\n    # TODO 6: Return the module object\n    pass\n\ndef _find_tests_in_module(module) -> Iterator[TestCase]:\n    \"\"\"\n    Yield TestCase objects for test functions found in a module.\n    \n    Args:\n        module: Imported module object\n        \n    Yields:\n        TestCase objects for each discovered test\n    \"\"\"\n    # TODO 1: Iterate through all members of the module using dir(module)\n    # TODO 2: For each member, get the actual object using getattr(module, name)\n    # TODO 3: Check if the object is a callable function (using inspect.isfunction)\n    # TODO 4: Check if the function name starts with 'test_'\n    # TODO 5: If so, create a TestCase with appropriate nodeid, func, etc.\n    # TODO 6: Also check for classes and their methods (xUnit style)\n    # TODO 7: Yield each discovered TestCase\n    pass\n```\n\n#### E. Language-Specific Hints\n\n- **Module Import Isolation:** Use `importlib.import_module` with a fresh `sys.modules` copy for each test to prevent side effects. Store the original `sys.modules`, create a copy, run the test, then restore the original.\n- **Parallel Execution:** Start with `ThreadPoolExecutor` for I/O-bound tests. Use `futures.as_completed()` to process results as they complete rather than waiting for all tests.\n- **Path Handling:** Always convert paths to `pathlib.Path` objects early and use their methods (`resolve()`, `relative_to()`, `parent`) instead of string manipulation.\n- **Inspection:** Use `inspect.signature(func).parameters` to detect fixture dependencies from parameter names. Remember that `inspect.getsourcefile()` can get the file path of a function.\n- **Dynamic Import:** When importing test modules dynamically, generate a unique module name like `f\"__apollo_test_{hash(file_path)}\"` to avoid conflicts with existing modules.\n\n#### F. Milestone Checkpoint\n\nAfter implementing the basic file structure and starter code:\n\n**Verify Setup:**\n```bash\n# From the project root directory\npython -m pytest scripts/cli.py --version  # Should show 0.1.0 if imports work\npython -c \"from src.apollo.data.test_case import TestCase; print('Data structures import OK')\"\n```\n\n**Expected Behavior:** No import errors. The CLI script should run (though it won't do anything yet). The data structures should be importable without circular dependency issues.\n\n**Signs of Trouble:**\n- `ModuleNotFoundError: No module named 'apollo'` → Check your `PYTHONPATH` or install the package in development mode with `pip install -e .`\n- `ImportError: cannot import name 'TestCase' from partially initialized module` → Circular dependency. Ensure `data/__init__.py` only imports from completed modules.\n- `AttributeError: module 'argparse' has no attribute 'ArgumentParser'` → Python version issue. Use Python 3.7+.\n\n\n## 4. Data Model\n\n> **Milestone(s):** All four milestones, as the data model defines the foundational structures that flow through every component.\n\nAt the heart of every well-architected system lies a clear, consistent data model—the vocabulary through which components communicate and the scaffolding upon which behavior is built. Think of the data model as the **DNA of the test framework**: just as DNA encodes the instructions for building and operating an organism, our data structures encode everything about a test—its identity, its dependencies, its outcome, and its relationship to other tests. Without a precise, well-defined data model, components would speak different languages, leading to confusion, bugs, and an architecture that's difficult to extend.\n\nThis section defines the core types that represent the essential concepts in our test framework—tests, results, and fixtures—along with supporting structures that orchestrate their interactions. These data structures flow through the pipeline architecture described earlier, transforming from one form to another as the framework discovers, executes, and reports on tests.\n\n### Core Types\n\nThe core types are the fundamental building blocks that every component understands and manipulates. They represent the irreducible concepts in the testing domain.\n\n#### TestStatus: The Lifecycle State of a Test\n\nThink of `TestStatus` as the **traffic light system** for your tests. Just as a traffic light tells drivers whether to go, slow down, or stop, the `TestStatus` tells the framework what stage a test is in and what should happen next. This enumeration captures every possible state in a test's lifecycle, from its initial discovery to its final outcome.\n\n| Value | Description |\n|-------|-------------|\n| `PENDING` | The test has been discovered but not yet scheduled for execution. This is the initial state for all discovered tests. |\n| `RUNNING` | The test is currently being executed. This state is useful for tracking concurrent execution and providing real-time feedback. |\n| `PASSED` | The test completed execution and all assertions within it passed. This is the desired outcome. |\n| `FAILED` | The test completed execution but one or more assertions failed. The test logic worked correctly but the expected condition wasn't met. |\n| `ERRORED` | The test encountered an unexpected exception during execution (e.g., a syntax error, missing import, or fixture setup failure). This indicates a problem with the test itself, not a failed assertion. |\n| `SKIPPED` | The test was intentionally skipped, either through framework directives or user configuration. Skipped tests are not executed. |\n\n> **Design Insight:** The distinction between `FAILED` and `ERRORED` is crucial for effective debugging. A failure means the code under test didn't behave as expected, while an error means the test itself is broken. Separating these states helps developers quickly diagnose whether they need to fix their production code or their test code.\n\n**Architecture Decision Record: Three-State vs. Six-State Test Outcomes**\n\n> **Decision: Use a six-state enumeration for test results**\n> - **Context:** We need to track test outcomes with enough granularity to support detailed reporting, CI integration, and user debugging. Some frameworks (like JUnit) use a simpler three-state model (pass/fail/error), while others (like pytest) have more nuanced states.\n> - **Options Considered:**\n>   1. **Three states:** `PASS`, `FAIL`, `ERROR` (simplest model)\n>   2. **Four states:** Add `SKIP` to three-state model\n>   3. **Six states:** `PENDING`, `RUNNING`, `PASSED`, `FAILED`, `ERRORED`, `SKIPPED` (full lifecycle tracking)\n> - **Decision:** Use the six-state enumeration as defined above.\n> - **Rationale:** The additional states provide valuable tracking for parallel execution (`RUNNING`), test discovery (`PENDING`), and intentional test exclusion (`SKIPPED`). This granularity enables richer reporting (e.g., showing currently executing tests) and better integration with CI systems that distinguish between skipped and pending tests. The `ERRORED`/`FAILED` distinction is particularly valuable for test maintenance.\n> - **Consequences:** More complex state management throughout the framework, but enables more sophisticated reporting and debugging features. The state machine (shown below) becomes more elaborate but more informative.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Three states | Simple to implement, easy to reason about | Loses valuable diagnostic information; can't distinguish skipped from pending tests | ❌ |\n| Four states | Adds skip capability useful for conditional tests | Still misses execution lifecycle states needed for parallel execution | ❌ |\n| Six states | Full lifecycle tracking, supports parallel execution reporting, clear error/failure distinction | More complex implementation, more states to handle | ✅ |\n\nThe state transitions for a `TestResult` follow a clear lifecycle, which we can visualize as a state machine:\n\n![Test Result State Machine](./diagrams/test-state-machine.svg)\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|-------|------------|---------------|\n| `PENDING` | `test_started` | `RUNNING` | Record start timestamp, begin test execution |\n| `RUNNING` | `assertion_passed` | `RUNNING` | Continue test execution (no state change) |\n| `RUNNING` | `assertion_failed` | `FAILED` | Record failure message and traceback, stop test execution |\n| `RUNNING` | `exception_raised` | `ERRORED` | Record exception and traceback, stop test execution |\n| `RUNNING` | `test_completed` | `PASSED` | Record end timestamp, calculate duration |\n| `PENDING` | `test_skipped` | `SKIPPED` | Record skip reason, never execute test |\n| Any state | `execution_interrupted` | `ERRORED` | Record interruption reason (e.g., keyboard interrupt) |\n\n#### TestCase: The Blueprint of a Test\n\nA `TestCase` represents a single test that can be executed. Think of it as a **recipe card** for a test: it contains all the information needed to prepare and execute the test, but none of the results. Just as a recipe card lists ingredients (fixtures) and instructions (the test function) without tracking whether you burned the dinner, the `TestCase` describes *what* to test, not *how it went*.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `nodeid` | `str` | A unique identifier for the test, following a hierarchical path format (e.g., `\"tests/test_math.py::test_addition\"` or `\"tests/test_math.py::TestCalculator::test_division\"`). This serves as the test's primary key throughout the framework. |\n| `func` | `Callable` | The actual test function or method to be executed. This is a callable Python object that takes zero or more parameters (for fixture injection). |\n| `file_path` | `str` | Absolute filesystem path to the Python file containing this test. Used for module imports and error reporting. |\n| `line_no` | `int` | Line number in the source file where the test function is defined. Crucial for IDE integration and pinpointing test location in failure reports. |\n| `fixtures` | `list[str]` | Names of fixtures required by this test, extracted from the function's parameter names. This drives the dependency injection system. |\n\n**Example Walkthrough:** Consider a test function defined in `/home/user/project/tests/test_math.py` at line 42:\n```python\ndef test_addition(db_connection, mock_logger):\n    result = 2 + 2\n    assert result == 4\n```\nThe corresponding `TestCase` would be:\n- `nodeid`: `\"tests/test_math.py::test_addition\"`\n- `func`: The `test_addition` function object\n- `file_path`: `\"/home/user/project/tests/test_math.py\"`\n- `line_no`: `42`\n- `fixtures`: `[\"db_connection\", \"mock_logger\"]`\n\n> **Design Insight:** The `nodeid` format follows a convention-over-configuration approach inspired by pytest. It's human-readable, parseable, and provides a natural hierarchy (file → class → test). This format enables powerful filtering (e.g., run all tests in `test_math.py` or all tests in `TestCalculator` class) without complex configuration.\n\n#### TestResult: The Outcome of a Test Execution\n\nWhile `TestCase` describes *what* to run, `TestResult` captures *what happened* when it ran. Think of it as the **medical chart** for a test: it records symptoms (failures), diagnoses (error types), treatment duration (execution time), and final outcome. Just as a medical chart follows a patient through their hospital stay, the `TestResult` follows a test through its execution lifecycle.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `test_case` | `TestCase` | Reference to the test case that was executed. This maintains the connection between outcome and blueprint. |\n| `status` | `TestStatus` | The final state of the test after execution (one of `PASSED`, `FAILED`, `ERRORED`, `SKIPPED`). |\n| `message` | `Optional[str]` | Human-readable description of what happened, especially for failures and errors. For assertions, this includes the failure message; for errors, it might include the exception message. |\n| `exception` | `Optional[Exception]` | The actual exception object raised during test execution (if any). Preserved for advanced debugging and programmatic inspection. |\n| `traceback` | `Optional[str]` | Formatted traceback string showing the call stack at the point of failure. Essential for debugging test errors. |\n| `duration` | `float` | Execution time in seconds, measured with high precision (typically using `time.perf_counter()`). |\n\n**Example Scenario:** When `test_addition` from our previous example fails due to `assert result == 5`, the resulting `TestResult` would contain:\n- `test_case`: The `TestCase` object for `test_addition`\n- `status`: `FAILED`\n- `message`: `\"AssertionError: Expected 5 but got 4\"`\n- `exception`: The `AssertionError` instance\n- `traceback`: `\"Traceback (most recent call last):\\n  File \\\"/home/user/project/tests/test_math.py\\\", line 44, in test_addition\\n    assert result == 5\\nAssertionError: Expected 5 but got 4\"`\n- `duration`: `0.000142` (seconds)\n\n> **Architecture Decision Record: Storing Exceptions vs. Serializing Them**\n> \n> **Decision: Store both the exception object and a formatted traceback string**\n> - **Context:** When tests fail or error, we need to capture diagnostic information for reporting and debugging. Exceptions in Python contain valuable information but can be tricky to serialize/store long-term.\n> - **Options Considered:**\n>   1. **Store only exception object:** Keep the live exception instance for programmatic access\n>   2. **Store only serialized form:** Convert to string immediately for simplicity\n>   3. **Store both:** Keep the exception object and a pre-formatted traceback string\n> - **Decision:** Store both the exception object (`exception`) and formatted traceback string (`traceback`).\n> - **Rationale:** The exception object is valuable for advanced users who might want to inspect exception attributes programmatically (e.g., custom assertion libraries). The pre-formatted traceback string ensures we have a human-readable representation even if the exception object becomes problematic to serialize (e.g., for parallel execution where objects might need to be pickled). This dual approach provides maximum utility with reasonable complexity.\n> - **Consequences:** Slightly more memory usage per test result, but enables both programmatic inspection and reliable reporting.\n\n#### Fixture: Reusable Test Resources\n\nFixtures are the **stage crew** of the test framework. While tests are the actors performing the main show, fixtures work behind the scenes to set up the stage (setup), provide props (test data), and clean up afterward (teardown). A `Fixture` object describes one such crew member—what resources it provides, how to create them, and who depends on it.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `name` | `str` | The unique name by which this fixture is referenced in test parameter lists. This serves as the key for dependency resolution. |\n| `func` | `Callable` | The fixture function that, when called, returns the fixture value. This may include setup and teardown logic (especially if implemented as a generator). |\n| `scope` | `str` | Determines the lifecycle and caching of the fixture. One of: `\"function\"`, `\"class\"`, `\"module\"`, `\"session\"`. |\n| `dependencies` | `list[str]` | Names of other fixtures that this fixture depends on, extracted from its parameter names. Forms a directed acyclic graph (DAG) of dependencies. |\n\n**Fixture Scope Behavior:**\n\n| Scope | Creation Time | Teardown Time | Shared Across |\n|-------|---------------|---------------|---------------|\n| `\"function\"` | Before each test that uses it | After each test that uses it | No tests (fresh per test) |\n| `\"class\"` | Before first test in a class | After last test in a class | All tests in the same test class |\n| `\"module\"` | Before first test in a module | After last test in a module | All tests in the same module |\n| `\"session\"` | Once at test session start | After all tests complete | All tests in the entire test run |\n\n> **Mental Model:** Think of fixture scopes as **caching policies with cleanup**. A function-scoped fixture is like disposable cutlery—used once and thrown away. A session-scoped fixture is like the venue itself—set up once for the entire performance, then cleaned up after everyone leaves. The scope determines the trade-off between isolation (safety) and performance (speed).\n\n**Example:** A database connection fixture might be defined as:\n```python\ndef db_connection():\n    conn = create_expensive_database_connection()\n    yield conn\n    conn.close()\n```\nThe corresponding `Fixture` object would have:\n- `name`: `\"db_connection\"`\n- `func`: The `db_connection` generator function\n- `scope`: `\"function\"` (default if not specified)\n- `dependencies`: `[]` (no parameters, so no dependencies)\n\nIf it depended on a configuration fixture:\n```python\ndef db_connection(config):\n    conn = create_connection(config.database_url)\n    yield conn\n    conn.close()\n```\nThen `dependencies` would be `[\"config\"]`.\n\n> **Architecture Decision Record: Generator-Based vs. Context Manager Fixtures**\n> \n> **Decision: Use generator functions for fixture lifecycle management**\n> - **Context:** Fixtures need to support both setup and teardown logic. We need a clean way to specify \"do this before the test, provide this value during the test, do this after the test.\"\n> - **Options Considered:**\n>   1. **Separate setup/teardown functions:** Explicit `setup_fixture()` and `teardown_fixture()` functions\n>   2. **Context managers:** Use `@contextmanager` decorator and `with` statement\n>   3. **Generator functions:** Use `yield` to separate setup from teardown (pytest's approach)\n> - **Decision:** Use generator functions where `yield` returns the fixture value.\n> - **Rationale:** Generator functions provide a natural, linear flow for setup/teardown that's easy for users to understand. The code before `yield` is setup, the value after `yield` is the fixture value, and code after `yield` is teardown. This pattern also handles exceptions gracefully—if a test fails, the code after `yield` still executes (like a `finally` block). It's a proven pattern adopted by pytest that balances simplicity with power.\n> - **Consequences:** Users must understand generator semantics. The framework must carefully handle generator lifecycle (calling `next()` to get the value, then ensuring the generator completes even if tests fail).\n\n### Supporting Structures\n\nWhile core types represent the fundamental testing concepts, supporting structures provide the organizational scaffolding that orchestrates their interactions. These are the **containers, configuration, and coordination mechanisms** that allow the core types to work together effectively.\n\n#### TestSuite: Organizing Tests for Execution\n\nA `TestSuite` is a **playlist of tests**—it groups related tests together for organized execution, much like a music playlist groups songs by mood or genre. While individual tests can be executed independently, suites provide logical grouping for reporting, filtering, and parallel execution strategies.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `name` | `str` | Descriptive name for the suite, typically derived from the discovery path or pattern (e.g., `\"tests/\"` or `\"math tests\"`). |\n| `tests` | `list[TestCase]` | Ordered collection of test cases to be executed as part of this suite. Order may reflect discovery order or be shuffled for certain testing strategies. |\n\n> **Design Insight:** The `TestSuite` serves as the primary data structure passed from the Discoverer to the Runner. Its simplicity (just a name and list) belies its importance as the **handoff point** between discovery and execution phases. By keeping it minimal, we allow different discovery strategies (e.g., pattern-based, tag-based) to all produce the same structure for the runner to consume.\n\n**Suite Organization Strategies:**\n\n| Strategy | How Tests Are Grouped | When to Use |\n|----------|----------------------|-------------|\n| **By module** | All tests from one Python file in one suite | Simple projects, module-level parallelism |\n| **By directory** | All tests from a directory tree in one suite | Large projects with hierarchical organization |\n| **By test type** | Unit tests, integration tests in separate suites | Mixed test types with different resource requirements |\n| **Dynamic grouping** | Tests grouped by estimated execution time | Optimizing parallel execution balance |\n\n#### Configuration: Framework Behavior as Data\n\nThe `Configuration` object is the **control panel** for the test framework—every knob, switch, and dial that affects framework behavior is captured here. Think of it as a recipe that specifies not what to test, but *how* to test: which tests to run, how to run them, and what to do with the results.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `file_patterns` | `list[str]` | Glob patterns for test file discovery (e.g., `[\"test_*.py\", \"*_test.py\"]`). Defaults to framework conventions. |\n| `start_dir` | `Path` | Directory to start discovery from. Typically the current working directory or a user-specified path. |\n| `test_name_filters` | `list[str]` | Substring or regex patterns to filter test names (e.g., `[\"test_addition\", \".*math.*\"]`). |\n| `verbose` | `bool` | Enable detailed output, including test names as they run and additional diagnostic information. |\n| `quiet` | `bool` | Minimize output, showing only final summary or errors. Mutually exclusive with `verbose`. |\n| `output_format` | `str` | Format for test results: `\"terminal\"` for human-readable, `\"junit-xml\"` for CI integration. |\n| `parallel` | `bool` | Whether to run tests in parallel. Enables concurrent execution for speed. |\n| `max_workers` | `Optional[int]` | Maximum number of concurrent worker processes/threads for parallel execution. `None` means use CPU count. |\n| `show_durations` | `bool` | Whether to report individual test execution times in the summary. |\n| `junit_xml_path` | `Optional[Path]` | Filesystem path where JUnit XML report should be written (if `output_format` includes XML). |\n\n> **Architecture Decision Record: Configuration as Immutable Data Structure**\n> \n> **Decision: Treat Configuration as an immutable data class after parsing**\n> - **Context:** Configuration flows through multiple components, and we need to ensure consistency—no component should modify configuration in ways that affect other components unexpectedly.\n> - **Options Considered:**\n>   1. **Mutable dictionary/object:** Allow components to modify configuration as needed\n>   2. **Immutable data class:** Freeze configuration after CLI parsing\n>   3. **Hierarchical configuration:** Separate global, project, and runtime configuration layers\n> - **Decision:** Use an immutable data class (Python's `dataclass` with `frozen=True`).\n> - **Rationale:** Immutability prevents subtle bugs where one component's configuration changes affect another component unexpectedly. It also makes the configuration easier to reason about—you can pass it anywhere without worrying about side effects. The hierarchical configuration approach is overkill for our educational framework.\n> - **Consequences:** Components that need to extend configuration (e.g., with derived values) must create new configuration objects. This is slightly more verbose but leads to more predictable behavior.\n\n#### Supporting Types for Fixture System\n\nThe fixture system requires additional structures to manage its complexity:\n\n**FixtureScope Enum:**\n| Value | Description |\n|-------|-------------|\n| `FUNCTION` | Fixture is created and torn down for each test function (default). |\n| `CLASS` | Fixture persists for all tests in a single test class. |\n| `MODULE` | Fixture persists for all tests in a single module. |\n| `SESSION` | Fixture persists for the entire test session. |\n\n**FixtureRequest: Context for Fixture Execution**\n| Field | Type | Description |\n|-------|------|-------------|\n| `fixture_name` | `str` | Name of the fixture being requested. |\n| `scope` | `FixtureScope` | Scope at which this fixture is being instantiated. |\n| `test_case` | `Optional[TestCase]` | The test case that triggered this request (if any). |\n| `cache_key` | `tuple` | A hashable key used for caching fixtures (combines fixture name, scope, and scope-specific identifiers). |\n\n> **Mental Model:** Think of `FixtureRequest` as a **work order** for the fixture factory. When a test needs a fixture, it doesn't call the fixture function directly—instead, it creates a work order describing what's needed, and the fixture system processes it according to the fixture's scope and dependencies, possibly reusing cached results from previous work orders.\n\n#### Relationship Diagram\n\nThe relationships between these data structures form the backbone of the framework's information architecture:\n\n![Data Model Relationships](./diagrams/data-model-diag.svg)\n\nKey relationships depicted:\n1. **Aggregation:** `TestSuite` contains multiple `TestCase` objects\n2. **Production:** Running a `TestCase` produces a `TestResult`\n3. **Dependency:** `TestCase` depends on `Fixture` objects (through the `fixtures` list)\n4. **Composition:** `TestResult` contains a reference to its `TestCase`\n5. **Dependency Graph:** `Fixture` objects can depend on other `Fixture` objects\n\n#### Data Flow Through the Pipeline\n\nTo understand how these data structures interact, let's trace a test through the framework:\n\n1. **Discovery Phase:** \n   - Input: `Configuration` (specifying `start_dir` and `file_patterns`)\n   - Process: `Discoverer` scans filesystem, imports modules, inspects functions\n   - Output: `TestSuite` containing `TestCase` objects\n   - Data transformation: `Path` → `Module` → `Callable` → `TestCase` → `TestSuite`\n\n2. **Fixture Resolution Phase:**\n   - Input: `TestCase` (with `fixtures` list) + `Fixture` registry\n   - Process: Resolve dependencies, check scopes, create caching keys\n   - Output: `FixtureRequest` objects for each required fixture\n   - Data transformation: `list[str]` (fixture names) → DAG of `FixtureRequest` objects\n\n3. **Execution Phase:**\n   - Input: `TestCase` + resolved fixture values\n   - Process: Execute test function with injected fixtures\n   - Output: `TestResult` with status, duration, and error info\n   - Data transformation: `TestCase` + fixture values → execution → `TestResult`\n\n4. **Reporting Phase:**\n   - Input: Collection of `TestResult` objects + `Configuration` (for output format)\n   - Process: Format results, calculate statistics, write output\n   - Output: Human-readable report or JUnit XML\n   - Data transformation: `list[TestResult]` → aggregated statistics → formatted output\n\n### Common Pitfalls\n\n#### ⚠️ **Pitfall: Mutable default arguments in data classes**\n- **Description:** Using mutable default values like `fixtures=[]` in data class definitions.\n- **Why it's wrong:** In Python, default argument values are evaluated once at function/class definition time, not each time an instance is created. All instances would share the same list object, leading to cross-test contamination.\n- **Fix:** Use `field(default_factory=list)` from the `dataclasses` module, which creates a new list for each instance.\n\n#### ⚠️ **Pitfall: Forgetting to handle generator completion in fixtures**\n- **Description:** When a fixture uses `yield`, the framework might not properly call `generator.close()` if a test fails, leaving resources dangling.\n- **Why it's wrong:** The teardown code (after `yield`) won't execute, causing resource leaks (unclosed files, database connections, etc.).\n- **Fix:** Always wrap fixture execution in a `try/finally` block that ensures the generator is exhausted even on exceptions.\n\n#### ⚠️ **Pitfall: Using floats for duration comparison**\n- **Description:** Comparing execution durations directly with `==` or threshold checks using raw floats.\n- **Why it's wrong:** Floating-point arithmetic has precision limitations; two durations that should be equal might differ by tiny fractions due to rounding errors.\n- **Fix:** Use `math.isclose()` for comparisons or store durations as `decimal.Decimal` if precise arithmetic is needed.\n\n#### ⚠️ **Pitfall: Storing absolute paths in TestCase**\n- **Description:** Storing absolute filesystem paths in `TestCase.file_path`.\n- **Why it's wrong:** Tests become non-portable—the same test suite would have different `nodeid` values on different machines or CI environments.\n- **Fix:** Store paths relative to a project root directory, or make them absolute but be aware of the portability trade-off. For our framework, we'll store absolute paths for simplicity but provide helpers to make them relative in reports.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Data Classes | Python's built-in `dataclasses` module | `attrs` library for more features and performance |\n| Enum Implementation | `enum.Enum` from standard library | `enum.IntEnum` if integer values needed for serialization |\n| Path Handling | `pathlib.Path` from standard library | `pathlib` with custom normalization for cross-platform consistency |\n| Type Hints | Basic type hints (`List[str]`, `Optional[int]`) | Full mypy-compatible hints with generics and protocols |\n| Serialization | Manual dict conversion for JSON/XML | `pydantic` for validation and serialization |\n\n#### B. Recommended File/Module Structure\n\n```\napollo/\n  ├── __init__.py              # Public API exports\n  ├── core/                    # Core data types and interfaces\n  │   ├── __init__.py\n  │   ├── types.py            # TestStatus, TestCase, TestResult, etc.\n  │   ├── fixtures.py         # Fixture, FixtureRequest, FixtureScope\n  │   └── configuration.py    # Configuration dataclass\n  ├── discovery/              # Test discovery components\n  ├── runner/                 # Test execution components  \n  ├── assertions/             # Assertion engine\n  ├── reporting/              # Reporting components\n  └── cli/                    # CLI interface\n```\n\n#### C. Infrastructure Starter Code\n\n**core/types.py - Foundation Data Types:**\n\n```python\n\"\"\"Core data types for the test framework.\"\"\"\nfrom __future__ import annotations\nimport time\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable, Optional\n\n\nclass TestStatus(Enum):\n    \"\"\"Possible states of a test during execution.\"\"\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    PASSED = \"passed\"\n    FAILED = \"failed\"\n    ERRORED = \"errored\"\n    SKIPPED = \"skipped\"\n\n\n@dataclass(frozen=True)\nclass TestCase:\n    \"\"\"Represents a single test that can be executed.\"\"\"\n    nodeid: str\n    func: Callable[..., Any]\n    file_path: str\n    line_no: int\n    fixtures: list[str] = field(default_factory=list)\n    \n    def __str__(self) -> str:\n        return self.nodeid\n\n\n@dataclass\nclass TestResult:\n    \"\"\"Captures the outcome of executing a TestCase.\"\"\"\n    test_case: TestCase\n    status: TestStatus = TestStatus.PENDING\n    message: Optional[str] = None\n    exception: Optional[Exception] = None\n    traceback: Optional[str] = None\n    duration: float = 0.0\n    _start_time: Optional[float] = field(default=None, init=False, repr=False)\n    \n    def start_timer(self) -> None:\n        \"\"\"Start timing the test execution.\"\"\"\n        self._start_time = time.perf_counter()\n    \n    def stop_timer(self) -> None:\n        \"\"\"Stop timing and calculate duration.\"\"\"\n        if self._start_time is not None:\n            self.duration = time.perf_counter() - self._start_time\n```\n\n**core/fixtures.py - Fixture Foundation:**\n\n```python\n\"\"\"Fixture-related data types.\"\"\"\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable, Optional\n\n\nclass FixtureScope(Enum):\n    \"\"\"Scope determining fixture lifecycle and caching.\"\"\"\n    FUNCTION = \"function\"\n    CLASS = \"class\"\n    MODULE = \"module\"\n    SESSION = \"session\"\n\n\n@dataclass(frozen=True)\nclass Fixture:\n    \"\"\"Represents a reusable test resource with setup/teardown.\"\"\"\n    name: str\n    func: Callable[..., Any]\n    scope: FixtureScope = FixtureScope.FUNCTION\n    dependencies: list[str] = field(default_factory=list)\n    \n    def __str__(self) -> str:\n        return f\"Fixture(name={self.name}, scope={self.scope.value})\"\n\n\n@dataclass\nclass FixtureRequest:\n    \"\"\"Context for creating or retrieving a fixture.\"\"\"\n    fixture_name: str\n    scope: FixtureScope\n    test_case: Optional[TestCase] = None\n    cache_key: tuple = field(default_factory=tuple)\n    \n    @property\n    def is_cached(self) -> bool:\n        \"\"\"Whether this fixture should be cached based on scope.\"\"\"\n        return self.scope != FixtureScope.FUNCTION\n```\n\n**core/configuration.py - CLI Configuration:**\n\n```python\n\"\"\"Configuration data structure parsed from CLI arguments.\"\"\"\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Optional\n\n\n@dataclass(frozen=True)\nclass Configuration:\n    \"\"\"Immutable configuration for test framework behavior.\"\"\"\n    file_patterns: list[str] = field(default_factory=lambda: [\"test_*.py\", \"*_test.py\"])\n    start_dir: Path = field(default_factory=lambda: Path.cwd())\n    test_name_filters: list[str] = field(default_factory=list)\n    verbose: bool = False\n    quiet: bool = False\n    output_format: str = \"terminal\"\n    parallel: bool = False\n    max_workers: Optional[int] = None\n    show_durations: bool = False\n    junit_xml_path: Optional[Path] = None\n    \n    def __post_init__(self) -> None:\n        \"\"\"Validate configuration after initialization.\"\"\"\n        if self.verbose and self.quiet:\n            raise ValueError(\"Cannot set both verbose and quiet modes\")\n        if self.output_format not in [\"terminal\", \"junit-xml\"]:\n            raise ValueError(f\"Invalid output format: {self.output_format}\")\n```\n\n#### D. Core Logic Skeleton Code\n\n**core/types.py - TestResult state management:**\n\n```python\nclass TestResult:\n    # ... (fields and timer methods from above)\n    \n    def mark_passed(self) -> None:\n        \"\"\"Mark the test as passed successfully.\"\"\"\n        self.stop_timer()\n        self.status = TestStatus.PASSED\n        \n    def mark_failed(self, message: str, exception: Optional[Exception] = None) -> None:\n        \"\"\"\n        Mark the test as failed due to assertion failure.\n        \n        Args:\n            message: Human-readable failure description\n            exception: The AssertionError that was raised\n        \"\"\"\n        self.stop_timer()\n        self.status = TestStatus.FAILED\n        self.message = message\n        self.exception = exception\n        # TODO: Capture traceback from exception if available\n        \n    def mark_errored(self, message: str, exception: Optional[Exception] = None) -> None:\n        \"\"\"\n        Mark the test as errored due to unexpected exception.\n        \n        Args:\n            message: Human-readable error description\n            exception: The unexpected exception that was raised\n        \"\"\"\n        self.stop_timer()\n        self.status = TestStatus.ERRORED\n        self.message = message\n        self.exception = exception\n        # TODO: Capture full traceback for debugging\n        \n    def mark_skipped(self, reason: str) -> None:\n        \"\"\"Mark the test as skipped with a reason.\"\"\"\n        self.status = TestStatus.SKIPPED\n        self.message = reason\n        \n    def is_successful(self) -> bool:\n        \"\"\"Return True if test passed or was skipped.\"\"\"\n        return self.status in (TestStatus.PASSED, TestStatus.SKIPPED)\n```\n\n#### E. Language-Specific Hints\n\n1. **Use `@dataclass(frozen=True)` for Configuration:** This makes instances immutable, preventing accidental modification after parsing.\n\n2. **Leverage `enum.auto()` for TestStatus:** If you don't care about the string values, use `auto()` to let Python assign values automatically.\n\n3. **Import `annotations` from future:** This allows forward references in type hints (e.g., `'TestCase'` as string) so you can reference types before they're defined.\n\n4. **Use `field(default_factory=list)` not `field(default=[])`:** This ensures each instance gets a new list, not a shared reference.\n\n5. **Consider `__slots__` for performance:** If you create thousands of TestResult objects, adding `__slots__` to the dataclass can reduce memory usage.\n\n#### F. Milestone Checkpoint\n\n**After implementing the data model (prerequisite for all milestones):**\n\n1. **Verification Test:** Create a simple test script that exercises each data type:\n   ```python\n   # test_data_model.py\n   from apollo.core.types import TestStatus, TestCase, TestResult\n   from apollo.core.fixtures import Fixture, FixtureScope\n   from apollo.core.configuration import Configuration\n   \n   def test_example():\n       pass\n   \n   # Create instances\n   tc = TestCase(\n       nodeid=\"test_example\",\n       func=test_example,\n       file_path=__file__,\n       line_no=10\n   )\n   \n   result = TestResult(test_case=tc)\n   result.start_timer()\n   # Simulate test execution\n   result.mark_passed()\n   \n   assert result.status == TestStatus.PASSED\n   assert result.duration > 0\n   \n   config = Configuration(verbose=True)\n   assert config.verbose is True\n   ```\n\n2. **Expected Behavior:** The script should run without errors and demonstrate that all data types can be instantiated correctly.\n\n3. **Signs of Problems:** \n   - Import errors: Check that your `apollo/` directory has an `__init__.py` and the module structure matches.\n   - Dataclass issues: Ensure you're using Python 3.7+ or have the dataclasses backport installed.\n   - Circular imports: Use string type hints or import inside methods if types reference each other.\n\n---\n\n\n## 5. Component Design: Discovery & Execution (Milestone 1)\n\n> **Milestone(s):** Milestone 1 (Test Discovery & Execution)\n\nThe **Discovery & Execution** subsystem forms the beating heart of the test framework — it's responsible for finding all the tests in your codebase and running them in a controlled, predictable manner. Think of this subsystem as a **concert production team** that first scouts for musicians (discovery), then ensures each performer gets their own soundproof practice room to avoid interfering with others (isolation), and finally schedules multiple rehearsals simultaneously to save time (parallel execution). This component transforms raw Python functions into executable test cases and orchestrates their execution while maintaining the fundamental guarantee that one test's failures don't cascade to others.\n\n### The Discoverer\n\n**Mental Model: The Talent Scout**  \nImagine you're organizing a massive music festival with thousands of potential performers scattered across a city. You need a methodical talent scout who can:\n1. Systematically search through every building (directory) \n2. Identify individuals who look like musicians (functions matching naming conventions)\n3. Create a registry with their name, location, and special requirements\n4. Ignore people who aren't performers (non-test code)\n\nThe Discoverer serves exactly this role in our test framework. It's the component that scans your codebase, identifies test functions based on naming conventions, and creates a structured inventory (a `TestSuite`) of all tests to be executed.\n\n#### Architecture Decision: Convention Over Configuration for Test Discovery\n\n> **Decision: Use Naming Convention-Based Discovery**\n> - **Context**: We need to automatically find test functions without requiring users to manually register each test. Test frameworks must balance explicitness (knowing exactly what will run) with convenience (not having to maintain a registry).\n> - **Options Considered**:\n>   1. **Explicit Registration**: Users manually list tests in a configuration file or decorator registry\n>   2. **File-System Scanning**: Run all `.py` files in a test directory regardless of content\n>   3. **Naming Convention**: Find functions/classes whose names start with `test_` or contain `Test` prefixes\n> - **Decision**: Use naming convention-based discovery (option 3) with configurable patterns\n> - **Rationale**: This approach follows the principle of \"convention over configuration\" that has made frameworks like pytest successful. It requires zero boilerplate from users while providing predictable behavior. The explicit registration approach adds maintenance overhead, while file-system scanning could run non-test code accidentally.\n> - **Consequences**: Tests are automatically discovered based on their names, making the framework intuitive for new users. However, this means refactoring that changes function names could accidentally exclude tests from the test suite.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Explicit Registration | Clear visibility of what runs; No false positives | High maintenance; Boilerplate code | ❌ |\n| File-System Scanning | Simple implementation; No naming rules | Runs non-test code; Dangerous | ❌ |\n| Naming Convention | Zero configuration; Predictable patterns; Industry standard | Refactoring risks; Limited expressiveness | ✅ |\n\n#### Discoverer Components and Data Flow\n\nThe Discoverer consists of three logical subcomponents that work together in a pipeline:\n\n1. **Path Resolver**: Converts file patterns (like `tests/unit/*.py`) to concrete file system paths\n2. **Module Loader**: Safely imports Python modules from file paths without side effects\n3. **Test Extractor**: Inspects module contents and yields `TestCase` objects for test functions\n\nThe following table details the complete interface for the Discoverer component:\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `discover_tests` | `start_path: Path, pattern: str = \"test_*.py\"` | `List[TestCase]` | Main entry point: discovers all tests under `start_path` matching the pattern |\n| `resolve_patterns_to_paths` | `patterns: List[str], base_dir: Path` | `List[Path]` | Converts glob patterns to absolute file paths for processing |\n| `module_path_to_name` | `file_path: Path, root_dir: Path` | `str` | Converts a filesystem path to a Python importable module name |\n| `_import_module_from_file` | `file_path: Path` | `module` | Internal: imports a module from a file path safely |\n| `_find_tests_in_module` | `module: module` | `Iterator[TestCase]` | Internal: yields TestCase objects for test functions in the module |\n\n#### The Discovery Algorithm\n\nThe discovery process follows a deterministic, depth-first traversal algorithm:\n\n1. **Pattern Resolution Phase**:\n   1. Receive one or more file patterns (e.g., `[\"tests/**/*.py\", \"test_*.py\"]`)\n   2. For each pattern, use `resolve_patterns_to_paths()` to expand globs to absolute file paths\n   3. Filter to only `.py` files and remove duplicates\n\n2. **Module Import Phase**:\n   1. For each `.py` file path, compute the importable module name using `module_path_to_name()`\n   2. Use `_import_module_from_file()` to import the module without polluting `sys.modules`\n   3. Handle import errors gracefully by recording them as discovery failures\n\n3. **Test Extraction Phase**:\n   1. Call `_find_tests_in_module()` on the successfully imported module\n   2. Inspect all members of the module using Python's `inspect` module\n   3. For each member, apply the **Test Identification Rules**:\n      - **Function Rule**: If it's a callable function whose name starts with `test_`\n      - **Class Rule**: If it's a class whose name starts with `Test`, inspect its methods for those starting with `test_`\n      - **Method Rule**: Methods inside test classes whose names start with `test_`\n   4. For each identified test, create a `TestCase` object with:\n      - `nodeid`: A unique identifier like `\"tests/unit/test_math.py::test_addition\"`\n      - `func`: The actual callable test function or method\n      - `file_path`: Absolute path to the source file\n      - `line_no`: Line number where the test is defined (using `inspect.getsourcelines()`)\n      - `fixtures`: Initially empty list (will be populated during fixture discovery)\n\n4. **Aggregation Phase**:\n   1. Collect all `TestCase` objects into a `TestSuite`\n   2. Apply any test name filters provided in the `Configuration`\n   3. Return the final `TestSuite` to the Runner\n\n#### TestCase Data Structure Details\n\nEach discovered test is represented as a `TestCase` object with the following complete specification:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `nodeid` | `str` | Unique identifier using the format `\"path/to/module.py::test_function\"` or `\"path/to/module.py::TestClass::test_method\"`. This serves as the test's primary key throughout the system. |\n| `func` | `Callable` | The actual Python function or method to execute. This is a reference to the test implementation that will be called by the Runner. |\n| `file_path` | `str` | Absolute filesystem path to the Python file containing the test. Used for error reporting and source linking. |\n| `line_no` | `int` | Line number within `file_path` where the test function is defined. Enables IDEs to jump directly to test failures. |\n| `fixtures` | `List[str]` | Names of fixtures required by this test. Initially empty during discovery; populated later by analyzing function parameters against the fixture registry. |\n\n#### Concrete Walk-Through Example\n\nLet's trace through discovery for a simple project structure:\n```\nproject/\n├── src/\n│   └── calculator.py\n└── tests/\n    ├── unit/\n    │   ├── test_math.py\n    │   └── test_calculator.py\n    └── integration/\n        └── test_integration.py\n```\n\nWhen the user runs `apollo tests/unit/*.py`, here's what happens:\n\n1. **Pattern Resolution**: `resolve_patterns_to_paths()` expands `tests/unit/*.py` to:\n   - `/absolute/path/project/tests/unit/test_math.py`\n   - `/absolute/path/project/tests/unit/test_calculator.py`\n\n2. **Module Import**: Each file is imported:\n   - `test_math.py` contains functions: `test_addition()`, `test_subtraction()`, `helper_function()`\n   - `test_calculator.py` contains class: `TestCalculator` with methods: `test_add()`, `test_multiply()`\n\n3. **Test Extraction**:\n   - From `test_math.py`: Creates `TestCase` for `test_addition` and `test_subtraction` (skips `helper_function`)\n   - From `test_calculator.py`: Creates `TestCase` for `TestCalculator::test_add` and `TestCalculator::test_multiply`\n\n4. **NodeID Generation**:\n   - `test_addition` → `\"tests/unit/test_math.py::test_addition\"`\n   - `TestCalculator::test_add` → `\"tests/unit/test_calculator.py::TestCalculator::test_add\"`\n\n![Test Discovery Flowchart](./diagrams/discovery-flowchart.svg)\n\n#### Common Pitfalls in Test Discovery\n\n⚠️ **Pitfall: Import Side Effects Polluting Test State**  \n**Description**: Directly importing test modules using standard `import` statements can cause module-level code (like global variable initialization) to execute during discovery, potentially affecting test execution later.  \n**Why It's Wrong**: If a module sets `DATABASE_CONNECTION = create_connection()` at module level, this code runs during discovery, not during test execution. The connection might timeout before tests run, or worse, tests might share state.  \n**How to Fix**: Use `_import_module_from_file()` which imports modules in a controlled way, potentially using `importlib` to load modules without executing top-level code multiple times.\n\n⚠️ **Pitfall: Incorrect Path Handling with Relative Imports**  \n**Description**: When test modules use relative imports (like `from ..src import my_module`), discovery from a different directory can cause `ImportError`.  \n**Why It's Wrong**: The framework imports modules from arbitrary locations but doesn't adjust `sys.path` to match the module's expected import context.  \n**How to Fix**: In `_import_module_from_file()`, temporarily modify `sys.path` to include the parent directory of the module being imported, then restore it after import.\n\n⚠️ **Pitfall: Test Discovery Performance with Large Codebases**  \n**Description**: Recursively scanning thousands of files and importing all of them can make discovery slow.  \n**Why It's Wrong**: Users expect near-instant feedback when running tests; a 10-second discovery phase feels sluggish.  \n**How to Fix**: Implement caching of discovery results (hash file contents to detect changes) and parallelize file system scanning where possible.\n\n### The Runner & Isolation\n\n**Mental Model: The Soundproof Practice Rooms**  \nImagine our music festival where each performer needs to practice without hearing others. We build individual soundproof rooms (test isolation) where:\n- Each room starts with identical basic equipment (clean environment)\n- Performers can bring their own instruments (test fixtures)\n- No sound leaks between rooms (no shared state)\n- Multiple rooms can be used simultaneously (parallel execution)\n- After each performance, the room is thoroughly cleaned (teardown)\n\nThe Runner implements this isolation guarantee while executing tests, ensuring that tests are hermetic units that don't affect one another.\n\n#### Test Execution State Machine\n\nEvery test progresses through a well-defined state machine captured in the `TestResult.status` field. This \"traffic light system\" provides clear visibility into test execution:\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|-------|------------|---------------|\n| `PENDING` | `test_started` | `RUNNING` | Record start timestamp; Initialize fixture context |\n| `RUNNING` | `assertion_passed` | (remain `RUNNING`) | Continue test execution |\n| `RUNNING` | `assertion_failed` | `FAILED` | Capture assertion message; Stop test execution; Begin teardown |\n| `RUNNING` | `exception_raised` | `ERRORED` | Capture exception and traceback; Begin teardown |\n| `RUNNING` | `test_completed` | `PASSED` | Record end timestamp; Begin teardown |\n| `RUNNING` | `test_skipped` | `SKIPPED` | Record skip reason; Begin teardown |\n| `FAILED` | `teardown_complete` | `FAILED` | Finalize duration calculation |\n| `ERRORED` | `teardown_complete` | `ERRORED` | Finalize duration calculation |\n| `PASSED` | `teardown_complete` | `PASSED` | Finalize duration calculation |\n| `SKIPPED` | `teardown_complete` | `SKIPPED` | Finalize duration calculation |\n\nThis state machine ensures that every test follows the same lifecycle regardless of outcome, and that teardown always runs (except in catastrophic failures).\n\n![Test Result State Machine](./diagrams/test-state-machine.svg)\n\n#### Architecture Decision: Process-Based Parallel Execution\n\n> **Decision: Use Multiprocessing for Parallel Test Execution**\n> - **Context**: To reduce total test suite execution time, we want to run independent tests concurrently. Python's Global Interpreter Lock (GIL) limits true parallelism with threads for CPU-bound tests.\n> - **Options Considered**:\n>   1. **Threading**: Use Python's `threading` module for concurrency\n>   2. **Subprocesses**: Launch separate Python processes via `subprocess`\n>   3. **Multiprocessing**: Use `multiprocessing` module with process pools\n> - **Decision**: Use `multiprocessing.Pool` for parallel execution (option 3)\n> - **Rationale**: Multiprocessing bypasses the GIL, providing true parallelism for CPU-bound tests. It offers better isolation than threading (each test gets its own memory space) and is more efficient than subprocesses (avokes Python interpreter startup overhead). The `multiprocessing` module provides high-level abstractions like `Pool.map` that simplify implementation.\n> - **Consequences**: Tests run in parallel by default when `--parallel` is specified. However, process startup has overhead, so very short test suites might run slower. Also, tests that rely on shared resources (database, files) need explicit coordination.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Threading | Low overhead; Shared memory | GIL limits CPU parallelism; State leakage risk | ❌ |\n| Subprocesses | Maximum isolation; No GIL issues | High overhead; Complex communication | ❌ |\n| Multiprocessing | Good isolation; Bypasses GIL; Standard library | Pickling requirements; Memory duplication | ✅ |\n\n#### Runner Components and Interfaces\n\nThe Runner component has two primary public interfaces as defined in the naming conventions:\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `SimpleRunner.run_test` | `test_case: TestCase` | `TestResult` | Executes a single test in isolation, handling setup, execution, and teardown |\n| `SimpleRunner.run_suite` | `suite: TestSuite` | `List[TestResult]` | Runs all tests in a suite, optionally in parallel, and collects results |\n\n#### Test Execution Algorithm\n\nWhen `SimpleRunner.run_test()` is called for a single test, it follows this precise sequence:\n\n1. **Initialization**:\n   1. Create a fresh `TestResult` object with `status=PENDING`, `test_case` set, and start time recorded\n   2. Change status to `RUNNING`\n   3. Create a clean execution namespace (dictionary) for the test\n\n2. **Fixture Setup**:\n   1. Analyze `test_case.fixtures` to determine required fixtures\n   2. For each fixture, retrieve or create it via the Fixture Registry (detailed in Milestone 3)\n   3. Inject fixture values into the execution namespace\n\n3. **Test Execution**:\n   1. Extract the test function from `test_case.func`\n   2. Call the function with arguments extracted from the execution namespace (matching parameter names to fixture names)\n   3. Wrap the call in a try-except block to catch any exceptions\n   4. If the Assertion Engine raises an `AssertionError`, transition to `FAILED` state and capture the error message\n   5. If any other exception occurs, transition to `ERRORED` state and capture the exception with traceback\n   6. If the function returns normally, transition to `PASSED` state\n\n4. **Fixture Teardown**:\n   1. Regardless of test outcome, execute fixture teardown procedures in reverse dependency order\n   2. For function-scoped fixtures, destroy them immediately\n   3. For higher-scoped fixtures, decrement reference counts\n\n5. **Finalization**:\n   1. Record end timestamp and calculate `duration`\n   2. Ensure `TestResult` has appropriate fields set based on outcome\n   3. Return the completed `TestResult`\n\n#### Parallel Execution Implementation\n\nFor `SimpleRunner.run_suite()` with parallel execution enabled, the algorithm is:\n\n1. **Partitioning Phase**:\n   1. Analyze the test suite to identify independent tests (no shared fixture dependencies at module or session scope)\n   2. Create partitions of tests that can run concurrently\n   3. For tests with shared state dependencies, group them to run sequentially\n\n2. **Worker Preparation**:\n   1. Create a `multiprocessing.Pool` with `max_workers` from configuration (defaults to CPU count)\n   2. Serialize test cases (must be picklable) along with necessary fixture definitions\n   3. Define a worker function that can run a single test in isolation\n\n3. **Execution Phase**:\n   1. Use `Pool.map()` or `Pool.imap_unordered()` to distribute tests across workers\n   2. Each worker:\n      - Re-imports necessary modules (fresh interpreter state)\n      - Recreates fixture context from serialized data\n      - Runs the test using the same algorithm as `run_test()`\n      - Serializes the `TestResult` and returns it\n\n4. **Aggregation Phase**:\n   1. Collect all `TestResult` objects from workers\n   2. Merge them maintaining original test order if required\n   3. Handle any worker failures (mark corresponding tests as `ERRORED`)\n\n#### TestResult Data Structure Details\n\nThe `TestResult` object serves as the \"medical chart\" for each test execution, containing complete diagnostic information:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `test_case` | `TestCase` | Reference to the test that was executed. This provides context for the result. |\n| `status` | `TestStatus` | One of: `PENDING`, `RUNNING`, `PASSED`, `FAILED`, `ERRORED`, `SKIPPED`. The definitive outcome of the test. |\n| `message` | `Optional[str]` | Human-readable message describing the failure or skip reason. For assertions, contains the formatted diff. |\n| `exception` | `Optional[Exception]` | The actual exception object if the test errored (not for assertion failures). |\n| `traceback` | `Optional[str]` | Formatted traceback string for debugging errors. Captures the full call stack. |\n| `duration` | `float` | Execution time in seconds with millisecond precision. Measured from just before fixture setup to after teardown. |\n\n#### Concrete Walk-Through: Parallel Execution Scenario\n\nConsider a test suite with 4 independent tests (A, B, C, D) and 2 workers:\n\n1. **Sequential Execution** (without parallel):\n   ```\n   Time: 0s - Start Test A (takes 3s)\n   Time: 3s - Start Test B (takes 2s)  \n   Time: 5s - Start Test C (takes 4s)\n   Time: 9s - Start Test D (takes 1s)\n   Time: 10s - All tests complete\n   ```\n\n2. **Parallel Execution** (with 2 workers):\n   ```\n   Worker 1: Time 0-3s: Test A\n   Worker 2: Time 0-2s: Test B\n   Worker 1: Time 3-7s: Test C  (after A finishes)\n   Worker 2: Time 2-3s: Test D  (after B finishes)\n   Time: 7s - All tests complete (30% faster!)\n   ```\n\nThe speedup isn't perfectly linear due to:\n- Process creation overhead\n- Tests with different durations\n- Potential resource contention\n\n![Test Execution Sequence](./diagrams/test-execution-seq.svg)\n\n#### Common Pitfalls in Test Execution\n\n⚠️ **Pitfall: Incomplete Isolation Between Tests**  \n**Description**: Tests accidentally share state through module-level variables, class attributes, or mutable default arguments.  \n**Why It's Wrong**: Test A sets `config.DEBUG = True`, then Test B sees this changed value and behaves differently. This leads to flaky tests that pass or fail depending on execution order.  \n**How to Fix**: The Runner must ensure each test runs in a truly clean environment. For parallel execution, use separate processes. For sequential, carefully reset module state between tests.\n\n⚠️ **Pitfall: Fixture Teardown Skipped on Test Failure**  \n**Description**: If a test fails with an exception, and the teardown code isn't in a `finally` block, resources (database connections, temporary files) might leak.  \n**Why It's Wrong**: Resource leaks accumulate across test runs, eventually causing \"too many open files\" errors or database connection limits.  \n**How to Fix**: Structure the execution algorithm so fixture teardown runs in a `finally` block after test execution, regardless of test outcome.\n\n⚠️ **Pitfall: Deadlocks in Parallel Execution**  \n**Description**: Tests that use shared resources (like a test database) might deadlock when run concurrently.  \n**Why It's Wrong**: Two tests both try to acquire locks on the same database table in different orders, causing them to wait forever for each other.  \n**How to Fix**: Provide clear documentation about parallel-safe test writing. Optionally, detect tests that use known shared resources and run them sequentially.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Module Import | `importlib.util.spec_from_file_location()` | AST parsing without execution for faster discovery |\n| Parallel Execution | `multiprocessing.Pool` with `pickle` | `concurrent.futures.ProcessPoolExecutor` with custom serialization |\n| Test Isolation | Fresh import per test process | Module reloading with `importlib.reload()` |\n| File Pattern Matching | `glob.glob()` with recursion | `pathlib.rglob()` with caching |\n\n#### B. Recommended File/Module Structure\n\n```\napollo/\n├── __init__.py\n├── __main__.py              # CLI entry point\n├── cli.py                   # CLI parsing (Milestone 4)\n├── discovery/\n│   ├── __init__.py\n│   ├── discoverer.py        # Main discovery logic\n│   └── path_resolver.py     # Pattern → path conversion\n├── runner/\n│   ├── __init__.py\n│   ├── base_runner.py       # Abstract runner interface\n│   ├── simple_runner.py     # Sequential runner implementation\n│   └── parallel_runner.py   # Parallel runner implementation\n├── models/\n│   ├── __init__.py\n│   ├── test_case.py         # TestCase dataclass\n│   ├── test_result.py       # TestResult dataclass\n│   └── test_suite.py        # TestSuite container\n└── utils/\n    ├── __init__.py\n    ├── import_utils.py      # Safe module import\n    └── introspection.py     # Function/class inspection helpers\n```\n\n#### C. Infrastructure Starter Code\n\n**File: `apollo/utils/import_utils.py`**\n```python\nimport importlib.util\nimport sys\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import Optional\n\n\ndef import_module_from_file(file_path: Path) -> Optional[ModuleType]:\n    \"\"\"\n    Safely import a Python module from a filesystem path.\n    \n    This function imports the module without adding it to sys.modules\n    under its normal name, preventing import side effects from polluting\n    the discovery process.\n    \n    Args:\n        file_path: Absolute path to the Python file to import\n        \n    Returns:\n        The imported module object, or None if import failed\n    \"\"\"\n    # Generate a unique module name to avoid collisions\n    module_name = f\"__apollo_discovery_{file_path.stem}_{hash(file_path)}\"\n    \n    # Remove any existing module with this name to ensure freshness\n    if module_name in sys.modules:\n        del sys.modules[module_name]\n    \n    try:\n        # Create module spec from file location\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        if spec is None or spec.loader is None:\n            return None\n        \n        # Create and execute the module\n        module = importlib.util.module_from_spec(spec)\n        \n        # Temporarily add parent directory to sys.path for relative imports\n        parent_dir = str(file_path.parent)\n        original_sys_path = sys.path.copy()\n        if parent_dir not in sys.path:\n            sys.path.insert(0, parent_dir)\n        \n        try:\n            spec.loader.exec_module(module)\n        finally:\n            # Restore original sys.path\n            sys.path = original_sys_path\n        \n        return module\n    except Exception:\n        # Import failed - could be syntax error, missing dependencies, etc.\n        # Clean up to avoid leaving half-loaded modules\n        if module_name in sys.modules:\n            del sys.modules[module_name]\n        return None\n```\n\n#### D. Core Logic Skeleton Code\n\n**File: `apollo/discovery/discoverer.py`**\n```python\nfrom pathlib import Path\nfrom typing import List, Iterator\nimport inspect\nfrom dataclasses import dataclass\n\nfrom ..models.test_case import TestCase\nfrom ..utils.import_utils import import_module_from_file\n\n\ndef discover_tests(start_path: Path, pattern: str = \"test_*.py\") -> List[TestCase]:\n    \"\"\"\n    Discover all test functions in modules under start_path matching the pattern.\n    \n    This is the main entry point for test discovery. It follows the algorithm:\n    1. Resolve file patterns to concrete file paths\n    2. Import each Python module safely\n    3. Extract test functions from each module\n    4. Return a list of TestCase objects\n    \n    Args:\n        start_path: Directory to start discovery from\n        pattern: Glob pattern for matching test files (default: \"test_*.py\")\n        \n    Returns:\n        List of TestCase objects representing discovered tests\n    \"\"\"\n    # TODO 1: Use resolve_patterns_to_paths() to convert pattern to list of file paths\n    # TODO 2: For each file path, call import_module_from_file() to import the module\n    # TODO 3: For each successfully imported module, call _find_tests_in_module()\n    # TODO 4: Collect all TestCase objects into a list\n    # TODO 5: Apply any additional filtering (by test name patterns if provided)\n    # TODO 6: Return the final list of TestCase objects\n    pass\n\n\ndef _find_tests_in_module(module) -> Iterator[TestCase]:\n    \"\"\"\n    Yield TestCase objects for test functions found in a module.\n    \n    This function inspects all members of a module and identifies:\n    - Functions whose names start with \"test_\"\n    - Methods inside classes whose names start with \"Test\" and method names start with \"test_\"\n    \n    Args:\n        module: The imported Python module to inspect\n        \n    Yields:\n        TestCase objects for each discovered test\n    \"\"\"\n    # TODO 1: Use inspect.getmembers() to get all members of the module\n    # TODO 2: For each member, check if it's a function with name starting with \"test_\"\n    # TODO 3: For function tests: create TestCase with nodeid format \"module.py::function_name\"\n    # TODO 4: For each member, check if it's a class with name starting with \"Test\"\n    # TODO 5: For test classes: inspect methods, find those starting with \"test_\"\n    # TODO 6: For method tests: create TestCase with nodeid format \"module.py::ClassName::method_name\"\n    # TODO 7: For each test, capture file_path and line_no using inspect.getfile() and inspect.getsourcelines()\n    # TODO 8: Set fixtures list to empty (will be populated later during fixture discovery)\n    # TODO 9: Yield the TestCase object\n    pass\n```\n\n**File: `apollo/runner/simple_runner.py`**\n```python\nimport time\nfrom typing import List\nfrom dataclasses import dataclass\n\nfrom ..models.test_case import TestCase\nfrom ..models.test_result import TestResult, TestStatus\n\n\nclass SimpleRunner:\n    \"\"\"\n    Sequential test runner that executes tests one at a time with isolation.\n    \n    This runner ensures each test runs in a clean environment and that\n    fixture teardown always occurs, even if the test fails.\n    \"\"\"\n    \n    def run_test(self, test_case: TestCase) -> TestResult:\n        \"\"\"\n        Execute a single test and return its result.\n        \n        The execution follows this sequence:\n        1. Create TestResult with PENDING status\n        2. Setup required fixtures\n        3. Execute test function\n        4. Teardown fixtures (always runs, even on failure)\n        5. Finalize TestResult with outcome\n        \n        Args:\n            test_case: The test to execute\n            \n        Returns:\n            TestResult containing the outcome and diagnostic information\n        \"\"\"\n        # TODO 1: Create TestResult object with test_case reference and status=PENDING\n        # TODO 2: Record start time using time.monotonic()\n        # TODO 3: Change status to RUNNING\n        # TODO 4: Setup fixtures (for Milestone 1, this is just record that it would happen)\n        # TODO 5: Wrap test execution in try-except block\n        # TODO 6: In try: call test_case.func() and on success set status=PASSED\n        # TODO 7: In except AssertionError: set status=FAILED and capture error message\n        # TODO 8: In except Exception: set status=ERRORED and capture exception with traceback\n        # TODO 9: In finally: execute fixture teardown (for Milestone 1, just record)\n        # TODO 10: Record end time and calculate duration\n        # TODO 11: Return the completed TestResult\n        pass\n    \n    def run_suite(self, suite) -> List[TestResult]:\n        \"\"\"\n        Run all tests in a suite and return their results.\n        \n        For Milestone 1, this runs tests sequentially. In later milestones,\n        this will handle parallel execution when configured.\n        \n        Args:\n            suite: TestSuite containing tests to run\n            \n        Returns:\n            List of TestResult objects in the same order as tests in the suite\n        \"\"\"\n        # TODO 1: Initialize empty list for results\n        # TODO 2: For each test_case in suite.tests:\n        # TODO 3:   Call self.run_test(test_case) and append result to list\n        # TODO 4: Return the list of results\n        pass\n```\n\n#### E. Language-Specific Hints (Python)\n\n1. **Use `inspect` module for reflection**: `inspect.isfunction()`, `inspect.getmembers()`, and `inspect.getsourcelines()` are essential for discovering test functions and capturing their metadata.\n\n2. **Handle imports carefully**: Use `importlib` instead of `__import__` for more control over the import process. Remember to clean up `sys.modules` to avoid state leakage between tests.\n\n3. **For parallel execution**: Use `multiprocessing.Pool` with `initializer` to set up each worker process. Make sure test cases and fixtures are picklable (implement `__reduce__` if needed).\n\n4. **Isolation technique**: The simplest isolation for sequential execution is to run each test in its own directory using `os.chdir()` to a temporary directory, then change back after the test.\n\n5. **Timing precision**: Use `time.monotonic()` instead of `time.time()` for measuring test durations, as it's not affected by system clock changes.\n\n#### F. Milestone 1 Checkpoint\n\nAfter implementing the Discovery & Execution components, you should be able to run:\n\n```bash\n# Create a simple test file\ncat > test_example.py << 'EOF'\ndef test_addition():\n    assert 1 + 1 == 2\n\ndef test_subtraction():\n    assert 5 - 3 == 2\n\ndef helper_function():\n    return \"not a test\"\nEOF\n\n# Run the framework on this test file\npython -m apollo test_example.py\n```\n\n**Expected Output**:\n```\nDiscovering tests...\nFound 2 tests in 1 file\n\nRunning tests...\ntest_example.py::test_addition ... PASSED (0.001s)\ntest_example.py::test_subtraction ... PASSED (0.001s)\n\nSummary:\n2 passed, 0 failed, 0 errored, 0 skipped in 0.002s\n```\n\n**Verification Checklist**:\n- [ ] Both `test_addition` and `test_subtraction` are discovered and executed\n- [ ] `helper_function` is NOT discovered as a test (doesn't start with `test_`)\n- [ ] Each test shows PASSED status with execution time\n- [ ] Tests run sequentially (one after another)\n- [ ] Summary shows correct counts and total time\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| No tests discovered | Incorrect file pattern or working directory | Print resolved file paths before import | Use absolute paths; Check `start_path` is correct |\n| Tests discovered but not executed | Test functions not callable or have parameters | Check `inspect.iscallable()` on discovered functions | Ensure test functions have no required parameters (yet) |\n| Tests interfering with each other | Module-level state shared between tests | Add print statements showing module variable values | Implement proper isolation: reload modules or use fresh processes |\n| Import errors during discovery | Missing dependencies or syntax errors | Catch and log import exceptions in `import_module_from_file` | Report import errors as discovery failures, continue with other files |\n| Parallel execution hangs | Deadlock in shared resources or pickling errors | Add timeout to worker processes; Check picklability of fixtures | Ensure tests don't use shared resources; Make fixtures picklable |\n\n\n## 6. Component Design: Assertions & Matchers (Milestone 2)\n\n> **Milestone(s):** Milestone 2 (Assertions & Matchers)\n\nThe **Assertion Engine** and **Matchers API** form the diagnostic core of the test framework—the system that transforms vague test failures into precise, actionable insights. While discovery finds tests and execution runs them, the assertion system determines whether tests actually pass or fail by comparing expected outcomes against actual results. This component embodies the framework's intelligence, moving beyond simple true/false checks to provide rich contextual information when things go wrong.\n\nThink of the assertion system as a **Laboratory Microscope with Comparison Lenses**. A basic microscope (simple `assert` statements) can tell you \"something looks wrong.\" Our framework provides specialized comparison lenses—some for comparing lists, others for checking exceptions, others for custom patterns—that not only identify discrepancies but also highlight them in high resolution with side-by-side comparisons, difference highlighting, and domain-specific diagnostics. The **Assertion Engine** is the microscope's optical system, while the **Matchers API** is the interchangeable lens kit that users can customize for their specific testing needs.\n\n### Assertion Engine\n\nThe Assertion Engine serves as the centralized evaluation and reporting system for all verification operations. Its primary responsibility is to transform predicate evaluations into structured failure information when expectations aren't met. Unlike Python's built-in `assert` statement (which provides minimal context like line numbers but lacks semantic understanding), the Assertion Engine understands the *intent* behind each check and can generate detailed diagnostic messages.\n\n> **Critical Insight**: The value of an assertion isn't just in determining pass/fail—it's in providing enough diagnostic information that a developer can understand *why* the failure occurred without needing to add debug prints or step through code. A good assertion failure message should be 80% of the debugging work.\n\n#### Mental Model: The Comparison Inspector\n\nImagine an **Evidence Examiner** who compares two artifacts. When they find a mismatch, they don't just say \"these don't match\"—they produce a detailed report showing: \"At position 3, expected 'blue' but found 'green'; here's a side-by-side comparison with differences highlighted; here's the context around the mismatch; here are the statistical properties that differ.\" The Assertion Engine is this examiner, specialized in comparing different data types with appropriate comparison strategies for each.\n\n#### Core Design Principles\n\n1. **Semantic Understanding Over Syntactic Checking**: The engine understands what \"equal\" means for different data types—structural equality for collections, approximate equality for floats, case-insensitive equality for strings in certain contexts.\n2. **Failure Messages as Primary Output**: The error message is the main deliverable, designed to be read by humans debugging test failures.\n3. **Type-Adaptive Comparison Strategies**: Different data types get different comparison algorithms optimized for that type's characteristics.\n4. **Minimal Performance Overhead in Success Paths**: When assertions pass, they should add minimal overhead. Expensive operations (like deep diffing) only occur on failures.\n\n#### Architecture Decision Record: Exception-Based Failure Signaling\n\n> **Decision: Use Python's Exception Mechanism for Assertion Failures**\n>\n> - **Context**: When an assertion fails, the framework needs to: (1) halt test execution at that point, (2) capture the failure reason, (3) continue running subsequent tests. We need a mechanism that naturally supports these requirements within Python's execution model.\n> - **Options Considered**:\n>   1. **Return Boolean + Side Channel**: Each assertion returns `True`/`False` and writes failure details to a shared mutable state. The test runner checks the boolean and then reads the failure details.\n>   2. **Custom Control Flow**: Assertions set a \"test failed\" flag and the test runner periodically checks this flag to decide whether to continue test execution.\n>   3. **Exception Raising**: Assertions raise a custom `AssertionError` (or subclass) with failure details attached.\n> - **Decision**: Use exception raising with a custom `AssertionError` subclass that carries rich failure information.\n> - **Rationale**:\n>   - Exceptions provide natural control flow interruption—when an assertion fails, execution immediately stops at that point, preventing \"zombie\" test code from running after a failure.\n>   - Python's exception system is optimized and familiar to all Python developers.\n>   - Exception objects can carry arbitrary data (like expected/actual values, diff results, custom messages) which can be extracted by the test runner for reporting.\n>   - Exception tracebacks automatically provide file/line context (though we enrich this with semantic information).\n>   - The approach aligns with existing Python testing conventions (`unittest`, `pytest`) and developer expectations.\n> - **Consequences**:\n>   - Tests must be written to expect assertions might raise exceptions (they already do with built-in `assert`).\n>   - The test runner needs proper exception handling to catch assertion failures and continue to next tests.\n>   - We get \"fail-fast\" behavior within a single test for free.\n>   - Stack traces will include our assertion function calls, which we need to clean up in reports to avoid noise.\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Return Boolean + Side Channel | Explicit control flow; no exception overhead | Requires manual propagation of failure state; easy to forget checks; test code continues after failure | Makes tests verbose; violates principle of least surprise |\n| Custom Control Flow | Could optimize for specific patterns; no stack unwinding | Complex to implement; breaks standard Python idioms; hard to debug | Reinvents control flow poorly; high maintenance cost |\n| Exception Raising | Natural Python idiom; automatic control flow; carries rich data | Slight performance overhead; stack traces need cleaning | **CHOSEN**: Standard, predictable, and powerful |\n\n#### Data Structures\n\nThe Assertion Engine introduces several core data structures:\n\n| Type Name | Fields | Description |\n|-----------|--------|-------------|\n| `AssertionFailure` | `message: str`, `expected: Any`, `actual: Any`, `diff: Optional[str]`, `hint: Optional[str]`, `assertion_type: str` | Container for all information about a failed assertion. This is *not* an exception itself but data packaged within an exception. |\n| `ComparisonContext` | `tolerance: float`, `ignore_case: bool`, `ignore_whitespace: bool`, `unordered: bool` | Configuration for how comparisons should be performed, allowing customization per assertion call. |\n| `DiffResult` | `summary: str`, `unified_diff: Optional[str]`, `context_lines: int` | Structured result of diff computation between expected and actual values. |\n\nThe relationship between these structures and the existing `TestResult` is critical:\n\n1. When an assertion fails, it creates an `AssertionFailure` with diagnostic details.\n2. This is wrapped in a custom `AssertionError` subclass (e.g., `TestFrameworkAssertionError`).\n3. The test runner catches this exception, extracts the `AssertionFailure` data, and stores it in `TestResult.message` and `TestResult.exception`.\n4. The reporter formats this information appropriately for output.\n\n#### Assertion Evaluation Algorithm\n\nThe flowchart ![Assertion Evaluation Flow](./diagrams/assertion-eval-flow.svg) shows the high-level process. Here's the detailed algorithm for `assert_equal(actual, expected, msg=None, **kwargs)`:\n\n1. **Parse Comparison Context**: Extract comparison options from `kwargs` (like `tolerance=0.001`) and create a `ComparisonContext` object.\n\n2. **Type Inspection**: Determine the types of `actual` and `expected`. The engine maintains a registry of type-specific comparison handlers:\n   - Basic types (int, str, bool): Use Python's `==` but with optional transformations (case-insensitive, etc.)\n   - Floating point numbers: Use relative/absolute tolerance comparison (like `math.isclose()`)\n   - Collections (list, tuple, dict, set): Use structural comparison with configurable ordering\n   - NumPy arrays/pandas DataFrames: Specialized handlers (could be added via Matchers API)\n   - Custom objects: Try `__eq__`, fall back to `repr()` comparison\n\n3. **Equality Check**: Apply the appropriate comparison algorithm based on types and context:\n   - For floats: `abs(a-b) <= max(rel_tol*max(abs(a), abs(b)), abs_tol)`\n   - For sequences with `unordered=True`: Convert to multisets or sorted lists\n   - For dicts: Compare keys then recursively compare values\n\n4. **If Match Passes**: Return `None` immediately (success path—minimal overhead).\n\n5. **If Match Fails**: Enter diagnostic generation phase:\n   a. **Compute Diff**: Based on types:\n      - Strings: Generate unified diff with 3 lines of context\n      - Lists/Tuples: Highlight first differing element with index\n      - Dicts: Show missing/extra keys and differing values\n      - Mixed types: Show type mismatch clearly\n   \n   b. **Format Message**: Assemble a human-readable message with:\n      - Custom message (if provided by user)\n      - Expected and actual values (with intelligent truncation for large objects)\n      - Diff output (if computed)\n      - Type information (especially helpful when `1 != 1.0`)\n      - Hint based on common mistakes (e.g., \"Did you forget to call the function?\")\n   \n   c. **Raise Exception**: Instantiate `TestFrameworkAssertionError` with the `AssertionFailure` data and raise it.\n\n#### Core Assertion Suite\n\nThe framework provides these essential assertions (all following the pattern above):\n\n| Assertion Method | Signature | Purpose | Special Comparison Logic |\n|-----------------|-----------|---------|--------------------------|\n| `assert_equal` | `(actual, expected, msg=None, **kwargs)` | General equality with type-aware comparison | Float tolerance, unordered collections, case/whitespace insensitivity |\n| `assert_not_equal` | `(actual, unexpected, msg=None, **kwargs)` | Inverse of assert_equal | Same comparison logic, fails if values *are* equal |\n| `assert_true` | `(condition, msg=None)` | Verify truthiness | Uses `bool()` conversion; suggests common falsy values in message |\n| `assert_false` | `(condition, msg=None)` | Verify falsiness | Inverse of assert_true |\n| `assert_is` | `(actual, expected, msg=None)` | Identity check (`is` operator) | Uses `id()` for objects; shows object details in failure |\n| `assert_is_not` | `(actual, unexpected, msg=None)` | Inverse identity check | |\n| `assert_is_none` | `(value, msg=None)` | Special case for `None` | Clear message when value is not None |\n| `assert_is_not_none` | `(value, msg=None)` | Verify non-None | Shows actual value in failure |\n| `assert_in` | `(item, container, msg=None)` | Membership test | For strings, shows surrounding context; for dicts, checks keys |\n| `assert_not_in` | `(item, container, msg=None)` | Inverse membership | |\n| `assert_is_instance` | `(obj, cls, msg=None)` | Type checking | Shows actual type and MRO in failure |\n| `assert_not_is_instance` | `(obj, cls, msg=None)` | Inverse type check | |\n| `assert_almost_equal` | `(actual, expected, places=7, msg=None, **kwargs)` | Numeric approximation | Decimal place rounding; auto-switches to significant figures for very small numbers |\n| `assert_not_almost_equal` | `(actual, unexpected, places=7, msg=None, **kwargs)` | Inverse numeric check | |\n| `assert_greater` | `(actual, expected, msg=None)` | Ordered comparison > | Works with any comparable types |\n| `assert_greater_equal` | `(actual, expected, msg=None)` | Ordered comparison >= | |\n| `assert_less` | `(actual, expected, msg=None)` | Ordered comparison < | |\n| `assert_less_equal` | `(actual, expected, msg=None)` | Ordered comparison <= | |\n| `assert_regex` | `(text, regex, msg=None)` | Pattern matching | Shows match failure location; extracts and shows matched groups on success |\n| `assert_not_regex` | `(text, regex, msg=None)` | Inverse pattern match | |\n\n#### Collection Assertions\n\nSpecialized assertions for collection types provide more semantic feedback:\n\n| Assertion Method | Signature | Purpose | Special Features |\n|-----------------|-----------|---------|------------------|\n| `assert_length` | `(collection, expected_len, msg=None)` | Verify collection size | Works with any `len()`-able object; suggests common off-by-one errors |\n| `assert_count_equal` | `(actual, expected, msg=None)` | Compare ignoring order | Counts occurrences of each element; shows frequency mismatches |\n| `assert_list_equal` | `(actual, expected, msg=None, **kwargs)` | Ordered list equality | Element-by-element diff with indices |\n| `assert_dict_equal` | `(actual, expected, msg=None, **kwargs)` | Dictionary equality | Key-by-key comparison; shows nested differences |\n| `assert_set_equal` | `(actual, expected, msg=None)` | Set equality | Shows symmetric difference |\n| `assert_contains_all` | `(container, items, msg=None)` | Superset check | Shows which items are missing |\n| `assert_contains_any` | `(container, items, msg=None)` | At least one match | Shows all attempted items |\n| `assert_contains_none` | `(container, items, msg=None)` | No matches allowed | Shows which items were unexpectedly found |\n| `assert_sorted` | `(collection, key=None, reverse=False, msg=None)` | Verify sorted order | Shows first out-of-order elements |\n\n#### Exception Assertions\n\nTesting exception behavior requires special handling because we're asserting that a *side effect* (exception raising) occurs:\n\n**Mental Model: The Exception Catcher** — Imagine setting up a specialized net to catch only specific types of flying objects (exceptions). The assertion verifies that: (1) the right type of object gets caught, (2) it has the expected properties (message, attributes), and (3) nothing else gets caught unexpectedly.\n\n**Architecture Decision Record: Context Manager vs Function Wrapper for Exception Testing**\n\n> **Decision: Use Context Manager Pattern for Exception Assertions**\n>\n> - **Context**: We need to verify that calling a function raises an expected exception. Two common patterns exist: decorator/wrapper that calls the function, or context manager that wraps a block of code.\n> - **Options Considered**:\n>   1. **Function Wrapper**: `assert_raises(exception_type, func, *args, **kwargs)` calls the function and checks if it raises.\n>   2. **Context Manager**: `with assert_raises(exception_type):` wraps a block where the exception should occur.\n>   3. **Hybrid Approach**: Support both patterns through a single flexible API.\n> - **Decision**: Implement as a context manager that can also be used as a function decorator/caller for backward compatibility.\n> - **Rationale**:\n>   - Context manager pattern is more Pythonic and readable for multi-line exception testing.\n>   - It naturally handles the case where the exception comes from code other than a single function call.\n>   - The context manager can capture and expose the exception object for further inspection (message, attributes).\n>   - We can add a `.__call__()` method to support the function wrapper pattern for simple cases.\n>   - This aligns with `pytest.raises` and modern Python testing practices.\n> - **Consequences**:\n>   - Slightly more complex implementation (context manager protocol).\n>   - Provides better diagnostic information (can show which line in the block actually raised).\n>   - Allows checking exception properties after it's caught.\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Function Wrapper | Simple implementation; single responsibility | Can't test multi-line blocks; limited inspection of caught exception | Too restrictive for real-world testing |\n| Context Manager | Flexible; captures exception for inspection; Pythonic | More complex to implement; two ways to use could confuse | **CHOSEN**: Industry standard; maximal flexibility |\n| Hybrid | Backward compatible; covers all use cases | Most complex; ambiguous API | Complexity not justified for learning framework |\n\n**Implementation Algorithm for `assert_raises`**:\n\n1. **Context Manager Entry**: When `with assert_raises(ExpectedException) as cm:` is entered, store expected exception type and any match criteria.\n\n2. **Block Execution**: Execute the indented block within a try-except.\n\n3. **Exception Monitoring**:\n   - If no exception is raised: FAIL → \"Expected exception ExpectedException but no exception was raised\"\n   - If wrong exception type is raised: FAIL → \"Expected exception ExpectedException but AnotherException was raised: message...\"\n   - If correct exception type is raised: PASS to step 4\n\n4. **Exception Validation**: If additional criteria provided (regex for message, predicate function), validate caught exception:\n   - If message regex: check exception message matches pattern\n   - If predicate: call `predicate(exc)` must return True\n   - If any validation fails: FAIL with specific reason\n\n5. **Exception Storage**: Store caught exception in context manager object for further inspection (`.value` attribute).\n\n6. **Context Manager Exit**: Clean up (nothing needed normally).\n\n**Additional Exception Assertions**:\n\n- `assert_raises_regex`: Combines type and message pattern matching\n- `assert_warns`: Similar pattern for warning capture\n- `assert_no_raises`: Context manager that fails if *any* exception is raised (useful for \"this shouldn't crash\" tests)\n\n#### Common Pitfalls in Assertion Engine Implementation\n\n⚠️ **Pitfall: Unhelpful Error Messages for Complex Objects**\n- **Description**: Showing `assert_equal(complex_obj1, complex_obj2)` with just `<object at 0x...> != <object at 0x...>`.\n- **Why It's Wrong**: Developer must add debug prints or use a debugger to see what actually differed.\n- **Fix**: Implement recursive `__repr__` inspection or use `pprint.pformat()` for nested structures. For custom objects, try to access `__dict__` or `__slots__`.\n\n⚠️ **Pitfall: Float Comparison with Exact Equality**\n- **Description**: `assert_equal(0.1 + 0.2, 0.3)` fails due to binary floating-point representation.\n- **Why It's Wrong**: Mathematical equality ≠ binary representation equality for floats.\n- **Fix**: Use relative/absolute tolerance (`math.isclose()` logic). Provide `tolerance` parameter and sensible defaults (like `rel_tol=1e-9, abs_tol=0.0`).\n\n⚠️ **Pitfall: Exception Context Loss**\n- **Description**: When `assert_raises` catches an exception, the original traceback is replaced with the assertion's traceback.\n- **Why It's Wrong**: Developer can't see where in their code the exception actually originated.\n- **Fix**: Use `raise ... from None` pattern or store original traceback and include it in failure output.\n\n⚠️ **Pitfall: Infinite Recursion in Nested Comparison**\n- **Description**: Comparing objects that reference each other (circular references) causes infinite recursion.\n- **Why It's Wrong**: Crashes the test runner with recursion depth exceeded.\n- **Fix**: Maintain a `visiting` set of object IDs already being compared. Skip or handle circular references specially.\n\n⚠️ **Pitfall: Mutation During Comparison**\n- **Description**: Assertion code inadvertently mutates the objects being compared (e.g., sorting in place for unordered comparison).\n- **Why It's Wrong**: Changes test behavior; makes tests non-idempotent.\n- **Fix**: Always work on copies. Use `sorted()` not `.sort()`, `copy.deepcopy()` for nested structures.\n\n### Matchers API\n\nWhile the Assertion Engine provides a comprehensive set of built-in assertions, real-world testing often requires domain-specific checks that aren't covered by generic equality or membership tests. The **Matchers API** provides an extensible, composable system for defining custom assertion logic with tailored failure messages.\n\n**Mental Model: The Rulebook Builder** — Imagine you're creating a rulebook for verifying architectural blueprints. The built-in assertions are like standard measuring tools (rulers, protractors). The Matchers API lets you define custom verification rules: \"Windows must comprise 20-30% of wall surface\" or \"All doors must open inward in public spaces.\" These domain-specific rules can be combined (\"and\" / \"or\" / \"not\") and provide specific failure messages (\"Window area 15% is below required 20%\").\n\n#### Design Philosophy\n\n1. **Composability**: Matchers can be combined using logical operators (`&` for AND, `|` for OR, `~` for NOT) to build complex conditions.\n2. **Descriptive Failure Messages**: Each matcher knows how to describe what it expected versus what it got, in domain-specific terms.\n3. **Reusability**: Define once, use across multiple tests and even multiple projects.\n4. **Readability**: Matcher-based assertions read like natural language specifications.\n\n#### Architecture Decision Record: Object-Oriented Matchers vs Function-Based Matchers\n\n> **Decision: Use Object-Oriented Matcher Classes with `__matches__` Protocol**\n>\n> - **Context**: We need an extensible system where users can define custom matching logic. Two approaches exist: class-based matchers with a standard method, or function-based matchers with a registration system.\n> - **Options Considered**:\n>   1. **Function-Based**: Users define `def is_even(x): return x % 2 == 0` and register it. Assertion is `assert_that(value, is_even)`.\n>   2. **Class-Based**: Users define `class EvenMatcher:` with a `matches(actual)` method and `describe()` method.\n>   3. **Hybrid Protocol**: Support both through single-dispatch or adapter pattern.\n> - **Decision**: Use class-based matchers with a defined protocol (`__matches__` and `__describe__` methods).\n> - **Rationale**:\n>   - Classes naturally encapsulate both the matching logic *and* the description logic.\n>   - Classes support stateful matchers (e.g., `GreaterThan(threshold)` where threshold is stored).\n>   - Operator overloading (`&`, `|`, `~`) is cleaner with classes.\n>   - Inheritance provides a clear way to extend base matcher functionality.\n>   - The pattern is well-established in libraries like Hamcrest (Java) and its Python ports.\n> - **Consequences**:\n>   - Slightly more verbose than simple functions.\n>   - Provides better structure for complex matchers.\n>   - Enables matcher composition out of the box.\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Function-Based | Simple; familiar; minimal syntax | Hard to compose; limited description capability | Too limited for serious use |\n| Class-Based | Full-featured; composable; extensible | More boilerplate; steeper learning curve | **CHOSEN**: Professional-grade flexibility |\n| Hybrid | Maximum flexibility; accommodates both styles | Complex implementation; ambiguous best practices | Complexity not justified for learning |\n\n#### Core Matcher Protocol\n\nAll matchers implement this simple protocol:\n\n| Method | Signature | Returns | Description |\n|--------|-----------|---------|-------------|\n| `__matches__` | `(self, actual: Any) -> bool` | Boolean | Core matching logic. Return `True` if actual value satisfies the matcher's condition. |\n| `__describe__` | `(self) -> str` | String | Description of what the matcher expects. Used in failure messages. |\n| `__describe_mismatch__` | `(self, actual: Any) -> str` | String | Optional. More specific description of why `actual` didn't match. |\n| `__and__` | `(self, other) -> AllOf` | Matcher | Operator overload for `&` (AND composition). |\n| `__or__` | `(self, other) -> AnyOf` | Matcher | Operator overload for `|` (OR composition). |\n| `__invert__` | `(self) -> Not` | Matcher | Operator overload for `~` (NOT composition). |\n\nThe `assert_that(actual, matcher, msg=None)` function is the bridge between matchers and the assertion engine:\n\n```python\n# Conceptual implementation\ndef assert_that(actual, matcher, msg=None):\n    if not matcher.__matches__(actual):\n        # Build failure message\n        description = matcher.__describe__()\n        mismatch = (matcher.__describe_mismatch__(actual) \n                    if hasattr(matcher, '__describe_mismatch__') \n                    else f\"was {repr(actual)}\")\n        \n        full_message = f\"Expected: {description}\\n\"\n        full_message += f\"Got: {mismatch}\"\n        \n        if msg:\n            full_message = f\"{msg}\\n{full_message}\"\n            \n        raise AssertionError(full_message)\n```\n\n#### Built-in Matcher Library\n\nThe framework provides a comprehensive set of built-in matchers:\n\n| Matcher Class | Constructor | Purpose | Example Usage |\n|---------------|-------------|---------|---------------|\n| `EqualTo` | `(expected)` | Value equality | `assert_that(5, EqualTo(5))` |\n| `IsInstanceOf` | `(type_or_types)` | Type checking | `assert_that(obj, IsInstanceOf(MyClass))` |\n| `Contains` | `(item)` | Membership | `assert_that([1,2,3], Contains(2))` |\n| `HasLength` | `(length)` | Collection size | `assert_that(\"abc\", HasLength(3))` |\n| `GreaterThan` | `(threshold)` | Numeric > | `assert_that(10, GreaterThan(5))` |\n| `LessThan` | `(threshold)` | Numeric < | `assert_that(3, LessThan(5))` |\n| `CloseTo` | `(expected, tolerance)` | Approximate equality | `assert_that(3.001, CloseTo(3.0, 0.01))` |\n| `MatchesRegex` | `(pattern)` | Pattern matching | `assert_that(\"hello\", MatchesRegex(r\"h.*o\"))` |\n| `HasAttribute` | `(name, value_matcher=None)` | Object attribute | `assert_that(obj, HasAttribute(\"size\", GreaterThan(0)))` |\n| `HasProperty` | `(name, value_matcher=None)` | Property/getter | `assert_that(obj, HasProperty(\"area\"))` |\n| `Is` | `(expected)` | Identity (`is`) | `assert_that(x, Is(None))` |\n| `IsNone` | `()` | Specialized for None | `assert_that(result, IsNone())` |\n| `IsTrue` | `()` | Truthiness | `assert_that(flag, IsTrue())` |\n| `IsFalse` | `()` | Falsiness | `assert_that(flag, IsFalse())` |\n| `IsEmpty` | `()` | Empty collection | `assert_that([], IsEmpty())` |\n| `IsNotEmpty` | `()` | Non-empty collection | `assert_that([1], IsNotEmpty())` |\n\n#### Matcher Composition\n\nThe real power emerges when matchers are combined:\n\n```python\n# Conceptual examples (actual API would use matcher objects)\nassert_that(response, \n    HasAttribute(\"status_code\", EqualTo(200)) &\n    HasAttribute(\"headers\", Contains(\"Content-Type\")) &\n    HasAttribute(\"json\", HasAttribute(\"success\", IsTrue())))\n\nassert_that(value,\n    IsInstanceOf(int) &\n    (GreaterThan(0) | EqualTo(-1)))  # Positive or -1\n\nassert_that(string,\n    MatchesRegex(r\"^\\d{3}-\\d{2}-\\d{4}$\") &\n    ~Contains(\"000-00-0000\"))  # Valid SSN but not the dummy one\n```\n\n**Composition Matchers**:\n\n| Composition Class | Creates | Description |\n|-------------------|---------|-------------|\n| `AllOf` | `matcher1 & matcher2 & ...` | Logical AND; all matchers must match |\n| `AnyOf` | `matcher1 | matcher2 | ...` | Logical OR; at least one matcher must match |\n| `Not` | `~matcher` | Logical NOT; inverts the matcher |\n| `IsAny` | `AnyOf(AlwaysTrue())` | Matches anything (useful as default) |\n| `IsNothing` | `AllOf(AlwaysFalse())` | Matches nothing (useful in combinations) |\n\n#### Custom Matcher Creation\n\nUsers create custom matchers by subclassing `BaseMatcher`:\n\n```python\n# Conceptual example\nclass IsEven(BaseMatcher):\n    def __matches__(self, actual):\n        return isinstance(actual, int) and actual % 2 == 0\n    \n    def __describe__(self):\n        return \"an even integer\"\n    \n    def __describe_mismatch__(self, actual):\n        if not isinstance(actual, int):\n            return f\"was a {type(actual).__name__} ({repr(actual)})\"\n        return f\"was odd ({actual})\"\n\n# Usage\nassert_that(4, IsEven())\nassert_that(4, IsEven() & GreaterThan(0))\n```\n\n**Stateful Matcher Example** (more realistic):\n\n```python\nclass HasStatusCode(BaseMatcher):\n    def __init__(self, expected_status, allowed_errors=None):\n        self.expected_status = expected_status\n        self.allowed_errors = allowed_errors or []\n    \n    def __matches__(self, actual):\n        if not hasattr(actual, 'status_code'):\n            return False\n        return (actual.status_code == self.expected_status or \n                actual.status_code in self.allowed_errors)\n    \n    def __describe__(self):\n        desc = f\"response with status {self.expected_status}\"\n        if self.allowed_errors:\n            desc += f\" or allowed errors {self.allowed_errors}\"\n        return desc\n    \n    def __describe_mismatch__(self, actual):\n        if not hasattr(actual, 'status_code'):\n            return f\"object has no 'status_code' attribute\"\n        return f\"had status {actual.status_code}\"\n\n# Usage in API tests\nassert_that(response, HasStatusCode(200))\nassert_that(error_response, HasStatusCode(200, allowed_errors=[500, 503]))\n```\n\n#### Integration with Assertion Engine\n\nMatchers integrate seamlessly with the core assertion system:\n\n1. **Failure Message Integration**: When `assert_that` fails, it creates an `AssertionFailure` with:\n   - `expected`: The matcher's `__describe__()` output\n   - `actual`: Either the raw value or matcher's `__describe_mismatch__()` output\n   - `hint`: Possible fixes based on matcher type\n\n2. **Type Registry**: Matchers can register themselves as handlers for specific types in the Assertion Engine's comparison registry. For example, a `NumpyArrayCloseTo` matcher could register itself for `numpy.ndarray` type, making `assert_equal(numpy_array1, numpy_array2)` automatically use approximate comparison.\n\n3. **Negation Support**: The `assert_not_that(actual, matcher)` function provides the inverse check, using the `Not` composition matcher internally.\n\n#### Common Pitfalls in Matchers API Implementation\n\n⚠️ **Pitfall: Matcher Side Effects in `__describe__`**\n- **Description**: The `__describe__()` method should be pure (no side effects), but developers might compute expensive descriptions or modify state.\n- **Why It's Wrong**: Description might be called even when test passes (in some implementations), causing performance issues or state corruption.\n- **Fix**: Make `__describe__()` a pure method. Compute expensive descriptions lazily or cache them. Document clearly that it should be side-effect free.\n\n⚠️ **Pitfall: Incorrect Operator Precedence**\n- **Description**: `matcher1 & matcher2 | matcher3` doesn't group as expected due to Python's operator precedence.\n- **Why It's Wrong**: `&` has higher precedence than `|`, so this evaluates as `(matcher1 & matcher2) | matcher3`, not `matcher1 & (matcher2 | matcher3)`.\n- **Fix**: Use parentheses explicitly or implement `__rand__`, `__ror__` to try to handle gracefully. Document the precedence clearly.\n\n⚠️ **Pitfall: Matcher Mutating the Actual Value**\n- **Description**: Matcher's `__matches__()` method modifies the actual value being tested.\n- **Why It's Wrong**: Changes test state unexpectedly; makes tests non-deterministic.\n- **Fix**: Never modify `actual`. Document as requirement. Use defensive copies if needed.\n\n⚠️ **Pitfall: Overly Complex Matcher Composition**\n- **Description**: Chaining too many matchers creates incomprehensible failure messages like \"Expected: (A and (B or (C and not D)))...\"\n- **Why It's Wrong**: Debugging becomes as hard as the original problem.\n- **Fix**: Encourage creating named composite matchers for common patterns. Simplify output formatting for nested compositions.\n\n⚠️ **Pitfall: Matcher Not Handling `None` Gracefully**\n- **Description**: `HasAttribute(\"name\")(None)` raises `AttributeError` instead of returning `False`.\n- **Why It's Wrong**: Matchers should handle all inputs gracefully, returning `False` for non-matching values, not raising exceptions.\n- **Fix**: Use `getattr()` with default, `isinstance()` checks, or try-except in `__matches__()`.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Comparison Engine | Python's `==` with `repr()` fallback | Type-registry with specialized comparators for numpy/pandas/dataclasses |\n| Diff Generation | `difflib.unified_diff` for strings only | Recursive diff with similarity scoring for nested structures |\n| Float Comparison | `abs(a-b) < tolerance` | `math.isclose()` with relative/absolute tolerance and ULPs |\n| Matcher Protocol | Simple `matches()` method | Full protocol with `describe_to()` and `describe_mismatch()` like Hamcrest |\n| Exception Testing | Function wrapper `assert_raises()` | Context manager with exception inspection and regex matching |\n\nFor a learning framework, start with the Simple options and evolve toward Advanced as needed.\n\n#### Recommended File/Module Structure\n\n```\napollo/\n├── __init__.py           # Public API: assert_equal, assert_that, etc.\n├── assertions/\n│   ├── __init__.py       # Re-export core assertions\n│   ├── engine.py         # AssertionEngine class, AssertionFailure\n│   ├── core.py           # assert_equal, assert_true, assert_raises, etc.\n│   ├── collections.py    # Collection-specific assertions\n│   ├── exceptions.py     # Exception-related assertions\n│   └── numeric.py        # Numeric comparison assertions\n├── matchers/\n│   ├── __init__.py       # BaseMatcher, assert_that, built-in matchers\n│   ├── base.py           # BaseMatcher, AllOf, AnyOf, Not\n│   ├── builtins.py       # EqualTo, GreaterThan, Contains, etc.\n│   ├── collections.py    # Collection matchers\n│   ├── numeric.py        # Numeric matchers  \n│   └── objects.py        # Object/attribute matchers\n└── internal/\n    ├── diff.py           # Diff computation utilities\n    └── comparison.py     # Type-specific comparison logic\n```\n\n#### Infrastructure Starter Code\n\n**`apollo/internal/diff.py`** (complete working code):\n\n```python\n\"\"\"Diff computation utilities for assertion messages.\"\"\"\nimport difflib\nfrom typing import Any, List, Optional, Tuple\nimport pprint\n\ndef compute_unified_diff(\n    expected: str, \n    actual: str, \n    fromfile: str = \"expected\",\n    tofile: str = \"actual\",\n    n: int = 3\n) -> str:\n    \"\"\"Compute unified diff between two strings.\n    \n    Args:\n        expected: Expected string value\n        actual: Actual string value  \n        fromfile: Label for expected in diff header\n        tofile: Label for actual in diff header\n        n: Number of context lines\n        \n    Returns:\n        Unified diff string or empty string if strings are identical\n    \"\"\"\n    if expected == actual:\n        return \"\"\n    \n    expected_lines = expected.splitlines(keepends=True)\n    actual_lines = actual.splitlines(keepends=True)\n    \n    diff = difflib.unified_diff(\n        expected_lines, actual_lines,\n        fromfile=fromfile, tofile=tofile,\n        n=n, lineterm=''\n    )\n    \n    return '\\n'.join(diff)\n\n\ndef format_truncated(value: Any, max_length: int = 200) -> str:\n    \"\"\"Format value with intelligent truncation.\n    \n    Args:\n        value: Any value to format\n        max_length: Maximum string length before truncation\n        \n    Returns:\n        Formatted string, possibly truncated with ellipsis\n    \"\"\"\n    try:\n        formatted = pprint.pformat(value, width=70, depth=3)\n    except Exception:\n        formatted = repr(value)\n    \n    if len(formatted) > max_length:\n        # Truncate at the last whitespace before max_length\n        truncated = formatted[:max_length]\n        last_space = truncated.rfind(' ')\n        if last_space > max_length * 0.8:  # Reasonable break point\n            truncated = truncated[:last_space]\n        return f\"{truncated}... (length: {len(formatted)})\"\n    \n    return formatted\n\n\ndef diff_dicts(expected: dict, actual: dict) -> List[str]:\n    \"\"\"Compute differences between two dictionaries.\n    \n    Args:\n        expected: Expected dictionary\n        actual: Actual dictionary\n        \n    Returns:\n        List of difference descriptions\n    \"\"\"\n    differences = []\n    \n    # Keys only in expected\n    missing_keys = set(expected.keys()) - set(actual.keys())\n    if missing_keys:\n        differences.append(f\"Missing keys: {sorted(missing_keys)}\")\n    \n    # Keys only in actual  \n    extra_keys = set(actual.keys()) - set(expected.keys())\n    if extra_keys:\n        differences.append(f\"Extra keys: {sorted(extra_keys)}\")\n    \n    # Common keys with different values\n    common_keys = set(expected.keys()) & set(actual.keys())\n    for key in sorted(common_keys):\n        if expected[key] != actual[key]:\n            exp_str = format_truncated(expected[key], 100)\n            act_str = format_truncated(actual[key], 100)\n            differences.append(f\"Key '{key}': expected {exp_str}, got {act_str}\")\n    \n    return differences\n\n\ndef diff_lists(expected: list, actual: list) -> Optional[Tuple[int, Any, Any]]:\n    \"\"\"Find first difference between two lists.\n    \n    Args:\n        expected: Expected list\n        actual: Actual list\n        \n    Returns:\n        Tuple of (index, expected_value, actual_value) or None if identical\n    \"\"\"\n    min_len = min(len(expected), len(actual))\n    \n    for i in range(min_len):\n        if expected[i] != actual[i]:\n            return i, expected[i], actual[i]\n    \n    if len(expected) != len(actual):\n        # Different lengths but all elements up to min_len match\n        return min_len, \n            expected[min_len] if len(expected) > min_len else \"<end of list>\",\n            actual[min_len] if len(actual) > min_len else \"<end of list>\"\n    \n    return None\n```\n\n**`apollo/internal/comparison.py`** (complete working code):\n\n```python\n\"\"\"Type-specific comparison logic.\"\"\"\nimport math\nfrom typing import Any, Dict, Callable, Optional\nfrom enum import Enum\n\nclass ComparisonResult(Enum):\n    \"\"\"Result of a comparison operation.\"\"\"\n    EQUAL = \"equal\"\n    DIFFERENT = \"different\"\n    TYPE_MISMATCH = \"type_mismatch\"\n    APPROXIMATELY_EQUAL = \"approximately_equal\"\n\n\ndef compare_floats(\n    a: float, \n    b: float, \n    rel_tol: float = 1e-9,\n    abs_tol: float = 0.0\n) -> ComparisonResult:\n    \"\"\"Compare two floats with tolerance.\n    \n    Uses same logic as math.isclose() but returns detailed result.\n    \n    Args:\n        a: First float\n        b: Second float\n        rel_tol: Relative tolerance\n        abs_tol: Absolute tolerance\n        \n    Returns:\n        ComparisonResult indicating equality level\n    \"\"\"\n    if math.isnan(a) and math.isnan(b):\n        return ComparisonResult.EQUAL\n    if math.isinf(a) and math.isinf(b) and a == b:\n        return ComparisonResult.EQUAL\n        \n    diff = abs(a - b)\n    threshold = max(rel_tol * max(abs(a), abs(b)), abs_tol)\n    \n    if diff <= threshold:\n        return (ComparisonResult.EQUAL if diff == 0 \n                else ComparisonResult.APPROXIMATELY_EQUAL)\n    \n    return ComparisonResult.DIFFERENT\n\n\ndef are_similar_types(a: Any, b: Any) -> bool:\n    \"\"\"Check if two values have similar/convertible types.\n    \n    Useful for detecting 1 vs 1.0, \"1\" vs 1, etc.\n    \n    Args:\n        a: First value\n        b: Second value\n        \n    Returns:\n        True if types are similar (int/float, str/bytes, etc.)\n    \"\"\"\n    type_a, type_b = type(a), type(b)\n    \n    # Same type\n    if type_a is type_b:\n        return True\n    \n    # Numeric types\n    numeric_types = (int, float, complex)\n    if isinstance(a, numeric_types) and isinstance(b, numeric_types):\n        return True\n    \n    # String/bytes types\n    text_types = (str, bytes, bytearray)\n    if isinstance(a, text_types) and isinstance(b, text_types):\n        return True\n    \n    # Collection types with similar interfaces\n    list_like = (list, tuple, range)\n    if isinstance(a, list_like) and isinstance(b, list_like):\n        return True\n    \n    dict_like = (dict,)\n    if isinstance(a, dict_like) and isinstance(b, dict_like):\n        return True\n    \n    set_like = (set, frozenset)\n    if isinstance(a, set_like) and isinstance(b, set_like):\n        return True\n    \n    return False\n\n\nclass TypeComparatorRegistry:\n    \"\"\"Registry of type-specific comparison functions.\"\"\"\n    \n    def __init__(self):\n        self._comparators: Dict[type, Callable] = {}\n        self._default_comparator = lambda a, b: ComparisonResult.EQUAL if a == b else ComparisonResult.DIFFERENT\n        \n    def register(self, type_obj: type, comparator: Callable):\n        \"\"\"Register a comparator for a specific type.\"\"\"\n        self._comparators[type_obj] = comparator\n    \n    def get_comparator(self, value: Any) -> Callable:\n        \"\"\"Get appropriate comparator for a value's type.\"\"\"\n        # Check exact type\n        comparator = self._comparators.get(type(value))\n        if comparator:\n            return comparator\n        \n        # Check parent classes\n        for cls in type(value).__mro__:\n            if cls in self._comparators:\n                return self._comparators[cls]\n        \n        # Default\n        return self._default_comparator\n    \n    def compare(self, a: Any, b: Any, **kwargs) -> ComparisonResult:\n        \"\"\"Compare two values using registered comparators.\"\"\"\n        if type(a) is not type(b) and not are_similar_types(a, b):\n            return ComparisonResult.TYPE_MISMATCH\n        \n        comparator = self.get_comparator(a)\n        return comparator(a, b, **kwargs)\n\n\n# Global registry instance\ncomparator_registry = TypeComparatorRegistry()\n\n# Register built-in comparators\ncomparator_registry.register(float, \n    lambda a, b, **kw: compare_floats(a, b, \n        kw.get('rel_tol', 1e-9), \n        kw.get('abs_tol', 0.0)))\ncomparator_registry.register(int,\n    lambda a, b, **kw: compare_floats(float(a), float(b), \n        kw.get('rel_tol', 1e-9), \n        kw.get('abs_tol', 0.0)) if kw.get('tolerance') else \n        (ComparisonResult.EQUAL if a == b else ComparisonResult.DIFFERENT))\n```\n\n#### Core Logic Skeleton Code\n\n**`apollo/assertions/engine.py`** (skeleton with TODOs):\n\n```python\n\"\"\"Assertion engine core logic.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\nfrom enum import Enum\nimport traceback\n\nfrom ..internal.diff import compute_unified_diff, format_truncated, diff_dicts, diff_lists\nfrom ..internal.comparison import ComparisonResult, comparator_registry\n\n\nclass AssertionType(Enum):\n    \"\"\"Type of assertion for categorization.\"\"\"\n    EQUALITY = \"equality\"\n    TRUTHINESS = \"truthiness\"\n    EXCEPTION = \"exception\"\n    COMPARISON = \"comparison\"\n    MEMBERSHIP = \"membership\"\n    TYPE_CHECK = \"type_check\"\n    COLLECTION = \"collection\"\n    CUSTOM = \"custom\"\n\n\n@dataclass\nclass AssertionFailure:\n    \"\"\"Detailed information about a failed assertion.\"\"\"\n    \n    message: str\n    expected: Any\n    actual: Any\n    diff: Optional[str] = None\n    hint: Optional[str] = None\n    assertion_type: AssertionType = AssertionType.CUSTOM\n    \n    def __str__(self) -> str:\n        \"\"\"Format the failure for display.\"\"\"\n        # TODO 1: Build a readable message with sections:\n        #   - Custom message (if provided)\n        #   - Expected vs Actual\n        #   - Diff (if available)\n        #   - Hint (if available)\n        # Use clear separators and indentation\n        pass\n\n\nclass AssertionError(Exception):\n    \"\"\"Custom assertion error with rich failure information.\"\"\"\n    \n    def __init__(self, failure: AssertionFailure):\n        self.failure = failure\n        super().__init__(str(failure))\n\n\nclass AssertionEngine:\n    \"\"\"Central engine for evaluating assertions.\"\"\"\n    \n    def assert_equal(\n        self,\n        actual: Any,\n        expected: Any,\n        msg: Optional[str] = None,\n        tolerance: Optional[float] = None,\n        rel_tol: float = 1e-9,\n        abs_tol: float = 0.0,\n        ignore_case: bool = False,\n        ignore_whitespace: bool = False,\n        unordered: bool = False\n    ) -> None:\n        \"\"\"Assert that actual equals expected with optional tolerance.\n        \n        Args:\n            actual: The actual value obtained\n            expected: The expected value\n            msg: Optional custom failure message\n            tolerance: Absolute tolerance for numeric comparison (shortcut)\n            rel_tol: Relative tolerance for numeric comparison\n            abs_tol: Absolute tolerance for numeric comparison\n            ignore_case: For strings, compare case-insensitively\n            ignore_whitespace: For strings, normalize whitespace\n            unordered: For collections, ignore order of elements\n            \n        Raises:\n            AssertionError: If values don't match with descriptive message\n        \"\"\"\n        # TODO 2: Normalize inputs based on flags\n        #   - If ignore_case and strings: convert both to lowercase\n        #   - If ignore_whitespace and strings: normalize whitespace\n        #   - If unordered and sequences: consider sorted versions\n        \n        # TODO 3: Determine comparison strategy based on types\n        #   - Use comparator_registry.compare() with tolerance parameters\n        #   - Handle tolerance parameter (convert to abs_tol)\n        \n        # TODO 4: Check comparison result\n        #   - If EQUAL or APPROXIMATELY_EQUAL: return None (success)\n        #   - If DIFFERENT or TYPE_MISMATCH: proceed to failure handling\n        \n        # TODO 5: Generate failure details\n        #   - Compute diff based on types (strings, lists, dicts, etc.)\n        #   - Create hint based on common mistakes\n        #   - Format expected/actual with truncation\n        \n        # TODO 6: Build AssertionFailure object\n        #   - Include custom message if provided\n        #   - Set assertion_type to AssertionType.EQUALITY\n        \n        # TODO 7: Raise AssertionError with the failure\n        pass\n    \n    def assert_true(self, condition: Any, msg: Optional[str] = None) -> None:\n        \"\"\"Assert that condition is truthy.\n        \n        Args:\n            condition: Value to check for truthiness\n            msg: Optional custom failure message\n            \n        Raises:\n            AssertionError: If condition is falsy\n        \"\"\"\n        # TODO 8: Convert condition to bool using bool()\n        # TODO 9: If falsy, build failure message showing actual value\n        # TODO 10: Include common falsy values in hint (0, '', [], None, False)\n        pass\n    \n    def assert_raises(\n        self,\n        expected_exception: type,\n        *args,\n        **kwargs\n    ):\n        \"\"\"Context manager to assert that code raises an exception.\n        \n        Can be used as:\n            with assert_raises(ValueError):\n                int(\"not a number\")\n                \n        Or as function call (legacy):\n            assert_raises(ValueError, int, \"not a number\")\n            \n        Args:\n            expected_exception: Exception type expected to be raised\n            *args: If provided, call the first arg as function with remaining args\n            **kwargs: May include 'msg' for custom message or 'match' for regex\n            \n        Returns:\n            Context manager or raises AssertionError\n        \"\"\"\n        # TODO 11: If args provided, use legacy function-call mode\n        #   - First arg is callable, rest are args to call it\n        #   - Call it and check for exception\n        \n        # TODO 12: Otherwise, return context manager instance\n        #   - The context manager should implement __enter__ and __exit__\n        #   - __exit__ should check if correct exception was raised\n        #   - Store exception in context manager for inspection\n        \n        # TODO 13: Support 'match' regex parameter to check exception message\n        pass\n    \n    def _format_failure_message(\n        self,\n        expected_desc: str,\n        actual_desc: str,\n        diff: Optional[str] = None,\n        hint: Optional[str] = None,\n        custom_msg: Optional[str] = None\n    ) -> str:\n        \"\"\"Format a consistent failure message.\n        \n        Args:\n            expected_desc: Description of expected value/condition\n            actual_desc: Description of actual value\n            diff: Optional diff output\n            hint: Optional hint for debugging\n            custom_msg: Optional custom message from user\n            \n        Returns:\n            Formatted failure message\n        \"\"\"\n        # TODO 14: Build message with clear sections\n        #   - Start with custom_msg if provided\n        #   - Expected: expected_desc\n        #   - Actual: actual_desc  \n        #   - Diff if available (with separator)\n        #   - Hint if available\n        # Use consistent indentation and line breaks\n        pass\n    \n    def _compute_diff(self, expected: Any, actual: Any) -> Optional[str]:\n        \"\"\"Compute appropriate diff based on value types.\n        \n        Args:\n            expected: Expected value\n            actual: Actual value\n            \n        Returns:\n            Diff string or None if not applicable\n        \"\"\"\n        # TODO 15: Handle different types:\n        #   - Strings: use compute_unified_diff()\n        #   - Lists: show first differing element with index\n        #   - Dicts: use diff_dicts() and format results\n        #   - Other: try repr() and string diff\n        pass\n\n\n# Global engine instance for convenience\nengine = AssertionEngine()\n\n# Public API functions\ndef assert_equal(actual: Any, expected: Any, msg: Optional[str] = None, **kwargs) -> None:\n    \"\"\"Public API for assert_equal.\"\"\"\n    return engine.assert_equal(actual, expected, msg, **kwargs)\n\ndef assert_true(condition: Any, msg: Optional[str] = None) -> None:\n    \"\"\"Public API for assert_true.\"\"\"\n    return engine.assert_true(condition, msg)\n\ndef assert_raises(expected_exception: type, *args, **kwargs):\n    \"\"\"Public API for assert_raises.\"\"\"\n    return engine.assert_raises(expected_exception, *args, **kwargs)\n```\n\n**`apollo/matchers/base.py`** (skeleton with TODOs):\n\n```python\n\"\"\"Base classes for matcher API.\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Optional\n\n\nclass BaseMatcher(ABC):\n    \"\"\"Abstract base class for all matchers.\"\"\"\n    \n    @abstractmethod\n    def __matches__(self, actual: Any) -> bool:\n        \"\"\"Check if actual value matches the condition.\n        \n        Args:\n            actual: The value to test\n            \n        Returns:\n            True if actual satisfies the matcher's condition\n        \"\"\"\n        pass\n    \n    @abstractmethod \n    def __describe__(self) -> str:\n        \"\"\"Describe what the matcher expects.\n        \n        Returns:\n            String description of expected condition\n        \"\"\"\n        pass\n    \n    def __describe_mismatch__(self, actual: Any) -> str:\n        \"\"\"Describe why actual value didn't match.\n        \n        Override for more specific mismatch messages.\n        \n        Args:\n            actual: The actual value that didn't match\n            \n        Returns:\n            String description of mismatch\n        \"\"\"\n        return f\"was {repr(actual)}\"\n    \n    def __and__(self, other: 'BaseMatcher') -> 'AllOf':\n        \"\"\"Operator overload for & (AND).\"\"\"\n        # TODO 1: Return AllOf matcher containing self and other\n        pass\n    \n    def __or__(self, other: 'BaseMatcher') -> 'AnyOf':\n        \"\"\"Operator overload for | (OR).\"\"\"\n        # TODO 2: Return AnyOf matcher containing self and other\n        pass\n    \n    def __invert__(self) -> 'Not':\n        \"\"\"Operator overload for ~ (NOT).\"\"\"\n        # TODO 3: Return Not matcher wrapping self\n        pass\n    \n    def __rand__(self, other: Any) -> 'AllOf':\n        \"\"\"Reverse AND operator.\"\"\"\n        # TODO 4: Handle case where other is not a matcher\n        #   - Try to convert other to a matcher (e.g., EqualTo(other))\n        #   - Then return AllOf of converted and self\n        pass\n    \n    def __ror__(self, other: Any) -> 'AnyOf':\n        \"\"\"Reverse OR operator.\"\"\"\n        # TODO 5: Similar to __rand__ but for OR\n        pass\n\n\nclass AllOf(BaseMatcher):\n    \"\"\"Matcher that requires ALL sub-matchers to match.\"\"\"\n    \n    def __init__(self, *matchers: BaseMatcher):\n        # TODO 6: Store matchers in instance variable\n        pass\n    \n    def __matches__(self, actual: Any) -> bool:\n        # TODO 7: Check that ALL matchers match actual\n        #   - Return False as soon as one fails\n        #   - Keep track of which failed for better messages\n        pass\n    \n    def __describe__(self) -> str:\n        # TODO 8: Describe as \"all of: [matcher1, matcher2, ...]\"\n        #   - Handle single matcher case specially\n        pass\n    \n    def __describe_mismatch__(self, actual: Any) -> str:\n        # TODO 9: Provide detailed mismatch showing which sub-matcher failed\n        #   - Test each matcher\n        #   - For first failing matcher, include its mismatch description\n        pass\n\n\nclass AnyOf(BaseMatcher):\n    \"\"\"Matcher that requires ANY sub-matcher to match.\"\"\"\n    \n    def __init__(self, *matchers: BaseMatcher):\n        # TODO 10: Store matchers in instance variable  \n        pass\n    \n    def __matches__(self, actual: Any) -> bool:\n        # TODO 11: Check if ANY matcher matches actual\n        #   - Return True as soon as one matches\n        pass\n    \n    def __describe__(self) -> str:\n        # TODO 12: Describe as \"any of: [matcher1, matcher2, ...]\"\n        pass\n    \n    def __describe_mismatch__(self, actual: Any) -> str:\n        # TODO 13: Show that NONE of the matchers matched\n        #   - Include mismatch from each matcher (could be long)\n        #   - Consider truncation for many matchers\n        pass\n\n\nclass Not(BaseMatcher):\n    \"\"\"Matcher that inverts another matcher.\"\"\"\n    \n    def __init__(self, matcher: BaseMatcher):\n        # TODO 14: Store wrapped matcher\n        pass\n    \n    def __matches__(self, actual: Any) -> bool:\n        # TODO 15: Return NOT of wrapped matcher's match result\n        pass\n    \n    def __describe__(self) -> str:\n        # TODO 16: Describe as \"not: [description of wrapped matcher]\"\n        pass\n    \n    def __describe_mismatch__(self, actual: Any) -> str:\n        # TODO 17: Special case: if NOT fails, the wrapped matcher matched\n        #   - Return something like \"unexpectedly matched: [description]\"\n        pass\n\n\ndef assert_that(actual: Any, matcher: BaseMatcher, msg: Optional[str] = None) -> None:\n    \"\"\"Assert that actual value satisfies the matcher.\n    \n    Args:\n        actual: Value to test\n        matcher: Matcher defining the condition\n        msg: Optional custom failure message\n        \n    Raises:\n        AssertionError: If actual doesn't match matcher\n    \"\"\"\n    # TODO 18: Check if matcher matches actual\n    # TODO 19: If not, get descriptions from matcher\n    # TODO 20: Build failure message using matcher.__describe__() and \n    #          matcher.__describe_mismatch__()\n    # TODO 21: Include custom message if provided\n    # TODO 22: Raise AssertionError with formatted message\n    pass\n```\n\n#### Language-Specific Hints (Python)\n\n- **Float Comparisons**: Use `math.isclose()` for approximate equality with `rel_tol` and `abs_tol` parameters. For very small numbers near zero, absolute tolerance is crucial.\n- **Exception Context Managers**: Implement `__enter__` returning `self` and `__exit__(self, exc_type, exc_val, exc_tb)` that returns `True` if exception was handled (suppressed), `False` to propagate.\n- **Operator Overloading**: Matchers use `__and__`, `__or__`, `__invert__` for composition. Also implement `__rand__` and `__ror__` to handle `value & matcher` syntax.\n- **Recursion Protection**: Use `sys.getrecursionlimit()` and `functools.lru_cache` or manual visited-sets when comparing nested structures.\n- **Type Inspection**: `isinstance()` is more flexible than `type() ==` for inheritance. Use `typing.get_args()` and `typing.get_origin()` for generic type checking.\n- **String Formatting**: Use f-strings with `!r` for `repr()`: `f\"Expected {expected!r}, got {actual!r}\"`.\n- **Diff Computation**: `difflib.unified_diff()` is good for strings; for nested structures, implement recursive diff with path tracking.\n\n#### Milestone Checkpoint\n\nAfter implementing Milestone 2, you should be able to run:\n\n```bash\n# Create a test file test_assertions.py\npython -m apollo.cli test_assertions.py\n```\n\n**Expected Behavior**:\n- Tests using `assert_equal()`, `assert_true()`, `assert_raises()` should pass or fail with descriptive messages.\n- Collection assertions should show element-by-element differences.\n- Float comparisons with tolerance should work correctly.\n- Matcher API: `assert_that(value, EqualTo(5))` should work.\n- Custom matchers should be creatable and composable.\n\n**Verification Tests** (create in `test_assertions.py`):\n\n```python\n# These should PASS\nassert_equal(1, 1)\nassert_equal(0.1 + 0.2, 0.3, tolerance=1e-10)\nassert_true(len([1,2,3]) > 0)\nwith assert_raises(ValueError):\n    int(\"not a number\")\nassert_that(5, EqualTo(5))\n\n# These should FAIL with good messages\nassert_equal([1,2,3], [1,2,4])  # Should show element 2: 3 != 4\nassert_equal({\"a\": 1}, {\"a\": 2})  # Should show key 'a' difference\nassert_equal(0.1 + 0.2, 0.3)  # Should fail without tolerance\nassert_that(\"hello\", Contains(\"z\"))  # Should say \"expected to contain 'z'\"\n```\n\n**Signs Something is Wrong**:\n- Failure messages show only `<object at 0x...>` → Missing `repr()` or formatting logic.\n- Float tests always fail → Tolerance not applied or wrong default.\n- Matcher composition `matcher1 & matcher2` errors → Operator overloading not implemented.\n- Exception assertions don't catch exceptions → Context manager `__exit__` logic incorrect.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Assertion passes but prints error message anyway | Message generation happens before checking equality | Add debug print to see if message generation has side effects or is called unconditionally | Generate messages lazily, only on failure |\n| Float comparison inconsistent | Wrong tolerance type (absolute vs relative) or wrong defaults | Print the actual diff: `abs(a-b)` and tolerance threshold | Use `math.isclose()` logic with both rel_tol and abs_tol |\n| Matcher `&` operator doesn't work with non-matcher on left | Missing `__rand__` implementation | Try `5 & EqualTo(5)` vs `EqualTo(5) & 5` | Implement `__rand__` that converts left operand to matcher |\n| Circular structure comparison crashes | Infinite recursion in nested comparison | Add print statement showing path being compared | Maintain set of `id()` of objects already being compared |\n| Exception context lost in `assert_raises` | Using `raise ... from None` incorrectly | Check traceback - does it show test code or assertion code? | Preserve original traceback in exception chaining |\n| Diff output is huge for large objects | No truncation logic | Print size of diff string | Implement intelligent truncation in `format_truncated()` |\n| Matcher describes itself with object address | Missing `__describe__` implementation or using default `repr` | Check if matcher class implements `__describe__` | Implement `__describe__` returning meaningful string |\n| `assert_that(None, HasAttribute(\"x\"))` crashes | Matcher assumes attribute exists without checking | Add try-except in `__matches__` or check with `hasattr()` | Use `getattr(actual, name, None)` with sentinel |\n\n\n## 7. Component Design: Fixtures & Setup/Teardown (Milestone 3)\n\n> **Milestone(s):** Milestone 3 (Fixtures & Setup/Teardown)\n\nThe **Fixture System** transforms the test framework from a simple sequence runner into a sophisticated resource management platform. While the runner handles the *execution* of tests and the assertion engine handles the *verification*, the fixture system handles the *preparation* and *cleanup*—creating the necessary environment for tests to run while ensuring no test pollutes another's environment. This milestone introduces one of the most powerful patterns in modern testing: dependency injection with automatic resource lifecycle management.\n\nThink of fixtures as the **\"stage crew\"** for your test performance. Before the actors (tests) take the stage, the crew sets up the scenery, props, and lighting. After each scene, they reset everything for the next performance, and after the entire play, they strike the set and return the theater to its original state. The fixture system is this crew, working behind the scenes to create, maintain, and dismantle test environments according to a precise schedule defined by **scope**.\n\n### Fixture System Architecture\n\nThe fixture architecture centers on two core challenges: **lifecycle management** (when to create and destroy resources) and **dependency resolution** (how to satisfy a test's resource requirements). The system must track resources, cache them appropriately based on scope, ensure cleanup even when tests fail, and resolve complex dependency graphs where one fixture depends on another.\n\n> **Mental Model: The Resource Pool Manager**\n>\n> Imagine a hotel with different room types (scopes). A \"function-scoped\" room is cleaned after every guest (test). A \"module-scoped\" suite is cleaned only after all guests from a particular group (test module) have left. A \"session-scoped\" banquet hall is set up once for the entire event (test run) and cleaned at the very end. The fixture registry is the hotel manager who tracks which rooms are occupied, when to clean them, and which guests need which amenities (dependencies).\n\n#### Core Components\n\nThe fixture system comprises three primary components working together:\n\n1.  **Fixture Registry**: A central catalog of all available fixtures, mapping fixture names to their factory functions and metadata. It's the system's \"phone book\" for resource lookup.\n2.  **Fixture Cache**: A scope-aware storage that holds created fixture instances, preventing expensive re-creation. It's the system's \"warehouse\" for ready-to-use resources.\n3.  **Fixture Lifecycle Manager**: The orchestration engine that sequences creation, caching, injection, and teardown based on scope boundaries. It's the system's \"conductor\" for resource lifecycle events.\n\nThese components collaborate through a well-defined data flow: when a test requests a fixture, the lifecycle manager checks the cache; if not present, it creates the fixture (resolving its dependencies recursively), stores it in the cache, injects it into the test, and schedules its eventual teardown at the appropriate scope boundary.\n\n#### Data Structures\n\nThe fixture system extends our existing data model with several new types:\n\n| Name | Type | Description |\n|------|------|-------------|\n| `Fixture` | Class/Struct | Metadata and factory for a single fixture. |\n| `FixtureScope` | Enum | Defines the lifetime of a fixture instance. Values: `FUNCTION`, `CLASS`, `MODULE`, `SESSION`. |\n| `FixtureRequest` | Class/Struct | A request for a specific fixture instance, containing the context needed to create/cache it. |\n| `FixtureCacheKey` | Tuple | A unique identifier for a cached fixture instance, typically `(fixture_name, scope, scope_id)` |\n\n**Fixture Definition Table:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `name` | `str` | The unique identifier used to request this fixture (matches test parameter name). |\n| `func` | `Callable` | The factory function that creates the fixture value. May be a generator for teardown. |\n| `scope` | `FixtureScope` | The lifetime scope: `FUNCTION` (per test), `CLASS` (per test class), `MODULE` (per module), `SESSION` (entire run). |\n| `dependencies` | `List[str]` | Names of other fixtures this fixture requires, resolved before this fixture's `func` is called. |\n\n**FixtureRequest Definition Table:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `fixture_name` | `str` | Name of the fixture being requested. |\n| `scope` | `FixtureScope` | The scope at which this fixture is being requested. |\n| `test_case` | `Optional[TestCase]` | The test case that triggered this request (for `FUNCTION` and `CLASS` scoping). |\n| `cache_key` | `Tuple` | Computed key used to store/retrieve the fixture from cache: `(fixture_name, scope, scope_id)` where `scope_id` is derived from the test/module. |\n\n> **Architecture Decision: Generator-Based Fixtures for Cleanup**\n>\n> **Context**: Fixtures often represent resources requiring cleanup (database connections, temporary files). We need a reliable mechanism to execute teardown logic regardless of test outcome.\n>\n> **Options Considered**:\n> 1. **Separate `setup()`/`teardown()` functions**: Require users to define two separate functions, manually paired by name.\n> 2. **Class-based fixtures with `__enter__`/`__exit__`**: Use context managers; requires wrapping fixture usage in `with` statements.\n> 3. **Generator functions**: Fixture function `yield`s the value; code after `yield` is teardown.\n>\n> **Decision**: Use generator functions where the fixture function contains a `yield` statement.\n>\n> **Rationale**: Generators provide a natural, linear flow: setup code before `yield`, teardown after. The framework can guarantee teardown execution by exhausting the generator (calling `next()` after the test). This pattern is intuitive (users write setup and teardown in one place) and widely adopted (pytest uses it).\n>\n> **Consequences**:\n> - **Enables**: Automatic cleanup even if test fails; simple syntax.\n> - **Trade-offs**: Slight complexity in framework to detect and handle generators; users must remember to `yield` exactly once.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Separate functions | Simple to implement; explicit. | Error-prone pairing; twice the boilerplate. | ❌ |\n| Context managers | Python-native pattern; strong guarantees. | Requires `with` in tests; breaks dependency injection. | ❌ |\n| Generator functions | Single function; automatic cleanup; intuitive. | Framework must handle generator lifecycle. | ✅ |\n\n#### Lifecycle Management by Scope\n\nEach scope defines a different caching and cleanup strategy. The lifecycle manager tracks \"scope boundaries\" (when all tests of a certain scope complete) to trigger teardown.\n\n**Scope Behavior Table:**\n\n| Scope | Creation Trigger | Cached Until | Typical Use |\n|-------|-----------------|--------------|-------------|\n| `FUNCTION` | Before each test that needs it. | After that test completes. | Fresh data for each test; no cross-test contamination. |\n| `CLASS` | Before first test in a test class that needs it. | After last test in that class completes. | Shared setup for all tests in a class (e.g., browser instance). |\n| `MODULE` | Before first test in a module that needs it. | After last test in that module completes. | Expensive setup shared across a file (e.g., database schema). |\n| `SESSION` | Once at start of test run. | After all tests complete. | Global resources (e.g., Docker container, API server). |\n\nThe sequence diagram `![Fixture Scope Lifecycle Sequence](./diagrams/fixture-scope-seq.svg)` illustrates the timing difference between `FUNCTION` and `MODULE` scopes across three tests.\n\n**Algorithm: Fixture Resolution and Creation**\n\nWhen a test requires a fixture, the lifecycle manager executes this recursive algorithm:\n\n1.  **Receive Request**: The manager receives a `FixtureRequest` for a fixture name within a specific scope context.\n2.  **Check Cache**: Compute the `cache_key` for this request. If the fixture exists in the cache for this key, return the cached instance.\n3.  **Resolve Dependencies**: For each fixture name in `Fixture.dependencies`, recursively call this algorithm (starting at step 1) to obtain the dependency values.\n4.  **Invoke Factory**: Call the fixture's `func` with the resolved dependencies as arguments.\n5.  **Handle Generator**:\n    a. If `func` is a generator (contains `yield`), advance it once to get the fixture value. Store the generator object for later teardown.\n    b. Otherwise, use the return value directly.\n6.  **Cache Instance**: Store the fixture value in the cache under its `cache_key`. For generator fixtures, also store the generator for later cleanup.\n7.  **Return Value**: Provide the fixture value to the requester (test or dependent fixture).\n\n**Algorithm: Scope Teardown**\n\nWhen a scope boundary is reached (e.g., after a test for `FUNCTION` scope, after a module's last test for `MODULE` scope), the lifecycle manager:\n\n1.  **Identify Fixtures to Teardown**: Find all cached fixtures where `scope` matches the ending boundary and `scope_id` matches the ending context.\n2.  **Reverse Order**: Process fixtures in reverse dependency order (dependents before their dependencies) to avoid tearing down a needed resource prematurely.\n3.  **Execute Teardown**:\n    a. For generator fixtures: call `next()` on the stored generator object (which executes code after the `yield`).\n    b. For non-generator fixtures: if they have a `__close__` or similar, call it (optional).\n4.  **Evict from Cache**: Remove the fixture from the cache.\n\n#### Common Pitfalls\n\n⚠️ **Pitfall: Forgetting to Handle Generator Exhaustion**\n- **Description**: When a generator fixture yields, the framework must call `next()` again to execute teardown code. Forgetting this leaves resources open.\n- **Why it's wrong**: Database connections stay open, temporary files aren't deleted, causing resource leaks and potentially affecting other tests.\n- **Fix**: Always check if the fixture function is a generator (using `inspect.isgeneratorfunction`). When caching the fixture value, store the generator object and register it for teardown at the appropriate scope boundary.\n\n⚠️ **Pitfall: Circular Dependencies**\n- **Description**: Fixture A depends on B, and B depends on A (directly or indirectly). The resolution algorithm enters infinite recursion.\n- **Why it's wrong**: The framework crashes with a recursion error or hangs, preventing any tests from running.\n- **Fix**: Detect cycles during fixture registration or resolution. Maintain a set of \"currently resolving\" fixtures; if a fixture is requested while already in this set, raise a clear error indicating the circular dependency path.\n\n⚠️ **Pitfall: Scope Leak (Fixture Used Beyond Its Lifetime)**\n- **Description**: A test holds a reference to a fixture value after the fixture has been torn down (e.g., storing a module-scoped database connection in a global variable accessed by a later session-scoped test).\n- **Why it's wrong**: The test may operate on a closed/resource, causing cryptic errors far from the actual problem.\n- **Fix**: The framework cannot fully prevent this, but it can emit warnings when fixture values are used after their scope ends. Document clearly that fixtures should not be stored beyond the test that receives them.\n\n### Dependency Injection Mechanism\n\nDependency injection is the magic that automatically provides fixtures to test functions. Instead of manually calling setup functions, tests simply declare what they need as parameters, and the framework fulfills those requests. This mechanism transforms test functions from isolated procedures into declarative specifications of their required context.\n\n> **Mental Model: The Restaurant Order**\n>\n> A test function is like a customer ordering a meal. The parameters (`def test_checkout(shopping_cart, payment_gateway)`) are the order items. The fixture system is the kitchen and waitstaff that prepare and deliver those items. The customer doesn't need to know how the kitchen creates the `shopping_cart` or where the `payment_gateway` comes from—they just receive what they asked for.\n\n#### Parameter Inspection\n\nThe injection process begins when the framework inspects a test function's signature to determine its requirements.\n\n**Algorithm: Parameter Extraction**\n\n1.  **Get Signature**: Use `inspect.signature(test_func)` to obtain a `Signature` object.\n2.  **Analyze Parameters**: Iterate through `signature.parameters`. For each parameter:\n    a. **Skip Special Parameters**: Ignore `self` (for methods), `cls`, or parameters with special meanings (e.g., `*args`, `**kwargs` unless specifically supported).\n    b. **Record Fixture Name**: The parameter's name is the fixture name to inject (e.g., `database`).\n3.  **Store Requirements**: Attach the list of fixture names to the `TestCase` object (in the `fixtures` field).\n\nThis process occurs during **test discovery** (Milestone 1) when `TestCase` objects are created. The `TestCase` thus carries both *what* to run (`func`) and *what it needs* (`fixtures`).\n\n#### Injection Process\n\nDuring test execution, the runner requests the fixture lifecycle manager to provide values for each required fixture name. The manager executes the resolution algorithm described earlier, returning a dictionary of `{fixture_name: value}`. The runner then calls the test function with these values as keyword arguments.\n\n**Algorithm: Test Execution with Injection**\n\n1.  **Pre-execution**: For a given `TestCase`, the runner extracts its `fixtures` list.\n2.  **Request Fixtures**: For each fixture name, create a `FixtureRequest` with appropriate scope (based on fixture definition and test context) and send to the lifecycle manager.\n3.  **Receive Values**: Obtain a dictionary mapping fixture names to their instantiated values.\n4.  **Call Test Function**: Execute `test_func(**fixture_values)`.\n5.  **Post-execution**: After test completes (success or failure), notify the lifecycle manager that the test's scope (e.g., `FUNCTION`) is ending, triggering teardown of appropriate fixtures.\n\nThis process is illustrated in the sequence diagram `![Test Execution Sequence](./diagrams/test-execution-seq.svg)`.\n\n> **Architecture Decision: Implicit vs. Explicit Fixture Registration**\n>\n> **Context**: How does the framework know about available fixtures? Should users explicitly register them, or should the framework discover them automatically?\n>\n> **Options Considered**:\n> 1. **Explicit Registration**: Users call a decorator or function to register fixtures in a central location.\n> 2. **Implicit Discovery**: The framework scans modules for functions decorated with `@fixture` (or named with a convention like `fixture_*`).\n>\n> **Decision**: Use implicit discovery via a decorator (e.g., `@apollo.fixture`).\n>\n> **Rationale**: Implicit discovery aligns with the \"convention-over-configuration\" philosophy of the overall framework, reducing boilerplate. A decorator is lightweight, clearly marks fixture functions, and can attach metadata (like `scope`). It also mirrors successful patterns in pytest.\n>\n> **Consequences**:\n> - **Enables**: Clean, declarative fixture definition near where it's used.\n> - **Trade-offs**: Requires module imports during discovery to find decorators; slightly more complex discovery logic.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Explicit Registration | Clear central catalog; no import side effects. | More boilerplate; fixtures distant from usage. | ❌ |\n| Implicit Discovery (decorator) | Minimal boilerplate; fixtures colocated with tests. | Requires importing modules; decorator must be processed. | ✅ |\n\n#### Integration with Setup/Teardown Hooks\n\nThe fixture system coexists with classic xUnit-style `setUp`/`tearDown` methods. They serve different purposes: fixtures are for **resource provisioning** (providing values to tests), while hooks are for **environment setup** (actions performed before/after tests). The framework executes them in a defined order:\n\n1.  **Module-level fixtures** (scope=`MODULE`) setup\n2.  **Class-level fixtures** (scope=`CLASS`) setup\n3.  **`setUpClass`** method (if present)\n4.  **Function-level fixtures** (scope=`FUNCTION`) setup\n5.  **`setUp`** method (if present)\n6.  **Test execution** (with injected fixtures)\n7.  **`tearDown`** method (if present)\n8.  **Function-level fixtures** teardown\n9.  **`tearDownClass`** method (if present)\n10. **Class-level fixtures** teardown\n11. **Module-level fixtures** teardown\n\nThis order ensures that fixtures are available during `setUp` and `tearDown` if needed, and that cleanup happens in reverse order of setup.\n\n### Implementation Guidance\n\n**Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Fixture Discovery | Scan module `__dict__` for functions with `_is_fixture` attribute set by decorator. | Use `inspect.getmembers()` with predicate checking for decorator metadata. |\n| Generator Detection | `inspect.isgeneratorfunction(func)` before calling. | Also handle async generators (`inspect.isasyncgenfunction`). |\n| Cache Storage | Nested dictionaries: `cache[scope][scope_id][fixture_name] = value`. | Use weak references for cached values to avoid preventing garbage collection. |\n| Scope Identification | For `MODULE`: use `test_case.file_path`. For `CLASS`: use class name. | Create a hierarchical scope tree to manage nested scopes (e.g., session → module → class → function). |\n\n**Recommended File/Module Structure:**\n\n```\napollo/\n├── __init__.py              # Public API: fixture decorator, hooks\n├── runner.py                # Enhanced runner with fixture support\n├── fixtures/                # Fixture subsystem\n│   ├── __init__.py\n│   ├── registry.py          # FixtureRegistry: stores fixture definitions\n│   ├── cache.py             # FixtureCache: scope-aware caching\n│   ├── lifecycle.py         # FixtureLifecycleManager: creation/teardown orchestration\n│   └── scopemanager.py      # ScopeManager: tracks scope boundaries\n├── discovery.py             # Enhanced to detect fixtures and parameter requirements\n└── hooks.py                 # setUp/tearDown hook execution\n```\n\n**Infrastructure Starter Code:**\n\nHere's a complete, ready-to-use implementation of the fixture decorator and registry. Place this in `apollo/fixtures/registry.py`:\n\n```python\nimport inspect\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional, Union\nfrom dataclasses import dataclass, field\n\nfrom ..models import TestCase\n\nclass FixtureScope(Enum):\n    \"\"\"Lifetime scope for a fixture.\"\"\"\n    FUNCTION = \"function\"\n    CLASS = \"class\"\n    MODULE = \"module\"\n    SESSION = \"session\"\n\n@dataclass\nclass Fixture:\n    \"\"\"Metadata and factory for a fixture.\"\"\"\n    name: str\n    func: Callable\n    scope: FixtureScope = FixtureScope.FUNCTION\n    dependencies: List[str] = field(default_factory=list)\n\nclass FixtureRegistry:\n    \"\"\"Central registry for fixture definitions.\"\"\"\n    \n    def __init__(self):\n        self._fixtures: Dict[str, Fixture] = {}\n        self._discovered = False\n    \n    def register(self, fixture: Fixture) -> None:\n        \"\"\"Register a fixture definition.\"\"\"\n        if fixture.name in self._fixtures:\n            raise ValueError(f\"Fixture '{fixture.name}' already registered\")\n        self._fixtures[fixture.name] = fixture\n    \n    def get(self, name: str) -> Fixture:\n        \"\"\"Get a fixture definition by name.\"\"\"\n        if name not in self._fixtures:\n            raise KeyError(f\"Fixture '{name}' not found\")\n        return self._fixtures[name]\n    \n    def scan_module(self, module) -> None:\n        \"\"\"Scan a module for fixture decorators and register them.\"\"\"\n        for attr_name in dir(module):\n            attr = getattr(module, attr_name)\n            if callable(attr) and hasattr(attr, '_is_apollo_fixture'):\n                # Extract metadata stored by the decorator\n                fixture_name = getattr(attr, '_fixture_name', attr_name)\n                scope = getattr(attr, '_fixture_scope', FixtureScope.FUNCTION)\n                dependencies = getattr(attr, '_fixture_dependencies', [])\n                \n                fixture = Fixture(\n                    name=fixture_name,\n                    func=attr,\n                    scope=scope,\n                    dependencies=dependencies\n                )\n                self.register(fixture)\n\n# Public decorator API\ndef fixture(func=None, *, scope: Union[str, FixtureScope] = \"function\", name: Optional[str] = None):\n    \"\"\"\n    Decorator to mark a function as a fixture.\n    \n    Args:\n        func: The fixture function (can be a generator).\n        scope: One of \"function\", \"class\", \"module\", \"session\".\n        name: Optional name for the fixture (defaults to function name).\n    \"\"\"\n    if isinstance(scope, str):\n        scope = FixtureScope(scope.lower())\n    \n    def decorator(f):\n        f._is_apollo_fixture = True\n        f._fixture_name = name or f.__name__\n        f._fixture_scope = scope\n        # Dependency extraction happens later during registration\n        f._fixture_dependencies = []\n        return f\n    \n    if func is None:\n        return decorator\n    return decorator(func)\n```\n\n**Core Logic Skeleton Code:**\n\nHere's the skeleton for the lifecycle manager, the heart of the fixture system. Place in `apollo/fixtures/lifecycle.py`:\n\n```python\nimport inspect\nfrom typing import Any, Dict, Generator, Optional, Tuple\nfrom .registry import FixtureRegistry, FixtureScope, Fixture\nfrom ..models import TestCase\n\nclass FixtureLifecycleManager:\n    \"\"\"Orchestrates fixture creation, caching, and teardown.\"\"\"\n    \n    def __init__(self, registry: FixtureRegistry):\n        self.registry = registry\n        self._cache: Dict[Tuple, Any] = {}  # (name, scope, scope_id) -> value\n        self._generators: Dict[Tuple, Generator] = {}  # For teardown\n        self._active_requests = set()  # For cycle detection\n    \n    def get_fixture_value(self, fixture_name: str, test_case: Optional[TestCase] = None) -> Any:\n        \"\"\"\n        Get the value of a fixture for a given test context.\n        \n        Args:\n            fixture_name: Name of the fixture to resolve.\n            test_case: The test case requesting the fixture (for scope context).\n            \n        Returns:\n            The fixture value.\n            \n        Raises:\n            RuntimeError: If circular dependency detected.\n            KeyError: If fixture not found.\n        \"\"\"\n        # TODO 1: Determine scope context from test_case and fixture definition\n        #   - Get fixture definition from registry\n        #   - Determine actual scope (fixture's scope, but test_case provides context)\n        #   - Compute scope_id based on scope (e.g., module path for MODULE, test class name for CLASS)\n        \n        # TODO 2: Create cache key: (fixture_name, scope, scope_id)\n        \n        # TODO 3: Check cache; if found, return cached value\n        \n        # TODO 4: Check for circular dependency: if fixture_name in self._active_requests,\n        #   raise RuntimeError with the dependency path\n        \n        # TODO 5: Add fixture_name to self._active_requests\n        \n        # TODO 6: Resolve dependencies recursively:\n        #   For each dep_name in fixture.dependencies:\n        #       dep_value = self.get_fixture_value(dep_name, test_case)\n        #   Collect dep_values in a dict\n        \n        # TODO 7: Invoke fixture factory with dependencies as kwargs\n        #   - If inspect.isgeneratorfunction(fixture.func): handle specially\n        #   - Call fixture.func(**dep_values)\n        #   - If generator: get first yield value; store generator in self._generators\n        #   - Else: use return value\n        \n        # TODO 8: Store value in cache with cache key\n        # TODO 9: Remove fixture_name from self._active_requests\n        # TODO 10: Return the fixture value\n        pass\n    \n    def teardown_scope(self, scope: FixtureScope, scope_id: str) -> None:\n        \"\"\"\n        Teardown all fixtures of a given scope and scope_id.\n        \n        Args:\n            scope: The scope level to teardown.\n            scope_id: The specific context identifier.\n        \"\"\"\n        # TODO 1: Collect all cache keys for this (scope, scope_id)\n        #   Note: Need to iterate through self._cache and self._generators\n        \n        # TODO 2: For each fixture in reverse dependency order (implement dependency tracking):\n        #   - If fixture has generator: call next(generator, None) to execute teardown code\n        #   - Remove from self._generators\n        #   - Remove from self._cache\n        #   - Optional: call close() or similar on value if it exists\n        pass\n```\n\n**Language-Specific Hints:**\n\n- Use `inspect.signature(func).parameters` to get parameter names and defaults.\n- Use `inspect.isgeneratorfunction(func)` to detect generator fixtures before calling them.\n- Use `functools.lru_cache` on scope ID computation functions for performance.\n- For safe generator teardown: `try: next(gen) \\ except StopIteration: pass`.\n- Store fixture dependencies by analyzing the fixture function's parameters during registration (enhance the decorator to extract them).\n\n**Milestone Checkpoint:**\n\nAfter implementing the fixture system, verify with this test structure:\n\nCreate `test_fixtures.py`:\n```python\nimport apollo\n\n@apollo.fixture(scope=\"module\")\ndef database():\n    print(\"Setting up database\")\n    yield {\"connected\": True}\n    print(\"Tearing down database\")\n\n@apollo.fixture\ndef user(database):\n    return {\"name\": \"Alice\", \"db\": database}\n\ndef test_user_creation(user):\n    assert user[\"name\"] == \"Alice\"\n    assert user[\"db\"][\"connected\"] == True\n\ndef test_database_available(database):\n    assert database[\"connected\"] == True\n```\n\nRun with:\n```bash\n$ python -m apollo test_fixtures.py -v\n```\n\n**Expected Output:**\n```\ntest_fixtures.py::test_user_creation\nSetting up database\nPASSED\ntest_fixtures.py::test_database_available\nPASSED\nTearing down database\n\n2 passed in 0.01s\n```\n\n**Verification:**\n- Database setup appears only once (module scope).\n- Both tests pass and receive their fixtures.\n- Database teardown happens after both tests complete.\n- Try adding a third test without the fixture parameter to ensure non-fixture tests still work.\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"Fixture 'x' not found\" error | Fixture not registered or name mismatch. | Check decorator is applied; check fixture name vs. test parameter name. | Ensure `@apollo.fixture` decorator is used; parameter name matches fixture function name. |\n| Database not cleaned up after tests | Generator teardown not called. | Add print statements before/after yield; see if teardown prints appear. | Ensure `teardown_scope` is called at appropriate times and generators are advanced. |\n| Tests hang during fixture resolution | Circular dependency. | Print `self._active_requests` during resolution. | Implement cycle detection; rework fixture dependencies to remove cycle. |\n| Fixture created multiple times per module | Wrong `scope_id` computation. | Print `scope_id` for each fixture request. | Ensure `scope_id` for MODULE scope uses `test_case.file_path` consistently. |\n| `setUp` method cannot access fixtures | Fixtures injected after `setUp` runs. | Check execution order. | Document that fixtures are injected into test method, not `setUp`. Use class-level fixtures instead. |\n\n\n## 8. Component Design: Reporting & CLI (Milestone 4)\n\n> **Milestone(s):** Milestone 4 (Reporting & CLI)\n\nThe **Reporting & CLI** subsystem transforms the test framework from a collection of internal components into a usable tool that developers interact with daily. This subsystem provides the **control panel** (CLI) for configuring test runs and the **performance review** (Reporter) that delivers actionable feedback on test outcomes. While earlier milestones focused on the framework's internal machinery—finding tests, executing them, verifying conditions, and managing resources—this milestone delivers the user-facing interface that makes the framework practical and professional. The CLI must be intuitive yet powerful, accepting various patterns and filters, while the Reporter must produce clear human-readable output and machine-readable formats compatible with CI/CD pipelines.\n\n### Mental Model: The Control Panel and Performance Review\n\nImagine the test framework as a sophisticated laboratory testing facility. The **CLI Parser** is the **control panel** where technicians input experiment parameters: which samples to test (file patterns), what specific tests to run (name filters), how detailed the report should be (verbosity), and where to send the results (output formats). The panel has dials, switches, and input fields that translate human intentions into precise machine instructions.\n\nThe **Reporter** is the **performance review** system that analyzes each test's execution. It acts as both a quality inspector who examines each test result in detail and a statistician who compiles aggregate metrics. For human consumption, it produces a clear summary report with pass/fail indicators, execution times, and helpful diagnostic information. For machine integration, it generates standardized **JUnit XML** reports—think of these as laboratory certification documents that can be filed with regulatory systems (CI servers). This dual-output capability ensures the framework works both for interactive development and automated build pipelines.\n\n### CLI Parser\n\nThe **CLI Parser** serves as the framework's primary entry point, translating command-line arguments into a structured `Configuration` object that drives the entire test execution pipeline. Its design follows the principle of **convention-over-configuration** by providing sensible defaults while allowing extensive customization through flags.\n\n#### Architecture Decision: Command-Line Interface Design\n\n> **Decision: argparse-Based CLI with Rich Pattern Matching**\n> - **Context**: The framework needs to accept various input patterns (files, directories, test names) with flexible filtering while maintaining compatibility with shell globbing and developer expectations from tools like pytest.\n> - **Options Considered**:\n>   1. **Simple positional arguments only** – Accept only file/directory paths without flags\n>   2. **argparse with custom pattern resolution** – Use Python's standard `argparse` with our pattern resolution logic\n>   3. **Click library** – Use a third-party CLI framework for more sophisticated features\n> - **Decision**: Use **argparse with custom pattern resolution** (Option 2)\n> - **Rationale**: `argparse` is Python's standard library solution, requiring no external dependencies. It provides sufficient flexibility for our needs: flag parsing, help generation, and type validation. The custom pattern resolution layer allows us to implement the exact file matching semantics needed (supporting glob patterns, recursive directory scanning, and Python module name conversion) without being constrained by argparse's limited path handling.\n> - **Consequences**: We avoid third-party dependencies, keep the framework lightweight, and maintain full control over pattern resolution logic. The downside is implementing pattern resolution ourselves, but this is straightforward and aligns with the educational goals of understanding how test discovery works under the hood.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Simple positional arguments | Easy to implement, no flags to learn | Limited expressiveness, can't combine filters | ❌ |\n| argparse + custom resolution | Standard library, full control, extensible | Need to implement pattern logic ourselves | ✅ |\n| Click library | Rich features, beautiful help, decorator-based | External dependency, overkill for our needs | ❌ |\n\n#### Core Responsibilities\n\nThe CLI Parser has three primary responsibilities:\n\n1. **Argument Parsing**: Parse command-line arguments into a structured `Configuration` object with validated fields\n2. **Pattern Resolution**: Convert file patterns (like `tests/*.py` or `**/test_*.py`) into concrete filesystem paths\n3. **Default Application**: Apply sensible defaults when arguments are omitted (e.g., default to current directory)\n\n#### Configuration Data Structure\n\nThe `Configuration` object (defined in the Data Model section) serves as the complete specification for a test run. The CLI Parser's job is to populate this object from command-line input:\n\n| Field | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| `file_patterns` | `list[str]` | File/directory patterns to search for tests | `[\".\"]` (current directory) |\n| `start_dir` | `Path` | Base directory for relative pattern resolution | Current working directory |\n| `test_name_filters` | `list[str]` | Substring patterns to match against test names (nodeids) | `[]` (all tests) |\n| `verbose` | `bool` | Show detailed output including passing tests | `False` |\n| `quiet` | `bool` | Show only summary, suppress individual test output | `False` |\n| `output_format` | `str` | Output format: \"console\", \"junit-xml\", or \"both\" | `\"console\"` |\n| `parallel` | `bool` | Enable parallel test execution | `False` |\n| `max_workers` | `Optional[int]` | Maximum number of parallel workers (None = auto) | `None` |\n| `show_durations` | `bool` | Show execution time for each test | `False` |\n| `junit_xml_path` | `Optional[Path]` | Path to write JUnit XML report | `None` (stdout if \"junit-xml\" format) |\n\n#### Command-Line Interface Specification\n\nThe CLI supports the following flags and arguments:\n\n| Flag/Argument | Description | Example | Maps to Configuration Field |\n|---------------|-------------|---------|----------------------------|\n| positional `patterns` | File/directory/glob patterns to search | `tests/ test_*.py` | `file_patterns` |\n| `-k` `--filter` | Filter tests by name substring | `-k \"test_login\"` | `test_name_filters` |\n| `-v` `--verbose` | Verbose output | `-v` | `verbose=True` |\n| `-q` `--quiet` | Quiet mode (summary only) | `-q` | `quiet=True` |\n| `--junit-xml` | Generate JUnit XML output | `--junit-xml=results.xml` | `output_format=\"junit-xml\"`, `junit_xml_path=Path(...)` |\n| `--parallel` | Run tests in parallel | `--parallel` | `parallel=True` |\n| `-n` `--num-workers` | Number of parallel workers | `-n 4` | `max_workers=4` |\n| `--durations` | Show test execution times | `--durations` | `show_durations=True` |\n| `--collect-only` | Collect tests but don't execute | N/A | Special mode (not in Configuration) |\n| `--version` | Show framework version | `--version` | N/A |\n| `-h` `--help` | Show help message | `-h` | N/A |\n\n> **Design Insight**: The `-k` filter uses substring matching rather than regular expressions initially, following the principle of progressive disclosure of complexity. Advanced regex matching could be added later as an extension.\n\n#### Algorithm: Parsing and Configuration Building\n\nWhen a user runs `python -m apollo tests/ -v -k \"login\"`, the following sequence occurs:\n\n1. **Raw Argument Parsing**: The `parse_cli_args()` function receives `[\"tests/\", \"-v\", \"-k\", \"login\"]` and uses `argparse` to:\n   - Validate flag combinations (e.g., `-v` and `-q` are mutually exclusive)\n   - Convert string values to appropriate types\n   - Handle `--help` and `--version` specially (print and exit)\n\n2. **Pattern Resolution**: For each pattern in `file_patterns`:\n   - If pattern is a directory (or `.`), recursively find all `*.py` files\n   - If pattern contains `*` or `**`, expand using `glob.glob()` with `recursive=True`\n   - Convert relative paths to absolute relative to `start_dir`\n   - Filter out non-Python files and directories\n\n3. **Configuration Assembly**: Create and return a `Configuration` object with:\n   - Resolved file patterns\n   - Applied defaults for unspecified fields\n   - Validated constraints (e.g., `max_workers` must be positive if specified)\n\n4. **Special Mode Handling**: If `--collect-only` is specified, the framework will discover tests, display them, and exit without execution.\n\n#### Common Pitfalls\n\n⚠️ **Pitfall: Inconsistent Pattern Resolution Across Platforms**\n- **Description**: Using naive string matching for glob patterns that behaves differently on Windows vs Linux due to path separator differences (`\\` vs `/`).\n- **Why it's wrong**: Tests might be discovered on one OS but not another, causing CI failures.\n- **How to fix**: Use `pathlib.Path` objects throughout and `pathlib.Path.rglob()` for recursive pattern matching, which handles platform differences automatically.\n\n⚠️ **Pitfall: Overly Permissive Default Patterns**\n- **Description**: Defaulting to `**/*.py` might scan virtual environments, build directories, or other non-test code.\n- **Why it's wrong**: Slow discovery, potential import errors from non-module files, and accidental test collection.\n- **How to fix**: Default to `.` (current directory) only, and let users explicitly specify broader patterns. Implement smart exclusion of common non-test directories (like `__pycache__`, `.git`, `venv/*`) in the discovery phase.\n\n⚠️ **Pitfall: Missing Exit Code on Help/Version**\n- **Description**: Forgetting to exit after printing help or version information, causing the framework to proceed with test discovery.\n- **Why it's wrong**: User expects immediate exit; instead they see unexpected test output.\n- **How to fix**: Call `sys.exit(0)` immediately after printing help or version information in `parse_cli_args()`.\n\n### Reporter\n\nThe **Reporter** component transforms raw `TestResult` objects into human-readable summaries and machine-readable reports. It serves as the framework's communication channel back to the user, providing both immediate feedback during development and structured data for continuous integration systems.\n\n#### Architecture Decision: Dual-Format Reporting Strategy\n\n> **Decision: Separate Formatters with Shared Statistics Calculation**\n> - **Context**: The framework needs to output both human-friendly console reports and machine-readable JUnit XML, each with different formatting requirements but sharing the same underlying statistics.\n> - **Options Considered**:\n>   1. **Monolithic reporter with format branches** – Single class with `format_console()`, `format_junit()` methods\n>   2. **Strategy pattern with separate formatters** – Base `Formatter` interface with `ConsoleFormatter` and `JUnitFormatter` implementations\n>   3. **Visitor pattern over TestResults** – Formatters visit each result to build output incrementally\n> - **Decision**: Use **Strategy pattern with separate formatters** (Option 2)\n> - **Rationale**: The Strategy pattern cleanly separates concerns—each formatter focuses entirely on its output format without being polluted by unrelated logic. The shared statistics calculation can be extracted to a separate `StatisticsCollector` that both formatters use. This design is more maintainable and extensible than branching conditionals and avoids the complexity overhead of the Visitor pattern for our relatively simple data structure.\n> - **Consequences**: We get clean, testable formatter classes that can be extended independently (e.g., adding a JSON formatter later). The trade-off is slightly more boilerplate code to wire up the formatters.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Monolithic with branches | Simple, all logic in one place | Hard to test, violates SRP, hard to extend | ❌ |\n| Strategy pattern | Clean separation, extensible, testable | More classes to manage | ✅ |\n| Visitor pattern | Elegant for complex hierarchies, open/closed | Over-engineered for our needs, more complex | ❌ |\n\n#### Core Responsibilities\n\nThe Reporter has four primary responsibilities:\n\n1. **Statistics Calculation**: Aggregate test results to compute totals, success rates, and timing summaries\n2. **Console Formatting**: Generate human-readable output with colors, indentation, and appropriate verbosity levels\n3. **JUnit XML Generation**: Produce standardized XML reports compatible with CI/CD tools\n4. **Output Routing**: Write reports to appropriate destinations (stdout, files, or both)\n\n#### Reporter Internal Architecture\n\nThe Reporter comprises several collaborating classes:\n\n```\nReporter (Coordinator)\n├── StatisticsCollector (accumulates results)\n├── ConsoleFormatter (human output)\n├── JUnitFormatter (XML output)\n└── OutputWriter (handles file/stdout)\n```\n\n##### StatisticsCollector\n\nThe `StatisticsCollector` processes a stream of `TestResult` objects and maintains running statistics:\n\n| Statistic | Type | Description |\n|-----------|------|-------------|\n| `total` | `int` | Total tests discovered |\n| `passed` | `int` | Tests with status `PASSED` |\n| `failed` | `int` | Tests with status `FAILED` (assertion failures) |\n| `errored` | `int` | Tests with status `ERRORED` (unexpected exceptions) |\n| `skipped` | `int` | Tests with status `SKIPPED` |\n| `total_time` | `float` | Sum of all test durations in seconds |\n| `start_time` | `datetime` | When test run began |\n| `end_time` | `datetime` | When test run completed |\n| `by_module` | `Dict[str, ModuleStats]` | Statistics grouped by module |\n| `slowest_tests` | `List[Tuple[str, float]]` | List of (test_name, duration) for slowest tests |\n\n##### ConsoleFormatter\n\nThe `ConsoleFormatter` produces terminal output with these visual elements:\n\n1. **Progress Indicator**: For non-verbose mode, show `.` for pass, `F` for fail, `E` for error, `s` for skip\n2. **Verbose Output**: For `-v` flag, show each test name with status icon and duration\n3. **Failure Details**: After summary, show detailed tracebacks and assertion messages for failed/errored tests\n4. **Summary Section**: Clear totals with color-coded status counts\n5. **Slow Tests**: If `--durations` flag, list slowest tests\n\n**Status Icons and Colors** (using ANSI escape codes):\n- ✅ PASSED (green)\n- ❌ FAILED (red)\n- ⚠️ ERRORED (yellow/red)\n- ⏭️ SKIPPED (blue)\n- ⏳ RUNNING (cyan, for live progress)\n\n##### JUnitFormatter\n\nThe `JUnitFormatter` generates XML following the [JUnit XML schema](https://llg.cubic.org/docs/junit/). Key elements:\n\n```xml\n<testsuites name=\"apollo\" tests=\"42\" failures=\"2\" errors=\"1\" skipped=\"3\" time=\"1.234\">\n  <testsuite name=\"test_login.py\" tests=\"10\" failures=\"1\" errors=\"0\" skipped=\"0\" time=\"0.456\">\n    <testcase name=\"test_valid_login\" classname=\"test_login\" time=\"0.123\">\n      <!-- If passed: no child elements -->\n    </testcase>\n    <testcase name=\"test_invalid_password\" classname=\"test_login\" time=\"0.045\">\n      <failure message=\"AssertionError: Expected status 200, got 401\">\n        <![CDATA[Traceback (most recent call last):\n  File \"test_login.py\", line 42, in test_invalid_password\n    assert response.status_code == 200\nAssertionError: Expected status 200, got 401]]>\n      </failure>\n    </testcase>\n  </testsuite>\n</testsuites>\n```\n\nThe JUnit formatter must handle:\n- **XML escaping**: Properly escape `<`, `>`, `&` in messages and tracebacks\n- **CDATA sections**: Wrap tracebacks in CDATA to avoid XML parsing issues\n- **Attribute calculation**: Compute aggregate statistics for testsuites\n- **Timestamp formatting**: ISO 8601 format for timestamps\n\n#### Algorithm: Reporting Pipeline\n\nWhen the Runner completes execution, it passes all `TestResult` objects to the Reporter:\n\n1. **Statistics Collection**:\n   ```\n   for result in test_results:\n       collector.add_result(result)\n   collector.finalize()  # Compute percentages, find slowest tests\n   ```\n\n2. **Format Selection** (based on `Configuration.output_format`):\n   - **\"console\"**: Use only `ConsoleFormatter`\n   - **\"junit-xml\"**: Use only `JUnitFormatter` \n   - **\"both\"**: Use both formatters sequentially\n\n3. **Formatting Process**:\n   ```\n   if output_format includes \"console\":\n       console_output = console_formatter.format(collector.stats, test_results)\n       output_writer.write_console(console_output)\n   \n   if output_format includes \"junit-xml\":\n       xml_output = junit_formatter.format(collector.stats, test_results)\n       output_writer.write_file(config.junit_xml_path, xml_output)\n   ```\n\n4. **Exit Code Determination**:\n   ```\n   if collector.stats.failed > 0 or collector.stats.errored > 0:\n       sys.exit(1)  # Non-zero exit code for CI\n   else:\n       sys.exit(0)  # Success\n   ```\n\n#### State Machine: Report Generation States\n\nThe Reporter progresses through distinct states during its lifecycle:\n\n| Current State | Event | Next State | Actions |\n|---------------|-------|------------|---------|\n| IDLE | `receive_results(test_results)` | COLLECTING | Initialize StatisticsCollector, start timer |\n| COLLECTING | `add_result(result)` for each result | COLLECTING | Update statistics, store result |\n| COLLECTING | `last_result_added()` | FORMATTING | Finalize statistics, compute aggregates |\n| FORMATTING | `format_complete()` | OUTPUTTING | Generate formatted strings |\n| OUTPUTTING | `write_complete()` | DONE | Set exit code, clean up resources |\n| DONE | (terminal state) | DONE | None |\n\n#### Concrete Walk-Through Example\n\nConsider running 5 tests with mixed outcomes:\n\n1. **Command**: `python -m apollo test_a.py test_b.py -v`\n2. **Execution**: Tests run, producing 5 `TestResult` objects\n3. **Statistics Collection**:\n   - `test_login.py::test_valid_login`: PASSED, 0.12s\n   - `test_login.py::test_invalid_password`: FAILED (assertion), 0.05s\n   - `test_login.py::test_network_error`: ERRORED (timeout), 1.5s\n   - `test_user.py::test_create_user`: PASSED, 0.08s\n   - `test_user.py::test_delete_user`: SKIPPED (decorator), 0.0s\n   \n   Statistics: total=5, passed=2, failed=1, errored=1, skipped=1, total_time=1.75s\n\n4. **Console Output** (verbose mode):\n   ```\n   test_login.py::test_valid_login ✅ 0.12s\n   test_login.py::test_invalid_password ❌ 0.05s\n   test_login.py::test_network_error ⚠️ 1.50s\n   test_user.py::test_create_user ✅ 0.08s\n   test_user.py::test_delete_user ⏭️ 0.00s (skip reason: feature disabled)\n   \n   ============================= FAILURES =============================\n   test_login.py::test_invalid_password\n   AssertionError: Expected status 200, got 401\n   > test_login.py:42\n   \n   ============================= ERRORS =============================\n   test_login.py::test_network_error\n   TimeoutError: Connection timed out after 1.5s\n   > test_login.py:67\n   \n   ============================= SUMMARY =============================\n   5 tests run in 1.75s\n   ✅ 2 passed | ❌ 1 failed | ⚠️ 1 errored | ⏭️ 1 skipped\n   \n   Exit code: 1 (failure)\n   ```\n\n5. **JUnit XML** (if `--junit-xml=results.xml`):\n   - File `results.xml` created with proper XML structure\n   - Contains all test cases with their outcomes\n   - Tracebacks included in `<failure>` and `<error>` elements\n\n#### Common Pitfalls\n\n⚠️ **Pitfall: Incomplete XML Escaping**\n- **Description**: Failing to escape special XML characters (`<`, `>`, `&`, `\"`, `'`) in test messages and tracebacks.\n- **Why it's wrong**: Invalid XML that breaks CI tools parsing the report, potentially causing false positive builds.\n- **How to fix**: Use `xml.sax.saxutils.escape()` for attribute values and wrap multiline tracebacks in `<![CDATA[...]]>` sections.\n\n⚠️ **Pitfall: Color Codes on Non-TTY Output**\n- **Description**: Emitting ANSI color codes when output is redirected to a file or pipe.\n- **Why it's wrong**: Files contain unreadable escape sequences like `\\033[32m`, breaking downstream processing.\n- **How to fix**: Check `sys.stdout.isatty()` before using colors, or provide a `--no-color` flag that disables them.\n\n⚠️ **Pitfall: Incorrect Exit Code for Mixed Results**\n- **Description**: Returning exit code 0 when tests fail but others pass, or vice versa.\n- **Why it's wrong**: CI systems rely on exit codes to detect build failures; wrong codes cause false passes/failures.\n- **How to fix**: Exit with code 1 if ANY test has status `FAILED` or `ERRORED`. Skipped tests don't affect exit code.\n\n⚠️ **Pitfall: Time Calculation Floating-Point Errors**\n- **Description**: Using float arithmetic for duration sums causing tiny rounding errors.\n- **Why it's wrong**: Displaying \"Total time: 1.0000000002s\" looks unprofessional and may confuse users.\n- **How to fix**: Round to millisecond precision (3 decimal places) for display, using `round(duration, 3)`.\n\n⚠️ **Pitfall: Memory Bloat from Storing All Results**\n- **Description**: Keeping all `TestResult` objects with full tracebacks in memory for large test suites.\n- **Why it's wrong**: Memory usage grows with test count, potentially causing OOM errors.\n- **How to fix**: For console output, stream results as they complete. For JUnit XML, accumulate only necessary data (not full Python objects) or write incrementally to a file.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| CLI Parsing | `argparse` (standard library) | `click` (third-party, richer features) |\n| Pattern Resolution | `pathlib.Path.glob()` + custom recursion | `pathlib.Path.rglob()` with ignore patterns |\n| Color Output | ANSI escape codes + `sys.stdout.isatty()` check | `colorama` (cross-platform color support) |\n| XML Generation | `xml.etree.ElementTree` | `lxml` (more features, better performance) |\n| Progress Display | Simple dots/chars printed inline | `tqdm` or custom progress bar with threading |\n\n#### B. Recommended File/Module Structure\n\n```\napollo/\n├── __main__.py              # CLI entry point: `python -m apollo`\n├── cli/\n│   ├── __init__.py\n│   ├── parser.py            # parse_cli_args(), Configuration class\n│   ├── resolver.py          # resolve_patterns_to_paths(), pattern utilities\n│   └── help.py              # Help text generation\n├── reporting/\n│   ├── __init__.py\n│   ├── reporter.py          # Reporter coordinator class\n│   ├── statistics.py        # StatisticsCollector class\n│   ├── console_formatter.py # ConsoleFormatter class\n│   ├── junit_formatter.py   # JUnitFormatter class\n│   ├── writer.py            # OutputWriter (handles stdout/file)\n│   └── colors.py            # Color constants and helpers\n└── api.py                   # Public API (if needed for programmatic use)\n```\n\n#### C. Infrastructure Starter Code\n\n**File: `apollo/cli/resolver.py`** (Complete implementation)\n```python\n\"\"\"\nPattern resolution utilities for converting user input to file paths.\n\"\"\"\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import List, Optional, Iterable\n\ndef resolve_patterns_to_paths(patterns: List[str], base_dir: Path) -> List[Path]:\n    \"\"\"\n    Convert file patterns to concrete Python file paths.\n    \n    Args:\n        patterns: List of glob patterns or directory paths\n        base_dir: Base directory for relative pattern resolution\n        \n    Returns:\n        List of Path objects to Python files matching patterns\n    \"\"\"\n    resolved_paths = set()\n    \n    for pattern in patterns:\n        # Convert to Path relative to base_dir\n        pattern_path = (base_dir / pattern).resolve()\n        \n        # Handle directory pattern (e.g., \"tests/\")\n        if pattern_path.is_dir():\n            # Recursively find all .py files in directory\n            for py_file in pattern_path.rglob(\"*.py\"):\n                if _is_test_file(py_file):\n                    resolved_paths.add(py_file)\n        \n        # Handle glob pattern (e.g., \"**/test_*.py\")\n        elif \"*\" in str(pattern):\n            # Need to handle glob relative to base_dir\n            for match in base_dir.glob(pattern):\n                if match.is_file() and match.suffix == \".py\" and _is_test_file(match):\n                    resolved_paths.add(match.resolve())\n        \n        # Handle explicit file path\n        elif pattern_path.is_file() and pattern_path.suffix == \".py\":\n            if _is_test_file(pattern_path):\n                resolved_paths.add(pattern_path)\n        else:\n            # Pattern doesn't match anything\n            sys.stderr.write(f\"Warning: Pattern '{pattern}' matched no files\\n\")\n    \n    # Sort for consistent ordering\n    return sorted(resolved_paths)\n\ndef _is_test_file(path: Path) -> bool:\n    \"\"\"Check if a file should be considered for test discovery.\"\"\"\n    # Skip common non-test directories\n    exclude_parts = {\"__pycache__\", \".git\", \".pytest_cache\", \"venv\", \"env\"}\n    for part in path.parts:\n        if part in exclude_parts:\n            return False\n    \n    # Skip hidden files\n    if path.name.startswith(\".\"):\n        return False\n    \n    return True\n```\n\n**File: `apollo/reporting/colors.py`** (Complete implementation)\n```python\n\"\"\"\nANSI color codes and color-aware output utilities.\n\"\"\"\nimport sys\n\n# ANSI escape codes\nRESET = \"\\033[0m\"\nBOLD = \"\\033[1m\"\nRED = \"\\033[31m\"\nGREEN = \"\\033[32m\"\nYELLOW = \"\\033[33m\"\nBLUE = \"\\033[34m\"\nMAGENTA = \"\\033[35m\"\nCYAN = \"\\033[36m\"\nWHITE = \"\\033[37m\"\nGRAY = \"\\033[90m\"\n\n# Status colors\nPASS_COLOR = GREEN\nFAIL_COLOR = RED\nERROR_COLOR = YELLOW\nSKIP_COLOR = BLUE\nRUNNING_COLOR = CYAN\n\ndef should_use_color() -> bool:\n    \"\"\"Determine if we should output color codes.\"\"\"\n    # Check if output is a terminal\n    if not sys.stdout.isatty():\n        return False\n    \n    # Check for NO_COLOR environment variable (standard)\n    if os.environ.get(\"NO_COLOR\"):\n        return False\n    \n    # Check for specific terminal capabilities\n    term = os.environ.get(\"TERM\", \"\")\n    if term == \"dumb\":\n        return False\n    \n    return True\n\ndef colorize(text: str, color_code: str) -> str:\n    \"\"\"Wrap text in color codes if colors are enabled.\"\"\"\n    if should_use_color():\n        return f\"{color_code}{text}{RESET}\"\n    return text\n\n# Status icons with color\nPASS_ICON = colorize(\"✓\", PASS_COLOR) if should_use_color() else \".\"\nFAIL_ICON = colorize(\"✗\", FAIL_COLOR) if should_use_color() else \"F\"\nERROR_ICON = colorize(\"⚠\", ERROR_COLOR) if should_use_color() else \"E\"\nSKIP_ICON = colorize(\"⏭\", SKIP_COLOR) if should_use_color() else \"s\"\n```\n\n#### D. Core Logic Skeleton Code\n\n**File: `apollo/cli/parser.py`** (TODO skeleton)\n```python\n\"\"\"\nCommand-line argument parsing for the test framework.\n\"\"\"\nimport argparse\nimport sys\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\nfrom ..data_model import Configuration\n\ndef parse_cli_args(args: Optional[List[str]] = None) -> Configuration:\n    \"\"\"\n    Parse command-line arguments into a Configuration object.\n    \n    Args:\n        args: Argument list (defaults to sys.argv[1:])\n        \n    Returns:\n        Configuration object with parsed settings\n        \n    Raises:\n        SystemExit: If --help or --version is requested\n    \"\"\"\n    if args is None:\n        args = sys.argv[1:]\n    \n    parser = argparse.ArgumentParser(\n        prog=\"apollo\",\n        description=\"Apollo Test Framework - A pytest-inspired test runner\",\n        epilog=\"See https://github.com/example/apollo for more information.\"\n    )\n    \n    # TODO 1: Add positional argument for file/directory patterns\n    #   - Argument name: \"patterns\"\n    #   - nargs: \"*\" (zero or more)\n    #   - default: [\".\"] (current directory)\n    #   - help: \"File/directory/glob patterns to search for tests\"\n    \n    # TODO 2: Add -k/--filter flag for test name filtering\n    #   - Action: \"append\" (can be used multiple times)\n    #   - Help: \"Only run tests matching substring pattern\"\n    \n    # TODO 3: Add -v/--verbose flag\n    #   - Action: \"store_true\"\n    #   - Help: \"Verbose output (show passing tests)\"\n    \n    # TODO 4: Add -q/--quiet flag (mutually exclusive with -v)\n    #   - Action: \"store_true\"\n    #   - Help: \"Quiet mode (summary only)\"\n    \n    # TODO 5: Add --junit-xml flag\n    #   - Type: str (file path)\n    #   - Help: \"Generate JUnit XML report to specified file\"\n    \n    # TODO 6: Add --parallel flag\n    #   - Action: \"store_true\"\n    #   - Help: \"Run tests in parallel\"\n    \n    # TODO 7: Add -n/--num-workers flag\n    #   - Type: int\n    #   - Help: \"Number of parallel workers (default: auto)\"\n    \n    # TODO 8: Add --durations flag\n    #   - Action: \"store_true\"\n    #   - Help: \"Show test execution durations\"\n    \n    # TODO 9: Add --collect-only flag\n    #   - Action: \"store_true\"\n    #   - Help: \"Collect tests but don't execute\"\n    \n    # TODO 10: Add --version flag\n    #   - Action: \"version\"\n    #   - Version: import metadata and get version\n    \n    # TODO 11: Parse arguments\n    #   parsed = parser.parse_args(args)\n    \n    # TODO 12: Validate mutually exclusive flags (-v and -q can't both be True)\n    \n    # TODO 13: Build Configuration object\n    #   config = Configuration(\n    #       file_patterns=parsed.patterns,\n    #       start_dir=Path.cwd(),\n    #       test_name_filters=parsed.filter or [],\n    #       verbose=parsed.verbose,\n    #       quiet=parsed.quiet,\n    #       output_format=\"junit-xml\" if parsed.junit_xml else \"console\",\n    #       parallel=parsed.parallel,\n    #       max_workers=parsed.num_workers,\n    #       show_durations=parsed.durations,\n    #       junit_xml_path=Path(parsed.junit_xml) if parsed.junit_xml else None\n    #   )\n    \n    # TODO 14: Return configuration\n    #   return config\n    pass\n```\n\n**File: `apollo/reporting/statistics.py`** (TODO skeleton)\n```python\n\"\"\"\nStatistics collection for test results.\n\"\"\"\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nfrom ..data_model import TestResult, TestStatus\n\n@dataclass\nclass ModuleStats:\n    \"\"\"Statistics for a single module.\"\"\"\n    name: str\n    total: int = 0\n    passed: int = 0\n    failed: int = 0\n    errored: int = 0\n    skipped: int = 0\n    time: float = 0.0\n\n@dataclass\nclass TestRunStatistics:\n    \"\"\"Complete statistics for a test run.\"\"\"\n    total: int = 0\n    passed: int = 0\n    failed: int = 0\n    errored: int = 0\n    skipped: int = 0\n    total_time: float = 0.0\n    start_time: Optional[datetime] = None\n    end_time: Optional[datetime] = None\n    by_module: Dict[str, ModuleStats] = field(default_factory=dict)\n    slowest_tests: List[Tuple[str, float]] = field(default_factory=list)\n    \n    @property\n    def success_rate(self) -> float:\n        \"\"\"Calculate success rate as percentage.\"\"\"\n        if self.total == 0:\n            return 0.0\n        return (self.passed / self.total) * 100\n\nclass StatisticsCollector:\n    \"\"\"Collects and aggregates test result statistics.\"\"\"\n    \n    def __init__(self):\n        self.stats = TestRunStatistics()\n        self._results: List[TestResult] = []\n        self._module_cache: Dict[str, ModuleStats] = {}\n    \n    def add_result(self, result: TestResult) -> None:\n        \"\"\"\n        Add a test result to statistics.\n        \n        Args:\n            result: TestResult to incorporate into statistics\n        \"\"\"\n        # TODO 1: Append result to internal _results list\n        \n        # TODO 2: Update overall counts based on result.status\n        #   Use match/case or if/elif for TestStatus enum\n        \n        # TODO 3: Extract module name from result.test_case.nodeid\n        #   Format: \"path/to/module.py::test_name\" → module = \"path/to/module.py\"\n        \n        # TODO 4: Get or create ModuleStats for this module\n        #   If not in _module_cache, create new ModuleStats\n        \n        # TODO 5: Update module statistics (counts and time)\n        \n        # TODO 6: Add to total_time\n        \n        # TODO 7: Track for slowest tests list\n        #   Keep only top 10 slowest tests by duration\n    \n    def finalize(self) -> None:\n        \"\"\"\n        Finalize statistics after all results added.\n        \n        Computes derived statistics and prepares for reporting.\n        \"\"\"\n        # TODO 1: Set end_time to current time if not already set\n        \n        # TODO 2: Sort slowest_tests by duration (descending)\n        \n        # TODO 3: Populate stats.by_module from _module_cache\n        \n        # TODO 4: Calculate any other derived statistics needed\n        \n        # TODO 5: Mark statistics as finalized (optional flag)\n    \n    def get_summary_string(self) -> str:\n        \"\"\"\n        Generate a human-readable summary string.\n        \n        Returns:\n            Formatted summary like \"5 tests, 3 passed, 1 failed, 1 skipped\"\n        \"\"\"\n        # TODO: Format summary string with counts and optional percentage\n        pass\n```\n\n**File: `apollo/reporting/console_formatter.py`** (TODO skeleton)\n```python\n\"\"\"\nConsole formatter for human-readable test output.\n\"\"\"\nfrom typing import List\nfrom ..data_model import TestResult\nfrom .statistics import TestRunStatistics\nfrom .colors import *\n\nclass ConsoleFormatter:\n    \"\"\"Formats test results for console output.\"\"\"\n    \n    def __init__(self, verbose: bool = False, show_durations: bool = False):\n        self.verbose = verbose\n        self.show_durations = show_durations\n    \n    def format(self, stats: TestRunStatistics, results: List[TestResult]) -> str:\n        \"\"\"\n        Format complete test run results for console output.\n        \n        Args:\n            stats: Aggregated statistics\n            results: Individual test results\n            \n        Returns:\n            Formatted string ready for printing\n        \"\"\"\n        output_parts = []\n        \n        # TODO 1: If not verbose, generate progress indicator\n        #   For each result: \n        #     PASSED → \".\" or PASS_ICON\n        #     FAILED → \"F\" or FAIL_ICON  \n        #     ERRORED → \"E\" or ERROR_ICON\n        #     SKIPPED → \"s\" or SKIP_ICON\n        #   Join into single line\n        \n        # TODO 2: If verbose, generate detailed test listing\n        #   For each result: format as \"nodeid [status_icon] [duration]\"\n        #   Include skip reason if test was skipped\n        \n        # TODO 3: Generate failures section\n        #   Filter results for FAILED status\n        #   For each: format traceback and assertion message\n        #   Include file:line information\n        \n        # TODO 4: Generate errors section  \n        #   Filter results for ERRORED status\n        #   For each: format exception traceback\n        #   Distinguish from assertion failures\n        \n        # TODO 5: Generate summary section\n        #   Show total tests, time, and counts by status\n        #   Format with colors/icons if enabled\n        #   Include success percentage\n        \n        # TODO 6: If show_durations, add slowest tests list\n        #   Show top N slowest tests with durations\n        \n        # TODO 7: Join all parts with appropriate section headers\n        #   Use separator lines like \"=\" * 60\n        \n        return \"\\n\".join(output_parts)\n    \n    def _format_test_result(self, result: TestResult) -> str:\n        \"\"\"Format a single test result line.\"\"\"\n        # TODO: Implement based on verbosity and show_durations settings\n        pass\n    \n    def _format_failure_details(self, result: TestResult) -> str:\n        \"\"\"Format detailed failure information for a test.\"\"\"\n        # TODO: Extract and format assertion message, traceback\n        pass\n```\n\n**File: `apollo/reporting/junit_formatter.py`** (TODO skeleton)\n```python\n\"\"\"\nJUnit XML formatter for CI/CD integration.\n\"\"\"\nimport xml.etree.ElementTree as ET\nfrom datetime import datetime\nfrom typing import List\nfrom xml.dom import minidom\n\nfrom ..data_model import TestResult\nfrom .statistics import TestRunStatistics\n\nclass JUnitFormatter:\n    \"\"\"Formats test results as JUnit XML.\"\"\"\n    \n    def format(self, stats: TestRunStatistics, results: List[TestResult]) -> str:\n        \"\"\"\n        Format test results as JUnit XML string.\n        \n        Args:\n            stats: Aggregated statistics\n            results: Individual test results\n            \n        Returns:\n            JUnit XML as string\n        \"\"\"\n        # TODO 1: Create root <testsuites> element\n        #   Add attributes: name=\"apollo\", tests, failures, errors, skipped, time\n        \n        # TODO 2: Group results by module (file)\n        #   results_by_module = group_results_by_file(results)\n        \n        # TODO 3: For each module, create <testsuite> element\n        #   Add attributes: name (filename), tests, failures, errors, skipped, time\n        \n        # TODO 4: For each test in module, create <testcase> element\n        #   Add attributes: name (test function name), classname (module), time\n        \n        # TODO 5: For failed tests, add <failure> child element\n        #   Add message attribute with assertion error\n        #   Add traceback as element text (wrap in CDATA)\n        \n        # TODO 6: For errored tests, add <error> child element  \n        #   Add message attribute with exception type\n        #   Add traceback as element text (wrap in CDATA)\n        \n        # TODO 7: For skipped tests, add <skipped> child element\n        #   Add message attribute with skip reason if available\n        \n        # TODO 8: Convert ElementTree to pretty-printed XML string\n        #   Use minidom for pretty printing or custom formatting\n        \n        # TODO 9: Return XML string\n        pass\n    \n    def _escape_xml(self, text: str) -> str:\n        \"\"\"\n        Escape text for safe inclusion in XML.\n        \n        Args:\n            text: Text to escape\n            \n        Returns:\n            Escaped text safe for XML attributes\n        \"\"\"\n        # TODO: Replace &, <, >, \", ' with XML entities\n        pass\n    \n    def _create_cdata(self, text: str) -> str:\n        \"\"\"\n        Wrap text in CDATA section if it contains special characters.\n        \n        Args:\n            text: Text to wrap\n            \n        Returns:\n            CDATA-wrapped text or escaped text\n        \"\"\"\n        # TODO: Check if text contains ]]> (illegal in CDATA)\n        #   If yes, split and wrap multiple CDATA sections\n        #   Otherwise, wrap in <![CDATA[ ... ]]>\n        pass\n```\n\n#### E. Language-Specific Hints\n\n1. **argparse Tips**:\n   - Use `argparse.ArgumentParser(add_help=False)` if you want custom help formatting\n   - Mutually exclusive groups: `parser.add_mutually_exclusive_group()` for `-v` and `-q`\n   - Subparsers can be used for advanced commands but aren't needed for our simple CLI\n\n2. **Path Handling**:\n   - Always use `pathlib.Path` over `os.path` for cleaner, more object-oriented code\n   - `Path.resolve()` converts to absolute paths and resolves symlinks\n   - `Path.rglob(\"*.py\")` recursively finds Python files\n\n3. **XML Generation**:\n   - `xml.etree.ElementTree` is in the standard library and sufficient for our needs\n   - Use `ET.SubElement()` to build the tree structure\n   - For pretty printing, use `xml.dom.minidom.parseString()` then `toprettyxml()`\n\n4. **Timing**:\n   - Use `time.perf_counter()` for high-resolution timing, not `time.time()`\n   - Store start time early in the process and calculate duration at the end\n\n5. **Exit Codes**:\n   - `sys.exit(0)` for success, `sys.exit(1)` for failures\n   - Consider using different codes for different failure types (e.g., 2 for configuration errors) if you want to be fancy\n\n#### F. Milestone Checkpoint\n\n**To verify Milestone 4 implementation**:\n\n1. **Run the basic test**:\n   ```\n   python -m apollo --version\n   ```\n   *Expected*: Shows version information and exits with code 0.\n\n2. **Test discovery and simple run**:\n   ```\n   python -m apollo tests/sample_tests.py -v\n   ```\n   *Expected*: Shows each test with ✅/❌ icons, summary, and appropriate exit code.\n\n3. **Test filtering**:\n   ```\n   python -m apollo tests/ -k \"login\" --durations\n   ```\n   *Expected*: Runs only tests with \"login\" in their name, shows durations for slowest tests.\n\n4. **JUnit XML output**:\n   ```\n   python -m apollo tests/ --junit-xml=test-results.xml\n   cat test-results.xml\n   ```\n   *Expected*: Creates well-formed XML file with proper escaping and CDATA sections.\n\n5. **Exit code verification**:\n   ```\n   python -m apollo tests/failing_tests.py\n   echo $?  # On Linux/Mac\n   ```\n   *Expected*: Exit code 1 if any tests fail or error.\n\n**Signs something is wrong**:\n- Tests run but no output appears (check `-v` flag or default verbosity)\n- XML file contains unescaped `<` or `&` characters (breaks CI parsers)\n- Exit code is 0 when tests fail (CI would incorrectly pass)\n- Color codes appear in redirected output (check `isatty()`)\n- Pattern `**/*.py` includes virtual environment files (need better filtering)\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| No tests found when pattern should match | Incorrect pattern resolution | Print resolved paths after `resolve_patterns_to_paths()` | Debug pattern matching logic, check `Path.glob()` vs `Path.rglob()` |\n| XML file fails to parse in CI | Missing XML escaping or CDATA | Validate XML with `xmllint --noout file.xml` | Add proper escaping for `<`, `>`, `&` and wrap tracebacks in CDATA |\n| Colors don't appear in terminal | `isatty()` check failing or NO_COLOR env var | Print `sys.stdout.isatty()` and check `os.environ.get(\"NO_COLOR\")` | Fix color detection logic or add `--color=always/auto/never` flag |\n| Exit code always 0 | Forgot to set exit code based on failures | Add debug print of failure count before `sys.exit()` | Set `sys.exit(1)` if any failures or errors |\n| JUnit report missing timestamps | Forgot to add `time` attribute to testcase | Check generated XML for `time=\"...\"` attributes | Ensure duration is recorded and formatted to 3 decimal places |\n| Verbose and quiet flags both work | Mutually exclusive group not set up | Check argparse mutual exclusion setup | Use `add_mutually_exclusive_group()` for `-v` and `-q` |\n| Pattern with `**` doesn't recurse | Using `glob()` instead of `rglob()` | Test pattern resolution with simple test script | Use `Path.rglob()` for recursive matching or `glob.glob(\"**/*.py\", recursive=True)` |\n\n\n## 9. Interactions and Data Flow\n\n> **Milestone(s):** All four milestones, as this section describes the end-to-end flow through all components.\n\nThe **Interactions and Data Flow** section reveals how the test framework's components collaborate like a well-orchestrated symphony. Imagine a **music recording studio** where the producer (CLI) receives the project brief (user command), gathers musicians (Discoverer), sets up instruments (Fixture System), records each track in isolation (Runner), reviews the recordings with a critical ear (Assertion Engine), and finally produces the album with track listings and performance notes (Reporter). This section traces the complete journey from user command to final report, showing how data transforms as it moves through the framework's **pipeline architecture**.\n\n### Main Execution Sequence\n\nThe framework follows a **linear pipeline architecture** where each component processes input and passes output to the next. This design ensures clear separation of concerns and makes the system easy to reason about. Let's trace through the most common happy path: a user runs `apollo tests/ --verbose` to execute all tests in a directory with detailed output.\n\n#### Phase 1: CLI Invocation and Configuration\n\n> **Mental Model:** The **Control Panel Activation** - The user interacts with the framework's control panel (CLI), dialing in settings that configure the entire test run.\n\n1. **User Command Entry**: The user types a command into their terminal, such as `apollo tests/ --verbose --parallel`. This command is received by the operating system and passed to the Python interpreter running the framework's entry point.\n\n2. **Argument Parsing**: The `parse_cli_args()` function ingests the raw command-line arguments and transforms them into a structured `Configuration` object. This process involves:\n   - Validating that mutually exclusive flags aren't used together (e.g., `--verbose` and `--quiet`)\n   - Resolving relative paths to absolute paths based on the current working directory\n   - Setting default values for unspecified options (e.g., default `output_format` is `\"console\"`)\n   - Normalizing file patterns (ensuring `*.py` suffix where needed)\n\n3. **Configuration Validation**: The framework performs basic sanity checks on the `Configuration`:\n   - Verifying that specified directories exist and are readable\n   - Ensuring `max_workers` for parallel execution is a positive integer\n   - Checking that the JUnit XML output path (if specified) is writable\n\n4. **Early Exit Conditions**: If the configuration includes help (`--help`) or version (`--version`) flags, the CLI prints the requested information and exits immediately without proceeding to discovery.\n\nAt this point, the abstract user intent has been transformed into a concrete, structured `Configuration` object that serves as the **work order** for the entire test run.\n\n#### Phase 2: Test Discovery and Suite Assembly\n\n> **Mental Model:** The **Casting Call** - The Discoverer acts as a casting director, scanning through the codebase to find all eligible \"performers\" (test functions) and organizing them into a \"playlist\" (TestSuite).\n\n5. **Pattern Resolution**: The framework calls `resolve_patterns_to_paths()` to convert the user's file patterns (like `tests/`, `test_*.py`) into concrete filesystem paths. This function:\n   - Recursively walks directories if specified\n   - Filters files to only include Python modules (`.py` files)\n   - Respects any `.gitignore` or similar exclusion patterns\n   - Returns a list of `Path` objects ready for import\n\n6. **Module Import and Inspection**: For each resolved file path, the Discoverer:\n   - Converts the filesystem path to an importable module name using `module_path_to_name()`\n   - Dynamically imports the module using `_import_module_from_file()`, which handles the Python import machinery\n   - Scans the imported module for test functions with `_find_tests_in_module()`\n   - For each discovered test function, creates a `TestCase` object with:\n     - `nodeid`: A unique identifier like `\"tests/test_math.py::test_addition\"`\n     - `func`: A reference to the actual test function\n     - `file_path` and `line_no`: Source location for error reporting\n     - `fixtures`: Initially empty list (populated during fixture scanning)\n\n7. **Fixture Registration**: Concurrently with test discovery, the framework scans each module for fixture definitions using `FixtureRegistry.scan_module()`. Each `@fixture` decorator registers a `Fixture` object in the global `FixtureRegistry`, recording its name, scope, dependencies, and the function that creates it.\n\n8. **Suite Construction**: All discovered `TestCase` objects are grouped into a `TestSuite` object. The suite acts as a **playlist of tests** that maintains the execution order (typically the order of discovery, though this can be configured).\n\n> **Architecture Decision: Pipeline vs. Callback Architecture**\n> \n> **Context:** We need to coordinate test discovery, fixture registration, and test execution. Two patterns emerged: a linear pipeline (each component completes before the next begins) vs. a callback architecture (components register callbacks that fire at specific events).\n> \n> **Options Considered:**\n> 1. **Linear Pipeline:** Discover all tests → Register all fixtures → Execute all tests → Report all results\n> 2. **Callback/Event-Driven:** Components subscribe to events (module_loaded, test_found, fixture_registered) and react accordingly\n> 3. **Hybrid Approach:** Pipeline for main flow with event hooks for extensions\n> \n> **Decision:** Linear Pipeline Architecture\n> \n> **Rationale:**\n> - Simplicity and predictability: The flow is easy to trace and debug\n> - Natural progression: Each phase has clear preconditions (e.g., fixtures must be registered before tests that depend on them can run)\n> - Error handling: If discovery fails, we can fail fast without partially executing tests\n> - Learning value: A linear pipeline is easier for learners to understand and implement\n> \n> **Consequences:**\n> - The framework loads all modules into memory before any test runs (higher memory usage)\n> - No streaming or incremental processing of very large test suites\n> - Clear separation between phases makes parallelization straightforward\n> \n> | Option | Pros | Cons | Chosen? |\n> |--------|------|------|---------|\n> | Linear Pipeline | Simple, predictable, easy to debug | All tests loaded into memory at once | ✅ |\n> | Callback/Event-Driven | Flexible, incremental, memory efficient | Complex to implement and debug | ❌ |\n> | Hybrid | Balance of flexibility and simplicity | Adds complexity without clear benefit for this project | ❌ |\n\n#### Phase 3: Fixture Preparation and Test Execution\n\n> **Mental Model:** The **Stage Crew Setup** - Before each \"performance\" (test), the stage crew (Fixture System) prepares the set, props, and lighting according to the \"stage directions\" (fixture dependencies).\n\n9. **Fixture Dependency Resolution**: For each `TestCase` in the `TestSuite`, the framework analyzes the test function's parameters to determine which fixtures it requires. This is done by:\n   - Inspecting the function signature using Python's `inspect` module\n   - Matching parameter names against registered fixture names in the `FixtureRegistry`\n   - Building a dependency graph if fixtures themselves depend on other fixtures\n   - Detecting and reporting circular dependencies with a clear error message\n\n10. **Fixture Value Calculation**: The `FixtureLifecycleManager` computes or retrieves cached values for each required fixture based on scope:\n    - For `FUNCTION` scope: Create a new value for each test\n    - For `CLASS` scope: Create once per test class and reuse for tests in the same class\n    - For `MODULE` scope: Create once per module and reuse for all tests in that module\n    - For `SESSION` scope: Create once for the entire test run and reuse everywhere\n\n    The manager uses a `FixtureRequest` object as a **work order** for each fixture creation, containing the fixture name, scope, and a cache key derived from the scope context.\n\n11. **Test Isolation Setup**: The Runner prepares an isolated environment for each test:\n    - Creates a fresh dictionary for the test's local namespace\n    - Sets up exception handling to catch any unanticipated errors\n    - Starts a timer to measure execution duration\n    - Updates the test's status from `PENDING` to `RUNNING`\n\n12. **Test Execution with Fixture Injection**: The Runner calls the test function with the prepared fixture values as arguments. This is the core **dependency injection mechanism**:\n    ```python\n    # Simplified conceptual view (not actual code)\n    fixture_values = {name: manager.get_fixture_value(name, test_case) for name in required_fixtures}\n    test_function(**fixture_values)\n    ```\n\n13. **Assertion Evaluation**: During test execution, when an assertion function like `assert_equal()` is called:\n    - The Assertion Engine compares actual vs. expected values\n    - If they match, execution continues silently\n    - If they differ, the engine constructs a detailed `AssertionFailure` object with diff information and raises an `AssertionError` with this rich context\n    - Custom matchers are evaluated by calling their `__matches__()` method and, on failure, `__describe_mismatch__()` for a tailored error message\n\n14. **Exception Handling**: The Runner wraps test execution in a try-except block to distinguish between:\n    - **Test Failure:** An `AssertionError` was raised (expected failure)\n    - **Test Error:** Any other exception was raised (unexpected error)\n    - **Test Success:** No exceptions were raised\n\n15. **Result Recording**: After test execution completes (whether successfully or with failure/error), the Runner:\n    - Stops the timer and calculates duration\n    - Creates a `TestResult` object with:\n      - `status`: `PASSED`, `FAILED`, or `ERRORED`\n      - `message`: A human-readable description of what went wrong (if applicable)\n      - `exception` and `traceback`: For debugging failures and errors\n      - `duration`: Execution time in seconds\n    - Passes the `TestResult` to the `StatisticsCollector`\n\n16. **Fixture Teardown**: After each test (or at appropriate **scope boundaries**), the `FixtureLifecycleManager` tears down fixtures:\n    - For generator-based fixtures (using `yield`), it resumes the generator to execute teardown code\n    - For regular fixtures, it calls any registered finalizers\n    - It removes cached values for fixtures whose scope has ended\n    - It handles teardown errors gracefully by logging them but continuing with other cleanup\n\n> **Concrete Walk-Through: Testing a Database Connection**\n> \n> Let's trace a concrete example: A test function `test_user_count(db_connection)` depends on a `db_connection` fixture with `MODULE` scope.\n> \n> 1. **Discovery:** The Discoverer finds `test_user_count` and notes it requires `db_connection` parameter\n> 2. **Fixture Registration:** The `@fixture(scope=\"module\")` decorator registers `db_connection` in the registry\n> 3. **Dependency Resolution:** The framework matches the parameter name `db_connection` to the fixture\n> 4. **Fixture Creation (First Test in Module):** Since it's `MODULE` scope and no cached value exists, `FixtureLifecycleManager`:\n>    - Creates a `FixtureRequest` with scope `MODULE` and cache key derived from module name\n>    - Calls the `db_connection()` fixture function\n>    - Caches the returned connection object\n> 5. **Test Execution:** The Runner calls `test_user_count(db_connection=connection_object)`\n> 6. **Assertion:** Inside the test, `assert_equal(connection.query(\"SELECT COUNT(*) FROM users\"), 5)` passes\n> 7. **Result Recording:** `TestResult` with status `PASSED` is created\n> 8. **Fixture Teardown (After Last Test in Module):** When all tests in the module complete:\n>    - The manager resumes the `db_connection` generator (or calls finalizer)\n>    - The connection is closed\n>    - The cached value is removed\n\n#### Phase 4: Result Collection and Reporting\n\n> **Mental Model:** The **Performance Review** - After all performances are complete, the reviewer (Reporter) analyzes each recording, writes critique notes, calculates statistics, and publishes the review in multiple formats (print program, digital album, XML for archives).\n\n17. **Statistics Aggregation**: As each `TestResult` is produced, the `StatisticsCollector`:\n    - Increments counters for the appropriate status (`passed`, `failed`, `errored`, `skipped`)\n    - Records execution time and tracks the slowest tests\n    - Groups results by module for detailed reporting\n    - Maintains overall start and end timestamps\n\n18. **Format Selection**: Based on the `Configuration.output_format`, the framework selects an appropriate formatter:\n    - `\"console\"`: Uses `ConsoleFormatter` for human-readable terminal output\n    - `\"junit\"`: Uses `JUnitFormatter` for CI/CD pipeline integration\n    - `\"both\"`: Generates both formats and writes to appropriate destinations\n\n19. **Output Generation**: The selected formatter processes all results:\n    - **Console Output:** Creates a visually appealing report with colors (if terminal supports it), progress indicators, and a summary table\n    - **JUnit XML:** Generates standards-compliant XML with proper escaping, CDATA sections for output, and timing information\n\n20. **Output Delivery**: The formatted output is written to:\n    - stdout for console output\n    - The specified file path for JUnit XML\n    - Both if configured\n\n21. **Exit Code Determination**: Finally, the framework sets the process exit code:\n    - `0`: All tests passed (or were skipped)\n    - `1`: Any tests failed or errored\n    This allows CI/CD systems to automatically detect test suite failures.\n\nThe entire sequence, from CLI invocation to exit, can be visualized in the following diagram:\n\n![CLI Workflow](./diagrams/cli-workflow.svg)\n\n### Data Flow Between Components\n\nThe framework's components communicate exclusively through well-defined data structures, creating a **contract-based interface** between each stage of the pipeline. This table summarizes the key data transformations as information flows through the system:\n\n| Source Component | Destination Component | Data Structure | Purpose | Key Transformations |\n|-----------------|----------------------|----------------|---------|-------------------|\n| **User Terminal** | **CLI Parser** | Raw command-line arguments (`List[str]`) | User intent in raw form | None (input) |\n| **CLI Parser** | **Discoverer** | `Configuration` object | Structured settings for the test run | Raw args → validated, normalized configuration |\n| **Discoverer** | **FixtureRegistry** | Module objects + fixture decorators | Register available fixtures | Module inspection → `Fixture` objects |\n| **Discoverer** | **Runner** | `TestSuite` containing `TestCase` objects | Collection of tests to execute | Module files → discovered functions → `TestCase` objects |\n| **FixtureRegistry** | **FixtureLifecycleManager** | `Fixture` definitions + dependency graphs | Knowledge of how to create fixture values | Registered fixtures → dependency-resolved graph |\n| **Runner** | **FixtureLifecycleManager** | `FixtureRequest` objects | Instructions for specific fixture instantiation | Test context + fixture name → cacheable request |\n| **FixtureLifecycleManager** | **Runner** | Fixture values (any type) | Resources to inject into tests | Fixture functions + caching → concrete values |\n| **Test Function** | **Assertion Engine** | Actual/expected values + comparison context | Verification of test conditions | Test state → comparison operation |\n| **Assertion Engine** | **Test Function** (via exception) | `AssertionFailure` object | Detailed failure information when assertion fails | Value comparison → diff calculation → error message |\n| **Runner** | **StatisticsCollector** | `TestResult` objects | Record of test execution outcome | Test execution + timing → status categorization |\n| **StatisticsCollector** | **Reporter** | `TestRunStatistics` + `List[TestResult]` | Complete results dataset for formatting | Individual results → aggregated statistics |\n| **Reporter** | **User/CI System** | Formatted strings (console/XML) | Human/machine-readable test report | Statistics + results → formatted output |\n\n#### Data Structure Lifecycle Transformations\n\nEach major data structure undergoes specific transformations as it flows through the pipeline:\n\n**1. `Configuration` Evolution:**\n- **Initial State:** Created by `parse_cli_args()` with user-provided values\n- **Transformation 1:** Paths are resolved from relative to absolute\n- **Transformation 2:** Default values are filled in for unspecified options\n- **Final State:** Complete, validated configuration used by all downstream components\n\n**2. `TestCase` Enrichment:**\n- **Discovery Phase:** Contains basic metadata (name, function reference, location)\n- **Fixture Analysis Phase:** `fixtures` list is populated with required fixture names\n- **Execution Phase:** Serves as the blueprint for test execution but isn't modified during execution\n\n**3. `TestResult` Creation Pipeline:**\n1. **Pre-execution:** Runner creates a skeleton `TestResult` with status `PENDING`\n2. **During execution:** Status updates to `RUNNING`, timer starts\n3. **Post-execution:** Status set to `PASSED`/`FAILED`/`ERRORED`, duration calculated, error details captured\n4. **Reporting phase:** Incorporated into statistics and formatted for output\n\n**4. `FixtureRequest` as Cache Key:**\nThe `FixtureRequest` serves as a **cache key** for fixture values, with its fields carefully chosen to ensure proper scoping:\n- `fixture_name`: Which fixture to create\n- `scope`: Determines caching lifetime\n- `test_case` (or derived `cache_key`): Provides context for scoping (e.g., module name for MODULE scope)\n- The tuple `(fixture_name, scope, cache_key)` uniquely identifies a fixture instance in the cache\n\n#### Parallel Execution Data Flow\n\nWhen parallel execution is enabled (`--parallel`), the data flow becomes more complex but follows the same conceptual pipeline:\n\n```mermaid\ngraph TD\n    A[Discoverer] --> B[TestSuite]\n    B --> C[Task Queue]\n    C --> D1[Worker Process 1]\n    C --> D2[Worker Process 2]\n    C --> D3[...]\n    D1 --> E1[Local FixtureManager]\n    D2 --> E2[Local FixtureManager]\n    D1 --> F1[Test Execution]\n    D2 --> F2[Test Execution]\n    F1 --> G1[TestResult]\n    F2 --> G2[TestResult]\n    G1 --> H[Result Queue]\n    G2 --> H\n    H --> I[StatisticsCollector]\n```\n\nKey adaptations for parallel flow:\n- **Shared FixtureRegistry:** Read-only access for all workers (fixture definitions are immutable after discovery)\n- **Per-Worker FixtureLifecycleManager:** Each worker maintains its own cache to avoid synchronization overhead\n- **Result Queue:** Workers send `TestResult` objects back to the main process for aggregation\n- **Scope Coordination:** SESSION-scoped fixtures require special coordination (created once and shared/serialized)\n\n> **Architecture Decision: Multi-process vs. Multi-thread Parallelism**\n> \n> **Context:** We need to run tests in parallel to reduce total execution time. Python's Global Interpreter Lock (GIL) limits true parallelism with threads for CPU-bound tasks.\n> \n> **Options Considered:**\n> 1. **Multi-process (ProcessPoolExecutor):** True parallelism, isolated memory spaces\n> 2. **Multi-thread (ThreadPoolExecutor):** Shared memory, limited by GIL for CPU work\n> 3. **Async/Coroutine-based:** Single-threaded concurrency good for I/O-bound tests\n> \n> **Decision:** Multi-process Parallelism\n> \n> **Rationale:**\n> - **Test Isolation Guarantee:** Processes provide stronger isolation than threads (separate memory spaces)\n> - **True Parallelism:** Bypasses GIL limitations for CPU-bound test code\n> - **Failure Containment:** A crashing test in one process won't bring down the entire test runner\n> - **Simplicity:** `concurrent.futures.ProcessPoolExecutor` provides a clean API\n> \n> **Consequences:**\n> - Higher memory usage (each process loads its own copy of modules)\n> - Fixture values must be serializable to pass between processes\n> - More complex setup/teardown for process-scoped resources\n> - Inter-process communication overhead for result collection\n> \n> | Option | Pros | Cons | Chosen? |\n> |--------|------|------|---------|\n> | Multi-process | True parallelism, strong isolation | Memory overhead, serialization requirements | ✅ |\n> | Multi-thread | Shared memory, low overhead | GIL limits CPU parallelism, weaker isolation | ❌ |\n> | Async/Coroutine | Efficient for I/O-bound tests | Complex, requires async test functions | ❌ |\n\n#### Error Propagation Through the Pipeline\n\nThe framework maintains clear error boundaries between components, with each component responsible for handling its own class of errors and converting them into appropriate `TestResult` statuses:\n\n| Error Origin | Detection Point | Error Type | Result Status | Recovery Strategy |\n|--------------|----------------|------------|---------------|-------------------|\n| **CLI Parsing** | `parse_cli_args()` | Invalid arguments | System exit with error message | Don't proceed; print usage |\n| **Module Import** | `_import_module_from_file()` | ImportError, SyntaxError | `ERRORED` for all tests in that module | Skip the module, continue with others |\n| **Test Execution** | Runner's exception handler | AssertionError | `FAILED` | Continue to next test |\n| **Test Execution** | Runner's exception handler | Any other Exception | `ERRORED` | Continue to next test |\n| **Fixture Creation** | `FixtureLifecycleManager.get_fixture_value()` | Exception in fixture function | `ERRORED` for all tests using that fixture | Mark dependent tests as errored |\n| **Fixture Teardown** | `FixtureLifecycleManager.teardown_scope()` | Exception during cleanup | Log warning | Continue teardown for other fixtures |\n| **Output Writing** | `OutputWriter.write()` | IOError, PermissionError | Log error to stderr | Attempt to write to alternative location |\n\nThis error handling strategy ensures that:\n1. **Fail-fast for configuration errors:** Invalid CLI arguments stop immediately\n2. **Graceful degradation for test errors:** One failing test doesn't stop the entire suite\n3. **Isolation preservation:** Errors in one module don't affect tests in other modules\n4. **Clean reporting:** All errors are captured and reported in the final output\n\n#### Data Flow Example: Complete Trace\n\nLet's trace a complete example with concrete data transformations:\n\n**User Command:** `apollo tests/math/ --verbose --junit-xml=results.xml`\n\n| Step | Component | Input Data | Output Data | Transformation |\n|------|-----------|------------|-------------|----------------|\n| 1 | CLI Parser | `[\"tests/math/\", \"--verbose\", \"--junit-xml=results.xml\"]` | `Configuration(file_patterns=[\"tests/math/\"], verbose=True, junit_xml_path=Path(\"results.xml\"), ...)` | String parsing → structured object |\n| 2 | Discoverer | `Configuration` + directory scan | `TestSuite(tests=[TestCase(nodeid=\"math/test_arithmetic.py::test_addition\", ...), ...])` | File pattern → module imports → test discovery |\n| 3 | FixtureRegistry | Module objects from discovery | Registered: `db_connection` (MODULE scope), `temp_dir` (FUNCTION scope) | Decorator scanning → fixture registration |\n| 4 | Runner | `TestSuite` + `FixtureRegistry` | For each test: calls test function with fixture values | Parameter matching → fixture value injection |\n| 5 | Assertion Engine | `assert_equal(calculate(2,2), 5)` → `actual=4, expected=5` | `AssertionFailure(message=\"Expected 5 but got 4\", expected=5, actual=4, diff=\"-4\\n+5\")` | Value comparison → diff generation |\n| 6 | Runner | Test execution outcome | `TestResult(status=FAILED, message=\"Expected 5 but got 4\", duration=0.12, ...)` | Exception capture → result packaging |\n| 7 | StatisticsCollector | Stream of `TestResult` objects | `TestRunStatistics(total=10, passed=8, failed=1, errored=1, total_time=2.45, ...)` | Aggregation → statistics calculation |\n| 8 | Reporter | `TestRunStatistics` + all `TestResult` objects | Console output + `results.xml` file | Formatting → rendering |\n| 9 | System | Exit code determination | Process exits with code `1` (because tests failed) | Status aggregation → exit code |\n\nThis data flow creates a **predictable, debuggable pipeline** where each component has clear responsibilities and well-defined interfaces. The separation allows for independent development and testing of each component, which is particularly valuable for an educational project where learners implement one milestone at a time.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Pipeline Coordination** | Linear procedural code in `main()` | `asyncio` event loop with async/await |\n| **Parallel Execution** | `concurrent.futures.ProcessPoolExecutor` | Custom process management with `multiprocessing` |\n| **Data Serialization** | `pickle` for inter-process communication | `marshal` or custom serialization for performance |\n| **Dependency Resolution** | Recursive depth-first search | Topological sort for detecting cycles earlier |\n| **Result Aggregation** | In-memory list collection | Streaming to disk for very large test suites |\n\n#### B. Recommended File/Module Structure\n\nAdd the following files to implement the pipeline coordination:\n\n```\napollo/\n  ├── __main__.py              # CLI entry point with main pipeline\n  ├── pipeline.py              # Pipeline coordination logic\n  ├── orchestrator.py          # High-level test run orchestration\n  └── utils/\n      └── parallel.py          # Parallel execution utilities\n```\n\n#### C. Infrastructure Starter Code\n\n**File: `apollo/pipeline.py`** - Complete pipeline coordinator:\n\n```python\n\"\"\"\nPipeline coordination for the test framework.\nImplements the linear pipeline architecture connecting all components.\n\"\"\"\n\nimport sys\nimport time\nfrom typing import List, Optional\nfrom pathlib import Path\n\nfrom .config import Configuration, parse_cli_args\nfrom .discovery import discover_tests, TestSuite, TestCase\nfrom .fixtures import FixtureRegistry, FixtureLifecycleManager\nfrom .runner import SimpleRunner\nfrom .reporting import StatisticsCollector, ConsoleFormatter, JUnitFormatter\nfrom .utils.parallel import ParallelRunner\n\n\nclass TestPipeline:\n    \"\"\"Orchestrates the complete test execution pipeline.\"\"\"\n    \n    def __init__(self):\n        self.config: Optional[Configuration] = None\n        self.suite: Optional[TestSuite] = None\n        self.fixture_registry = FixtureRegistry()\n        self.fixture_manager = FixtureLifecycleManager(self.fixture_registry)\n        self.stats_collector = StatisticsCollector()\n        self.start_time: Optional[float] = None\n        self.end_time: Optional[float] = None\n    \n    def run_from_cli(self, args: List[str]) -> int:\n        \"\"\"Execute the complete pipeline from CLI arguments.\"\"\"\n        try:\n            # Phase 1: Parse configuration\n            self.config = parse_cli_args(args)\n            \n            # Phase 2: Discover tests and fixtures\n            self.suite = self._discover_tests()\n            \n            # Phase 3: Execute tests\n            results = self._execute_tests()\n            \n            # Phase 4: Report results\n            exit_code = self._report_results(results)\n            \n            return exit_code\n            \n        except KeyboardInterrupt:\n            print(\"\\n\\nTest run interrupted by user\", file=sys.stderr)\n            return 130  # Standard interrupt exit code\n        except Exception as e:\n            print(f\"Fatal error in test pipeline: {e}\", file=sys.stderr)\n            if self.config and self.config.verbose:\n                import traceback\n                traceback.print_exc()\n            return 1\n    \n    def _discover_tests(self) -> TestSuite:\n        \"\"\"Discover all tests and fixtures.\"\"\"\n        if not self.config:\n            raise RuntimeError(\"Configuration not set\")\n        \n        print(f\"Discovering tests in {self.config.start_dir}...\")\n        \n        # Discover test functions\n        test_cases: List[TestCase] = []\n        for file_path in self.config.file_patterns:\n            discovered = discover_tests(file_path, pattern=\"test_*.py\")\n            test_cases.extend(discovered)\n        \n        # Scan for fixtures in the same modules\n        # (Implementation detail: we need to track which modules were scanned)\n        \n        return TestSuite(name=\"default\", tests=test_cases)\n    \n    def _execute_tests(self) -> List[TestResult]:\n        \"\"\"Execute all tests in the suite.\"\"\"\n        if not self.config or not self.suite:\n            raise RuntimeError(\"Pipeline not properly initialized\")\n        \n        self.start_time = time.time()\n        \n        if self.config.parallel:\n            runner = ParallelRunner(\n                max_workers=self.config.max_workers,\n                fixture_manager=self.fixture_manager\n            )\n        else:\n            runner = SimpleRunner(fixture_manager=self.fixture_manager)\n        \n        results = runner.run_suite(self.suite)\n        \n        self.end_time = time.time()\n        return results\n    \n    def _report_results(self, results: List[TestResult]) -> int:\n        \"\"\"Generate and output reports, return exit code.\"\"\"\n        if not self.config:\n            raise RuntimeError(\"Configuration not set\")\n        \n        # Add all results to statistics\n        for result in results:\n            self.stats_collector.add_result(result)\n        self.stats_collector.finalize()\n        \n        # Generate console output\n        if self.config.output_format in (\"console\", \"both\"):\n            console_output = ConsoleFormatter.format(\n                self.stats_collector.stats, \n                results,\n                verbose=self.config.verbose,\n                show_durations=self.config.show_durations\n            )\n            print(console_output)\n        \n        # Generate JUnit XML\n        if self.config.output_format in (\"junit\", \"both\") and self.config.junit_xml_path:\n            xml_output = JUnitFormatter.format(\n                self.stats_collector.stats,\n                results,\n                suite_name=\"Apollo Test Run\"\n            )\n            self.config.junit_xml_path.write_text(xml_output)\n            if self.config.verbose:\n                print(f\"\\nJUnit XML written to: {self.config.junit_xml_path}\")\n        \n        # Determine exit code\n        if self.stats_collector.stats.failed > 0 or self.stats_collector.stats.errored > 0:\n            return 1\n        return 0\n\n\ndef main():\n    \"\"\"Main entry point for the CLI.\"\"\"\n    pipeline = TestPipeline()\n    exit_code = pipeline.run_from_cli(sys.argv[1:])\n    sys.exit(exit_code)\n```\n\n#### D. Core Logic Skeleton Code\n\n**File: `apollo/orchestrator.py`** - High-level orchestration with detailed TODOs:\n\n```python\n\"\"\"\nHigh-level test orchestration with detailed step-by-step implementation.\n\"\"\"\n\nclass TestOrchestrator:\n    \"\"\"Orchestrates test discovery, fixture setup, execution, and reporting.\"\"\"\n    \n    def execute_test_run(self, config: Configuration) -> TestRunStatistics:\n        \"\"\"\n        Execute a complete test run from configuration to statistics.\n        \n        Args:\n            config: The configuration for this test run\n            \n        Returns:\n            Complete statistics about the test run\n            \n        TODO Implementation Steps:\n        1.  Validate the configuration (check paths exist, etc.)\n        2.  Initialize the StatisticsCollector and record start_time\n        3.  Call resolve_patterns_to_paths() to convert patterns to concrete file paths\n        4.  For each file path:\n            a. Convert to module name using module_path_to_name()\n            b. Import the module using _import_module_from_file()\n            c. Scan for fixtures with FixtureRegistry.scan_module()\n            d. Discover tests with _find_tests_in_module()\n        5.  Build TestSuite from all discovered TestCase objects\n        6.  Analyze each TestCase's parameters to determine fixture dependencies\n        7.  Initialize appropriate Runner (SimpleRunner or ParallelRunner)\n        8.  Execute the suite with runner.run_suite(), collecting TestResult objects\n        9.  For each TestResult, add to StatisticsCollector\n        10. Finalize statistics (calculate durations, slowest tests, etc.)\n        11. Return the TestRunStatistics object\n        \"\"\"\n        # TODO 1: Validate configuration\n        # TODO 2: Initialize StatisticsCollector and record start_time\n        # TODO 3: Convert patterns to paths\n        # TODO 4: Process each module (discover tests and fixtures)\n        # TODO 5: Build TestSuite\n        # TODO 6: Analyze fixture dependencies for each test\n        # TODO 7: Initialize appropriate Runner\n        # TODO 8: Execute suite and collect results\n        # TODO 9: Add results to statistics\n        # TODO 10: Finalize statistics\n        # TODO 11: Return statistics\n        pass\n    \n    def _discover_tests_and_fixtures(self, file_paths: List[Path]) -> Tuple[List[TestCase], FixtureRegistry]:\n        \"\"\"\n        Discover all tests and fixtures from the given file paths.\n        \n        TODO Implementation Steps:\n        1. Create empty FixtureRegistry\n        2. Create empty list for TestCase objects\n        3. For each file_path in file_paths:\n            a. Convert to module name (strip .py, replace / with .)\n            b. Try to import module; on ImportError, skip with warning\n            c. Call registry.scan_module(module) to find fixtures\n            d. Call _find_tests_in_module(module) to find tests\n            e. Extend test_cases list with discovered tests\n        4. Return (test_cases, registry)\n        \"\"\"\n        # TODO 1: Create FixtureRegistry\n        # TODO 2: Create empty test_cases list\n        # TODO 3: Process each file path\n        # TODO 4: Return results\n        pass\n    \n    def _analyze_fixture_dependencies(self, test_case: TestCase, registry: FixtureRegistry) -> None:\n        \"\"\"\n        Analyze a test case's parameters and update its fixtures list.\n        \n        TODO Implementation Steps:\n        1. Use inspect.signature() to get test function parameters\n        2. For each parameter name:\n            a. Check if registry.get(name) returns a fixture\n            b. If yes, add fixture name to test_case.fixtures list\n        3. Handle edge cases:\n            - Parameters with default values (ignore - they're not fixtures)\n            - *args and **kwargs parameters (ignore)\n            - Parameters that don't match any fixture (could be error or intentional)\n        \"\"\"\n        # TODO 1: Get function signature\n        # TODO 2: Check each parameter against registry\n        # TODO 3: Update test_case.fixtures list\n        pass\n```\n\n#### E. Language-Specific Hints\n\n**Python-Specific Implementation Tips:**\n\n1. **Module Import:** Use `importlib.util.spec_from_file_location()` and `importlib.util.module_from_spec()` for clean dynamic imports.\n2. **Parallel Execution:** Use `concurrent.futures.ProcessPoolExecutor` with `max_workers=None` to use all CPU cores.\n3. **Fixture Serialization:** For parallel execution, fixtures must be picklable. Use `__getstate__` and `__setstate__` for custom serialization.\n4. **Error Isolation:** Use `sys.excepthook` to catch unhandled exceptions in worker processes.\n5. **Resource Cleanup:** Use `atexit` handlers or context managers to ensure cleanup even on interruption.\n\n#### F. Milestone Checkpoint\n\nAfter implementing the pipeline coordination, verify with:\n\n**Command:**\n```bash\npython -m apollo tests/examples/ --verbose\n```\n\n**Expected Output:**\n```\nDiscovering tests in /path/to/tests/examples/...\nFound 3 test(s) in 1 module(s)\n\nRunning tests...\ntest_addition (math.test_arithmetic) ... PASSED (0.002s)\ntest_subtraction (math.test_arithmetic) ... FAILED (0.001s)\ntest_multiplication (math.test_arithmetic) ... PASSED (0.003s)\n\nFAILURES:\ntest_subtraction (math.test_arithmetic)\n  AssertionError: Expected 1 but got -1\n  File \"tests/examples/math/test_arithmetic.py\", line 15\n  assert_equal(calculate(1, 2), 1)\n\nSummary:\n3 tests, 2 passed, 1 failed, 0 errored, 0 skipped in 0.015s\n```\n\n**Verification Checklist:**\n- [ ] All phases execute in correct order (discovery → execution → reporting)\n- [ ] Test isolation: Failure in one test doesn't affect others\n- [ ] Timing is captured and displayed\n- [ ] Exit code is 1 when tests fail\n- [ ] Verbose flag produces detailed output\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Tests run but fixtures aren't injected** | Parameter name doesn't match fixture name | Add debug logging to `_analyze_fixture_dependencies()` | Ensure parameter names exactly match fixture names |\n| **Parallel tests hang indefinitely** | Deadlock in shared resource | Add timeout to `ProcessPoolExecutor.submit()` | Use `concurrent.futures.as_completed()` with timeout |\n| **Fixture teardown runs multiple times** | Incorrect scope caching | Log cache hits/misses in `FixtureLifecycleManager` | Ensure cache key includes proper scope context |\n| **Import errors skip entire module** | Exception not caught during discovery | Wrap `_import_module_from_file()` in try-except | Catch `ImportError` and log warning instead of crashing |\n| **JUnit XML contains invalid characters** | Test output has control characters | Check `_escape_xml()` function | Add filtering for non-printable characters |\n| **Exit code is 0 when tests failed** | Exit code logic error | Add debug print of statistics before exit | Ensure exit code checks both `failed` and `errored` counts |\n\n\n## 10. Error Handling and Edge Cases\n\n> **Milestone(s):** All four milestones, as error handling must be designed into every component from discovery through reporting.\n\nEvery complex system must anticipate and gracefully handle failure conditions, and a test framework is no exception. The **Error Handling and Edge Cases** design defines how the framework responds to unexpected situations—from test failures (which are expected) to system-level errors that could crash the entire test run. The mental model here is that of a **resilient conductor**: when a musician (test) plays a wrong note (assertion failure), the conductor notes it and continues; when an instrument breaks (fixture error), the conductor isolates the problem and proceeds with other musicians; when the stage collapses (system error), the conductor safely evacuates and provides a clear incident report.\n\nThis section catalogs the failure modes the framework will encounter and establishes clear recovery strategies and reporting formats for each. A well-designed error handling system ensures that tests can fail without taking down the entire test suite, provides actionable diagnostic information, and maintains the integrity of test isolation.\n\n### Common Failure Modes\n\nThe framework must distinguish between different categories of errors, as each requires different handling and reporting. The following table categorizes the primary failure modes:\n\n| Failure Category | Description | Typical Triggers | Expected Frequency |\n|-----------------|-------------|------------------|-------------------|\n| **Test Failures** | Expected assertion failures within test logic | `assert_equal(actual, expected)` when values differ, `assert_true(condition)` when condition is False | Common (part of normal test execution) |\n| **Test Errors** | Unexpected exceptions during test execution | Missing imports, syntax errors in test code, runtime exceptions (e.g., `KeyError`, `AttributeError`) | Occasional (developer mistakes) |\n| **Discovery Errors** | Problems during test discovery phase | Invalid module paths, import errors in test files, malformed test functions | Occasional during development |\n| **Fixture Lifecycle Errors** | Problems during fixture setup, execution, or teardown | Setup exceptions, teardown exceptions, circular dependencies, scope violations | Infrequent but critical |\n| **System-Level Errors** | Framework or environmental failures | Memory exhaustion, disk full, signal interruptions (Ctrl+C), internal framework bugs | Rare |\n\n#### Test Failures vs. Test Errors\n\nA critical architectural distinction is between **test failures** (expected) and **test errors** (unexpected):\n\n- **Test Failure**: Occurs when a test's assertion does not hold true. This is a *normal outcome*—the test framework is doing its job by detecting that expected behavior differs from actual behavior. The framework must capture the assertion details (expected vs. actual) and continue executing other tests.\n\n- **Test Error**: Occurs when an unanticipated exception is raised during test execution (outside of assertion checks). This indicates a problem with the test code itself or its dependencies (e.g., `NameError`, `ImportError`, `TypeError`). The framework must capture the exception and traceback, mark the test as errored, and continue with other tests.\n\n> **Architecture Decision: Distinguishing Test Failures from Errors**\n> - **Context**: The framework must report different outcomes for assertion failures (expected) versus unexpected exceptions (errors) to give developers clear signals about what went wrong.\n> - **Options Considered**:\n>   1. **Single outcome type**: Treat all non-passing tests as \"failed\" regardless of cause.\n>   2. **Separate failure and error statuses**: Distinguish between assertion failures (FAILED) and unexpected exceptions (ERRORED).\n> - **Decision**: Separate failure and error statuses (`TestStatus.FAILED` vs `TestStatus.ERRORED`).\n> - **Rationale**: This distinction provides crucial diagnostic information. A failure indicates the code under test didn't meet expectations; an error indicates the test itself is broken. CI systems often treat errors and failures differently (e.g., blocking merge vs. warning).\n> - **Consequences**: The `TestResult` structure must capture both assertion failures (via `AssertionFailure`) and unexpected exceptions (via `exception` and `traceback` fields). Reporting must visually distinguish between failures and errors.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Single outcome type | Simpler implementation, fewer statuses to manage | Loses diagnostic information, harder to debug test problems | ❌ |\n| Separate statuses | Clearer diagnostics, aligns with industry practice (pytest, unittest) | Slightly more complex status handling | ✅ |\n\n#### Discovery Errors\n\nDiscovery errors occur when the framework cannot successfully scan for or load tests. These are particularly problematic because they prevent tests from even being attempted. Common scenarios include:\n\n1. **Invalid file patterns**: User provides patterns that match no files\n2. **Import errors**: Test modules have syntax errors or missing dependencies\n3. **Permission errors**: Cannot read test files due to filesystem permissions\n4. **Circular imports**: Test modules import each other creating import loops\n\nThe framework must decide whether to treat discovery errors as fatal (stop execution) or recoverable (skip problematic modules and continue).\n\n#### Fixture Lifecycle Errors\n\nThe fixture system introduces several new failure modes due to its complex lifecycle management:\n\n1. **Setup failures**: Exception during fixture function execution\n2. **Teardown failures**: Exception during fixture cleanup (even when test passed)\n3. **Circular dependencies**: Fixture A depends on B, B depends on A (directly or indirectly)\n4. **Scope violations**: Test attempts to use a fixture with incompatible scope\n5. **Generator fixture misuse**: Fixture yields multiple times or doesn't yield at all\n\nFixture errors are particularly tricky because they can affect multiple tests and may leave resources in an inconsistent state if not handled properly.\n\n#### System-Level Errors\n\nThese are catastrophic failures that threaten the entire test run:\n\n1. **Memory exhaustion**: Test suite or fixture consumes all available memory\n2. **Disk full**: Cannot write test reports or temporary files\n3. **Signal interruptions**: User presses Ctrl+C (SIGINT) or process receives SIGTERM\n4. **Framework bugs**: Internal errors in the test framework itself\n5. **Worker process crashes**: In parallel execution, a worker process dies unexpectedly\n\n> **Key Insight**: The framework must maintain the **fail-fast vs. fail-safe** balance. For test-level issues (failures, errors), continue running other tests (fail-safe). For system-level issues, terminate gracefully with clear diagnostics (fail-fast for catastrophic issues).\n\n### Recovery & Reporting Strategy\n\nFor each failure category, the framework must implement a specific recovery strategy and ensure error information is captured and reported clearly. The following table defines the behavior for each failure mode:\n\n| Failure Mode | Detection Point | Recovery Strategy | Reporting Strategy |\n|--------------|----------------|-------------------|-------------------|\n| **Assertion Failure** | During test execution in `assert_*` functions | Continue test execution to completion (test marked as FAILED), then continue with next test | Include `AssertionFailure` details: message, expected/actual values, diff, hint |\n| **Test Error (Unexpected Exception)** | During test execution (outside assertions) | Catch exception, mark test as ERRORED, continue with next test | Include exception type, message, and full traceback in `TestResult` |\n| **Discovery Import Error** | During `_import_module_from_file()` in discovery | Skip problematic module, log warning, continue discovering other modules | Report in summary as \"discovery errors\" with count and module names |\n| **Fixture Setup Failure** | During `FixtureLifecycleManager.get_fixture_value()` | Mark all dependent tests as ERRORED, continue with tests not depending on failed fixture | Include fixture name and setup error in test result message |\n| **Fixture Teardown Failure** | During `FixtureLifecycleManager.teardown_scope()` | Log error but don't affect test outcomes (test already completed), attempt to teardown other fixtures | Report in summary as \"teardown errors\" with fixture names and errors |\n| **Circular Dependency** | During fixture registration or dependency resolution in `FixtureRegistry` | Raise error during discovery phase, prevent test execution | Fail fast with clear error message showing dependency cycle |\n| **Parallel Worker Crash** | During `SimpleRunner.run_suite()` with parallel execution | Mark tests assigned to crashed worker as ERRORED, continue with remaining tests | Report worker crash in summary with affected test count |\n| **User Interruption (Ctrl+C)** | Signal handler or keyboard interrupt in main execution loop | Stop test execution gracefully, run pending fixture teardowns, exit with non-zero code | Report \"interrupted by user\" in summary with partial results |\n| **Report Generation Error** | During `JUnitFormatter.format()` or file writing | Fall back to console output only, exit with error code | Print error to stderr about report failure |\n\n#### Error State Transitions\n\nThe `TestResult` state machine from previous sections must handle error transitions. Below is the complete state transition table including error conditions:\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|-------|------------|---------------|\n| `PENDING` | `test_started` | `RUNNING` | Record start time, initialize fixtures |\n| `RUNNING` | `assertion_passed` | `RUNNING` | Continue test execution (no state change) |\n| `RUNNING` | `assertion_failed` | `FAILED` | Capture `AssertionFailure`, stop test execution, run test teardown |\n| `RUNNING` | `exception_raised` (non-assertion) | `ERRORED` | Capture exception and traceback, stop test execution, run test teardown |\n| `RUNNING` | `test_completed` | `PASSED` | Record end time, calculate duration, run test teardown |\n| `RUNNING` | `test_skipped` | `SKIPPED` | Record skip reason, run test teardown |\n| `RUNNING` | `fixture_setup_failed` | `ERRORED` | Capture fixture error, stop test execution, attempt fixture teardown |\n| Any state | `interrupted` | `ERRORED` | Mark as interrupted, attempt cleanup if possible |\n\n#### Error Message Design Principles\n\nClear error messages are critical for developer productivity. The framework follows these principles:\n\n1. **Actionable**: Tell the developer what went wrong and suggest possible fixes\n2. **Contextual**: Include relevant context (test name, fixture name, values)\n3. **Hierarchical**: Show root cause first, then supporting details\n4. **Consistent**: Use consistent formatting and terminology across error types\n5. **Non-overwhelming**: Provide essential information without excessive verbosity\n\nFor example, an assertion failure message should show:\n- What assertion failed (e.g., \"assert_equal failed\")\n- Expected and actual values (formatted for readability)\n- A diff highlighting differences (for strings/collections)\n- Additional context like variable names when available\n\n#### Fixture Error Handling Deep Dive\n\nFixture errors require special handling due to their potential to affect multiple tests. Consider this scenario: a module-scoped fixture fails during setup. The framework must:\n\n1. Detect the exception in `FixtureLifecycleManager.get_fixture_value()`\n2. Mark the fixture as \"failed\" in the cache (store the exception)\n3. For each test that depends on this fixture:\n   - Immediately mark the test as `ERRORED` without executing it\n   - Include a message indicating which fixture failed and why\n4. Continue executing tests that don't depend on the failed fixture\n5. Skip teardown for the failed fixture (it never successfully setup)\n\n> **Architecture Decision: Fixture Teardown Error Handling**\n> - **Context**: Fixture teardown failures occur after test execution completes, potentially leaving resources leaked.\n> - **Options Considered**:\n>   1. **Ignore teardown errors**: Log but don't affect test outcomes\n>   2. **Treat as test errors**: Mark associated tests as failed/errored\n>   3. **Propagate as separate error category**: Track separately from test outcomes\n> - **Decision**: Log teardown errors but don't affect test outcomes (Option 1).\n> - **Rationale**: Teardown happens after test verification, so test results should reflect the code-under-test behavior, not cleanup issues. However, teardown failures must be reported to prevent resource leaks.\n> - **Consequences**: The framework needs separate tracking for teardown errors in the `TestRunStatistics`, and must ensure teardown exceptions don't crash the framework.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Ignore teardown errors | Clean separation of concerns, test outcomes reflect code behavior | Resource leaks might go unnoticed, inconsistent state between tests | ❌ |\n| Treat as test errors | Encourages proper cleanup, visible in test results | Punishes tests for cleanup issues unrelated to code under test | ❌ |\n| Propagate as separate category | Clear visibility without affecting test outcomes | More complex reporting, additional data structure needed | ✅ |\n\n#### Parallel Execution Error Handling\n\nWhen tests run in parallel, error handling becomes more complex:\n\n1. **Worker process isolation**: Each worker runs in its own process to ensure test isolation\n2. **Error propagation**: Worker errors must be communicated back to the main process\n3. **Resource cleanup**: Failed workers must not leak resources or orphan processes\n\nThe parallel execution strategy uses the following error handling approach:\n\n1. Each worker process catches exceptions and returns structured error information\n2. The main process collects worker results, mapping errors back to originating tests\n3. If a worker process crashes unexpectedly (e.g., segmentation fault), the main process detects the crash via broken pipe or timeout and marks all tests assigned to that worker as `ERRORED`\n4. Worker processes use `atexit` handlers to attempt cleanup on abnormal exit\n\n#### Common Pitfalls in Error Handling\n\n⚠️ **Pitfall: Swallowing exceptions in fixture teardown**\n- **Description**: Using a bare `except:` clause in fixture teardown that catches and ignores all exceptions\n- **Why it's wrong**: Critical errors (KeyboardInterrupt, SystemExit) get swallowed, preventing proper shutdown\n- **How to fix**: Catch specific exception types (`Exception`) or re-raise critical exceptions:\n  ```python\n  try:\n      yield resource\n  finally:\n      try:\n          cleanup(resource)\n      except Exception as e:  # Don't catch KeyboardInterrupt, SystemExit\n          log_teardown_error(e)\n  ```\n\n⚠️ **Pitfall: Not isolating test errors in parallel execution**\n- **Description**: When one test causes a worker process to crash, all tests in that worker fail\n- **Why it's wrong**: Loss of test isolation, one buggy test hides other test results\n- **How to fix**: Implement per-test exception boundaries within workers, or run each test in a subprocess\n\n⚠️ **Pitfall: Incomplete error information in assertion failures**\n- **Description**: Assertion messages like \"assertion failed\" without showing values\n- **Why it's wrong**: Developer must add debugging prints to understand failure\n- **How to fix**: Always include expected and actual values in assertion failure messages\n\n⚠️ **Pitfall: Discovery errors stopping entire test run**\n- **Description**: One malformed test file prevents discovery of all other tests\n- **Why it's wrong**: Reduces developer productivity, fails fast for wrong reason\n- **How to fix**: Continue discovery after import errors, report problematic modules separately\n\n### Implementation Guidance\n\n#### Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Exception handling | Python built-in `try/except/finally` | Structured error types with error codes |\n| Parallel error handling | `multiprocessing.Pool` with error callbacks | `concurrent.futures` with detailed error propagation |\n| Signal handling | Simple `signal.signal()` for SIGINT | `signal` module with proper signal masking in workers |\n| Error serialization | Pickle for worker-process communication | Custom serialization with error type preservation |\n\n#### Recommended File/Module Structure\n\n```\napollo/\n  framework/\n    errors.py                    # Error type definitions and utilities\n    discovery/\n      discoverer.py              # Discovery with error handling\n      errors.py                  # Discovery-specific errors\n    runner/\n      simple_runner.py           # Test runner with error handling\n      parallel_runner.py         # Parallel runner with worker error handling\n      errors.py                  # Runner-specific errors\n    fixtures/\n      lifecycle_manager.py       # Fixture error handling\n      errors.py                  # Fixture-specific errors\n    reporting/\n      formatters.py              # Error message formatting\n      statistics.py              # Error statistics collection\n```\n\n#### Infrastructure Starter Code\n\n**Complete error type definitions (`framework/errors.py`):**\n\n```python\n\"\"\"Framework error types and utilities.\"\"\"\n\nimport sys\nimport traceback\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Optional, List\n\n\nclass ApolloError(Exception):\n    \"\"\"Base exception for all framework errors.\"\"\"\n    pass\n\n\nclass DiscoveryError(ApolloError):\n    \"\"\"Error during test discovery.\"\"\"\n    \n    def __init__(self, message: str, file_path: Optional[str] = None):\n        self.file_path = file_path\n        super().__init__(message)\n\n\nclass FixtureError(ApolloError):\n    \"\"\"Error related to fixtures.\"\"\"\n    \n    def __init__(self, message: str, fixture_name: Optional[str] = None):\n        self.fixture_name = fixture_name\n        super().__init__(message)\n\n\nclass CircularDependencyError(FixtureError):\n    \"\"\"Circular dependency detected in fixtures.\"\"\"\n    pass\n\n\nclass ScopeViolationError(FixtureError):\n    \"\"\"Fixture scope violation.\"\"\"\n    pass\n\n\n@dataclass\nclass FormattedError:\n    \"\"\"Structured error information for reporting.\"\"\"\n    type_name: str\n    message: str\n    traceback: Optional[str] = None\n    context: Optional[dict] = None\n    \n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"type\": self.type_name,\n            \"message\": self.message,\n            \"traceback\": self.traceback,\n            \"context\": self.context or {}\n        }\n\n\ndef capture_exception(exc: Exception) -> FormattedError:\n    \"\"\"Capture an exception into a FormattedError.\"\"\"\n    exc_type = type(exc).__name__\n    exc_msg = str(exc) or repr(exc)\n    \n    # Get traceback if available\n    tb = None\n    if sys.exc_info()[2]:\n        tb = \"\".join(traceback.format_exception(type(exc), exc, exc.__traceback__))\n    \n    # Extract context from exception attributes\n    context = {}\n    for attr in [\"fixture_name\", \"file_path\", \"test_name\"]:\n        if hasattr(exc, attr):\n            context[attr] = getattr(exc, attr)\n    \n    return FormattedError(\n        type_name=exc_type,\n        message=exc_msg,\n        traceback=tb,\n        context=context\n    )\n```\n\n#### Core Logic Skeleton Code\n\n**Error-handling test runner skeleton (`runner/simple_runner.py`):**\n\n```python\n\"\"\"Test runner with comprehensive error handling.\"\"\"\n\nimport time\nimport traceback\nfrom typing import List, Optional\nfrom dataclasses import dataclass\nfrom .errors import capture_exception\nfrom ..data_model import TestCase, TestResult, TestStatus\n\n\n@dataclass\nclass RunContext:\n    \"\"\"Context for test execution with error tracking.\"\"\"\n    test_case: TestCase\n    start_time: float\n    end_time: Optional[float] = None\n    error: Optional[Exception] = None\n    assertion_failure: Optional[AssertionError] = None\n    fixtures_loaded: List[str] = []\n    \n\nclass SimpleRunner:\n    \"\"\"Execute tests with proper error isolation and handling.\"\"\"\n    \n    def run_test(self, test_case: TestCase) -> TestResult:\n        \"\"\"\n        Execute a single test and return its result with error handling.\n        \n        TODO 1: Initialize RunContext with test_case and start_time\n        TODO 2: Set TestResult status to RUNNING\n        TODO 3: Try to load required fixtures using FixtureLifecycleManager\n            - If fixture setup fails: catch FixtureError, set status to ERRORED\n            - Store fixture names in context.fixtures_loaded\n        TODO 4: If fixtures loaded successfully, execute test function\n            - Wrap in try-except to catch AssertionError (test failure)\n            - Wrap in another try-except to catch other Exception (test error)\n            - If KeyboardInterrupt: re-raise to allow framework shutdown\n        TODO 5: After test execution (success or error), run teardown\n            - Call FixtureLifecycleManager.teardown_scope for function-scoped fixtures\n            - Catch and log teardown errors (don't affect test status)\n        TODO 6: Determine final TestStatus:\n            - If assertion_failure: FAILED\n            - If error (non-assertion): ERRORED\n            - If no exceptions: PASSED\n        TODO 7: Build TestResult with appropriate message and traceback\n        TODO 8: Calculate duration and return TestResult\n        \"\"\"\n        # TODO: Implement the above steps\n        pass\n    \n    \n    def run_suite(self, suite: TestSuite) -> List[TestResult]:\n        \"\"\"\n        Run all tests in a suite and return their results.\n        \n        TODO 1: Initialize empty list for results\n        TODO 2: For each test in suite.tests:\n            - Call run_test(test_case)\n            - Append result to results list\n            - If KeyboardInterrupt caught: break loop, run cleanup\n        TODO 3: Return results list\n        TODO 4: Ensure tests run in isolation (no shared state between iterations)\n        \"\"\"\n        # TODO: Implement the above steps\n        pass\n```\n\n**Parallel runner error handling skeleton (`runner/parallel_runner.py`):**\n\n```python\n\"\"\"Parallel test runner with worker error handling.\"\"\"\n\nimport multiprocessing\nfrom typing import List, Tuple\nfrom .simple_runner import SimpleRunner, TestResult\nfrom ..data_model import TestSuite\n\n\nclass ParallelRunner:\n    \"\"\"Run tests in parallel processes with error isolation.\"\"\"\n    \n    def run_suite(self, suite: TestSuite, max_workers: Optional[int] = None) -> List[TestResult]:\n        \"\"\"\n        Run tests in parallel using worker processes.\n        \n        TODO 1: Split test suite into chunks for each worker\n        TODO 2: Create multiprocessing.Pool with max_workers\n        TODO 3: Define worker function that runs tests and returns (results, errors)\n        TODO 4: Use pool.map_async to distribute work\n        TODO 5: Set up timeout and handle worker crashes:\n            - If worker dies: mark its tests as ERRORED with \"worker crashed\" message\n        TODO 6: Collect results from all workers\n        TODO 7: Handle KeyboardInterrupt: terminate pool, collect partial results\n        TODO 8: Combine all results and return\n        \"\"\"\n        # TODO: Implement the above steps\n        pass\n    \n    \n    def _worker_function(self, test_chunk: List[TestCase]) -> Tuple[List[TestResult], List[dict]]:\n        \"\"\"\n        Worker process function - runs tests and returns results.\n        \n        TODO 1: Create SimpleRunner instance inside worker\n        TODO 2: Run each test, collecting results\n        TODO 3: Catch all exceptions and serialize for return to main process\n        TODO 4: Return (results, errors) tuple\n        \"\"\"\n        # TODO: Implement the above steps\n        pass\n```\n\n#### Language-Specific Hints\n\n- **Exception chaining**: Use `raise NewError from original_error` to preserve exception context in Python 3\n- **Signal handling**: Use `signal.signal(signal.SIGINT, handler)` but be careful in multi-threaded contexts\n- **Multiprocessing errors**: Worker processes should use `sys.exit(1)` on critical errors, not raise exceptions\n- **Traceback capture**: Use `traceback.format_exception()` for full traceback strings\n- **Resource cleanup**: Use `contextlib.ExitStack` for managing multiple resources with cleanup\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"Test passes but fixture not cleaned up\" | Exception in teardown swallowed | Add debug logging in fixture finally blocks | Ensure teardown errors are logged |\n| \"Worker process hangs forever\" | Deadlock in shared resources or fixture | Check for locks not released in fixtures | Use timeout in `pool.map_async`, avoid shared state |\n| \"KeyboardInterrupt doesn't stop tests\" | Signal handler not propagating to workers | Check signal handling in worker processes | Use `pool.terminate()` on interrupt |\n| \"Error message shows <class '__main__.MyError'>\" | Error class not pickleable in parallel mode | Check if error class is defined at module level | Define error classes in importable modules |\n| \"Fixture circular dependency not detected\" | Dependency resolution doesn't detect cycles | Add dependency graph validation | Implement cycle detection using DFS |\n\n#### Milestone Checkpoint\n\nAfter implementing error handling, verify with this test scenario:\n\n```bash\n# Create test file with various errors\ncat > test_errors.py << 'EOF'\nimport sys\n\ndef test_normal():\n    assert 1 == 1\n\ndef test_failure():\n    assert 1 == 2  # Should be FAILED\n\ndef test_error():\n    raise ValueError(\"Something went wrong\")  # Should be ERRORED\n\ndef test_fixture_error(boom_fixture):\n    pass  # Never reached if fixture fails\nEOF\n\n# Run with error handling\npython -m apollo.cli test_errors.py -v\n```\n\n**Expected output**:\n```\ntest_errors.py::test_normal ... PASSED\ntest_errors.py::test_failure ... FAILED\ntest_errors.py::test_error ... ERRORED\ntest_errors.py::test_fixture_error ... ERRORED (Fixture 'boom_fixture' setup failed: ...)\n\n=== Summary ===\n4 tests, 1 passed, 1 failed, 2 errored in 0.12s\n```\n\n**Signs of problems**:\n- Tests stop after first failure/error (missing error isolation)\n- No distinction between FAILED and ERRORED statuses\n- No error messages or tracebacks in output\n- Framework crashes when test raises exception\n\n\n## 11. Testing Strategy\n\n> **Milestone(s):** All four milestones, as testing the framework is an ongoing concern that evolves with each milestone.\n\nTesting a test framework presents a unique **meta-problem**: we must build a system that verifies other code while simultaneously verifying that our verification system itself works correctly. This section addresses this recursive challenge with a layered strategy combining **self-validation** (using the framework to test itself), **property-based verification** (ensuring mathematical correctness of assertions), and **golden master testing** (validating output formats against known-good references). We also provide concrete milestone checkpoints that serve as integration tests for each development phase, ensuring the framework meets its acceptance criteria.\n\n### Testing the Framework\n\n#### Self-Validation: The Framework as Its First User\n\n> **Mental Model: \"Eating Your Own Dog Food\"** – The most powerful validation of a test framework comes from using it to test itself. This approach creates a virtuous cycle: as we add features to the framework, we immediately write tests for those features using the framework itself. Any bugs in the framework will be caught by the framework's own tests, creating immediate feedback.\n\n**Self-validation strategy** involves organizing the framework's codebase with clear separation between the **framework implementation** (`src/apollo/`) and **framework tests** (`tests/`). The test suite for the framework should mirror its architecture:\n\n| Test Category | What It Tests | Example Test Cases |\n|---------------|---------------|-------------------|\n| **Discovery Tests** | Test the `Discoverer` component's ability to find test functions and classes | `test_discover_tests_finds_test_functions`, `test_discover_tests_handles_nested_modules`, `test_discover_tests_respects_patterns` |\n| **Runner Tests** | Test the `Runner`'s execution logic and isolation mechanisms | `test_run_test_returns_test_result`, `test_test_isolation_state_reset`, `test_parallel_execution_no_race_conditions` |\n| **Assertion Tests** | Test the `Assertion Engine`'s correctness and error messages | `test_assert_equal_passes_when_equal`, `test_assert_equal_fails_with_diff`, `test_assert_raises_captures_exception` |\n| **Fixture Tests** | Test the `FixtureRegistry` and `FixtureLifecycleManager` | `test_fixture_scope_function_creates_per_test`, `test_fixture_dependency_injection`, `test_fixture_teardown_on_error` |\n| **CLI & Reporting Tests** | Test the `CLI Parser` and `Reporter` components | `test_cli_parses_file_patterns`, `test_reporter_generates_junit_xml`, `test_exit_code_on_failure` |\n\n**Implementation approach:** The framework's test suite should be runnable by the framework itself (self-hosting) as well as by other test runners (like `pytest`) for comparison. This dual approach provides a **cross-validation** mechanism: if both the framework and an established runner like `pytest` pass the same test suite, we gain confidence in correctness.\n\n> **ADR: Self-Hosting vs. External Test Runner**\n> - **Context:** We need to verify the framework's implementation but cannot trust it completely during development.\n> - **Options Considered:**\n>   1. **Pure self-hosting:** Only use the framework to test itself\n>   2. **External runner only:** Use `pytest` to test the framework\n>   3. **Dual approach:** Use both, with framework tests written to be compatible with both runners\n> - **Decision:** Dual approach (Option 3)\n> - **Rationale:** Pure self-hosting creates a circular dependency where bugs can hide (a bug in assertion logic might cause assertion tests to incorrectly pass). External-only testing doesn't exercise the framework's own discovery/execution path. The dual approach provides cross-validation: if `pytest` and our framework both pass the same test, confidence is high. It also allows us to compare output formats.\n> - **Consequences:** Requires writing tests that are compatible with both runners (minimal use of framework-specific decorators), adds some complexity to test setup, but provides strongest validation.\n\n#### Property-Based Testing for Assertions\n\n> **Mental Model: \"Fuzzing the Assertion Engine\"** – Property-based testing treats the assertion engine as a mathematical function with certain invariants that must always hold, regardless of input. We generate thousands of random inputs to verify these invariants systematically.\n\nThe assertion engine's correctness can be verified through **property-based tests** using a library like `hypothesis`. These tests define logical properties that should hold for all inputs:\n\n| Property | Description | Example Implementation |\n|----------|-------------|------------------------|\n| **Reflexivity** | `assert_equal(x, x)` should always pass | Generate random Python values, verify assertion passes |\n| **Symmetry** | If `assert_equal(a, b)` passes, then `assert_equal(b, a)` should also pass | Generate random pairs where equality holds |\n| **Transitivity** | If `assert_equal(a, b)` and `assert_equal(b, c)` pass, then `assert_equal(a, c)` should pass | Generate chains of equal values |\n| **Exception Propagation** | `assert_raises(ExceptionType, func)` should fail if `func` doesn't raise `ExceptionType` | Generate random functions and exception types |\n| **Float Tolerance** | `assert_equal(a, b, tolerance=0.1)` should pass when `abs(a - b) <= 0.1` | Generate float pairs within and outside tolerance |\n\n**Algorithm for property-based assertion testing:**\n1. Define a hypothesis strategy that generates relevant test data (integers, strings, lists, custom objects)\n2. For each property, write a test that uses `@given` decorator to generate many input examples\n3. In the test body, execute the assertion with generated inputs\n4. Verify the assertion behaves correctly (either passes or raises appropriate `AssertionError`)\n5. For failure cases, also verify the error message contains the expected information (actual vs expected)\n\nThis approach systematically uncovers edge cases like `NaN != NaN`, `-0.0 == 0.0`, or recursive data structures that might cause infinite recursion in diff generation.\n\n#### Golden Master Tests for Output\n\n> **Mental Model: \"Snapshot Testing for Reports\"** – Golden master testing captures known-good output (the \"golden master\") and compares future test runs against this reference. This ensures the reporting format remains stable and compatible.\n\nThe reporting system, particularly JUnit XML generation, must produce consistent, well-formed output that integrates with CI/CD tools. **Golden master testing** validates this by:\n\n1. **Capturing reference output:** Run a known test suite and save the console output and JUnit XML to `tests/golden_masters/`\n2. **Comparing during test runs:** In the test suite, run the same test suite again and compare output character-by-character (with allowances for timing differences and paths)\n3. **Updating references:** Provide a script to update golden masters when intentional changes are made to output format\n\n**Key areas for golden master testing:**\n\n| Output Type | What to Verify | Comparison Strategy |\n|-------------|---------------|---------------------|\n| **Console Output** | Formatting, colors, indentation, progress indicators | Normalize timing (replace `0.123s` with `{duration}`), ignore ANSI color codes in comparison |\n| **JUnit XML** | XML structure, attribute values, escaping, CDATA sections | Parse both as XML trees and compare structurally; allow differences in `timestamp` and `time` attributes |\n| **Exit Codes** | Process return values for success/failure scenarios | Exact integer comparison |\n| **Error Messages** | Format of assertion failures and exception traces | Normalize file paths and line numbers to placeholders |\n\n**Implementation workflow for golden master updates:**\n1. When making intentional changes to output format, run `python -m tests.update_golden_masters`\n2. This re-generates all golden master files from the current implementation\n3. Review the diff to ensure only expected changes appear\n4. Commit the updated golden masters alongside code changes\n\n> **ADR: Exact vs. Fuzzy Golden Master Comparison**\n> - **Context:** Test output contains variable data (timings, file paths, process IDs) that change between runs.\n> - **Options Considered:**\n>   1. **Exact string comparison:** Requires perfectly reproducible runs (fixed timestamps, paths)\n>   2. **Normalized comparison:** Replace variable parts with placeholders before comparing\n>   3. **Structural comparison:** Parse output (e.g., XML) and compare logical structure ignoring variable attributes\n> - **Decision:** Hybrid approach - normalized comparison for console output, structural comparison for XML\n> - **Rationale:** Console output benefits from simple normalization (regex replacement of timings/paths). XML benefits from structural comparison which ignores attribute order and whitespace differences while still validating required fields and hierarchy.\n> - **Consequences:** Requires implementing both normalization patterns and XML comparison logic, but provides robust validation without fragile exact matching.\n\n#### Common Pitfalls in Testing the Framework\n\n⚠️ **Pitfall: Recursive Test Discovery Loop**\n- **Description:** When the framework's test suite uses the framework itself for discovery, and the discovery logic has a bug that causes it to recursively discover the test suite's test files infinitely.\n- **Why it's wrong:** Causes infinite recursion, stack overflow, or extremely slow test runs.\n- **How to avoid:** In the framework's own test configuration, explicitly limit discovery paths to avoid re-scanning the framework source code. Use a dedicated `tests/` directory separate from `src/`.\n\n⚠️ **Pitfall: Assertion Tests That Don't Actually Test Assertions**\n- **Description:** Writing `assert_equal(2, 1+1)` in a test for the `assert_equal` function - this tests Python's addition, not the assertion logic.\n- **Why it's wrong:** The test passes even if `assert_equal` is completely broken (e.g., always passes).\n- **How to avoid:** Test assertion functions by verifying they **raise** `AssertionError` when they should fail. Use `assert_raises` (from the framework or unittest) to check that `assert_equal(1, 2)` raises.\n\n⚠️ **Pitfall: Golden Master Tests That Are Too Brittle**\n- **Description:** Golden tests fail due to insignificant differences (millisecond timing variations, temporary file path differences).\n- **Why it's wrong:** Creates noise, causes tests to fail randomly, reduces trust in test suite.\n- **How to avoid:** Implement robust normalization: replace timings with placeholders, use relative paths, sort dictionary outputs, ignore whitespace differences in XML comparison.\n\n⚠️ **Pitfall: Fixture Tests That Don't Clean Up Properly**\n- **Description:** Tests for the fixture system leave resources (files, database connections) open, causing subsequent tests to fail.\n- **Why it's wrong:** Tests aren't isolated, causing flaky failures that depend on execution order.\n- **How to avoid:** Each fixture test should run in a subprocess, or should explicitly verify teardown occurred. Use context managers even in tests to ensure cleanup.\n\n### Milestone Checkpoints\n\nEach milestone includes concrete verification steps that serve as **integration tests** for that milestone's functionality. These checkpoints assume the framework is invoked as `apollo` from the command line after installation.\n\n#### Milestone 1: Test Discovery & Execution\n\n**Test File Structure:**\n```\ntest_milestone1/\n├── test_simple.py\n├── test_isolated.py\n└── nested/\n    └── test_nested.py\n```\n\n**Example Test File (`test_simple.py`):**\n```python\n# test_simple.py\ndef test_addition():\n    assert 1 + 1 == 2\n\ndef test_subtraction():\n    assert 5 - 3 == 2\n\ndef test_failure():\n    assert 2 * 2 == 5  # This will fail\n\ndef some_helper():\n    return 42  # Not a test - shouldn't be discovered\n```\n\n**Verification Commands and Expected Output:**\n\n| Command | Expected Output | What to Verify |\n|---------|----------------|----------------|\n| `apollo discover test_milestone1/` | Lists 3 test functions: `test_addition`, `test_subtraction`, `test_failure` from `test_simple.py` and any from other test files | Discovery finds test-prefixed functions, ignores non-test functions |\n| `apollo run test_milestone1/` | Runs 3 tests, shows 2 passed, 1 failed, with test names and failure message for `test_failure` | Runner executes tests, captures failures, continues execution after failure |\n| `apollo run test_milestone1/ --parallel` | Same results as above, but runs tests concurrently (may complete faster) | Parallel execution works, tests remain isolated |\n| `apollo run test_milestone1/test_simple.py::test_addition` | Runs only `test_addition`, shows 1 passed | Test selection by file and name works |\n\n**Success Criteria:**\n- All 3 tests are discovered and executed\n- `test_addition` and `test_subtraction` pass\n- `test_failure` fails with a clear message showing `4 != 5` or similar\n- The helper function `some_helper` is NOT discovered as a test\n- Execution continues after `test_failure` (doesn't stop the whole run)\n- Parallel execution completes (may show different order but same results)\n\n#### Milestone 2: Assertions & Matchers\n\n**Test File Structure:**\n```\ntest_milestone2/\n├── test_assertions.py\n└── test_matchers.py\n```\n\n**Example Test File (`test_assertions.py`):**\n```python\n# test_assertions.py\nfrom apollo import assert_equal, assert_true, assert_raises\n\ndef test_basic_assertions():\n    assert_equal(1, 1)\n    assert_equal([1, 2], [1, 2])\n    assert_true(True)\n\ndef test_assert_equal_failure():\n    # This should fail with helpful message\n    assert_equal([1, 2, 3], [1, 2, 4])\n\ndef test_assert_raises():\n    with assert_raises(ValueError):\n        int(\"not_a_number\")\n\ndef test_assert_raises_failure():\n    # This should fail because no exception raised\n    with assert_raises(ValueError):\n        int(\"42\")\n```\n\n**Verification Commands and Expected Output:**\n\n| Command | Expected Output | What to Verify |\n|---------|----------------|----------------|\n| `apollo run test_milestone2/test_assertions.py` | 3 tests pass, 1 test fails (`test_assert_equal_failure`) with diff showing lists differ at index 2, 1 test fails (`test_assert_raises_failure`) with message about expected exception | `assert_equal` shows diff, `assert_raises` catches missing exceptions |\n| `apollo run test_milestone2/test_assertions.py::test_assert_equal_failure -v` | Detailed output showing: `Expected: [1, 2, 3]`, `Actual: [1, 2, 4]`, diff highlighting position 2 | Failure messages are helpful and show comparison |\n\n**Custom Matcher Test (`test_matchers.py`):**\n```python\n# test_matchers.py\nfrom apollo import assert_that\nfrom apollo.matchers import Matcher\n\nclass IsEven(Matcher):\n    def __matches__(self, actual):\n        return actual % 2 == 0\n    \n    def __describe__(self):\n        return \"an even number\"\n    \n    def __describe_mismatch__(self, actual):\n        return f\"{actual} is odd\"\n\ndef test_custom_matcher():\n    assert_that(4, IsEven())\n    # Next line should fail with \"Expected an even number, but 3 is odd\"\n    assert_that(3, IsEven())\n```\n\n**Verification Command:**\n- `apollo run test_milestone2/test_matchers.py` - Should show 1 pass, 1 fail with custom message \"Expected an even number, but 3 is odd\"\n\n**Success Criteria:**\n- Basic assertions (`assert_equal`, `assert_true`) work correctly\n- `assert_equal` on lists shows diff highlighting difference\n- `assert_raises` passes when exception raised, fails with clear message when not\n- Custom matchers work with `assert_that` and produce domain-specific messages\n\n#### Milestone 3: Fixtures & Setup/Teardown\n\n**Test File Structure:**\n```\ntest_milestone3/\n├── test_fixtures.py\n├── test_setup_teardown.py\n└── conftest.py\n```\n\n**Example Files:**\n```python\n# conftest.py\nimport tempfile\nimport os\nfrom apollo import fixture\n\n@fixture(scope=\"function\")\ndef temp_file():\n    f = tempfile.NamedTemporaryFile(delete=False)\n    yield f.name\n    os.unlink(f.name)\n\n@fixture(scope=\"module\")\ndef shared_data():\n    return {\"counter\": 0}\n\n# test_fixtures.py\ndef test_uses_fixture(temp_file):\n    # temp_file is automatically injected\n    assert os.path.exists(temp_file)\n    with open(temp_file, 'w') as f:\n        f.write(\"test\")\n    # File should be deleted after test by fixture teardown\n\ndef test_shared_fixture(shared_data):\n    shared_data[\"counter\"] += 1\n    assert shared_data[\"counter\"] == 1  # First test to use it\n\ndef test_shared_again(shared_data):\n    # If scope=\"module\", this should see counter=1 from previous test\n    # If scope=\"function\", this should see counter=0\n    # We'll test module scope\n    assert shared_data[\"counter\"] == 1\n\n# test_setup_teardown.py\nfrom apollo import TestCase\n\nclass TestExample(TestCase):\n    def setUp(self):\n        self.data = []\n    \n    def tearDown(self):\n        assert len(self.data) == 1  # Verify test ran\n    \n    def test_setup_works(self):\n        self.data.append(1)\n        assert self.data == [1]\n```\n\n**Verification Commands and Expected Output:**\n\n| Command | Expected Output | What to Verify |\n|---------|----------------|----------------|\n| `apollo run test_milestone3/test_fixtures.py -v` | All 3 tests pass. Shows fixture setup/teardown in verbose output. Temp file is created and deleted. | Fixture injection works, function-scoped fixture recreated per test, module-scoped fixture shared |\n| `apollo run test_milestone3/test_setup_teardown.py` | Test passes. If `tearDown` assertion fails, test fails (verifying `tearDown` runs even after test failure in some implementations) | `setUp` runs before test, `tearDown` runs after |\n| `apollo run test_milestone3/ --setup-show` (if implemented) | Shows fixture setup/teardown hierarchy | Verbose fixture debugging works |\n\n**Success Criteria:**\n- Tests receive fixture values via parameter names\n- Function-scoped fixtures created fresh for each test\n- Module-scoped fixtures created once and shared across tests in same module\n- Fixtures clean up resources (temp file deleted)\n- `setUp` and `tearDown` methods called for test classes\n- Circular dependency detection (if attempted) gives clear error\n\n#### Milestone 4: Reporting & CLI\n\n**Test File Structure:**\n```\ntest_milestone4/\n├── test_reporting.py\n└── test_cli.py\n```\n\n**Example Mixed-Result Test File:**\n```python\n# test_reporting.py\nimport time\n\ndef test_pass():\n    assert True\n\ndef test_fail():\n    assert False, \"Intentional failure\"\n\ndef test_error():\n    raise RuntimeError(\"Unexpected error\")\n\ndef test_slow():\n    time.sleep(0.1)\n    assert True\n```\n\n**Verification Commands and Expected Output:**\n\n| Command | Expected Output | What to Verify |\n|---------|----------------|----------------|\n| `apollo run test_milestone4/` | Shows 4 tests: 1 pass, 1 fail, 1 error, 1 pass. Summary: \"4 tests, 1 passed, 1 failed, 1 error, 1 passed\" with total time. Exit code 1. | Basic reporting shows status per test, summary statistics, non-zero exit on failure |\n| `apollo run test_milestone4/ --verbose` | Shows detailed output for each test including traceback for error and failure | Verbose flag shows more details |\n| `apollo run test_milestone4/ --junit-xml=results.xml` | Creates `results.xml` with 4 testcases, each with correct status and timing. Exit code 1. | JUnit XML generated, can be parsed by CI tools |\n| `apollo run test_milestone4/ --quiet` | Shows only summary line, no per-test output | Quiet flag suppresses details |\n| `apollo run test_milestone4/ --show-durations` | Shows timing for each test, slowest test highlighted | Duration reporting works |\n| `apollo --help` | Shows usage instructions, all command-line options | CLI help is comprehensive |\n\n**Success Criteria:**\n- Console output clearly distinguishes passed/failed/errored tests (colors if supported)\n- Summary statistics are accurate (counts, total time)\n- Exit code is 0 when all tests pass, non-zero when any fail or error\n- JUnit XML is well-formed, contains all required attributes (`name`, `status`, `time`), escapes special characters\n- Command-line flags work as documented (verbose, quiet, junit-xml, show-durations)\n- Help text is clear and complete\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Self-Testing** | Use Python's `unittest` for framework tests | Use the framework itself + `pytest` for cross-validation |\n| **Property-Based Testing** | `hypothesis` library for generating test cases | Custom generators for framework-specific types |\n| **Golden Master Testing** | Simple file comparison with regex normalization | Structural XML comparison using `xml.etree.ElementTree` |\n| **Test Isolation** | `subprocess` module to run framework in separate process | In-process isolation with careful cleanup |\n\n#### Recommended File/Module Structure for Tests\n\n```\napollo/                    # Framework source\ntests/                    # Framework tests\n├── __init__.py\n├── conftest.py           # Shared fixtures for testing the framework\n├── test_discovery.py     # Tests for Milestone 1\n├── test_assertions.py    # Tests for Milestone 2  \n├── test_fixtures.py      # Tests for Milestone 3\n├── test_cli.py           # Tests for Milestone 4\n├── property_based/       # Property-based tests\n│   ├── test_assertion_properties.py\n│   └── strategies.py     # Custom hypothesis strategies\n├── golden_masters/       # Reference outputs\n│   ├── console_output.txt\n│   ├── junit_output.xml\n│   └── update.py         # Script to update golden masters\n├── test_files/           # Test files used by the tests\n│   ├── milestone1/\n│   ├── milestone2/\n│   └── ...\n└── integration/          # Integration tests\n    ├── test_self_host.py # Framework tests itself\n    └── test_examples.py  # Example user tests\n```\n\n#### Infrastructure Starter Code for Golden Master Testing\n\n```python\n# tests/golden_masters/__init__.py\n\"\"\"Golden master testing utilities.\"\"\"\nimport re\nimport os\nimport xml.etree.ElementTree as ET\nfrom pathlib import Path\n\nGOLDEN_MASTER_DIR = Path(__file__).parent\n\ndef normalize_output(output: str) -> str:\n    \"\"\"Normalize variable parts of test output for comparison.\"\"\"\n    # Replace timings like \"0.123s\" with \"{duration}\"\n    output = re.sub(r'\\d+\\.\\d{3}s', '{duration}', output)\n    # Replace file paths with placeholders\n    output = re.sub(r'/tmp/[a-zA-Z0-9_]+', '{temp_file}', output)\n    # Remove ANSI color codes\n    output = re.sub(r'\\x1b\\[[0-9;]*m', '', output)\n    return output\n\ndef compare_xml(actual: str, expected: str) -> list[str]:\n    \"\"\"Compare two XML strings structurally, ignoring whitespace and order.\"\"\"\n    def normalize_xml(xml_str: str) -> ET.Element:\n        \"\"\"Parse and normalize XML for comparison.\"\"\"\n        root = ET.fromstring(xml_str)\n        # Sort attributes for consistent comparison\n        for el in root.iter():\n            if el.attrib:\n                # Sort attributes alphabetically\n                el.attrib = {k: el.attrib[k] for k in sorted(el.attrib)}\n        return root\n    \n    actual_root = normalize_xml(actual)\n    expected_root = normalize_xml(expected)\n    \n    differences = []\n    # Simple comparison - in practice, use more sophisticated XML diff\n    if ET.tostring(actual_root) != ET.tostring(expected_root):\n        differences.append(\"XML structure differs\")\n    \n    return differences\n\ndef load_golden_master(name: str) -> str:\n    \"\"\"Load a golden master file.\"\"\"\n    path = GOLDEN_MASTER_DIR / f\"{name}.txt\"\n    return path.read_text()\n\ndef save_golden_master(name: str, content: str):\n    \"\"\"Save content as a golden master.\"\"\"\n    path = GOLDEN_MASTER_DIR / f\"{name}.txt\"\n    path.write_text(content)\n```\n\n#### Core Logic Skeleton for Self-Testing\n\n```python\n# tests/test_self_host.py\n\"\"\"Test that the framework can run its own test suite.\"\"\"\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef test_framework_can_run_itself():\n    \"\"\"Test that Apollo can discover and run its own test suite.\"\"\"\n    # Get the path to the tests directory\n    tests_dir = Path(__file__).parent\n    \n    # TODO 1: Import the framework's main entry point\n    # TODO 2: Use discover_tests to find tests in tests/test_discovery.py\n    # TODO 3: Run the discovered tests using SimpleRunner\n    # TODO 4: Verify all tests pass (or at least no new failures)\n    # TODO 5: Compare with running same tests via pytest for cross-validation\n    pass\n\ndef test_cli_interface():\n    \"\"\"Test the CLI end-to-end.\"\"\"\n    # TODO 1: Use subprocess.run to execute \"python -m apollo.cli --help\"\n    # TODO 2: Verify exit code is 0 and help text contains expected options\n    # TODO 3: Run a known test file via CLI\n    # TODO 4: Capture output and verify it matches expected format\n    # TODO 5: Verify exit code is correct based on test results\n    pass\n```\n\n#### Property-Based Testing Skeleton\n\n```python\n# tests/property_based/test_assertion_properties.py\n\"\"\"Property-based tests for assertion engine.\"\"\"\nfrom hypothesis import given, strategies as st, assume\nimport pytest\n\n# TODO 1: Import the assertion functions from the framework\n# from apollo.assertions import assert_equal, AssertionError\n\n@given(st.integers(), st.integers())\ndef test_assert_equal_reflexive(a: int, b: int):\n    \"\"\"assert_equal(x, x) should always pass.\"\"\"\n    # TODO 2: For all integers a, assert_equal(a, a) should not raise\n    # Use try/except to catch AssertionError\n    # If AssertionError is raised, that's a test failure\n    pass\n\n@given(st.lists(st.integers()))\ndef test_assert_equal_list_self_comparison(lst: list):\n    \"\"\"assert_equal on a list compared to itself should pass.\"\"\"\n    # TODO 3: assert_equal(lst, lst) should always pass\n    # This tests that the comparison handles nested structures\n    pass\n\n@given(st.floats(allow_nan=False), st.floats(allow_nan=False))\ndef test_assert_equal_with_tolerance(a: float, b: float):\n    \"\"\"assert_equal with tolerance should pass when values are close.\"\"\"\n    # TODO 4: If abs(a - b) <= 0.1, then assert_equal(a, b, tolerance=0.1) should pass\n    # TODO 5: Otherwise, it should raise AssertionError\n    # Use hypothesis assume to filter test cases\n    pass\n```\n\n#### Milestone Checkpoint Implementation\n\n```python\n# tests/integration/test_milestone_checkpoints.py\n\"\"\"Integration tests for each milestone checkpoint.\"\"\"\nimport subprocess\nimport tempfile\nimport textwrap\n\ndef test_milestone1_checkpoint():\n    \"\"\"Verify Milestone 1 acceptance criteria.\"\"\"\n    # Create a temporary test file\n    test_code = textwrap.dedent(\"\"\"\n        def test_pass():\n            assert 1 + 1 == 2\n        \n        def test_fail():\n            assert 2 * 2 == 5\n    \"\"\")\n    \n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(test_code)\n        test_file = f.name\n    \n    try:\n        # TODO 1: Run the test file using the framework\n        # TODO 2: Capture output and exit code\n        # TODO 3: Verify: 2 tests discovered, 1 passed, 1 failed\n        # TODO 4: Verify: Execution continues after failure\n        # TODO 5: Verify: Non-test functions are not discovered\n        pass\n    finally:\n        import os\n        os.unlink(test_file)\n```\n\n#### Debugging Tips for Framework Tests\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Tests pass when run with pytest but fail with framework | Framework bug in discovery or execution | Run with `--verbose` in both runners, compare which tests are found and their execution order | Check `discover_tests` logic for missed tests or incorrect test case creation |\n| Property-based tests find edge case failures | Assertion logic doesn't handle special values (NaN, inf, -0.0) | Add debug print in assertion to see the failing values | Add special handling for edge cases in comparison logic |\n| Golden master tests fail on CI but pass locally | Path or timing differences | Compare normalized output, check what differs | Improve normalization to handle CI environment (e.g., container paths) |\n| Fixture tests leak resources affecting subsequent tests | Teardown not called or not cleaning completely | Add logging to fixture teardown, check if it's called | Ensure `yield` fixture properly cleans up in finally block |\n| Self-test causes infinite recursion | Discovery scanning framework source code | Add debug prints to see what paths are being scanned | Limit discovery to test directories only in framework's own tests |\n\n#### Language-Specific Hints for Python\n\n- Use `sys.path.insert(0, str(project_root))` in test setup to ensure framework modules are importable\n- For subprocess testing, use `subprocess.run(capture_output=True, text=True)` to capture stdout/stderr\n- Use `tempfile.TemporaryDirectory()` context manager for tests that need temporary files\n- For mocking time in duration tests, use `unittest.mock.patch('time.time', side_effect=[start, end])`\n- When comparing floats in tests, use `pytest.approx()` instead of exact equality to avoid precision issues\n\n\n## 12. Debugging Guide\n\n> **Milestone(s):** All four milestones, as debugging strategies are essential throughout development.\n\nEven the most meticulously designed system will encounter bugs during implementation. This guide serves as your **troubleshooting playbook**, helping you diagnose and fix common issues that arise when building a test framework. Think of debugging as **forensic analysis**—you're presented with symptoms (unexpected test behavior, crashes, or incorrect outputs), and must trace them back to their root cause in the code. This section provides a systematic approach to investigation, starting with a catalog of known issues and their fixes, followed by proactive techniques for uncovering hidden problems.\n\n### Common Bugs & Fixes\n\nThe following table documents common symptoms you might encounter during development, organized by the subsystem where they typically originate. Each entry follows a **symptom → cause → investigation → fix** pattern, transforming vague problems into concrete action plans.\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Tests pass but fixtures aren't cleaned up** (e.g., database connections remain open, temporary files persist) | **Fixture teardown not triggered:** The `FixtureLifecycleManager.teardown_scope()` method isn't being called at the appropriate **scope boundary**, or the fixture function isn't structured as a generator (using `yield`) when it should be. | 1. Add debug logging to `teardown_scope()` to see if/when it's called.<br>2. Check if fixture functions use `yield` for function-scoped cleanup.<br>3. Verify that `FixtureLifecycleManager._generators` dictionary is being populated and cleared. | 1. Ensure runner calls `teardown_scope()` after all tests in a scope complete.<br>2. Convert `@fixture` functions to use `yield` pattern: `setup; yield resource; cleanup`.<br>3. In `get_fixture_value()`, store yielded generators in `_generators` and ensure finalization. |\n| **Fixture value reused incorrectly across tests** (state leaks between tests) | **Incorrect caching key:** The `FixtureRequest.cache_key` doesn't include all necessary scope identifiers (e.g., test class name for class scope). Different tests mistakenly receive the same cached fixture instance. | 1. Print `cache_key` for each fixture request during test execution.<br>2. Check if `_cache` dictionary keys differ between tests that should have separate instances.<br>3. Verify `FixtureRequest` creation includes correct `scope_id` based on scope. | 1. Update `cache_key` calculation to include: `(fixture_name, scope, scope_id)` where `scope_id` is module name for MODULE scope, class name for CLASS scope, etc.<br>2. Ensure `FixtureLifecycleManager` creates unique `scope_id` values for each scope level. |\n| **Circular dependency error crashes discovery** | **Dependency graph cycle:** Fixture A depends on B, B depends on C, and C depends on A (directly or indirectly). The framework attempts infinite recursion when resolving dependencies. | 1. Check `Fixture.dependencies` lists for cycles.<br>2. Add cycle detection in `FixtureRegistry.get()` or `FixtureLifecycleManager.get_fixture_value()`.<br>3. Look for `RecursionError` in stack trace. | 1. Implement topological sort or cycle detection before fixture resolution.<br>2. Maintain `_active_requests` set in `FixtureLifecycleManager` to detect circular requests during resolution and raise a clear error. |\n| **Assertion failure messages show unhelpful `True != False`** | **Generic assertion implementation:** Using Python's built-in `assert` statement or a simple `assert condition, msg` without detailed diff generation. The framework isn't intercepting the assertion to provide rich context. | 1. Check if test uses framework's `assert_equal()` or just Python's `assert`.<br>2. Verify that assertion functions raise a custom exception (not plain `AssertionError`).<br>3. Look for missing `expected`/`actual` values in error output. | 1. Ensure all assertion functions (`assert_equal`, `assert_true`, etc.) construct an `AssertionFailure` with detailed context before raising.<br>2. Implement `__str__` method on `AssertionFailure` that formats expected vs actual with diff.<br>3. Use `inspect` module to capture the assertion call's arguments for better messages. |\n| **Test discovery misses functions in nested classes** | **Shallow traversal:** The `_find_tests_in_module()` function only examines top-level module attributes, not recursively searching within class definitions. | 1. Print all objects discovered during module scan.<br>2. Check if classes with `test_` methods are being examined.<br>3. Verify the discovery convention includes class methods. | 1. Implement recursive inspection: for each class in module, inspect its methods for `test_` prefix.<br>2. Update `TestCase` creation to include class name in `nodeid` (e.g., `TestClass.test_method`).<br>3. Consider supporting unittest-style test classes as well as plain functions. |\n| **Parallel tests interfere with each other (shared state)** | **Lack of process isolation:** When using `multiprocessing` for parallel execution, tests share module-level or global state because modules are imported in parent process and inherited by children. | 1. Check if test modifies a module-level variable and another test sees the change.<br>2. Verify each test runs in a separate process with fresh interpreter state.<br>3. Look for `pickle` errors when passing complex objects to worker processes. | 1. Ensure `_worker_function` re-imports modules in child process rather than inheriting parent's loaded modules.<br>2. Use `multiprocessing` with `'spawn'` start method (forces clean state).<br>3. Pass only serializable data (via `pickle`) between processes; reconstruct test execution context in worker. |\n| **JUnit XML contains invalid characters or malformed XML** | **Missing XML escaping:** Special characters (`<`, `>`, `&`, `\"`, `'`) in test names, error messages, or output aren't escaped before inclusion in XML. | 1. Examine generated XML with a validator or XML parser.<br>2. Look for parser errors mentioning \"unescaped\" or \"malformed\".<br>3. Check if `_escape_xml()` function is called on all text fields. | 1. Implement robust `_escape_xml()` that replaces `&`, `<`, `>`, `\"`, `'` with corresponding entities.<br>2. Use CDATA sections (`_create_cdata()`) for stack traces or multiline output that may contain special characters.<br>3. Ensure `JUnitFormatter` escapes all string fields before XML generation. |\n| **`assert_raises` doesn't catch exception or catches wrong one** | **Exception context management:** The `assert_raises` context manager isn't properly catching and comparing exception types, or it's swallowing exceptions incorrectly. | 1. Check if exception is raised but not caught by the context manager.<br>2. Verify exception type comparison uses `isinstance()` not exact type equality.<br>3. Look for exceptions propagating outside the test function. | 1. Implement `assert_raises` as a context manager that: catches exception, stores it, compares type, re-raises if mismatch.<br>2. Use `try/except` block in context manager's `__exit__` method.<br>3. Include exception message in assertion failure if provided. |\n| **Verbose output shows duplicate test names or missing tests** | **Incorrect `nodeid` generation:** The `TestCase.nodeid` field isn't unique across tests, or discovery is scanning the same module multiple times due to symlinks or duplicate paths. | 1. Print all discovered `nodeid` values.<br>2. Check if `resolve_patterns_to_paths()` returns duplicate absolute paths.<br>3. Verify `module_path_to_name()` produces consistent module names. | 1. Ensure `nodeid` includes full module path + function/class/method name (e.g., `tests/test_math.py::TestCalc::test_add`).<br>2. Normalize paths and deduplicate in discovery phase.<br>3. Use Python's `inspect.getsourcefile()` to get canonical file path. |\n| **Test status incorrectly reported as ERROR instead of FAILED** | **Misclassification of exceptions:** The runner catches all exceptions and marks tests as `ERRORED`, not distinguishing between assertion failures (FAILED) and unexpected errors. | 1. Check runner's exception handling logic in `SimpleRunner.run_test()`.<br>2. Verify if assertion functions raise a distinguishable exception type (e.g., `AssertionFailure` subclass).<br>3. Look for `TestResult.status` assignment logic. | 1. In `run_test`, catch `AssertionFailure` (or similar) separately from other exceptions.<br>2. Set `TestResult.status` to `FAILED` for assertion failures, `ERRORED` for other exceptions.<br>3. Store the exception in `TestResult.exception` for both cases. |\n| **`--quiet` flag still shows progress dots** | **Output logic bypasses verbosity check:** The reporter's output methods aren't respecting the `Configuration.quiet` flag, or progress indicators are printed directly from runner. | 1. Check where progress dots are printed (likely in runner or reporter).<br>2. Verify `Configuration` is passed to all components that output.<br>3. Look for `if not config.quiet:` guards missing. | 1. Centralize output through `OutputWriter` component that checks `quiet` and `verbose` flags.<br>2. Move progress indicator logic to reporter, not runner.<br>3. Add conditionals around all print statements that should respect quiet mode. |\n| **Float comparisons fail due to tiny precision differences** | **Exact equality for floats:** `assert_equal` uses `==` for float comparison instead of approximate equality with tolerance. | 1. Check `assert_equal` implementation for float handling.<br>2. Test with values like `0.1 + 0.2` (which isn't exactly `0.3`).<br>3. Look for missing `tolerance` parameter propagation. | 1. Add `tolerance` parameter to `assert_equal` (default `1e-9`).<br>2. Implement approximate comparison: `abs(actual - expected) <= tolerance`.<br>3. Create specialized `assert_almost_equal` function with relative/absolute tolerance options. |\n| **Fixture dependency injection fails with `TypeError`** | **Signature inspection error:** The framework fails to match parameter names in test function signature with available fixture names, or tries to pass incorrect arguments. | 1. Check error traceback for `inspect.signature` or `getargspec` failures.<br>2. Print test function parameters and available fixtures.<br>3. Verify `FixtureLifecycleManager.get_fixture_value()` returns appropriate values. | 1. Use `inspect.signature(test_func).parameters` to get parameter names safely.<br>2. Match parameter names to fixture names in registry (case-sensitive).<br>3. Handle optional parameters/default values gracefully (skip if fixture not found). |\n| **Test execution hangs indefinitely (deadlock)** | **Resource contention in parallel mode:** Multiple tests or fixtures acquire locks/resources in conflicting order, or a fixture generator doesn't yield. | 1. Check if deadlock occurs only with `--parallel` flag.<br>2. Look for `@fixture` functions without `yield` (blocking).<br>3. Check for shared resources (files, ports) without proper locking. | 1. Ensure fixture generators always yield (use `contextlib`'s `@contextmanager` as pattern).<br>2. Implement timeout mechanism for test execution.<br>3. Use resource pools or random port assignment for shared resources in parallel tests. |\n\n### Debugging Techniques\n\nWhen the symptom isn't in the table above, or you need to understand the system's internal state, employ these systematic debugging techniques. Think of yourself as a **framework archeologist**, uncovering the layers of execution to find where the behavior diverges from expectations.\n\n#### 1. Strategic Logging and Introspection\n\nThe most powerful technique is adding temporary logging at key architectural boundaries. Create a **debug mode** that can be enabled via environment variable or command-line flag, then instrument the framework's components to log their inputs, outputs, and state transitions.\n\n> **Debugging Principle:** Log data transformations, not just function entries. The most valuable logs show what data enters a component, how it's transformed, and what leaves.\n\n**What to log:**\n- **Discovery phase:** Log each file examined, module imported, and test function found with its `nodeid`.\n- **Fixture lifecycle:** Log fixture creation, caching (`cache_key`), and teardown events with scope information.\n- **Test execution:** Log test start/end, status transitions, and any exceptions caught.\n- **Assertion evaluation:** Log the `actual`/`expected` values and comparison results.\n- **Parallel execution:** Log worker process creation, test distribution, and result collection.\n\n**Implementation approach:**\n```python\n# In a debug module\nDEBUG = os.environ.get('APOLLO_DEBUG', '0') == '1'\n\ndef debug_log(component, message, data=None):\n    if DEBUG:\n        timestamp = time.strftime(\"%H:%M:%S\")\n        print(f\"[{timestamp}] [{component}] {message}\", file=sys.stderr)\n        if data:\n            print(f\"    Data: {data!r}\", file=sys.stderr)\n```\n\nUse this in components:\n```python\n# In SimpleRunner.run_test()\ndebug_log(\"Runner\", f\"Starting test {test_case.nodeid}\")\ntry:\n    result = # ... execute test\n    debug_log(\"Runner\", f\"Test completed with status {result.status}\")\nexcept Exception as e:\n    debug_log(\"Runner\", f\"Test raised exception: {e}\")\n```\n\n#### 2. Interactive Debugging with PDB\n\nFor complex issues where logging isn't enough, use Python's built-in debugger (`pdb`) to pause execution and inspect state interactively. This is especially useful for understanding **control flow** and **variable state** at specific moments.\n\n**Techniques:**\n- **Post-mortem debugging:** When a test fails unexpectedly, add `import pdb; pdb.post_mortem()` in the exception handler to examine the traceback.\n- **Breakpoint decorator:** Create a decorator that sets a breakpoint when a specific condition is met:\n  ```python\n  def breakpoint_on(condition):\n      def decorator(func):\n          def wrapper(*args, **kwargs):\n              if condition(*args, **kwargs):\n                  import pdb; pdb.set_trace()\n              return func(*args, **kwargs)\n          return wrapper\n      return decorator\n  \n  # Use to break when fixture name contains \"db\"\n  @breakpoint_on(lambda name, *_: \"db\" in name)\n  def get_fixture_value(self, fixture_name, test_case):\n      # ...\n  ```\n- **Remote debugging:** For parallel execution issues, use `rconsole` or `remote-pdb` to connect to worker processes.\n\n#### 3. State Inspection via Test Hooks\n\nAdd special **diagnostic hooks** that tests can use to report framework internal state. For example, create a `debug_fixtures()` function that tests can call to see which fixtures are currently cached and their values.\n\n**Example diagnostic utility:**\n```python\n# In framework's public API\ndef get_framework_state():\n    \"\"\"Return a snapshot of framework internal state for debugging.\"\"\"\n    state = {\n        'active_fixtures': list(lifecycle_manager._cache.keys()),\n        'pending_teardowns': list(lifecycle_manager._generators.keys()),\n        'test_queue_size': len(test_queue) if hasattr(runner, 'test_queue') else 0,\n    }\n    return state\n\n# Test can use it:\ndef test_debug_fixtures():\n    state = apollo.get_framework_state()\n    print(f\"Active fixtures: {state['active_fixtures']}\")\n    # Continue with test\n```\n\n#### 4. Minimal Reproduction Creation\n\nWhen encountering a bug, immediately create a **minimal reproduction case**—the smallest possible test file that demonstrates the issue. This isolates the problem from your larger codebase and makes debugging exponentially easier.\n\n**Process:**\n1. Start with the failing test in your project\n2. Remove all unrelated imports and setup\n3. Strip the test down to the essential lines that trigger the bug\n4. If it's a fixture issue, create a minimal fixture and test that shows the problem\n5. Save this as `reproduce_bug.py` and use it for focused debugging\n\n**Example minimal reproduction:**\n```python\n# reproduce_parallel_bug.py\nimport apollo\n\n@apollo.fixture\ndef shared():\n    return {\"counter\": 0}\n\ndef test_one(shared):\n    shared[\"counter\"] += 1\n    assert shared[\"counter\"] == 1  # Fails if shared across processes\n\ndef test_two(shared):\n    shared[\"counter\"] += 1  \n    assert shared[\"counter\"] == 1  # Should be fresh copy\n```\n\n#### 5. Component Isolation Testing\n\nTest each component in isolation by creating **unit tests that mock adjacent components**. This helps determine whether a bug originates in the component being tested or in its dependencies.\n\n**Strategy:**\n- **Mock the Discoverer** when testing the Runner\n- **Mock the Runner** when testing the Reporter  \n- **Use fake fixtures** when testing fixture dependency resolution\n- **Create in-memory implementations** of file system operations for discovery tests\n\n**Example isolated test:**\n```python\ndef test_runner_with_mocked_fixtures():\n    \"\"\"Test runner handles fixture injection errors gracefully.\"\"\"\n    mock_fixture_registry = Mock(spec=FixtureRegistry)\n    mock_fixture_registry.get.side_effect = KeyError(\"fixture not found\")\n    \n    runner = SimpleRunner(fixture_registry=mock_fixture_registry)\n    test_case = TestCase(nodeid=\"test\", func=lambda: None, ...)\n    \n    result = runner.run_test(test_case)\n    \n    assert result.status == TestStatus.ERRORED\n    assert \"fixture not found\" in str(result.exception)\n```\n\n#### 6. Visualization of Data Flow\n\nFor complex bugs involving multiple components, **draw the data flow** on paper or a whiteboard. Trace a test execution through:\n\n1. **CLI arguments** → `Configuration` object\n2. **File patterns** → concrete paths via `resolve_patterns_to_paths()`\n3. **Module discovery** → `TestCase` objects\n4. **Fixture resolution** → dependency graph resolution\n5. **Test execution** → `TestResult` creation\n6. **Result collection** → statistics and reporting\n\nMark where actual behavior diverges from expected behavior. This often reveals missing transformations or incorrect assumptions about data formats.\n\n#### 7. Binary Search Debugging\n\nFor particularly elusive bugs, use a **binary search approach** through the codebase:\n\n1. Add a check halfway through the execution pipeline (e.g., after discovery but before fixture setup)\n2. Verify state is correct at that point\n3. If bug occurs before that point, check halfway through the first half\n4. If bug occurs after that point, check halfway through the second half\n5. Repeat until you isolate the exact location\n\nThis is especially effective for **intermittent bugs** or those with complex triggering conditions.\n\n#### 8. Version Comparison\n\nIf a feature worked in an earlier milestone but now fails, use **git diff** to compare the working version with the current version. Focus on changes to the component exhibiting the bug.\n\n```bash\n# Compare current with last known good commit\ngit diff LAST_GOOD_COMMIT -- component/\n\n# Or use git bisect to automatically find the breaking commit\ngit bisect start\ngit bisect bad  # Current commit is broken\ngit bisect good LAST_GOOD_COMMIT\n# Git will guide you through testing intermediate commits\n```\n\n### Implementation Guidance\n\nWhile the main body focuses on conceptual debugging approaches, this section provides concrete code and tools to implement those strategies.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Debug Logging | Custom `debug_log()` function with environment variable toggle | Structured logging with `logging` module, configurable levels, and file/stream handlers |\n| Interactive Debugging | Python's built-in `pdb` with manual breakpoints | `ipdb` for IPython-enhanced debugging, or `debugpy` for VS Code remote debugging |\n| State Inspection | Manual print statements in strategic locations | `inspect` module for runtime introspection of call stacks and object graphs |\n| Test Isolation | `unittest.mock` for mocking dependencies | Property-based testing with `hypothesis` to generate edge cases |\n| Visualization | Manual drawing or ASCII diagrams | Graphviz for generating dependency graphs, or custom HTML reports |\n\n#### B. Recommended File/Module Structure\n\nAdd debugging utilities in a dedicated module:\n\n```\napollo/\n  __init__.py\n  cli.py\n  discover.py\n  runner.py\n  fixtures.py\n  assertions.py\n  reporter.py\n  debug/                    # ← New debugging module\n    __init__.py\n    logger.py              # Debug logging utilities\n    inspector.py           # State inspection functions  \n    hooks.py               # Debug hooks for tests\n    visualizer.py          # Data flow visualization (optional)\n  utils/\n    __init__.py\n```\n\n#### C. Infrastructure Starter Code\n\nHere's complete, ready-to-use debug logging infrastructure:\n\n```python\n# apollo/debug/logger.py\nimport os\nimport sys\nimport time\nimport threading\nfrom contextlib import contextmanager\nfrom typing import Any, Optional\n\n# Global debug state\nDEBUG_ENABLED = os.environ.get(\"APOLLO_DEBUG\", \"0\") == \"1\"\nDEBUG_LEVEL = int(os.environ.get(\"APOLLO_DEBUG_LEVEL\", \"1\"))\nDEBUG_COMPONENTS = os.environ.get(\"APOLLO_DEBUG_COMPONENTS\", \"\").split(\",\")\n\nclass DebugLogger:\n    \"\"\"Structured debug logger for the framework.\"\"\"\n    \n    def __init__(self, component: str):\n        self.component = component\n        self.thread_id = threading.get_ident()\n        \n    def log(self, message: str, data: Any = None, level: int = 1):\n        \"\"\"Log a debug message if debugging is enabled.\"\"\"\n        if not DEBUG_ENABLED:\n            return\n        if level > DEBUG_LEVEL:\n            return\n        if DEBUG_COMPONENTS and self.component not in DEBUG_COMPONENTS:\n            return\n            \n        timestamp = time.strftime(\"%H:%M:%S\")\n        thread_info = f\"T{self.thread_id}\" if threading.active_count() > 1 else \"\"\n        \n        print(f\"[{timestamp}] [{thread_info}] [{self.component}] {message}\", \n              file=sys.stderr)\n        if data is not None:\n            # Pretty-print data structures\n            import pprint\n            formatted = pprint.pformat(data, width=100, compact=True)\n            for line in formatted.splitlines():\n                print(f\"    {line}\", file=sys.stderr)\n    \n    @contextmanager\n    def scope(self, operation: str, **context):\n        \"\"\"Context manager for timing and logging operation scope.\"\"\"\n        self.log(f\"START {operation}\", context)\n        start_time = time.time()\n        try:\n            yield\n        except Exception as e:\n            self.log(f\"ERROR {operation}: {e}\", level=2)\n            raise\n        finally:\n            duration = time.time() - start_time\n            self.log(f\"END {operation} ({duration:.3f}s)\", level=2)\n\n# Convenience function\ndef get_logger(component: str) -> DebugLogger:\n    return DebugLogger(component)\n```\n\n#### D. Core Logic Skeleton Code\n\nHere are diagnostic functions to add to key components:\n\n```python\n# apollo/debug/inspector.py\nfrom typing import Dict, Any, List\nfrom ..fixtures import FixtureLifecycleManager, FixtureRegistry\nfrom ..runner import SimpleRunner\n\ndef inspect_fixture_state(lifecycle_manager: FixtureLifecycleManager) -> Dict[str, Any]:\n    \"\"\"\n    Return current state of fixture system for debugging.\n    \n    Returns:\n        Dictionary with keys:\n        - cached_fixtures: List of (cache_key, value_type) tuples\n        - active_generators: List of fixture names with pending teardown\n        - dependency_graph: Adjacency list of fixture dependencies\n    \"\"\"\n    # TODO 1: Extract cache keys and value types from lifecycle_manager._cache\n    # TODO 2: Extract active generators from lifecycle_manager._generators  \n    # TODO 3: Build dependency graph from fixture registry\n    # TODO 4: Format into a readable dictionary structure\n    # TODO 5: Include scope information for each cached fixture\n    pass\n\ndef diagnose_test_failure(test_result, runner: SimpleRunner) -> Dict[str, Any]:\n    \"\"\"\n    Analyze a test failure and provide diagnostic information.\n    \n    Returns:\n        Dictionary with potential causes and suggestions.\n    \"\"\"\n    # TODO 1: Check if failure is assertion vs exception\n    # TODO 2: If assertion, extract expected/actual values\n    # TODO 3: Check fixture dependencies for the test\n    # TODO 4: Look for common patterns (e.g., float comparison, missing imports)\n    # TODO 5: Return structured diagnosis with suggested fixes\n    pass\n\n# apollo/runner.py - Add diagnostic method to SimpleRunner\nclass SimpleRunner:\n    # ... existing code ...\n    \n    def get_execution_context(self, test_case) -> Dict[str, Any]:\n        \"\"\"\n        Return the execution context for a test (for debugging).\n        \n        This shows what fixtures will be injected, their values,\n        and other runtime context.\n        \"\"\"\n        # TODO 1: Get test function signature parameters\n        # TODO 2: For each parameter, check if matching fixture exists\n        # TODO 3: For existing fixtures, get their current cached value or None\n        # TODO 4: Return dictionary mapping param -> (fixture_name, has_value, value_type)\n        pass\n```\n\n#### E. Language-Specific Hints\n\n- **Use `inspect.signature()`** for safe parameter inspection instead of `inspect.getargspec()` (deprecated).\n- **For parallel debugging**, set `multiprocessing` start method to `'spawn'` for cleaner process isolation: `multiprocessing.set_start_method('spawn', force=True)`.\n- **To catch pickling errors** in parallel execution, wrap test execution in try-except and log the exact object that fails serialization.\n- **Use `sys.settrace()`** or `threading.settrace()` for low-level execution tracing in single-threaded debugging scenarios.\n- **For memory leaks**, use `tracemalloc` to track fixture objects that aren't being garbage collected after teardown.\n\n#### F. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **`pickle.PickleError` in parallel mode** | Test case or fixture contains non-pickleable objects (e.g., lambda functions, database connections). | 1. Check which object is being pickled when error occurs.<br>2. Add `__reduce__` method to custom objects.<br>3. Use `pickle.dumps()` to test serialization. | 1. Make test dependencies pickleable.<br>2. Recreate non-pickleable objects in worker processes.<br>3. Use `multiprocessing.Queue` for sending results instead of return values. |\n| **Tests pass locally but fail in CI** | Environment differences: Python version, working directory, missing dependencies, or timing issues. | 1. Compare Python versions (`sys.version`).<br>2. Check current working directory.<br>3. Look for absolute paths in tests.<br>4. Add longer timeouts for slower CI environments. | 1. Use relative paths and `pathlib.Path`.<br>2. Isolate environment-specific code behind abstractions.<br>3. Add CI-specific configuration or skips. |\n| **Fixture teardown called multiple times** | `teardown_scope()` invoked redundantly due to incorrect scope boundary detection. | 1. Add log to `teardown_scope()` showing caller.<br>2. Check if multiple tests trigger same scope teardown.<br>3. Verify scope counting logic. | 1. Implement reference counting for scope teardown.<br>2. Use `weakref.finalize` for automatic cleanup.<br>3. Track active tests per scope and only teardown when last test completes. |\n\n#### G. Milestone Checkpoint for Debugging\n\nAfter implementing the debugging utilities, verify they work:\n\n```bash\n# Test debug logging\nAPOLLO_DEBUG=1 python -m apollo run tests/example.py\n\n# Should show debug output like:\n# [14:30:25] [Discover] Scanning module tests.example\n# [14:30:25] [Discover] Found test test_addition\n\n# Test fixture inspection\npython -c \"\nimport apollo\nfrom apollo.debug import inspect_fixture_state\n\n@apollo.fixture\ndef my_fixture():\n    return {'data': 42}\n\n# Get lifecycle manager (might need to access internal)\n# Call inspect_fixture_state() and print result\n\"\n\n# Test minimal reproduction creation\n# Create a simple test that demonstrates a bug, then run:\nAPOLLO_DEBUG=1 APOLLO_DEBUG_COMPONENTS=fixtures python reproduce_bug.py\n```\n\nThe debugging system should help you quickly identify issues during development and provide clear insights into the framework's internal state. As you encounter new bugs, add them to the Common Bugs table with their solutions to build institutional knowledge.\n\n\n## 13. Future Extensions\n\n> **Milestone(s):** This section explores potential enhancements that build upon all four completed milestones, extending the framework's capabilities beyond the core educational goals.\n\nThe architecture of Project Apollo has been designed with **extensibility** as a core principle. The component-based pipeline, clear separation of concerns, and well-defined data structures create a solid foundation upon which advanced features can be built. This section explores several **future extensions** that could transform the framework from a learning tool into a more powerful, production-ready system. Each idea is presented with a **mental model**, a description of how it would integrate with the existing architecture, and an analysis of what changes would be required.\n\n### Ideas for Extension\n\nThe following extensions represent natural evolutions of the test framework, addressing common needs in real-world testing workflows. They are listed in approximate order of increasing complexity and architectural impact.\n\n---\n\n#### 1. Plugin System: The Modular Toolbox\n\n**Mental Model: The App Store for Test Features**\nThink of the plugin system as an **app store** or **modular toolbox** that allows third-party developers to add new capabilities without modifying the framework's core. Just as power tools can be attached to a common drill base, plugins can hook into the framework's lifecycle to add new reporters, discovery mechanisms, assertion matchers, or fixture types.\n\n**Description:**\nA plugin system would allow developers to extend the framework's behavior at defined **extension points**. Plugins would be discoverable (e.g., via entry points in `setup.py` or `pyproject.toml`) and would integrate with the main pipeline. Key extension points would include:\n- **Discovery Hooks**: Modify how tests are discovered (e.g., scanning non-Python files).\n- **Test Execution Wrappers**: Add pre/post-processing for each test run (e.g., resource monitoring).\n- **Reporting Sinks**: Add new output formats beyond console and JUnit XML.\n- **CLI Command Addition**: Add entirely new subcommands to the CLI (e.g., `apollo bench` for benchmarking).\n- **Fixture Injection Overrides**: Provide alternative resolution strategies for fixture dependencies.\n\n**Architectural Accommodation:**\nThe current architecture is a **linear pipeline**, which is simple but not inherently pluggable. To accommodate plugins, the framework would need to evolve into a **publish-subscribe** or **middleware** model. The `Runner`, `Discoverer`, and `Reporter` would need to expose **hook registries** where plugins can register callbacks.\n\n> **Decision: Plugin Architecture Pattern**\n> - **Context**: The framework needs to be extended by users without forking the core codebase.\n> - **Options Considered**:\n>   1. **Subclassing**: Require users to subclass core components and override methods.\n>   2. **Middleware Pipeline**: Define a chain of processors for each major operation (discovery, execution, reporting).\n>   3. **Event Bus**: Emit events at key lifecycle points (e.g., `test_discovered`, `test_started`, `test_finished`) and allow plugins to subscribe.\n> - **Decision**: Adopt a hybrid **event bus + middleware** model. Simple hooks use events; complex transformations use middleware interfaces.\n> - **Rationale**: Events are great for side effects (logging, monitoring). Middleware is better for transformational operations (modifying the test suite, intercepting fixture values). The hybrid model provides flexibility without overcomplicating simple use cases.\n> - **Consequences**: Increases framework complexity significantly. Requires careful design of event/middleware interfaces to remain stable across versions. Enables a rich ecosystem but adds maintenance burden.\n\n**Plugin System Options Comparison:**\n| Option | Pros | Cons | Viability with Current Architecture |\n|--------|------|------|-------------------------------------|\n| Subclassing | Simple to implement, familiar OOP pattern | Tight coupling, requires deep knowledge of internals, hard to compose multiple extensions | High – minimal changes, but poor user experience |\n| Middleware Pipeline | Powerful, composable, clear data flow | More complex to implement, can become a \"tower of middleware\" | Medium – requires refactoring core components into processor chains |\n| Event Bus | Loose coupling, easy to add side-effect plugins | Harder to modify core data (events are typically immutable), can lead to event spaghetti | Medium – requires adding an event emitter to key lifecycle methods |\n| Hybrid Model | Best of both worlds, flexible for different extension types | Most complex to design and implement | Low – significant architectural redesign required |\n\n**Required Changes:**\n1. Introduce a `PluginRegistry` component that loads plugins at startup.\n2. Define a set of standard event types (e.g., `TestDiscoveryEvent`, `TestExecutionEvent`) and emit them at appropriate points.\n3. Refactor the `Runner` and `Discoverer` to use injectable middleware components.\n4. Update the `CLI Parser` to accept `--plugin` arguments and load plugins from specified paths.\n\n---\n\n#### 2. Custom Markers and Filtering: The Labeling System\n\n**Mental Model: Color-Coded Sticky Notes**\nImagine being able to place **color-coded sticky notes** on tests: red for \"slow\", green for \"integration\", yellow for \"flaky\". The test conductor can then filter which tests to run based on these labels, just as you might pull only the \"urgent\" notes from a board.\n\n**Description:**\nMarkers are metadata annotations attached to test functions or classes that allow for **categorization** and **selective execution**. Users could mark tests as `@slow`, `@integration`, `@database`, etc. The CLI would gain options to run only tests with certain markers (`-m integration`), exclude tests with markers (`-m \"not slow\"`), or combine markers with boolean logic.\n\n**Architectural Accommodation:**\nThe current `TestCase` data structure already has a `fixtures` list; adding a `markers: set[str]` field would be trivial. The main challenge is in **discovery** (reading decorators) and **filtering**. The `Discoverer` would need to inspect decorators and populate the `markers` field. The `CLI Parser` would need to accept marker expressions, and the `Runner` (or a new `Filter` component) would need to filter the `TestSuite` based on those expressions before execution.\n\n**Marker Implementation Options:**\n| Option | Pros | Cons | Integration Path |\n|--------|------|------|------------------|\n| Decorator-based (`@mark('slow')`) | Pythonic, explicit, easy to parse | Requires modifying test code, cannot mark third-party tests | Extend `_find_tests_in_module` to check for `__markers__` attribute |\n| Docstring parsing (`# marker: slow`) | No runtime overhead, works with any function | Fragile, depends on comment format, harder to parse | Add docstring parsing in discovery phase |\n| External configuration file | Centralized management, no code modification | Disconnected from test code, maintenance burden | Create a YAML/TOML mapping of test names to markers, merge during discovery |\n\n**Required Changes:**\n1. Add `markers: Set[str]` field to `TestCase`.\n2. Extend `_find_tests_in_module` to detect marker decorators and populate the field.\n3. Add `-m/--marker` and `-k/--keyword` (for name substring filtering) options to `CLI Parser`.\n4. Create a `TestFilter` component that takes a `TestSuite` and a filter expression, returning a filtered `TestSuite`.\n5. Integrate the `TestFilter` between `Discoverer` and `Runner`.\n\n---\n\n#### 3. Parameterized Tests: The Test Factory\n\n**Mental Model: The Cookie Cutter**\nA parameterized test is like a **cookie cutter** – a single test definition (the cutter) that can produce multiple test instances (cookies) with different inputs (dough shapes). This avoids copy-pasting test code for each input variant.\n\n**Description:**\nParameterization allows a single test function to be run multiple times with different input arguments. For example, `@parametrize('input,expected', [(1, 2), (3, 4)])` would generate two test cases. Each generated test should appear as a separate entry in discovery and reporting (with distinct `nodeid` values). Parameters can be sourced from lists, functions, or external files.\n\n**Architectural Accommodation:**\nThis feature heavily impacts the **discovery** and **test case identity** layers. The `Discoverer` would need to recognize parameterization decorators and generate multiple `TestCase` objects from a single function. Each generated `TestCase` must have a unique `nodeid` (e.g., `test_foo[1-2]`) and must encapsulate the parameter values. The `Runner` must then call the original function with the bound parameters.\n\n**Parameterization Strategy ADR:**\n> **Decision: Generation Time vs. Runtime Parameterization**\n> - **Context**: We need to decide when parameter values are bound to test functions.\n> - **Options Considered**:\n>   1. **Generation Time**: During discovery, create a separate `TestCase` for each parameter set. Each `TestCase` has a closure that binds the parameters.\n>   2. **Runtime**: During execution, the runner reads the parameter set and calls the test function with those values dynamically.\n> - **Decision**: **Generation Time**.\n> - **Rationale**: Generation time creates clear separation of test instances, making reporting, filtering, and parallel execution straightforward. Each parameterized variant is a first-class test with its own result. Runtime parameterization would require the runner to handle loops, complicating result collection and making it harder to re-run a specific failing variant.\n> - **Consequences**: Increases memory usage (many `TestCase` objects). Requires careful design of `nodeid` to uniquely identify each variant. Enables fine-grained control over test execution.\n\n**Required Changes:**\n1. Define a `@parametrize` decorator that stores parameter sets in a `__param_sets__` attribute on the function.\n2. Extend `_find_tests_in_module` to detect this attribute and generate multiple `TestCase` instances.\n3. Augment `TestCase` with a `parameters: Dict[str, Any]` field to store the bound values.\n4. Modify `SimpleRunner.run_test` to inject parameters into the test function call (either as keyword arguments or via `functools.partial`).\n5. Update the `Reporter` to display parameter values in failure messages.\n\n---\n\n#### 4. Test Doubles and Mocking Integration: The Understudy System\n\n**Mental Model: Hollywood Stunt Doubles**\nMocks and stubs are like **stunt doubles** for your code's dependencies. They stand in for real objects during testing, allowing you to simulate specific behaviors (like throwing an error) without triggering real side effects (like charging a credit card).\n\n**Description:**\nIntegrate a mocking library (e.g., `unittest.mock` pattern) directly into the framework. Provide convenient APIs to patch objects, create mock instances, and make assertions about how they were called. This could include automatic cleanup of patches after each test (integrating with the fixture system) and specialized assertion matchers for mock calls.\n\n**Architectural Accommodation:**\nThe current architecture has no built-in mocking support. Integration would primarily involve providing a **convenient API** and ensuring **proper cleanup**. The fixture system's teardown mechanisms could be leveraged to automatically undo patches. A new `MockFixture` could be added to the standard library. The `Assertion Engine` could be extended with matchers like `assert_called_once_with`.\n\n**Mocking Integration Approaches:**\n| Approach | Pros | Cons | Implementation Path |\n|----------|------|------|---------------------|\n| **Bundled Wrapper**: Provide thin wrappers around `unittest.mock` | Leverages standard library, robust, well-tested | Adds dependency on stdlib mocking concepts, less innovation | Add `apollo.mock` module that re-exports and extends `unittest.mock` |\n| **Native Implementation**: Build a custom mocking system | Full control, can optimize for framework idioms | Huge effort, likely buggy, reinvents the wheel | Create `Mock`, `MagicMock`, `patch` classes from scratch (not recommended) |\n| **Plugin-Based**: Allow mocking libraries to integrate via plugin system | Keeps core lean, encourages ecosystem diversity | Users must install separate packages, inconsistent APIs | Define mocking extension points in the plugin system |\n\n**Required Changes:**\n1. Add an `apollo.mock` module that imports and extends `unittest.mock`.\n2. Create a `mock_fixture` helper that returns a Mock and ensures reset after test.\n3. Integrate automatic patching cleanup into the test isolation mechanism (ensure `patch.stopall()` is called after each test).\n4. Add `assert_called_with`-style matchers to the `Assertion Engine`.\n\n---\n\n#### 5. Parallel Test Execution with Fixture Scope Awareness\n\n**Mental Model: The Assembly Line with Shared Workstations**\nRunning tests in parallel is like an **assembly line** with multiple workers. Some workstations (fixtures) are personal (function-scoped), some are shared by a team (class-scoped), and some are factory-wide (session-scoped). The line manager must schedule tasks so workers don't conflict over shared resources.\n\n**Description:**\nMilestone 1 includes basic parallel execution, but it assumes all tests are independent. In reality, tests that share **fixtures with scope above FUNCTION** (CLASS, MODULE, SESSION) cannot run truly concurrently if they use the same fixture instance. The framework needs to **detect fixture dependencies** and **schedule tests** to avoid conflicts—either by running them sequentially or by creating duplicate fixture instances.\n\n**Architectural Accommodation:**\nThe current `Fixture` model already includes `scope` and `dependencies`. The `Runner` would need to analyze the fixture graph for each test and determine which tests can run in parallel without conflicting. This requires a **scheduling algorithm** that groups tests by their required fixture instances.\n\n**Scheduling Algorithm Options:**\n| Algorithm | Pros | Cons | Complexity |\n|-----------|------|------|------------|\n| **Dependency-Based Grouping**: Group tests that share non-FUNCTION scoped fixtures | Maximizes parallelism within groups, avoids conflicts | Requires graph analysis, may create many small groups | Medium – must compute fixture instance keys for each test |\n| **Sequential by Scope**: Run all tests of a CLASS scope together, then next CLASS, etc. | Simple to implement, guarantees no conflicts | May serialize too much, reducing parallelism | Low – just sort tests by their highest non-FUNCTION scope |\n| **Fixture Instance Locking**: Lock fixture instances during use, queue tests waiting for same instance | Fine-grained, can achieve high parallelism | Complex deadlock avoidance, requires runtime locking | High – needs a lock manager and wait queues |\n\n**Required Changes:**\n1. Enhance `TestCase` to include a `required_fixture_instances: Set[Tuple[str, str]]` field, where the tuple is `(fixture_name, scope_id)`.\n2. Modify discovery to compute these instance keys based on fixture scope and test location.\n3. Implement a `Scheduler` component that takes a `TestSuite` and partitions it into runnable batches where tests in the same batch do not share non-FUNCTION fixture instances.\n4. Update the `ParallelRunner` to use the `Scheduler`'s batches instead of naive chunking.\n\n---\n\n#### 6. Snapshot Testing: The Golden Master\n\n**Mental Model: The Photographic Evidence**\nSnapshot testing is like taking a **photograph** of your code's output the first time it runs. On subsequent runs, you compare new output to the stored photograph. If they differ, you either found a bug (and update the snapshot) or detected an intentional change (and accept the new snapshot).\n\n**Description:**\nSnapshot testing automatically captures the serialized output of a function (e.g., JSON, HTML, text) and stores it in a file. Future test runs compare the new output to the stored snapshot. The framework would provide an `assert_match_snapshot(actual)` assertion that loads the appropriate snapshot file, compares, and shows a diff on mismatch. It would also include a CLI command to update snapshots (`--update-snapshots`).\n\n**Architectural Accommodation:**\nThis extension fits neatly into the **Assertion Engine** and **CLI**. A new `SnapshotMatcher` could be added to the matchers API. The main complexity is **snapshot storage** (file management, version control friendliness) and **diff presentation**. The existing `DiffResult` infrastructure could be reused for comparison.\n\n**Snapshot Storage Strategy:**\n| Strategy | Pros | Cons | Recommendation |\n|----------|------|------|----------------|\n| **Single file per test**: Each test has a `.snap` file with its snapshots | Easy to manage, clear mapping | Many files, may clutter project | Preferred for clarity and git diff readability |\n| **Centralized snapshot registry**: One file mapping test IDs to snapshots | Centralized, easy to prune | Large file, merge conflicts likely, harder to read diffs | Not recommended |\n| **Inline snapshots**: Store snapshot as a string literal in the test file itself | No external files, self-contained | Bloats test files, harder to update programmatically | Possible but requires AST manipulation |\n\n**Required Changes:**\n1. Create a `SnapshotStore` component that manages reading/writing snapshot files, keyed by test `nodeid`.\n2. Add a `snapshot` matcher that implements `__matches__` by comparing actual to stored snapshot.\n3. Extend the CLI with `--update-snapshots` flag that writes new snapshots instead of comparing.\n4. Integrate snapshot cleanup into the `Reporter` (e.g., list unused snapshots).\n\n---\n\n#### 7. Property-Based Testing: The Fuzzing Lab\n\n**Mental Model: The Stress-Testing Machine**\nProperty-based testing is like a **stress-testing machine** that feeds your code random inputs, checking that certain properties always hold. Instead of testing specific examples, you define rules like \"for any list, sorting it twice should equal sorting it once\" and let the machine try to break the rule.\n\n**Description:**\nIntegrate property-based testing à la Hypothesis or QuickCheck. Users would write test functions that accept parameters, and the framework would automatically generate many random inputs (including edge cases) to validate the property. The framework would need to provide **generators** for common types, **shrinking** of failing cases to minimal examples, and **replay** of failing seeds.\n\n**Architectural Accommodation:**\nThis is a major extension that would essentially add a new **test generation engine**. Property-based tests would be discovered as a special test type. The `Runner` would need to invoke the property-based engine, which would run multiple iterations, handle failures, and report the minimal counterexample. The existing `Assertion Engine` could still be used for property checks.\n\n**Property-Based Integration Depth:**\n| Depth | Pros | Cons | Feasibility |\n|-------|------|------|-------------|\n| **Thin Wrapper**: Integrate Hypothesis as an external dependency | Powerful, mature library, less work | Adds heavy dependency, less control | High – provide `@apollo.given` decorator that delegates to Hypothesis |\n| **Native Lightweight Engine**: Build a simple generator/shrinker for core types | No external dependencies, full control | Limited generator variety, poor shrinking, bug-prone | Medium – significant development effort, likely inferior to Hypothesis |\n| **Plugin**: Make property-based testing a plugin | Keeps core lean, allows multiple backends | Fragmentation, users must install plugin | Medium – depends on plugin system being implemented first |\n\n**Required Changes:**\n1. Add a `@given` decorator that marks a test as property-based and attaches generator strategies.\n2. Create a `PropertyBasedRunner` that extends `SimpleRunner` to handle generation, iteration, and shrinking.\n3. Extend `TestResult` to include counterexample data and seed for reproduction.\n4. Update the `Reporter` to display shrunk counterexamples and seeds.\n\n---\n\n#### 8. Code Coverage Integration: The Coverage Map\n\n**Mental Model: The Heat Map of Execution**\nCode coverage tools generate a **heat map** showing which parts of your codebase were \"touched\" during test execution. Integrating coverage gives immediate feedback on test thoroughness and helps identify untested corners.\n\n**Description:**\nIntegrate with coverage measurement tools (e.g., `coverage.py`) to automatically collect and report coverage data during test runs. The CLI could add `--coverage` to enable collection, and the reporter could print a summary or generate HTML reports. Coverage could be measured per test, per module, or for the entire suite.\n\n**Architectural Accommodation:**\nCoverage measurement is a **cross-cutting concern** that touches test execution. The `Runner` would need to start coverage before running tests and stop it after. The `Reporter` would need to format coverage data. The main challenge is **correctly measuring coverage in parallel execution** (requires coverage workers or merging coverage data from multiple processes).\n\n**Coverage Collection Strategy:**\n| Strategy | Pros | Cons | Implementation Notes |\n|----------|------|------|----------------------|\n| **Subprocess Wrapping**: Run the entire test process under coverage | Simple, works with any runner | Cannot measure coverage per test, hard to integrate with parallel workers | Use `coverage run -m apollo.cli` – trivial but limited |\n| **In-Process API**: Use coverage's Python API to start/stop around each test | Fine-grained, can attribute coverage to specific tests | Overhead per test, complex with parallelism | Integrate with `SimpleRunner.run_test`, use `coverage.Coverage` instance |\n| **Separate Coverage Workers**: Each parallel worker collects its own coverage, merge at end | Works with parallelism, detailed data | Complex merging, data duplication | Use `FixtureLifecycleManager`-like merging of coverage data structures |\n\n**Required Changes:**\n1. Add `coverage_enabled` and `coverage_config` fields to `Configuration`.\n2. Modify `SimpleRunner` to start/stop coverage measurement per test or per suite.\n3. Create a `CoverageMerger` component for parallel runs.\n4. Extend `Reporter` to output coverage summary and optionally generate HTML reports.\n\n---\n\n#### 9. Test Retries and Flaky Test Detection: The Reliability Engineer\n\n**Mental Model: The Intermittent Fault Detector**\nFlaky tests are like **intermittent electrical faults** – they sometimes fail, sometimes pass under identical conditions. A retry system acts as a **reliability engineer** that re-runs failing tests to see if the failure persists, automatically marking transient failures as \"flaky\" and persistent failures as true failures.\n\n**Description:**\nAdd options to automatically retry failing tests a configurable number of times (`--retries=3`). If a test passes on a retry, it could be marked as **flaky** in the report. The framework could also track flakiness statistics over time and provide warnings. Advanced features could include **detecting flaky patterns** (e.g., failures only on certain times of day) and **quarantining** flaky tests.\n\n**Architectural Accommodation:**\nRetries are a **runner-level concern**. The `SimpleRunner.run_test` would need to be wrapped in a retry loop. Results aggregation would need to handle the \"flaky\" status (perhaps a new `TestStatus.FLAKY`). The `Reporter` would need to report flaky tests separately.\n\n**Retry Implementation Considerations:**\n| Consideration | Options | Recommendation |\n|---------------|---------|----------------|\n| **When to retry** | Only on test failure (not error), or on both failure and error? | Retry only on assertion failures (`FAILED`), not on Python errors (`ERRORED`), as errors often indicate setup issues that won't fix themselves. |\n| **Isolation between retries** | Complete reset (re-import modules, re-create fixtures) vs. lightweight (keep fixture instances) | Complete reset to eliminate state leakage as cause of flakiness. Use `importlib.reload` and fixture re-creation. |\n| **Reporting** | Show all attempts, or only final result? | Show final result but log retry attempts in verbose mode. Add `--show-retries` flag. |\n\n**Required Changes:**\n1. Add `retries` and `retry_delay` fields to `Configuration`.\n2. Create a `RetryingRunner` wrapper that delegates to `SimpleRunner` and implements the retry loop.\n3. Add `TestStatus.FLAKY` and adjust reporting logic.\n4. Extend `TestResult` to include `attempts: List[TestResult]` for history.\n\n---\n\n#### 10. Interactive Debugger and PDB Integration: The Surgical Suite\n\n**Mental Model: The Surgical Suite with Live Monitoring**\nWhen a test fails, you often want to **operate** on the failing code immediately. Interactive debugging integration is like a **surgical suite** where you can pause execution at the point of failure, inspect variables, and step through code, all without leaving the test framework environment.\n\n**Description:**\nAdd a `--pdb` flag that drops into the Python debugger (`pdb`) on test failure or error. More advanced features could include **post-mortem debugging** (automatically enter debugger after exception), **breakpoint decorators** (`@breakpoint`), and integration with IPython's richer debugger. The framework could also provide **debugging fixtures** that capture system state (logs, memory) on failure.\n\n**Architectural Accommodation:**\nDebugging hooks would be integrated into the **error handling** layer. When a test raises an exception, the framework would intercept it and launch the debugger before proceeding to the next test. This requires careful handling of **test isolation** – the debugger should not leave global state modified for subsequent tests.\n\n**Debugger Integration Options:**\n| Option | Pros | Cons | Implementation |\n|--------|------|------|----------------|\n| **Simple PDB**: Call `pdb.post_mortem()` on any exception | Standard library, no dependencies | Basic interface, no IDE integration | Wrap `run_test` in try/except and call post_mortem |\n| **IPython Debugger**: Use `ipdb` if available, fallback to `pdb` | Richer features, better UX | Requires IPython optional dependency | Check for `ipdb` import, use `ipdb.post_mortem` |\n| **Debugger Hooks**: Provide hooks for IDE debuggers (e.g., VS Code) | Seamless IDE integration | Complex, requires emitting debugger protocol | Set `PYTHONBREAKPOINT` environment variable or use `sys.breakpointhook` |\n\n**Required Changes:**\n1. Add `pdb` flag to `Configuration`.\n2. Modify `SimpleRunner.run_test` to catch exceptions and invoke `pdb.post_mortem()` when flag is set.\n3. Ensure debugger sessions don't break test isolation (warn user about state contamination).\n4. Optionally add `--pdb-failures` (debug only assertion failures) and `--pdb-errors` (debug all errors) flags.\n\n---\n\n### Summary of Architectural Impact\n\nThe table below summarizes the estimated impact of each extension on the existing components:\n\n| Extension | Components Most Affected | Architectural Changes Required | Effort Level |\n|-----------|--------------------------|--------------------------------|--------------|\n| Plugin System | All components, new PluginRegistry | Major redesign to event/middleware model | High |\n| Custom Markers | Discoverer, CLI Parser, new Filter | Moderate – new fields and filtering logic | Medium |\n| Parameterized Tests | Discoverer, TestCase, Runner | Moderate – generation logic and nodeid changes | Medium |\n| Mocking Integration | Assertion Engine, Fixture System | Low – API layer addition | Low |\n| Parallel + Fixture Awareness | Runner, Discoverer, new Scheduler | High – scheduling algorithm and dependency analysis | High |\n| Snapshot Testing | Assertion Engine, CLI, new SnapshotStore | Medium – file I/O and diff integration | Medium |\n| Property-Based Testing | Runner, Discoverer, new Generator engine | High – entirely new execution model | High |\n| Code Coverage | Runner, Reporter, new CoverageMerger | Medium – coverage API integration and merging | Medium |\n| Test Retries | Runner, TestResult, Reporter | Low – retry loop and status additions | Low |\n| Interactive Debugger | Runner, Error Handling | Low – exception interception | Low |\n\n> **Key Insight:** The architecture's clear separation of concerns allows many extensions to be developed **in isolation**. For example, snapshot testing primarily extends the Assertion Engine, while markers extend Discovery and Filtering. This modularity means you can pick and choose which extensions to implement based on need, without rewriting the entire framework.\n\n---\n\n### Implementation Guidance\n\nWhile the full implementation of these extensions is beyond the scope of the core milestones, here is guidance for starting on the most feasible ones.\n\n**Technology Recommendations:**\n| Extension | Recommended Libraries/Tools | Notes |\n|-----------|----------------------------|-------|\n| Plugin System | `importlib.metadata` (Python 3.8+) for entry point discovery | Use `entry_points` in `pyproject.toml` for plugin registration |\n| Mocking Integration | `unittest.mock` (standard library) | Wrap, don't rewrite. Provide `apollo.mock` module. |\n| Snapshot Testing | `difflib` for diffing, `json`/`yaml` for serialization | Consider `syrupy` or `snapshottest` for inspiration |\n| Property-Based Testing | `hypothesis` as optional dependency | Integrate via plugin or optional extra: `apollo[property]` |\n| Code Coverage | `coverage.py` as optional dependency | Use its public API: `coverage.Coverage()` |\n| Interactive Debugger | `pdb` (standard library) | For advanced users, recommend `ipdb` |\n\n**Recommended File/Module Structure for Extensions:**\n```\napollo/\n├── core/                 # Existing core components\n├── extensions/           # New directory for official extensions\n│   ├── __init__.py\n│   ├── markers.py       # Custom markers implementation\n│   ├── parameterize.py  # Parameterized tests\n│   ├── snapshots.py     # Snapshot testing\n│   ├── retries.py       # Test retries\n│   └── debugging.py     # PDB integration\n├── plugins/              # Plugin infrastructure (if implemented)\n│   ├── __init__.py\n│   ├── registry.py\n│   └── events.py\n└── cli.py               # Extend CLI with new flags\n```\n\n**Core Logic Skeleton for Custom Markers:**\n```python\n# In extensions/markers.py\nimport functools\nfrom typing import Set\n\ndef mark(*marker_names: str):\n    \"\"\"Decorator to add markers to a test function or class.\"\"\"\n    def decorator(obj):\n        if not hasattr(obj, '__markers__'):\n            obj.__markers__ = set()\n        obj.__markers__.update(marker_names)\n        return obj\n    return decorator\n\ndef get_markers(obj) -> Set[str]:\n    \"\"\"Retrieve markers from a test function or class.\"\"\"\n    return getattr(obj, '__markers__', set())\n\n# In core/discoverer.py, augment _find_tests_in_module:\n# TODO 1: After identifying a test callable, check for __markers__ attribute\n# TODO 2: Create TestCase with markers=set(obj.__markers__) if exists\n# TODO 3: For test classes, inherit class markers to methods (union of class and method markers)\n```\n\n**Core Logic Skeleton for Test Retries:**\n```python\n# In extensions/retries.py\nfrom typing import List\nfrom core.runner import SimpleRunner\nfrom core.data_model import TestCase, TestResult, TestStatus\n\nclass RetryingRunner:\n    def __init__(self, base_runner: SimpleRunner, max_retries: int = 3, delay: float = 0.0):\n        self.base_runner = base_runner\n        self.max_retries = max_retries\n        self.delay = delay\n    \n    def run_test(self, test_case: TestCase) -> TestResult:\n        attempts: List[TestResult] = []\n        for attempt in range(self.max_retries + 1):  # +1 for initial attempt\n            result = self.base_runner.run_test(test_case)\n            attempts.append(result)\n            # TODO 1: If test passed, break loop\n            # TODO 2: If delay > 0 and not last attempt, sleep(delay)\n            # TODO 3: If test failed (not errored) and attempts remain, continue\n            # TODO 4: If test errored or no attempts remain, break\n        # TODO 5: Determine final status: \n        #   - If any attempt passed -> FLAKY (if different from first) or PASSED\n        #   - Else -> use last result's status\n        # TODO 6: Create final TestResult with aggregated attempts\n        pass\n```\n\n**Language-Specific Hints for Python:**\n- Use `functools.wraps` when creating decorators to preserve function metadata.\n- For plugin discovery, leverage `importlib.metadata.entry_points()` (Python ≥3.8) or the backport `importlib_metadata`.\n- When implementing parallel fixture-aware scheduling, consider using `networkx` for graph analysis of fixture dependencies.\n- For snapshot testing, use `json.dumps(obj, indent=2, sort_keys=True)` for deterministic serialization.\n- Use `contextlib.ExitStack` to manage multiple patch objects in mocking integration.\n\n**Debugging Tips for Extension Development:**\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Markers not discovered on test functions | Decorator not setting `__markers__` attribute | Print `dir(test_func)` after decoration | Ensure `mark` decorator correctly sets attribute |\n| Parameterized tests all have same nodeid | Not generating unique IDs per parameter set | Check `TestCase.nodeid` for each generated test | Include parameter values in nodeid (e.g., `test_foo[input=1]`) |\n| Snapshot comparison always fails due to whitespace | Serialization not normalized | Compare raw strings with `repr()` | Use `json.dumps` with consistent formatting or strip whitespace |\n| Retries cause fixture teardown between attempts | Runner creating new fixture context each attempt | Check `FixtureLifecycleManager.get_fixture_value` logs | Cache fixture instances across retries for same test |\n| Coverage data missing in parallel runs | Each worker starts its own coverage, not merged | Check coverage files in temp directories | Implement `CoverageMerger` that combines `.coverage` files |\n\n**Next Steps for Learners:**\nChoose one extension that aligns with your interests and implement it as a **capstone project**. Start by forking the completed Milestone 4 codebase. Implement the extension in the `extensions/` directory, and ensure it integrates smoothly with the existing components. Write tests for your extension using the framework itself (self-validation). This deepens your understanding of both test framework design and practical Python engineering.\n\n\n## 14. Glossary\n> **Milestone(s):** All four milestones, as this section defines terminology used throughout the entire design document.\n\nClear terminology establishes a **shared vocabulary** that prevents confusion and ensures precise communication about the framework's architecture. This glossary defines all key terms, concepts, and component names used throughout this design document, providing a single reference point for understanding the system's language.\n\n### Term Definitions\n\n| Term | Definition | First Introduced In |\n|------|------------|---------------------|\n| **Actionable Error Messages** | Error messages that not only describe what went wrong but also suggest potential fixes or next steps for the developer. | Section 10: Error Handling and Edge Cases |\n| **Assertion Engine** | The core component responsible for evaluating assertion conditions, comparing values, and generating helpful failure messages with diffs. | Section 3: High-Level Architecture |\n| **AssertionFailure** | A structured data type representing a failed assertion, containing the expected and actual values, a formatted message, and optional diff information. | Section 4: Data Model |\n| **Binary Search Debugging** | A debugging technique where you repeatedly test halfway points in the execution flow to isolate the source of a bug. | Section 12: Debugging Guide |\n| **CDATA Section** | An XML construct (Character Data) that marks text content as not containing markup, allowing inclusion of special characters without escaping. | Section 8: Component Design: Reporting & CLI |\n| **Circular Dependency** | A problematic situation in the fixture system where fixture A depends on fixture B, and fixture B (directly or indirectly) depends on fixture A. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **Code Coverage Integration** | A potential future extension that tracks which lines of source code are executed during test runs to measure test thoroughness. | Section 13: Future Extensions |\n| **ComparisonContext** | A configuration object that controls how equality comparisons are performed, with settings like tolerance, case sensitivity, and ordering. | Section 4: Data Model |\n| **Comparison Inspector** | The mental model for the assertion engine as an examiner who compares artifacts and produces detailed comparison reports. | Section 6: Component Design: Assertions & Matchers |\n| **Configuration** | The primary data structure representing all CLI arguments and runtime settings, serving as the **control panel** for test execution. | Section 4: Data Model |\n| **ConsoleFormatter** | The component responsible for formatting test results into human-readable output for terminal display, including colors and progress indicators. | Section 8: Component Design: Reporting & CLI |\n| **Control Panel** | The mental model for the CLI as an interface providing knobs, switches, and dials to configure test execution behavior. | Section 8: Component Design: Reporting & CLI |\n| **Convention-over-Configuration** | A design paradigm where sensible defaults are used instead of requiring explicit configuration, reducing boilerplate code. | Section 1: Context and Problem Statement |\n| **Cross-Validation** | A testing strategy that verifies system behavior using multiple independent methods to increase confidence in correctness. | Section 11: Testing Strategy |\n| **Custom Markers** | A potential future extension allowing metadata annotations on tests for categorization, filtering, and special handling. | Section 13: Future Extensions |\n| **DebugLogger** | A utility component for structured debug logging that helps trace execution flow and diagnose issues within framework components. | Section 4: Data Model |\n| **DiffResult** | A structured representation of differences between expected and actual values, containing both summary information and formatted diffs. | Section 4: Data Model |\n| **DNA of the Test Framework** | The mental model portraying data structures like `TestCase`, `TestResult`, and `Fixture` as fundamental instructions encoding framework behavior. | Section 4: Data Model |\n| **Error Isolation** | The design principle that errors in one test should not affect the execution or reporting of other tests in the same suite. | Section 10: Error Handling and Edge Cases |\n| **Evidence Examiner** | The mental model for detailed comparison operations that produce side-by-side highlighting of differences between values. | Section 6: Component Design: Assertions & Matchers |\n| **Exit Code** | The numeric value returned by a process to indicate success (0) or failure (non-zero), used by CI systems to detect test failures. | Section 8: Component Design: Reporting & CLI |\n| **Fail-Fast vs. Fail-Safe** | A design tradeoff between stopping test execution immediately on critical errors vs. continuing to collect as many results as possible. | Section 10: Error Handling and Edge Cases |\n| **Fixture** | A reusable resource or setup/teardown logic that provides test dependencies, managed by the framework with controlled lifecycle and scope. | Section 3: High-Level Architecture |\n| **Fixture-Aware Parallel Execution** | A potential future extension where parallel test scheduling accounts for shared fixture instances to prevent conflicts. | Section 13: Future Extensions |\n| **FixtureLifecycleManager** | The component responsible for creating, caching, and tearing down fixture instances according to their scope and dependency relationships. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **FixtureRegistry** | The central registry that stores fixture definitions discovered through decorator scanning, mapping fixture names to their functions. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **FixtureRequest** | A **work order** data structure representing a request for a specific fixture value, containing scope information and caching keys. | Section 4: Data Model |\n| **FixtureScope** | An enumeration defining the lifetime boundaries for fixtures: `FUNCTION`, `CLASS`, `MODULE`, and `SESSION`. | Section 4: Data Model |\n| **Fixture System** | The subsystem responsible for managing test dependencies through setup/teardown hooks and dependency injection. | Section 3: High-Level Architecture |\n| **Forensic Analysis** | A debugging approach that treats symptoms as clues to trace back through execution flow and identify root causes. | Section 12: Debugging Guide |\n| **FormattedError** | A structured representation of an exception captured during test execution, containing type, message, traceback, and context. | Section 4: Data Model |\n| **Given** | A potential future extension decorator for property-based testing that generates parameter values from strategies. | Section 13: Future Extensions |\n| **God Object** | An anti-pattern where a single class or component handles too many responsibilities, making the system difficult to maintain. | Section 3: High-Level Architecture |\n| **Golden Master Testing** | A testing approach that compares system output against known-good reference files, useful for verifying formatted output. | Section 11: Testing Strategy |\n| **Hierarchical Error Reporting** | An error presentation strategy that shows the root cause first followed by supporting details and context. | Section 10: Error Handling and Edge Cases |\n| **Integration Tests** | Tests that verify multiple components working together, as opposed to unit tests that test components in isolation. | Section 11: Testing Strategy |\n| **Interactive Debugger Integration** | A potential future extension that automatically drops into a debugger when tests fail, allowing immediate inspection. | Section 13: Future Extensions |\n| **Isolation** | The property ensuring tests do not interfere with each other's state, achieved through independent execution environments. | Section 5: Component Design: Discovery & Execution |\n| **JUnit XML** | A standardized XML format for test results used by CI/CD systems like Jenkins and GitHub Actions for reporting and analytics. | Section 4: Data Model |\n| **JUnitFormatter** | The component responsible for converting test results into JUnit XML format for CI system consumption. | Section 8: Component Design: Reporting & CLI |\n| **Matchers API** | An extensible API allowing users to define custom assertion predicates with tailored failure messages through the **Rulebook Builder** pattern. | Section 6: Component Design: Assertions & Matchers |\n| **Medical Chart** | The mental model for `TestResult` as a detailed record of test execution, including status, duration, and diagnostic information. | Section 4: Data Model |\n| **Minimal Reproduction Case** | The smallest possible test case that demonstrates a bug, essential for effective debugging and issue reporting. | Section 12: Debugging Guide |\n| **ModuleStats** | Statistics collected for a specific module, including counts of passed, failed, errored, and skipped tests with execution time. | Section 4: Data Model |\n| **Mutually Exclusive** | A property of command-line flags that cannot be used together (e.g., `--verbose` and `--quiet`). | Section 8: Component Design: Reporting & CLI |\n| **Normalization** | The process of replacing variable parts of test output (like timestamps or IDs) with placeholders for consistent comparison. | Section 11: Testing Strategy |\n| **OutputWriter** | The component responsible for writing formatted output to stdout or files, handling encoding and stream management. | Section 8: Component Design: Reporting & CLI |\n| **Parameterized Tests** | A potential future extension where a single test definition is executed multiple times with different input values. | Section 13: Future Extensions |\n| **Pattern Resolution** | The process of converting file/directory/glob patterns (like `tests/*.py`) to concrete file system paths. | Section 5: Component Design: Discovery & Execution |\n| **Performance Review** | The mental model for the Reporter as a system that analyzes test outcomes and produces comprehensive reports. | Section 8: Component Design: Reporting & CLI |\n| **Pipeline Architecture** | The architectural pattern where components are arranged in a linear sequence, with output of one becoming input to the next. | Section 3: High-Level Architecture |\n| **Playlist of Tests** | The mental model for `TestSuite` as an organized collection of tests ready for execution. | Section 4: Data Model |\n| **Plugin Registry** | A potential future extension component that manages plugin registration and event handling for extensibility. | Section 13: Future Extensions |\n| **Plugin System** | A potential future extension providing a modular mechanism for third-party enhancements and integrations. | Section 13: Future Extensions |\n| **Post-Mortem Debug** | A potential future extension function that enters a debugger after an exception occurs during test execution. | Section 13: Future Extensions |\n| **Progress Indicator** | Visual feedback during test execution (like dots, letters, or progress bars) that shows test completion status. | Section 8: Component Design: Reporting & CLI |\n| **Property-Based Testing** | A testing methodology that generates random inputs to verify system invariants, as opposed to example-based testing. | Section 11: Testing Strategy |\n| **Recipe Card** | The mental model for `TestCase` as a complete set of instructions for executing a single test. | Section 4: Data Model |\n| **Resilient Conductor** | The mental model for the framework as an orchestra conductor that handles musician errors gracefully without crashing the entire performance. | Section 10: Error Handling and Edge Cases |\n| **Resource Pool Manager** | The mental model for the fixture system as a manager of scoped resources that are allocated and released on demand. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **RetryingRunner** | A potential future extension component that automatically retries failing tests to handle flaky test scenarios. | Section 13: Future Extensions |\n| **Rulebook Builder** | The mental model for the Matchers API as a system for creating custom verification rules for domain-specific checks. | Section 6: Component Design: Assertions & Matchers |\n| **RunContext** | A data structure capturing the execution context of a test, including timing, loaded fixtures, and any errors. | Section 4: Data Model |\n| **Scope Boundary** | The point in test execution when all tests of a certain scope complete, triggering teardown of fixtures at that scope. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **Scope Leak** | A problematic situation where a test holds a reference to a fixture value after its teardown, potentially causing issues. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **Self-Validation** | The practice of using the test framework to test itself, creating a virtuous cycle of quality assurance. | Section 11: Testing Strategy |\n| **SnapshotStore** | A potential future extension component that manages storage and retrieval of snapshot references for comparison. | Section 13: Future Extensions |\n| **Snapshot Testing** | A potential future extension approach where actual output is automatically compared against stored reference snapshots. | Section 13: Future Extensions |\n| **Stage Crew** | The mental model for fixtures as behind-the-scenes support that sets up the stage before tests and cleans up afterward. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **StatisticsCollector** | The component responsible for accumulating test results and computing aggregate statistics like pass rates and timing. | Section 8: Component Design: Reporting & CLI |\n| **Strategy Pattern** | A design pattern defining a family of algorithms, encapsulating each one, and making them interchangeable. | Section 8: Component Design: Reporting & CLI |\n| **Test Conductor** | The overarching mental model for the entire framework as an orchestra conductor coordinating tests, fixtures, and reporting. | Section 1: Context and Problem Statement |\n| **Test Doubles** | A potential future extension category including mocks, stubs, and fakes that replace real dependencies during testing. | Section 13: Future Extensions |\n| **Test Execution** | The process of running test functions, evaluating assertions, and recording outcomes within isolated environments. | Section 5: Component Design: Discovery & Execution |\n| **Test Filter** | A data structure representing filtering criteria for tests, including name patterns and marker requirements. | Section 4: Data Model |\n| **Test Isolation** | The architectural guarantee that tests run in independent environments without shared state, preventing interference. | Section 5: Component Design: Discovery & Execution |\n| **Test Retries** | A potential future extension feature that automatically re-executes failing tests a specified number of times. | Section 13: Future Extensions |\n| **Test Run Statistics** | Comprehensive aggregate data about a complete test run, including totals, timing, and per-module breakdowns. | Section 4: Data Model |\n| **TestDiscovery** | The process of automatically scanning modules and identifying test functions based on naming conventions. | Section 5: Component Design: Discovery & Execution |\n| **TestCase** | The **recipe card** data structure representing a single test to execute, containing function reference, metadata, and dependencies. | Section 4: Data Model |\n| **TestResult** | The **medical chart** data structure capturing the outcome of executing a test, including status, message, and timing. | Section 4: Data Model |\n| **TestStatus** | An enumeration representing the possible states of a test: `PENDING`, `RUNNING`, `PASSED`, `FAILED`, `ERRORED`, `SKIPPED`. | Section 4: Data Model |\n| **TestSuite** | The **playlist** data structure representing a collection of `TestCase` objects organized for execution. | Section 4: Data Model |\n| **The Restaurant Order** | The mental model for test parameters as order items that are fulfilled by the fixture kitchen before test execution. | Section 7: Component Design: Fixtures & Setup/Teardown |\n| **Traffic Light System** | The mental model for `TestStatus` values as indicators of test state (green for PASSED, red for FAILED, etc.). | Section 4: Data Model |\n| **Troubleshooting Playbook** | A structured guide for diagnosing and fixing common issues, organized by symptoms and solutions. | Section 12: Debugging Guide |\n| **TypeComparatorRegistry** | A registry mapping Python types to specialized comparison functions for handling complex equality checks. | Section 6: Component Design: Assertions & Matchers |\n| **Work Order** | The mental model for `FixtureRequest` as an instruction sheet for the fixture system to create a specific fixture value. | Section 4: Data Model |\n| **xUnit** | The family of testing frameworks following a class-based pattern with setup/teardown methods (e.g., unittest, JUnit). | Section 1: Context and Problem Statement |\n| **XML Escaping** | The process of replacing special XML characters (`<`, `>`, `&`, `\"`, `'`) with their corresponding entity references. | Section 8: Component Design: Reporting & CLI |\n\n---\n\n### Implementation Guidance\n\nFor a glossary section, the implementation guidance focuses on how to maintain and evolve this terminology as the framework develops.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|--------------|-----------------|\n| **Terminology Management** | Maintain a single glossary markdown file | Use a documentation generator with term extraction |\n| **Cross-Reference Links** | Manual section number references | Automated link generation using documentation tooling |\n| **Term Validation** | Manual review during code reviews | Linter that checks for undefined terms in documentation |\n\n#### B. Recommended File/Module Structure\n\n```\nproject-apollo/\n├── docs/\n│   ├── design-doc.md          # Main design document\n│   └── glossary.md            # Standalone glossary file (this section)\n├── src/\n│   └── apollo/\n│       ├── __init__.py        # Public API exports\n│       ├── cli.py             # CLI interface\n│       ├── discover.py        # Test discovery\n│       ├── runner.py          # Test execution\n│       ├── assertions.py      # Assertion engine\n│       ├── fixtures.py        # Fixture system\n│       ├── reporting.py       # Result formatting\n│       └── types.py           # Core data types (TestCase, TestResult, etc.)\n└── tests/\n    └── test_apollo.py         # Self-tests for the framework\n```\n\n#### C. Glossary Maintenance Script\n\nWhile the glossary itself is documentation, you can create a simple script to help maintain consistency:\n\n```python\n\"\"\"\nglossary_validator.py - Simple script to validate terminology usage.\n\"\"\"\nimport re\nfrom pathlib import Path\nfrom typing import Dict, Set\n\n# Load glossary terms from a text file\ndef load_glossary_terms(glossary_path: Path) -> Dict[str, str]:\n    \"\"\"Load terms and their definitions from a markdown glossary file.\"\"\"\n    terms = {}\n    term_pattern = re.compile(r'\\*\\*(.*?)\\*\\*')  # Matches **Term** in markdown\n    \n    with open(glossary_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        for i, line in enumerate(lines):\n            match = term_pattern.search(line)\n            if match and \"**\" in line and \"|\" in line:\n                # Simple extraction from markdown table\n                parts = line.split('|')\n                if len(parts) >= 2:\n                    term = parts[1].strip('* ').strip()\n                    if term and len(term) > 2:\n                        terms[term] = i + 1  # Store line number\n    return terms\n\ndef find_undefined_terms(source_dir: Path, glossary_terms: Set[str]) -> Dict[str, List[str]]:\n    \"\"\"Scan source code for potential undefined terms.\"\"\"\n    undefined = {}\n    source_pattern = re.compile(r'[A-Z][a-zA-Z]+')  # Matches CapitalizedTerms\n    \n    for py_file in source_dir.rglob(\"*.py\"):\n        with open(py_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n            # Find capitalized multi-word terms that might need definition\n            for match in source_pattern.finditer(content):\n                term = match.group(0)\n                if (term not in glossary_terms and \n                    len(term) > 3 and \n                    term[0].isupper() and \n                    not term.isupper()):  # Exclude ALL_CAPS constants\n                    if term not in undefined:\n                        undefined[term] = []\n                    undefined[term].append(str(py_file))\n    \n    return undefined\n\nif __name__ == \"__main__\":\n    # Simple validation example\n    glossary_path = Path(\"docs/glossary.md\")\n    source_dir = Path(\"src/apollo\")\n    \n    if glossary_path.exists():\n        terms = load_glossary_terms(glossary_path)\n        print(f\"Loaded {len(terms)} terms from glossary\")\n        \n        undefined = find_undefined_terms(source_dir, set(terms.keys()))\n        if undefined:\n            print(\"\\nPotential undefined terms found in source:\")\n            for term, files in sorted(undefined.items()):\n                print(f\"  {term}: {', '.join(files[:3])}\")\n        else:\n            print(\"All capitalized terms appear to be defined in glossary.\")\n```\n\n#### D. Documentation Standards\n\nWhen adding new terms to the glossary:\n\n1. **Add entry immediately** when introducing a new concept in the design document\n2. **Use consistent formatting**: bold term, clear definition, section reference\n3. **Update alphabetically**: Maintain alphabetical order for easy lookup\n4. **Cross-reference**: Link between related terms in definitions\n5. **Review periodically**: Schedule glossary reviews as the framework evolves\n\n#### E. Language-Specific Hints\n\n- **Python docstrings**: Use glossary terms consistently in module and function docstrings\n- **Type hints**: Reference glossary types like `TestCase` and `TestResult` in type annotations\n- **Error messages**: Use terminology from the glossary in user-facing error messages for consistency\n- **API documentation**: When generating API docs, link back to glossary definitions for key concepts\n\n#### F. Milestone Checkpoint\n\nAfter completing each milestone, verify that all new terminology introduced in that milestone has been added to the glossary:\n\n```bash\n# Run the glossary validator to check for undefined terms\npython glossary_validator.py\n\n# Expected output example:\n# Loaded 45 terms from glossary\n# All capitalized terms appear to be defined in glossary.\n```\n\n#### G. Debugging Terminology Issues\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Confusion about a term's meaning | Term not defined or ambiguous definition | Search glossary for term; check if multiple definitions exist | Add or clarify definition in glossary; ensure single source of truth |\n| Inconsistent usage in code | Different names for same concept | Search codebase for synonyms; check variable/function names | Standardize on glossary term; rename variables/functions |\n| Missing documentation for new feature | Forgot to add terms to glossary | Compare new feature documentation against glossary | Add all new terms introduced by the feature |\n\n---\n"}