{"html":"<h1 id=\"build-your-own-transformer-design-document\">Build Your Own Transformer: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This system implements a GPT-style transformer from scratch for autoregressive text generation. The key architectural challenge is building the multi-layer attention mechanism that allows each token to attend to all previous tokens while efficiently processing sequences in parallel during training.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - understanding why transformers exist and the challenges in implementing them</p>\n</blockquote>\n<p>The transformer architecture fundamentally changed how we think about sequence modeling in natural language processing. Before diving into implementation details, we need to understand why transformers were revolutionary, what problems they solved, and the specific challenges we&#39;ll face when building one from scratch. This section establishes the conceptual foundation and practical motivation for our implementation journey.</p>\n<h3 id=\"mental-model-the-cocktail-party\">Mental Model: The Cocktail Party</h3>\n<p>Imagine you&#39;re at a bustling cocktail party with dozens of conversations happening simultaneously. As a human, you have a remarkable ability to focus your attention selectively - you can tune into the conversation with your colleague while filtering out background chatter, or suddenly shift focus when you hear your name mentioned across the room. Moreover, you can dynamically adjust your attention based on context: when someone mentions a topic you&#39;re passionate about, you lean in and focus intently, but when the conversation turns to something irrelevant, your attention naturally drifts.</p>\n<p><strong>Attention mechanisms in transformers work remarkably similarly.</strong> Each word (or token) in a sentence is like a person at the cocktail party. When processing the word &quot;bank&quot; in the sentence &quot;I went to the bank to deposit money,&quot; the attention mechanism allows the word &quot;bank&quot; to &quot;listen&quot; to all the other words in the sentence. It pays more attention to words like &quot;deposit&quot; and &quot;money&quot; (which suggest a financial institution) while giving less attention to words like &quot;went&quot; or &quot;to&quot; (which are less informative for disambiguation).</p>\n<p>The key insight is that this attention is <strong>learned, not programmed.</strong> Just as you&#39;ve learned through experience that certain conversational cues indicate important information, the transformer learns through training which words should pay attention to which other words. The attention weights emerge naturally from the data, allowing the model to discover linguistic patterns like:</p>\n<ul>\n<li>Pronouns attending to their antecedents (&quot;The cat sat on its bed&quot; - &quot;its&quot; strongly attends to &quot;cat&quot;)</li>\n<li>Modifiers attending to what they modify (&quot;The red car&quot; - &quot;red&quot; attends to &quot;car&quot;)</li>\n<li>Syntactic relationships (&quot;The book that I read&quot; - &quot;book&quot; and &quot;read&quot; attend to each other)</li>\n</ul>\n<blockquote>\n<p><strong>Key Insight:</strong> Unlike rule-based systems that explicitly encode grammatical relationships, transformers discover these relationships implicitly through attention patterns that emerge during training.</p>\n</blockquote>\n<p>This cocktail party analogy captures three crucial aspects of transformer attention:</p>\n<ol>\n<li><strong>Selective focus</strong>: Not all word pairs are equally important</li>\n<li><strong>Dynamic adjustment</strong>: Attention patterns change based on context</li>\n<li><strong>Learned behavior</strong>: The model discovers what to pay attention to through training data</li>\n</ol>\n<h3 id=\"pre-transformer-approaches\">Pre-Transformer Approaches</h3>\n<p>Before transformers revolutionized NLP in 2017, researchers relied primarily on recurrent neural networks (RNNs) and convolutional neural networks (CNNs) for sequence modeling. Understanding their limitations illuminates why the transformer architecture was so groundbreaking.</p>\n<h4 id=\"recurrent-neural-networks-the-sequential-bottleneck\">Recurrent Neural Networks: The Sequential Bottleneck</h4>\n<p>RNNs, including their more sophisticated variants like LSTMs and GRUs, process sequences one token at a time in strict left-to-right order. Think of an RNN as a person reading a book who can only look at one word at a time and must remember everything important from previous words in their limited working memory.</p>\n<p><strong>The fundamental limitation</strong> is that information must pass through a sequential bottleneck. To understand the relationship between &quot;cat&quot; and &quot;its&quot; in &quot;The cat walked across the room and sat on its bed,&quot; the RNN must:</p>\n<ol>\n<li>Process &quot;cat&quot; and encode it in hidden state h₁</li>\n<li>Process &quot;walked&quot; and update the hidden state to h₂ (potentially diluting information about &quot;cat&quot;)</li>\n<li>Continue through &quot;across,&quot; &quot;the,&quot; &quot;room,&quot; &quot;and,&quot; &quot;sat,&quot; &quot;on&quot; - each step potentially losing more information about the original &quot;cat&quot;</li>\n<li>Finally reach &quot;its&quot; with a hidden state h₉ that hopefully still contains enough information about &quot;cat&quot; to make the connection</li>\n</ol>\n<p>This creates several problems:</p>\n<table>\n<thead>\n<tr>\n<th>Problem</th>\n<th>Description</th>\n<th>Impact on Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Vanishing Gradients</strong></td>\n<td>Information from early tokens gets exponentially diluted through sequential processing</td>\n<td>Long-range dependencies are poorly captured</td>\n</tr>\n<tr>\n<td><strong>Sequential Processing</strong></td>\n<td>Cannot parallelize computation across sequence positions</td>\n<td>Training is slow, especially on long sequences</td>\n</tr>\n<tr>\n<td><strong>Fixed Context Window</strong></td>\n<td>Hidden state has limited capacity to store information</td>\n<td>Complex relationships in long texts are lost</td>\n</tr>\n<tr>\n<td><strong>Recency Bias</strong></td>\n<td>Recent tokens have disproportionate influence on the hidden state</td>\n<td>Earlier context gets forgotten</td>\n</tr>\n</tbody></table>\n<h4 id=\"convolutional-neural-networks-local-patterns-only\">Convolutional Neural Networks: Local Patterns Only</h4>\n<p>CNNs excel at capturing local patterns through sliding windows of fixed size. In NLP, this translates to detecting n-gram patterns like &quot;not good&quot; (negative sentiment) or &quot;New York&quot; (named entity). However, CNNs face their own limitations for sequence modeling:</p>\n<p><strong>Limited receptive field</strong>: A CNN with kernel size 3 can only directly relate words that are within 3 positions of each other. To capture longer dependencies, you need either:</p>\n<ul>\n<li>Very deep networks (many layers) which causes vanishing gradients and increases computational cost</li>\n<li>Very large kernels which become parameter-intensive and lose the inductive bias of local patterns</li>\n</ul>\n<p><strong>No positional flexibility</strong>: CNNs assume that patterns are translation-invariant (the same pattern matters regardless of position), but language is highly position-dependent. The phrase &quot;not happy&quot; has different meaning than &quot;happy, not sad.&quot;</p>\n<h4 id=\"the-transformer-revolution-parallel-attention\">The Transformer Revolution: Parallel Attention</h4>\n<p>Transformers solve these fundamental limitations through a radically different approach: instead of processing sequences sequentially or locally, they compute attention between all pairs of positions simultaneously.</p>\n<blockquote>\n<p><strong>Architecture Decision: Self-Attention Over Sequential Processing</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to capture long-range dependencies without sequential processing bottlenecks</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Deeper RNNs with better memory mechanisms</li>\n<li>CNN architectures with dilated convolutions for larger receptive fields  </li>\n<li>Self-attention mechanisms that directly model all pairwise relationships</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Self-attention with parallel computation across all sequence positions</li>\n<li><strong>Rationale</strong>: Eliminates sequential bottleneck, enables parallelization, provides direct paths for information flow between any two positions regardless of distance</li>\n<li><strong>Consequences</strong>: Enables efficient training on long sequences and better capture of long-range dependencies, but introduces quadratic memory complexity in sequence length</li>\n</ul>\n</blockquote>\n<p>The transformer&#39;s self-attention mechanism creates direct connections between every pair of tokens. In our &quot;cat...its&quot; example, the attention mechanism can directly compute how much &quot;its&quot; should attend to &quot;cat&quot; without any intermediate processing steps. This eliminates the vanishing gradient problem for long-range dependencies and enables parallel computation.</p>\n<table>\n<thead>\n<tr>\n<th>Architecture</th>\n<th>Dependency Path Length</th>\n<th>Parallel Operations</th>\n<th>Memory Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>RNN</strong></td>\n<td>O(sequence_length)</td>\n<td>O(1) per timestep</td>\n<td>O(sequence_length × hidden_size)</td>\n</tr>\n<tr>\n<td><strong>CNN</strong></td>\n<td>O(log(sequence_length)) with dilated convolutions</td>\n<td>O(sequence_length)</td>\n<td>O(sequence_length × hidden_size)</td>\n</tr>\n<tr>\n<td><strong>Transformer</strong></td>\n<td>O(1) - direct connections</td>\n<td>O(sequence_length²)</td>\n<td>O(sequence_length² + sequence_length × hidden_size)</td>\n</tr>\n</tbody></table>\n<p>The transformer trades increased memory usage (quadratic in sequence length) for dramatically improved modeling capability and training efficiency. This trade-off has proven worthwhile for most NLP applications, leading to the current dominance of transformer-based models.</p>\n<h3 id=\"implementation-challenges\">Implementation Challenges</h3>\n<p>Building a transformer from scratch involves several interconnected mathematical and computational challenges. Unlike simpler neural network architectures where you can implement and test components in isolation, transformers require careful coordination between multiple sophisticated mechanisms.</p>\n<h4 id=\"mathematical-complexity-the-attention-computation\">Mathematical Complexity: The Attention Computation</h4>\n<p>The core mathematical challenge lies in implementing scaled dot-product attention correctly. The computation appears deceptively simple:</p>\n<p><strong>Attention(Q, K, V) = softmax(QK^T / √d_k)V</strong></p>\n<p>However, this simple equation conceals several implementation pitfalls:</p>\n<p>⚠️ <strong>Pitfall: Dimension Mismatches</strong>\nThe matrix multiplication QK^T requires Q to have shape (batch_size, seq_len, d_k) and K to have shape (batch_size, seq_len, d_k), producing an attention matrix of shape (batch_size, seq_len, seq_len). Many implementations fail because they transpose the wrong dimensions or assume K should be transposed before the operation. The correct implementation transposes K during the multiplication: <code>torch.matmul(Q, K.transpose(-2, -1))</code>.</p>\n<p>⚠️ <strong>Pitfall: Missing Scale Factor</strong><br>The division by √d_k is crucial for numerical stability. Without this scaling, the dot products can become very large (especially for large d_k), causing the softmax to produce extremely sharp distributions with gradients close to zero. This leads to training instability and poor convergence. The scale factor keeps the variance of the dot products approximately constant regardless of the key dimension.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Masking</strong>\nCausal attention masking must set future positions to negative infinity BEFORE applying softmax, not after. Setting masked positions to zero after softmax breaks the probability distribution (the remaining probabilities don&#39;t sum to 1) and provides misleading attention patterns during debugging.</p>\n<h4 id=\"multi-head-attention-coordination\">Multi-Head Attention Coordination</h4>\n<p>Multi-head attention requires splitting the model dimension across multiple attention heads and then recombining their outputs. This creates several coordination challenges:</p>\n<p><strong>Dimension partitioning</strong>: With model dimension d_model = 512 and num_heads = 8, each head operates on d_k = d_v = 64 dimensions. The implementation must:</p>\n<ol>\n<li>Split the input embeddings into 8 chunks of size 64</li>\n<li>Apply separate Q, K, V projections to each chunk  </li>\n<li>Compute attention independently for each head</li>\n<li>Concatenate the 8 outputs back into a 512-dimensional vector</li>\n<li>Apply a final linear projection</li>\n</ol>\n<p><strong>Memory layout optimization</strong>: Naive implementations might use loops over attention heads, but efficient implementations reshape tensors to compute all heads in parallel. This requires careful tensor manipulation:</p>\n<ul>\n<li>Input shape: (batch_size, seq_len, d_model)</li>\n<li>Reshaped for multi-head: (batch_size, seq_len, num_heads, d_k)  </li>\n<li>Transposed for parallel computation: (batch_size, num_heads, seq_len, d_k)</li>\n<li>After attention: (batch_size, num_heads, seq_len, d_v)</li>\n<li>Concatenated output: (batch_size, seq_len, d_model)</li>\n</ul>\n<h4 id=\"numerical-stability-issues\">Numerical Stability Issues</h4>\n<p>Transformers are particularly susceptible to numerical instability due to the combination of softmax operations, layer normalization, and deep architectures.</p>\n<p><strong>Softmax overflow</strong>: For large attention scores, softmax can overflow or underflow. Consider attention scores [10, 8, 12] - after exponential, these become [22026, 2981, 162754], which may exceed float32 precision. The standard solution is to subtract the maximum value before exponential: [10-12, 8-12, 12-12] = [-2, -4, 0], giving stable exponentials [0.135, 0.018, 1.0].</p>\n<p><strong>Gradient explosion</strong>: Deep transformer stacks can suffer from exploding gradients, especially in the early stages of training. This is typically addressed through:</p>\n<ul>\n<li>Gradient clipping to limit the norm of gradients</li>\n<li>Careful weight initialization (Xavier/Glorot or He initialization)  </li>\n<li>Layer normalization to stabilize activations</li>\n<li>Residual connections to provide stable gradient paths</li>\n</ul>\n<p><strong>Accumulating precision errors</strong>: During autoregressive generation, small numerical errors can accumulate over many generation steps. This is particularly problematic when using mixed precision training (float16 for speed, float32 for accuracy) where careful casting between precisions is required.</p>\n<h4 id=\"memory-and-computational-scalability\">Memory and Computational Scalability</h4>\n<p>The quadratic memory complexity of self-attention creates significant scalability challenges:</p>\n<p><strong>Memory usage</strong>: For a sequence of length L with model dimension d, self-attention requires O(L²) memory for storing attention weights, plus O(L × d) for embeddings. For L=512 and batch size 32, the attention matrix alone requires 32 × 512 × 512 × 4 bytes = 33MB per layer. A 12-layer model uses nearly 400MB just for attention matrices.</p>\n<p><strong>Computational complexity</strong>: Each attention computation requires O(L² × d) operations. For long sequences, this dominates training time. Modern implementations use several optimizations:</p>\n<ul>\n<li><strong>KV caching</strong>: During generation, cache key and value computations for previous tokens to avoid recomputation</li>\n<li><strong>Attention chunking</strong>: Process attention in blocks to reduce peak memory usage</li>\n<li><strong>Flash attention</strong>: Fused attention kernels that minimize memory transfers</li>\n</ul>\n<h4 id=\"integration-and-testing-complexity\">Integration and Testing Complexity</h4>\n<p>Unlike simpler architectures where you can test each layer independently, transformer components are tightly coupled:</p>\n<p><strong>Forward pass dependencies</strong>: Testing the transformer block requires working implementations of multi-head attention, layer normalization, feed-forward networks, and residual connections. A bug in any component can cause failures throughout the system.</p>\n<p><strong>Training loop complexity</strong>: The language modeling objective requires careful coordination between tokenization, batch preparation, loss computation, and gradient updates. Label shifting (input tokens vs. target tokens) is a common source of bugs.</p>\n<p><strong>Generation complexity</strong>: Autoregressive text generation involves different code paths than training, with additional complexity from sampling strategies, KV caching, and stopping criteria.</p>\n<blockquote>\n<p><strong>Implementation Strategy</strong>: Given these interdependencies, we&#39;ll build and test components in carefully ordered milestones. Each milestone produces a testable artifact that validates the correctness of previous components while building toward the complete system.</p>\n</blockquote>\n<p>The combination of mathematical sophistication, numerical stability requirements, and tight component coupling makes transformer implementation significantly more challenging than simpler neural architectures. However, understanding these challenges upfront allows us to structure our implementation to minimize debugging complexity and ensure correctness at each step.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides the practical foundation for building your transformer, with technology choices optimized for learning and debugging rather than production performance.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Recommended for Learning)</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Deep Learning Framework</strong></td>\n<td>PyTorch with manual tensor operations</td>\n<td>PyTorch Lightning with automated training loops</td>\n</tr>\n<tr>\n<td><strong>Tensor Operations</strong></td>\n<td>Explicit <code>torch.matmul</code>, <code>torch.softmax</code> calls</td>\n<td><code>torch.nn.functional</code> high-level operations</td>\n</tr>\n<tr>\n<td><strong>Model Definition</strong></td>\n<td>Custom <code>nn.Module</code> classes with explicit forward methods</td>\n<td><code>nn.Sequential</code> or config-driven model builders</td>\n</tr>\n<tr>\n<td><strong>Data Loading</strong></td>\n<td>Simple character-level tokenization with manual batching</td>\n<td>HuggingFace tokenizers with DataLoader optimizations</td>\n</tr>\n<tr>\n<td><strong>Training Loop</strong></td>\n<td>Manual gradient computation and optimization steps</td>\n<td>PyTorch Lightning Trainer with callbacks</td>\n</tr>\n<tr>\n<td><strong>Debugging</strong></td>\n<td>Print statements and tensor shape inspection</td>\n<td>TensorBoard integration with attention visualizations</td>\n</tr>\n</tbody></table>\n<p><strong>Rationale for simple options</strong>: When learning transformers, explicit implementations help you understand what each operation does. High-level abstractions hide important details about tensor shapes, attention computations, and gradient flow that are crucial for debugging and intuition building.</p>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>Organize your code to mirror the conceptual hierarchy and enable incremental testing:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer-project/\n├── src/\n│   ├── __init__.py\n│   ├── attention.py          ← Self-attention mechanism (Milestone 1)\n│   ├── transformer_block.py  ← Complete transformer block (Milestone 2)  \n│   ├── model.py             ← Full transformer stack and embedding\n│   ├── training.py          ← Training pipeline (Milestone 3)\n│   ├── generation.py        ← Text generation (Milestone 4)\n│   └── utils.py             ← Helper functions and utilities\n├── tests/\n│   ├── test_attention.py    ← Unit tests for attention mechanisms\n│   ├── test_transformer_block.py  ← Unit tests for complete blocks\n│   ├── test_integration.py  ← End-to-end pipeline tests\n│   └── test_data/           ← Small datasets for testing\n├── data/\n│   ├── tiny_shakespeare.txt ← Small training corpus\n│   └── processed/           ← Tokenized and batched data\n├── notebooks/\n│   ├── debug_attention.ipynb    ← Attention weight visualization\n│   ├── training_analysis.ipynb ← Loss curves and training metrics\n│   └── generation_examples.ipynb ← Sample outputs and analysis\n├── configs/\n│   └── model_config.py      ← Hyperparameter configurations\n└── requirements.txt         ← Python dependencies</code></pre></div>\n\n<p>This structure separates concerns cleanly while maintaining clear dependencies: <code>attention.py</code> has no internal dependencies, <code>transformer_block.py</code> depends only on <code>attention.py</code>, and so forth.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Configuration Management</strong> (<code>configs/model_config.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete configuration for transformer model and training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Model architecture</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    vocab_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 65</span><span style=\"color:#6A737D\">        # Number of unique tokens (characters for tiny dataset)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 256</span><span style=\"color:#6A737D\">   # Maximum sequence length for training</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 384</span><span style=\"color:#6A737D\">          # Model dimension (embedding size)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">          # Number of attention heads (d_model must be divisible)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">         # Number of transformer blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_ff: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1536</span><span style=\"color:#6A737D\">           # Feed-forward dimension (typically 4 * d_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">   # Dropout probability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Training hyperparameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3e-4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_grad_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#6A737D\">  # Gradient clipping threshold</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generation settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_k: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_p: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_generation_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"d_model (</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.d_model</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) must be divisible by num_heads (</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.num_heads</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_ff </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Feed-forward dimension must be positive\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.dropout_rate </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Dropout rate must be in [0, 1)\"</span></span></code></pre></div>\n\n<p><strong>Utility Functions</strong> (<code>src/utils.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn.functional </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> F</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_causal_mask</span><span style=\"color:#E1E4E8\">(seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: torch.device) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Create lower triangular causal mask for self-attention.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        seq_length: Length of sequence</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        device: Device to create tensor on</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Boolean mask of shape (seq_length, seq_length) where True means \"allow attention\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create lower triangular matrix using torch.tril</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert to boolean (1s become True, 0s become False)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Move to specified device</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> apply_causal_mask</span><span style=\"color:#E1E4E8\">(attention_scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Apply causal mask to attention scores before softmax.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        attention_scores: Raw attention scores of shape (batch, num_heads, seq_len, seq_len)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        mask: Boolean mask of shape (seq_len, seq_len)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Masked attention scores with -inf at masked positions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use torch.where to set masked positions to -inf</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Ensure mask is broadcasted correctly across batch and head dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use float('-inf') for masked positions to ensure softmax gives 0 probability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(model: torch.nn.Module) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Count total number of trainable parameters in model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> p.requires_grad)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_attention_weights</span><span style=\"color:#E1E4E8\">(attention_weights: torch.Tensor, tokens: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, save_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Visualize attention weights as heatmap.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        attention_weights: Attention weights of shape (num_layers, num_heads, seq_len, seq_len)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tokens: List of token strings for axis labels</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        layer: Which layer to visualize  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        head: Which attention head to visualize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        save_path: Optional path to save the plot</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract weights for specified layer and head</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create matplotlib heatmap with token labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add colorbar and title</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Save or display the plot</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Character-level tokenizer for educational purposes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, vocab: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize tokenizer with vocabulary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If vocab is None, create default vocab with printable ASCII chars</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create char_to_idx and idx_to_char mappings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store special token indices (if any)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> encode</span><span style=\"color:#E1E4E8\">(self, text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert text string to list of token indices.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert each character to its vocabulary index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle unknown characters (either skip or use special UNK token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> decode</span><span style=\"color:#E1E4E8\">(self, token_ids: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert list of token indices back to text string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert each index back to character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Join characters into string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle invalid indices gracefully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_text</span><span style=\"color:#E1E4E8\">(cls, text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'SimpleTokenizer'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create tokenizer by extracting vocabulary from text corpus.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find all unique characters in text</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Sort characters for consistent ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return new tokenizer instance with this vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Basic Data Loading</strong> (<code>src/data_loader.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.utils.data </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dataset, DataLoader</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleTokenizer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TextDataset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Dataset</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Dataset for character-level language modeling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokenizer: SimpleTokenizer, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, stride: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Create dataset from text string.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            text: Raw text content</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            tokenizer: Tokenizer to convert text to indices  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            seq_length: Length of each training sequence</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stride: Step size between sequences (default: seq_length for no overlap)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize the entire text</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create overlapping sequences of length seq_length + 1 </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         (extra token for target)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store sequences as list of (input, target) pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Input is tokens[i:i+seq_length], target is tokens[i+1:i+seq_length+1]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __len__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return number of sequences in dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return length of stored sequences</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __getitem__</span><span style=\"color:#E1E4E8\">(self, idx: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return input and target tensors for given index.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get the idx-th sequence pair</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert to PyTorch tensors with dtype=torch.long</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_data_loaders</span><span style=\"color:#E1E4E8\">(text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config, train_split: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.9</span><span style=\"color:#E1E4E8\">) -> Tuple[DataLoader, DataLoader, SimpleTokenizer]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Create training and validation data loaders from text.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        text: Raw text content</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        config: TransformerConfig with batch_size and seq_length</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        train_split: Fraction of data to use for training</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (train_loader, val_loader, tokenizer)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create tokenizer from text vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Split text into train and validation portions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create datasets for each split</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create DataLoader instances with specified batch_size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return loaders and tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Self-Attention Implementation</strong> (<code>src/attention.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn.functional </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> F</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> create_causal_mask, apply_causal_mask</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultiHeadAttention</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-head self-attention mechanism.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize multi-head attention.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            d_model: Model dimension (must be divisible by num_heads)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            num_heads: Number of parallel attention heads</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            dropout_rate: Dropout probability for attention weights</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate d_k = d_v = d_model // num_heads</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create linear projections for Q, K, V (single linear layer that outputs d_model dimensions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create output projection layer  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create dropout layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Initialize scale factor for attention scores</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use single linear layer for efficiency: self.qkv_proj = nn.Linear(d_model, 3 * d_model)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Apply multi-head self-attention to input sequence.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Optional causal mask of shape (seq_length, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch_size, seq_length, d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply QKV projection and split into Q, K, V</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Reshape Q, K, V for multi-head attention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         From: (batch_size, seq_length, d_model)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         To: (batch_size, num_heads, seq_length, d_k)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute attention scores: Q @ K.transpose(-2, -1)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Scale attention scores by sqrt(d_k)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply causal mask if provided (set masked positions to -inf)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply softmax to get attention weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Apply dropout to attention weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Apply attention weights to values: attention_weights @ V</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Reshape output back to (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Apply final output projection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.matmul for batch matrix multiplication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For reshaping, use .view() and .contiguous() as needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>PyTorch Tensor Operations</strong>:</p>\n<ul>\n<li>Use <code>tensor.view()</code> to reshape tensors, but call <code>.contiguous()</code> first if the tensor is not contiguous in memory</li>\n<li><code>torch.matmul()</code> handles batch matrix multiplication automatically - it multiplies the last two dimensions and broadcasts over earlier dimensions</li>\n<li>For numerical stability in softmax, PyTorch&#39;s <code>F.softmax()</code> automatically handles overflow by subtracting the max value</li>\n<li>Use <code>tensor.transpose(-2, -1)</code> to swap the last two dimensions regardless of total tensor dimensionality</li>\n</ul>\n<p><strong>Memory Management</strong>:</p>\n<ul>\n<li>Use <code>torch.no_grad()</code> context manager during inference to save memory and speed up computation</li>\n<li>Call <code>optimizer.zero_grad()</code> before each backward pass to clear accumulated gradients</li>\n<li>Use <code>del</code> to explicitly free large tensors when done with them during debugging</li>\n</ul>\n<p><strong>Device Handling</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.device(</span><span style=\"color:#9ECBFF\">'cuda'</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'cpu'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.to(device)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data.to(device)</span></span></code></pre></div>\n\n<p><strong>Gradient Debugging</strong>:</p>\n<ul>\n<li>Use <code>torch.autograd.grad_mode.set_grad_enabled(True)</code> to ensure gradients are being computed</li>\n<li>Check if gradients are flowing: <code>print(f&quot;Parameter grad norm: {param.grad.norm().item()}&quot;)</code> after backward pass</li>\n<li>Use <code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code> for gradient clipping</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint - Self-Attention</strong>:\nAfter implementing the attention mechanism, verify correct behavior:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> tests/</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> test_attention.py</span></span></code></pre></div>\n\n<p>Expected output:</p>\n<ul>\n<li>Attention weights sum to 1.0 for each query position</li>\n<li>Causal mask prevents attending to future positions (upper triangular of attention matrix should be zeros)</li>\n<li>Output shape matches input shape: (batch_size, seq_length, d_model)</li>\n<li>Gradient flows backward through attention computation</li>\n</ul>\n<p>Manual verification:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create small test input</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># batch=2, seq_len=4, d_model=64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">attention </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MultiHeadAttention(</span><span style=\"color:#FFAB70\">d_model</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">num_heads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention(x)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Input shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">x.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Output shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Signs of problems</strong>:</p>\n<ul>\n<li>NaN values in output → Check for division by zero in scaling or softmax overflow</li>\n<li>Wrong output shape → Verify tensor reshaping and transposition operations  </li>\n<li>No gradient flow → Ensure all operations are differentiable and tensors require_grad=True</li>\n<li>Attention weights don&#39;t sum to 1 → Check softmax application and mask timing</li>\n</ul>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - defining the scope and boundaries of our transformer implementation</p>\n</blockquote>\n<p>Building a transformer from scratch is an ambitious undertaking that could easily expand into months of work if we tried to implement every optimization and feature found in production systems. Like an architect who must choose between building a sturdy house versus a sprawling mansion, we need to clearly define what we will and won&#39;t build to maintain focus on the core learning objectives.</p>\n<p>The primary challenge in scoping a transformer implementation lies in balancing educational value with practical constraints. We want to build something substantial enough to understand the fundamental mechanisms, yet focused enough to complete within a reasonable timeframe. This means making deliberate trade-offs between completeness and clarity, between performance and simplicity.</p>\n<p>Our approach follows the principle of <strong>progressive complexity</strong>: we&#39;ll implement the essential components that make a transformer work, ensuring each piece is solid and well-understood, rather than attempting to build a production-ready system with all the bells and whistles. Think of this as building a working bicycle before attempting to construct a Formula 1 race car.</p>\n<h3 id=\"what-we-will-build\">What We Will Build</h3>\n<p>Our transformer implementation will focus on the <strong>core mechanisms that define the architecture</strong>, providing a solid foundation for understanding how these models work under the hood. Every component we build serves a direct educational purpose in understanding attention, autoregression, or neural language modeling.</p>\n<h4 id=\"core-architecture-components\">Core Architecture Components</h4>\n<p>We will implement a complete <strong>decoder-only transformer</strong> similar to the GPT architecture. This includes building every layer of the transformer stack from the ground up, starting with the mathematical primitives and working up to the full model.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Implementation Scope</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MultiHeadAttention</code></td>\n<td>Core attention mechanism with Q/K/V projections</td>\n<td>Scaled dot-product attention with configurable heads</td>\n</tr>\n<tr>\n<td><code>TransformerBlock</code></td>\n<td>Complete transformer layer</td>\n<td>Attention + FFN + normalization + residuals</td>\n</tr>\n<tr>\n<td><code>TransformerModel</code></td>\n<td>Full model stack</td>\n<td>Embedding + multiple transformer blocks + output head</td>\n</tr>\n<tr>\n<td><code>SimpleTokenizer</code></td>\n<td>Text-to-token conversion</td>\n<td>Character-level tokenization with encode/decode</td>\n</tr>\n<tr>\n<td><code>TextDataset</code></td>\n<td>Training data handling</td>\n<td>Sequence batching with proper label shifting</td>\n</tr>\n</tbody></table>\n<p>The attention mechanism will be implemented with <strong>full mathematical transparency</strong>. Rather than using high-level library functions, we&#39;ll compute the scaled dot-product attention manually, implement the multi-head splitting and concatenation, and handle the causal masking ourselves. This hands-on approach ensures deep understanding of how attention weights are computed and applied.</p>\n<blockquote>\n<p><strong>Decision: Decoder-Only Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: We could implement encoder-decoder (like original Transformer), encoder-only (like BERT), or decoder-only (like GPT)</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Full encoder-decoder for translation tasks</li>\n<li>Encoder-only for understanding/classification  </li>\n<li>Decoder-only for text generation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Decoder-only transformer (GPT-style)</li>\n<li><strong>Rationale</strong>: Autoregressive generation provides the clearest demonstration of attention mechanisms, and the training objective (next-token prediction) is straightforward to implement and debug</li>\n<li><strong>Consequences</strong>: We focus on generation tasks rather than understanding tasks, but gain simplicity in architecture and training pipeline</li>\n</ul>\n</blockquote>\n<h4 id=\"training-infrastructure\">Training Infrastructure</h4>\n<p>Our training pipeline will implement the fundamental components needed for <strong>language modeling</strong> with next-token prediction. This includes proper data loading, loss computation, and optimization, but keeps the training loop simple and transparent.</p>\n<table>\n<thead>\n<tr>\n<th>Training Component</th>\n<th>Functionality</th>\n<th>Educational Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Language Modeling Objective</td>\n<td>Cross-entropy loss for next-token prediction</td>\n<td>Understanding autoregressive training</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>Backpropagation through transformer blocks</td>\n<td>How gradients flow through attention</td>\n</tr>\n<tr>\n<td>Basic Optimization</td>\n<td>Adam optimizer with configurable learning rate</td>\n<td>Standard neural network training</td>\n</tr>\n<tr>\n<td>Loss Monitoring</td>\n<td>Training loss logging and convergence tracking</td>\n<td>Diagnosing training progress</td>\n</tr>\n</tbody></table>\n<p>The training loop will be implemented as a straightforward iteration over batches, computing forward passes, calculating losses, and updating parameters. We&#39;ll include essential monitoring to track convergence, but avoid complex learning rate schedules or advanced optimization techniques that obscure the core learning process.</p>\n<h4 id=\"text-generation-system\">Text Generation System</h4>\n<p>Our generation system will implement multiple <strong>sampling strategies</strong> to demonstrate how different decoding methods affect output quality. This provides hands-on experience with the trade-offs between deterministic and stochastic generation.</p>\n<table>\n<thead>\n<tr>\n<th>Generation Method</th>\n<th>Behavior</th>\n<th>Learning Objective</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Greedy Decoding</td>\n<td>Always select highest probability token</td>\n<td>Deterministic baseline generation</td>\n</tr>\n<tr>\n<td>Temperature Sampling</td>\n<td>Scale logits before sampling</td>\n<td>Control randomness vs coherence</td>\n</tr>\n<tr>\n<td>Top-k Sampling</td>\n<td>Sample from k most likely tokens</td>\n<td>Truncated probability distributions</td>\n</tr>\n<tr>\n<td>Top-p (Nucleus) Sampling</td>\n<td>Sample from cumulative probability mass p</td>\n<td>Dynamic vocabulary filtering</td>\n</tr>\n</tbody></table>\n<p>Each sampling strategy will be implemented as a separate function that takes raw logits and returns token selections, making the differences between approaches explicit and easy to experiment with.</p>\n<h4 id=\"development-and-debugging-tools\">Development and Debugging Tools</h4>\n<p>To support the learning process, we&#39;ll build essential tools for <strong>understanding and debugging</strong> the transformer&#39;s behavior. These tools make the model&#39;s internal state visible and help diagnose common implementation issues.</p>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>Purpose</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>plot_attention_weights()</code></td>\n<td>Visualize attention patterns</td>\n<td>Heatmap of attention scores across heads/layers</td>\n</tr>\n<tr>\n<td><code>count_parameters()</code></td>\n<td>Model size analysis</td>\n<td>Count trainable parameters by component</td>\n</tr>\n<tr>\n<td>Gradient monitoring</td>\n<td>Training stability diagnosis</td>\n<td>Track gradient norms and detect vanishing/exploding</td>\n</tr>\n<tr>\n<td>Loss curve plotting</td>\n<td>Training progress visualization</td>\n<td>Simple matplotlib charts of training metrics</td>\n</tr>\n</tbody></table>\n<p>These debugging tools will be integrated into the training and inference pipelines, providing immediate feedback on model behavior and making it easier to spot implementation bugs or training issues.</p>\n<h3 id=\"what-we-won39t-build\">What We Won&#39;t Build</h3>\n<p>While our transformer will be functionally complete, we&#39;ll deliberately omit numerous <strong>production optimizations and advanced features</strong> that would complicate the implementation without proportional educational benefit. These omissions are strategic choices to maintain focus on the core mechanisms.</p>\n<h4 id=\"performance-optimizations\">Performance Optimizations</h4>\n<p>We will skip the complex optimizations that make production transformers efficient but obscure their fundamental operation. Our implementation prioritizes clarity over speed, accepting performance trade-offs in service of educational goals.</p>\n<table>\n<thead>\n<tr>\n<th>Optimization</th>\n<th>Why We Skip It</th>\n<th>Production Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>KV caching during generation</td>\n<td>Complex memory management obscures generation logic</td>\n<td>10-100x speedup for long sequences</td>\n</tr>\n<tr>\n<td>Flash Attention</td>\n<td>Requires deep CUDA knowledge and specialized kernels</td>\n<td>Significant memory reduction</td>\n</tr>\n<tr>\n<td>Mixed precision training</td>\n<td>Adds complexity to gradient handling</td>\n<td>2x memory savings, faster training</td>\n</tr>\n<tr>\n<td>Gradient checkpointing</td>\n<td>Complex memory/compute trade-off implementation</td>\n<td>Enables training much larger models</td>\n</tr>\n<tr>\n<td>Model parallelism</td>\n<td>Distributed systems complexity</td>\n<td>Required for models that don&#39;t fit on one GPU</td>\n</tr>\n</tbody></table>\n<p>While KV caching provides dramatic speedups during autoregressive generation, implementing it correctly requires careful management of tensor slicing, concatenation, and device placement. The educational value doesn&#39;t justify the implementation complexity for our purposes.</p>\n<blockquote>\n<p><strong>Decision: No KV Cache Optimization</strong></p>\n<ul>\n<li><strong>Context</strong>: Autoregressive generation repeatedly computes attention over previously generated tokens</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Implement full KV caching with proper memory management</li>\n<li>Simple caching without memory optimization</li>\n<li>No caching - recompute everything each step</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: No caching - recompute attention weights each generation step  </li>\n<li><strong>Rationale</strong>: KV caching requires complex tensor manipulation and memory management that obscures the core attention mechanism. The performance cost is acceptable for educational sequences</li>\n<li><strong>Consequences</strong>: Generation will be slower (O(n²) instead of O(n) per token), but attention computation remains transparent and debuggable</li>\n</ul>\n</blockquote>\n<h4 id=\"advanced-architectural-features\">Advanced Architectural Features</h4>\n<p>Modern transformers incorporate numerous architectural refinements that improve performance but add implementation complexity. We&#39;ll use the original transformer design principles rather than chasing the latest architectural innovations.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Educational Value</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rotary Positional Embedding (RoPE)</td>\n<td>Minimal - core attention still the same</td>\n<td>High - complex trigonometric operations</td>\n</tr>\n<tr>\n<td>Layer-wise learning rates</td>\n<td>Moderate - optimization insight</td>\n<td>Medium - requires per-parameter learning rates</td>\n</tr>\n<tr>\n<td>Sparse attention patterns</td>\n<td>Moderate - attention efficiency</td>\n<td>High - custom attention masks and indexing</td>\n</tr>\n<tr>\n<td>Mixture of Experts</td>\n<td>Low - doesn&#39;t teach core transformer concepts</td>\n<td>Very High - routing and load balancing</td>\n</tr>\n<tr>\n<td>Advanced normalization (RMSNorm, etc.)</td>\n<td>Low - layer normalization concepts transfer</td>\n<td>Medium - different mathematical formulations</td>\n</tr>\n</tbody></table>\n<p>These features represent active areas of research and engineering optimization, but they build upon the fundamental attention mechanisms we&#39;ll implement. Understanding our basic transformer provides the foundation needed to comprehend these advanced techniques later.</p>\n<h4 id=\"production-engineering-requirements\">Production Engineering Requirements</h4>\n<p>Real-world transformer deployments require extensive engineering infrastructure that falls outside our educational scope. These systems are complex enough to be projects in their own right.</p>\n<table>\n<thead>\n<tr>\n<th>System Component</th>\n<th>Complexity</th>\n<th>Why We Skip</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Distributed training</td>\n<td>Very High</td>\n<td>Requires deep understanding of distributed systems</td>\n</tr>\n<tr>\n<td>Model serving infrastructure</td>\n<td>High</td>\n<td>Focus is on model internals, not deployment</td>\n</tr>\n<tr>\n<td>Efficient tokenization (BPE, SentencePiece)</td>\n<td>Medium</td>\n<td>Character-level tokenization sufficient for learning</td>\n</tr>\n<tr>\n<td>Checkpointing and model versioning</td>\n<td>Medium</td>\n<td>Standard deep learning infrastructure</td>\n</tr>\n<tr>\n<td>Production monitoring and logging</td>\n<td>Medium</td>\n<td>Standard MLOps practices</td>\n</tr>\n<tr>\n<td>ONNX/TensorRT optimization</td>\n<td>High</td>\n<td>Specialized inference optimization</td>\n</tr>\n</tbody></table>\n<p>Our <code>SimpleTokenizer</code> will use character-level tokenization rather than implementing subword algorithms like Byte Pair Encoding. While BPE provides better modeling efficiency, character tokenization eliminates the complexity of building and managing subword vocabularies, allowing us to focus on the transformer architecture itself.</p>\n<blockquote>\n<p><strong>Decision: Character-Level Tokenization</strong></p>\n<ul>\n<li><strong>Context</strong>: Need tokenization for text processing, with options ranging from word-level to subword to character-level</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Byte Pair Encoding (BPE) for efficiency</li>\n<li>Word-level tokenization for simplicity</li>\n<li>Character-level tokenization for minimal complexity</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Character-level tokenization via <code>SimpleTokenizer</code></li>\n<li><strong>Rationale</strong>: Character tokenization eliminates vocabulary management complexity and subword algorithm implementation, while still demonstrating the core concept of text-to-token conversion</li>\n<li><strong>Consequences</strong>: Less efficient token usage (longer sequences for same text), but dramatically simpler implementation and debugging</li>\n</ul>\n</blockquote>\n<h4 id=\"advanced-training-techniques\">Advanced Training Techniques</h4>\n<p>Modern transformer training employs sophisticated techniques for stability, efficiency, and performance. We&#39;ll use standard gradient descent optimization rather than implementing these advanced methods.</p>\n<table>\n<thead>\n<tr>\n<th>Training Technique</th>\n<th>Benefit</th>\n<th>Implementation Burden</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Learning rate scheduling (cosine, linear warmup)</td>\n<td>Better convergence</td>\n<td>Complex scheduling logic</td>\n</tr>\n<tr>\n<td>Gradient clipping with adaptive norms</td>\n<td>Training stability</td>\n<td>Gradient norm computation and clipping</td>\n</tr>\n<tr>\n<td>Loss scaling for mixed precision</td>\n<td>Numerical stability</td>\n<td>Dynamic loss scale adjustment</td>\n</tr>\n<tr>\n<td>Curriculum learning</td>\n<td>Faster convergence</td>\n<td>Sophisticated data ordering</td>\n</tr>\n<tr>\n<td>Regularization techniques (weight decay, dropout scheduling)</td>\n<td>Better generalization</td>\n<td>Multiple regularization implementations</td>\n</tr>\n</tbody></table>\n<p>Our training loop will use a fixed learning rate and standard dropout, providing a clear baseline for understanding how transformer training works. Once this foundation is solid, learners can experiment with advanced techniques as extensions.</p>\n<p>The scope we&#39;ve defined creates a <strong>complete yet manageable</strong> transformer implementation that demonstrates every essential concept while remaining achievable for a dedicated learner. Each component we include directly contributes to understanding attention mechanisms, autoregressive modeling, or neural language generation.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides the practical foundation for building our scoped transformer, with clear technology choices and file organization that support the educational goals we&#39;ve defined.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<p>Our technology stack prioritizes <strong>learning accessibility</strong> over cutting-edge performance, choosing tools that are widely available and well-documented rather than specialized frameworks that require expert knowledge.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Deep Learning Framework</td>\n<td>PyTorch with standard layers</td>\n<td>PyTorch with custom CUDA kernels</td>\n</tr>\n<tr>\n<td>Tokenization</td>\n<td>Character-level with Python strings</td>\n<td>SentencePiece or Hugging Face tokenizers</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Matplotlib for attention heatmaps</td>\n<td>TensorBoard or Weights &amp; Biases</td>\n</tr>\n<tr>\n<td>Data Loading</td>\n<td>PyTorch DataLoader with simple batching</td>\n<td>Custom data pipeline with preprocessing</td>\n</tr>\n<tr>\n<td>Model Serialization</td>\n<td>PyTorch state_dict save/load</td>\n<td>ONNX export for production deployment</td>\n</tr>\n<tr>\n<td>Development Environment</td>\n<td>Local GPU or Google Colab</td>\n<td>Multi-GPU cluster or cloud instances</td>\n</tr>\n</tbody></table>\n<p>The simple options provide everything needed for understanding transformers while keeping dependency management and setup straightforward. Advanced options offer better performance but require additional expertise that distracts from the core learning objectives.</p>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>Organizing code clearly from the start prevents the common mistake of implementing everything in a single monolithic file. This structure separates concerns while keeping related components together.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer-from-scratch/\n├── src/\n│   ├── __init__.py\n│   ├── config.py              # TransformerConfig and hyperparameters\n│   ├── tokenizer.py           # SimpleTokenizer implementation  \n│   ├── attention.py           # MultiHeadAttention and masking utilities\n│   ├── transformer.py         # TransformerBlock and full model\n│   ├── training.py            # Training loop and optimization\n│   ├── generation.py          # Text generation and sampling strategies\n│   └── utils.py               # Helper functions and debugging tools\n├── data/\n│   └── sample_text.txt        # Training data\n├── notebooks/\n│   ├── attention_visualization.ipynb  # Interactive attention analysis\n│   └── training_demo.ipynb    # Step-by-step training walkthrough\n├── tests/\n│   ├── test_attention.py      # Unit tests for attention mechanism\n│   ├── test_transformer.py    # Integration tests for full model\n│   └── test_generation.py     # Generation quality and correctness tests\n├── requirements.txt           # Python dependencies\n└── README.md                  # Setup and usage instructions</code></pre></div>\n\n<p>This structure scales well as the implementation grows, with clear separation between core components (<code>attention.py</code>, <code>transformer.py</code>) and supporting infrastructure (<code>training.py</code>, <code>utils.py</code>). The notebooks provide interactive environments for experimentation and visualization.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>These complete utility functions handle the foundational operations that support transformer implementation but aren&#39;t the primary learning focus. Copy these directly and build the core transformer components on top of them.</p>\n<p><strong>Configuration Management</strong> (<code>config.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete configuration for transformer model and training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Model architecture</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#6A737D\">              # embedding dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#6A737D\">              # number of attention heads  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#6A737D\">                   # key/query dimension per head</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#6A737D\">                   # value dimension per head</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#6A737D\">           # maximum sequence length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    vocab_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 128</span><span style=\"color:#6A737D\">           # vocabulary size (for character-level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">             # number of transformer blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_ff: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2048</span><span style=\"color:#6A737D\">               # feed-forward hidden dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">       # dropout probability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Training parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0001</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    device: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cuda\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"cpu\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration and set derived values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"d_model must be divisible by num_heads\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span></span></code></pre></div>\n\n<p><strong>Utility Functions</strong> (<code>utils.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> seaborn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> sns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(model: torch.nn.Module) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Count total trainable parameters in model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> p.requires_grad)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_causal_mask</span><span style=\"color:#E1E4E8\">(seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: torch.device) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create lower triangular causal attention mask.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create seq_length x seq_length matrix of ones</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use torch.tril to make it lower triangular  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert to boolean mask (1 = attend, 0 = mask)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Move to specified device</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> apply_causal_mask</span><span style=\"color:#E1E4E8\">(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Apply causal mask to attention scores before softmax.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Replace masked positions with large negative value (-1e9)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure mask broadcast correctly across batch and head dimensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_attention_weights</span><span style=\"color:#E1E4E8\">(weights: torch.Tensor, tokens: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, save_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Visualize attention weights as heatmap.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract specific layer and head from weights tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create matplotlib heatmap with token labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add title indicating layer and head number  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Save or display plot based on save_path parameter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>These signatures define the interfaces for the main transformer components that learners should implement themselves. Each TODO maps directly to the algorithm steps described in the design sections.</p>\n<p><strong>Multi-Head Attention</strong> (<code>attention.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultiHeadAttention</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-head self-attention with causal masking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize W_q, W_k, W_v projection matrices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize W_o output projection matrix  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store config values (d_model, num_heads, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize dropout layer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Forward pass through multi-head attention.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input embeddings (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Causal attention mask (seq_length, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Attended embeddings (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply Q, K, V projections to input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Reshape projections for multi-head computation  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute scaled dot-product attention scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply causal mask if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply softmax to get attention weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply dropout to attention weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute weighted sum of values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Concatenate heads and apply output projection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Transformer Block</strong> (<code>transformer.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerBlock</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single transformer block with attention and feed-forward layers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize MultiHeadAttention layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize feed-forward network (two linear layers with ReLU)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize layer normalization layers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize dropout layers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Forward pass through transformer block.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input embeddings (batch_size, seq_length, d_model)  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Causal attention mask</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Transformed embeddings (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply attention sub-layer with residual connection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply layer normalization after attention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply feed-forward sub-layer with residual connection  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply layer normalization after feed-forward</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing each major component, verify correct behavior with these concrete tests and expected outputs.</p>\n<p><strong>Milestone 1 Checkpoint - Self-Attention</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_attention.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected behavior:</p>\n<ul>\n<li>Attention weights sum to 1.0 across sequence dimension for each query position</li>\n<li>Causal mask prevents attention to future positions (weights are 0 for masked positions)  </li>\n<li>Multi-head outputs have correct dimensions after concatenation</li>\n<li>Gradient flows correctly through attention computation</li>\n</ul>\n<p><strong>Milestone 2 Checkpoint - Transformer Block</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> tests/test_transformer.py</span></span></code></pre></div>\n<p>Expected behavior:</p>\n<ul>\n<li>Input and output tensors have identical shapes (residual connections preserve dimensions)</li>\n<li>Layer normalization produces zero mean, unit variance activations</li>\n<li>Feed-forward network expands to 4x dimension internally then projects back</li>\n<li>Dropout is applied during training but not during evaluation</li>\n</ul>\n<p><strong>Training Verification</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Simple overfitting test - model should memorize small dataset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TransformerConfig(</span><span style=\"color:#FFAB70\">seq_length</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batch_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">num_layers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Train on 10 short sequences, loss should approach zero</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>PyTorch-Specific Guidance</strong>:</p>\n<ul>\n<li>Use <code>torch.nn.Linear</code> for all projection matrices rather than manual weight initialization</li>\n<li>Apply <code>torch.nn.functional.softmax(scores, dim=-1)</code> for attention weight normalization  </li>\n<li>Use <code>scores.masked_fill(mask == 0, -1e9)</code> for efficient causal masking</li>\n<li>Call <code>model.train()</code> before training loops and <code>model.eval()</code> before inference</li>\n<li>Use <code>torch.no_grad()</code> context manager during evaluation to disable gradient computation</li>\n</ul>\n<p><strong>Debugging Tips</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Attention weights all equal</td>\n<td>Missing causal mask or wrong softmax dimension</td>\n<td>Print attention matrix shape and values</td>\n<td>Apply mask before softmax, verify dim=-1</td>\n</tr>\n<tr>\n<td>Loss stays constant</td>\n<td>Wrong label shifting in data loader</td>\n<td>Check if targets are shifted by one position</td>\n<td>Shift targets: <code>targets = inputs[:, 1:]</code></td>\n</tr>\n<tr>\n<td>Memory errors during training</td>\n<td>Gradient accumulation without clearing</td>\n<td>Check if gradients are zeroed each step</td>\n<td>Call <code>optimizer.zero_grad()</code> before each batch</td>\n</tr>\n<tr>\n<td>Model generates repetitive text</td>\n<td>No sampling diversity</td>\n<td>Check if using greedy decoding only</td>\n<td>Implement temperature or top-k sampling</td>\n</tr>\n</tbody></table>\n<p>This implementation guidance provides the scaffolding needed to build the scoped transformer we&#39;ve defined, with clear separation between infrastructure code (copy directly) and core learning components (implement with provided skeletons).</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - understanding how transformer components compose together and organizing code for implementation</p>\n</blockquote>\n<p>Understanding the transformer architecture requires grasping how multiple sophisticated components work together to transform input text into coherent generated output. The key insight is that transformers operate as a <strong>hierarchical refinement system</strong> - starting with raw tokens, progressively building richer representations through multiple layers of attention and processing, and finally producing probability distributions over the vocabulary for text generation.</p>\n<h3 id=\"mental-model-the-literary-editor39s-process\">Mental Model: The Literary Editor&#39;s Process</h3>\n<p>Think of a transformer like a team of literary editors working on a manuscript. Each editor (transformer block) reads through the entire document, paying <strong>selective attention</strong> to different parts based on context, and makes refinements to improve clarity and coherence. The first editor might focus on basic grammar and word choice, while later editors handle more sophisticated aspects like narrative flow and thematic consistency. Each editor passes their refined version to the next, and the final editor produces suggestions for what word should come next to continue the story most naturally.</p>\n<p>Just as each editor has access to the entire manuscript but focuses on different aspects, each transformer block processes all tokens in the sequence but learns to attend to different patterns and relationships. The hierarchical structure allows the model to build increasingly sophisticated understanding - from simple token relationships in early layers to complex semantic and syntactic patterns in deeper layers.</p>\n<h3 id=\"component-hierarchy\">Component Hierarchy</h3>\n<p>The transformer system consists of four primary hierarchical layers, each with distinct responsibilities and data transformations. Understanding this hierarchy is crucial for implementation because it determines how information flows through the system and how gradients propagate during training.</p>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Fsystem-components.svg\" alt=\"Transformer System Components\"></p>\n<h4 id=\"input-processing-layer\">Input Processing Layer</h4>\n<p>The input processing layer converts raw text into numerical representations that the transformer can manipulate mathematically. This layer handles the critical transition from discrete symbolic information (text) to continuous vector representations.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Type</th>\n<th>Output Type</th>\n<th>Primary Responsibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SimpleTokenizer</code></td>\n<td>Raw text string</td>\n<td>List of token IDs</td>\n<td>Convert text to integer sequences</td>\n</tr>\n<tr>\n<td>Token Embedding</td>\n<td>Token ID integers</td>\n<td>Dense vectors (d_model)</td>\n<td>Map discrete tokens to continuous space</td>\n</tr>\n<tr>\n<td>Positional Encoding</td>\n<td>Sequence position</td>\n<td>Position vectors (d_model)</td>\n<td>Inject sequence order information</td>\n</tr>\n</tbody></table>\n<p>The tokenizer serves as the bridge between the human-readable text domain and the numerical computation domain. Character-level tokenization, while simpler to implement, creates longer sequences but ensures no out-of-vocabulary issues. The token embeddings transform these discrete identifiers into dense <code>d_model</code>-dimensional vectors that can encode rich semantic information through gradient-based learning.</p>\n<p>Positional encoding addresses a fundamental limitation of the attention mechanism - unlike recurrent networks, attention has no inherent notion of sequence order. Without positional information, the transformer would treat &quot;The cat sat on the mat&quot; identically to &quot;Mat the on sat cat the.&quot; The positional encoding adds learned or computed position-dependent vectors to the token embeddings, allowing the model to distinguish between tokens based on their sequence positions.</p>\n<h4 id=\"representation-learning-layer\">Representation Learning Layer</h4>\n<p>The representation learning layer contains the core transformer blocks that iteratively refine token representations through self-attention and feed-forward processing. This is where the model learns complex patterns and relationships in the data.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Shape</th>\n<th>Output Shape</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MultiHeadAttention</code></td>\n<td>(batch, seq_len, d_model)</td>\n<td>(batch, seq_len, d_model)</td>\n<td>Compute attention weights, aggregate values</td>\n</tr>\n<tr>\n<td>Feed-Forward Network</td>\n<td>(batch, seq_len, d_model)</td>\n<td>(batch, seq_len, d_model)</td>\n<td>Non-linear transformation with 4x expansion</td>\n</tr>\n<tr>\n<td>Layer Normalization</td>\n<td>(batch, seq_len, d_model)</td>\n<td>(batch, seq_len, d_model)</td>\n<td>Normalize activations per token</td>\n</tr>\n<tr>\n<td>Residual Connections</td>\n<td>Input + Sublayer Output</td>\n<td>(batch, seq_len, d_model)</td>\n<td>Preserve gradient flow</td>\n</tr>\n</tbody></table>\n<p>Each transformer block applies the same architectural pattern: multi-head self-attention followed by a feed-forward network, with layer normalization and residual connections around each sublayer. This design creates a powerful inductive bias for learning hierarchical representations while maintaining stable gradient flow during training.</p>\n<p>The multi-head attention mechanism allows each token to selectively focus on relevant parts of the sequence, learning different types of relationships in parallel across multiple attention heads. Some heads might learn syntactic relationships (subject-verb agreement), while others capture semantic associations (pronoun-antecedent relationships) or long-range dependencies (opening and closing of parenthetical statements).</p>\n<h4 id=\"generation-layer\">Generation Layer</h4>\n<p>The generation layer converts the final transformer block outputs into probability distributions over the vocabulary and implements sampling strategies for autoregressive text generation.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Shape</th>\n<th>Output Shape</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Output Projection</td>\n<td>(batch, seq_len, d_model)</td>\n<td>(batch, seq_len, vocab_size)</td>\n<td>Map representations to vocabulary logits</td>\n</tr>\n<tr>\n<td>Sampling Strategy</td>\n<td>Vocabulary logits</td>\n<td>Next token ID</td>\n<td>Select next token using various strategies</td>\n</tr>\n<tr>\n<td>Generation Loop</td>\n<td>Prompt tokens</td>\n<td>Generated sequence</td>\n<td>Orchestrate autoregressive generation</td>\n</tr>\n</tbody></table>\n<p>The output projection typically shares weights with the input token embedding matrix, creating a tied embedding structure that reduces parameters and often improves performance. The projection produces logits (unnormalized log probabilities) for each vocabulary token, which are then processed by sampling strategies to select the next token.</p>\n<p>The generation layer implements multiple sampling strategies to balance coherence and creativity. Greedy decoding always selects the most probable token, producing deterministic but potentially repetitive output. Temperature-based sampling introduces controlled randomness, while top-k and top-p (nucleus) sampling restrict the candidate set to maintain quality while preserving diversity.</p>\n<h4 id=\"system-integration-patterns\">System Integration Patterns</h4>\n<p>The components interact through well-defined interfaces that maintain tensor shape consistency and enable efficient computation. Understanding these interaction patterns is essential for debugging and extending the system.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The transformer&#39;s power comes from the combination of three key architectural innovations: (1) self-attention for flexible sequence modeling, (2) residual connections for deep network training, and (3) layer normalization for stable optimization. Each component is necessary but not sufficient - their combination creates emergent capabilities.</p>\n</blockquote>\n<p><strong>Decision: Decoder-Only Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to choose between encoder-decoder, encoder-only, or decoder-only transformer architectures for text generation</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Encoder-decoder (like original Transformer): Separate encoding and decoding stacks</li>\n<li>Encoder-only (like BERT): Bidirectional attention for representation learning</li>\n<li>Decoder-only (like GPT): Unidirectional attention for autoregressive generation</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement decoder-only architecture with causal masking</li>\n<li><strong>Rationale</strong>: Decoder-only architectures are simpler to implement, require fewer components, and have proven highly effective for text generation tasks. The causal masking naturally enforces the autoregressive constraint without additional architectural complexity.</li>\n<li><strong>Consequences</strong>: Enables straightforward text generation but cannot perform bidirectional tasks like fill-in-the-blank without modification. Simplifies implementation by eliminating encoder-decoder attention complexity.</li>\n</ul>\n<h3 id=\"recommended-code-organization\">Recommended Code Organization</h3>\n<p>Organizing transformer code requires balancing modularity, reusability, and clarity. The recommended structure separates concerns while maintaining clear dependencies and enabling incremental development through the milestones.</p>\n<h4 id=\"module-structure-overview\">Module Structure Overview</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer/\n├── __init__.py                 # Package initialization and public API\n├── config.py                   # Configuration dataclasses and constants\n├── tokenizer.py               # Text tokenization and vocabulary management\n├── embeddings.py              # Token and positional embeddings\n├── attention.py               # Self-attention mechanism implementation\n├── transformer.py             # Complete transformer model and blocks\n├── training.py                # Training loop, loss, and optimization\n├── generation.py              # Text generation and sampling strategies\n├── utils.py                   # Utility functions and debugging tools\n└── tests/                     # Unit tests for each component\n    ├── test_attention.py\n    ├── test_transformer.py\n    ├── test_training.py\n    └── test_generation.py</code></pre></div>\n\n<p>This organization follows the <strong>dependency layering principle</strong> - lower-level modules (config, tokenizer, embeddings) have no dependencies on higher-level modules, while higher-level modules (training, generation) can import from all lower levels. This structure prevents circular dependencies and enables clean unit testing.</p>\n<h4 id=\"configuration-management-strategy\">Configuration Management Strategy</h4>\n<p>All model hyperparameters and training configuration should be centralized in <code>config.py</code> to enable easy experimentation and reproducible results. The configuration system should support both programmatic usage and command-line parameter specification.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Class</th>\n<th>Responsibility</th>\n<th>Key Parameters</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TransformerConfig</code></td>\n<td>Model architecture parameters</td>\n<td><code>d_model</code>, <code>num_heads</code>, <code>seq_length</code>, <code>vocab_size</code></td>\n</tr>\n<tr>\n<td><code>TrainingConfig</code></td>\n<td>Training procedure parameters</td>\n<td>Learning rate, batch size, epochs, gradient clipping</td>\n</tr>\n<tr>\n<td><code>GenerationConfig</code></td>\n<td>Text generation parameters</td>\n<td>Temperature, top_k, top_p, max_length</td>\n</tr>\n</tbody></table>\n<p>The configuration system should validate parameter consistency (e.g., <code>d_model</code> must be divisible by <code>num_heads</code>) and provide sensible defaults for experimentation. Consider using dataclasses with type hints to enable automatic validation and IDE support.</p>\n<p><strong>Decision: Separate Configuration Classes</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to organize the numerous hyperparameters and configuration options for the transformer system</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Single monolithic config class with all parameters</li>\n<li>Separate classes for model, training, and generation configs</li>\n<li>Configuration file-based approach (YAML/JSON)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use separate dataclasses for different configuration domains</li>\n<li><strong>Rationale</strong>: Separating concerns makes it easier to reuse model configs with different training configs, enables cleaner testing, and reduces the likelihood of accidentally modifying unrelated parameters during experiments</li>\n<li><strong>Consequences</strong>: Slightly more complex to pass multiple config objects but much cleaner organization and better maintainability</li>\n</ul>\n<h4 id=\"component-interface-design\">Component Interface Design</h4>\n<p>Each module should expose clean, minimal interfaces that hide implementation complexity while providing necessary flexibility for experimentation and debugging. The interface design should follow PyTorch conventions for neural network modules.</p>\n<table>\n<thead>\n<tr>\n<th>Module</th>\n<th>Primary Classes</th>\n<th>Key Methods</th>\n<th>Public Interface Principle</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>attention.py</code></td>\n<td><code>MultiHeadAttention</code></td>\n<td><code>forward(x, mask)</code></td>\n<td>Standard PyTorch nn.Module interface</td>\n</tr>\n<tr>\n<td><code>transformer.py</code></td>\n<td><code>TransformerBlock</code>, <code>GPTModel</code></td>\n<td><code>forward(x)</code>, <code>generate(prompt)</code></td>\n<td>Composable building blocks</td>\n</tr>\n<tr>\n<td><code>tokenizer.py</code></td>\n<td><code>SimpleTokenizer</code></td>\n<td><code>encode(text)</code>, <code>decode(tokens)</code></td>\n<td>Stateless text processing</td>\n</tr>\n<tr>\n<td><code>training.py</code></td>\n<td><code>Trainer</code></td>\n<td><code>train_epoch(model, data)</code></td>\n<td>Encapsulate training logic</td>\n</tr>\n</tbody></table>\n<p>The interface design should prioritize <strong>composability</strong> - each component should work independently and combine naturally with others. For example, the <code>MultiHeadAttention</code> module should not know about transformer blocks, and the <code>TransformerBlock</code> should not know about the complete model architecture.</p>\n<h4 id=\"debugging-and-visualization-support\">Debugging and Visualization Support</h4>\n<p>Include debugging utilities from the beginning rather than adding them retroactively. Debugging transformer implementations requires specialized tools for visualizing attention patterns, monitoring gradient flow, and analyzing generation quality.</p>\n<table>\n<thead>\n<tr>\n<th>Utility Function</th>\n<th>Purpose</th>\n<th>Usage Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>plot_attention_weights(weights, tokens, layer, head)</code></td>\n<td>Visualize attention patterns</td>\n<td>Understanding learned relationships</td>\n</tr>\n<tr>\n<td><code>count_parameters(model)</code></td>\n<td>Monitor model size</td>\n<td>Architecture comparison</td>\n</tr>\n<tr>\n<td><code>check_gradient_flow(model)</code></td>\n<td>Detect vanishing/exploding gradients</td>\n<td>Training diagnosis</td>\n</tr>\n<tr>\n<td><code>analyze_generation_quality(samples)</code></td>\n<td>Measure generation metrics</td>\n<td>Generation evaluation</td>\n</tr>\n</tbody></table>\n<p>These utilities should be designed for interactive use in Jupyter notebooks as well as programmatic use in training scripts. Consider providing both high-level convenience functions and lower-level access to intermediate computations for advanced debugging.</p>\n<h4 id=\"testing-strategy-integration\">Testing Strategy Integration</h4>\n<p>The code organization should facilitate comprehensive testing at multiple levels. Unit tests should cover individual components, integration tests should verify component interactions, and end-to-end tests should validate complete workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Scope</th>\n<th>Example Test Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Tests</td>\n<td>Individual functions/classes</td>\n<td>Attention weight computation, mask application</td>\n</tr>\n<tr>\n<td>Integration Tests</td>\n<td>Component interactions</td>\n<td>Transformer block forward/backward pass</td>\n</tr>\n<tr>\n<td>End-to-End Tests</td>\n<td>Complete workflows</td>\n<td>Training convergence, generation quality</td>\n</tr>\n<tr>\n<td>Property Tests</td>\n<td>Invariant verification</td>\n<td>Shape consistency, gradient computation</td>\n</tr>\n</tbody></table>\n<p>Consider using property-based testing for verifying mathematical invariants like attention weight normalization and tensor shape consistency across different batch sizes and sequence lengths. These tests can catch subtle bugs that are difficult to find with example-based testing.</p>\n<h4 id=\"performance-optimization-considerations\">Performance Optimization Considerations</h4>\n<p>While the primary goal is educational clarity, the code organization should not preclude future performance optimizations. Design interfaces that can accommodate optimizations like KV caching, gradient checkpointing, and mixed precision training.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: Optimize for learning first, performance second. The code should be clear enough for educational purposes but structured to enable future optimizations without major refactoring.</p>\n</blockquote>\n<p><strong>Decision: PyTorch as Primary Framework</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose a deep learning framework that balances educational clarity with practical utility</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Pure NumPy implementation for maximum educational transparency</li>\n<li>PyTorch for its dynamic computation graphs and educational adoption</li>\n<li>TensorFlow/Keras for its high-level APIs and production readiness</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use PyTorch with clear separation between mathematical concepts and framework-specific details</li>\n<li><strong>Rationale</strong>: PyTorch&#39;s dynamic computation graphs make debugging easier, its nn.Module system provides clean abstractions for components, and it&#39;s widely adopted in educational settings. The framework handles automatic differentiation and GPU acceleration while remaining close to the mathematical formulation.</li>\n<li><strong>Consequences</strong>: Slightly steeper learning curve than pure NumPy but much more practical for experimentation and extension to larger models</li>\n</ul>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Tightly Coupled Components</strong>\nMany implementations create circular dependencies between components, making testing and debugging extremely difficult. For example, embedding the tokenizer logic directly in the model class or having the attention mechanism know about the complete transformer architecture. This makes it impossible to test individual components in isolation and creates fragile code that breaks when any component changes. <strong>Fix</strong>: Design each component with a single responsibility and clean interfaces. The attention mechanism should only know about queries, keys, and values - not about transformer blocks or models.</p>\n<p>⚠️ <strong>Pitfall: Configuration Parameter Scattering</strong>\nHardcoding hyperparameters throughout the codebase or passing individual parameters to every function makes experimentation and debugging nearly impossible. When <code>d_model=512</code> is scattered across ten different files, changing the model size becomes an error-prone search-and-replace operation. <strong>Fix</strong>: Centralize all configuration in a single module and pass config objects to components. Use type hints and validation to catch configuration errors early.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Tensor Shape Conventions</strong>\nMixing different tensor shape conventions (batch-first vs sequence-first, or inconsistent dimension ordering) causes subtle bugs that are difficult to track down. These bugs often manifest as training instability or incorrect attention patterns rather than obvious errors. <strong>Fix</strong>: Establish consistent shape conventions from the beginning and document them clearly. Use shape assertions in forward passes during development to catch dimension mismatches immediately.</p>\n<p>⚠️ <strong>Pitfall: Missing Intermediate Value Access</strong>\nImplementing components as black boxes without access to intermediate computations makes debugging and analysis impossible. When attention isn&#39;t working correctly, you need to inspect attention weights, but if they&#39;re not accessible from outside the component, debugging becomes guesswork. <strong>Fix</strong>: Design components to optionally return intermediate values for debugging and visualization. Consider using hooks or debug modes that expose internal states.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Error Handling</strong>\nTransformer implementations often fail silently when given invalid inputs, making it difficult to identify problems during development. For example, sequence lengths exceeding the model&#39;s maximum or attention masks with incorrect shapes might cause training instability rather than clear error messages. <strong>Fix</strong>: Add comprehensive input validation with clear error messages. Check tensor shapes, value ranges, and consistency constraints at component boundaries.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Deep Learning Framework</td>\n<td>PyTorch with nn.Module</td>\n<td>PyTorch with custom CUDA kernels</td>\n<td>PyTorch provides excellent educational clarity and debugging capabilities</td>\n</tr>\n<tr>\n<td>Tokenization</td>\n<td>Character-level with Python dict</td>\n<td>SentencePiece or Hugging Face tokenizers</td>\n<td>Character-level is simpler to implement and debug for learning purposes</td>\n</tr>\n<tr>\n<td>Data Loading</td>\n<td>Simple list-based batching</td>\n<td>PyTorch DataLoader with custom collate</td>\n<td>Start simple for understanding, upgrade for performance</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Matplotlib for attention heatmaps</td>\n<td>Weights &amp; Biases or TensorBoard</td>\n<td>Matplotlib provides immediate feedback during development</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>pytest with parametrize</td>\n<td>pytest with hypothesis property testing</td>\n<td>Parametrized tests cover multiple configurations efficiently</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer_project/\n├── transformer/                # Main package\n│   ├── __init__.py\n│   ├── config.py              # TransformerConfig, TrainingConfig\n│   ├── tokenizer.py           # SimpleTokenizer implementation\n│   ├── embeddings.py          # Token and positional embeddings\n│   ├── attention.py           # MultiHeadAttention core logic\n│   ├── transformer.py         # TransformerBlock, GPTModel\n│   ├── training.py            # Trainer class, loss computation\n│   ├── generation.py          # Sampling strategies, generation loop\n│   └── utils.py               # count_parameters, visualization\n├── tests/                     # Comprehensive test suite\n│   ├── __init__.py\n│   ├── test_attention.py      # Attention mechanism tests\n│   ├── test_transformer.py    # Block and model tests\n│   ├── test_training.py       # Training pipeline tests\n│   ├── test_generation.py     # Generation quality tests\n│   └── conftest.py            # Shared test fixtures\n├── experiments/               # Training scripts and notebooks\n│   ├── train_model.py         # Main training script\n│   ├── generate_text.py       # Text generation script\n│   └── analysis.ipynb         # Interactive analysis notebook\n├── data/                      # Training data and outputs\n│   ├── text_data.txt          # Training corpus\n│   └── models/                # Saved model checkpoints\n├── requirements.txt           # Project dependencies\n└── README.md                  # Project documentation</code></pre></div>\n\n<p>This structure supports incremental development - you can implement and test each component independently before integrating them into the complete system.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Configuration Management</strong> (config.py):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for transformer model architecture.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#6A737D\">              # Model embedding dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#6A737D\">              # Number of attention heads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">             # Number of transformer blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_ff: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2048</span><span style=\"color:#6A737D\">               # Feed-forward hidden dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\">         # Maximum sequence length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    vocab_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50000</span><span style=\"color:#6A737D\">        # Vocabulary size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">      # Dropout probability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration consistency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"d_model (</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.d_model</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) must be divisible by num_heads (</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.num_heads</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Derived parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads  </span><span style=\"color:#6A737D\"># Key/query dimension per head</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads  </span><span style=\"color:#6A737D\"># Value dimension per head</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for model training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_clip_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    device: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cuda\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"cpu\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GenerationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for text generation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_k: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 50</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_p: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    do_sample: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>Utility Functions</strong> (utils.py):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> seaborn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> sns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(model: torch.nn.Module) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Count trainable parameters in model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> p.requires_grad)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_causal_mask</span><span style=\"color:#E1E4E8\">(seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: torch.device) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create lower triangular causal mask for autoregressive attention.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.tril(torch.ones(seq_length, seq_length, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">device))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> mask.bool()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_attention_weights</span><span style=\"color:#E1E4E8\">(weights: torch.Tensor, tokens: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, save_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Visualize attention weights as heatmap.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sns.heatmap(weights.detach().cpu().numpy(), </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                xticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens, </span><span style=\"color:#FFAB70\">yticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Blues'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cbar</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.title(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'Attention Weights - Layer </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">layer</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Head </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">head</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.xlabel(</span><span style=\"color:#9ECBFF\">'Key Position'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.ylabel(</span><span style=\"color:#9ECBFF\">'Query Position'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> save_path:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.savefig(save_path, </span><span style=\"color:#FFAB70\">dpi</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">bbox_inches</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'tight'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.show()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> apply_causal_mask</span><span style=\"color:#E1E4E8\">(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Apply causal mask to attention scores before softmax.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> scores.masked_fill(</span><span style=\"color:#F97583\">~</span><span style=\"color:#E1E4E8\">mask, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">))</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Multi-Head Attention</strong> (attention.py):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> create_causal_mask, apply_causal_mask</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultiHeadAttention</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-head self-attention mechanism with causal masking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize query, key, value projection matrices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use nn.Linear(d_model, d_model) for each projection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize output projection matrix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize dropout layer with config.dropout_rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Register causal mask as buffer (doesn't require gradients)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: torch.Tensor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Apply multi-head self-attention to input sequence.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Optional attention mask</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor of same shape as input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch_size, seq_length, d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Project input to queries, keys, values using linear layers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Reshape Q, K, V to (batch, num_heads, seq_len, d_k)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use .view() and .transpose() to rearrange dimensions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute scaled dot-product attention scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # scores = Q @ K^T / sqrt(d_k)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply causal mask to prevent attending to future positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use apply_causal_mask utility function</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply softmax to get attention weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply dropout to attention weights (during training only)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Compute weighted sum of values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # attention_output = attention_weights @ V</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Concatenate heads and apply output projection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Reshape back to (batch, seq_len, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this when implementing</span></span></code></pre></div>\n\n<p><strong>Transformer Block</strong> (transformer.py):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .attention </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MultiHeadAttention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TransformerConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerBlock</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single transformer block with attention and feed-forward layers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize multi-head attention layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize first layer normalization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize feed-forward network (two linear layers with ReLU)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # First layer: d_model -> d_ff, Second layer: d_ff -> d_model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize second layer normalization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Initialize dropout layers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Apply transformer block processing to input.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor of same shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply first sub-layer (multi-head attention with residual connection)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use pre-normalization: LayerNorm -> Attention -> Dropout -> Residual</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply second sub-layer (feed-forward with residual connection)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use pre-normalization: LayerNorm -> FFN -> Dropout -> Residual</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove this when implementing</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>PyTorch-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>torch.nn.functional.scaled_dot_product_attention</code> for optimized attention computation (PyTorch 2.0+)</li>\n<li>Register masks and positional encodings as buffers with <code>self.register_buffer()</code> to automatically handle device placement</li>\n<li>Use <code>torch.nn.utils.clip_grad_norm_()</code> for gradient clipping during training</li>\n<li>Enable mixed precision training with <code>torch.cuda.amp.GradScaler</code> for faster training on modern GPUs</li>\n<li>Use <code>model.eval()</code> vs <code>model.train()</code> to control dropout behavior during inference vs training</li>\n</ul>\n<p><strong>Memory and Performance Optimization:</strong></p>\n<ul>\n<li>Use <code>torch.no_grad()</code> context manager during inference to reduce memory usage</li>\n<li>Implement gradient checkpointing with <code>torch.utils.checkpoint.checkpoint()</code> for training larger models</li>\n<li>Consider using <code>torch.compile()</code> (PyTorch 2.0+) for automatic optimization of the forward pass</li>\n<li>Use appropriate data types: <code>torch.float16</code> for mixed precision, <code>torch.int64</code> for token IDs</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing attention mechanism (Milestone 1):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_attention.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from transformer.attention import MultiHeadAttention</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from transformer.config import TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import torch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">config = TransformerConfig(d_model=512, num_heads=8, seq_length=128)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">attention = MultiHeadAttention(config)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">x = torch.randn(2, 64, 512)  # batch_size=2, seq_length=64</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">output = attention(x)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Input shape: {x.shape}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Output shape: {output.shape}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ Attention mechanism working correctly')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Expected behavior:</strong> Attention layer should process input tensors without shape errors, produce output of identical shape to input, and generate reasonable attention weight patterns when visualized.</p>\n<p><strong>After implementing transformer block (Milestone 2):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from transformer.transformer import TransformerBlock</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from transformer.config import TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import torch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">config = TransformerConfig()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">block = TransformerBlock(config)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">x = torch.randn(4, 128, 512)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">output = block(x)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Parameters: {sum(p.numel() for p in block.parameters()):,}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Output statistics: mean={output.mean():.3f}, std={output.std():.3f}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ Transformer block implemented correctly')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Expected behavior:</strong> Transformer block should maintain reasonable output statistics (mean near 0, std around 1) due to layer normalization, and gradients should flow cleanly through residual connections.</p>\n<h2 id=\"data-model-and-types\">Data Model and Types</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - data structures and types that support attention mechanism (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)</p>\n</blockquote>\n<p>The foundation of any transformer implementation lies in carefully designed data structures and type definitions that capture the mathematical relationships between embeddings, attention mechanisms, and generation parameters. Getting these definitions right from the start prevents subtle bugs and makes the codebase maintainable as we build increasingly complex components.</p>\n<p>Think of the data model as the architectural blueprint for a skyscraper - every beam, joint, and connection must be precisely specified before construction begins. Just as structural engineers define load-bearing capacities and material specifications, we must define tensor dimensions, configuration parameters, and their relationships to ensure our transformer can handle the computational loads of attention mechanisms and gradient flows.</p>\n<p>The complexity of transformers stems from their multi-dimensional nature: sequences of tokens become matrices of embeddings, which are processed by multi-head attention across multiple layers. Each transformation must preserve certain dimensional invariants while allowing information to flow freely between positions. This requires a careful balance between flexibility for different model sizes and rigidity to prevent dimension mismatches that would cause runtime failures.</p>\n<blockquote>\n<p><strong>Critical Insight:</strong> The transformer&#39;s power comes from its ability to process sequences in parallel while maintaining positional relationships. This parallelism demands that all tensor dimensions are known at model construction time, making careful type design essential for both correctness and performance.</p>\n</blockquote>\n<h3 id=\"tensor-dimensions\">Tensor Dimensions</h3>\n<p>The transformer operates on multi-dimensional tensors with specific shape relationships that must be maintained throughout the forward pass. Understanding these dimensional constraints is crucial for implementing attention mechanisms and avoiding the most common source of transformer bugs: tensor shape mismatches.</p>\n<p><strong>Mental Model: The Assembly Line</strong></p>\n<p>Imagine a transformer as a sophisticated assembly line where each workstation (layer) processes batches of products (token sequences) in parallel. Each product has specific dimensions - length, width, height - that determine which machines it can pass through. Just as an assembly line would break if we tried to force a 10-foot beam through a 5-foot opening, our transformer will crash if we try to multiply tensors with incompatible dimensions.</p>\n<p>The key insight is that while the batch size and sequence length can vary dynamically, the embedding dimension (<code>d_model</code>) acts as a universal &quot;width&quot; that must match at every layer. This allows us to stack transformer blocks like standardized assembly line segments, each expecting inputs and producing outputs of the same embedding dimension.</p>\n<p>The fundamental tensor shapes flow through the transformer in a predictable pattern:</p>\n<table>\n<thead>\n<tr>\n<th>Tensor Name</th>\n<th>Shape</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Tokens</td>\n<td><code>[batch_size, seq_length]</code></td>\n<td>Integer token IDs before embedding</td>\n</tr>\n<tr>\n<td>Token Embeddings</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Dense vector representations of tokens</td>\n</tr>\n<tr>\n<td>Query Matrix</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Attention queries for all positions</td>\n</tr>\n<tr>\n<td>Key Matrix</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Attention keys for all positions</td>\n</tr>\n<tr>\n<td>Value Matrix</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Attention values for all positions</td>\n</tr>\n<tr>\n<td>Attention Scores</td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td>Raw attention weights before masking</td>\n</tr>\n<tr>\n<td>Attention Weights</td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td>Normalized attention weights after softmax</td>\n</tr>\n<tr>\n<td>Head Output</td>\n<td><code>[batch_size, seq_length, d_k]</code></td>\n<td>Single attention head output</td>\n</tr>\n<tr>\n<td>Multi-Head Output</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Concatenated and projected head outputs</td>\n</tr>\n<tr>\n<td>FFN Hidden</td>\n<td><code>[batch_size, seq_length, 4 * d_model]</code></td>\n<td>Feed-forward network intermediate layer</td>\n</tr>\n<tr>\n<td>Final Logits</td>\n<td><code>[batch_size, seq_length, vocab_size]</code></td>\n<td>Unnormalized probabilities over vocabulary</td>\n</tr>\n</tbody></table>\n<p>The attention mechanism introduces additional dimensional complexity through multi-head computation. Each attention head operates on a smaller slice of the embedding dimension to allow parallel processing of different representation subspaces:</p>\n<table>\n<thead>\n<tr>\n<th>Dimension Name</th>\n<th>Formula</th>\n<th>Typical Value</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>d_model</code></td>\n<td>Model hyperparameter</td>\n<td>512, 768, 1024</td>\n<td>Primary embedding dimension</td>\n</tr>\n<tr>\n<td><code>d_k</code></td>\n<td><code>d_model // num_heads</code></td>\n<td>64, 96, 128</td>\n<td>Key/query dimension per head</td>\n</tr>\n<tr>\n<td><code>d_v</code></td>\n<td><code>d_model // num_heads</code></td>\n<td>64, 96, 128</td>\n<td>Value dimension per head</td>\n</tr>\n<tr>\n<td><code>num_heads</code></td>\n<td>Model hyperparameter</td>\n<td>8, 12, 16</td>\n<td>Number of parallel attention heads</td>\n</tr>\n<tr>\n<td><code>seq_length</code></td>\n<td>Input dependent</td>\n<td>128, 512, 2048</td>\n<td>Maximum sequence length</td>\n</tr>\n<tr>\n<td><code>vocab_size</code></td>\n<td>Dataset dependent</td>\n<td>10000, 30000, 50000</td>\n<td>Number of unique tokens</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Equal Head Dimensions</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to divide the embedding dimension across multiple attention heads</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Equal dimensions per head (<code>d_k = d_v = d_model // num_heads</code>)</li>\n<li>Varying head dimensions with learned allocation</li>\n<li>Fixed small head dimensions independent of <code>d_model</code></li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use equal dimensions per head with <code>d_k = d_v = d_model // num_heads</code></li>\n<li><strong>Rationale</strong>: This ensures the concatenated head outputs exactly reconstruct the original embedding dimension without requiring additional padding or projection. It also simplifies the math and aligns with the original Transformer paper.</li>\n<li><strong>Consequences</strong>: Requires <code>d_model</code> to be divisible by <code>num_heads</code>, but provides clean dimensional relationships and efficient computation.</li>\n</ul>\n</blockquote>\n<p>The dimensional relationships create important constraints that must be validated at model construction time:</p>\n<table>\n<thead>\n<tr>\n<th>Constraint</th>\n<th>Mathematical Relationship</th>\n<th>Validation Check</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Head Division</td>\n<td><code>d_model % num_heads == 0</code></td>\n<td>Embedding dimension evenly divisible</td>\n</tr>\n<tr>\n<td>Head Reconstruction</td>\n<td><code>num_heads * d_k == d_model</code></td>\n<td>Concatenated heads match embedding size</td>\n</tr>\n<tr>\n<td>Sequence Bounds</td>\n<td><code>input_length &lt;= seq_length</code></td>\n<td>Input fits within position embeddings</td>\n</tr>\n<tr>\n<td>Vocabulary Bounds</td>\n<td><code>max(token_ids) &lt; vocab_size</code></td>\n<td>All tokens have valid embeddings</td>\n</tr>\n<tr>\n<td>Batch Consistency</td>\n<td>All sequences in batch have same length (after padding)</td>\n<td>Efficient tensor operations</td>\n</tr>\n</tbody></table>\n<p>The careful design of these dimensional relationships enables the transformer&#39;s key capability: processing variable-length sequences in fixed-size tensor operations. During training, sequences are padded to a common length within each batch, while during generation, we can process sequences of any length up to <code>seq_length</code> by simply slicing the appropriate tensor dimensions.</p>\n<h3 id=\"configuration-structures\">Configuration Structures</h3>\n<p>Configuration management in transformers requires balancing flexibility with type safety. We need structures that can express the full range of model architectures (from small experimental models to large production systems) while preventing invalid combinations of hyperparameters that would cause training failures or numerical instabilities.</p>\n<p><strong>Mental Model: The Recipe Card</strong></p>\n<p>Think of configuration structures as detailed recipe cards for baking a transformer. Just as a recipe specifies ingredients (model dimensions), cooking methods (training procedures), and serving suggestions (generation parameters), our configurations encode all the decisions needed to create a working model. Like a professional baker who maintains separate recipe cards for different occasions, we separate model architecture, training procedures, and generation behavior into distinct configuration structures.</p>\n<p>The separation prevents common mistakes like accidentally using training-specific parameters during inference or mixing up model dimensions when switching between architectures. Each configuration structure has a single responsibility and clear validation rules.</p>\n<h4 id=\"model-architecture-configuration\">Model Architecture Configuration</h4>\n<p>The <code>TransformerConfig</code> structure captures all architectural decisions that determine the model&#39;s capacity and computational requirements. These parameters are typically set once during model design and remain fixed throughout training and deployment:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Typical Values</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>d_model</code></td>\n<td><code>int</code></td>\n<td>Primary embedding dimension for all layers</td>\n<td>512, 768, 1024, 1536</td>\n</tr>\n<tr>\n<td><code>num_heads</code></td>\n<td><code>int</code></td>\n<td>Number of parallel attention heads per layer</td>\n<td>8, 12, 16, 24</td>\n</tr>\n<tr>\n<td><code>d_k</code></td>\n<td><code>int</code></td>\n<td>Key and query dimension per attention head</td>\n<td>64, 96, 128 (computed as d_model // num_heads)</td>\n</tr>\n<tr>\n<td><code>d_v</code></td>\n<td><code>int</code></td>\n<td>Value dimension per attention head</td>\n<td>64, 96, 128 (typically equals d_k)</td>\n</tr>\n<tr>\n<td><code>seq_length</code></td>\n<td><code>int</code></td>\n<td>Maximum sequence length for positional embeddings</td>\n<td>128, 512, 1024, 2048</td>\n</tr>\n<tr>\n<td><code>vocab_size</code></td>\n<td><code>int</code></td>\n<td>Size of token vocabulary for embeddings and output projection</td>\n<td>10000, 30000, 50000</td>\n</tr>\n<tr>\n<td><code>num_layers</code></td>\n<td><code>int</code></td>\n<td>Number of transformer blocks in the stack</td>\n<td>6, 12, 24, 48</td>\n</tr>\n<tr>\n<td><code>dropout_rate</code></td>\n<td><code>float</code></td>\n<td>Dropout probability for regularization (0.0 to 1.0)</td>\n<td>0.1, 0.2, 0.3</td>\n</tr>\n<tr>\n<td><code>ffn_expansion</code></td>\n<td><code>int</code></td>\n<td>Multiplier for feed-forward network hidden dimension</td>\n<td>4 (gives 4 * d_model)</td>\n</tr>\n</tbody></table>\n<p>The <code>TransformerConfig</code> includes built-in validation to catch common configuration errors before they cause cryptic runtime failures:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Check</th>\n<th>Error Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Head Division</td>\n<td><code>d_model % num_heads == 0</code></td>\n<td>&quot;d_model must be divisible by num_heads&quot;</td>\n</tr>\n<tr>\n<td>Positive Dimensions</td>\n<td><code>d_model &gt; 0 and num_heads &gt; 0</code></td>\n<td>&quot;All dimensions must be positive&quot;</td>\n</tr>\n<tr>\n<td>Dropout Range</td>\n<td><code>0.0 &lt;= dropout_rate &lt;= 1.0</code></td>\n<td>&quot;Dropout rate must be between 0.0 and 1.0&quot;</td>\n</tr>\n<tr>\n<td>Sequence Length</td>\n<td><code>seq_length &gt;= 1</code></td>\n<td>&quot;Sequence length must be at least 1&quot;</td>\n</tr>\n<tr>\n<td>Vocabulary Size</td>\n<td><code>vocab_size &gt;= 2</code></td>\n<td>&quot;Vocabulary must contain at least 2 tokens&quot;</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Computed vs Explicit Head Dimensions</strong></p>\n<ul>\n<li><strong>Context</strong>: We could either compute <code>d_k</code> and <code>d_v</code> from <code>d_model</code> and <code>num_heads</code>, or require them to be specified explicitly</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Always compute <code>d_k = d_v = d_model // num_heads</code> automatically</li>\n<li>Require explicit specification but validate consistency</li>\n<li>Allow arbitrary head dimensions independent of <code>d_model</code></li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Require explicit specification with validation that <code>d_k * num_heads == d_model</code></li>\n<li><strong>Rationale</strong>: Makes the dimensional relationships explicit in configuration files, easier to debug when dimensions don&#39;t align, and allows for future flexibility if we want to experiment with varying head sizes</li>\n<li><strong>Consequences</strong>: Slightly more verbose configuration files, but much clearer error messages when dimensions are mismatched</li>\n</ul>\n</blockquote>\n<h4 id=\"training-configuration\">Training Configuration</h4>\n<p>The <code>TrainingConfig</code> structure encapsulates all hyperparameters that control the optimization process. These parameters can be adjusted between training runs without changing the model architecture:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Typical Values</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>learning_rate</code></td>\n<td><code>float</code></td>\n<td>Initial learning rate for optimizer</td>\n<td>1e-3, 5e-4, 1e-4, 3e-5</td>\n</tr>\n<tr>\n<td><code>batch_size</code></td>\n<td><code>int</code></td>\n<td>Number of sequences processed in parallel</td>\n<td>16, 32, 64, 128</td>\n</tr>\n<tr>\n<td><code>num_epochs</code></td>\n<td><code>int</code></td>\n<td>Number of complete passes through training data</td>\n<td>10, 50, 100, 200</td>\n</tr>\n<tr>\n<td><code>gradient_clip_norm</code></td>\n<td><code>float</code></td>\n<td>Maximum L2 norm for gradient clipping</td>\n<td>1.0, 0.5, 0.25</td>\n</tr>\n<tr>\n<td><code>weight_decay</code></td>\n<td><code>float</code></td>\n<td>L2 regularization coefficient</td>\n<td>0.01, 0.001, 0.0001</td>\n</tr>\n<tr>\n<td><code>warmup_steps</code></td>\n<td><code>int</code></td>\n<td>Number of steps for learning rate warmup</td>\n<td>1000, 4000, 10000</td>\n</tr>\n<tr>\n<td><code>save_interval</code></td>\n<td><code>int</code></td>\n<td>Save checkpoint every N epochs</td>\n<td>5, 10, 25</td>\n</tr>\n<tr>\n<td><code>log_interval</code></td>\n<td><code>int</code></td>\n<td>Log metrics every N training steps</td>\n<td>100, 500, 1000</td>\n</tr>\n<tr>\n<td><code>eval_interval</code></td>\n<td><code>int</code></td>\n<td>Run validation every N training steps</td>\n<td>1000, 2500, 5000</td>\n</tr>\n</tbody></table>\n<p>The training configuration includes validation to prevent common training mistakes:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Check</th>\n<th>Error Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Positive Learning Rate</td>\n<td><code>learning_rate &gt; 0.0</code></td>\n<td>&quot;Learning rate must be positive&quot;</td>\n</tr>\n<tr>\n<td>Reasonable Batch Size</td>\n<td><code>1 &lt;= batch_size &lt;= 1024</code></td>\n<td>&quot;Batch size must be between 1 and 1024&quot;</td>\n</tr>\n<tr>\n<td>Gradient Clipping</td>\n<td><code>gradient_clip_norm &gt; 0.0</code></td>\n<td>&quot;Gradient clip norm must be positive&quot;</td>\n</tr>\n<tr>\n<td>Weight Decay Range</td>\n<td><code>0.0 &lt;= weight_decay &lt;= 1.0</code></td>\n<td>&quot;Weight decay must be between 0.0 and 1.0&quot;</td>\n</tr>\n<tr>\n<td>Interval Consistency</td>\n<td><code>log_interval &lt;= eval_interval</code></td>\n<td>&quot;Log interval should not exceed evaluation interval&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"generation-configuration\">Generation Configuration</h4>\n<p>The <code>GenerationConfig</code> structure controls the behavior of autoregressive text generation. These parameters can be adjusted at inference time to produce different styles of generated text:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Typical Values</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>temperature</code></td>\n<td><code>float</code></td>\n<td>Scaling factor for output logits (higher = more random)</td>\n<td>0.7, 1.0, 1.2, 1.5</td>\n</tr>\n<tr>\n<td><code>top_k</code></td>\n<td><code>int</code></td>\n<td>Number of highest-probability tokens to consider</td>\n<td>0 (disabled), 10, 50, 100</td>\n</tr>\n<tr>\n<td><code>top_p</code></td>\n<td><code>float</code></td>\n<td>Cumulative probability threshold for nucleus sampling</td>\n<td>0.0 (disabled), 0.9, 0.95, 0.99</td>\n</tr>\n<tr>\n<td><code>max_length</code></td>\n<td><code>int</code></td>\n<td>Maximum number of tokens to generate</td>\n<td>50, 100, 200, 500</td>\n</tr>\n<tr>\n<td><code>pad_token_id</code></td>\n<td><code>int</code></td>\n<td>Token ID used for padding sequences</td>\n<td>0, vocab_size - 1</td>\n</tr>\n<tr>\n<td><code>eos_token_id</code></td>\n<td><code>int</code></td>\n<td>Token ID that signals end of sequence</td>\n<td>1, vocab_size - 2</td>\n</tr>\n<tr>\n<td><code>repetition_penalty</code></td>\n<td><code>float</code></td>\n<td>Penalty applied to recently generated tokens</td>\n<td>1.0 (disabled), 1.1, 1.2</td>\n</tr>\n<tr>\n<td><code>length_penalty</code></td>\n<td><code>float</code></td>\n<td>Penalty applied to longer sequences</td>\n<td>1.0 (disabled), 0.8, 1.2</td>\n</tr>\n</tbody></table>\n<p>Generation configuration validation focuses on preventing parameter combinations that would produce degenerate text:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Check</th>\n<th>Error Message</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Temperature Range</td>\n<td><code>temperature &gt; 0.0</code></td>\n<td>&quot;Temperature must be positive (use small values like 0.01 instead of 0.0)&quot;</td>\n</tr>\n<tr>\n<td>Top-k Bounds</td>\n<td><code>top_k &gt;= 0</code></td>\n<td>&quot;Top-k must be non-negative (0 disables top-k sampling)&quot;</td>\n</tr>\n<tr>\n<td>Top-p Range</td>\n<td><code>0.0 &lt;= top_p &lt;= 1.0</code></td>\n<td>&quot;Top-p must be between 0.0 and 1.0&quot;</td>\n</tr>\n<tr>\n<td>Length Bounds</td>\n<td><code>max_length &gt; 0 and max_length &lt;= seq_length</code></td>\n<td>&quot;Max length must be positive and within model sequence limit&quot;</td>\n</tr>\n<tr>\n<td>Token ID Validity</td>\n<td><code>pad_token_id &lt; vocab_size and eos_token_id &lt; vocab_size</code></td>\n<td>&quot;Special token IDs must be within vocabulary&quot;</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Temperature Zero Handling</strong></p>\n<ul>\n<li><strong>Context</strong>: Setting temperature to 0.0 would cause division by zero in the softmax temperature scaling</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Allow temperature = 0.0 and implement special case for greedy decoding</li>\n<li>Require temperature &gt; 0.0 and suggest small values like 0.01 for near-greedy behavior</li>\n<li>Automatically convert temperature = 0.0 to greedy decoding mode</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Require temperature &gt; 0.0 with clear error message suggesting alternatives</li>\n<li><strong>Rationale</strong>: Explicit is better than implicit - users should consciously choose between deterministic and probabilistic sampling rather than triggering special cases accidentally</li>\n<li><strong>Consequences</strong>: Users must set temperature to small positive values like 0.01 for nearly deterministic generation, but this makes the sampling behavior explicit</li>\n</ul>\n</blockquote>\n<h4 id=\"configuration-relationships-and-dependencies\">Configuration Relationships and Dependencies</h4>\n<p>The three configuration structures have important relationships that must be maintained for correct operation:</p>\n<table>\n<thead>\n<tr>\n<th>Relationship</th>\n<th>Description</th>\n<th>Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequence Length Consistency</td>\n<td>Training sequences must fit within model&#39;s maximum</td>\n<td><code>training_seq_len &lt;= model.seq_length</code></td>\n</tr>\n<tr>\n<td>Batch Size Memory Scaling</td>\n<td>Larger batches with longer sequences require more GPU memory</td>\n<td><code>batch_size * seq_length^2 &lt; memory_limit</code> (approximate)</td>\n</tr>\n<tr>\n<td>Generation Length Bounds</td>\n<td>Generated sequences cannot exceed model capacity</td>\n<td><code>generation.max_length &lt;= model.seq_length</code></td>\n</tr>\n<tr>\n<td>Vocabulary Consistency</td>\n<td>Special tokens must exist in model vocabulary</td>\n<td><code>generation.eos_token_id &lt; model.vocab_size</code></td>\n</tr>\n<tr>\n<td>Learning Rate Scaling</td>\n<td>Larger batch sizes typically require higher learning rates</td>\n<td>No automatic validation, but worth noting</td>\n</tr>\n</tbody></table>\n<p>The configuration design supports common workflows like hyperparameter sweeps and model scaling:</p>\n<ol>\n<li><strong>Hyperparameter Sweeps</strong>: Training configuration can be varied independently while keeping model architecture fixed</li>\n<li><strong>Model Scaling</strong>: Architecture configuration supports scaling from small experimental models to large production systems</li>\n<li><strong>Generation Tuning</strong>: Generation parameters can be adjusted at inference time without reloading the model</li>\n<li><strong>Distributed Training</strong>: Batch size and learning rate can be scaled together for multi-GPU training</li>\n</ol>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data model implementation requires careful attention to type safety, validation, and integration with deep learning frameworks. We&#39;ll use Python with PyTorch as our primary implementation language, leveraging dataclasses for clean configuration management and PyTorch&#39;s tensor operations for efficient computation.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Management</td>\n<td>Python dataclasses with basic validation</td>\n<td>Pydantic with automatic validation and JSON schema</td>\n</tr>\n<tr>\n<td>Type Hints</td>\n<td>Basic Python typing</td>\n<td>MyPy with strict mode for compile-time checking</td>\n</tr>\n<tr>\n<td>Tensor Operations</td>\n<td>PyTorch tensors with manual shape checking</td>\n<td>TorchScript for optimized inference</td>\n</tr>\n<tr>\n<td>Configuration Files</td>\n<td>JSON or YAML with manual parsing</td>\n<td>Hydra for hierarchical configuration management</td>\n</tr>\n<tr>\n<td>Parameter Counting</td>\n<td>Simple loop over model.parameters()</td>\n<td>PyTorch Lightning for automatic logging</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer/\n├── config/\n│   ├── __init__.py\n│   ├── model_config.py      ← TransformerConfig definition\n│   ├── training_config.py   ← TrainingConfig definition\n│   ├── generation_config.py ← GenerationConfig definition\n│   └── validation.py        ← Configuration validation utilities\n├── data/\n│   ├── __init__.py\n│   ├── tokenizer.py        ← SimpleTokenizer implementation\n│   └── dataset.py          ← TextDataset implementation\n├── models/\n│   ├── __init__.py\n│   ├── attention.py        ← MultiHeadAttention implementation\n│   ├── transformer.py     ← Main transformer model\n│   └── utils.py           ← count_parameters, create_causal_mask\n└── examples/\n    ├── small_model.json    ← Example configurations\n    ├── medium_model.json\n    └── large_model.json</code></pre></div>\n\n<h4 id=\"configuration-infrastructure-complete-implementation\">Configuration Infrastructure (Complete Implementation)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># config/model_config.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for transformer model architecture.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    All architectural parameters that determine model capacity and</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    computational requirements. These are typically set once and</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    remain fixed throughout training and deployment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#6A737D\">           # Primary embedding dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#6A737D\">           # Number of attention heads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#6A737D\">               # Key/query dimension per head  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#6A737D\">               # Value dimension per head</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#6A737D\">        # Maximum sequence length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    vocab_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#6A737D\">      # Token vocabulary size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">          # Number of transformer blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">    # Dropout probability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ffn_expansion: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">       # FFN hidden dim multiplier</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration parameters after initialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.validate()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration parameters for consistency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"d_model must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"num_heads must be positive\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"d_model must be divisible by num_heads\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"d_k must equal d_model // num_heads = </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"d_v must equal d_model // num_heads = </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.seq_length </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"seq_length must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.vocab_size </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"vocab_size must be at least 2\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.dropout_rate </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"dropout_rate must be between 0.0 and 1.0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.ffn_expansion </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ffn_expansion must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_json</span><span style=\"color:#E1E4E8\">(cls, json_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'TransformerConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from JSON file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(json_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            config_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_json</span><span style=\"color:#E1E4E8\">(self, json_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save configuration to JSON file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(json_path, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            json.dump(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">__dict__</span><span style=\"color:#E1E4E8\">, f, </span><span style=\"color:#FFAB70\">indent</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for training hyperparameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_epochs: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_clip_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    weight_decay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eval_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.validate()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate training configuration parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.learning_rate </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"learning_rate must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.batch_size </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"batch_size must be between 1 and 1024\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_epochs </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"num_epochs must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"gradient_clip_norm must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.weight_decay </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"weight_decay must be between 0.0 and 1.0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GenerationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for text generation parameters.\"\"\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">              # 0 disables top-k</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_p: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#6A737D\">          # 0.0 disables top-p  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pad_token_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eos_token_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    repetition_penalty: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    length_penalty: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.validate()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate generation configuration parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.temperature </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"temperature must be positive (use small values like 0.01 instead of 0.0)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.top_k </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"top_k must be non-negative (0 disables top-k sampling)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.top_p </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"top_p must be between 0.0 and 1.0\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_length </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max_length must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"tensor-utilities-complete-implementation\">Tensor Utilities (Complete Implementation)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># models/utils.py  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> seaborn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> sns</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_causal_mask</span><span style=\"color:#E1E4E8\">(seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: torch.device) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create lower triangular causal mask for autoregressive attention.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        seq_length: Length of the sequence</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        device: Device to create tensor on</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Boolean mask of shape [seq_length, seq_length] where True indicates</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        positions that should be masked (set to -inf in attention scores)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create upper triangular matrix (above diagonal = True = masked)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.triu(torch.ones(seq_length, seq_length, </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.bool, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">device), </span><span style=\"color:#FFAB70\">diagonal</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> mask</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> apply_causal_mask</span><span style=\"color:#E1E4E8\">(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Apply causal mask to attention scores.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        scores: Attention scores of shape [batch_size, num_heads, seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        mask: Boolean mask of shape [seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Masked scores with -inf at masked positions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Use large negative value instead of -inf to avoid numerical issues</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    masked_scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores.masked_fill(mask, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1e9</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> masked_scores</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(model: nn.Module) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Count the total number of trainable parameters in a model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        model: PyTorch model</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Total number of trainable parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> p.requires_grad)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_attention_weights</span><span style=\"color:#E1E4E8\">(weights: torch.Tensor, tokens: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Visualize attention weights as a heatmap.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        weights: Attention weights of shape [batch_size, num_heads, seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tokens: List of token strings for axis labels</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        layer: Layer index for title</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        head: Head index for title</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Extract single head weights [seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    head_weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> weights[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, head].detach().cpu().numpy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sns.heatmap(head_weights, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                xticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                yticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Blues'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cbar</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.title(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'Attention Weights - Layer </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">layer</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Head </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">head</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.xlabel(</span><span style=\"color:#9ECBFF\">'Key Positions'</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.ylabel(</span><span style=\"color:#9ECBFF\">'Query Positions'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.show()</span></span></code></pre></div>\n\n<h4 id=\"core-configuration-skeletons\">Core Configuration Skeletons</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># config/validation.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .model_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .training_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TrainingConfig  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .generation_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> GenerationConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_config_compatibility</span><span style=\"color:#E1E4E8\">(model_config: TransformerConfig, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                training_config: TrainingConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                generation_config: GenerationConfig) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate compatibility between different configuration structures.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of validation error messages (empty if all valid)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that generation max_length doesn't exceed model seq_length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify that special token IDs are within model vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate memory requirements don't exceed reasonable limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check that batch_size and seq_length combination is feasible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Warn about learning rate scaling for large batch sizes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> estimate_memory_usage</span><span style=\"color:#E1E4E8\">(config: TransformerConfig, batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Estimate GPU memory usage in MB for given configuration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Dictionary with memory breakdown by component</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate embedding table memory: vocab_size * d_model * 4 bytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate attention memory: batch * heads * seq_len^2 * 4 bytes  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate activation memory: batch * seq_len * d_model * layers * 4 bytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate parameter memory: count_parameters(model) * 4 bytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add optimizer state memory (2x parameters for Adam)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'embeddings_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'attention_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'activations_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'parameters_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'optimizer_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'total_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span></code></pre></div>\n\n<h4 id=\"configuration-examples\">Configuration Examples</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// examples/small_model.json</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"d_model\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"num_heads\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"d_k\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"d_v\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"seq_length\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"vocab_size\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">5000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"num_layers\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"dropout_rate\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"ffn_expansion\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// examples/medium_model.json  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"d_model\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">512</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"num_heads\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"d_k\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"d_v\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"seq_length\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">512</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"vocab_size\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"num_layers\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"dropout_rate\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"ffn_expansion\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the data model structures, verify the following behavior:</p>\n<p><strong>Test Configuration Loading:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Should successfully load and validate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TransformerConfig.from_json(</span><span style=\"color:#9ECBFF\">'examples/small_model.json'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Model has </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">config.d_model</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> embedding dimension\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Each head processes </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">config.d_k</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> dimensions\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Test Configuration Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Should raise ValueError about head division</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bad_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TransformerConfig(</span><span style=\"color:#FFAB70\">d_model</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">num_heads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">d_k</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">d_v</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Caught expected error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Test Tensor Utilities:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Should create proper triangular mask</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_causal_mask(</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, torch.device(</span><span style=\"color:#9ECBFF\">'cpu'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Causal mask (True = masked):\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(mask)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># [[False, True,  True,  True ],</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#  [False, False, True,  True ],  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#  [False, False, False, True ],</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#  [False, False, False, False]]</span></span></code></pre></div>\n\n<p>Signs that something is wrong:</p>\n<ul>\n<li>Configuration validation doesn&#39;t catch obvious errors (like negative dimensions)</li>\n<li>Causal mask has True values below the diagonal  </li>\n<li><code>count_parameters</code> returns 0 for a model with weights</li>\n<li>JSON loading/saving fails with basic configuration files</li>\n</ul>\n<p>The next step is implementing the self-attention mechanism using these data structures as the foundation for tensor operations.</p>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Tensor Shapes and Data Types\"></p>\n<h2 id=\"self-attention-mechanism\">Self-Attention Mechanism</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 - Self-Attention (implementing scaled dot-product attention, multi-head attention, and causal masking)</p>\n</blockquote>\n<p>The self-attention mechanism represents the revolutionary core of transformer architecture, fundamentally changing how neural networks process sequential data. Unlike recurrent neural networks that process tokens sequentially, self-attention allows every token in a sequence to directly interact with every other token in parallel. This section implements the complete self-attention system, including the scaled dot-product attention computation, multi-head attention with parallel processing heads, and causal masking that enables autoregressive text generation.</p>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Fattention-mechanism.svg\" alt=\"Multi-Head Attention Architecture\"></p>\n<p>Understanding self-attention requires building intuition about how relevance-based information aggregation works in practice. We&#39;ll start with mental models that make the mathematical formulation intuitive, then dive deep into each component&#39;s implementation details, architectural decisions, and common pitfalls that derail many transformer implementations.</p>\n<h3 id=\"mental-model-selective-focus\">Mental Model: Selective Focus</h3>\n<p>Think of self-attention as a sophisticated <strong>cocktail party listener</strong> who can dynamically focus on different conversations simultaneously. Imagine you&#39;re at a crowded party where multiple conversations happen around you. Your brain naturally focuses more attention on certain speakers based on context, relevance, and your current interests. When someone mentions your name, you instantly shift more attention to that conversation. When discussing a technical topic, you might simultaneously listen to multiple related discussions and synthesize information from all of them.</p>\n<p>Self-attention works similarly with tokens in a text sequence. Each token (like you at the party) needs to decide how much attention to pay to every other token (like each speaker) in the sequence. The attention mechanism computes <strong>relevance scores</strong> between tokens, then uses these scores to create a weighted summary of information from all positions.</p>\n<p>Consider the sentence: &quot;The cat sat on the mat because it was comfortable.&quot; When processing the word &quot;it,&quot; the attention mechanism should focus heavily on &quot;cat&quot; (high attention weight) while paying minimal attention to words like &quot;the&quot; or &quot;on&quot; (low attention weights). This relevance-based focusing allows the model to capture long-range dependencies and resolve ambiguities that would be impossible with purely local context windows.</p>\n<p>The mathematical beauty of self-attention lies in how it formalizes this intuitive process. Each token creates three representations of itself:</p>\n<ul>\n<li><strong>Query (Q)</strong>: &quot;What am I looking for?&quot; - represents what information this token needs</li>\n<li><strong>Key (K)</strong>: &quot;What information do I offer?&quot; - represents what this token can provide to others  </li>\n<li><strong>Value (V)</strong>: &quot;Here&#39;s my actual content&quot; - represents the information this token contributes</li>\n</ul>\n<p>The attention computation measures compatibility between queries and keys (relevance), then uses these scores to weight the values (content aggregation). This creates a context-aware representation where each token incorporates relevant information from across the entire sequence.</p>\n<h3 id=\"scaled-dot-product-attention\">Scaled Dot-Product Attention</h3>\n<p>The scaled dot-product attention mechanism forms the mathematical foundation of transformer models. This computation takes three matrices - queries, keys, and values - and produces attention-weighted outputs through a carefully designed sequence of linear algebra operations.</p>\n<p><strong>Attention Computation Formula:</strong>\nThe core computation follows this mathematical sequence:</p>\n<ol>\n<li>Compute compatibility scores between queries and keys using matrix multiplication</li>\n<li>Scale the scores by the square root of the key dimension to maintain stable gradients</li>\n<li>Apply softmax normalization to convert scores into probability distributions</li>\n<li>Use attention weights to compute weighted averages of value vectors</li>\n</ol>\n<p>The mathematical formulation captures this as: <code>Attention(Q, K, V) = softmax(Q @ K.T / sqrt(d_k)) @ V</code></p>\n<p>Let&#39;s examine each step in detail with concrete tensor operations:</p>\n<p><strong>Step 1: Score Computation</strong>\nThe raw attention scores measure compatibility between each query vector and every key vector. Given input sequences of length <code>seq_length</code> with embedding dimension <code>d_model</code>, the query matrix Q and key matrix K both have shape <code>[batch_size, seq_length, d_k]</code>. The score computation <code>Q @ K.T</code> produces a matrix of shape <code>[batch_size, seq_length, seq_length]</code> where entry (i,j) represents how much token i should attend to token j.</p>\n<p>This compatibility scoring captures semantic relationships between positions. Words with similar meanings or strong contextual relationships produce higher dot-product scores, while unrelated tokens produce scores closer to zero. The dot-product operation efficiently computes these pairwise interactions in parallel across all positions.</p>\n<p><strong>Step 2: Score Scaling</strong>\nThe scaling factor <code>1/sqrt(d_k)</code> prevents attention scores from growing too large as the model dimension increases. Without scaling, dot-products between high-dimensional vectors tend to produce extreme values that push the softmax function into saturation regions where gradients vanish. The square root scaling maintains reasonable score magnitudes regardless of dimension.</p>\n<p>This scaling decision represents a critical stability consideration. Large unscaled scores cause the softmax function to produce near-one attention weights for the highest-scoring position and near-zero weights everywhere else. Such extreme attention distributions eliminate the model&#39;s ability to attend to multiple relevant positions simultaneously and create gradient flow problems during training.</p>\n<p><strong>Step 3: Softmax Normalization</strong>\nThe softmax operation converts raw scores into a probability distribution over positions. For each query position, the softmax ensures attention weights sum to 1.0 while maintaining relative ordering from the raw scores. This normalization step is crucial for interpreting attention weights as relevance probabilities.</p>\n<p>The softmax computation <code>exp(score_i) / sum(exp(score_j))</code> amplifies differences between scores while ensuring valid probability distributions. Higher-scoring positions receive proportionally more attention weight, but the probabilistic interpretation allows the model to attend to multiple positions when appropriate.</p>\n<p><strong>Step 4: Value Aggregation</strong>\nThe final matrix multiplication combines attention weights with value vectors to produce context-aware outputs. Each output position receives a weighted combination of value vectors from all positions, with weights determined by the attention distribution. This aggregation step creates representations that incorporate relevant information from across the sequence.</p>\n<p>The value matrix V has shape <code>[batch_size, seq_length, d_v]</code>, and the attention weights have shape <code>[batch_size, seq_length, seq_length]</code>. The multiplication produces output vectors of shape <code>[batch_size, seq_length, d_v]</code>, where each output vector represents a context-aware encoding of its corresponding input position.</p>\n<blockquote>\n<p><strong>Decision: Separate Key and Value Dimensions</strong></p>\n<ul>\n<li><strong>Context</strong>: The original attention formulation could use the same dimension for keys and values, or allow them to differ</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Unified dimension where <code>d_k = d_v = d_model</code></li>\n<li>Separate dimensions where <code>d_k</code> optimizes similarity computation and <code>d_v</code> optimizes content representation</li>\n<li>Fixed small dimensions regardless of model size</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use separate configurable dimensions with typical values <code>d_k = d_v = d_model / num_heads</code></li>\n<li><strong>Rationale</strong>: Separate dimensions provide flexibility to optimize similarity computation (keys/queries) independently from content representation (values), while the per-head scaling maintains reasonable parameter counts</li>\n<li><strong>Consequences</strong>: Adds configuration complexity but enables better parameter allocation between attention computation and content transformation</li>\n</ul>\n</blockquote>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Fattention-states.svg\" alt=\"Attention Computation States\"></p>\n<p><strong>Attention Score Interpretation</strong>\nUnderstanding attention weights requires recognizing them as learned similarity measures. High attention weights between positions indicate the model has learned these positions provide mutually relevant information. The specific patterns depend on the task and training data, but common patterns emerge:</p>\n<table>\n<thead>\n<tr>\n<th>Attention Pattern</th>\n<th>Description</th>\n<th>Example Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Local Dependencies</td>\n<td>High weights between adjacent tokens</td>\n<td>Syntactic relationships, phrases</td>\n</tr>\n<tr>\n<td>Long-Range Dependencies</td>\n<td>High weights across distant positions</td>\n<td>Pronoun resolution, discourse coherence</td>\n</tr>\n<tr>\n<td>Content-Based Similarity</td>\n<td>High weights between semantically similar tokens</td>\n<td>Synonym relationships, topic coherence</td>\n</tr>\n<tr>\n<td>Positional Bias</td>\n<td>Consistent attention to specific relative positions</td>\n<td>Sentence beginnings, punctuation</td>\n</tr>\n</tbody></table>\n<h3 id=\"multi-head-attention\">Multi-Head Attention</h3>\n<p>Multi-head attention extends the single attention mechanism by running multiple attention computations in parallel, each with different learned projection matrices. This architectural choice allows the model to capture different types of relationships simultaneously - some heads might focus on syntactic relationships while others capture semantic similarities or positional patterns.</p>\n<p><strong>Parallel Attention Heads</strong>\nEach attention head operates independently with its own query, key, and value projection matrices. Given input embeddings of dimension <code>d_model</code>, each head projects to smaller dimensions <code>d_k</code> and <code>d_v</code>, typically <code>d_model / num_heads</code>. This dimensionality reduction allows multiple heads to operate within the same computational budget as a single large head.</p>\n<p>The parallel computation creates <code>num_heads</code> different attention patterns over the same input sequence. Head h computes its attention as: <code>head_h = Attention(X @ W_Q_h, X @ W_K_h, X @ W_V_h)</code> where <code>W_Q_h</code>, <code>W_K_h</code>, and <code>W_V_h</code> are the learned projection matrices specific to head h.</p>\n<p><strong>Head Specialization and Diversity</strong>\nDifferent attention heads naturally specialize during training to capture complementary aspects of the input relationships. This emergent specialization provides several advantages:</p>\n<p>Training dynamics naturally encourage head diversity through the model&#39;s objective to minimize loss. Heads that focus on redundant patterns provide less marginal benefit than heads discovering new relevant relationships. This creates pressure for each head to find unique useful patterns in the data.</p>\n<p>Research has identified several common head specialization patterns that emerge across different transformer models and tasks. Some heads consistently focus on local syntactic relationships like part-of-speech patterns or phrase boundaries. Other heads specialize in long-range semantic relationships, connecting entities mentioned far apart in the text. Positional heads learn to attend to specific relative positions regardless of content, useful for tasks with strong structural patterns.</p>\n<p><strong>Head Dimension Scaling</strong>\nThe choice of per-head dimensions involves balancing expressiveness against computational efficiency. Smaller head dimensions force each head to focus on the most important relationships, potentially improving interpretability and reducing overfitting. Larger head dimensions provide more representational capacity but may lead to redundant computations across heads.</p>\n<blockquote>\n<p><strong>Decision: Per-Head Dimension Scaling</strong></p>\n<ul>\n<li><strong>Context</strong>: Multi-head attention requires deciding how to allocate the total model dimension across attention heads</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Fixed small dimensions (e.g., d_k=64) regardless of model size</li>\n<li>Proportional scaling where <code>d_k = d_model / num_heads</code> </li>\n<li>Hybrid approach with minimum dimension limits</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use proportional scaling with <code>d_k = d_v = d_model / num_heads</code></li>\n<li><strong>Rationale</strong>: Proportional scaling maintains constant total parameters while distributing capacity evenly across heads; ensures each head has sufficient capacity as model size grows</li>\n<li><strong>Consequences</strong>: Larger models get more expressive individual heads, but very small models might have insufficient per-head capacity</li>\n</ul>\n</blockquote>\n<p><strong>Multi-Head Projection Matrices</strong>\nEach attention head requires three projection matrices to transform input embeddings into queries, keys, and values. These matrices are learned parameters that allow each head to focus on different aspects of the input representation.</p>\n<table>\n<thead>\n<tr>\n<th>Projection Type</th>\n<th>Matrix Shape</th>\n<th>Purpose</th>\n<th>Learning Objective</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Projection (<code>W_Q_h</code>)</td>\n<td><code>[d_model, d_k]</code></td>\n<td>Transform input to &quot;what I&#39;m looking for&quot; representation</td>\n<td>Learn to identify information needs</td>\n</tr>\n<tr>\n<td>Key Projection (<code>W_K_h</code>)</td>\n<td><code>[d_model, d_k]</code></td>\n<td>Transform input to &quot;what I can provide&quot; representation</td>\n<td>Learn to advertise available information</td>\n</tr>\n<tr>\n<td>Value Projection (<code>W_V_h</code>)</td>\n<td><code>[d_model, d_v]</code></td>\n<td>Transform input to &quot;actual content&quot; representation</td>\n<td>Learn to extract and format useful content</td>\n</tr>\n</tbody></table>\n<p>The projection matrices enable each head to create specialized views of the input data. A head focusing on syntactic relationships might learn projections that emphasize part-of-speech information, while a head capturing semantic similarity might project tokens into a space where synonyms cluster together.</p>\n<p><strong>Output Concatenation and Final Projection</strong>\nAfter computing attention outputs from all heads, multi-head attention concatenates the results and applies a final linear projection. This concatenation combines the specialized representations from each head into a unified output that preserves information from all attention patterns.</p>\n<p>The concatenation operation stacks head outputs along the feature dimension: if each head produces output shape <code>[batch_size, seq_length, d_v]</code>, concatenating <code>num_heads</code> heads yields shape <code>[batch_size, seq_length, num_heads * d_v]</code>. When <code>d_v = d_model / num_heads</code>, the concatenated output has the original <code>d_model</code> dimension.</p>\n<p>The final output projection <code>W_O</code> has shape <code>[num_heads * d_v, d_model]</code> and serves several important purposes. It allows the model to learn how to combine information from different heads optimally. It can resolve potential conflicts when heads produce contradictory information. It provides a final transformation to match the residual connection requirements in the transformer block.</p>\n<p><strong>Multi-Head Implementation Strategy</strong>\nEfficient multi-head attention implementation requires careful consideration of tensor operations and memory layout. Two main approaches exist for organizing the computation:</p>\n<p><strong>Sequential Head Processing</strong> computes each head independently in a loop, concatenating results at the end. This approach is conceptually simple and matches the mathematical definition directly, but may not utilize modern GPU parallelism optimally.</p>\n<p><strong>Batched Head Processing</strong> reshapes projection matrices to compute all heads simultaneously. The key insight is reorganizing dimensions so that head computation becomes an additional batch dimension that GPUs can parallelize naturally.</p>\n<table>\n<thead>\n<tr>\n<th>Implementation Approach</th>\n<th>Memory Layout</th>\n<th>Computational Efficiency</th>\n<th>Code Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential Processing</td>\n<td>Head-by-head computation</td>\n<td>Moderate, limited parallelism</td>\n<td>Low, matches math directly</td>\n</tr>\n<tr>\n<td>Batched Processing</td>\n<td>All heads in single tensor</td>\n<td>High, full GPU utilization</td>\n<td>Moderate, requires reshaping</td>\n</tr>\n<tr>\n<td>Fused Operations</td>\n<td>Combined QKV projection</td>\n<td>Highest, minimal memory transfers</td>\n<td>Higher, hardware-specific optimization</td>\n</tr>\n</tbody></table>\n<h3 id=\"causal-attention-masking\">Causal Attention Masking</h3>\n<p>Causal attention masking represents a crucial architectural constraint that enables autoregressive text generation. The causal mask prevents tokens from attending to future positions in the sequence, ensuring that each position can only use information from previous and current positions when making predictions. This constraint transforms the general bidirectional attention mechanism into a unidirectional system suitable for language modeling.</p>\n<p><strong>The Causality Constraint</strong>\nIn autoregressive language modeling, each token must predict the next token using only the tokens that have been generated so far. If tokens could attend to future positions during training, the model would learn to &quot;cheat&quot; by looking ahead at the targets, creating a mismatch between training and generation conditions where future tokens are unavailable.</p>\n<p>The causal constraint requires that attention weights <code>A[i,j]</code> be zero whenever <code>j &gt; i</code>, meaning position i cannot attend to any position j that comes after it in the sequence. This creates a lower triangular attention pattern where each position can attend to itself and all previous positions, but never to future positions.</p>\n<p>Consider processing the sequence &quot;The cat sat on&quot; during training. When computing representations for &quot;cat&quot; (position 1), the model can attend to &quot;The&quot; (position 0) and &quot;cat&quot; itself, but not to &quot;sat&quot; (position 2) or &quot;on&quot; (position 3). This constraint ensures the model learns to predict &quot;sat&quot; based only on the available context &quot;The cat&quot;, matching the generation scenario where future tokens don&#39;t exist yet.</p>\n<p><strong>Mask Implementation Strategy</strong>\nImplementing causal masking requires careful consideration of numerical stability and computational efficiency. The standard approach applies the mask to attention scores before the softmax operation, setting future positions to negative infinity so they receive zero attention weight after normalization.</p>\n<p>The causal mask is a binary matrix of shape <code>[seq_length, seq_length]</code> where entry (i,j) is 1 if position i can attend to position j, and 0 otherwise. For causal attention, this creates a lower triangular matrix with ones on and below the diagonal, zeros above the diagonal.</p>\n<p>Applying the mask involves replacing masked positions with large negative values (typically -1e9 or -inf) before softmax. The softmax function transforms these extreme negative values into near-zero probabilities, effectively eliminating attention to future positions. Using negative infinity guarantees exactly zero attention weights, but may cause numerical issues in some implementations.</p>\n<p><strong>Mask Generation and Caching</strong>\nGenerating causal masks efficiently requires understanding their structure and reusability properties. Since causal masks depend only on sequence length and not on actual content, they can be pre-computed and reused across different inputs and training steps.</p>\n<p>The mask generation process creates a lower triangular matrix using efficient tensor operations. Most deep learning frameworks provide utilities like <code>triu</code> (upper triangular) that can be inverted to create lower triangular masks. Alternatively, comparing position indices generates the mask: <code>mask[i,j] = (j &lt;= i)</code>.</p>\n<p>Caching causal masks provides significant computational savings since mask generation involves expensive tensor operations that would be repeated for every attention computation. A well-designed implementation caches masks for common sequence lengths and reuses them across batches and training steps.</p>\n<blockquote>\n<p><strong>Decision: Mask Application Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Causal masks can be applied at different stages of attention computation with different numerical properties</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Pre-softmax masking with negative infinity replacement</li>\n<li>Pre-softmax masking with large negative finite values (-1e9)</li>\n<li>Post-softmax masking by directly zeroing attention weights</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use pre-softmax masking with large negative finite values (-1e4 to -1e9)</li>\n<li><strong>Rationale</strong>: Negative infinity can cause NaN gradients in some frameworks; large negative finite values achieve near-zero attention weights while maintaining numerical stability; post-softmax masking breaks the probability distribution property</li>\n<li><strong>Consequences</strong>: Requires choosing appropriate negative value magnitude; maintains stable gradients; preserves softmax probability interpretation</li>\n</ul>\n</blockquote>\n<p><strong>Attention Pattern Analysis</strong>\nUnderstanding the effect of causal masking on attention patterns provides insights into how autoregressive models process sequences. The mask creates a characteristic triangular attention pattern where early tokens have limited context (only previous positions) while later tokens can attend to increasingly larger context windows.</p>\n<p>This triangular attention pattern has several important implications for model behavior. Tokens at the beginning of sequences must be predicted with minimal context, often leading to higher uncertainty and loss for early positions. Later tokens benefit from full left-context, typically achieving lower loss and higher confidence predictions.</p>\n<p>The causal structure also influences how information propagates through the sequence. Information can only flow forward in the sequence, creating a directed acyclic graph of dependencies. This constrains the types of relationships the model can capture compared to bidirectional attention, but matches the fundamental constraint of text generation where future tokens are unavailable.</p>\n<table>\n<thead>\n<tr>\n<th>Position in Sequence</th>\n<th>Available Context</th>\n<th>Typical Prediction Difficulty</th>\n<th>Information Flow</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Early Positions</td>\n<td>Very limited context</td>\n<td>High difficulty, high loss</td>\n<td>Receives little information</td>\n</tr>\n<tr>\n<td>Middle Positions</td>\n<td>Moderate left context</td>\n<td>Moderate difficulty</td>\n<td>Accumulates forward-flowing information</td>\n</tr>\n<tr>\n<td>Later Positions</td>\n<td>Full left context</td>\n<td>Lower difficulty</td>\n<td>Benefits from full sequence context</td>\n</tr>\n</tbody></table>\n<p><strong>Mask Broadcasting and Batching</strong>\nImplementing causal masking in batched settings requires understanding tensor broadcasting and memory efficiency. The same causal mask applies to all sequences in a batch, so the mask should broadcast across the batch dimension to avoid memory duplication.</p>\n<p>The causal mask has shape <code>[seq_length, seq_length]</code> but attention scores have shape <code>[batch_size, num_heads, seq_length, seq_length]</code>. Broadcasting rules allow the mask to expand automatically across batch and head dimensions when applied to attention scores. This broadcasting saves memory by storing only one mask copy regardless of batch size.</p>\n<p>Advanced implementations may combine causal masking with padding masks for variable-length sequences, creating compound masks that handle both causality and sequence boundaries. These compound masks require careful logical operations to ensure both constraints are enforced simultaneously.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Implementing self-attention involves numerous subtle details where small mistakes can completely break the mechanism or cause silent failures that are difficult to debug. Understanding these common pitfalls helps avoid weeks of frustrating debugging.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Attention Score Scaling</strong>\nMany implementations forget the scaling factor or apply it incorrectly. Without scaling by <code>1/sqrt(d_k)</code>, attention scores grow proportionally with the key dimension, pushing softmax into saturation regions where gradients vanish. This causes training to stall with very sharp attention distributions that prevent the model from learning to attend to multiple positions.</p>\n<p>The scaling must be applied to the raw dot-product scores before softmax, not after. Scaling the attention weights after softmax changes their probabilistic interpretation and doesn&#39;t address the gradient saturation problem. The scaling factor must use the actual key dimension per head, not the full model dimension.</p>\n<p><strong>Symptoms</strong>: Training loss plateaus early, attention weights are extremely peaked (one position gets &gt;0.99 weight), gradients are very small or zero for query/key projections.\n<strong>Fix</strong>: Verify scaling factor is <code>1/sqrt(d_k)</code> where <code>d_k</code> is the per-head key dimension, applied to scores before softmax.</p>\n<p>⚠️ <strong>Pitfall: Causal Mask Applied After Softmax</strong>\nApplying the causal mask to attention weights after softmax breaks the probability distribution property and creates incorrect attention patterns. Post-softmax masking zeros out some attention weights but doesn&#39;t renormalize the remaining weights, so they no longer sum to 1.0.</p>\n<p>The mask must be applied to raw attention scores before softmax. Setting future positions to large negative values (-1e9) ensures they receive near-zero probability after softmax normalization, while preserving the probability distribution over valid positions.</p>\n<p><strong>Symptoms</strong>: Attention weights don&#39;t sum to 1.0, model generates text that seems to use future context during training, training/generation performance mismatch.\n<strong>Fix</strong>: Apply causal mask by setting <code>scores[mask == 0] = -1e9</code> before softmax, not by zeroing attention weights after softmax.</p>\n<p>⚠️ <strong>Pitfall: Dimension Mismatches in Multi-Head Attention</strong>\nMulti-head attention involves numerous tensor reshaping operations where dimension mismatches can cause subtle bugs. Common mistakes include using <code>d_model</code> instead of <code>d_k</code> for per-head dimensions, incorrect concatenation ordering, or wrong final projection dimensions.</p>\n<p>The per-head dimensions must be <code>d_k = d_v = d_model / num_heads</code> for standard implementations. Query and key projections output <code>d_k</code> dimensions, value projections output <code>d_v</code> dimensions. After concatenating all heads, the final projection must map from <code>num_heads * d_v</code> back to <code>d_model</code>.</p>\n<p><strong>Symptoms</strong>: Runtime shape errors, attention outputs have wrong dimensions, final projection fails, concatenated head outputs don&#39;t match expected dimensions.\n<strong>Fix</strong>: Carefully track tensor shapes through each operation, verify <code>d_k * num_heads = d_model</code>, ensure projection matrices have correct input/output dimensions.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Key Transpose in Score Computation</strong>\nThe attention score computation requires transposing the key matrix: <code>scores = Q @ K.transpose()</code>. Forgetting the transpose or transposing the wrong matrix completely breaks the attention mechanism by computing meaningless dot products between incompatible dimensions.</p>\n<p>The transpose must be applied to the last two dimensions of the key tensor to align query and key dimensions for dot product computation. For batched multi-head attention with shape <code>[batch_size, num_heads, seq_length, d_k]</code>, the transpose should swap the last two dimensions.</p>\n<p><strong>Symptoms</strong>: Runtime shape errors during score computation, attention patterns that don&#39;t make sense, very poor model performance despite correct loss computation.\n<strong>Fix</strong>: Ensure score computation uses <code>Q @ K.transpose(-2, -1)</code> where the transpose swaps sequence length and key dimensions.</p>\n<p>⚠️ <strong>Pitfall: Shared Projection Matrices Across Heads</strong>\nUsing the same projection matrices for all attention heads eliminates the benefit of multi-head attention. Each head must have its own learned query, key, and value projections to capture different relationships.</p>\n<p>A common implementation mistake creates projection matrices of shape <code>[d_model, d_k]</code> and reuses them across heads, rather than creating separate matrices <code>[d_model, num_heads * d_k]</code> that can be split per head or using individual matrices per head.</p>\n<p><strong>Symptoms</strong>: All attention heads produce identical patterns, model performance doesn&#39;t improve with more heads, attention visualizations show redundant head behaviors.\n<strong>Fix</strong>: Ensure each head has separate projection matrices, either as individual parameters or as slices of larger concatenated parameter matrices.</p>\n<p>⚠️ <strong>Pitfall: Gradient Flow Problems from Extreme Attention Weights</strong>\nVery sharp attention distributions can cause gradient flow problems where most positions receive zero gradients. This happens when attention scores are too large (insufficient scaling), when temperature is too low, or when causal masking uses inappropriate negative values.</p>\n<p>Extreme attention weights (one position &gt;0.99, others &lt;0.01) prevent the model from learning distributed representations and can cause training instability. The gradients for low-attention positions become negligible, effectively removing them from training.</p>\n<p><strong>Symptoms</strong>: Very peaked attention distributions, some token embeddings don&#39;t update during training, unstable training loss, poor generalization to different sequence lengths.\n<strong>Fix</strong>: Verify proper score scaling, check causal mask values aren&#39;t too extreme (use -1e4 to -1e9), consider attention temperature or dropout for regularization.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete code implementations and practical guidance for building the self-attention mechanism in PyTorch. The implementation balances clarity with efficiency, providing complete working code for supporting components and detailed skeleton code for the core attention logic.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tensor Operations</td>\n<td>PyTorch with basic operations</td>\n<td>PyTorch with fused CUDA kernels</td>\n</tr>\n<tr>\n<td>Attention Implementation</td>\n<td>Separate Q, K, V projections</td>\n<td>Fused QKV projection for efficiency</td>\n</tr>\n<tr>\n<td>Masking Strategy</td>\n<td>Boolean masks with broadcasting</td>\n<td>In-place masking with memory views</td>\n</tr>\n<tr>\n<td>Multi-Head Processing</td>\n<td>Sequential head computation</td>\n<td>Batched reshape for parallel processing</td>\n</tr>\n<tr>\n<td>Numerical Precision</td>\n<td>Float32 throughout</td>\n<td>Mixed precision (float16 forward, float32 backward)</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer/\n├── attention/\n│   ├── __init__.py\n│   ├── multi_head_attention.py     ← Core MultiHeadAttention class\n│   ├── attention_utils.py          ← Mask creation and utility functions  \n│   └── attention_test.py           ← Unit tests for attention mechanisms\n├── config/\n│   ├── __init__.py\n│   └── model_config.py             ← TransformerConfig definition\n├── utils/\n│   ├── __init__.py\n│   └── tensor_utils.py             ← Shape checking and debugging utilities\n└── tests/\n    └── test_attention_integration.py ← End-to-end attention tests</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (COMPLETE)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># attention/attention_utils.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> seaborn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> sns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_causal_mask</span><span style=\"color:#E1E4E8\">(seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: torch.device) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Creates a causal (lower triangular) attention mask.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        seq_length: Maximum sequence length</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        device: Device to create tensor on</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Boolean mask of shape [seq_length, seq_length] where True indicates</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        positions that can be attended to (lower triangle including diagonal)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.tril(torch.ones(seq_length, seq_length, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">device))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> mask.bool()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> apply_causal_mask</span><span style=\"color:#E1E4E8\">(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Applies causal mask to attention scores by setting masked positions to -1e9.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        scores: Attention scores of shape [batch_size, num_heads, seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        mask: Boolean mask of shape [seq_len, seq_len] where True = can attend</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Masked attention scores with same shape as input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Invert mask so False positions (cannot attend) become True for masking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inverted_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> ~</span><span style=\"color:#E1E4E8\">mask</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Broadcast mask across batch and head dimensions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores.masked_fill(inverted_mask, mask_value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> scores</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(model: nn.Module) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Count total trainable parameters in model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> p.requires_grad)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_attention_weights</span><span style=\"color:#E1E4E8\">(weights: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         tokens: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         save_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Visualizes attention weights as a heatmap.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        weights: Attention weights [num_heads, seq_len, seq_len] or [seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tokens: List of token strings for axis labels</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        layer: Layer index for title</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        head: Head index for title  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        save_path: Optional path to save figure</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> weights.dim() </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract specific head</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> weights[head].detach().cpu().numpy()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> weights.detach().cpu().numpy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sns.heatmap(attn, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                xticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                yticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Blues'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cbar_kws</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'label'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'Attention Weight'</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.title(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'Attention Weights - Layer </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">layer</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Head </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">head</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.xlabel(</span><span style=\"color:#9ECBFF\">'Key Positions'</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.ylabel(</span><span style=\"color:#9ECBFF\">'Query Positions'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.tight_layout()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> save_path:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        plt.savefig(save_path, </span><span style=\"color:#FFAB70\">dpi</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">bbox_inches</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'tight'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.show()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># config/model_config.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for transformer model architecture.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#6A737D\">              # Model embedding dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#6A737D\">              # Number of parallel attention heads  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#6A737D\">                   # Key/query dimension per head</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#6A737D\">                   # Value dimension per head</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\">          # Maximum sequence length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    vocab_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50257</span><span style=\"color:#6A737D\">         # Size of token vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">             # Number of transformer blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">       # Probability for dropout regularization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ffn_expansion: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">          # Feed-forward network expansion ratio</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration and set derived values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure head dimensions divide evenly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, \\</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"d_model (</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.d_model</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) must be divisible by num_heads (</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.num_heads</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Set per-head dimensions if not explicitly provided</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validate dimensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model, \\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"Total key dimension exceeds model dimension\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model, \\</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"Total value dimension exceeds model dimension\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># utils/tensor_utils.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_attention_shapes</span><span style=\"color:#E1E4E8\">(queries: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          keys: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          values: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          expected_batch: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          expected_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          expected_seq_len: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          expected_d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          expected_d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validates attention tensor shapes for debugging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        queries: Query tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        keys: Key tensor  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        values: Value tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        expected_*: Expected dimensions for validation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        AssertionError: If any tensor has incorrect shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_q_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (expected_batch, expected_heads, expected_seq_len, expected_d_k)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_k_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (expected_batch, expected_heads, expected_seq_len, expected_d_k)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_v_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (expected_batch, expected_heads, expected_seq_len, expected_d_v)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> queries.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_q_shape, \\</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Query shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queries.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_q_shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> keys.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_k_shape, \\</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Key shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">keys.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_k_shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> values.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_v_shape, \\</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        f</span><span style=\"color:#9ECBFF\">\"Value shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">values.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_v_shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> print_attention_debug_info</span><span style=\"color:#E1E4E8\">(queries: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              keys: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              values: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              scores: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              attention_weights: torch.Tensor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              output: torch.Tensor) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Prints detailed shape and statistics for attention debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=== Attention Debug Info ===\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Queries: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queries.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, mean=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queries.mean()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, std=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">queries.std()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Keys: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">keys.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, mean=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">keys.mean()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, std=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">keys.std()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Values: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">values.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, mean=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">values.mean()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, std=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">values.std()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Scores: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">scores.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, mean=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">scores.mean()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, std=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">scores.std()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Attention Weights: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attention_weights.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, mean=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attention_weights.mean()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Output: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, mean=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.mean()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, std=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.std()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for common issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> torch.isnan(queries).any():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"⚠️  WARNING: NaN values in queries\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> torch.isnan(attention_weights).any():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"⚠️  WARNING: NaN values in attention weights\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> scores.max() </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"⚠️  WARNING: Very large attention scores (max: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">scores.max()</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> attention_weights.max() </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.99</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"⚠️  WARNING: Very peaked attention (max weight: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attention_weights.max()</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code (Multi-Head Attention)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># attention/multi_head_attention.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..config.model_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .attention_utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> create_causal_mask, apply_causal_mask</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultiHeadAttention</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Multi-head self-attention mechanism with causal masking support.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Implements parallel attention heads that capture different types of </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    relationships simultaneously, with optional causal masking for </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    autoregressive generation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.d_model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.num_heads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.d_k</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.d_v</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.seq_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.seq_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dropout_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.dropout_rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create query projection matrix W_Q </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Shape: [d_model, num_heads * d_k]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use nn.Linear(d_model, num_heads * d_k, bias=False)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.query_projection </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create key projection matrix W_K</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Shape: [d_model, num_heads * d_k] </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Same as query projection</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_projection </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create value projection matrix W_V</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Shape: [d_model, num_heads * d_v]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use nn.Linear(d_model, num_heads * d_v, bias=False)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_projection </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create final output projection W_O</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Shape: [num_heads * d_v, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Combines multi-head outputs back to d_model dimension</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.output_projection </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create dropout layer for attention weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use nn.Dropout(dropout_rate)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.attention_dropout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Register causal mask as buffer (not a learnable parameter)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This creates the mask once and reuses it across forward passes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        causal_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_causal_mask(config.seq_length, torch.device(</span><span style=\"color:#9ECBFF\">'cpu'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.register_buffer(</span><span style=\"color:#9ECBFF\">'causal_mask'</span><span style=\"color:#E1E4E8\">, causal_mask)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize projection weights using Xavier/Glorot initialization</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._initialize_weights()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _initialize_weights</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize projection matrices using Xavier initialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Initialize all projection matrices with Xavier uniform</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use nn.init.xavier_uniform_(module.weight) for each projection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Forward pass through multi-head attention.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input embeddings [batch_size, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Optional attention mask [seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Attention output [batch_size, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch_size, seq_len, d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Project input to queries, keys, values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply query_projection, key_projection, value_projection to x</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: queries = self.query_projection(x), etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queries </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Shape: [batch_size, seq_len, num_heads * d_k]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">     # Shape: [batch_size, seq_len, num_heads * d_k]  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        values </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">   # Shape: [batch_size, seq_len, num_heads * d_v]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Reshape projections for multi-head processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Reshape from [batch_size, seq_len, num_heads * d_k] </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # to [batch_size, num_heads, seq_len, d_k]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use .view() and .transpose() operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queries </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Target: [batch_size, num_heads, seq_len, d_k]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">     # Target: [batch_size, num_heads, seq_len, d_k]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        values </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">   # Target: [batch_size, num_heads, seq_len, d_v]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Compute scaled dot-product attention scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Formula: scores = queries @ keys.transpose(-2, -1) / sqrt(d_k)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.matmul() and math.sqrt()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attention_scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Shape: [batch_size, num_heads, seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Apply causal mask to attention scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use the causal_mask buffer and apply_causal_mask utility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Slice causal mask to current sequence length</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> mask </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Use default causal mask</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.causal_mask[:seq_len, :seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> mask</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply mask to scores</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        masked_scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Compute attention weights using softmax</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply softmax along the last dimension (key positions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.softmax(masked_scores, dim=-1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attention_weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Shape: [batch_size, num_heads, seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Apply dropout to attention weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.attention_dropout during training</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attention_weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Compute attention output by weighting values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Formula: output = attention_weights @ values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.matmul()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attention_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Shape: [batch_size, num_heads, seq_len, d_v]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Concatenate multi-head outputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Reshape from [batch_size, num_heads, seq_len, d_v]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # to [batch_size, seq_len, num_heads * d_v]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use .transpose() and .contiguous().view()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        concatenated_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Apply final output projection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Project from num_heads * d_v back to d_model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.output_projection()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        final_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Shape: [batch_size, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> final_output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_attention_weights</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns attention weights for visualization (without dropout).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input embeddings [batch_size, seq_len, d_model]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Attention weights [batch_size, num_heads, seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 16: Implement attention weight extraction for visualization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Similar to forward() but return attention_weights before applying to values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Don't apply dropout for visualization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> torch.no_grad():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Implement same steps as forward() through attention weight computation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<p><strong>PyTorch Tensor Operations:</strong></p>\n<ul>\n<li>Use <code>torch.matmul()</code> or <code>@</code> operator for matrix multiplication</li>\n<li>Use <code>tensor.transpose(-2, -1)</code> to swap last two dimensions for key transpose</li>\n<li>Use <code>tensor.view()</code> for reshaping when memory layout allows, <code>.contiguous().view()</code> when needed</li>\n<li>Use <code>F.softmax(tensor, dim=-1)</code> for softmax along last dimension</li>\n<li>Use <code>tensor.masked_fill(mask, value)</code> for efficient masking</li>\n</ul>\n<p><strong>Memory and Performance:</strong></p>\n<ul>\n<li>Register masks as buffers with <code>self.register_buffer()</code> to avoid recomputation</li>\n<li>Use in-place operations like <code>tensor.masked_fill_()</code> when possible</li>\n<li>Consider <code>torch.backends.cuda.enable_flash_sdp(True)</code> for optimized attention on compatible hardware</li>\n<li>Use <code>torch.no_grad()</code> for visualization functions to save memory</li>\n</ul>\n<p><strong>Numerical Stability:</strong></p>\n<ul>\n<li>Use <code>math.sqrt(self.d_k)</code> for scaling factor, not <code>self.d_k ** 0.5</code> </li>\n<li>Set masked positions to -1e9, not -float(&#39;inf&#39;), to avoid NaN gradients</li>\n<li>Initialize projection matrices with Xavier initialization: <code>nn.init.xavier_uniform_()</code></li>\n<li>Consider gradient clipping if attention scores become very large</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the multi-head attention mechanism, verify correctness with these checkpoints:</p>\n<p><strong>Unit Tests to Run:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_attention_integration.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"from attention.multi_head_attention import MultiHeadAttention; print('Import successful')\"</span></span></code></pre></div>\n\n<p><strong>Expected Behavior Verification:</strong></p>\n<ol>\n<li><p><strong>Shape Consistency</strong>: Create a <code>MultiHeadAttention</code> layer with <code>d_model=512</code>, <code>num_heads=8</code>. Pass input tensor of shape <code>[2, 10, 512]</code>. Output should have shape <code>[2, 10, 512]</code>.</p>\n</li>\n<li><p><strong>Causal Masking</strong>: Generate attention weights for input sequence &quot;The cat sat&quot;. Position 0 should only attend to position 0. Position 1 should attend to positions 0-1. Position 2 should attend to positions 0-2.</p>\n</li>\n<li><p><strong>Attention Weight Properties</strong>: Attention weights should sum to 1.0 along the last dimension (key positions). No NaN values should appear. Weights should be non-negative.</p>\n</li>\n<li><p><strong>Multi-Head Diversity</strong>: Different attention heads should produce different attention patterns. Visualize attention weights for multiple heads - they should not be identical.</p>\n</li>\n</ol>\n<p><strong>Manual Verification Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TransformerConfig(</span><span style=\"color:#FFAB70\">d_model</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">num_heads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seq_length</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">attention </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MultiHeadAttention(config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># [batch=1, seq_len=8, d_model=64]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention(x)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Input shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">x.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Output shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> output.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> x.shape, </span><span style=\"color:#9ECBFF\">\"Shape mismatch!\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check attention weights</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">weights </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention.get_attention_weights(x)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Attention weights shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">weights.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should be [1, 4, 8, 8]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Weights sum along keys: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">weights.sum(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should be all 1.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Causal pattern check - pos 0 can only see pos 0: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">weights[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:].sum() </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-6}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Signs of Problems and Debugging:</strong></p>\n<ul>\n<li><strong>Runtime Errors</strong>: Check tensor shapes at each step, especially after reshape operations</li>\n<li><strong>NaN Values</strong>: Usually caused by extreme attention scores; verify scaling factor and mask values</li>\n<li><strong>Identical Heads</strong>: Check that projection matrices are different across heads</li>\n<li><strong>Non-Causal Attention</strong>: Verify mask is applied before softmax and has correct triangular pattern</li>\n<li><strong>Poor Performance</strong>: May indicate initialization problems or incorrect gradient flow</li>\n</ul>\n<h2 id=\"transformer-block-design\">Transformer Block Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 - Transformer Block (building complete transformer blocks with feed-forward networks, layer normalization, and residual connections)</p>\n</blockquote>\n<p>The transformer block represents the fundamental computational unit that makes transformers so powerful for sequence modeling. While the self-attention mechanism from Milestone 1 allows tokens to communicate and share information, the complete transformer block creates a sophisticated <strong>information refinement pipeline</strong> that iteratively processes and enhances token representations through multiple complementary operations.</p>\n<h3 id=\"mental-model-information-refinement-pipeline\">Mental Model: Information Refinement Pipeline</h3>\n<p>Think of a transformer block like a <strong>sophisticated document editing workflow</strong> in a professional writing environment. Imagine you&#39;re working on an important document with a team of editors, each with different specializations:</p>\n<ol>\n<li><p><strong>The Communication Phase (Self-Attention)</strong>: First, all team members read the entire document and discuss which parts are most relevant to each section. They share insights about how different paragraphs relate to each other, ensuring that each section incorporates the most relevant information from other parts of the document.</p>\n</li>\n<li><p><strong>The Individual Processing Phase (Feed-Forward Network)</strong>: Next, each team member works independently on their assigned section, applying their specialized knowledge to enhance the content. They expand on ideas, add details, and refine the language without communicating with others during this phase.</p>\n</li>\n<li><p><strong>The Quality Control Phase (Layer Normalization)</strong>: Throughout both phases, a quality controller ensures that all changes maintain consistent style, tone, and format across the document, preventing any section from becoming too different from the others.</p>\n</li>\n<li><p><strong>The Preservation Phase (Residual Connections)</strong>: Finally, the team ensures that the original essence of each section is preserved by explicitly comparing the revised version with the original and making sure important information isn&#39;t lost in the editing process.</p>\n</li>\n</ol>\n<p>This analogy captures the key insight that transformer blocks don&#39;t just process information—they <strong>refine</strong> it through complementary operations that alternate between global communication (attention) and local processing (feed-forward), while maintaining stability and coherence through normalization and residual connections.</p>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Ftransformer-block.svg\" alt=\"Transformer Block Internal Structure\"></p>\n<h3 id=\"feed-forward-network\">Feed-Forward Network</h3>\n<p>The <strong>feed-forward network (FFN)</strong> within each transformer block serves as the &quot;individual processing&quot; component in our mental model. While self-attention enables global communication between tokens, the FFN provides each token position with dedicated computational capacity to process and transform its representation independently.</p>\n<p>The FFN implements a simple but powerful two-layer architecture that expands the representation to a higher-dimensional space, applies a non-linear activation function, then projects back to the original dimension. This expansion-contraction pattern is crucial for the transformer&#39;s expressiveness.</p>\n<blockquote>\n<p><strong>Decision: 4x Hidden Dimension Expansion</strong></p>\n<ul>\n<li><strong>Context</strong>: The FFN needs sufficient capacity to process token representations, but computational cost grows quadratically with hidden dimension</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>2x expansion (used in some efficient models)</li>\n<li>4x expansion (standard in most transformers)  </li>\n<li>8x expansion (used in very large models)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use 4x expansion ratio as the standard</li>\n<li><strong>Rationale</strong>: The 4x expansion provides a good balance between model capacity and computational efficiency. Empirically, this ratio has proven effective across many transformer variants and scales well from small to large models</li>\n<li><strong>Consequences</strong>: Each FFN layer will require 8x the parameters of the attention layer (2 linear layers × 4x expansion), making FFN the dominant parameter consumer in transformer blocks</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Expansion Ratio</th>\n<th>Parameters</th>\n<th>Computational Cost</th>\n<th>Model Capacity</th>\n<th>Empirical Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>2x</td>\n<td>4 × d_model²</td>\n<td>Low</td>\n<td>Limited</td>\n<td>Adequate for simple tasks</td>\n</tr>\n<tr>\n<td>4x</td>\n<td>8 × d_model²</td>\n<td>Moderate</td>\n<td>High</td>\n<td>Strong across diverse tasks</td>\n</tr>\n<tr>\n<td>8x</td>\n<td>16 × d_model²</td>\n<td>High</td>\n<td>Very High</td>\n<td>Marginal gains for most tasks</td>\n</tr>\n</tbody></table>\n<p>The FFN architecture consists of exactly two linear transformations with a non-linear activation function between them:</p>\n<ol>\n<li><strong>Expansion Layer</strong>: Projects input from <code>d_model</code> dimensions to <code>ffn_expansion × d_model</code> dimensions (typically 4 × d_model)</li>\n<li><strong>Activation Function</strong>: Applies ReLU, GELU, or SwiGLU activation to introduce non-linearity</li>\n<li><strong>Projection Layer</strong>: Projects back from expanded dimension to original <code>d_model</code> dimensions</li>\n<li><strong>Dropout</strong>: Applied after the projection layer for regularization during training</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Shape</th>\n<th>Output Shape</th>\n<th>Parameters</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Expansion Linear</td>\n<td>(batch, seq_len, d_model)</td>\n<td>(batch, seq_len, 4×d_model)</td>\n<td>d_model × 4×d_model</td>\n<td>Expand representation space</td>\n</tr>\n<tr>\n<td>Activation</td>\n<td>(batch, seq_len, 4×d_model)</td>\n<td>(batch, seq_len, 4×d_model)</td>\n<td>0</td>\n<td>Add non-linearity</td>\n</tr>\n<tr>\n<td>Projection Linear</td>\n<td>(batch, seq_len, 4×d_model)</td>\n<td>(batch, seq_len, d_model)</td>\n<td>4×d_model × d_model</td>\n<td>Contract to original dimension</td>\n</tr>\n<tr>\n<td>Dropout</td>\n<td>(batch, seq_len, d_model)</td>\n<td>(batch, seq_len, d_model)</td>\n<td>0</td>\n<td>Regularization</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: GELU Activation Function</strong></p>\n<ul>\n<li><strong>Context</strong>: Need non-linear activation function for the FFN hidden layer</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>ReLU (fast, simple, but can cause dead neurons)</li>\n<li>GELU (smooth, probabilistic, better gradients)</li>\n<li>SwiGLU (state-of-the-art but more complex)</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use GELU as the standard activation</li>\n<li><strong>Rationale</strong>: GELU provides smoother gradients than ReLU and performs better empirically on language tasks. The smooth activation helps with gradient flow during training, and the probabilistic interpretation (input × P(X ≤ input)) aligns well with the stochastic nature of language</li>\n<li><strong>Consequences</strong>: Slightly more computational cost than ReLU, but significantly better training dynamics and final performance</li>\n</ul>\n</blockquote>\n<p>The mathematical formulation of the complete FFN is:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>FFN(x) = dropout(W₂ · GELU(W₁ · x + b₁) + b₂)</code></pre></div>\n\n<p>Where:</p>\n<ul>\n<li>W₁ has shape (d_model, 4×d_model) - expansion weights</li>\n<li>W₂ has shape (4×d_model, d_model) - projection weights  </li>\n<li>b₁ and b₂ are bias vectors</li>\n<li>GELU(x) = x · P(X ≤ x) where X ~ N(0,1)</li>\n</ul>\n<p><strong>Why the FFN is Essential</strong>: The FFN serves several critical purposes that attention alone cannot provide:</p>\n<ol>\n<li><strong>Non-linear Processing</strong>: While attention is fundamentally a linear operation (weighted averages), the FFN introduces non-linearity that enables complex transformations</li>\n<li><strong>Position-wise Processing</strong>: Each token position is processed independently, allowing the model to apply learned transformations specific to the content at that position</li>\n<li><strong>Memory and Storage</strong>: The FFN weights act as a form of memory, storing patterns and transformations learned from the training data</li>\n<li><strong>Computational Bottleneck</strong>: The expanded dimension provides additional computational capacity for complex reasoning</li>\n</ol>\n<h3 id=\"layer-normalization-strategy\">Layer Normalization Strategy</h3>\n<p><strong>Layer normalization</strong> serves as the &quot;quality control&quot; mechanism in our transformer block, ensuring that activations maintain stable statistics throughout the forward pass. Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes across the feature dimension for each individual token, making it particularly well-suited for sequence models where batch and sequence dimensions have different semantic meanings.</p>\n<p>The core layer normalization operation normalizes each token&#39;s feature vector to have zero mean and unit variance, then applies learned scale and shift parameters:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>LayerNorm(x) = γ ⊙ (x - μ) / σ + β</code></pre></div>\n\n<p>Where:</p>\n<ul>\n<li>μ = mean(x) across the feature dimension</li>\n<li>σ = std(x) across the feature dimension  </li>\n<li>γ = learned scale parameter (initialized to 1)</li>\n<li>β = learned bias parameter (initialized to 0)</li>\n<li>⊙ = element-wise multiplication</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Shape</th>\n<th>Initialization</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>γ (gamma)</td>\n<td>(d_model,)</td>\n<td>ones</td>\n<td>Learned scale factors</td>\n</tr>\n<tr>\n<td>β (beta)</td>\n<td>(d_model,)</td>\n<td>zeros</td>\n<td>Learned bias terms</td>\n</tr>\n<tr>\n<td>ε (epsilon)</td>\n<td>scalar</td>\n<td>1e-5</td>\n<td>Numerical stability constant</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Pre-Normalization Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Layer normalization can be applied before or after the sublayers (attention/FFN), significantly impacting training dynamics</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Post-normalization (original Transformer paper): LayerNorm(x + Sublayer(x))</li>\n<li>Pre-normalization (modern practice): x + Sublayer(LayerNorm(x))</li>\n<li>Sandwich normalization: LayerNorm(x + Sublayer(LayerNorm(x)))</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use pre-normalization architecture</li>\n<li><strong>Rationale</strong>: Pre-norm provides better gradient flow and training stability, especially for deep models. The normalization operation occurs in the residual branch rather than the main path, preventing gradient scaling issues. Modern transformer implementations consistently use pre-norm for improved convergence</li>\n<li><strong>Consequences</strong>: Training converges faster and is more stable, but requires careful initialization since the residual path carries the primary gradient signal</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Normalization Placement</th>\n<th>Gradient Flow</th>\n<th>Training Stability</th>\n<th>Convergence Speed</th>\n<th>Modern Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Post-norm</td>\n<td>Poor for deep models</td>\n<td>Requires warm-up</td>\n<td>Slow initial training</td>\n<td>Legacy only</td>\n</tr>\n<tr>\n<td>Pre-norm</td>\n<td>Excellent</td>\n<td>High stability</td>\n<td>Fast convergence</td>\n<td>Standard practice</td>\n</tr>\n<tr>\n<td>Sandwich</td>\n<td>Best theoretical</td>\n<td>Highest stability</td>\n<td>Fastest but complex</td>\n<td>Research only</td>\n</tr>\n</tbody></table>\n<p>The pre-normalization transformer block applies layer normalization before each sublayer rather than after:</p>\n<ol>\n<li><strong>Attention Sublayer</strong>: <code>x = x + MultiHeadAttention(LayerNorm(x))</code></li>\n<li><strong>FFN Sublayer</strong>: <code>x = x + FFN(LayerNorm(x))</code></li>\n</ol>\n<p>This arrangement creates a &quot;clean&quot; residual path where gradients can flow directly from the output back to the input without passing through normalization operations that might scale or distort the gradient signal.</p>\n<p><strong>Why Layer Normalization Works</strong>: The effectiveness of layer normalization in transformers stems from several factors:</p>\n<ol>\n<li><strong>Activation Stabilization</strong>: Prevents activations from growing too large or small, maintaining healthy gradient magnitudes</li>\n<li><strong>Reduced Internal Covariate Shift</strong>: Normalizing inputs to each layer reduces the change in input distributions during training</li>\n<li><strong>Improved Gradient Flow</strong>: Stable activations lead to more consistent gradients throughout the network</li>\n<li><strong>Regularization Effect</strong>: The normalization operation acts as a mild regularizer, reducing overfitting</li>\n</ol>\n<p><strong>Numerical Considerations</strong>: Layer normalization requires careful handling of numerical stability, particularly when computing the standard deviation:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>variance = mean((x - mean)²) + ε\nstd = sqrt(variance)\nnormalized = (x - mean) / std</code></pre></div>\n\n<p>The epsilon term (typically 1e-5) prevents division by zero and numerical instability when the variance is very small.</p>\n<h3 id=\"residual-connections\">Residual Connections</h3>\n<p><strong>Residual connections</strong> implement the &quot;preservation&quot; aspect of our information refinement pipeline, ensuring that the original token representations are explicitly preserved even as they undergo complex transformations. These skip connections, popularized by ResNet and adapted for transformers, address the fundamental challenge of training deep neural networks: the vanishing gradient problem.</p>\n<p>In the transformer block context, residual connections create direct paths from the input to the output that bypass the complex transformations of attention and feed-forward layers. This allows gradients to flow directly backwards during training and ensures that the model can always fall back to the identity transformation if the learned transformations prove harmful.</p>\n<p>The mathematical formulation for each sublayer becomes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>output = input + Sublayer(input)</code></pre></div>\n\n<p>Where <code>Sublayer(input)</code> represents either the multi-head attention operation or the feed-forward network. This simple addition has profound implications for training dynamics and model expressiveness.</p>\n<table>\n<thead>\n<tr>\n<th>Sublayer</th>\n<th>Transformation</th>\n<th>Residual Connection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-Head Attention</td>\n<td><code>MultiHeadAttention(LayerNorm(x))</code></td>\n<td><code>x + MultiHeadAttention(LayerNorm(x))</code></td>\n</tr>\n<tr>\n<td>Feed-Forward Network</td>\n<td><code>FFN(LayerNorm(x))</code></td>\n<td><code>x + FFN(LayerNorm(x))</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Addition-Based Residual Connections</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple ways exist to combine the residual connection with the sublayer output</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Element-wise addition: x + f(x)</li>\n<li>Concatenation followed by linear projection: Linear(concat(x, f(x)))</li>\n<li>Weighted combination: αx + (1-α)f(x) with learned α</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Use simple element-wise addition</li>\n<li><strong>Rationale</strong>: Addition preserves gradient flow exactly, requires no additional parameters, and has proven effective across millions of transformer deployments. More complex combinations add parameters without consistent performance gains</li>\n<li><strong>Consequences</strong>: Requires that residual and sublayer outputs have identical dimensions, but provides optimal gradient flow and parameter efficiency</li>\n</ul>\n</blockquote>\n<p><strong>Why Residual Connections are Critical</strong>: The importance of residual connections in transformers extends beyond simple gradient flow:</p>\n<ol>\n<li><p><strong>Gradient Flow Preservation</strong>: During backpropagation, gradients can flow directly through the residual connections without being modified by sublayer computations, preventing vanishing gradients in deep networks</p>\n</li>\n<li><p><strong>Identity Mapping Capability</strong>: If a sublayer learns to output zeros, the overall transformation becomes the identity function, allowing the network to preserve information when complex transformations aren&#39;t beneficial</p>\n</li>\n<li><p><strong>Training Stability</strong>: Residual connections reduce the risk of training instability by ensuring that even poorly initialized or temporarily poorly performing sublayers cannot completely disrupt the forward pass</p>\n</li>\n<li><p><strong>Incremental Learning</strong>: The network can learn incremental refinements to token representations rather than completely new representations, leading to more stable training dynamics</p>\n</li>\n</ol>\n<p><strong>Gradient Flow Analysis</strong>: Consider the backward pass through a transformer block with residual connections:</p>\n<ol>\n<li><strong>Output Gradient</strong>: ∂L/∂output flows from the loss function</li>\n<li><strong>Residual Branch</strong>: ∂L/∂input includes a direct copy of ∂L/∂output due to the addition operation</li>\n<li><strong>Sublayer Branch</strong>: ∂L/∂input also includes ∂L/∂sublayer_output × ∂sublayer_output/∂input</li>\n<li><strong>Combined Flow</strong>: The total gradient is the sum of both branches, ensuring strong gradient signal</li>\n</ol>\n<p>This dual-path gradient flow is the key insight that makes deep transformers trainable without the gradient decay that plagued earlier deep architectures.</p>\n<p><strong>Residual Connection Requirements</strong>: For residual connections to work properly, several conditions must be met:</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Specification</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dimension Matching</td>\n<td>input.shape == sublayer_output.shape</td>\n<td>Enable element-wise addition</td>\n</tr>\n<tr>\n<td>Initialization</td>\n<td>Sublayer outputs near zero initially</td>\n<td>Start near identity transformation</td>\n</tr>\n<tr>\n<td>Normalization</td>\n<td>Apply before sublayers in pre-norm</td>\n<td>Maintain stable residual path</td>\n</tr>\n<tr>\n<td>Dropout</td>\n<td>Applied to sublayer outputs</td>\n<td>Regularize transformations, not residuals</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Understanding the most frequent implementation mistakes helps developers avoid debugging nightmares and ensures correct transformer block behavior from the start.</p>\n<p>⚠️ <strong>Pitfall: Pre-norm vs Post-norm Confusion</strong></p>\n<p>Many developers implement post-normalization architecture because it appears in the original &quot;Attention Is All You Need&quot; paper, not realizing that modern practice has shifted to pre-normalization for better training dynamics.</p>\n<p><strong>Incorrect Implementation (Post-norm)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Wrong: applying normalization after the residual connection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.norm1(x </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.attention(x))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.norm2(x </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.ffn(x))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x</span></span></code></pre></div>\n\n<p><strong>Correct Implementation (Pre-norm)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Right: applying normalization before the sublayer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.attention(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.norm1(x))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.ffn(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.norm2(x))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> x</span></span></code></pre></div>\n\n<p><strong>Why Post-norm Causes Problems</strong>: In post-norm architecture, gradients must flow through the normalization operation, which can scale and distort gradient magnitudes. This leads to training instability, especially in deep models, and often requires careful learning rate warm-up schedules.</p>\n<p><strong>How to Detect</strong>: If training loss oscillates wildly or requires extremely small learning rates to converge, check your normalization placement.</p>\n<p>⚠️ <strong>Pitfall: Missing or Incorrect Residual Connections</strong></p>\n<p>Residual connections are easy to forget or implement incorrectly, leading to models that fail to train or converge to poor solutions.</p>\n<p><strong>Common Mistakes</strong>:</p>\n<ol>\n<li>Forgetting residual connections entirely: <code>x = self.attention(x)</code> instead of <code>x = x + self.attention(x)</code></li>\n<li>Applying residuals at wrong locations: Adding input to normalized output instead of sublayer output</li>\n<li>Dimension mismatches preventing addition: Sublayer changes dimensions without accounting for residuals</li>\n</ol>\n<p><strong>Detection</strong>: Model trains extremely slowly, loss decreases very gradually, or training becomes unstable with deeper models.</p>\n<p><strong>Fix</strong>: Ensure every sublayer has a corresponding residual connection with matching input/output dimensions.</p>\n<p>⚠️ <strong>Pitfall: FFN Dimension Errors</strong></p>\n<p>The feed-forward network&#39;s expansion and projection dimensions are frequently implemented incorrectly, leading to shape mismatches or suboptimal capacity.</p>\n<p><strong>Common Dimension Errors</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Incorrect Dimension</th>\n<th>Correct Dimension</th>\n<th>Consequence</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No expansion</td>\n<td>d_model → d_model</td>\n<td>d_model → 4×d_model</td>\n<td>Severely limited capacity</td>\n</tr>\n<tr>\n<td>Wrong expansion ratio</td>\n<td>d_model → 2×d_model</td>\n<td>d_model → 4×d_model</td>\n<td>Reduced capacity</td>\n</tr>\n<tr>\n<td>Missing projection</td>\n<td>4×d_model → 4×d_model</td>\n<td>4×d_model → d_model</td>\n<td>Dimension mismatch</td>\n</tr>\n<tr>\n<td>Swapped dimensions</td>\n<td>(4×d_model, d_model)</td>\n<td>(d_model, 4×d_model)</td>\n<td>Runtime shape errors</td>\n</tr>\n</tbody></table>\n<p><strong>Detection</strong>: PyTorch will typically throw shape mismatch errors during forward pass, or the model will have far fewer parameters than expected.</p>\n<p><strong>Fix</strong>: Double-check that the FFN expands to 4×d_model then projects back to d_model, with weight matrices shaped correctly for matrix multiplication.</p>\n<p>⚠️ <strong>Pitfall: Dropout Applied in Wrong Locations</strong></p>\n<p>Dropout placement significantly affects training dynamics, and applying dropout incorrectly can harm performance or training stability.</p>\n<p><strong>Incorrect Dropout Applications</strong>:</p>\n<ol>\n<li>Applying dropout to residual connections: breaks gradient flow</li>\n<li>Forgetting to disable dropout during inference: corrupts generation quality</li>\n<li>Applying dropout before layer normalization: destabilizes training</li>\n</ol>\n<p><strong>Correct Dropout Placement</strong>:</p>\n<ul>\n<li>Apply dropout to attention outputs before residual addition</li>\n<li>Apply dropout to FFN outputs before residual addition  </li>\n<li>Never apply dropout to the residual path itself</li>\n<li>Ensure dropout is disabled during evaluation/generation</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Layer Normalization Parameter Initialization</strong></p>\n<p>Incorrect initialization of layer normalization parameters can prevent the model from training effectively.</p>\n<p><strong>Common Initialization Errors</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Wrong Init</th>\n<th>Correct Init</th>\n<th>Problem if Wrong</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>γ (scale)</td>\n<td>zeros</td>\n<td>ones</td>\n<td>Kills gradient signal</td>\n</tr>\n<tr>\n<td>β (bias)</td>\n<td>random</td>\n<td>zeros</td>\n<td>Biases initial distribution</td>\n</tr>\n<tr>\n<td>ε (epsilon)</td>\n<td>1e-8</td>\n<td>1e-5</td>\n<td>Numerical instability</td>\n</tr>\n</tbody></table>\n<p><strong>Detection</strong>: Model fails to train from the beginning, loss remains constant, or numerical instabilities (NaN values) appear early in training.</p>\n<p><strong>Fix</strong>: Initialize γ to ones and β to zeros, use epsilon around 1e-5 for numerical stability.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Training vs Inference Behavior</strong></p>\n<p>The transformer block behaves differently during training and inference due to dropout and layer normalization statistics, but developers sometimes fail to properly manage these modes.</p>\n<p><strong>Common Mode Errors</strong>:</p>\n<ol>\n<li>Forgetting to call <code>model.eval()</code> during generation</li>\n<li>Manually managing dropout instead of using PyTorch&#39;s training mode</li>\n<li>Not understanding that layer normalization statistics are computed per forward pass (no stored statistics like batch norm)</li>\n</ol>\n<p><strong>Detection</strong>: Generated text quality is poor despite good training loss, or model produces inconsistent outputs for identical inputs during inference.</p>\n<p><strong>Fix</strong>: Always call <code>model.train()</code> during training and <code>model.eval()</code> during inference, and let PyTorch handle dropout automatically based on the model&#39;s mode.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides complete, working code for transformer blocks along with the organizational structure and debugging tools needed to successfully complete Milestone 2.</p>\n<h4 id=\"a-technology-recommendations\">A. Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Framework</td>\n<td>PyTorch with nn.Module</td>\n<td>PyTorch with custom autograd functions</td>\n</tr>\n<tr>\n<td>Activation</td>\n<td>F.gelu (built-in)</td>\n<td>Custom GELU implementation</td>\n</tr>\n<tr>\n<td>Initialization</td>\n<td>Default PyTorch init</td>\n<td>Xavier/Kaiming uniform initialization</td>\n</tr>\n<tr>\n<td>Debugging</td>\n<td>Print statements</td>\n<td>TensorBoard logging with histograms</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Manual tensor checks</td>\n<td>Automated shape and gradient tests</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-file-structure\">B. Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer_project/\n├── src/\n│   ├── model/\n│   │   ├── __init__.py\n│   │   ├── attention.py           ← MultiHeadAttention from Milestone 1\n│   │   ├── transformer_block.py   ← This milestone's core implementation  \n│   │   ├── ffn.py                 ← Feed-forward network component\n│   │   ├── layer_norm.py          ← Layer normalization utilities\n│   │   └── config.py              ← TransformerConfig definition\n│   ├── utils/\n│   │   ├── __init__.py\n│   │   └── model_utils.py         ← count_parameters, initialization helpers\n│   └── tests/\n│       ├── test_transformer_block.py\n│       ├── test_ffn.py\n│       └── test_layer_norm.py\n├── notebooks/\n│   └── transformer_block_analysis.ipynb  ← Visualization and debugging\n└── requirements.txt</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>File: <code>src/model/layer_norm.py</code></strong> (Complete implementation - copy and use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LayerNorm</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Layer normalization with learnable scale and bias parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Normalizes across the feature dimension for each token independently.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, eps: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1e-5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.eps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> eps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Learnable parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gamma </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.Parameter(torch.ones(d_model))  </span><span style=\"color:#6A737D\"># Scale</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.beta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.Parameter(torch.zeros(d_model))  </span><span style=\"color:#6A737D\"># Bias</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Apply layer normalization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Normalized tensor with same shape as input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute mean and variance across the feature dimension (last dim)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mean </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.mean(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keepdim</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># (batch_size, seq_length, 1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        variance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.var(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keepdim</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">unbiased</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># (batch_size, seq_length, 1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Normalize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (x </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> mean) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> torch.sqrt(variance </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.eps)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply learnable scale and bias</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gamma </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> normalized </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.beta</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_layer_norm</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test LayerNorm implementation with known inputs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch_size, seq_length, d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    layer_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LayerNorm(d_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create test input with known statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    x </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(batch_size, seq_length, d_model) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#6A737D\">  # Mean≈10, std≈5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Apply layer norm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> layer_norm(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check that output has approximately zero mean and unit variance per token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_means </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalized.mean(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Should be ≈ 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    token_vars </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalized.var(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)    </span><span style=\"color:#6A737D\"># Should be ≈ 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Input mean: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">x.mean(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, std: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">x.std(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.3f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Output mean: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token_means[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.6f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, std: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">torch.sqrt(token_vars[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">:.6f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> torch.allclose(token_means, torch.zeros_like(token_means), </span><span style=\"color:#FFAB70\">atol</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-6</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> torch.allclose(token_vars, torch.ones_like(token_vars), </span><span style=\"color:#FFAB70\">atol</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-6</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"LayerNorm test passed!\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_layer_norm()</span></span></code></pre></div>\n\n<p><strong>File: <code>src/utils/model_utils.py</code></strong> (Complete utilities):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> seaborn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> sns</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(model: nn.Module) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Count trainable and total parameters in a model.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        model: PyTorch model</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Dictionary with parameter counts</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    trainable_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> p.requires_grad)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_params </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'trainable'</span><span style=\"color:#E1E4E8\">: trainable_params,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'total'</span><span style=\"color:#E1E4E8\">: total_params,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'non_trainable'</span><span style=\"color:#E1E4E8\">: total_params </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> trainable_params</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> initialize_transformer_weights</span><span style=\"color:#E1E4E8\">(module: nn.Module):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Initialize transformer weights using best practices.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        module: PyTorch module to initialize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(module, nn.Linear):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Xavier uniform initialization for linear layers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        torch.nn.init.xavier_uniform_(module.weight)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> module.bias </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.nn.init.zeros_(module.bias)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(module, nn.LayerNorm):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Standard layer norm initialization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        torch.nn.init.ones_(module.weight)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        torch.nn.init.zeros_(module.bias)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(module, nn.Embedding):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Normal initialization for embeddings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        torch.nn.init.normal_(module.weight, </span><span style=\"color:#FFAB70\">mean</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">std</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.02</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> visualize_attention_patterns</span><span style=\"color:#E1E4E8\">(attention_weights: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                tokens: </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Visualize attention weights as a heatmap.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        attention_weights: Attention weights of shape (num_heads, seq_len, seq_len)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tokens: List of token strings</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        layer: Layer number for title</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        head: Head number to visualize</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Extract single head's attention</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    head_attention </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention_weights[head].detach().cpu().numpy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sns.heatmap(head_attention, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                xticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                yticklabels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokens,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Blues'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                annot</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                fmt</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'.3f'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cbar_kws</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'label'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'Attention Weight'</span><span style=\"color:#E1E4E8\">})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.title(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'Layer </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">layer</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Head </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">head</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> Attention Pattern'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.xlabel(</span><span style=\"color:#9ECBFF\">'Key Position'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.ylabel(</span><span style=\"color:#9ECBFF\">'Query Position'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.tight_layout()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.show()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradient_flow</span><span style=\"color:#E1E4E8\">(model: nn.Module, loss: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Check gradient flow through model parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        model: PyTorch model after backward pass</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        loss: Loss tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Dictionary with gradient statistics per layer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.named_parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> param.grad.data.norm(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">).item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gradient_stats[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> grad_norm</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gradient_stats[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradient_stats</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_shape_consistency</span><span style=\"color:#E1E4E8\">(model: nn.Module, input_shape: </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">, device: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'cpu'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Test that model maintains shape consistency through forward pass.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        model: PyTorch model to test</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        input_shape: Expected input shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        device: Device to run test on</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model.eval()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create random input</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(input_shape, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">device)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Testing model with input shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">test_input.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Forward pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> torch.no_grad():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model(test_input)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Output shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check that batch and sequence dimensions are preserved</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> output.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> input_shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Batch size mismatch: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">input_shape[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> output.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> input_shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Sequence length mismatch: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> vs </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">input_shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Shape consistency test passed!\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> output</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>File: <code>src/model/ffn.py</code></strong> (Skeleton for learner implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn.functional </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> F</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TransformerConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FeedForwardNetwork</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Position-wise feed-forward network with 4x hidden dimension expansion.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Implements: FFN(x) = dropout(W2 * GELU(W1 * x + b1) + b2)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize expansion layer (d_model -> 4 * d_model)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use nn.Linear(config.d_model, config.ffn_expansion * config.d_model)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expansion </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize projection layer (4 * d_model -> d_model)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use nn.Linear(config.ffn_expansion * config.d_model, config.d_model)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.projection </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize dropout layer with config.dropout_rate</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dropout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Apply feed-forward network transformation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Transformed tensor with same shape as input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply expansion layer to input x</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store result in variable called 'expanded'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expanded </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply GELU activation to expanded tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use F.gelu(expanded)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        activated </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply projection layer to bring back to d_model dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        projected </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Apply dropout to final output (only during training)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: self.dropout(projected)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> output</span></span></code></pre></div>\n\n<p><strong>File: <code>src/model/transformer_block.py</code></strong> (Core skeleton):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .attention </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MultiHeadAttention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .ffn </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> FeedForwardNetwork</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .layer_norm </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> LayerNorm</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TransformerConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerBlock</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Complete transformer block with multi-head attention and feed-forward network.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Uses pre-normalization architecture for better training stability.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize multi-head attention layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use MultiHeadAttention(config) from previous milestone</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.attention </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize feed-forward network</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.ffn </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize layer normalization for attention sublayer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: LayerNorm(config.d_model)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.norm1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize layer normalization for FFN sublayer  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.norm2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: torch.Tensor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Forward pass through transformer block.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Optional causal mask for attention</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor with same shape as input</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Implement attention sublayer with pre-norm and residual connection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pattern: x = x + attention(layer_norm(x))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store intermediate result for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normed_x1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        attention_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        x_after_attention </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Implement FFN sublayer with pre-norm and residual connection  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pattern: x = x + ffn(layer_norm(x))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normed_x2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ffn_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        x_after_ffn </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> x_after_ffn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_attention_weights</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: torch.Tensor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Extract attention weights for visualization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Optional causal mask</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Attention weights of shape (batch_size, num_heads, seq_length, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Call attention layer's get_attention_weights method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Apply layer norm first, then get attention weights</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        normed_x </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.norm1(x)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.attention.get_attention_weights(normed_x, mask)</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<p><strong>PyTorch Specific Tips:</strong></p>\n<ol>\n<li><p><strong>Module Registration</strong>: All <code>nn.Module</code> subcomponents are automatically registered when assigned as attributes in <code>__init__</code>, enabling proper parameter tracking and device movement.</p>\n</li>\n<li><p><strong>Training Mode Management</strong>: Use <code>self.training</code> to check if model is in training mode, but prefer letting PyTorch handle dropout automatically via <code>F.dropout(x, training=self.training)</code>.</p>\n</li>\n<li><p><strong>Memory Efficiency</strong>: For large models, consider using <code>torch.utils.checkpoint.checkpoint()</code> to trade computation for memory by recomputing activations during backward pass.</p>\n</li>\n<li><p><strong>Gradient Clipping</strong>: Apply gradient clipping before optimizer step with <code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code>.</p>\n</li>\n<li><p><strong>Device Handling</strong>: Use <code>x.device</code> to ensure all tensors are on the same device, especially when creating masks or constants.</p>\n</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the transformer block, verify correct behavior with these checkpoints:</p>\n<p><strong>Test 1: Shape Consistency</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.model.transformer_block import TransformerBlock</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.model.config import TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.utils.model_utils import test_shape_consistency</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import torch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">config = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=10000)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">block = TransformerBlock(config)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">test_shape_consistency(block, (2, 10, 512))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p>Expected output: &quot;Shape consistency test passed!&quot; with matching input/output shapes.</p>\n<p><strong>Test 2: Parameter Count Verification</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.model.transformer_block import TransformerBlock</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.model.config import TransformerConfig  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.utils.model_utils import count_parameters</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">config = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=10000)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">block = TransformerBlock(config)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">params = count_parameters(block)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Parameters: {params}')</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Expected: ~2.1M parameters for d_model=512</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># FFN: 2 * (512 * 2048) = 2,097,152 parameters  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Attention: ~1M parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Layer norms: ~2K parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Test 3: Gradient Flow Check</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import torch</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.model.transformer_block import TransformerBlock</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.model.config import TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.utils.model_utils import check_gradient_flow</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">config = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=10000)  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">block = TransformerBlock(config)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Forward and backward pass</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">x = torch.randn(2, 10, 512, requires_grad=True)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">output = block(x)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">loss = output.mean()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">loss.backward()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">grad_stats = check_gradient_flow(block, loss)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">for name, grad_norm in grad_stats.items():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(f'{name}: {grad_norm:.6f}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p>Expected: All parameters should have non-zero gradients, with reasonable magnitudes (typically 1e-6 to 1e-2).</p>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape mismatch errors</td>\n<td>FFN dimension errors</td>\n<td>Print tensor shapes at each step</td>\n<td>Check expansion/projection layer dimensions</td>\n</tr>\n<tr>\n<td>Training loss not decreasing</td>\n<td>Missing residual connections</td>\n<td>Check if loss oscillates vs stays flat</td>\n<td>Add residual connections to each sublayer</td>\n</tr>\n<tr>\n<td>Gradient explosion</td>\n<td>No gradient clipping</td>\n<td>Print gradient norms</td>\n<td>Add gradient clipping with max_norm=1.0</td>\n</tr>\n<tr>\n<td>Very slow convergence</td>\n<td>Post-norm instead of pre-norm</td>\n<td>Check layer norm placement</td>\n<td>Move layer norm before sublayers</td>\n</tr>\n<tr>\n<td>NaN values appearing</td>\n<td>Layer norm epsilon too small</td>\n<td>Check for division by zero</td>\n<td>Increase epsilon to 1e-5</td>\n</tr>\n<tr>\n<td>Poor generation quality</td>\n<td>Dropout active during inference</td>\n<td>Check model.training flag</td>\n<td>Call model.eval() before generation</td>\n</tr>\n<tr>\n<td>Memory usage too high</td>\n<td>No gradient checkpointing</td>\n<td>Monitor GPU memory</td>\n<td>Use torch.utils.checkpoint for large models</td>\n</tr>\n</tbody></table>\n<h2 id=\"training-pipeline\">Training Pipeline</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 - Training Pipeline (implementing tokenization, data loading, and language modeling objective with next-token prediction)</p>\n</blockquote>\n<p>The training pipeline transforms our transformer from a mathematical abstraction into a learning system that can acquire language understanding from raw text. This pipeline bridges the gap between static model architecture and dynamic learning, orchestrating the complex dance between data preprocessing, batch formation, loss computation, and parameter optimization.</p>\n<p>Think of the training pipeline as a sophisticated factory assembly line for language understanding. Raw text enters at one end, gets broken down into standardized components (tokens), assembled into training batches, fed through our transformer, and emerges as gradient updates that incrementally improve the model&#39;s language capabilities. Like any efficient factory, each stage has specific responsibilities and quality controls to ensure the final product meets our standards.</p>\n<p>The training pipeline encompasses four critical stages: tokenization converts raw text into discrete tokens our model can process, data loading organizes these tokens into efficient training batches, the language modeling objective defines what constitutes &quot;correct&quot; predictions, and the training loop orchestrates the entire learning process. Each stage presents unique challenges that can make or break the entire training effort.</p>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Ftraining-flow.svg\" alt=\"Training Pipeline Sequence\"></p>\n<p>The complexity lies not just in implementing each component correctly, but in ensuring they work harmoniously together. A subtle bug in tokenization can propagate through the entire pipeline, manifesting as mysterious training failures hours later. Similarly, improper label shifting or gradient clipping can transform a promising training run into a frustrating plateau.</p>\n<h3 id=\"tokenization-approach\">Tokenization Approach</h3>\n<p>The tokenization strategy fundamentally shapes how our transformer perceives and processes language. This seemingly simple preprocessing step—converting text strings into integer sequences—involves profound trade-offs that affect model capabilities, training efficiency, and final performance.</p>\n<p>Consider tokenization as the process of choosing a vocabulary for communication between humans and our transformer. Just as different human languages carve up the space of possible meanings differently, different tokenization strategies carve up the space of text differently. Some approaches favor fine-grained control with character-level tokens, while others prioritize efficiency with subword units.</p>\n<blockquote>\n<p><strong>Decision: Character-Level vs Subword Tokenization</strong></p>\n<ul>\n<li><strong>Context</strong>: Our transformer needs a way to convert arbitrary text into fixed-vocabulary token sequences. We must choose between character-level tokenization (each character becomes a token) and subword tokenization (common character sequences become tokens).</li>\n<li><strong>Options Considered</strong>: Character-level tokenization, Byte-Pair Encoding (BPE), SentencePiece subword tokenization</li>\n<li><strong>Decision</strong>: Character-level tokenization for initial implementation</li>\n<li><strong>Rationale</strong>: Character-level tokenization provides simplicity for learning purposes—no complex subword training, deterministic encoding/decoding, and universal applicability to any language. While less efficient than subword approaches, it eliminates an entire class of preprocessing complexity.</li>\n<li><strong>Consequences</strong>: Longer sequences for the same text (affecting memory usage), but simpler implementation and debugging. Can upgrade to subword tokenization later without changing model architecture.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Tokenization Approach</th>\n<th>Vocabulary Size</th>\n<th>Sequence Length</th>\n<th>Implementation Complexity</th>\n<th>Language Coverage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Character-level</td>\n<td>~100-500</td>\n<td>Long (5-10x words)</td>\n<td>Simple</td>\n<td>Universal</td>\n</tr>\n<tr>\n<td>Byte-Pair Encoding</td>\n<td>~10k-50k</td>\n<td>Medium (1-2x words)</td>\n<td>Complex training</td>\n<td>Language-specific</td>\n</tr>\n<tr>\n<td>SentencePiece</td>\n<td>~10k-50k</td>\n<td>Medium (1-2x words)</td>\n<td>Complex training</td>\n<td>Configurable</td>\n</tr>\n</tbody></table>\n<p>The <code>SimpleTokenizer</code> serves as our interface to the tokenization system, providing clean separation between text processing and model training. This abstraction allows us to experiment with different tokenization strategies without modifying downstream components.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>encode</code></td>\n<td>text: str</td>\n<td>List[int]</td>\n<td>Converts text string to integer token sequence</td>\n</tr>\n<tr>\n<td><code>decode</code></td>\n<td>tokens: List[int]</td>\n<td>str</td>\n<td>Converts integer token sequence back to text</td>\n</tr>\n<tr>\n<td><code>vocab_size</code></td>\n<td>None</td>\n<td>int</td>\n<td>Returns the size of the token vocabulary</td>\n</tr>\n<tr>\n<td><code>pad_token_id</code></td>\n<td>None</td>\n<td>int</td>\n<td>Returns the token ID used for padding sequences</td>\n</tr>\n<tr>\n<td><code>eos_token_id</code></td>\n<td>None</td>\n<td>int</td>\n<td>Returns the token ID used to mark end of sequence</td>\n</tr>\n</tbody></table>\n<p>Character-level tokenization operates through a straightforward character-to-integer mapping. We construct a vocabulary containing all unique characters in our training corpus, plus special tokens for padding, end-of-sequence, and unknown characters. Each character maps to a unique integer ID, and tokenization becomes a simple lookup operation.</p>\n<p>The encoding process follows these steps:</p>\n<ol>\n<li>Initialize vocabulary with special tokens (PAD=0, EOS=1, UNK=2)</li>\n<li>Scan training corpus to identify all unique characters</li>\n<li>Assign each character a unique integer ID starting from 3</li>\n<li>Store character-to-ID and ID-to-character mappings</li>\n<li>For encoding: map each character to its ID, handling unknowns as UNK token</li>\n<li>For decoding: map each ID back to its character, concatenating to form text</li>\n</ol>\n<blockquote>\n<p>Character-level tokenization&#39;s greatest strength is its universality—given enough training data, a character-level model can theoretically represent any text in its character set. However, this comes at the cost of longer sequences and potentially slower learning of word-level patterns.</p>\n</blockquote>\n<p>Special token handling requires careful consideration. The padding token (<code>pad_token_id</code>) fills shorter sequences to match batch dimensions, the end-of-sequence token (<code>eos_token_id</code>) signals natural text boundaries, and the unknown token handles characters absent from the vocabulary. These special tokens need reserved positions in the vocabulary and special handling during training and generation.</p>\n<p>For robust tokenization, we must handle edge cases: empty strings, strings containing only whitespace, strings with characters outside our vocabulary, and very long strings that exceed our maximum sequence length. Each case requires specific handling to prevent downstream errors.</p>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Ftraining-states.svg\" alt=\"Model Training States\"></p>\n<h3 id=\"data-loading-and-batching\">Data Loading and Batching</h3>\n<p>Efficient data loading transforms our tokenized text into training batches that maximize GPU utilization while preserving the sequential structure necessary for language modeling. This stage must balance memory efficiency, computational throughput, and proper sequence alignment for next-token prediction.</p>\n<p>Think of data loading as organizing a library for optimal research efficiency. Rather than reading books one word at a time, we want to arrange text into convenient chunks that researchers (our model) can process in parallel. However, unlike a physical library, our digital library must maintain the sequential flow of language while enabling parallel processing across multiple sequences.</p>\n<p>The <code>TextDataset</code> serves as our primary data structure, implementing PyTorch&#39;s <code>Dataset</code> interface to integrate seamlessly with PyTorch&#39;s data loading ecosystem. This dataset handles the complexities of sequence windowing, overlap management, and proper label creation.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>__init__</code></td>\n<td>text_data: List[int], seq_length: int, stride: int</td>\n<td>None</td>\n<td>Initialize dataset with tokenized text and windowing parameters</td>\n</tr>\n<tr>\n<td><code>__len__</code></td>\n<td>None</td>\n<td>int</td>\n<td>Return number of sequences available in dataset</td>\n</tr>\n<tr>\n<td><code>__getitem__</code></td>\n<td>idx: int</td>\n<td>Tuple[torch.Tensor, torch.Tensor]</td>\n<td>Return input sequence and target labels for given index</td>\n</tr>\n<tr>\n<td><code>create_windows</code></td>\n<td>None</td>\n<td>List[Tuple[torch.Tensor, torch.Tensor]]</td>\n<td>Split text into overlapping sequence windows</td>\n</tr>\n</tbody></table>\n<p>The windowing strategy determines how we extract fixed-length training sequences from variable-length text. We slide a window of size <code>seq_length</code> across our tokenized text, creating overlapping sequences that maximize data utilization while maintaining sequential coherence.</p>\n<p>Sequence windowing algorithm:</p>\n<ol>\n<li>Start with concatenated tokenized text from all training documents</li>\n<li>Initialize window start position at index 0</li>\n<li>Extract sequence from current position to position + seq_length</li>\n<li>Create input sequence from tokens[pos:pos+seq_length]</li>\n<li>Create target sequence from tokens[pos+1:pos+seq_length+1] (shifted by 1)</li>\n<li>Add (input, target) pair to dataset</li>\n<li>Advance window start position by stride amount</li>\n<li>Repeat until insufficient tokens remain for full sequence</li>\n</ol>\n<p>The stride parameter controls overlap between consecutive windows. A stride equal to <code>seq_length</code> creates non-overlapping windows, maximizing data efficiency but potentially losing context at boundaries. A smaller stride creates overlapping windows, improving context continuity but increasing computational cost due to repeated processing.</p>\n<blockquote>\n<p><strong>Decision: Sequence Windowing Stride</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to decide how much to advance the window between consecutive training sequences. This affects both data efficiency and context preservation.</li>\n<li><strong>Options Considered</strong>: Stride = seq_length (non-overlapping), stride = seq_length//2 (50% overlap), stride = 1 (maximum overlap)</li>\n<li><strong>Decision</strong>: Stride = seq_length//4 (75% overlap)</li>\n<li><strong>Rationale</strong>: Provides good balance between context preservation and computational efficiency. Overlapping windows help the model see how sequences transition while not being excessively redundant.</li>\n<li><strong>Consequences</strong>: 4x more training sequences than non-overlapping, but better context learning. Increases training time but improves model quality.</li>\n</ul>\n</blockquote>\n<p>Label shifting represents the most critical aspect of sequence preparation for language modeling. The fundamental insight is that for next-token prediction, the target for each input token is the following token in the sequence. This creates an off-by-one relationship between inputs and targets.</p>\n<p>Proper label shifting works as follows:</p>\n<ul>\n<li>Input sequence: [token₁, token₂, token₃, ..., tokenₙ]</li>\n<li>Target sequence: [token₂, token₃, token₄, ..., tokenₙ₊₁]</li>\n<li>The model predicts token₂ given token₁, token₃ given [token₁, token₂], and so forth</li>\n</ul>\n<p>Batching multiple sequences together requires careful attention to padding and masking. When sequences in a batch have different lengths, we pad shorter sequences with the <code>pad_token_id</code>. However, we must ensure the loss function ignores padded positions to prevent the model from learning spurious patterns from padding tokens.</p>\n<p>The <code>DataLoader</code> configuration affects both training efficiency and model performance. Key parameters include batch size (affecting GPU memory usage and gradient noise), shuffling (preventing overfitting to sequence order), and the number of worker processes (affecting data loading speed).</p>\n<table>\n<thead>\n<tr>\n<th>DataLoader Parameter</th>\n<th>Value</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>batch_size</code></td>\n<td>16-32</td>\n<td>Balance between GPU memory and gradient stability</td>\n</tr>\n<tr>\n<td><code>shuffle</code></td>\n<td>True</td>\n<td>Prevent overfitting to document order</td>\n</tr>\n<tr>\n<td><code>num_workers</code></td>\n<td>4-8</td>\n<td>Parallel data loading to keep GPU busy</td>\n</tr>\n<tr>\n<td><code>drop_last</code></td>\n<td>True</td>\n<td>Ensure consistent batch sizes for training</td>\n</tr>\n<tr>\n<td><code>pin_memory</code></td>\n<td>True</td>\n<td>Faster GPU transfer on CUDA systems</td>\n</tr>\n</tbody></table>\n<p>Batch composition requires attention to sequence boundaries. Ideally, we want each batch to contain sequences from different documents to maximize diversity. However, for simplicity, we can randomly sample sequences regardless of their source document, relying on shuffling to provide adequate mixing.</p>\n<h3 id=\"language-modeling-objective\">Language Modeling Objective</h3>\n<p>The language modeling objective transforms our transformer&#39;s raw output logits into a learning signal that drives parameter optimization. This objective function defines what constitutes &quot;correct&quot; behavior and shapes how our model learns to predict the next token in a sequence.</p>\n<p>Consider the language modeling objective as the grading system for our transformer student. Just as a teacher evaluates student responses against correct answers, our objective function evaluates the transformer&#39;s token predictions against the actual next tokens in our training sequences. The &quot;grades&quot; (losses) guide the optimization process toward better predictions.</p>\n<p>Language modeling treats each position in a sequence as an independent prediction task. Given tokens [token₁, token₂, ..., tokenᵢ], the model must predict tokenᵢ₊₁. This creates multiple supervised learning tasks within each sequence, dramatically increasing the learning signal from limited data.</p>\n<p>The <strong>cross-entropy loss</strong> serves as our primary objective function, measuring the difference between the model&#39;s predicted probability distribution over tokens and the true next token. Cross-entropy loss penalizes confident wrong predictions more heavily than uncertain wrong predictions, encouraging the model to express appropriate confidence levels.</p>\n<p>Cross-entropy computation proceeds as follows:</p>\n<ol>\n<li>Model outputs logits of shape [batch_size, seq_length, vocab_size]</li>\n<li>Apply softmax to logits to get probability distributions: P(token) = exp(logit) / Σ exp(logits)</li>\n<li>For each position, extract the probability assigned to the correct next token</li>\n<li>Compute negative log probability: loss = -log(P(correct_token))</li>\n<li>Average losses across all positions and sequences in the batch</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Loss Component</th>\n<th>Mathematical Formula</th>\n<th>Intuitive Meaning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Position Loss</td>\n<td>-log(P(correct_token))</td>\n<td>Penalty for assigning low probability to correct token</td>\n</tr>\n<tr>\n<td>Sequence Loss</td>\n<td>Σ position_losses / seq_length</td>\n<td>Average penalty across all positions in sequence</td>\n</tr>\n<tr>\n<td>Batch Loss</td>\n<td>Σ sequence_losses / batch_size</td>\n<td>Average penalty across all sequences in batch</td>\n</tr>\n</tbody></table>\n<p>The logarithmic nature of cross-entropy loss creates desirable properties for optimization. When the model assigns high probability (close to 1.0) to the correct token, the loss approaches zero. When the model assigns low probability to the correct token, the loss grows large, creating strong gradients that drive learning.</p>\n<p>Masking padded positions becomes crucial when computing loss over batched sequences. Padded tokens don&#39;t represent real language and shouldn&#39;t contribute to the loss computation. We achieve this by setting the loss to zero for padded positions or by excluding them from the loss computation entirely.</p>\n<p>Loss masking implementation:</p>\n<ol>\n<li>Create boolean mask identifying non-padded positions: mask = (targets != pad_token_id)</li>\n<li>Compute raw cross-entropy loss for all positions</li>\n<li>Apply mask to zero out losses for padded positions: masked_loss = loss * mask</li>\n<li>Sum masked losses and divide by number of non-padded positions for proper averaging</li>\n</ol>\n<blockquote>\n<p>The language modeling objective&#39;s elegance lies in its simplicity—by learning to predict the next token, the model implicitly learns grammar, semantics, world knowledge, and reasoning patterns present in the training data. This simple objective enables the emergence of complex language understanding.</p>\n</blockquote>\n<p><strong>Teacher forcing</strong> represents a key training strategy where we feed the model the actual previous tokens rather than its own predictions during training. This accelerates learning by providing stable, correct context for each prediction task. During generation, however, the model must use its own predictions (autoregressive generation), creating a train-test mismatch that can affect performance.</p>\n<p>The objective function must handle special tokens appropriately. End-of-sequence tokens provide natural boundaries for text completion tasks, while padding tokens should be masked out of loss computation. Some implementations also mask certain special tokens to prevent the model from over-relying on them.</p>\n<p>Regularization through the objective function can help prevent overfitting. Label smoothing replaces hard targets (probability 1.0 for correct token, 0.0 for others) with soft targets (probability 0.9 for correct token, 0.1/vocab_size for others), encouraging the model to be less confident and more robust.</p>\n<h3 id=\"training-loop-design\">Training Loop Design</h3>\n<p>The training loop orchestrates the entire learning process, coordinating forward passes, loss computation, backward passes, and parameter updates across thousands of iterations. This loop must balance computational efficiency, numerical stability, and training progress monitoring while gracefully handling various failure modes.</p>\n<p>Think of the training loop as the conductor of a complex orchestra. Each component—data loading, model forward pass, loss computation, backpropagation, and optimization—must execute in precise timing and coordination. The conductor (training loop) ensures smooth transitions between movements while monitoring the overall performance and making adjustments as needed.</p>\n<p>The training loop operates as a nested iteration structure: epochs contain batches, and each batch requires a complete forward-backward cycle. This structure allows for progress tracking at multiple granularities and provides natural checkpoints for saving model state and evaluating performance.</p>\n<p>Training loop algorithm:</p>\n<ol>\n<li>Initialize model with random weights using <code>initialize_transformer_weights</code></li>\n<li>Initialize optimizer (AdamW) with learning rate and weight decay</li>\n<li>Initialize learning rate scheduler for warmup and decay</li>\n<li>For each epoch from 1 to num_epochs:\na. Set model to training mode (enables dropout and batch norm)\nb. Shuffle dataset to ensure different batch composition\nc. For each batch in DataLoader:\n   i. Move batch to GPU device if available\n   ii. Zero gradients from previous batch: optimizer.zero_grad()\n   iii. Forward pass: logits = model(input_sequences)\n   iv. Compute cross-entropy loss with proper masking\n   v. Backward pass: loss.backward()\n   vi. Gradient clipping: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n   vii. Optimizer step: optimizer.step()\n   viii. Learning rate scheduler step: scheduler.step()\n   ix. Log metrics if at logging interval\nd. Evaluate on validation set if at evaluation interval\ne. Save model checkpoint if at save interval</li>\n</ol>\n<p>The <code>TrainingConfig</code> centralizes all training hyperparameters, making experiments reproducible and configuration management straightforward. This configuration object contains both optimization parameters and training logistics.</p>\n<table>\n<thead>\n<tr>\n<th>TrainingConfig Field</th>\n<th>Type</th>\n<th>Typical Value</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>learning_rate</code></td>\n<td>float</td>\n<td>3e-4</td>\n<td>Base learning rate for optimizer</td>\n</tr>\n<tr>\n<td><code>batch_size</code></td>\n<td>int</td>\n<td>32</td>\n<td>Number of sequences per training batch</td>\n</tr>\n<tr>\n<td><code>num_epochs</code></td>\n<td>int</td>\n<td>10-100</td>\n<td>Number of complete passes through dataset</td>\n</tr>\n<tr>\n<td><code>gradient_clip_norm</code></td>\n<td>float</td>\n<td>1.0</td>\n<td>Maximum gradient norm before clipping</td>\n</tr>\n<tr>\n<td><code>weight_decay</code></td>\n<td>float</td>\n<td>0.1</td>\n<td>L2 regularization strength</td>\n</tr>\n<tr>\n<td><code>warmup_steps</code></td>\n<td>int</td>\n<td>2000</td>\n<td>Steps for learning rate warmup</td>\n</tr>\n<tr>\n<td><code>save_interval</code></td>\n<td>int</td>\n<td>1000</td>\n<td>Steps between model checkpoints</td>\n</tr>\n<tr>\n<td><code>log_interval</code></td>\n<td>int</td>\n<td>100</td>\n<td>Steps between progress logging</td>\n</tr>\n<tr>\n<td><code>eval_interval</code></td>\n<td>int</td>\n<td>500</td>\n<td>Steps between validation evaluations</td>\n</tr>\n</tbody></table>\n<p><strong>Gradient clipping</strong> prevents exploding gradients that can destabilize training. When gradients become too large (indicating the optimization step would be too aggressive), we scale them down to a maximum norm. This maintains the gradient direction while limiting step size.</p>\n<p>The gradient clipping process:</p>\n<ol>\n<li>Compute the L2 norm of all gradients: grad_norm = sqrt(Σ grad²)</li>\n<li>If grad_norm &gt; gradient_clip_norm:\na. Compute scaling factor: scale = gradient_clip_norm / grad_norm\nb. Scale all gradients: grad = grad * scale</li>\n<li>Proceed with optimizer step using clipped gradients</li>\n</ol>\n<p><strong>Learning rate scheduling</strong> adapts the learning rate during training to improve convergence. The warmup phase starts with a very low learning rate and gradually increases it to the base learning rate over the first few thousand steps. This prevents early training instability when gradients are large and unpredictable.</p>\n<p>Popular learning rate schedules include:</p>\n<ul>\n<li>Linear warmup followed by cosine decay</li>\n<li>Linear warmup followed by linear decay</li>\n<li>Linear warmup followed by constant learning rate</li>\n<li>Exponential decay with warmup</li>\n</ul>\n<blockquote>\n<p><strong>Decision: Learning Rate Schedule</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to choose how the learning rate evolves during training. Different schedules affect convergence speed and final performance.</li>\n<li><strong>Options Considered</strong>: Constant learning rate, linear warmup + cosine decay, linear warmup + linear decay</li>\n<li><strong>Decision</strong>: Linear warmup (2000 steps) followed by cosine decay</li>\n<li><strong>Rationale</strong>: Warmup prevents early instability, cosine decay provides smooth convergence to near-zero learning rate, widely successful in transformer training</li>\n<li><strong>Consequences</strong>: More complex scheduling logic, but better training dynamics and potentially higher final performance</li>\n</ul>\n</blockquote>\n<p><strong>Checkpointing</strong> saves model state at regular intervals, providing insurance against training interruptions and enabling model evaluation at different training stages. Effective checkpointing saves not just model weights but also optimizer state, learning rate scheduler state, and training progress metrics.</p>\n<p>Checkpoint content should include:</p>\n<ul>\n<li>Model state dictionary (all model parameters)</li>\n<li>Optimizer state dictionary (momentum terms, etc.)</li>\n<li>Learning rate scheduler state</li>\n<li>Current epoch and step count</li>\n<li>Training loss history</li>\n<li>Validation metrics history</li>\n<li>Random number generator states for reproducibility</li>\n</ul>\n<p><strong>Mixed precision training</strong> can significantly speed up training and reduce memory usage on modern GPUs. By using 16-bit floating point for most computations while keeping 32-bit precision for critical operations like loss computation, we can achieve faster training with minimal accuracy impact.</p>\n<p><strong>Distributed training</strong> considerations become important for larger models and datasets. While our initial implementation targets single-GPU training, designing the training loop with clean interfaces makes future scaling easier. Key considerations include gradient synchronization, batch size scaling, and checkpoint coordination across multiple processes.</p>\n<p>Progress monitoring requires careful attention to which metrics to track and how frequently to evaluate them. Training loss should be logged frequently (every 100 steps) as it&#39;s computationally cheap. Validation loss requires a full forward pass through the validation set, so it&#39;s computed less frequently (every 500-1000 steps). Generation quality evaluation is expensive and might only be performed once per epoch.</p>\n<table>\n<thead>\n<tr>\n<th>Monitoring Metric</th>\n<th>Frequency</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training Loss</td>\n<td>Every 100 steps</td>\n<td>Track learning progress</td>\n</tr>\n<tr>\n<td>Training Perplexity</td>\n<td>Every 100 steps</td>\n<td>Interpretable loss measure</td>\n</tr>\n<tr>\n<td>Validation Loss</td>\n<td>Every 500 steps</td>\n<td>Detect overfitting</td>\n</tr>\n<tr>\n<td>Validation Perplexity</td>\n<td>Every 500 steps</td>\n<td>Comparable across models</td>\n</tr>\n<tr>\n<td>Gradient Norms</td>\n<td>Every 100 steps</td>\n<td>Monitor training stability</td>\n</tr>\n<tr>\n<td>Learning Rate</td>\n<td>Every 100 steps</td>\n<td>Verify schedule correctness</td>\n</tr>\n<tr>\n<td>Generation Samples</td>\n<td>Every 1000 steps</td>\n<td>Qualitative assessment</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Training pipelines present numerous opportunities for subtle bugs that can sabotage the entire learning process. These pitfalls often manifest as mysterious training failures, poor convergence, or degraded model quality, making them particularly frustrating to debug.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Label Shifting</strong>\nThe most common and destructive error in language model training is incorrect label shifting. This occurs when the target tokens don&#39;t properly correspond to the &quot;next&quot; tokens for each input position. For example, using the same sequence for both inputs and targets (no shift), shifting in the wrong direction, or shifting by the wrong amount. This completely breaks the learning signal, as the model learns to predict irrelevant tokens. To fix this, ensure targets are always inputs shifted forward by exactly one position: targets = inputs[1:] with appropriate handling of sequence boundaries.</p>\n<p>⚠️ <strong>Pitfall: Padding Tokens in Loss Computation</strong>\nIncluding padding tokens in loss computation teaches the model to predict padding tokens as likely next tokens in real text, degrading generation quality. This happens when using sequences of different lengths without proper masking in the loss function. The model learns that padding tokens are common &quot;words,&quot; leading to generations filled with padding characters. Fix this by creating a boolean mask identifying non-padding positions and applying it before averaging the loss: <code>loss = loss * mask</code> followed by <code>loss = loss.sum() / mask.sum()</code>.</p>\n<p>⚠️ <strong>Pitfall: Missing Gradient Clipping</strong>\nLarge gradients can destabilize training, causing loss to explode or oscillate wildly. This often occurs early in training when the model&#39;s predictions are random and gradients are consequently large and unpredictable. Without gradient clipping, a single bad batch can push parameters to extreme values that take many steps to recover from. Implement gradient clipping with <code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code> before every optimizer step.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Loss Averaging</strong>\nComputing loss incorrectly by averaging over wrong dimensions or including padded positions in the average leads to biased training signals. This commonly occurs when using PyTorch&#39;s default reduction behavior without accounting for variable sequence lengths or padding. The model may learn to favor shorter sequences or give undue weight to padding tokens. Ensure loss averaging only includes valid (non-padded) tokens by manually computing the average with proper masking.</p>\n<p>⚠️ <strong>Pitfall: Learning Rate Too High or Too Low</strong>\nAn inappropriate learning rate can prevent convergence entirely. Too high causes training loss to oscillate or explode, while too low results in extremely slow convergence or getting stuck in poor local minima. Start with the established learning rate of 3e-4 for transformer models, and use learning rate scheduling with warmup to ensure stable early training. Monitor training loss—if it&#39;s not decreasing after several hundred steps, the learning rate may be too low.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Train/Eval Modes</strong>\nForgetting to set model.train() during training or model.eval() during evaluation can lead to inconsistent behavior due to dropout and layer normalization differences. During training, dropout should be active and layer norm should use batch statistics. During evaluation, dropout should be disabled and layer norm should use running statistics. This affects both training dynamics and evaluation accuracy.</p>\n<p>⚠️ <strong>Pitfall: Batch Size and Memory Issues</strong>\nSetting batch size too large causes out-of-memory errors, while too small leads to noisy gradients and poor convergence. GPU memory usage scales with batch_size × seq_length × d_model, so all three factors affect memory consumption. Start with smaller batch sizes (8-16) and increase until you approach memory limits. Consider gradient accumulation if you need larger effective batch sizes than GPU memory allows.</p>\n<p>⚠️ <strong>Pitfall: Tokenizer Vocabulary Mismatch</strong>\nUsing a tokenizer vocabulary size that doesn&#39;t match the model&#39;s configured vocab_size parameter leads to dimension mismatches and training failures. This occurs when changing tokenization strategies without updating model configuration, or when loading pretrained tokenizers with different vocabulary sizes. Ensure the model&#39;s vocab_size exactly matches the tokenizer&#39;s vocabulary size, including all special tokens.</p>\n<p>⚠️ <strong>Pitfall: Checkpoint Corruption</strong>\nSaving checkpoints at inappropriate times (during gradient updates) or with incomplete state can corrupt training runs. This leads to training resumption failures or subtle performance degradation. Save checkpoints only after complete training steps, include optimizer and scheduler state for proper resumption, and verify checkpoint integrity by loading immediately after saving.</p>\n<p>⚠️ <strong>Pitfall: Data Loading Bottlenecks</strong>\nInsufficient data loading parallelization causes GPU utilization to drop as the model waits for new batches. This dramatically increases training time and can hide other performance issues. Use multiple DataLoader workers (num_workers=4-8), enable pin_memory for faster GPU transfer, and consider prefetching to keep the GPU consistently busy.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The training pipeline requires careful orchestration of multiple components working in harmony. For transformer training, we&#39;ll build upon PyTorch&#39;s data loading utilities while implementing custom logic for tokenization, loss computation, and training coordination.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenization</td>\n<td>Character-level with dict mapping</td>\n<td>HuggingFace Tokenizers with BPE</td>\n</tr>\n<tr>\n<td>Data Loading</td>\n<td>PyTorch DataLoader with custom Dataset</td>\n<td>WebDataset with streaming</td>\n</tr>\n<tr>\n<td>Loss Function</td>\n<td>PyTorch CrossEntropyLoss with masking</td>\n<td>Label smoothing + focal loss</td>\n</tr>\n<tr>\n<td>Optimization</td>\n<td>AdamW with cosine scheduling</td>\n<td>AdamW with custom warmup schedules</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Print statements with loss tracking</td>\n<td>Weights &amp; Biases or TensorBoard</td>\n</tr>\n<tr>\n<td>Checkpointing</td>\n<td>Simple state dict saving</td>\n<td>HuggingFace transformers format</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  src/\n    data/\n      tokenizer.py              ← SimpleTokenizer implementation\n      dataset.py               ← TextDataset and data loading utilities\n      preprocessing.py         ← Text cleaning and preparation\n    training/\n      trainer.py               ← Main training loop and coordination\n      loss.py                  ← Loss computation with masking\n      optimization.py          ← Optimizer and scheduler setup\n      checkpointing.py         ← Model saving and loading\n    models/\n      transformer.py           ← Core transformer architecture (from previous sections)\n      config.py               ← Configuration classes\n    utils/\n      monitoring.py           ← Training metrics and logging\n      reproducibility.py     ← Random seed management\n  data/\n    raw/                      ← Original text files\n    processed/                ← Tokenized and prepared datasets\n  checkpoints/                ← Saved model states\n  logs/                       ← Training logs and metrics</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Here&#39;s complete, ready-to-use tokenizer implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/data/tokenizer.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pickle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Counter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Character-level tokenizer for transformer training.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides clean interface for encoding/decoding text while handling</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    special tokens and unknown characters gracefully.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, pad_token: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"&#x3C;PAD>\"</span><span style=\"color:#E1E4E8\">, eos_token: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"&#x3C;EOS>\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 unk_token: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"&#x3C;UNK>\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pad_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pad_token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.eos_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> eos_token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.unk_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> unk_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize special token mappings</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.char_to_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pad_token: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            eos_token: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            unk_token: </span><span style=\"color:#79B8FF\">2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.id_to_char </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">: pad_token, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">: eos_token, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">: unk_token}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._vocab_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> build_vocab</span><span style=\"color:#E1E4E8\">(self, texts: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Build character vocabulary from training texts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char_counts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Counter()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> text </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> texts:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            char_counts.update(text)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add characters in frequency order for consistent vocab</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> char, _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> char_counts.most_common():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.char_to_id:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.char_to_id[char] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._vocab_size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.id_to_char[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._vocab_size] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._vocab_size </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> encode</span><span style=\"color:#E1E4E8\">(self, text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, add_eos: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert text to list of token IDs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> text:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tokens.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.char_to_id.get(char, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.char_to_id[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.unk_token]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> add_eos:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tokens.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.char_to_id[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.eos_token])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> decode</span><span style=\"color:#E1E4E8\">(self, token_ids: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">], skip_special_tokens: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert token IDs back to text.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        chars </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        special_ids </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.char_to_id[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pad_token], </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                      self</span><span style=\"color:#E1E4E8\">.char_to_id[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.eos_token]}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> token_id </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> token_ids:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> skip_special_tokens </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> token_id </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> special_ids:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            chars.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.id_to_char.get(token_id, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.unk_token))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> ''</span><span style=\"color:#E1E4E8\">.join(chars)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> vocab_size</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._vocab_size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pad_token_id</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.char_to_id[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pad_token]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> eos_token_id</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.char_to_id[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.eos_token]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save</span><span style=\"color:#E1E4E8\">(self, path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save tokenizer state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'char_to_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.char_to_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'id_to_char'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.id_to_char,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'vocab_size'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._vocab_size,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'special_tokens'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'pad_token'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pad_token,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'eos_token'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.eos_token,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'unk_token'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.unk_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(path, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            json.dump(state, f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(cls, path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'SimpleTokenizer'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load tokenizer from saved state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">state[</span><span style=\"color:#9ECBFF\">'special_tokens'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokenizer.char_to_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> state[</span><span style=\"color:#9ECBFF\">'char_to_id'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokenizer.id_to_char </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(k): v </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> state[</span><span style=\"color:#9ECBFF\">'id_to_char'</span><span style=\"color:#E1E4E8\">].items()}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tokenizer._vocab_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> state[</span><span style=\"color:#9ECBFF\">'vocab_size'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tokenizer</span></span></code></pre></div>\n\n<p>Here&#39;s the complete dataset implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/data/dataset.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.utils.data </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dataset, DataLoader</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TextDataset</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Dataset</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Dataset for autoregressive language modeling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Handles sequence windowing, label shifting, and proper batching</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    for next-token prediction training.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, token_ids: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">], seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, stride: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            token_ids: Flattened list of all training tokens</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            seq_length: Length of each training sequence</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stride: Step size between sequences (default: seq_length // 4)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.token_ids </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> token_ids</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.seq_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> seq_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stride </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stride </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, seq_length </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 75% overlap by default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pre-compute all valid sequence start positions</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sequence_starts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(token_ids) </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> seq_length, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.stride):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> seq_length </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(token_ids):  </span><span style=\"color:#6A737D\"># Need +1 for target</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.sequence_starts.append(i)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.sequence_starts) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Text too short for seq_length=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">seq_length</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __len__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.sequence_starts)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __getitem__</span><span style=\"color:#E1E4E8\">(self, idx: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return (input_sequence, target_sequence) pair.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_pos </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.sequence_starts[idx]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract input and target sequences with proper shifting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_seq </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.token_ids[start_pos:start_pos </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.seq_length]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        target_seq </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.token_ids[start_pos </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:start_pos </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.seq_length </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.tensor(input_seq, </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.long), torch.tensor(target_seq, </span><span style=\"color:#FFAB70\">dtype</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.long)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_data_loaders</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    train_tokens: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    val_tokens: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[DataLoader, DataLoader]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create train and validation data loaders.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    train_dataset </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TextDataset(train_tokens, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    val_dataset </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TextDataset(val_tokens, seq_length, </span><span style=\"color:#FFAB70\">stride</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">seq_length)  </span><span style=\"color:#6A737D\"># No overlap for validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    train_loader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DataLoader(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        train_dataset,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        batch_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">batch_size,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        shuffle</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        num_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">num_workers,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        drop_last</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        pin_memory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.cuda.is_available()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    val_loader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DataLoader(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        val_dataset,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        batch_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">batch_size,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        shuffle</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        num_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">num_workers,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        drop_last</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        pin_memory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.cuda.is_available()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> train_loader, val_loader</span></span></code></pre></div>\n\n<h4 id=\"core-training-loop-skeleton\">Core Training Loop Skeleton</h4>\n<p>This skeleton provides the training coordination logic you&#39;ll need to implement:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/training/trainer.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.utils.data </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DataLoader</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerTrainer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates transformer training with proper gradient handling and monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: nn.Module, config: </span><span style=\"color:#9ECBFF\">'TrainingConfig'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 tokenizer: </span><span style=\"color:#9ECBFF\">'SimpleTokenizer'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.device(</span><span style=\"color:#9ECBFF\">'cuda'</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'cpu'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Move model to device</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model.to(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.device)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize training components</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._create_optimizer()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scheduler </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._create_scheduler()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.criterion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.CrossEntropyLoss(</span><span style=\"color:#FFAB70\">ignore_index</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tokenizer.pad_token_id, </span><span style=\"color:#FFAB70\">reduction</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'none'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Training state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.step </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.epoch </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.best_val_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'inf'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> train</span><span style=\"color:#E1E4E8\">(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main training loop with proper error handling and monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to training mode and initialize metrics tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each epoch in range(config.num_epochs):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each batch in train_loader:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Move batch to device and extract input/target sequences  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Zero gradients: optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Forward pass: logits = model(input_sequences)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Compute masked loss using _compute_loss helper</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Backward pass: loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Clip gradients: torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Optimizer and scheduler steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Log metrics at config.log_interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Validate at config.eval_interval  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Save checkpoint at config.save_interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Update self.step counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use _compute_loss, _validate, _save_checkpoint helper methods</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_loss</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute cross-entropy loss with proper padding mask.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Reshape logits to [batch_size * seq_length, vocab_size]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Reshape targets to [batch_size * seq_length]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute per-position losses using self.criterion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create mask for non-padded positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply mask and average over valid positions only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: targets != self.tokenizer.pad_token_id creates the mask</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate</span><span style=\"color:#E1E4E8\">(self, val_loader: DataLoader) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute validation loss without gradient computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to eval mode and disable gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Iterate through validation batches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute losses and accumulate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return average validation loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Remember to set model back to train mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.no_grad() context manager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_optimizer</span><span style=\"color:#E1E4E8\">(self) -> torch.optim.Optimizer:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create AdamW optimizer with weight decay.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.optim.AdamW(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.model.parameters(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            lr</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.learning_rate,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            weight_decay</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.weight_decay,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            betas</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.9</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.95</span><span style=\"color:#E1E4E8\">),  </span><span style=\"color:#6A737D\"># Standard transformer betas</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            eps</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1e-8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_scheduler</span><span style=\"color:#E1E4E8\">(self) -> torch.optim.lr_scheduler._LRScheduler:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create learning rate scheduler with warmup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement linear warmup for config.warmup_steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Follow with cosine decay to training end  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use torch.optim.lr_scheduler.LambdaLR with custom lambda</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Combine linear warmup (lr = step / warmup_steps * base_lr) </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # with cosine decay (lr = 0.5 * (1 + cos(π * progress)))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _save_checkpoint</span><span style=\"color:#E1E4E8\">(self, val_loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, filepath: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save complete training state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create checkpoint dictionary with model, optimizer, scheduler state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Include current step, epoch, and best validation loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Save tokenizer state alongside model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Use torch.save to write checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Include everything needed to resume training exactly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_sample</span><span style=\"color:#E1E4E8\">(self, prompt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate text sample for qualitative evaluation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize prompt and move to device</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set model to eval mode  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate tokens autoregressively</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Decode and return generated text</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.no_grad() and model.eval()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>PyTorch Training Best Practices:</strong></p>\n<ul>\n<li>Use <code>torch.cuda.amp.autocast()</code> for automatic mixed precision training to speed up training and reduce memory usage</li>\n<li>Set <code>torch.backends.cudnn.benchmark = True</code> if input sizes are consistent to optimize CUDA operations</li>\n<li>Use <code>model.train()</code> before training loops and <code>model.eval()</code> before validation/generation</li>\n<li>Always use <code>torch.no_grad()</code> context manager during validation and generation to save memory</li>\n</ul>\n<p><strong>Memory Management:</strong></p>\n<ul>\n<li>Monitor GPU memory with <code>torch.cuda.memory_allocated()</code> and <code>torch.cuda.memory_reserved()</code></li>\n<li>Use gradient accumulation if desired batch size exceeds memory: accumulate gradients over multiple mini-batches before optimizer step</li>\n<li>Clear unnecessary variables with <code>del variable</code> or let them go out of scope in tight loops</li>\n<li>Consider using <code>torch.utils.checkpoint.checkpoint()</code> for memory-efficient gradient computation in very deep models</li>\n</ul>\n<p><strong>Numerical Stability:</strong></p>\n<ul>\n<li>Initialize transformer weights using scaled random initialization: <code>nn.init.normal_(tensor, mean=0, std=0.02)</code></li>\n<li>Use <code>torch.nn.utils.clip_grad_norm_()</code> with max_norm=1.0 to prevent gradient explosions</li>\n<li>Monitor gradient norms: <code>torch.nn.utils.clip_grad_norm_()</code> returns the computed norm before clipping</li>\n<li>Consider using <code>torch.nn.functional.cross_entropy()</code> instead of manual softmax + log for better numerical stability</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the training pipeline, verify correct behavior:</p>\n<p><strong>Step 1: Test Tokenizer</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.data.tokenizer import SimpleTokenizer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tokenizer = SimpleTokenizer()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tokenizer.build_vocab(['hello world', 'test text'])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tokens = tokenizer.encode('hello')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Tokens: {tokens}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Decoded: {tokenizer.decode(tokens)}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n<p>Expected: Tokens should be list of integers, decoded text should match input.</p>\n<p><strong>Step 2: Test Dataset</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.data.dataset import TextDataset</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tokens = list(range(100))  # Simple test data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">dataset = TextDataset(tokens, seq_length=10)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">input_seq, target_seq = dataset[0]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Input: {input_seq.tolist()[:5]}...')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Target: {target_seq.tolist()[:5]}...')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Correct shift: {target_seq[0] == input_seq[1]}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n<p>Expected: Target should be input shifted by 1 position.</p>\n<p><strong>Step 3: Training Loop Test</strong>\nRun training for a few steps on tiny dataset:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> train.py</span><span style=\"color:#79B8FF\"> --seq_length</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#79B8FF\"> --batch_size</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#79B8FF\"> --num_epochs</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#79B8FF\"> --save_interval</span><span style=\"color:#79B8FF\"> 50</span></span></code></pre></div>\n<p>Expected behaviors:</p>\n<ul>\n<li>Loss should be computed and printed (initial loss around ln(vocab_size) ≈ 4-6 for character-level)</li>\n<li>No CUDA out of memory errors</li>\n<li>Gradient norms should be reasonable (1.0-10.0 range initially)</li>\n<li>Model should save checkpoint after 50 steps</li>\n</ul>\n<p><strong>Signs of Problems:</strong></p>\n<ul>\n<li>Loss = NaN: Check for division by zero in loss computation or exploding gradients</li>\n<li>Loss not decreasing after 500+ steps: Learning rate too low, or incorrect label shifting</li>\n<li>CUDA OOM: Reduce batch_size or seq_length</li>\n<li>Very slow training: Increase num_workers in DataLoader, check data loading bottleneck</li>\n</ul>\n<h2 id=\"text-generation\">Text Generation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 - Text Generation (implementing autoregressive generation with various sampling strategies and KV caching optimization)</p>\n</blockquote>\n<p>Text generation represents the culmination of our transformer implementation—the point where our carefully crafted attention mechanisms, transformer blocks, and training pipeline converge to produce coherent text sequences. Unlike training, where we process entire sequences in parallel, text generation operates in an autoregressive manner, predicting one token at a time and using each prediction to inform the next. This fundamental shift from parallel processing to sequential generation introduces unique challenges around computational efficiency, sampling strategies, and maintaining coherence over long sequences.</p>\n<p>The text generation process transforms our transformer from a pattern recognition system into a creative engine capable of extending prompts, completing thoughts, and generating novel content. However, this transformation requires careful consideration of how we sample from probability distributions, how we cache intermediate computations for efficiency, and how we balance creativity with coherence in the generated output.</p>\n<h3 id=\"mental-model-iterative-prediction\">Mental Model: Iterative Prediction</h3>\n<p>Think of text generation like a careful storyteller who must decide each word one at a time, considering all the words that came before but never knowing what comes next until the decision is made. The storyteller has learned patterns from thousands of stories and can predict which word is most likely to come next, but still has creative freedom in making each choice.</p>\n<p>At each step, the transformer examines the entire sequence generated so far—from the initial prompt through all previously generated tokens—and produces a probability distribution over the entire vocabulary. This distribution represents the model&#39;s confidence about what should come next: perhaps 30% chance of &quot;the&quot;, 15% chance of &quot;a&quot;, 10% chance of &quot;and&quot;, and thousands of other possibilities with smaller probabilities. The generation process then samples from this distribution to select the actual next token.</p>\n<p>This process creates a feedback loop where each generated token influences all subsequent predictions. If we generate &quot;The cat&quot;, the model might strongly favor &quot;sat&quot; or &quot;ran&quot; as the next token. But if we had generated &quot;The dog&quot; instead, those same positions would favor different continuations. This dependency chain explains why text generation is inherently sequential—we cannot parallelize the generation of multiple tokens because each one depends on the concrete choices made for all previous positions.</p>\n<p>The iterative nature also explains why generation can sometimes produce repetitive or incoherent text. Early poor choices compound through the sequence, leading the model down paths that become increasingly difficult to recover from. Advanced sampling strategies help mitigate these issues by introducing controlled randomness that prevents the model from getting stuck in repetitive loops while maintaining overall coherence.</p>\n<p><strong>Decision: Autoregressive Generation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Transformers can theoretically generate multiple tokens in parallel, but maintaining coherence requires each token to depend on concrete previous choices</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Fully parallel generation with iterative refinement</li>\n<li>Autoregressive generation with caching optimizations</li>\n<li>Hybrid approach with parallel generation for independent positions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Pure autoregressive generation with KV caching</li>\n<li><strong>Rationale</strong>: Autoregressive generation provides the strongest guarantees of coherence and is the standard approach used by successful language models. The sequential dependency ensures each token is conditioned on the actual previous tokens rather than parallel predictions that might conflict</li>\n<li><strong>Consequences</strong>: Generation is slower than parallel approaches but produces more coherent text. Requires careful optimization through caching to achieve acceptable performance</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Generation Approach</th>\n<th>Coherence</th>\n<th>Speed</th>\n<th>Implementation Complexity</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parallel with refinement</td>\n<td>Medium</td>\n<td>High</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Autoregressive with caching</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Hybrid parallel/sequential</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Very High</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The autoregressive approach treats text generation as a sequential decision process where each token is sampled independently from a distribution conditioned on all previous tokens. This ensures maximum coherence but requires optimization strategies like KV caching to achieve practical performance.</p>\n<h3 id=\"sampling-strategy-design\">Sampling Strategy Design</h3>\n<p>Once our transformer produces a probability distribution over the vocabulary, we must decide how to select the next token from this distribution. This choice fundamentally shapes the character of the generated text—deterministic selection produces predictable but potentially repetitive text, while random sampling introduces creativity at the cost of potential incoherence. The art of text generation lies in balancing these competing objectives through sophisticated sampling strategies.</p>\n<p>The probability distribution produced by the transformer&#39;s final linear layer contains the raw logits for each vocabulary token. These logits represent the model&#39;s unnormalized confidence in each token, but they require transformation into proper probabilities through the softmax function. However, we can manipulate these logits before applying softmax to influence the resulting probability distribution and, consequently, the sampling behavior.</p>\n<p><strong>Decision: Multiple Sampling Strategy Support</strong></p>\n<ul>\n<li><strong>Context</strong>: Different applications require different trade-offs between creativity and predictability in generated text</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single greedy decoding for simplicity</li>\n<li>Temperature sampling only for controlled randomness</li>\n<li>Multiple strategies (greedy, temperature, top-k, top-p) for flexibility</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement multiple complementary sampling strategies</li>\n<li><strong>Rationale</strong>: Different use cases demand different sampling behaviors. Code completion benefits from greedy decoding, creative writing needs temperature sampling, and chatbots work well with top-p sampling. Supporting multiple strategies allows users to choose the appropriate method</li>\n<li><strong>Consequences</strong>: Increased implementation complexity but much greater flexibility for different applications</li>\n</ul>\n<h4 id=\"greedy-decoding\">Greedy Decoding</h4>\n<p>Greedy decoding represents the simplest sampling strategy: always select the token with the highest probability at each step. This deterministic approach produces the same output given identical inputs, making it valuable for applications requiring predictable behavior like code completion or factual question answering.</p>\n<p>The greedy strategy operates by applying <code>torch.argmax()</code> to the logits after softmax normalization, identifying the single most likely token according to the model&#39;s learned patterns. This approach maximizes the likelihood of each individual token choice, though it does not necessarily maximize the likelihood of the entire generated sequence due to the sequential dependencies in autoregressive generation.</p>\n<table>\n<thead>\n<tr>\n<th>Greedy Decoding Characteristics</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Determinism</td>\n<td>Fully deterministic</td>\n</tr>\n<tr>\n<td>Creativity</td>\n<td>Low - always most likely choice</td>\n</tr>\n<tr>\n<td>Coherence</td>\n<td>High - follows strongest patterns</td>\n</tr>\n<tr>\n<td>Repetition Risk</td>\n<td>High - can get stuck in loops</td>\n</tr>\n<tr>\n<td>Best Use Cases</td>\n<td>Code completion, factual answers</td>\n</tr>\n<tr>\n<td>Implementation Complexity</td>\n<td>Very low</td>\n</tr>\n</tbody></table>\n<p>Greedy decoding often produces highly coherent short sequences but struggles with longer generations where the deterministic nature can lead to repetitive patterns. The model may generate the same phrases repeatedly because the highest-probability continuation often cycles back to similar contexts.</p>\n<h4 id=\"temperature-based-sampling\">Temperature-Based Sampling</h4>\n<p>Temperature sampling introduces controlled randomness by scaling the logits before applying softmax normalization. The temperature parameter τ (tau) controls how &quot;sharp&quot; or &quot;flat&quot; the probability distribution becomes: lower temperatures make the distribution more peaked around high-probability tokens, while higher temperatures flatten the distribution to make all tokens more equally likely.</p>\n<p>The mathematical transformation applies the temperature by dividing all logits by τ before softmax: <code>P(token) = softmax(logits / temperature)</code>. When temperature equals 1.0, the distribution remains unchanged. Values below 1.0 (like 0.7) sharpen the distribution, making high-probability tokens even more likely. Values above 1.0 (like 1.5) flatten the distribution, increasing the likelihood of selecting lower-probability tokens.</p>\n<table>\n<thead>\n<tr>\n<th>Temperature Value</th>\n<th>Effect</th>\n<th>Typical Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0.1 - 0.5</td>\n<td>Very focused, near-deterministic</td>\n<td>Technical writing, factual content</td>\n</tr>\n<tr>\n<td>0.6 - 0.8</td>\n<td>Moderately focused, some variation</td>\n<td>General-purpose text generation</td>\n</tr>\n<tr>\n<td>0.9 - 1.1</td>\n<td>Balanced creativity and coherence</td>\n<td>Creative writing, dialogue</td>\n</tr>\n<tr>\n<td>1.2 - 2.0</td>\n<td>High creativity, less coherent</td>\n<td>Brainstorming, experimental text</td>\n</tr>\n<tr>\n<td>2.0+</td>\n<td>Very random, potentially incoherent</td>\n<td>Debugging, exploration</td>\n</tr>\n</tbody></table>\n<p>Temperature sampling provides a simple but effective way to control the creativity-coherence trade-off. The strategy samples from the temperature-adjusted distribution using <code>torch.multinomial()</code>, ensuring that even low-probability tokens have a chance of selection when temperatures are higher.</p>\n<h4 id=\"top-k-sampling\">Top-k Sampling</h4>\n<p>Top-k sampling addresses a key limitation of temperature sampling: it still considers the entire vocabulary, including tokens that are contextually inappropriate or nonsensical. By restricting sampling to only the k most likely tokens, top-k sampling prevents the selection of highly improbable tokens while maintaining diversity among reasonable choices.</p>\n<p>The algorithm first identifies the k tokens with highest logits, sets all other logits to negative infinity (effectively zero probability), then applies temperature and samples from this restricted distribution. This approach ensures that sampling never selects tokens outside the top-k most likely options, regardless of the temperature setting.</p>\n<table>\n<thead>\n<tr>\n<th>Top-k Value</th>\n<th>Vocabulary Restriction</th>\n<th>Generation Character</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>k = 1</td>\n<td>Single token (greedy)</td>\n<td>Deterministic</td>\n</tr>\n<tr>\n<td>k = 5-10</td>\n<td>Very restricted</td>\n<td>Conservative, highly coherent</td>\n</tr>\n<tr>\n<td>k = 20-50</td>\n<td>Moderately restricted</td>\n<td>Balanced creativity and coherence</td>\n</tr>\n<tr>\n<td>k = 100-200</td>\n<td>Lightly restricted</td>\n<td>High creativity, filters extreme outliers</td>\n</tr>\n<tr>\n<td>k = vocab_size</td>\n<td>No restriction</td>\n<td>Equivalent to pure temperature sampling</td>\n</tr>\n</tbody></table>\n<p>Top-k sampling works particularly well for technical domains where terminology is specific and most vocabulary tokens would be inappropriate in context. However, the fixed k value can be problematic when the confidence distribution varies significantly across different contexts—sometimes the model is very confident and only needs k=5 options, while other times it&#39;s uncertain and benefits from k=50 choices.</p>\n<h4 id=\"top-p-nucleus-sampling\">Top-p (Nucleus) Sampling</h4>\n<p>Top-p sampling, also known as nucleus sampling, provides an adaptive solution to top-k&#39;s fixed restriction problem. Instead of selecting a fixed number of tokens, top-p dynamically determines how many tokens to include based on their cumulative probability mass. The algorithm includes the minimum number of tokens needed to reach a cumulative probability of p.</p>\n<p>The process sorts tokens by probability in descending order, then includes tokens until their cumulative probability reaches the threshold p. If the most likely token has probability 0.8 and p=0.9, only one additional token is needed to cross the threshold. But if probabilities are more evenly distributed, many more tokens might be included to reach the same cumulative probability.</p>\n<table>\n<thead>\n<tr>\n<th>Top-p Value</th>\n<th>Probability Mass</th>\n<th>Typical Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>p = 0.1-0.3</td>\n<td>Very conservative</td>\n<td>Near-deterministic, high confidence only</td>\n</tr>\n<tr>\n<td>p = 0.5-0.7</td>\n<td>Moderately selective</td>\n<td>Good balance of safety and creativity</td>\n</tr>\n<tr>\n<td>p = 0.8-0.9</td>\n<td>Standard setting</td>\n<td>Most common choice for general use</td>\n</tr>\n<tr>\n<td>p = 0.95-0.99</td>\n<td>Very inclusive</td>\n<td>High creativity, includes low-probability options</td>\n</tr>\n<tr>\n<td>p = 1.0</td>\n<td>No restriction</td>\n<td>Equivalent to pure temperature sampling</td>\n</tr>\n</tbody></table>\n<p>Top-p sampling adapts naturally to the model&#39;s confidence level. When the model is highly confident about the next token, the nucleus remains small and generation stays focused. When the model is uncertain, the nucleus expands to include more options, allowing for greater creativity precisely when the model lacks strong preferences.</p>\n<p><strong>The <code>GenerationConfig</code> structure captures all sampling parameters:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>temperature</code></td>\n<td>float</td>\n<td>Temperature scaling factor (0.1 = focused, 2.0 = creative)</td>\n</tr>\n<tr>\n<td><code>top_k</code></td>\n<td>int</td>\n<td>Maximum number of tokens to consider (0 = disabled)</td>\n</tr>\n<tr>\n<td><code>top_p</code></td>\n<td>float</td>\n<td>Cumulative probability threshold for nucleus sampling</td>\n</tr>\n<tr>\n<td><code>max_length</code></td>\n<td>int</td>\n<td>Maximum sequence length including prompt</td>\n</tr>\n<tr>\n<td><code>pad_token_id</code></td>\n<td>int</td>\n<td>Token ID used for padding in batched generation</td>\n</tr>\n<tr>\n<td><code>eos_token_id</code></td>\n<td>int</td>\n<td>Token ID that signals end of sequence</td>\n</tr>\n<tr>\n<td><code>repetition_penalty</code></td>\n<td>float</td>\n<td>Penalty factor for recently used tokens</td>\n</tr>\n<tr>\n<td><code>length_penalty</code></td>\n<td>float</td>\n<td>Penalty factor favoring longer or shorter sequences</td>\n</tr>\n</tbody></table>\n<p>The sampling strategies can be combined effectively: temperature adjustment shapes the overall distribution, top-k or top-p restricts the candidate set, and repetition penalties discourage cycling. This layered approach provides fine-grained control over generation behavior.</p>\n<h3 id=\"kv-cache-optimization\">KV Cache Optimization</h3>\n<p>The autoregressive nature of text generation creates a significant computational inefficiency: at each step, we must recompute attention for all previous tokens, even though their key and value representations haven&#39;t changed. For a sequence of length n, this means the first token&#39;s key and value are computed n times, the second token&#39;s n-1 times, and so on. This quadratic redundancy makes naive autoregressive generation prohibitively expensive for longer sequences.</p>\n<p>KV caching solves this problem by storing the computed key and value tensors for all previous tokens, reusing them across generation steps. When generating the next token, we only compute keys and values for the new token, then concatenate them with the cached values from previous steps. This optimization transforms the computational complexity from O(n²) to O(n) for the key-value computations.</p>\n<p><strong>Decision: KV Cache Implementation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Autoregressive generation recomputes the same key-value pairs repeatedly, creating significant computational waste</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>No caching - recompute everything each step for simplicity</li>\n<li>Full KV caching with careful tensor management</li>\n<li>Sliding window cache with fixed memory usage</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Full KV caching with dynamic tensor growth</li>\n<li><strong>Rationale</strong>: The performance benefits of KV caching are too significant to ignore—often 5-10x speedup for longer sequences. Full caching provides maximum performance and the implementation complexity is manageable with proper tensor operations</li>\n<li><strong>Consequences</strong>: Significantly faster generation at the cost of increased memory usage and more complex cache management code</li>\n</ul>\n<h4 id=\"cache-structure-and-management\">Cache Structure and Management</h4>\n<p>The KV cache maintains separate storage for keys and values at each transformer layer, since multi-layer models need to cache intermediate states throughout the entire stack. Each cache entry stores tensors with shape <code>[batch_size, num_heads, seq_length, d_k]</code> for keys and <code>[batch_size, num_heads, seq_length, d_v]</code> for values.</p>\n<table>\n<thead>\n<tr>\n<th>Cache Component</th>\n<th>Shape</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Layer key cache</td>\n<td><code>[batch_size, num_heads, cached_length, d_k]</code></td>\n<td>Stores computed key projections for reuse</td>\n</tr>\n<tr>\n<td>Layer value cache</td>\n<td><code>[batch_size, num_heads, cached_length, d_v]</code></td>\n<td>Stores computed value projections for reuse</td>\n</tr>\n<tr>\n<td>Position tracker</td>\n<td><code>int</code></td>\n<td>Current sequence length for indexing new tokens</td>\n</tr>\n<tr>\n<td>Attention mask</td>\n<td><code>[1, 1, total_length, total_length]</code></td>\n<td>Causal mask expanded for cached + new tokens</td>\n</tr>\n</tbody></table>\n<p>The cache grows dynamically as new tokens are generated. At step t, we concatenate the new token&#39;s keys and values with the cached tensors from steps 1 through t-1. This concatenation happens along the sequence length dimension, preserving the batch and head dimensions.</p>\n<h4 id=\"cache-update-process\">Cache Update Process</h4>\n<p>The cache update process follows a precise sequence to maintain consistency between cached values and current computations:</p>\n<ol>\n<li><p><strong>Extract New Token Keys and Values</strong>: Compute key and value projections for the current input token using the learned projection matrices. These new tensors have shape <code>[batch_size, num_heads, 1, d_k/d_v]</code> since we&#39;re processing a single new token.</p>\n</li>\n<li><p><strong>Concatenate with Cache</strong>: Combine the new key and value tensors with their respective caches along the sequence dimension. If the cache contains keys for positions 0 through t-1, concatenation produces tensors spanning positions 0 through t.</p>\n</li>\n<li><p><strong>Update Attention Mask</strong>: Expand the causal attention mask to accommodate the new sequence length. The mask must prevent position t from attending to any position beyond t, maintaining the autoregressive property.</p>\n</li>\n<li><p><strong>Compute Attention</strong>: Perform scaled dot-product attention using the full concatenated keys and values but only computing outputs for the new token position. This requires careful indexing to avoid recomputing attention for cached positions.</p>\n</li>\n<li><p><strong>Update Cache Storage</strong>: Store the concatenated key and value tensors back into the cache, replacing the previous cached values. Update the position tracker to reflect the new sequence length.</p>\n</li>\n</ol>\n<p>The attention computation during cached generation requires special handling because we only need the attention output for the new token, but the attention scores must be computed using all previous keys. The query tensor for the new token attends to keys from all previous positions, but we only need to compute and return the weighted sum for the current position.</p>\n<h4 id=\"memory-management-considerations\">Memory Management Considerations</h4>\n<p>KV caching trades memory for computational efficiency, and this trade-off becomes more pronounced for longer sequences or larger models. The memory requirements scale linearly with sequence length, number of layers, model dimension, and batch size. For a model with <code>num_layers</code> layers, maximum sequence length <code>max_length</code>, and batch size <code>batch_size</code>, the total cache memory is approximately:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Total Cache Memory ≈ 2 × num_layers × batch_size × num_heads × max_length × (d_k + d_v) × sizeof(float)</code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>Memory Optimization Strategy</th>\n<th>Memory Impact</th>\n<th>Performance Impact</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No optimization</td>\n<td>High memory usage</td>\n<td>Maximum performance</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Sliding window cache</td>\n<td>Fixed memory</td>\n<td>Moderate performance loss</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Gradient checkpointing</td>\n<td>Low memory</td>\n<td>Significant performance loss</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Mixed precision caching</td>\n<td>50% memory reduction</td>\n<td>Minimal performance impact</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p>For most applications, the memory cost is justified by the dramatic performance improvement. However, for very long sequences or memory-constrained environments, sliding window caching can maintain a fixed-size cache containing only the most recent tokens, though this may impact generation quality for tasks requiring long-range dependencies.</p>\n<h4 id=\"cache-consistency-and-debugging\">Cache Consistency and Debugging</h4>\n<p>KV cache implementations are prone to subtle bugs that can be difficult to diagnose because they often manifest as slight quality degradations rather than obvious failures. The most common issues involve dimension mismatches, incorrect concatenation operations, or mask inconsistencies.</p>\n<table>\n<thead>\n<tr>\n<th>Common Cache Bug</th>\n<th>Symptom</th>\n<th>Diagnostic</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Wrong concatenation dimension</td>\n<td>Shape errors or gibberish text</td>\n<td>Check tensor shapes before/after concat</td>\n<td>Concatenate along seq_length dimension (dim=2)</td>\n</tr>\n<tr>\n<td>Stale cache between sequences</td>\n<td>First token of new sequence is wrong</td>\n<td>Clear cache completely between sequences</td>\n<td>Reset cache to None or empty tensors</td>\n</tr>\n<tr>\n<td>Mask-cache length mismatch</td>\n<td>Attention errors or NaN values</td>\n<td>Verify mask shape matches cached sequence length</td>\n<td>Update mask size when updating cache</td>\n</tr>\n<tr>\n<td>Incorrect position indexing</td>\n<td>Repetitive or inconsistent text</td>\n<td>Log cache positions and verify monotonic increase</td>\n<td>Use cache size to determine next position</td>\n</tr>\n</tbody></table>\n<p>Cache validation can be implemented by occasionally running generation both with and without caching and comparing the outputs. Identical results confirm cache correctness, while differences indicate bugs in the caching logic.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Text generation combines multiple complex systems—attention mechanisms, sampling strategies, and cache management—creating numerous opportunities for subtle errors. These pitfalls often produce plausible but incorrect behavior, making them particularly challenging to diagnose and fix.</p>\n<p>⚠️ <strong>Pitfall: Repetitive Text Generation</strong></p>\n<p>The most common generation quality issue is repetitive text where the model generates the same phrases or tokens in loops. This typically occurs when using greedy decoding or very low temperatures, where the deterministic selection process leads the model into cycles.</p>\n<p><strong>Why it happens</strong>: Deterministic sampling strategies can create feedback loops where a particular token sequence makes itself the most likely continuation. For example, generating &quot;the the the&quot; makes &quot;the&quot; highly likely as the next token, perpetuating the repetition.</p>\n<p><strong>How to fix</strong>: Implement repetition penalty mechanisms that temporarily reduce the probability of recently generated tokens. A typical approach multiplies the logits of recent tokens by a penalty factor (like 0.9) before sampling. Alternatively, switch to stochastic sampling methods like top-p sampling that introduce enough randomness to break repetitive patterns.</p>\n<p>⚠️ <strong>Pitfall: Temperature Edge Cases</strong></p>\n<p>Temperature scaling can create numerical instability or unexpected behavior at extreme values. Setting temperature to exactly 0.0 causes division by zero, while very high temperatures can make logits so small that softmax produces uniform distributions.</p>\n<p><strong>Why it happens</strong>: The temperature transformation <code>logits / temperature</code> becomes undefined when temperature equals zero, and very small temperatures can cause logit overflow when large values are divided by tiny denominators.</p>\n<p><strong>How to fix</strong>: Clamp temperature to a minimum value (like 1e-6) to prevent division by zero, and implement special handling for very low temperatures by using greedy decoding directly instead of temperature sampling.</p>\n<p>⚠️ <strong>Pitfall: KV Cache Dimension Mismatches</strong></p>\n<p>Cache concatenation often fails due to dimension mismatches between new keys/values and cached tensors. This typically happens when batch sizes change between generation steps or when the cache isn&#39;t properly initialized.</p>\n<p><strong>Why it happens</strong>: Tensor concatenation requires exact shape matching in all dimensions except the concatenation dimension. If the cache was initialized with a different batch size or head configuration, concatenation fails with cryptic error messages.</p>\n<p><strong>How to fix</strong>: Always validate tensor shapes before concatenation operations. Clear and reinitialize caches when generation parameters change. Use assertion statements to verify shape consistency: <code>assert new_keys.shape[:-2] == cached_keys.shape[:-2]</code>.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Label Shifting in Generation Loop</strong></p>\n<p>Unlike training where we have ground-truth target tokens, generation must use the model&#39;s own previous predictions as input for the next step. Failing to properly append generated tokens to the input sequence breaks the autoregressive dependency chain.</p>\n<p><strong>Why it happens</strong>: Training uses teacher forcing with ground-truth previous tokens, but generation must use model predictions. Code written for training often doesn&#39;t properly accumulate generated tokens into the input sequence.</p>\n<p><strong>How to fix</strong>: Maintain an explicit generated sequence that grows with each prediction. At each step, append the newly generated token to this sequence and use the entire accumulated sequence as input for the next prediction.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Special Tokens During Generation</strong></p>\n<p>Generation systems must handle special tokens like <code>pad_token_id</code> and <code>eos_token_id</code> correctly. Failing to stop generation when encountering end-of-sequence tokens leads to continued generation beyond natural stopping points.</p>\n<p><strong>Why it happens</strong>: The model learns to predict end-of-sequence tokens at natural completion points, but generation code often ignores these predictions and continues generating until reaching maximum length.</p>\n<p><strong>How to fix</strong>: Check each generated token against <code>eos_token_id</code> and terminate generation immediately when encountered. For batched generation, track completion status per sequence and mask completed sequences from further processing.</p>\n<p>⚠️ <strong>Pitfall: Softmax Numerical Instability</strong></p>\n<p>Large logit values can cause softmax computation to overflow or underflow, producing NaN values that corrupt the entire generation process. This is particularly problematic with very low temperatures that amplify logit magnitudes.</p>\n<p><strong>Why it happens</strong>: Softmax involves exponential operations that can produce very large or very small values when logits are extreme. The numerical precision of floating-point arithmetic cannot handle the full range of possible exponential values.</p>\n<p><strong>How to fix</strong>: Implement numerically stable softmax by subtracting the maximum logit value before exponentiation: <code>softmax(x) = softmax(x - max(x))</code>. Modern frameworks like PyTorch handle this automatically, but custom implementations must include this normalization.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Device Placement</strong></p>\n<p>Generation involves multiple tensors (model parameters, input sequences, caches, masks) that must all reside on the same device (CPU or GPU). Device mismatches cause runtime errors that can be difficult to trace.</p>\n<p><strong>Why it happens</strong>: Tensor operations require all operands to be on the same device, but generation code often creates new tensors (like attention masks) without explicitly placing them on the model&#39;s device.</p>\n<p><strong>How to fix</strong>: Always specify device placement explicitly when creating new tensors during generation. Use the model&#39;s device as the reference: <code>new_tensor = torch.tensor(data, device=model.device)</code>. Implement device checking assertions to catch mismatches early.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The text generation system requires careful orchestration of multiple components—the trained transformer model, sampling strategies, cache management, and sequence building logic. The implementation must balance simplicity for basic use cases with flexibility for advanced sampling techniques and performance optimizations.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sampling</td>\n<td>Temperature + greedy fallback</td>\n<td>Full top-k/top-p with temperature</td>\n</tr>\n<tr>\n<td>Caching</td>\n<td>Manual tensor concatenation</td>\n<td>Optimized KV cache class with memory pooling</td>\n</tr>\n<tr>\n<td>Batching</td>\n<td>Single sequence generation</td>\n<td>Dynamic batching with padding</td>\n</tr>\n<tr>\n<td>Device Management</td>\n<td>CPU-only generation</td>\n<td>CUDA with automatic device placement</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Unlimited cache growth</td>\n<td>Memory-bounded cache with eviction</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure\">File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer/\n├── generation/\n│   ├── __init__.py              ← Export main generation classes\n│   ├── generator.py             ← Main TextGenerator class\n│   ├── sampling.py              ← Sampling strategy implementations\n│   ├── cache.py                 ← KV cache management\n│   └── utils.py                 ← Generation utilities and helpers\n├── models/\n│   └── transformer.py          ← Core transformer model (from previous sections)\n└── training/\n    └── tokenizer.py             ← SimpleTokenizer (from previous sections)</code></pre></div>\n\n<h4 id=\"core-generation-infrastructure\">Core Generation Infrastructure</h4>\n<p><strong>Complete KV Cache Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GenerationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for text generation parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  # 0 means disabled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_p: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#6A737D\">  # 1.0 means disabled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pad_token_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eos_token_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    repetition_penalty: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    length_penalty: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> KVCache</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages key-value caching for efficient autoregressive generation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: torch.device):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_layers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_heads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_k</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_v</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> device</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize empty cache storage</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_caches: List[Optional[torch.Tensor]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_caches: List[Optional[torch.Tensor]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> clear</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clear all cached values and reset length counter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_caches </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_caches </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, layer_idx: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, new_keys: torch.Tensor, new_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Update cache with new keys and values, returning full concatenated tensors.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            layer_idx: Which transformer layer this cache belongs to</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            new_keys: New key tensor [batch_size, num_heads, 1, d_k]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            new_values: New value tensor [batch_size, num_heads, 1, d_v]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (full_keys, full_values) after concatenation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx] </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # First generation step - initialize cache with new tensors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_keys.clone()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.value_caches[layer_idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_values.clone()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Subsequent steps - concatenate with existing cache</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cat([</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx], new_keys], </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.value_caches[layer_idx] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cat([</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.value_caches[layer_idx], new_values], </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update length after first layer (avoid counting multiple times)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> layer_idx </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx], </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.value_caches[layer_idx]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_cache_info</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return cache statistics for debugging and monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> layer_idx </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_layers):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx] </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                memory_usage </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx].numel() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">  # 4 bytes per float32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                memory_usage </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.value_caches[layer_idx].numel() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"current_length\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_length,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"memory_usage_bytes\"</span><span style=\"color:#E1E4E8\">: memory_usage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"memory_usage_mb\"</span><span style=\"color:#E1E4E8\">: memory_usage </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"cached_layers\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> cache </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_caches </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> cache </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SamplingStrategies</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Collection of token sampling methods for generation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> greedy_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Select token with highest probability.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.argmax(logits, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> temperature_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor, temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Sample with temperature scaling for controlled randomness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> temperature </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.greedy_sample(logits)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scaled_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logits </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> temperature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        probabilities </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.softmax(scaled_logits, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.multinomial(probabilities, </span><span style=\"color:#FFAB70\">num_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">).squeeze(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> top_k_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor, k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Sample from top-k most likely tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> logits.size(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.temperature_sample(logits, temperature)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Get top-k values and indices</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        top_k_values, top_k_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.topk(logits, k, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create mask for non-top-k tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.full_like(logits, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mask.scatter_(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, top_k_indices, top_k_values)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.temperature_sample(mask, temperature)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> top_p_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor, p: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Sample from nucleus of tokens with cumulative probability p.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.temperature_sample(logits, temperature)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort logits in descending order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_logits, sorted_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.sort(logits, </span><span style=\"color:#FFAB70\">descending</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute cumulative probabilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_probs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.softmax(sorted_logits </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> temperature, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cumulative_probs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cumsum(sorted_probs, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create mask for tokens beyond nucleus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nucleus_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cumulative_probs </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> p</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Always include at least the top token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nucleus_mask[</span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply mask to sorted logits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filtered_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sorted_logits.clone()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filtered_logits[</span><span style=\"color:#F97583\">~</span><span style=\"color:#E1E4E8\">nucleus_mask] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sample from filtered distribution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> temperature </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            selected_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.zeros_like(sorted_indices[</span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">, :</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            filtered_probs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.softmax(filtered_logits </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> temperature, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            selected_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.multinomial(filtered_probs, </span><span style=\"color:#FFAB70\">num_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Map back to original vocabulary indices</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.gather(sorted_indices, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, selected_indices).squeeze(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"core-generation-logic-skeleton-for-implementation\">Core Generation Logic (Skeleton for Implementation)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TextGenerator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main class for autoregressive text generation with transformer models.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: nn.Module, tokenizer: </span><span style=\"color:#9ECBFF\">'SimpleTokenizer'</span><span style=\"color:#E1E4E8\">, config: GenerationConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(model.parameters()).device</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize KV cache based on model configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model.config  </span><span style=\"color:#6A737D\"># Assumes model has config attribute</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.kv_cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> KVCache(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            num_layers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_config.num_layers,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            num_heads</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_config.num_heads,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            d_k</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_config.d_k,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            d_v</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_config.d_v,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            device</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.device</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate</span><span style=\"color:#E1E4E8\">(self, prompt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_new_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate text continuation for given prompt using autoregressive sampling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            prompt: Initial text to continue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_new_tokens: Maximum number of tokens to generate (overrides config)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Generated text including original prompt</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Encode prompt to token sequence and move to device</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Clear KV cache from any previous generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set model to evaluation mode and disable gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize generation loop with encoded prompt as current sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each generation step up to max_length:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Get model logits for current sequence (use KV caching if available)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Apply repetition penalty to recently generated tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Sample next token using configured sampling strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Check if next token is EOS and break if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Append next token to current sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Decode final token sequence back to text string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return generated text with proper handling of special tokens</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _apply_repetition_penalty</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor, generated_tokens: torch.Tensor, penalty: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Apply repetition penalty to reduce probability of recently generated tokens.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            logits: Current token logits [batch_size, vocab_size]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            generated_tokens: Previously generated token sequence [batch_size, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            penalty: Penalty multiplier for repeated tokens (&#x3C; 1.0 reduces probability)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Modified logits with repetition penalty applied</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create penalty mask for tokens that appear in generated sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply penalty multiplier to logits of repeated tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consider recency weighting (recent tokens get higher penalty)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.gather and torch.scatter to modify specific token logits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _sample_next_token</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Sample next token using configured sampling strategy.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            logits: Token logits from model [batch_size, vocab_size]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Selected token indices [batch_size]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract sampling parameters from self.config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply appropriate sampling strategy based on configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle edge cases (temperature=0, top_k=1, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return sampled token indices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use the SamplingStrategies static methods implemented above</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _update_kv_cache</span><span style=\"color:#E1E4E8\">(self, layer_outputs: List[Tuple[torch.Tensor, torch.Tensor]]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Update KV cache with new key-value pairs from model forward pass.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            layer_outputs: List of (keys, values) tuples from each transformer layer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through layer outputs and update cache for each layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle cache initialization on first generation step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure proper tensor concatenation along sequence dimension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update cache length tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_batch</span><span style=\"color:#E1E4E8\">(self, prompts: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], max_new_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate text for multiple prompts in parallel (advanced feature).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            prompts: List of input prompts</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_new_tokens: Maximum tokens to generate per prompt</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of generated text strings</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize and pad all prompts to same length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create batch tensor with proper padding masks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Implement batched generation loop with per-sequence EOS handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Decode each sequence separately, handling padding appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of generated strings in same order as input prompts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span></code></pre></div>\n\n<h4 id=\"integration-with-transformer-model\">Integration with Transformer Model</h4>\n<p>The generation system must integrate cleanly with the transformer model implemented in previous sections. The model needs slight modifications to support KV caching during generation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Addition to MultiHeadAttention class to support KV caching</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> forward_with_cache</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: torch.Tensor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      kv_cache: Optional[KVCache] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, layer_idx: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Forward pass with optional KV caching for efficient generation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (attention_output, keys, values) for cache management</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute Q, K, V projections for input tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If using cache, only compute K, V for new tokens and concatenate with cache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Perform attention computation with full K, V but return output for new tokens only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return attention output along with K, V tensors for cache updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing text generation, verify functionality with these tests:</p>\n<p><strong>Basic Generation Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_basic_generation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Load trained model and tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.load(</span><span style=\"color:#9ECBFF\">'trained_transformer.pt'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleTokenizer.load(</span><span style=\"color:#9ECBFF\">'tokenizer.json'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GenerationConfig(</span><span style=\"color:#FFAB70\">temperature</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.8</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">max_length</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    generator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TextGenerator(model, tokenizer, config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generator.generate(</span><span style=\"color:#9ECBFF\">\"The quick brown fox\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Generated: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Should produce coherent continuation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(result) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"The quick brown fox\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> result.startswith(</span><span style=\"color:#9ECBFF\">\"The quick brown fox\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Sampling Strategy Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_sampling_strategies</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.tensor([[</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1.5</span><span style=\"color:#E1E4E8\">]])  </span><span style=\"color:#6A737D\"># Batch size 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test greedy (should always select index 3)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    greedy_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SamplingStrategies.greedy_sample(logits)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> greedy_token.item() </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test temperature (should vary across runs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temp_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [SamplingStrategies.temperature_sample(logits, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">).item() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">(temp_tokens)) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # Should have some variety</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"All sampling strategy tests passed!\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>KV Cache Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_kv_cache</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> KVCache(</span><span style=\"color:#FFAB70\">num_layers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">num_heads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">d_k</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">d_v</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.device(</span><span style=\"color:#9ECBFF\">'cpu'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Simulate cache updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keys1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    values1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    full_keys, full_values </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cache.update(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, keys1, values1)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> full_keys.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> cache.current_length </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Add second token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keys2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    values2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randn(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    full_keys, full_values </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cache.update(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, keys2, values2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> full_keys.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> cache.current_length </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"KV cache tests passed!\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Generation produces only padding tokens</td>\n<td>EOS token generated immediately</td>\n<td>Check if prompt encodes to empty sequence</td>\n<td>Verify tokenizer handles prompt correctly</td>\n</tr>\n<tr>\n<td>Repetitive text loops</td>\n<td>Greedy decoding with deterministic patterns</td>\n<td>Try higher temperature or top-p sampling</td>\n<td>Add repetition penalty or use stochastic sampling</td>\n</tr>\n<tr>\n<td>Generation is very slow</td>\n<td>No KV caching or inefficient implementation</td>\n<td>Profile generation loop timing</td>\n<td>Implement KV caching and batch operations</td>\n</tr>\n<tr>\n<td>CUDA out of memory during generation</td>\n<td>Cache memory grows too large</td>\n<td>Monitor cache memory usage</td>\n<td>Implement cache size limits or use smaller batch sizes</td>\n</tr>\n<tr>\n<td>Generated text ends abruptly</td>\n<td>EOS token sampled unexpectedly</td>\n<td>Check if EOS appears in top-k/top-p candidates</td>\n<td>Filter EOS from sampling or adjust sampling parameters</td>\n</tr>\n<tr>\n<td>Incoherent text with high perplexity</td>\n<td>Model not properly trained or wrong sampling</td>\n<td>Evaluate model perplexity on validation set</td>\n<td>Verify training convergence and adjust sampling strategy</td>\n</tr>\n</tbody></table>\n<p>The text generation system represents the final piece of the transformer implementation puzzle. When working correctly, it should produce coherent continuations of input prompts, demonstrate sensitivity to sampling parameters, and generate text significantly faster with KV caching enabled. The combination of proper sampling strategies and efficient caching transforms the transformer from a training artifact into a practical text generation tool.</p>\n<p><img src=\"/api/project/build-transformer/architecture-doc/asset?path=diagrams%2Fgeneration-flow.svg\" alt=\"Autoregressive Generation Process\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The text generation implementation requires careful coordination between sampling strategies, cache management, and the core transformer model. Focus on building a robust foundation with simple greedy generation first, then add sophisticated sampling and caching optimizations.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Generation</td>\n<td>Greedy decoding with temperature fallback</td>\n<td>Multi-strategy sampling with all options</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>CPU generation with manual loops</td>\n<td>GPU acceleration with batched operations</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Unlimited cache growth</td>\n<td>Memory-bounded cache with sliding windows</td>\n</tr>\n<tr>\n<td>Sampling</td>\n<td>Temperature and top-k only</td>\n<td>Full nucleus sampling with repetition penalties</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Basic shape validation</td>\n<td>Comprehensive error recovery and graceful degradation</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-organization\">File Organization</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer/\n├── generation/\n│   ├── __init__.py              # Export TextGenerator and GenerationConfig\n│   ├── generator.py             # Main TextGenerator class implementation\n│   ├── sampling.py              # All sampling strategy implementations\n│   ├── cache.py                 # KVCache class and memory management\n│   └── utils.py                 # Generation utilities and helper functions\n├── models/\n│   ├── transformer.py          # Core transformer (modified for cache support)\n│   └── attention.py             # MultiHeadAttention with caching methods\n├── training/\n│   └── tokenizer.py             # SimpleTokenizer for encoding/decoding\n└── examples/\n    └── generate_text.py         # Complete generation example script</code></pre></div>\n\n<h4 id=\"complete-kv-cache-infrastructure\">Complete KV Cache Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, List, Dict, Any, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> warnings</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GenerationConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration parameters for text generation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">  # 0 disables top-k</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    top_p: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#6A737D\">  # 1.0 disables top-p</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pad_token_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    eos_token_id: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    repetition_penalty: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    length_penalty: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> KVCache</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Efficient key-value caching for autoregressive generation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: torch.device, max_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_layers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_heads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_k</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_v</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> device</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Pre-allocate cache tensors for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_caches: List[Optional[torch.Tensor]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_caches: List[Optional[torch.Tensor]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cache_initialized </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> initialize_cache</span><span style=\"color:#E1E4E8\">(self, batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Pre-allocate cache tensors to avoid repeated allocations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_caches </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.zeros(batch_size, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_heads, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_length, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.d_k, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.device)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_layers)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_caches </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            torch.zeros(batch_size, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_heads, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_length, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.d_v, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.device)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_layers)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cache_initialized </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> clear</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reset cache state for new generation sequence.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cache_initialized </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Don't deallocate tensors - just reset length for reuse</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, layer_idx: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, new_keys: torch.Tensor, new_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update cache with new key-value pairs and return current cached values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch_size, num_heads, seq_len, d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_keys.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache_initialized:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.initialize_cache(batch_size)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> seq_len </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_length:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            warnings.warn(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Sequence length </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> seq_len</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> exceeds cache capacity </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.max_length</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Implement sliding window or raise error based on requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> new_keys, new_values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update cache tensors in-place</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        end_pos </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> seq_len</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx][:, :, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_length:end_pos, :] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_keys</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_caches[layer_idx][:, :, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_length:end_pos, :] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return view of cached data up to current position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cached_keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_caches[layer_idx][:, :, :end_pos, :]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cached_values </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.value_caches[layer_idx][:, :, :end_pos, :]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update length only after first layer to avoid double counting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> layer_idx </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> end_pos</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> cached_keys, cached_values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_memory_info</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return detailed memory usage statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache_initialized:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"memory_mb\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"utilization\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_elements </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> cache </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_caches </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.value_caches:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> cache </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_elements </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> cache.numel()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_bytes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_elements </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">  # Assume float32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> memory_bytes </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        utilization </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_length </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_length </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_length </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"memory_mb\"</span><span style=\"color:#E1E4E8\">: memory_mb,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"utilization\"</span><span style=\"color:#E1E4E8\">: utilization,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"current_length\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_length,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"max_length\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SamplingStrategies</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive collection of token sampling methods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> greedy_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deterministic selection of highest probability token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.argmax(logits, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keepdim</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> temperature_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor, temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Temperature-controlled stochastic sampling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle edge case of zero temperature</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> temperature </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.greedy_sample(logits)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply temperature scaling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scaled_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logits </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> temperature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute probabilities with numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.max(scaled_logits, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keepdim</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stable_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scaled_logits </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> max_logits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        probabilities </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.softmax(stable_logits, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sample from distribution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.multinomial(probabilities, </span><span style=\"color:#FFAB70\">num_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> top_k_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor, k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Sample from top-k highest probability tokens.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> logits.size(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.temperature_sample(logits, temperature)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Get top-k values and their indices</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        top_k_logits, top_k_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.topk(logits, k, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create filtered logit tensor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filtered_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.full_like(logits, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filtered_logits.scatter_(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, top_k_indices, top_k_logits)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.temperature_sample(filtered_logits, temperature)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> top_p_sample</span><span style=\"color:#E1E4E8\">(logits: torch.Tensor, p: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Nucleus sampling with cumulative probability threshold.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> SamplingStrategies.temperature_sample(logits, temperature)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort in descending order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_logits, sorted_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.sort(logits, </span><span style=\"color:#FFAB70\">descending</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply temperature to sorted logits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> temperature </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sorted_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sorted_logits </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> temperature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute cumulative probabilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sorted_probs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.softmax(sorted_logits, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cumulative_probs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cumsum(sorted_probs, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create nucleus mask (always include first token)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nucleus_mask </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cumulative_probs </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> p</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nucleus_mask[</span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#6A737D\">  # Always include top token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply mask</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        masked_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sorted_logits.clone()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        masked_logits[</span><span style=\"color:#F97583\">~</span><span style=\"color:#E1E4E8\">nucleus_mask] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'-inf'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sample from nucleus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nucleus_probs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.softmax(masked_logits, </span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        selected_sorted_indices </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.multinomial(nucleus_probs, </span><span style=\"color:#FFAB70\">num_samples</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Map back to original indices</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.gather(sorted_indices, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, selected_sorted_indices)</span></span></code></pre></div>\n\n<h4 id=\"core-generation-logic-implementation-skeleton\">Core Generation Logic Implementation Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TextGenerator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main autoregressive text generation system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: nn.Module, tokenizer: </span><span style=\"color:#9ECBFF\">'SimpleTokenizer'</span><span style=\"color:#E1E4E8\">, config: GenerationConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> next</span><span style=\"color:#E1E4E8\">(model.parameters()).device</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract model configuration for cache initialization</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(model, </span><span style=\"color:#9ECBFF\">'config'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model_config </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Model must have 'config' attribute with layer and attention parameters\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.kv_cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> KVCache(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            num_layers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model_config.num_layers,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            num_heads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model_config.num_heads,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            d_k</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model_config.d_k,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            d_v</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model_config.d_v,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            device</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.device</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate</span><span style=\"color:#E1E4E8\">(self, prompt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_new_tokens: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate text continuation from prompt using autoregressive sampling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Handle empty prompt edge case</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Encode prompt using tokenizer.encode() and convert to tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Move input tensor to model device</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clear KV cache and set model to eval mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Initialize generation variables (generated_tokens, current_length)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Main generation loop:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Forward pass through model (handle caching)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Extract logits for last position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Apply repetition penalty if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Sample next token using _sample_next_token()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Check for EOS token and break if found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Append new token to sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Check length limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Decode final sequence back to text</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Handle special token filtering in output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Your implementation here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _sample_next_token</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply configured sampling strategy to select next token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract current sampling configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle multiple sampling strategies in priority order:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - If top_k > 0: use top_k_sample</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Else if top_p &#x3C; 1.0: use top_p_sample  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Else: use temperature_sample</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle edge cases (temperature=0, invalid parameters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return sampled token indices</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Your implementation here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _apply_repetition_penalty</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor, recent_tokens: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reduce probability of recently generated tokens to prevent repetition.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if repetition penalty is enabled (penalty != 1.0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract unique tokens from recent generation window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply penalty by dividing/multiplying logits at those positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Consider implementing recency weighting (recent tokens penalized more)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return modified logits tensor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Your implementation here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _model_forward_with_cache</span><span style=\"color:#E1E4E8\">(self, input_ids: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Forward pass through model with KV caching support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if model supports caching (has forward_with_cache method)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If caching supported:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call model.forward_with_cache() with current cache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Update cache with returned key-value pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Return logits for new tokens only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If no caching:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Standard forward pass with full input sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Return logits for all positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle device placement and gradient context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Your implementation here</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_generation_example</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete example demonstrating text generation usage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This would normally load a trained model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # For testing, create a minimal model structure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    from</span><span style=\"color:#E1E4E8\"> models.transformer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TransformerModel  </span><span style=\"color:#6A737D\"># Your transformer implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    from</span><span style=\"color:#E1E4E8\"> training.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SimpleTokenizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Initialize tokenizer and model</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleTokenizer(</span><span style=\"color:#FFAB70\">vocab_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Your tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TransformerModel(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">your_config)  </span><span style=\"color:#6A737D\"># Your trained model</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Create generation configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gen_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GenerationConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        temperature</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.8</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        top_k</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        top_p</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.9</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        max_length</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        repetition_penalty</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Initialize generator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    generator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TextGenerator(model, tokenizer, gen_config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Generate text</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prompt </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"The future of artificial intelligence\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    generated_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> generator.generate(prompt, </span><span style=\"color:#FFAB70\">max_new_tokens</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Prompt: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prompt</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Generated: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">generated_text</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> generated_text</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Example usage and testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test sampling strategies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.tensor([[</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">]])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Testing sampling strategies:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Greedy: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">SamplingStrategies.greedy_sample(test_logits)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Temperature (0.5): </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">SamplingStrategies.temperature_sample(test_logits, </span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Top-k (3): </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">SamplingStrategies.top_k_sample(test_logits, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Top-p (0.8): </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">SamplingStrategies.top_p_sample(test_logits, </span><span style=\"color:#79B8FF\">0.8</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test cache functionality</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> KVCache(</span><span style=\"color:#FFAB70\">num_layers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">num_heads</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">d_k</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">d_v</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.device(</span><span style=\"color:#9ECBFF\">'cpu'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Initial cache memory: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cache.get_memory_info()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Would continue with full generation example...</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the text generation system, verify functionality with these comprehensive tests:</p>\n<p><strong>Test 1: Basic Generation Functionality</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from generation.generator import TextGenerator, GenerationConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Load your trained model and tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Test basic generation works and produces reasonable output</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p>Expected behavior: Generator produces coherent text continuations of reasonable length, respects max_length parameter, and handles EOS tokens correctly.</p>\n<p><strong>Test 2: Sampling Strategy Verification</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> test_sampling.py</span></span></code></pre></div>\n\n<p>Expected behavior: Different sampling strategies produce different outputs, temperature affects randomness level, top-k/top-p filtering works as expected, and edge cases (temperature=0) handle gracefully.</p>\n<p><strong>Test 3: KV Cache Performance</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> benchmark_generation.py</span><span style=\"color:#79B8FF\"> --with-cache</span><span style=\"color:#79B8FF\"> --without-cache</span></span></code></pre></div>\n\n<p>Expected behavior: Generation with caching is significantly faster (3-10x for longer sequences), produces identical outputs to non-cached generation, and memory usage scales reasonably with sequence length.</p>\n<h4 id=\"language-specific-implementation-notes\">Language-Specific Implementation Notes</h4>\n<p><strong>PyTorch-specific optimizations:</strong></p>\n<ul>\n<li>Use <code>torch.no_grad()</code> context during generation to prevent gradient computation</li>\n<li>Implement <code>model.eval()</code> to disable dropout and batch normalization training behavior  </li>\n<li>Utilize <code>torch.multinomial()</code> for efficient sampling from probability distributions</li>\n<li>Apply <code>tensor.to(device)</code> consistently for proper GPU utilization</li>\n</ul>\n<p><strong>Memory management strategies:</strong></p>\n<ul>\n<li>Pre-allocate cache tensors to avoid repeated memory allocation during generation</li>\n<li>Use <code>torch.cat()</code> efficiently by pre-sizing tensors when possible</li>\n<li>Monitor GPU memory usage and implement cache size limits for long sequences</li>\n<li>Consider using <code>torch.cuda.empty_cache()</code> between generation sessions</li>\n</ul>\n<h4 id=\"debugging-common-generation-issues\">Debugging Common Generation Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Problem</th>\n<th>Symptoms</th>\n<th>Diagnostic Steps</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Repetitive text loops</td>\n<td>Same phrase repeated infinitely</td>\n<td>Check repetition penalty, try higher temperature</td>\n<td>Implement repetition penalty or use stochastic sampling</td>\n</tr>\n<tr>\n<td>Generation stops immediately</td>\n<td>Output equals input prompt exactly</td>\n<td>Check if EOS token generated on first step</td>\n<td>Verify model training, check prompt encoding</td>\n</tr>\n<tr>\n<td>Slow generation speed</td>\n<td>Each token takes several seconds</td>\n<td>Profile forward pass timing</td>\n<td>Implement KV caching, use GPU acceleration</td>\n</tr>\n<tr>\n<td>Incoherent gibberish output</td>\n<td>Random characters or broken words</td>\n<td>Check model training loss convergence</td>\n<td>Verify model training completed successfully</td>\n</tr>\n<tr>\n<td>CUDA memory errors</td>\n<td>Generation crashes with OOM</td>\n<td>Monitor memory usage during generation</td>\n<td>Reduce batch size or implement cache memory limits</td>\n</tr>\n<tr>\n<td>Inconsistent output quality</td>\n<td>Some prompts work well, others poorly</td>\n<td>Test various prompts and sampling settings</td>\n<td>Adjust sampling parameters, check training data coverage</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-interactions-and-data-flow\">Component Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - understanding how data flows through the transformer architecture from tokens to generated text, with different patterns for training (Milestone 3) and inference (Milestone 4)</p>\n</blockquote>\n<p>Understanding how components communicate and data flows through a transformer is essential for successful implementation. Unlike traditional neural networks with simple sequential processing, transformers involve complex interactions between attention mechanisms, normalization layers, and residual connections that must be orchestrated precisely. The data flow patterns differ significantly between training and inference modes, requiring careful design of component interfaces and state management.</p>\n<h3 id=\"mental-model-the-assembly-line\">Mental Model: The Assembly Line</h3>\n<p>Think of the transformer as a sophisticated assembly line in a factory. Raw materials (tokens) enter at one end and flow through multiple workstations (transformer blocks). At each workstation, specialized workers (attention heads) examine the materials from different perspectives, while quality control inspectors (layer normalization) ensure consistency. Conveyor belts (residual connections) carry materials forward while allowing workers to compare current work with the original materials. The assembly line operates differently during training (processing many products simultaneously with feedback from the end result) versus production (creating one product at a time based on customer specifications).</p>\n<p>This mental model captures the key aspects of transformer data flow: parallel processing within layers, sequential processing across layers, quality control at each stage, and different operational modes for training versus inference.</p>\n<h3 id=\"forward-pass-sequence\">Forward Pass Sequence</h3>\n<p>The forward pass represents the core data transformation pipeline that converts input tokens into probability distributions over the vocabulary. Understanding this sequence is crucial because every component must maintain tensor shape consistency while performing its specialized function.</p>\n<h4 id=\"token-input-and-embedding\">Token Input and Embedding</h4>\n<p>The forward pass begins when integer token IDs enter the transformer through the embedding layer. The <code>SimpleTokenizer</code> has already converted raw text into a sequence of integers, each representing a token in the vocabulary. These token IDs have shape <code>[batch_size, seq_length]</code> and contain values in the range <code>[0, vocab_size-1]</code>.</p>\n<p>The embedding layer performs the first critical transformation by converting discrete token IDs into dense vector representations. This learnable lookup table has dimensions <code>[vocab_size, d_model]</code>, allowing each token to be represented as a <code>d_model</code>-dimensional vector. After embedding lookup, the tensor shape becomes <code>[batch_size, seq_length, d_model]</code>, which remains constant throughout most of the transformer.</p>\n<blockquote>\n<p><strong>Decision: Learnable vs Fixed Embeddings</strong></p>\n<ul>\n<li><strong>Context</strong>: Token representations can use either randomly initialized learnable embeddings or pre-trained fixed vectors</li>\n<li><strong>Options Considered</strong>: Random initialization, pre-trained GloVe/Word2Vec, learned from scratch</li>\n<li><strong>Decision</strong>: Learnable embeddings trained from scratch</li>\n<li><strong>Rationale</strong>: Allows the model to learn optimal token representations for the specific task and vocabulary, avoiding distribution mismatch with pre-trained embeddings</li>\n<li><strong>Consequences</strong>: Requires more training data and time but provides better task-specific representations</li>\n</ul>\n</blockquote>\n<h4 id=\"positional-information-integration\">Positional Information Integration</h4>\n<p>After token embedding, positional information must be added because the self-attention mechanism is inherently position-agnostic. The transformer needs to understand token order to generate coherent text. Positional encodings are added directly to the token embeddings, maintaining the <code>[batch_size, seq_length, d_model]</code> shape.</p>\n<table>\n<thead>\n<tr>\n<th>Position Integration Method</th>\n<th>Description</th>\n<th>Shape Preservation</th>\n<th>Learning Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sinusoidal Encodings</td>\n<td>Fixed mathematical patterns based on position</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Learnable Positional Embeddings</td>\n<td>Trained position-specific vectors</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Relative Position Encodings</td>\n<td>Position differences in attention computation</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n</tbody></table>\n<p>The combined token and positional representations now contain both semantic and structural information needed for the transformer blocks.</p>\n<h4 id=\"multi-layer-transformer-processing\">Multi-Layer Transformer Processing</h4>\n<p>The heart of the forward pass occurs as data flows through the stack of transformer blocks. Each <code>TransformerBlock</code> performs identical operations but with different learned parameters. The processing within each block follows a precise sequence that maintains gradient flow and numerical stability.</p>\n<p><strong>Within Each Transformer Block:</strong></p>\n<ol>\n<li><p><strong>Pre-Normalization</strong>: Input tensors first pass through <code>LayerNorm</code>, which normalizes activations across the <code>d_model</code> dimension for each token independently. This stabilizes training by ensuring activations maintain reasonable magnitudes.</p>\n</li>\n<li><p><strong>Multi-Head Attention</strong>: The normalized input flows into the <code>MultiHeadAttention</code> module, where it undergoes the most computationally intensive transformation. The attention mechanism computes relevance-weighted summaries of the entire sequence for each token position.</p>\n</li>\n<li><p><strong>First Residual Connection</strong>: The attention output is added to the original block input (before normalization), allowing gradient flow and preserving information from earlier layers.</p>\n</li>\n<li><p><strong>Second Pre-Normalization</strong>: The result passes through another <code>LayerNorm</code> layer, preparing for the feed-forward processing.</p>\n</li>\n<li><p><strong>Feed-Forward Network</strong>: The <code>FeedForwardNetwork</code> applies position-wise transformations, expanding to <code>ffn_expansion * d_model</code> dimensions internally before projecting back to <code>d_model</code>.</p>\n</li>\n<li><p><strong>Second Residual Connection</strong>: The FFN output is added to the input of the FFN sublayer, completing the transformer block processing.</p>\n</li>\n</ol>\n<p>Throughout this sequence, the tensor shape remains <code>[batch_size, seq_length, d_model]</code>, ensuring compatibility between all components.</p>\n<h4 id=\"attention-computation-flow\">Attention Computation Flow</h4>\n<p>Within the attention mechanism, data undergoes several transformations that deserve detailed explanation due to their complexity and importance for understanding transformer behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Attention Step</th>\n<th>Input Shape</th>\n<th>Output Shape</th>\n<th>Operation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Q/K/V Projection</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Linear transformations</td>\n</tr>\n<tr>\n<td>Head Reshaping</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td><code>[batch_size, num_heads, seq_length, d_k]</code></td>\n<td>Tensor reshaping</td>\n</tr>\n<tr>\n<td>Attention Scores</td>\n<td><code>[batch_size, num_heads, seq_length, d_k]</code></td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td>Q @ K^T / sqrt(d_k)</td>\n</tr>\n<tr>\n<td>Causal Masking</td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td>Apply mask before softmax</td>\n</tr>\n<tr>\n<td>Attention Weights</td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td>Softmax normalization</td>\n</tr>\n<tr>\n<td>Weighted Values</td>\n<td><code>[batch_size, num_heads, seq_length, seq_length]</code></td>\n<td><code>[batch_size, num_heads, seq_length, d_v]</code></td>\n<td>Attention @ V</td>\n</tr>\n<tr>\n<td>Head Concatenation</td>\n<td><code>[batch_size, num_heads, seq_length, d_v]</code></td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Reshape and concatenate</td>\n</tr>\n<tr>\n<td>Output Projection</td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td><code>[batch_size, seq_length, d_model]</code></td>\n<td>Final linear transformation</td>\n</tr>\n</tbody></table>\n<p>The <code>create_causal_mask</code> function generates a lower triangular mask that prevents tokens from attending to future positions, maintaining the autoregressive property essential for language modeling.</p>\n<h4 id=\"output-head-and-logit-generation\">Output Head and Logit Generation</h4>\n<p>After processing through all transformer blocks, the final hidden states have shape <code>[batch_size, seq_length, d_model]</code>. These rich contextual representations must be converted into probability distributions over the vocabulary for next-token prediction.</p>\n<p>The output head consists of a final <code>LayerNorm</code> followed by a linear projection layer with weight matrix shape <code>[d_model, vocab_size]</code>. This transformation produces logits with shape <code>[batch_size, seq_length, vocab_size]</code>, where each position contains unnormalized scores for every token in the vocabulary.</p>\n<p>During training, these logits are compared against target tokens using cross-entropy loss. During inference, only the logits for the final sequence position are used for next-token prediction, as previous positions already contain known tokens.</p>\n<h4 id=\"data-flow-dependencies\">Data Flow Dependencies</h4>\n<p>The forward pass involves several critical dependencies that must be respected for correct implementation:</p>\n<table>\n<thead>\n<tr>\n<th>Dependency Type</th>\n<th>Source Component</th>\n<th>Target Component</th>\n<th>Requirement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape Consistency</td>\n<td>All layers</td>\n<td>Sequential layers</td>\n<td>Maintain <code>[batch_size, seq_length, d_model]</code></td>\n</tr>\n<tr>\n<td>Causal Ordering</td>\n<td>Attention mask</td>\n<td>Attention computation</td>\n<td>Prevent future information leakage</td>\n</tr>\n<tr>\n<td>Gradient Flow</td>\n<td>Residual connections</td>\n<td>Backward pass</td>\n<td>Preserve gradients across deep layers</td>\n</tr>\n<tr>\n<td>Numerical Stability</td>\n<td>Layer normalization</td>\n<td>All computations</td>\n<td>Prevent activation explosion/vanishing</td>\n</tr>\n<tr>\n<td>Memory Efficiency</td>\n<td>Attention computation</td>\n<td>Training/inference</td>\n<td>Quadratic memory scaling with sequence length</td>\n</tr>\n</tbody></table>\n<p>Understanding these dependencies helps identify potential implementation issues and optimization opportunities.</p>\n<h4 id=\"common-forward-pass-pitfalls\">Common Forward Pass Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Inconsistent Tensor Shapes</strong>\nMany implementations fail because tensor shapes don&#39;t match between components. The most common issue is incorrectly computing attention head dimensions where <code>d_k * num_heads</code> must equal <code>d_model</code>. Debug by printing tensor shapes after each major operation and verifying they match expected dimensions from the configuration.</p>\n<p>⚠️ <strong>Pitfall: Missing Causal Mask Application</strong>\nForgetting to apply the causal mask or applying it after softmax instead of before breaks the autoregressive property. The mask must set future position scores to negative infinity before the softmax operation. Verify by checking that attention weights for position <code>i</code> sum to zero for all positions <code>j &gt; i</code>.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Residual Connection Placement</strong>\nResidual connections must add the input to each sublayer (before normalization) to its output. Adding normalized inputs breaks gradient flow and hurts training stability. The correct pattern is: <code>output = sublayer(norm(x)) + x</code>, not <code>output = sublayer(x) + norm(x)</code>.</p>\n<p>⚠️ <strong>Pitfall: Attention Score Scaling Missing</strong>\nOmitting the <code>1/sqrt(d_k)</code> scaling factor in attention computation causes training instability as sequence length or model size increases. Without scaling, attention weights become too sharp, leading to vanishing gradients. Always verify that scores are divided by <code>sqrt(d_k)</code> before softmax.</p>\n<h3 id=\"training-vs-inference-flows\">Training vs Inference Flows</h3>\n<p>The transformer architecture supports two fundamentally different operational modes that require distinct data flow patterns and component interactions. Understanding these differences is crucial for implementing both training loops and text generation systems effectively.</p>\n<h4 id=\"training-flow-characteristics\">Training Flow Characteristics</h4>\n<p>During training, the transformer processes complete sequences in parallel using <strong>teacher forcing</strong>, where the model learns to predict each token given all previous tokens in the ground truth sequence. This parallel processing enables efficient training on modern hardware but requires careful handling of the causal mask and label shifting.</p>\n<p><strong>Teacher Forcing Data Flow:</strong></p>\n<p>Training begins with complete input sequences of shape <code>[batch_size, seq_length]</code> containing actual tokens from the training dataset. The <code>TextDataset</code> provides these sequences along with corresponding target sequences that are shifted by one position for next-token prediction. The key insight is that during training, the model can process all positions simultaneously because we know all the correct previous tokens.</p>\n<p>The forward pass processes the entire sequence through all transformer blocks in parallel. Each token at position <code>i</code> attends only to positions <code>0</code> through <code>i-1</code> due to the causal mask, but computationally, all positions are processed together. This parallel processing dramatically reduces training time compared to sequential generation.</p>\n<p><strong>Training-Specific Component Interactions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Training Behavior</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TextDataset</code></td>\n<td>Returns <code>(input_tensor, target_tensor)</code> pairs</td>\n<td>Handles sequence padding and batch creation</td>\n</tr>\n<tr>\n<td><code>MultiHeadAttention</code></td>\n<td>Processes full sequences with causal masking</td>\n<td>Computes attention for all positions simultaneously</td>\n</tr>\n<tr>\n<td>Cross-entropy Loss</td>\n<td>Computes loss across all sequence positions</td>\n<td>Uses label smoothing and padding mask</td>\n</tr>\n<tr>\n<td>Optimizer</td>\n<td>Updates parameters based on averaged gradients</td>\n<td>Includes gradient clipping and weight decay</td>\n</tr>\n<tr>\n<td><code>LayerNorm</code></td>\n<td>Uses batch statistics for normalization</td>\n<td>Running statistics updated during training</td>\n</tr>\n</tbody></table>\n<p>The loss computation deserves special attention in training flow. The model produces logits for every position in the sequence, but the loss is only computed for positions where we have valid target tokens (excluding padding). The <code>_compute_loss</code> method masks padded positions to prevent them from contributing to gradient updates.</p>\n<p><strong>Gradient Flow in Training:</strong></p>\n<p>Training involves backward pass computation where gradients flow from the loss through all transformer blocks back to the embeddings. The residual connections play a crucial role here, allowing gradients to flow both through the transformer blocks and around them via skip connections. This prevents vanishing gradients that would otherwise make training deep transformers impossible.</p>\n<blockquote>\n<p>The residual connections create multiple gradient pathways through the network. Without them, gradients must pass through many matrix multiplications and nonlinearities, causing them to vanish exponentially. With residuals, gradients can take shorter paths directly to earlier layers, maintaining training signal throughout the network depth.</p>\n</blockquote>\n<h4 id=\"inference-flow-characteristics\">Inference Flow Characteristics</h4>\n<p>Inference operates fundamentally differently because the model must generate tokens sequentially without access to future ground truth tokens. This <strong>autoregressive generation</strong> requires the model to use its own predictions as input for subsequent token predictions, creating a sequential dependency that cannot be parallelized.</p>\n<p><strong>Sequential Generation Data Flow:</strong></p>\n<p>Inference begins with a prompt sequence and generates new tokens one at a time. At each generation step, the model processes the entire sequence up to the current position to predict the next token. This creates a pattern where the sequence length grows with each iteration, requiring careful memory management and optimization.</p>\n<p>The <code>TextGenerator</code> orchestrates this process by maintaining the growing sequence and calling the model repeatedly. Each forward pass processes a longer sequence than the previous one, making naive implementation computationally expensive due to redundant attention computations for previously processed tokens.</p>\n<p><strong>Inference-Specific Optimizations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Optimization</th>\n<th>Purpose</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>KV Caching</td>\n<td>Avoid recomputing attention for previous tokens</td>\n<td>Store key/value tensors from previous steps</td>\n</tr>\n<tr>\n<td>Batch Inference</td>\n<td>Generate multiple sequences simultaneously</td>\n<td>Process multiple prompts in parallel</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Handle growing sequence lengths efficiently</td>\n<td>Limit maximum generation length</td>\n</tr>\n<tr>\n<td>Sampling Strategies</td>\n<td>Control randomness and quality of generation</td>\n<td>Temperature, top-k, top-p sampling</td>\n</tr>\n</tbody></table>\n<p>The <code>KVCache</code> optimization is particularly important for inference efficiency. Instead of recomputing attention keys and values for all previous tokens at each generation step, the cache stores these tensors and only computes them for the new token. This reduces the computational complexity from quadratic to linear in the sequence length.</p>\n<p><strong>KV Cache Data Flow:</strong></p>\n<p>During the first generation step, the model computes and caches key/value tensors for all attention heads and layers. For subsequent steps, the cache provides stored tensors for previous positions and only computes new tensors for the current position. The <code>update</code> method handles concatenating cached and new tensors to form complete key/value matrices for attention computation.</p>\n<h4 id=\"mode-specific-component-behavior\">Mode-Specific Component Behavior</h4>\n<p>Components must adapt their behavior based on whether they&#39;re operating in training or inference mode. This adaptation affects both computational patterns and memory usage.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Training Mode</th>\n<th>Inference Mode</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MultiHeadAttention</code></td>\n<td>Processes full sequences in parallel</td>\n<td>Processes one new token, uses KV cache</td>\n</tr>\n<tr>\n<td><code>LayerNorm</code></td>\n<td>Uses current batch statistics</td>\n<td>Uses running statistics from training</td>\n</tr>\n<tr>\n<td>Dropout</td>\n<td>Randomly zeros activations</td>\n<td>Disabled (no randomness)</td>\n</tr>\n<tr>\n<td>Causal Mask</td>\n<td>Applied to full attention matrix</td>\n<td>Only masks new token position</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>Fixed for batch and sequence length</td>\n<td>Grows with generated sequence length</td>\n</tr>\n</tbody></table>\n<p><strong>Training Loop vs Generation Loop:</strong></p>\n<p>The training loop and generation loop have different control flow patterns that reflect their distinct objectives:</p>\n<p><strong>Training Loop Pattern:</strong></p>\n<ol>\n<li>Load batch of sequences from <code>DataLoader</code></li>\n<li>Forward pass through entire transformer stack</li>\n<li>Compute loss against target tokens</li>\n<li>Backward pass to compute gradients</li>\n<li>Optimizer step to update parameters</li>\n<li>Repeat for all batches in epoch</li>\n</ol>\n<p><strong>Generation Loop Pattern:</strong></p>\n<ol>\n<li>Initialize sequence with prompt tokens</li>\n<li>Forward pass to get logits for next position</li>\n<li>Apply sampling strategy to select next token</li>\n<li>Append token to sequence</li>\n<li>Update KV cache with new key/value tensors</li>\n<li>Repeat until end condition (max length or EOS token)</li>\n</ol>\n<h4 id=\"memory-and-computational-trade-offs\">Memory and Computational Trade-offs</h4>\n<p>The different flow patterns create distinct resource usage profiles that must be considered during implementation:</p>\n<table>\n<thead>\n<tr>\n<th>Resource</th>\n<th>Training</th>\n<th>Inference</th>\n<th>Implication</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory</td>\n<td>Fixed per batch</td>\n<td>Grows with sequence</td>\n<td>Need generation length limits</td>\n</tr>\n<tr>\n<td>Computation</td>\n<td>Parallel across positions</td>\n<td>Sequential token generation</td>\n<td>Training is more hardware-efficient</td>\n</tr>\n<tr>\n<td>Latency</td>\n<td>Batch processing delay</td>\n<td>Per-token delay</td>\n<td>Inference optimized for individual requests</td>\n</tr>\n<tr>\n<td>Throughput</td>\n<td>High (parallel processing)</td>\n<td>Lower (sequential dependency)</td>\n<td>Training can process more tokens/second</td>\n</tr>\n</tbody></table>\n<h4 id=\"error-handling-differences\">Error Handling Differences</h4>\n<p>Training and inference require different error handling strategies due to their distinct failure modes:</p>\n<p><strong>Training Error Patterns:</strong></p>\n<ul>\n<li>Gradient explosion (handled by gradient clipping)</li>\n<li>Loss spikes (handled by learning rate scheduling)</li>\n<li>Memory overflow (handled by batch size adjustment)</li>\n<li>Convergence issues (handled by validation monitoring)</li>\n</ul>\n<p><strong>Inference Error Patterns:</strong></p>\n<ul>\n<li>Repetitive generation (handled by repetition penalty)</li>\n<li>Incoherent output (handled by sampling strategy tuning)</li>\n<li>Memory growth (handled by maximum length limits)</li>\n<li>Cache inconsistency (handled by cache validation)</li>\n</ul>\n<p>Understanding these operational differences ensures robust implementation of both training and inference systems.</p>\n<h4 id=\"common-mode-specific-pitfalls\">Common Mode-Specific Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Inconsistent Model State Between Modes</strong>\nForgetting to call <code>model.eval()</code> during inference or <code>model.train()</code> during training causes components like dropout and layer normalization to behave incorrectly. Always explicitly set the model mode and verify that components respond appropriately to the training/evaluation state.</p>\n<p>⚠️ <strong>Pitfall: KV Cache Shape Mismatches</strong>\nDuring inference, the KV cache must maintain consistent tensor shapes as sequences grow. Common errors include incorrect concatenation dimensions or failing to account for batch size in cache tensors. Debug by logging cache tensor shapes after each update and verifying they match expected attention input shapes.</p>\n<p>⚠️ <strong>Pitfall: Label Shifting Confusion in Training</strong>\nThe training data loader must shift labels by one position relative to inputs for next-token prediction. Input sequence <code>[A, B, C, D]</code> should have target sequence <code>[B, C, D, EOS]</code>. Incorrect shifting causes the model to learn identity mapping instead of next-token prediction.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in Long Generation</strong>\nInference with very long sequences can cause memory leaks if intermediate tensors aren&#39;t properly released. The KV cache and attention computations create large temporary tensors that must be explicitly cleared. Implement maximum generation length limits and monitor memory usage during long sequences.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The component interaction patterns require careful orchestration of data flow and state management. This implementation guidance provides concrete code structure for managing the complex interactions between training and inference modes.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model State Management</td>\n<td>Manual mode switching with <code>model.train()/eval()</code></td>\n<td>Context managers for automatic mode handling</td>\n</tr>\n<tr>\n<td>Memory Profiling</td>\n<td>Manual tensor shape logging</td>\n<td>PyTorch profiler with memory tracking</td>\n</tr>\n<tr>\n<td>KV Cache Implementation</td>\n<td>Simple dictionary storage</td>\n<td>Optimized circular buffers</td>\n</tr>\n<tr>\n<td>Batch Processing</td>\n<td>Fixed batch sizes</td>\n<td>Dynamic batching with padding</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>Understanding component interactions requires organizing code to clearly separate concerns while enabling efficient data flow:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer/\n  core/\n    transformer.py          ← Main model orchestrating all components\n    attention.py           ← MultiHeadAttention with KV cache support\n    transformer_block.py   ← TransformerBlock with mode-aware processing\n    generation.py          ← TextGenerator with sampling strategies\n  training/\n    trainer.py            ← TransformerTrainer managing training flow\n    data.py              ← TextDataset and data loading utilities\n  utils/\n    cache.py             ← KVCache implementation\n    flow_utils.py        ← Data flow debugging utilities</code></pre></div>\n\n<h4 id=\"core-model-orchestration-code\">Core Model Orchestration Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerModel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main transformer model orchestrating component interactions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Core components with proper initialization order</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.token_embedding </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.Embedding(config.vocab_size, config.d_model)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position_embedding </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.Embedding(config.seq_length, config.d_model)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dropout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.Dropout(config.dropout_rate)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Transformer block stack</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.transformer_blocks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.ModuleList([</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TransformerBlock(config) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(config.num_layers)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Output head</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.layer_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LayerNorm(config.d_model)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.output_projection </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nn.Linear(config.d_model, config.vocab_size, </span><span style=\"color:#FFAB70\">bias</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize weights</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.apply(initialize_transformer_weights)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                input_ids: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                attention_mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                kv_cache: Optional[KVCache] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                use_cache: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> Tuple[torch.Tensor, Optional[KVCache]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Forward pass supporting both training and inference modes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Training mode: processes full sequences in parallel</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Inference mode: supports KV caching for efficient generation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract batch_size and seq_length from input_ids shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate position indices for current sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute token embeddings and position embeddings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add embeddings and apply dropout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create causal mask if attention_mask not provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Process through transformer blocks with optional caching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Apply final layer norm and output projection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return logits and updated cache if requested</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"training-flow-implementation\">Training Flow Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerTrainer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages training data flow and component coordination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: TransformerModel, config: TrainingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.optim.AdamW(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            model.parameters(), </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            lr</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.learning_rate,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            weight_decay</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.weight_decay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> train</span><span style=\"color:#E1E4E8\">(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main training loop with proper mode management.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to training mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize tracking variables (loss, step count)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Iterate through epochs and batches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each batch: forward pass, loss computation, backward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply gradient clipping before optimizer step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log training metrics at specified intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Run validation at specified intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Save checkpoints based on validation performance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Return training history and metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_loss</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute cross-entropy loss with proper label shifting and masking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Reshape logits to [batch_size * seq_length, vocab_size]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Reshape targets to [batch_size * seq_length]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create padding mask to exclude pad tokens from loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute cross-entropy loss with reduction='none'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply padding mask and compute mean over valid tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use F.cross_entropy with ignore_index for padding tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"inference-flow-implementation\">Inference Flow Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TextGenerator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles inference data flow with KV cache optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: TransformerModel, tokenizer: SimpleTokenizer, config: GenerationConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate</span><span style=\"color:#E1E4E8\">(self, prompt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, max_new_tokens: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main generation loop with efficient caching.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set model to evaluation mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Encode prompt to token sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize KV cache for efficient generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate tokens one by one in loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each step: forward pass, sampling, cache update</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check for early stopping conditions (EOS token, max length)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Decode final token sequence to text</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Clean up cache and return generated text</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _sample_next_token</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply sampling strategy to select next token.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract logits for last position only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply temperature scaling if specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply repetition penalty for recently generated tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Use configured sampling strategy (greedy, top-k, top-p)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return selected token ID as tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use SamplingStrategies static methods for different approaches</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"kv-cache-implementation\">KV Cache Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> KVCache</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Efficient key-value caching for autoregressive generation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, device: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_layers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_layers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> num_heads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_k</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_v</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> device</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize cache storage</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># layer_idx -> [batch_size, num_heads, seq_len, d_k]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.value_cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># layer_idx -> [batch_size, num_heads, seq_len, d_v]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, layer_idx: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, new_keys: torch.Tensor, new_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update cache and return concatenated keys/values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if this is first update for layer (cache empty)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If first update, store tensors directly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If subsequent update, concatenate with cached tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update cache with concatenated tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return concatenated keys and values for attention computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use torch.cat along sequence dimension (dim=2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> clear</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reset cache for new generation sequence.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Clear key_cache dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Clear value_cache dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Optionally trigger garbage collection for memory cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"data-flow-debugging-utilities\">Data Flow Debugging Utilities</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> trace_tensor_flow</span><span style=\"color:#E1E4E8\">(model: TransformerModel, input_ids: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Size]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Trace tensor shapes through forward pass for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Register forward hooks on all model components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run forward pass and capture intermediate tensor shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return dictionary mapping component names to output shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify all shapes match expected dimensions from config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use model.named_modules() to iterate through components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_attention_patterns</span><span style=\"color:#E1E4E8\">(attention_weights: torch.Tensor, seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate that attention weights respect causal masking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that attention_weights has correct shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify that weights[i, j] == 0 for all j > i (causal property)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify that weights sum to 1.0 along last dimension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return True if all validations pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use torch.triu to check upper triangular zeros</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"mode-specific-context-managers\">Mode-Specific Context Managers</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InferenceMode</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for inference-specific model configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model: TransformerModel):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.training_mode </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __enter__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store current training mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set model to evaluation mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Disable gradient computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clear any existing caches</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __exit__</span><span style=\"color:#E1E4E8\">(self, exc_type, exc_val, exc_tb):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Restore original training mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Re-enable gradients if originally enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Clean up any resources</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage example:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># with InferenceMode(model):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#     generated_text = generator.generate(prompt, max_tokens=100)</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Forward Pass Verification:</strong>\nAfter implementing the forward pass, run these checks:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from transformer import TransformerModel, TransformerConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import torch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">config = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=1000, num_layers=6)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model = TransformerModel(config)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">input_ids = torch.randint(0, config.vocab_size, (2, 64))  # batch_size=2, seq_length=64</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Should output logits with correct shape</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">logits, _ = model(input_ids)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">assert logits.shape == (2, 64, 1000), f'Expected (2, 64, 1000), got {logits.shape}'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ Forward pass shape verification passed')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Training Flow Verification:</strong>\nAfter implementing training components:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Create small dataset and train for a few steps</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Loss should decrease over iterations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Gradients should flow to all parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ Training flow verification - check logs for decreasing loss')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>KV Cache Verification:</strong>\nAfter implementing caching:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Generate same sequence with and without cache</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Results should be identical but cache version faster</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Cache memory usage should be reasonable</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ KV cache verification - check generation consistency')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<h4 id=\"common-implementation-issues\">Common Implementation Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Forward pass shape errors</td>\n<td>Incorrect tensor reshaping</td>\n<td>Print shapes after each operation</td>\n<td>Fix attention head dimension calculations</td>\n</tr>\n<tr>\n<td>Training loss not decreasing</td>\n<td>Wrong label shifting or learning rate</td>\n<td>Check loss computation and gradients</td>\n<td>Verify target sequences are shifted correctly</td>\n</tr>\n<tr>\n<td>Generation produces repetitive text</td>\n<td>Missing sampling randomness</td>\n<td>Check sampling strategy application</td>\n<td>Add temperature &gt; 0 or top-k/top-p sampling</td>\n</tr>\n<tr>\n<td>Memory usage grows unbounded</td>\n<td>KV cache not clearing properly</td>\n<td>Monitor cache size during generation</td>\n<td>Implement proper cache cleanup and limits</td>\n</tr>\n<tr>\n<td>Attention weights sum &gt; 1.0</td>\n<td>Causal mask applied after softmax</td>\n<td>Verify mask timing in attention computation</td>\n<td>Apply mask to scores before softmax operation</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - ensuring robust implementation across self-attention (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)</p>\n</blockquote>\n<p>Error handling in transformer implementations goes far beyond simple try-catch blocks. The numerical computations involved in self-attention, the massive parameter spaces of transformer models, and the sequential nature of text generation create unique failure modes that can silently corrupt results or cause catastrophic training failures. Understanding these edge cases and implementing robust handling strategies is crucial for building reliable transformer systems.</p>\n<p>The complexity stems from the mathematical nature of attention mechanisms, where small numerical instabilities can cascade through multiple layers, and the autoregressive generation process, where a single incorrect token can derail an entire sequence. Unlike traditional software where failures are often obvious, transformer failures can manifest as subtle degradations in text quality, training instabilities that emerge after thousands of iterations, or seemingly random generation artifacts that only appear under specific conditions.</p>\n<h3 id=\"numerical-stability\">Numerical Stability</h3>\n<blockquote>\n<p><strong>Mental Model: The Numerical Tightrope Walk</strong>\nThink of transformer computations as walking a tightrope between numerical extremes. On one side lies the abyss of vanishing gradients where your model learns nothing, and on the other side the cliff of exploding values that crash your training. Layer normalization, gradient clipping, and careful attention scaling act as your balance pole, helping you navigate safely between these extremes while maintaining the delicate precision needed for coherent text generation.</p>\n</blockquote>\n<p>Numerical stability in transformers presents unique challenges due to the combination of softmax operations in attention mechanisms, deep layer stacks that can amplify small errors, and the need to maintain precision across widely varying scales of embeddings, attention weights, and gradients. The softmax operation in particular creates a critical vulnerability point where input values that are too large can cause overflow, while values that are too small can underflow to zero.</p>\n<p>The <strong>scaled dot-product attention</strong> computation creates the primary numerical stability challenge through its softmax operation. When attention scores become too large, the softmax function produces probabilities that are numerically indistinguishable from hard attention (all weight on one token), eliminating the model&#39;s ability to attend to multiple relevant tokens. Conversely, when scores are too small, all attention weights become nearly uniform, preventing the model from focusing on important information.</p>\n<p><strong>Softmax Overflow Prevention</strong> requires careful management of the input range before the softmax operation. The fundamental issue occurs when the maximum attention score exceeds the floating-point representation limit, typically around 88 for float32. When this happens, the exponential function returns infinity, and the resulting probability distribution contains NaN values that corrupt all downstream computations.</p>\n<blockquote>\n<p><strong>Decision: Attention Score Clipping Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Attention scores can grow unboundedly during training, especially in early iterations when weights are randomly initialized, leading to softmax overflow and NaN gradients</li>\n<li><strong>Options Considered</strong>: Pre-softmax clipping, temperature scaling, attention score normalization</li>\n<li><strong>Decision</strong>: Implement pre-softmax clipping with learnable temperature parameter</li>\n<li><strong>Rationale</strong>: Clipping preserves the relative ordering of scores while preventing overflow, and learnable temperature allows the model to adjust the attention sharpness during training</li>\n<li><strong>Consequences</strong>: Adds one parameter per attention head but provides robust protection against numerical instability without compromising model expressiveness</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Stability Technique</th>\n<th>Implementation</th>\n<th>Numerical Range</th>\n<th>Gradient Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pre-softmax clipping</td>\n<td><code>torch.clamp(scores, max=20.0)</code></td>\n<td>[-inf, 20.0]</td>\n<td>Gradient flow preserved</td>\n</tr>\n<tr>\n<td>Temperature scaling</td>\n<td><code>scores / max(temperature, 1e-4)</code></td>\n<td>Scaled by temperature</td>\n<td>Temperature gets gradients</td>\n</tr>\n<tr>\n<td>LogSumExp trick</td>\n<td><code>scores - scores.max(dim=-1)</code></td>\n<td>Relative to maximum</td>\n<td>No additional parameters</td>\n</tr>\n<tr>\n<td>Attention dropout</td>\n<td>Random masking before softmax</td>\n<td>Original range</td>\n<td>Prevents overfitting</td>\n</tr>\n</tbody></table>\n<p>The <strong>LogSumExp numerical trick</strong> provides the most robust solution for softmax stability. Instead of computing softmax directly, we subtract the maximum score from all scores before applying the exponential function. This mathematical identity ensures that the maximum input to the exponential is always zero, preventing overflow while maintaining exact mathematical equivalence to the original softmax computation.</p>\n<p><strong>Gradient explosion and vanishing</strong> present complementary challenges in transformer training. Gradient explosion occurs when the chain rule multiplication through many layers produces gradients with exponentially growing magnitude, while vanishing gradients result from repeated multiplication of small values. The residual connections in transformer blocks specifically address vanishing gradients, but gradient explosion requires explicit clipping strategies.</p>\n<p>The gradient clipping implementation must occur after computing gradients for all parameters but before the optimizer step. The total gradient norm is computed across all model parameters, and if it exceeds the specified threshold, all gradients are scaled down proportionally. This preserves the relative gradient directions while preventing the parameter updates from being too large.</p>\n<table>\n<thead>\n<tr>\n<th>Gradient Issue</th>\n<th>Detection Method</th>\n<th>Typical Range</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Explosion</td>\n<td>Gradient norm &gt; 10.0</td>\n<td>100-1000+</td>\n<td>Clip to max norm 1.0</td>\n</tr>\n<tr>\n<td>Vanishing</td>\n<td>Gradient norm &lt; 1e-6</td>\n<td>1e-8 to 1e-4</td>\n<td>Increase learning rate</td>\n</tr>\n<tr>\n<td>NaN gradients</td>\n<td><code>torch.isnan(grad).any()</code></td>\n<td>N/A</td>\n<td>Reset to checkpoint</td>\n</tr>\n<tr>\n<td>Inf gradients</td>\n<td><code>torch.isinf(grad).any()</code></td>\n<td>N/A</td>\n<td>Skip optimizer step</td>\n</tr>\n</tbody></table>\n<p><strong>Precision issues</strong> arise from the limited precision of floating-point arithmetic, particularly when mixing operations that produce vastly different scales. The attention mechanism combines embedding vectors (typically in range [-1, 1]) with position encodings, then scales by the square root of the dimension, creating opportunities for precision loss when different components have mismatched scales.</p>\n<p>Mixed precision training introduces additional complexity by using 16-bit floats for forward passes and 32-bit floats for gradient accumulation. While this provides significant memory and speed benefits, it requires careful management of the loss scaling to prevent gradient underflow. The loss scaling factor must be large enough to move small gradients into the representable range of 16-bit floats, but small enough to avoid overflow during backpropagation.</p>\n<blockquote>\n<p><strong>Decision: Automatic Mixed Precision Configuration</strong></p>\n<ul>\n<li><strong>Context</strong>: Manual mixed precision management is error-prone and requires careful tuning of loss scaling factors for different model sizes and learning rates</li>\n<li><strong>Options Considered</strong>: Manual FP16 casting, automatic mixed precision (AMP), full FP32 training</li>\n<li><strong>Decision</strong>: Use PyTorch&#39;s automatic mixed precision with dynamic loss scaling</li>\n<li><strong>Rationale</strong>: AMP automatically handles precision decisions and dynamic loss scaling adapts to the training dynamics, reducing manual tuning while maintaining numerical stability</li>\n<li><strong>Consequences</strong>: Requires PyTorch 1.6+ and modern GPUs but provides significant memory savings and training speedup with minimal stability risk</li>\n</ul>\n</blockquote>\n<p>The <strong>layer normalization</strong> operation introduces its own numerical stability considerations. The variance computation requires careful handling when the variance approaches zero, which can occur when all activations in a layer become very similar. The standard implementation adds a small epsilon value (typically 1e-5) to the variance before taking the square root, preventing division by zero while maintaining numerical stability.</p>\n<p><strong>Common Numerical Stability Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Ignoring Attention Score Magnitude</strong>\nMany implementations fail to monitor attention scores before softmax, allowing them to grow unboundedly during training. This manifests as sudden spikes in training loss or NaN values appearing after several stable epochs. The fix requires implementing attention score monitoring and either clipping or temperature scaling to keep scores in a reasonable range.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Gradient Accumulation</strong>\nWhen implementing gradient accumulation for large effective batch sizes, failing to scale the loss correctly leads to gradient magnitudes that depend on the accumulation steps rather than the actual loss value. Each accumulated gradient should be divided by the number of accumulation steps before adding to the total gradient.</p>\n<p>⚠️ <strong>Pitfall: Mixed Precision Loss Scaling Errors</strong>\nUsing static loss scaling in mixed precision training often leads to either gradient underflow (scaling too small) or overflow (scaling too large). The scaling factor needs dynamic adjustment based on whether gradients contain inf/NaN values, requiring careful monitoring and adjustment logic.</p>\n<h3 id=\"input-validation\">Input Validation</h3>\n<p>Input validation in transformer systems extends beyond simple type checking to include semantic constraints that ensure mathematical operations remain well-defined and computationally feasible. The sequential nature of transformer processing means that invalid inputs can corrupt not just individual computations but entire generation sequences, making comprehensive validation essential for robust operation.</p>\n<p>The validation strategy must operate at multiple levels: token-level validation ensuring individual tokens are within vocabulary bounds, sequence-level validation maintaining length constraints and proper formatting, batch-level validation ensuring consistent dimensions across sequences, and model-level validation verifying that input dimensions match the model&#39;s expected configuration.</p>\n<p><strong>Sequence Length Limits</strong> represent the most critical constraint in transformer systems. The quadratic memory complexity of attention means that sequence length directly impacts both memory usage and computational requirements. The maximum sequence length is typically set during model initialization and cannot be exceeded without retraining the model, as the positional embeddings and causal attention masks are pre-allocated for the maximum length.</p>\n<p>The sequence length validation must occur at multiple points in the processing pipeline. During tokenization, the raw text must be checked to ensure the resulting token sequence won&#39;t exceed limits. During data loading, batched sequences must be validated to ensure consistent lengths within batches. During generation, the accumulated sequence length must be monitored to prevent infinite generation loops.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Point</th>\n<th>Length Check</th>\n<th>Action on Violation</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenization</td>\n<td><code>len(tokens) &lt;= seq_length</code></td>\n<td>Truncate or split</td>\n<td>Preserve important tokens</td>\n</tr>\n<tr>\n<td>Data loading</td>\n<td><code>max(batch_lengths) &lt;= seq_length</code></td>\n<td>Filter or pad</td>\n<td>Use sliding window</td>\n</tr>\n<tr>\n<td>Generation</td>\n<td><code>current_length &lt; max_length</code></td>\n<td>Stop generation</td>\n<td>Return partial sequence</td>\n</tr>\n<tr>\n<td>Attention computation</td>\n<td><code>seq_len &lt;= model.seq_length</code></td>\n<td>Raise exception</td>\n<td>Cannot recover</td>\n</tr>\n</tbody></table>\n<p>The <strong>sliding window approach</strong> provides a robust strategy for handling sequences that exceed the maximum length. Instead of simply truncating long sequences, the sliding window maintains context by processing overlapping segments of the sequence. This is particularly important for generation tasks where losing early context can significantly impact coherence.</p>\n<p><strong>Token Vocabulary Bounds</strong> validation ensures that all token IDs fall within the valid range defined by the tokenizer&#39;s vocabulary. Out-of-vocabulary tokens typically manifest as IDs that are negative, exceed the vocabulary size, or correspond to undefined token positions. The validation must handle both individual token validation during processing and batch validation during data loading.</p>\n<p>The token validation strategy must account for special tokens that have specific semantic meaning. The <code>pad_token_id</code> is used for sequence padding and should not appear in the middle of sequences. The <code>eos_token_id</code> indicates sequence termination and should trigger special handling during generation. Unknown tokens or corrupted token IDs should be either corrected through fallback strategies or cause graceful failure with informative error messages.</p>\n<blockquote>\n<p><strong>Decision: Token Validation Timing Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Token validation can occur at tokenization time, data loading time, or model forward pass time, each with different performance and error detection trade-offs</li>\n<li><strong>Options Considered</strong>: Early validation at tokenization, lazy validation at model input, comprehensive validation at each step</li>\n<li><strong>Decision</strong>: Implement validation at tokenization with spot checks during data loading and generation</li>\n<li><strong>Rationale</strong>: Early validation catches errors close to their source while spot checks provide safety nets without significant performance overhead</li>\n<li><strong>Consequences</strong>: Requires tokenizer to maintain vocabulary bounds checking but provides fast error detection and clear error attribution</li>\n</ul>\n</blockquote>\n<p><strong>Shape Consistency Checking</strong> validates that tensor dimensions align correctly throughout the transformer pipeline. The attention mechanism requires that the sequence length dimension matches across query, key, and value tensors, while the batch dimension must be consistent across all inputs. The embedding dimension must match the model&#39;s configured <code>d_model</code> parameter, and the attention head dimensions must divide evenly into the total model dimension.</p>\n<p>The shape validation becomes particularly complex during generation, where the sequence length grows dynamically, and during training with variable-length sequences in batches. The KV cache mechanism adds additional shape constraints, as cached key and value tensors must maintain consistency with newly computed values.</p>\n<table>\n<thead>\n<tr>\n<th>Tensor</th>\n<th>Expected Shape</th>\n<th>Validation Rule</th>\n<th>Error Indication</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input embeddings</td>\n<td><code>[batch_size, seq_len, d_model]</code></td>\n<td>All dims &gt; 0, <code>d_model</code> matches config</td>\n<td>Shape mismatch error</td>\n</tr>\n<tr>\n<td>Attention mask</td>\n<td><code>[seq_len, seq_len]</code> or <code>[batch, seq_len, seq_len]</code></td>\n<td>Square matrix, compatible with input</td>\n<td>Broadcast error</td>\n</tr>\n<tr>\n<td>Query/Key/Value</td>\n<td><code>[batch_size, seq_len, d_model]</code></td>\n<td>Match input embeddings</td>\n<td>Attention computation fails</td>\n</tr>\n<tr>\n<td>KV cache</td>\n<td><code>[batch_size, num_heads, cached_len, d_k]</code></td>\n<td>Consistent with new keys/values</td>\n<td>Cache concatenation fails</td>\n</tr>\n</tbody></table>\n<p><strong>Batch Consistency Validation</strong> ensures that all sequences in a batch have compatible properties for efficient processing. While sequences can have different actual lengths (handled through padding and masking), they must all fit within the model&#39;s maximum sequence length and use the same vocabulary. The batch validation also checks that attention masks have the correct shape and that any provided position IDs are within valid ranges.</p>\n<p>The batch validation strategy must balance thoroughness with performance, as validation overhead can become significant for large batches or high-throughput scenarios. Critical validations that prevent crashes or silent failures should always be performed, while optional validations that catch rare edge cases can be enabled through debug flags.</p>\n<blockquote>\n<p><strong>Decision: Validation Granularity and Performance Trade-off</strong></p>\n<ul>\n<li><strong>Context</strong>: Comprehensive validation provides safety but adds computational overhead that can impact training and inference performance</li>\n<li><strong>Options Considered</strong>: Full validation always, validation only in debug mode, critical validations always with optional detailed checking</li>\n<li><strong>Decision</strong>: Implement tiered validation with critical checks always enabled and detailed validation controllable via configuration</li>\n<li><strong>Rationale</strong>: Critical failures (crashes, NaN propagation) must always be prevented, while performance-impacting detailed validation can be disabled in production</li>\n<li><strong>Consequences</strong>: Requires careful categorization of validation checks but provides both safety and performance when needed</li>\n</ul>\n</blockquote>\n<p><strong>Memory Constraint Validation</strong> monitors system resources to prevent out-of-memory failures that can crash training or generation processes. The validation must account for the quadratic memory growth of attention mechanisms, the linear growth of KV caches during generation, and the temporary memory spikes during gradient computation and optimizer updates.</p>\n<p>The memory validation strategy includes pre-computation estimation of memory requirements based on model configuration and input sizes, runtime monitoring of actual memory usage with warnings before critical thresholds, and graceful degradation strategies when memory limits are approached.</p>\n<p><strong>Common Input Validation Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Trusting Tokenizer Output</strong>\nMany implementations assume that tokenizers always produce valid output, but tokenizers can generate out-of-vocabulary IDs, exceed length limits, or produce inconsistent special token placement. Always validate tokenizer output before passing to the model, checking for ID bounds, length limits, and proper special token usage.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Batch Size Variations</strong>\nVariable batch sizes during training or inference can cause shape mismatches in attention computations or gradient accumulation. Implement explicit batch size checking and ensure that all tensor operations can handle the actual batch size rather than assuming a fixed size from configuration.</p>\n<p>⚠️ <strong>Pitfall: Missing Edge Case Handling</strong>\nEmpty sequences, single-token sequences, or sequences containing only special tokens can cause mathematical operations to produce undefined results. Implement explicit handling for these edge cases, either through early returns with appropriate default values or graceful error messages explaining the limitation.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation of robust error handling and edge case management in transformers requires careful attention to both the mathematical foundations and the practical realities of deep learning systems. The following guidance provides concrete approaches for implementing the stability and validation strategies described above.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Numerical Stability</td>\n<td>Manual gradient clipping + basic overflow checks</td>\n<td>PyTorch AMP + custom stability monitors</td>\n</tr>\n<tr>\n<td>Input Validation</td>\n<td>Simple assertion-based checking</td>\n<td>Comprehensive schema validation with detailed error messages</td>\n</tr>\n<tr>\n<td>Error Logging</td>\n<td>Python logging + print statements</td>\n<td>Structured logging (loguru) + metric collection</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Basic unit tests for edge cases</td>\n<td>Property-based testing (hypothesis) + fuzzing</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer/\n  validation/\n    __init__.py                     ← validation utilities\n    input_validators.py             ← input validation functions\n    numerical_stability.py          ← stability monitoring and fixes\n    error_handlers.py               ← error recovery strategies\n  utils/\n    logging_config.py               ← logging setup\n    monitoring.py                   ← runtime monitoring utilities\n  tests/\n    test_validation.py              ← validation unit tests\n    test_edge_cases.py              ← edge case integration tests</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># validation/numerical_stability.py - Complete stability utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NumericalStabilityMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete monitoring and fixing utilities for numerical stability issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, gradient_clip_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, attention_clip_value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 20.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> gradient_clip_norm</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.attention_clip_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attention_clip_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stabilize_attention_scores</span><span style=\"color:#E1E4E8\">(self, scores: torch.Tensor, temperature: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply multiple stability techniques to attention scores.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clamp extreme values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.clamp(scores, </span><span style=\"color:#FFAB70\">max</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.attention_clip_value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply temperature scaling with minimum temperature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        temperature </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(temperature, </span><span style=\"color:#79B8FF\">1e-4</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> temperature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # LogSumExp trick for softmax stability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scores_max </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores.max(</span><span style=\"color:#FFAB70\">dim</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">keepdim</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scores </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> scores_max</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> scores</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_and_clip_gradients</span><span style=\"color:#E1E4E8\">(self, model: nn.Module) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clip gradients and return diagnostic information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        param_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Check for NaN or Inf gradients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> torch.isnan(param.grad).any():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#9ECBFF\">\"NaN gradients detected\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    param.grad.zero_()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> torch.isinf(param.grad).any():</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#9ECBFF\">\"Inf gradients detected\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    param.grad.zero_()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                param_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> param.grad.data.norm(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_norm </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> param_norm.item() </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                param_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_norm </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">. </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clip gradients if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> total_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            clip_coef </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (total_norm </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1e-6</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    param.grad.data.mul_(clip_coef)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'total_norm'</span><span style=\"color:#E1E4E8\">: total_norm,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'param_count'</span><span style=\"color:#E1E4E8\">: param_count,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'clipped'</span><span style=\"color:#E1E4E8\">: total_norm </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.gradient_clip_norm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># validation/input_validators.py - Complete validation utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerInputValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete input validation for transformer components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: </span><span style=\"color:#9ECBFF\">'TransformerConfig'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_token_sequence</span><span style=\"color:#E1E4E8\">(self, tokens: torch.Tensor, sequence_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sequence\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate a token sequence tensor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Shape validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tokens.dim() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">sequence_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> must be 2D tensor (batch_size, seq_len), got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tokens.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch_size, seq_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokens.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Length validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> seq_len </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.seq_length:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">sequence_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> length </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">seq_len</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> exceeds maximum </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.seq_length</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Vocabulary bounds validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tokens.min() </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">sequence_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> contains negative token IDs\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tokens.max() </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.vocab_size:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">sequence_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> contains token IDs >= vocab_size </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.vocab_size</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_attention_inputs</span><span style=\"color:#E1E4E8\">(self, query: torch.Tensor, key: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 value: torch.Tensor, mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate attention mechanism inputs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Shape consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> query.shape </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> key.shape:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Query shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">query.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != key shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> query.shape </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> value.shape:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Query shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">query.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != value shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">value.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Dimension validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> query.shape[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.d_model:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Input dimension </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">query.shape[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> != d_model </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.config.d_model</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Mask validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> mask </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            seq_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> query.shape[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expected_mask_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (seq_len, seq_len)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> mask.shape[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">:] </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expected_mask_shape:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Mask shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">mask.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> incompatible with sequence length </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">seq_len</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># utils/monitoring.py - Runtime monitoring utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RuntimeMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitor system resources and model behavior during training/inference.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_memory_usage</span><span style=\"color:#E1E4E8\">(self, threshold_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.9</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check GPU memory usage and warn if approaching limits.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"gpu_available\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        allocated </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cached </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.memory_reserved() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.max_memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.get_device_properties(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">).total_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1e9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        usage_ratio </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cached </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> total_memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> usage_ratio </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> threshold_gb:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"GPU memory usage </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">usage_ratio</span><span style=\"color:#F97583\">:.2%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> exceeds threshold </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">threshold_gb</span><span style=\"color:#F97583\">:.2%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"gpu_available\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"allocated_gb\"</span><span style=\"color:#E1E4E8\">: allocated,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"cached_gb\"</span><span style=\"color:#E1E4E8\">: cached,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"max_gb\"</span><span style=\"color:#E1E4E8\">: max_memory,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"total_gb\"</span><span style=\"color:#E1E4E8\">: total_memory,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"usage_ratio\"</span><span style=\"color:#E1E4E8\">: usage_ratio,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"threshold_exceeded\"</span><span style=\"color:#E1E4E8\">: usage_ratio </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> threshold_gb</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># In your MultiHeadAttention class</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Forward pass with comprehensive error handling and numerical stability.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input tensor shapes and dimensions using input validator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply Q, K, V projections and check for numerical issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute attention scores with stability monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply causal mask and validate mask compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply softmax with LogSumExp trick for numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check attention weights for NaN/Inf and handle gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Apply attention dropout only during training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Compute weighted values and validate output shapes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Apply output projection and final validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use stability_monitor.stabilize_attention_scores() before softmax</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Always validate shapes before mathematical operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Log warnings for numerical issues but continue processing when possible</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># In your TransformerTrainer class  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _handle_training_step_errors</span><span style=\"color:#E1E4E8\">(self, batch_idx: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handle errors during training steps with recovery strategies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Identify error type (numerical instability, OOM, data corruption)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Log detailed error information including batch characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For numerical errors, reset gradients and continue with next batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For OOM errors, reduce batch size temporarily if possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For data errors, skip batch and log problematic data indices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update error counters and decide whether to continue training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return True to continue training, False to abort</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: torch.cuda.empty_cache() can help recover from OOM</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Keep error counters to detect systematic issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Different error types require different recovery strategies</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># In your TextGenerator class</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _validate_generation_state</span><span style=\"color:#E1E4E8\">(self, tokens: torch.Tensor, step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate generation state and detect potential issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if sequence length exceeds configured maximum</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate all token IDs are within vocabulary bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for repetitive patterns that indicate generation issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Monitor memory usage for KV cache growth</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Detect infinite generation loops (no EOS token progress)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate that probability distributions are well-formed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return False if generation should be terminated early</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Track recent tokens to detect repetition loops</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Set maximum steps to prevent infinite generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Check that probability distributions sum to approximately 1.0</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>PyTorch Stability</strong>: Use <code>torch.nn.utils.clip_grad_norm_()</code> for gradient clipping and <code>torch.cuda.amp.autocast()</code> for automatic mixed precision</li>\n<li><strong>Memory Management</strong>: Call <code>torch.cuda.empty_cache()</code> after handling OOM errors, and use <code>torch.cuda.memory_summary()</code> for detailed memory debugging</li>\n<li><strong>Numerical Checking</strong>: Use <code>torch.isfinite()</code> to check for both NaN and Inf simultaneously, and <code>torch.finfo(dtype).max</code> to get safe numerical limits</li>\n<li><strong>Error Recovery</strong>: Implement exponential backoff for retrying failed operations, and maintain error budgets to prevent infinite retry loops</li>\n</ul>\n<p><strong>Milestone Checkpoints:</strong></p>\n<p><strong>After Implementing Numerical Stability (All Milestones):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_numerical_stability.py -v</code></li>\n<li>Expected: All tests pass, including edge cases with extreme attention scores</li>\n<li>Manual verification: Train a small model with intentionally large learning rates - should see gradient clipping warnings but stable training loss</li>\n<li>Warning signs: NaN losses, exploding gradients that aren&#39;t caught by clipping, or attention weights that are all zeros or ones</li>\n</ul>\n<p><strong>After Implementing Input Validation (All Milestones):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_input_validation.py -v</code></li>\n<li>Expected: Validation catches all malformed inputs with informative error messages</li>\n<li>Manual verification: Try feeding invalid inputs (wrong shapes, out-of-bounds tokens) - should get clear error messages, not crashes</li>\n<li>Warning signs: Cryptic error messages from deep in PyTorch, silent failures with wrong outputs, or crashes without informative errors</li>\n</ul>\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training loss becomes NaN</td>\n<td>Gradient explosion or numerical instability</td>\n<td>Check gradient norms, attention score ranges</td>\n<td>Reduce learning rate, add gradient clipping</td>\n</tr>\n<tr>\n<td>Attention weights all uniform</td>\n<td>Attention scores too small or numerical underflow</td>\n<td>Print attention scores before softmax</td>\n<td>Increase temperature, check score computation</td>\n</tr>\n<tr>\n<td>Generation produces repetitive text</td>\n<td>Sampling strategy issues or numerical precision</td>\n<td>Check probability distributions, sampling logic</td>\n<td>Adjust temperature, implement repetition penalty</td>\n</tr>\n<tr>\n<td>Memory usage grows unexpectedly</td>\n<td>KV cache not cleared between sequences</td>\n<td>Monitor cache size, check clear() calls</td>\n<td>Implement proper cache management</td>\n</tr>\n<tr>\n<td>Model outputs become deterministic</td>\n<td>Dropout not disabled during inference</td>\n<td>Check model.eval() calls, dropout behavior</td>\n<td>Ensure model.eval() before generation</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy-and-milestones\">Testing Strategy and Milestones</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - systematic verification approaches for self-attention mechanism (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)</p>\n</blockquote>\n<p>Testing a transformer implementation requires a comprehensive strategy that validates both individual components and their integration. The challenge lies in verifying mathematical correctness, architectural consistency, and end-to-end behavior across multiple interconnected systems. Unlike traditional software testing that focuses primarily on business logic, transformer testing must validate tensor operations, gradient flows, attention patterns, and generative capabilities.</p>\n<p><strong>Mental Model: The Quality Assurance Pyramid</strong></p>\n<p>Think of transformer testing as a quality assurance pyramid in a manufacturing plant. At the base, we have component unit tests that verify each individual part meets specifications - like testing that each gear has the right number of teeth and rotates smoothly. In the middle, we have integration tests that verify assembled modules work together - like testing that the gear assembly transfers power correctly. At the top, we have end-to-end tests that verify the complete machine produces the desired output - like testing that the entire assembly line produces finished products to specification. Each level catches different types of defects, and failures at higher levels often trace back to issues at lower levels.</p>\n<p>The testing pyramid for transformers follows this same structure. Unit tests catch mathematical errors in individual attention heads or layer normalization computations. Integration tests catch architectural issues like dimension mismatches between transformer blocks or incorrect data flow through the training pipeline. End-to-end tests catch behavioral issues like repetitive text generation or failure to learn coherent patterns from training data.</p>\n<h3 id=\"component-unit-tests\">Component Unit Tests</h3>\n<p>Component unit tests form the foundation of transformer validation, focusing on the mathematical correctness and architectural consistency of individual modules. Each component must be tested in isolation to ensure it implements the expected mathematical operations with correct tensor shapes and numerical stability.</p>\n<p><strong>Attention Mechanism Testing Strategy</strong></p>\n<p>The attention mechanism requires particularly thorough testing due to its mathematical complexity and central role in transformer performance. Testing must validate the scaled dot-product computation, multi-head parallelization, causal masking behavior, and numerical stability under various input conditions.</p>\n<p>The core attention computation involves matrix multiplications, scaling operations, masking applications, and softmax normalizations that must maintain precise mathematical relationships. Testing verifies that query-key similarity scores are computed correctly, that scaling by the square root of the key dimension prevents gradient explosion, that causal masks properly zero out future positions, and that attention weights sum to unity after softmax normalization.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Focus Area</th>\n<th>Key Validations</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shape Consistency</td>\n<td>Tensor dimensions</td>\n<td>Input/output tensor shapes match specifications</td>\n<td>Q, K, V projections produce correct head dimensions</td>\n</tr>\n<tr>\n<td>Mathematical Correctness</td>\n<td>Attention computation</td>\n<td>Scaled dot-product follows formula exactly</td>\n<td>Attention scores = softmax(Q@K^T / sqrt(d_k))</td>\n</tr>\n<tr>\n<td>Masking Behavior</td>\n<td>Causal attention</td>\n<td>Future positions masked to negative infinity</td>\n<td>Upper triangular attention matrix is zero</td>\n</tr>\n<tr>\n<td>Multi-head Parallelization</td>\n<td>Head computation</td>\n<td>Each head operates on correct dimension slice</td>\n<td>Concatenated heads restore original d_model dimension</td>\n</tr>\n<tr>\n<td>Numerical Stability</td>\n<td>Softmax overflow</td>\n<td>Large attention scores don&#39;t cause NaN values</td>\n<td>LogSumExp trick prevents numerical overflow</td>\n</tr>\n<tr>\n<td>Gradient Flow</td>\n<td>Backpropagation</td>\n<td>Gradients flow correctly through attention layers</td>\n<td>Non-zero gradients reach input embeddings</td>\n</tr>\n</tbody></table>\n<p><strong>Feed-Forward Network Testing Strategy</strong></p>\n<p>The feed-forward network testing focuses on the two-layer MLP structure with expansion and projection operations. Testing must validate dimension transformations, activation function application, and dropout behavior during training and inference modes.</p>\n<p>The feed-forward network expands input representations to four times the model dimension, applies a non-linear activation, then projects back to the original dimension. This expansion-contraction pattern must preserve batch and sequence dimensions while transforming the feature dimension according to the specified expansion ratio.</p>\n<table>\n<thead>\n<tr>\n<th>Test Component</th>\n<th>Validation Focus</th>\n<th>Test Cases</th>\n<th>Expected Results</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Expansion Layer</td>\n<td>Dimension transformation</td>\n<td>Input (batch, seq, d_model) → Hidden (batch, seq, 4*d_model)</td>\n<td>Correct 4x expansion ratio</td>\n</tr>\n<tr>\n<td>Activation Function</td>\n<td>Non-linearity</td>\n<td>ReLU/GELU application to expanded representations</td>\n<td>Proper activation of positive values</td>\n</tr>\n<tr>\n<td>Projection Layer</td>\n<td>Dimension restoration</td>\n<td>Hidden (batch, seq, 4*d_model) → Output (batch, seq, d_model)</td>\n<td>Return to original dimensions</td>\n</tr>\n<tr>\n<td>Dropout Behavior</td>\n<td>Regularization</td>\n<td>Different behavior in train vs eval mode</td>\n<td>Random zeroing in training, identity in inference</td>\n</tr>\n<tr>\n<td>Parameter Initialization</td>\n<td>Weight values</td>\n<td>Proper initialization prevents vanishing/exploding gradients</td>\n<td>Weights follow recommended initialization scheme</td>\n</tr>\n<tr>\n<td>Residual Addition</td>\n<td>Skip connections</td>\n<td>Output added to input maintains gradient flow</td>\n<td>Input + FFN(LayerNorm(Input)) pattern</td>\n</tr>\n</tbody></table>\n<p><strong>Layer Normalization Testing Strategy</strong></p>\n<p>Layer normalization testing validates the stabilization of activations across the feature dimension for each token independently. Testing must verify mean centering, variance scaling, learnable parameter application, and numerical stability across different input distributions.</p>\n<p>Layer normalization computes statistics across the feature dimension for each token in each sequence independently, then applies learned scale and shift parameters. This per-token normalization differs from batch normalization and requires specific validation of the statistical computations and parameter applications.</p>\n<table>\n<thead>\n<tr>\n<th>Normalization Aspect</th>\n<th>Test Focus</th>\n<th>Validation Method</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Mean Centering</td>\n<td>Zero mean per token</td>\n<td>Compute mean across d_model dimension</td>\n<td>Mean ≈ 0 within numerical precision</td>\n</tr>\n<tr>\n<td>Variance Scaling</td>\n<td>Unit variance per token</td>\n<td>Compute variance across d_model dimension</td>\n<td>Variance ≈ 1 within numerical precision</td>\n</tr>\n<tr>\n<td>Parameter Application</td>\n<td>Scale and shift</td>\n<td>Apply learned gamma and beta parameters</td>\n<td>Output = gamma * normalized + beta</td>\n</tr>\n<tr>\n<td>Epsilon Handling</td>\n<td>Numerical stability</td>\n<td>Very small variance inputs</td>\n<td>No division by zero or NaN values</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>Parameter updates</td>\n<td>Gradients flow to gamma and beta</td>\n<td>Non-zero gradients for learnable parameters</td>\n</tr>\n<tr>\n<td>Batch Independence</td>\n<td>Per-token operation</td>\n<td>Different sequences don&#39;t affect each other</td>\n<td>Normalization statistics computed independently</td>\n</tr>\n</tbody></table>\n<p><strong>Tokenization and Data Loading Testing</strong></p>\n<p>Tokenization and data loading components require validation of text-to-integer conversion, vocabulary handling, special token management, and batch construction. Testing must verify correct encoding and decoding, proper sequence length handling, and appropriate padding or truncation behavior.</p>\n<p>The tokenization system converts between text strings and integer sequences that feed into the transformer. This conversion must be deterministic, reversible, and handle edge cases like unknown characters, empty strings, and sequences exceeding maximum length limits.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Test Category</th>\n<th>Validation Points</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Text Encoding</td>\n<td>String to tokens</td>\n<td>Consistent tokenization of same text</td>\n<td>Deterministic token ID sequences</td>\n</tr>\n<tr>\n<td>Text Decoding</td>\n<td>Tokens to string</td>\n<td>Reconstruction of original text</td>\n<td>Exact or semantically equivalent text recovery</td>\n</tr>\n<tr>\n<td>Vocabulary Handling</td>\n<td>Token ID bounds</td>\n<td>All tokens within valid vocabulary range</td>\n<td>Token IDs in [0, vocab_size) range</td>\n</tr>\n<tr>\n<td>Special Tokens</td>\n<td>Padding and EOS</td>\n<td>Correct insertion of special tokens</td>\n<td>Proper pad_token_id and eos_token_id usage</td>\n</tr>\n<tr>\n<td>Sequence Length</td>\n<td>Truncation and padding</td>\n<td>Handling of variable-length sequences</td>\n<td>Fixed-length sequences for batching</td>\n</tr>\n<tr>\n<td>Batch Construction</td>\n<td>Data loader output</td>\n<td>Correct input-target pair construction</td>\n<td>Proper label shifting for next-token prediction</td>\n</tr>\n</tbody></table>\n<h3 id=\"end-to-end-integration\">End-to-End Integration</h3>\n<p>End-to-end integration testing validates the complete transformer pipeline from text input to generated output, ensuring that all components work together correctly and produce expected behaviors. Integration testing catches issues that component unit tests miss, particularly around data flow, shape consistency, and behavioral correctness across component boundaries.</p>\n<p><strong>Training Pipeline Integration</strong></p>\n<p>Training pipeline integration testing validates the complete learning process from tokenized input through gradient computation and parameter updates. Testing must verify that the model can overfit on small datasets, that loss decreases consistently, and that gradients flow properly through all components.</p>\n<p>The training integration test starts with a minimal dataset that the model should easily memorize, then verifies that training loss decreases to near zero over multiple epochs. This overfitting test confirms that the model architecture can learn and that the training loop correctly computes gradients and updates parameters.</p>\n<blockquote>\n<p><strong>Decision: Overfitting Test as Primary Integration Validation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to verify complete training pipeline works correctly before testing on realistic datasets</li>\n<li><strong>Options Considered</strong>: Large dataset convergence test, synthetic data pattern learning, small dataset overfitting test</li>\n<li><strong>Decision</strong>: Use small dataset overfitting test as primary integration validation</li>\n<li><strong>Rationale</strong>: Overfitting test isolates architectural issues from data complexity, provides fast feedback, and guarantees expected outcome if implementation is correct</li>\n<li><strong>Consequences</strong>: Enables rapid validation of training correctness but doesn&#39;t test generalization capabilities until later testing phases</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Integration Test</th>\n<th>Dataset Characteristics</th>\n<th>Success Criteria</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Overfitting Test</td>\n<td>10-20 short sequences, repeated patterns</td>\n<td>Training loss &lt; 0.01 within 100 epochs</td>\n<td>Loss plateaus above 1.0, gradients vanish</td>\n</tr>\n<tr>\n<td>Shape Consistency</td>\n<td>Various sequence lengths and batch sizes</td>\n<td>No dimension mismatch errors during training</td>\n<td>Runtime errors about tensor shapes</td>\n</tr>\n<tr>\n<td>Gradient Flow</td>\n<td>Deep transformer stack</td>\n<td>Non-zero gradients reach embedding layer</td>\n<td>Zero gradients in early layers</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Large batch sizes</td>\n<td>Stable memory usage throughout training</td>\n<td>Out-of-memory errors or memory leaks</td>\n</tr>\n<tr>\n<td>Checkpoint Saving</td>\n<td>Training interruption and resumption</td>\n<td>Identical loss trajectory after restart</td>\n<td>Different loss values after checkpoint reload</td>\n</tr>\n<tr>\n<td>Validation Loop</td>\n<td>Separate validation dataset</td>\n<td>Validation loss computed without gradients</td>\n<td>Validation loss changes during eval mode</td>\n</tr>\n</tbody></table>\n<p><strong>Generation Pipeline Integration</strong></p>\n<p>Generation pipeline integration testing validates the autoregressive text generation process from input prompt to coherent output text. Testing must verify that generation produces valid token sequences, respects length limits, and applies sampling strategies correctly.</p>\n<p>The generation integration test evaluates both the mechanical correctness of the generation loop and the quality of generated text. Mechanical correctness includes proper token prediction, sampling strategy application, and sequence termination. Text quality assessment focuses on coherence, repetition avoidance, and adherence to the input prompt.</p>\n<table>\n<thead>\n<tr>\n<th>Generation Aspect</th>\n<th>Test Approach</th>\n<th>Validation Method</th>\n<th>Quality Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Prediction</td>\n<td>Simple prompts</td>\n<td>Verify logits shape and vocabulary bounds</td>\n<td>Output logits have shape (1, vocab_size)</td>\n</tr>\n<tr>\n<td>Sampling Strategies</td>\n<td>Deterministic vs stochastic</td>\n<td>Compare greedy, temperature, top-k, top-p outputs</td>\n<td>Different strategies produce different texts</td>\n</tr>\n<tr>\n<td>Sequence Termination</td>\n<td>EOS token generation</td>\n<td>Verify generation stops at EOS or max length</td>\n<td>Generation respects configured length limits</td>\n</tr>\n<tr>\n<td>Prompt Conditioning</td>\n<td>Specific input prompts</td>\n<td>Generated text continues prompt appropriately</td>\n<td>Output relates semantically to input prompt</td>\n</tr>\n<tr>\n<td>Repetition Handling</td>\n<td>Repetition penalty application</td>\n<td>Generated text avoids excessive repetition</td>\n<td>Repetition penalty reduces token probability</td>\n</tr>\n<tr>\n<td>KV Cache Correctness</td>\n<td>Cache vs non-cache generation</td>\n<td>Identical outputs with and without caching</td>\n<td>Cache optimization doesn&#39;t change generation</td>\n</tr>\n</tbody></table>\n<p><strong>Memory and Performance Integration</strong></p>\n<p>Memory and performance integration testing validates resource usage patterns, computational efficiency, and scalability characteristics across different model sizes and sequence lengths. Testing must verify that memory usage remains within expected bounds and that performance scales appropriately with model parameters.</p>\n<p>The transformer architecture has specific memory and computational scaling characteristics that must be validated during integration testing. Attention mechanisms scale quadratically with sequence length, while feed-forward networks scale linearly with model dimension. These scaling patterns must be verified to match theoretical expectations.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Aspect</th>\n<th>Scaling Pattern</th>\n<th>Measurement Method</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Training Memory</td>\n<td>Linear with batch size</td>\n<td>Monitor GPU memory during training</td>\n<td>Memory usage = base + batch_size * per_sample</td>\n</tr>\n<tr>\n<td>Attention Memory</td>\n<td>Quadratic with sequence length</td>\n<td>Measure attention matrix memory</td>\n<td>Memory ∝ seq_length² * num_heads</td>\n</tr>\n<tr>\n<td>Generation Memory</td>\n<td>Linear with sequence length</td>\n<td>KV cache memory growth during generation</td>\n<td>Memory grows linearly with generated tokens</td>\n</tr>\n<tr>\n<td>Training Speed</td>\n<td>Linear with model parameters</td>\n<td>Time per training step measurement</td>\n<td>Step time ∝ parameter count</td>\n</tr>\n<tr>\n<td>Generation Speed</td>\n<td>Constant per token</td>\n<td>Time per generated token</td>\n<td>Consistent token generation speed</td>\n</tr>\n<tr>\n<td>Parameter Scaling</td>\n<td>Expected parameter count</td>\n<td>Count trainable parameters</td>\n<td>Parameters match theoretical calculation</td>\n</tr>\n</tbody></table>\n<h3 id=\"milestone-verification\">Milestone Verification</h3>\n<p>Milestone verification provides structured checkpoints for validating progress through the transformer implementation. Each milestone has specific acceptance criteria that must be met before proceeding to the next phase, ensuring that foundational components work correctly before building dependent functionality.</p>\n<p><strong>Milestone 1: Self-Attention Verification</strong></p>\n<p>The self-attention milestone verification focuses on the mathematical correctness and architectural consistency of the attention mechanism. Verification must confirm that attention computations follow the scaled dot-product formula, that multi-head attention operates correctly, and that causal masking prevents information leakage from future positions.</p>\n<p>Attention verification requires both mathematical validation and behavioral testing. Mathematical validation confirms that attention weights are computed according to the specified formula and that tensor dimensions match architectural requirements. Behavioral testing verifies that attention patterns make intuitive sense for language modeling tasks.</p>\n<table>\n<thead>\n<tr>\n<th>Verification Component</th>\n<th>Test Method</th>\n<th>Success Criteria</th>\n<th>Validation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Q, K, V Projections</td>\n<td>Shape inspection</td>\n<td>Projections produce (batch, seq, num_heads, d_k) tensors</td>\n<td>Assert tensor shapes after linear transformations</td>\n</tr>\n<tr>\n<td>Attention Score Computation</td>\n<td>Mathematical validation</td>\n<td>Scores = Q@K^T / sqrt(d_k) exactly</td>\n<td>Compare computed scores with manual calculation</td>\n</tr>\n<tr>\n<td>Causal Mask Application</td>\n<td>Upper triangle verification</td>\n<td>Future positions have attention weight 0</td>\n<td>Verify attention matrix upper triangle is zero</td>\n</tr>\n<tr>\n<td>Multi-head Concatenation</td>\n<td>Dimension restoration</td>\n<td>Concatenated heads restore d_model dimension</td>\n<td>Assert output shape matches input embedding shape</td>\n</tr>\n<tr>\n<td>Attention Pattern Visualization</td>\n<td>Heatmap inspection</td>\n<td>Attention focuses on relevant previous tokens</td>\n<td>Visual inspection of attention weight matrices</td>\n</tr>\n<tr>\n<td>Gradient Flow Validation</td>\n<td>Backpropagation test</td>\n<td>Gradients propagate to Q, K, V projections</td>\n<td>Verify non-zero gradients after loss.backward()</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 2: Transformer Block Verification</strong></p>\n<p>The transformer block milestone verification validates the complete transformer layer including attention, feed-forward network, layer normalization, and residual connections. Verification must confirm that all sub-components integrate correctly and that information flows properly through the block architecture.</p>\n<p>Transformer block verification tests the interaction between attention and feed-forward sublayers, proper placement of layer normalization, and effectiveness of residual connections for gradient flow. The verification must also confirm that dropout operates correctly during training and is disabled during inference.</p>\n<table>\n<thead>\n<tr>\n<th>Block Component</th>\n<th>Verification Focus</th>\n<th>Test Procedure</th>\n<th>Expected Results</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sublayer Integration</td>\n<td>Attention + FFN composition</td>\n<td>Forward pass through complete block</td>\n<td>Output shape matches input shape</td>\n</tr>\n<tr>\n<td>Residual Connections</td>\n<td>Skip connection functionality</td>\n<td>Compare with and without residual connections</td>\n<td>Residual connections improve gradient flow</td>\n</tr>\n<tr>\n<td>Layer Normalization Placement</td>\n<td>Pre-norm architecture</td>\n<td>Verify LayerNorm applied before sublayers</td>\n<td>LayerNorm(x) fed to attention and FFN</td>\n</tr>\n<tr>\n<td>Dropout Behavior</td>\n<td>Training vs inference modes</td>\n<td>Test dropout in both modes</td>\n<td>Dropout active in training, disabled in inference</td>\n</tr>\n<tr>\n<td>Parameter Count</td>\n<td>Expected parameter total</td>\n<td>Count block parameters</td>\n<td>Parameters match theoretical calculation</td>\n</tr>\n<tr>\n<td>Information Flow</td>\n<td>Input to output transformation</td>\n<td>End-to-end block processing</td>\n<td>Block processes information without errors</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 3: Training Pipeline Verification</strong></p>\n<p>The training pipeline milestone verification validates the complete learning system including tokenization, data loading, loss computation, and parameter optimization. Verification must confirm that the model can learn from training data and that all training components operate correctly.</p>\n<p>Training pipeline verification requires both mechanical and learning validation. Mechanical validation confirms that data flows correctly through the training loop without errors. Learning validation confirms that the model actually learns patterns from training data by demonstrating decreasing loss and improved performance.</p>\n<table>\n<thead>\n<tr>\n<th>Pipeline Component</th>\n<th>Verification Method</th>\n<th>Success Indicators</th>\n<th>Troubleshooting Signs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenization Accuracy</td>\n<td>Encode-decode test</td>\n<td>Perfect reconstruction of input text</td>\n<td>Text corruption indicates tokenization errors</td>\n</tr>\n<tr>\n<td>Data Loader Functionality</td>\n<td>Batch inspection</td>\n<td>Correct input-target pair construction</td>\n<td>Shape mismatches indicate data loading issues</td>\n</tr>\n<tr>\n<td>Loss Computation</td>\n<td>Cross-entropy validation</td>\n<td>Loss decreases monotonically on training data</td>\n<td>Increasing loss indicates gradient or learning rate issues</td>\n</tr>\n<tr>\n<td>Parameter Updates</td>\n<td>Gradient inspection</td>\n<td>Parameters change after optimizer step</td>\n<td>Unchanged parameters indicate optimization issues</td>\n</tr>\n<tr>\n<td>Overfitting Capability</td>\n<td>Small dataset memorization</td>\n<td>Training loss approaches zero</td>\n<td>Inability to overfit indicates architectural problems</td>\n</tr>\n<tr>\n<td>Validation Integration</td>\n<td>Separate validation loop</td>\n<td>Validation loss computed without affecting training</td>\n<td>Validation affecting training indicates mode switching issues</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 4: Text Generation Verification</strong></p>\n<p>The text generation milestone verification validates the autoregressive generation capability including sampling strategies, sequence management, and text quality. Verification must confirm that generation produces coherent text that follows the input prompt and respects generation constraints.</p>\n<p>Generation verification encompasses both mechanical correctness and output quality assessment. Mechanical verification confirms that generation mechanics work without errors and respect configured constraints. Quality assessment evaluates whether generated text demonstrates learning and produces reasonable continuations of input prompts.</p>\n<table>\n<thead>\n<tr>\n<th>Generation Aspect</th>\n<th>Verification Approach</th>\n<th>Quality Metrics</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Generation</td>\n<td>Simple prompt completion</td>\n<td>Generated text continues prompt logically</td>\n<td>Immediate repetition or incoherent output</td>\n</tr>\n<tr>\n<td>Sampling Strategy Variety</td>\n<td>Compare different sampling methods</td>\n<td>Different strategies produce different outputs</td>\n<td>All strategies produce identical text</td>\n</tr>\n<tr>\n<td>Length Control</td>\n<td>Maximum length constraints</td>\n<td>Generation respects max_length parameter</td>\n<td>Generation exceeds specified length limits</td>\n</tr>\n<tr>\n<td>EOS Token Handling</td>\n<td>End-of-sequence termination</td>\n<td>Generation stops appropriately at EOS</td>\n<td>Generation continues past EOS token</td>\n</tr>\n<tr>\n<td>Repetition Avoidance</td>\n<td>Repetition penalty effectiveness</td>\n<td>Generated text avoids excessive repetition</td>\n<td>Pathological repetition patterns</td>\n</tr>\n<tr>\n<td>Prompt Adherence</td>\n<td>Semantic continuation</td>\n<td>Generated text relates to input prompt</td>\n<td>Generated text ignores prompt context</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing implementation requires a systematic approach to component validation, integration verification, and milestone checkpoints. The testing framework should provide clear feedback about implementation correctness and guide developers toward fixing identified issues.</p>\n<p><strong>Technology Recommendations for Testing</strong></p>\n<table>\n<thead>\n<tr>\n<th>Testing Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing Framework</td>\n<td>pytest with basic assertions</td>\n<td>pytest with fixtures and parameterization</td>\n</tr>\n<tr>\n<td>Tensor Validation</td>\n<td>Manual shape assertions</td>\n<td>torch.testing.assert_close for numerical comparison</td>\n</tr>\n<tr>\n<td>Visualization Tools</td>\n<td>matplotlib for attention heatmaps</td>\n<td>wandb for experiment tracking and visualization</td>\n</tr>\n<tr>\n<td>Performance Monitoring</td>\n<td>time.time() for basic timing</td>\n<td>torch.profiler for detailed performance analysis</td>\n</tr>\n<tr>\n<td>Memory Tracking</td>\n<td>torch.cuda.memory_allocated()</td>\n<td>torch.cuda.profiler for memory timeline</td>\n</tr>\n<tr>\n<td>Integration Testing</td>\n<td>Simple overfitting test</td>\n<td>Comprehensive benchmark suite</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure for Testing</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  tests/\n    unit/\n      test_attention.py           ← attention mechanism unit tests\n      test_transformer_block.py   ← transformer block unit tests  \n      test_tokenizer.py           ← tokenization unit tests\n      test_data_loading.py        ← data loading unit tests\n    integration/\n      test_training_pipeline.py   ← end-to-end training tests\n      test_generation_pipeline.py ← end-to-end generation tests\n      test_memory_performance.py  ← resource usage tests\n    fixtures/\n      sample_data.txt             ← test data for integration tests\n      expected_outputs.json       ← expected results for validation\n    utils/\n      test_helpers.py             ← common testing utilities\n      visualization.py            ← attention visualization helpers\n  src/\n    transformer/\n      attention.py                ← attention implementation\n      transformer_block.py        ← transformer block implementation\n      training.py                 ← training pipeline implementation\n      generation.py               ← generation implementation</code></pre></div>\n\n<p><strong>Component Testing Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerTestSuite</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive testing utilities for transformer components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.device(</span><span style=\"color:#9ECBFF\">'cuda'</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'cpu'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_test_inputs</span><span style=\"color:#E1E4E8\">(self, batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">, seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create test input tensors with valid token IDs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.randint(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.vocab_size, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           (batch_size, seq_length), </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.device)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_attention_output_shapes</span><span style=\"color:#E1E4E8\">(self, attention: MultiHeadAttention, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       input_tensor: torch.Tensor) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that attention produces correct output shapes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create causal mask for input sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run forward pass through attention layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Assert output shape matches input shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify attention weights have correct dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that attention weights sum to 1 across key dimension</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_attention_causality</span><span style=\"color:#E1E4E8\">(self, attention: MultiHeadAttention,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               input_tensor: torch.Tensor) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify that attention respects causal masking constraints.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract attention weights from forward pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify upper triangular portion is zero (future masking)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that each position only attends to previous positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that diagonal and lower triangle have positive weights</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> measure_gradient_flow</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            loss: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze gradient magnitudes throughout the model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Perform backward pass on loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract gradients from each named parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute gradient norms for each layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return dictionary mapping layer names to gradient magnitudes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify layers with vanishing gradients (norm &#x3C; 1e-6)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> plot_attention_weights</span><span style=\"color:#E1E4E8\">(weights: torch.Tensor, tokens: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Visualize attention patterns as heatmap.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Complete implementation for attention visualization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    weights_np </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> weights[layer, head].detach().cpu().numpy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.figure(</span><span style=\"color:#FFAB70\">figsize</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.imshow(weights_np, </span><span style=\"color:#FFAB70\">cmap</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Blues'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interpolation</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'nearest'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.colorbar(</span><span style=\"color:#FFAB70\">label</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'Attention Weight'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.xticks(</span><span style=\"color:#79B8FF\">range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(tokens)), tokens, </span><span style=\"color:#FFAB70\">rotation</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">45</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.yticks(</span><span style=\"color:#79B8FF\">range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(tokens)), tokens)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.xlabel(</span><span style=\"color:#9ECBFF\">'Key Position'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.ylabel(</span><span style=\"color:#9ECBFF\">'Query Position'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.title(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'Attention Weights - Layer </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">layer</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, Head </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">head</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.tight_layout()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plt.show()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> count_parameters</span><span style=\"color:#E1E4E8\">(model: torch.nn.Module) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Count total trainable parameters in model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(p.numel() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.parameters() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> p.requires_grad)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_shape_consistency</span><span style=\"color:#E1E4E8\">(model: torch.nn.Module, input_shape: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         device: torch.device) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Verify model maintains correct tensor shapes throughout forward pass.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Complete implementation for shape validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model.eval()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.randint(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, input_shape, </span><span style=\"color:#FFAB70\">device</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">device)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> torch.no_grad():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            output </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model(test_input)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_output_shape </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">input_shape, model.config.vocab_size)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> output.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expected_output_shape, \\</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Expected output shape </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_output_shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">output.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Shape consistency test failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> check_gradient_flow</span><span style=\"color:#E1E4E8\">(model: torch.nn.Module, loss: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Analyze gradient magnitudes after backward pass.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Complete implementation for gradient analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    loss.backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, param </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> model.named_parameters():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> param.grad </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> param.grad.data.norm().item()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gradient_info[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'grad_norm'</span><span style=\"color:#E1E4E8\">: grad_norm,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'param_norm'</span><span style=\"color:#E1E4E8\">: param.data.norm().item(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'grad_param_ratio'</span><span style=\"color:#E1E4E8\">: grad_norm </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (param.data.norm().item() </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1e-8</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            gradient_info[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'grad_norm'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'param_norm'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'grad_param_ratio'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> gradient_info</span></span></code></pre></div>\n\n<p><strong>Integration Testing Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerIntegrationTester</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"End-to-end integration testing for transformer pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig, training_config: TrainingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.training_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> training_config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.device </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.device(</span><span style=\"color:#9ECBFF\">'cuda'</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available() </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'cpu'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_overfitting_dataset</span><span style=\"color:#E1E4E8\">(self) -> Tuple[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create small dataset for overfitting test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Complete implementation with small, repetitive dataset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        train_texts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"The cat sat on the mat.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"The dog ran in the park.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"The bird flew in the sky.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"The fish swam in the water.\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"The cat sat on the mat.\"</span><span style=\"color:#6A737D\">  # Repeat for memorization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ] </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#6A737D\">  # Replicate for sufficient training data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        val_texts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> train_texts[:</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Small validation set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> train_texts, val_texts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_training_overfitting</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                tokenizer: SimpleTokenizer) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test that model can overfit on small dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create small training dataset with repetitive patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up training loop with high learning rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Train for multiple epochs until loss &#x3C; 0.01</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record loss trajectory and final loss value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify that model memorizes training sequences exactly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return training metrics and success indicators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_generation_quality</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              tokenizer: SimpleTokenizer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              generator: TextGenerator) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate generation quality on various prompts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Define test prompts covering different scenarios</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate text using different sampling strategies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Evaluate coherence, repetition, and prompt adherence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for pathological patterns (infinite loops, repetition)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure generation speed and memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return quality metrics and generated samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_kv_cache_correctness</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    generator: TextGenerator) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify KV cache optimization produces identical results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate text with KV cache enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate same text with KV cache disabled  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare outputs token by token for exact match</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure speed improvement from caching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify memory usage patterns are as expected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_data_loaders</span><span style=\"color:#E1E4E8\">(train_tokens: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">], val_tokens: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       num_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> Tuple[torch.utils.data.DataLoader, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                                     torch.utils.data.DataLoader]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create train and validation data loaders for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Complete implementation for data loader creation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    train_dataset </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TextDataset(train_tokens, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    val_dataset </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TextDataset(val_tokens, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    train_loader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.utils.data.DataLoader(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        train_dataset, </span><span style=\"color:#FFAB70\">batch_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">batch_size, </span><span style=\"color:#FFAB70\">shuffle</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        num_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">num_workers, </span><span style=\"color:#FFAB70\">pin_memory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.cuda.is_available()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    val_loader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.utils.data.DataLoader(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        val_dataset, </span><span style=\"color:#FFAB70\">batch_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">batch_size, </span><span style=\"color:#FFAB70\">shuffle</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        num_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">num_workers, </span><span style=\"color:#FFAB70\">pin_memory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">torch.cuda.is_available()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> train_loader, val_loader</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MilestoneValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Automated validation for transformer implementation milestones.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_1_attention</span><span style=\"color:#E1E4E8\">(self, attention: MultiHeadAttention,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                     config: TransformerConfig) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate self-attention implementation (Milestone 1).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test Q, K, V projection dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create test input: (batch=2, seq=8, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify projections produce (batch=2, seq=8, num_heads, d_k) tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that d_k * num_heads == d_model for dimension consistency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test attention score computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Manually compute Q@K^T/sqrt(d_k) and compare with attention output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify scaling factor is applied correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check numerical stability with large input values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test causal mask application  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify upper triangle of attention weights is exactly zero</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that mask is applied before softmax, not after</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure diagonal and lower triangle have positive attention weights</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test multi-head concatenation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify concatenated output has shape (batch, seq, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that final projection restores original embedding dimension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure all heads contribute to final output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_2_transformer_block</span><span style=\"color:#E1E4E8\">(self, block: TransformerBlock,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                             config: TransformerConfig) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate transformer block implementation (Milestone 2).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test feed-forward network expansion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify FFN expands to 4x hidden dimension then projects back</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that activation function is applied between layers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure output dimension matches input dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test layer normalization behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify mean ≈ 0 and variance ≈ 1 across feature dimension per token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that learned gamma and beta parameters are applied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure normalization is applied before sublayers (pre-norm)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test residual connections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify input is added to sublayer output: output = input + sublayer(norm(input))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check gradient flow improvement with residuals vs without</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure residual paths preserve gradient magnitudes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test dropout functionality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify dropout is active during training mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that dropout is disabled during eval mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure dropout rate matches configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_3_training_pipeline</span><span style=\"color:#E1E4E8\">(self, trainer: TransformerTrainer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                             tokenizer: SimpleTokenizer) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate training pipeline implementation (Milestone 3).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test tokenization accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Encode and decode text strings, verify perfect reconstruction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check special token handling (pad_token_id, eos_token_id)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure vocabulary bounds are respected</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test data loader functionality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify batch construction with correct input-target pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check label shifting for next-token prediction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure sequence length handling and padding behavior</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test loss computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify cross-entropy loss decreases on training data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that loss ignores padding tokens in computation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure gradients are computed correctly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test parameter updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify parameters change after optimizer step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check learning rate scheduling if implemented</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure gradient clipping prevents explosion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_4_text_generation</span><span style=\"color:#E1E4E8\">(self, generator: TextGenerator,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                           config: GenerationConfig) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate text generation implementation (Milestone 4).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test greedy decoding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify deterministic selection of highest probability token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that same prompt always produces same output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure greedy decoding respects length limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test temperature sampling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify temperature scaling affects output randomness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that temperature=0 approaches greedy decoding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure higher temperatures increase diversity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test top-k and top-p sampling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify top-k restricts to k most likely tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check top-p restricts to nucleus of cumulative probability p</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure both methods respect temperature parameter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test sequence management</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify generation stops at EOS token or max length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check that generated tokens are within vocabulary bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure repetition penalty reduces repetitive patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Debugging utilities for systematic issue diagnosis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> diagnose_attention_issues</span><span style=\"color:#E1E4E8\">(attention: MultiHeadAttention, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            input_tensor: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Systematic diagnosis of attention mechanism problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diagnostics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check input tensor properties</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify input shape, data type, device placement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for NaN or infinite values in input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Ensure input values are within reasonable range</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze attention weight patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Extract and visualize attention weight matrices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for degenerate patterns (all attention on first/last token)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify attention weights sum to 1 across key dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test gradient flow through attention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Compute gradients with respect to Q, K, V projections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for vanishing gradients in deep attention stacks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify gradient magnitudes are reasonable</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> diagnostics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> diagnose_training_issues</span><span style=\"color:#E1E4E8\">(trainer: TransformerTrainer, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           train_loader: torch.utils.data.DataLoader) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Systematic diagnosis of training pipeline problems.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    diagnostics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze loss trajectory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check if loss is decreasing, increasing, or plateauing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Identify potential learning rate issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Detect gradient explosion or vanishing symptoms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Examine data loading</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify batch construction and label shifting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for data corruption or preprocessing errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Ensure consistent data loader behavior across epochs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Monitor resource usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Track memory usage patterns during training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check for memory leaks or excessive allocations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Monitor GPU utilization and potential bottlenecks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> diagnostics</span></span></code></pre></div>\n\n<p>The testing strategy provides comprehensive validation across all transformer components and milestones. Component unit tests ensure mathematical correctness and architectural consistency at the individual module level. Integration tests validate end-to-end behavior and catch issues that emerge from component interactions. Milestone verification provides structured checkpoints that guide systematic implementation progress.</p>\n<p>The testing infrastructure includes both automated validation and manual inspection tools. Automated tests catch regression errors and verify correctness properties that can be mathematically validated. Manual inspection tools like attention visualization and gradient flow analysis help developers understand model behavior and diagnose subtle issues that automated tests might miss.</p>\n<p>Successful completion of all milestone validations ensures that the transformer implementation meets the specified requirements and demonstrates expected behavior across self-attention mechanisms, transformer blocks, training pipelines, and text generation capabilities.</p>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - systematic diagnosis and resolution of implementation bugs across self-attention mechanism (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)</p>\n</blockquote>\n<p>Debugging transformer implementations requires a systematic approach that addresses the unique challenges of attention mechanisms, gradient-based training, and autoregressive generation. Unlike simpler neural networks, transformers involve complex matrix operations with specific dimension requirements, numerical stability concerns, and interdependent components where bugs in one layer can cascade through the entire architecture.</p>\n<h3 id=\"mental-model-the-debugging-detective\">Mental Model: The Debugging Detective</h3>\n<p>Think of debugging a transformer like being a detective investigating a crime scene. You have multiple suspects (attention weights, gradient flows, sampling strategies), various pieces of evidence (loss curves, attention heatmaps, generated text quality), and you need to systematically eliminate possibilities to find the root cause. Just as a detective follows a methodical process - securing the scene, gathering evidence, forming hypotheses, testing theories - transformer debugging requires structured investigation rather than random code changes.</p>\n<p>The key insight is that transformer bugs often manifest in predictable patterns. A dimension mismatch in attention computation creates specific error signatures. Gradient explosion produces characteristic loss spikes. Poor generation quality has distinct patterns based on the underlying cause. By learning to recognize these patterns and following systematic diagnostic procedures, you can quickly isolate and fix even complex bugs.</p>\n<h3 id=\"attention-mechanism-debugging\">Attention Mechanism Debugging</h3>\n<p>The self-attention mechanism represents the most mathematically complex component of transformers, making it a frequent source of implementation bugs. Attention debugging requires understanding both the mathematical relationships between queries, keys, and values, and the expected patterns in attention weight distributions.</p>\n<p><strong>Attention Weight Pattern Analysis</strong></p>\n<p>Healthy attention patterns exhibit specific characteristics that reveal whether the mechanism is functioning correctly. In causal self-attention, each position should only attend to previous positions, creating a lower-triangular pattern when visualized. The attention weights for each query position should sum to 1.0 after softmax normalization, and the distribution should show meaningful focus on relevant tokens rather than uniform attention across all positions.</p>\n<table>\n<thead>\n<tr>\n<th>Pattern Type</th>\n<th>Expected Behavior</th>\n<th>Problematic Signs</th>\n<th>Diagnostic Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Causal Masking</td>\n<td>Lower triangular attention matrix</td>\n<td>Upper triangular has non-zero values</td>\n<td>Check mask application order</td>\n</tr>\n<tr>\n<td>Weight Normalization</td>\n<td>Each row sums to 1.0</td>\n<td>Row sums deviate significantly</td>\n<td>Verify softmax implementation</td>\n</tr>\n<tr>\n<td>Attention Focus</td>\n<td>Concentrated weights on relevant tokens</td>\n<td>Uniform distribution across all positions</td>\n<td>Check scaling and temperature</td>\n</tr>\n<tr>\n<td>Multi-Head Diversity</td>\n<td>Different heads show different patterns</td>\n<td>All heads identical or random</td>\n<td>Verify head initialization</td>\n</tr>\n<tr>\n<td>Position Encoding</td>\n<td>Positional patterns in early layers</td>\n<td>No positional structure visible</td>\n<td>Check positional embedding addition</td>\n</tr>\n</tbody></table>\n<p><strong>Dimension Mismatch Diagnosis</strong></p>\n<p>Attention mechanisms involve multiple matrix operations with strict dimension requirements. The most common dimension errors occur during the scaled dot-product computation, multi-head concatenation, and output projection. These errors often produce cryptic PyTorch error messages that don&#39;t clearly indicate the source of the problem.</p>\n<p>The query, key, and value tensors must maintain consistent dimensions throughout the attention computation. For batch size <code>B</code>, sequence length <code>S</code>, number of heads <code>H</code>, and head dimension <code>D_K</code>, the expected shapes are: Q, K, V should be <code>[B, H, S, D_K]</code> after head projection and reshaping. The attention scores matrix should be <code>[B, H, S, S]</code>, and the final output should be <code>[B, S, d_model]</code> after concatenation and projection.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Stage</th>\n<th>Input Shapes</th>\n<th>Output Shape</th>\n<th>Common Errors</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Initial Projection</td>\n<td><code>[B, S, d_model]</code> → QKV</td>\n<td><code>[B, S, d_model]</code> each</td>\n<td>Wrong projection dimensions</td>\n</tr>\n<tr>\n<td>Head Reshaping</td>\n<td><code>[B, S, d_model]</code></td>\n<td><code>[B, H, S, d_k]</code></td>\n<td>Incorrect reshape calculation</td>\n</tr>\n<tr>\n<td>Score Computation</td>\n<td>Q:<code>[B, H, S, d_k]</code>, K:<code>[B, H, S, d_k]</code></td>\n<td><code>[B, H, S, S]</code></td>\n<td>Transpose applied incorrectly</td>\n</tr>\n<tr>\n<td>Score Masking</td>\n<td>Scores:<code>[B, H, S, S]</code>, Mask:<code>[S, S]</code></td>\n<td><code>[B, H, S, S]</code></td>\n<td>Broadcasting dimension mismatch</td>\n</tr>\n<tr>\n<td>Value Weighting</td>\n<td>Weights:<code>[B, H, S, S]</code>, V:<code>[B, H, S, d_v]</code></td>\n<td><code>[B, H, S, d_v]</code></td>\n<td>Value dimension inconsistency</td>\n</tr>\n<tr>\n<td>Head Concatenation</td>\n<td><code>[B, H, S, d_v]</code></td>\n<td><code>[B, S, H*d_v]</code></td>\n<td>Concatenation axis wrong</td>\n</tr>\n<tr>\n<td>Output Projection</td>\n<td><code>[B, S, H*d_v]</code></td>\n<td><code>[B, S, d_model]</code></td>\n<td>Final projection size mismatch</td>\n</tr>\n</tbody></table>\n<p><strong>Attention Score Stability Issues</strong></p>\n<p>Numerical instability in attention computation manifests as exploding or vanishing gradients, NaN values in attention weights, or attention collapse where all weights concentrate on a single position. The scaled dot-product attention is particularly sensitive to the magnitude of query and key vectors, requiring proper scaling by the square root of the key dimension.</p>\n<p>Temperature control becomes critical when attention scores grow too large. Without proper scaling, the softmax function can produce extremely peaked distributions that create vanishing gradients for non-selected positions. Conversely, insufficient scaling can lead to overly uniform attention that fails to focus on relevant information.</p>\n<table>\n<thead>\n<tr>\n<th>Stability Issue</th>\n<th>Symptoms</th>\n<th>Root Cause</th>\n<th>Diagnostic Check</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Attention Collapse</td>\n<td>All weights focus on one position</td>\n<td>Unscaled dot products too large</td>\n<td>Check score magnitudes before softmax</td>\n<td>Apply temperature scaling</td>\n</tr>\n<tr>\n<td>Uniform Attention</td>\n<td>Weights spread equally across positions</td>\n<td>Scores too small relative to softmax</td>\n<td>Examine score variance</td>\n<td>Reduce temperature or check scaling</td>\n</tr>\n<tr>\n<td>NaN in Weights</td>\n<td>Attention matrix contains NaN values</td>\n<td>Softmax overflow from large scores</td>\n<td>Log maximum score values</td>\n<td>Implement LogSumExp trick</td>\n</tr>\n<tr>\n<td>Gradient Explosion</td>\n<td>Loss spikes during training</td>\n<td>Attention gradients too large</td>\n<td>Monitor attention gradient norms</td>\n<td>Apply gradient clipping</td>\n</tr>\n<tr>\n<td>Vanishing Focus</td>\n<td>Model ignores relevant context</td>\n<td>Poor initialization or learning rate</td>\n<td>Visualize attention evolution</td>\n<td>Adjust learning rate schedule</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Applying Causal Mask After Softmax</strong></p>\n<p>A common mistake is applying the causal mask after the softmax operation instead of before. This produces attention weights that don&#39;t sum to 1.0 and breaks the fundamental assumption of attention as a probability distribution. The mask should set future positions to negative infinity before softmax, allowing the exponential function to naturally zero out these positions. Applying the mask after softmax by simply setting values to zero destroys the normalization property and can cause numerical instabilities in gradient computation.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Multi-Head Dimension Calculations</strong></p>\n<p>Many implementations incorrectly calculate the per-head dimension by dividing <code>d_model</code> by <code>num_heads</code> without ensuring the division is exact. When <code>d_model</code> is not evenly divisible by <code>num_heads</code>, this creates dimension mismatches during concatenation. The head dimension <code>d_k</code> should be explicitly defined and verified that <code>num_heads * d_k == d_model</code> before beginning attention computation.</p>\n<h3 id=\"training-issues\">Training Issues</h3>\n<p>Training transformer models involves complex interactions between gradient computation, parameter updates, and numerical stability. Training issues often manifest gradually, making them challenging to diagnose without systematic monitoring of training dynamics.</p>\n<p><strong>Loss Convergence Problems</strong></p>\n<p>Transformer training loss should follow a predictable pattern: initial rapid decrease as the model learns basic token distributions, followed by slower convergence as it learns longer-range dependencies. Deviations from this pattern indicate specific implementation problems or hyperparameter issues.</p>\n<p>Non-decreasing loss often results from learning rate problems, gradient computation errors, or data preprocessing issues. The language modeling objective should show consistent improvement when the model architecture and training loop are implemented correctly. Oscillating loss indicates instability in the optimization process, while sudden loss spikes suggest gradient explosion or numerical overflow.</p>\n<table>\n<thead>\n<tr>\n<th>Loss Pattern</th>\n<th>Probable Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Flat/Non-decreasing</td>\n<td>Learning rate too low or data issues</td>\n<td>Check gradient magnitudes, verify data loading</td>\n<td>Increase learning rate, inspect training data</td>\n</tr>\n<tr>\n<td>Oscillating Wildly</td>\n<td>Learning rate too high or gradient instability</td>\n<td>Monitor gradient norms, check loss smoothing</td>\n<td>Reduce learning rate, add gradient clipping</td>\n</tr>\n<tr>\n<td>Sudden Spikes</td>\n<td>Gradient explosion or numerical overflow</td>\n<td>Log gradient norms, check for NaN/inf values</td>\n<td>Implement gradient clipping, reduce learning rate</td>\n</tr>\n<tr>\n<td>Rapid Initial Drop Then Plateau</td>\n<td>Model memorizing instead of generalizing</td>\n<td>Compare train/validation loss, check overfitting</td>\n<td>Add dropout, reduce model size, increase data</td>\n</tr>\n<tr>\n<td>Negative Loss Values</td>\n<td>Incorrect loss computation</td>\n<td>Verify cross-entropy implementation</td>\n<td>Fix loss calculation, check label preprocessing</td>\n</tr>\n</tbody></table>\n<p><strong>Gradient Flow Analysis</strong></p>\n<p>Healthy gradient flow is essential for transformer training success. Gradients should decrease in magnitude from the output layer toward the input layer, but not vanish completely in early layers. The residual connections and layer normalization are specifically designed to maintain gradient flow through the deep transformer stack.</p>\n<p>Vanishing gradients manifest as extremely small gradient magnitudes in early transformer layers, preventing these layers from learning meaningful representations. Exploding gradients appear as exponentially increasing gradient norms that cause parameter updates to overshoot optimal values. Both conditions can be diagnosed by monitoring gradient statistics throughout the network.</p>\n<table>\n<thead>\n<tr>\n<th>Layer Position</th>\n<th>Expected Gradient Magnitude</th>\n<th>Vanishing Signs</th>\n<th>Exploding Signs</th>\n<th>Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Output Layer</td>\n<td>Largest gradients (1e-2 to 1e-1)</td>\n<td>Gradients below 1e-6</td>\n<td>Gradients above 1e2</td>\n<td>Adjust loss scaling</td>\n</tr>\n<tr>\n<td>Middle Layers</td>\n<td>Moderate gradients (1e-3 to 1e-2)</td>\n<td>Exponential decrease toward input</td>\n<td>Exponential increase from output</td>\n<td>Check residual connections</td>\n</tr>\n<tr>\n<td>Input Layers</td>\n<td>Smallest but non-zero (1e-4 to 1e-3)</td>\n<td>Near-zero gradients</td>\n<td>Unstable oscillations</td>\n<td>Verify layer norm placement</td>\n</tr>\n<tr>\n<td>Embedding Layer</td>\n<td>Small gradients (1e-5 to 1e-4)</td>\n<td>Zero gradients</td>\n<td>Large embedding updates</td>\n<td>Consider embedding freezing</td>\n</tr>\n</tbody></table>\n<p><strong>Learning Rate Schedule Problems</strong></p>\n<p>Transformer training benefits from learning rate warmup followed by decay, but incorrect scheduling can prevent convergence or cause instability. The warmup phase allows the model to stabilize before applying full learning rates, while the decay phase enables fine-tuning of learned representations.</p>\n<p>Insufficient warmup can cause early training instability, particularly in attention mechanisms that are sensitive to initialization. Excessive warmup delays learning and wastes computational resources. Similarly, aggressive decay can prematurely stop learning, while insufficient decay can prevent final convergence.</p>\n<blockquote>\n<p><strong>Decision: Pre-Layer Normalization with Warmup Scheduling</strong></p>\n<ul>\n<li><strong>Context</strong>: Transformers require careful coordination between normalization placement and learning rate scheduling to achieve stable training</li>\n<li><strong>Options Considered</strong>: Post-norm with standard scheduling, pre-norm with warmup, mixed normalization strategies</li>\n<li><strong>Decision</strong>: Pre-layer normalization with linear warmup over 2000 steps followed by cosine decay</li>\n<li><strong>Rationale</strong>: Pre-norm provides better gradient flow and stability, while warmup prevents early training instability from random initialization</li>\n<li><strong>Consequences</strong>: Enables stable training of deeper models but requires tuning warmup steps for different model sizes</li>\n</ul>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Incorrect Label Shifting</strong></p>\n<p>Language modeling requires shifting the target sequence by one position relative to the input sequence, so the model predicts the next token at each position. A common error is failing to implement this shift correctly, either by not shifting at all or shifting in the wrong direction. The input sequence should be <code>[token_0, token_1, ..., token_n-1]</code> while the target sequence should be <code>[token_1, token_2, ..., token_n]</code>, ensuring each input position predicts the subsequent token.</p>\n<p>⚠️ <strong>Pitfall: Missing Gradient Accumulation in Large Batch Training</strong></p>\n<p>When memory constraints prevent using large batch sizes directly, gradient accumulation simulates larger batches by accumulating gradients over multiple forward passes before updating parameters. A common mistake is forgetting to scale the accumulated gradients by the number of accumulation steps, leading to effectively larger learning rates than intended. The accumulated gradients should be divided by the accumulation factor before the optimizer step.</p>\n<h3 id=\"generation-quality-issues\">Generation Quality Issues</h3>\n<p>Text generation problems often stem from improper sampling strategies, KV cache implementation errors, or insufficient training. Unlike training bugs that produce clear error messages, generation issues manifest as poor text quality that requires qualitative assessment.</p>\n<p><strong>Repetitive Text Patterns</strong></p>\n<p>Repetitive generation is one of the most common quality issues in transformer text generation. The model may repeat individual tokens, phrases, or entire sequences. This behavior often results from overconfident predictions, poor sampling strategies, or inadequate training diversity.</p>\n<p>Token-level repetition occurs when the model assigns extremely high probability to recently generated tokens, creating loops where the same token is selected repeatedly. Phrase-level repetition happens when the model learns strong sequential patterns during training but lacks diversity mechanisms during generation. Sequence-level repetition indicates that the model has memorized training examples and reproduces them during generation.</p>\n<table>\n<thead>\n<tr>\n<th>Repetition Type</th>\n<th>Pattern Example</th>\n<th>Probable Cause</th>\n<th>Sampling Fix</th>\n<th>Training Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Repetition</td>\n<td>&quot;the the the the...&quot;</td>\n<td>Overconfident predictions</td>\n<td>Apply repetition penalty</td>\n<td>Increase training diversity</td>\n</tr>\n<tr>\n<td>Phrase Repetition</td>\n<td>&quot;in the world in the world...&quot;</td>\n<td>Poor sampling diversity</td>\n<td>Use top-p sampling</td>\n<td>Add dropout during training</td>\n</tr>\n<tr>\n<td>Sequence Repetition</td>\n<td>Reproducing training examples verbatim</td>\n<td>Model memorization</td>\n<td>Increase temperature</td>\n<td>Larger training dataset</td>\n</tr>\n<tr>\n<td>Pattern Loops</td>\n<td>Cycling through fixed patterns</td>\n<td>Insufficient context modeling</td>\n<td>Implement cycle detection</td>\n<td>Longer sequence training</td>\n</tr>\n<tr>\n<td>Semantic Repetition</td>\n<td>Rephrasing same idea repeatedly</td>\n<td>Lack of semantic diversity</td>\n<td>Semantic similarity penalty</td>\n<td>Diverse training objectives</td>\n</tr>\n</tbody></table>\n<p><strong>Incoherent Output Generation</strong></p>\n<p>Incoherent text manifests as grammatically incorrect sentences, logical inconsistencies, or failure to maintain context over longer sequences. These issues often indicate problems with attention mechanism training, insufficient model capacity, or inappropriate sampling parameters.</p>\n<p>Grammatical incoherence suggests the model hasn&#39;t learned proper language structure, often due to insufficient training or poor tokenization. Logical incoherence indicates failure to maintain semantic consistency, typically resulting from attention mechanisms that don&#39;t properly model long-range dependencies. Contextual incoherence shows that the model loses track of conversation context or narrative flow.</p>\n<table>\n<thead>\n<tr>\n<th>Coherence Issue</th>\n<th>Symptoms</th>\n<th>Attention Problem</th>\n<th>Context Problem</th>\n<th>Sampling Problem</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Grammar Errors</td>\n<td>Incorrect verb tense, word order</td>\n<td>Poor local attention focus</td>\n<td>N/A</td>\n<td>Temperature too high</td>\n</tr>\n<tr>\n<td>Logic Breaks</td>\n<td>Contradictory statements</td>\n<td>Failure to attend to relevant context</td>\n<td>Context window too short</td>\n<td>Insufficient top-k filtering</td>\n</tr>\n<tr>\n<td>Context Loss</td>\n<td>Forgetting previous information</td>\n<td>Attention weights too uniform</td>\n<td>Exceeding trained sequence length</td>\n<td>Greedy decoding too rigid</td>\n</tr>\n<tr>\n<td>Topic Drift</td>\n<td>Gradual shift away from subject</td>\n<td>No attention on topic-relevant tokens</td>\n<td>Insufficient topic reinforcement</td>\n<td>Need topic-aware sampling</td>\n</tr>\n<tr>\n<td>Factual Errors</td>\n<td>Incorrect information generation</td>\n<td>Training data quality issues</td>\n<td>Limited factual training</td>\n<td>Confidence-based filtering needed</td>\n</tr>\n</tbody></table>\n<p><strong>Sampling Strategy Optimization</strong></p>\n<p>Different sampling strategies produce distinct quality characteristics and failure modes. Greedy decoding provides deterministic, locally optimal choices but can lead to repetitive or overly conservative text. Temperature sampling introduces randomness but may produce incoherent results with poor temperature tuning. Top-k and top-p sampling balance quality and diversity but require careful parameter selection.</p>\n<p>Understanding the interaction between sampling parameters is crucial for achieving desired generation characteristics. Temperature affects the sharpness of the probability distribution, with lower values producing more focused selections and higher values increasing randomness. Top-k filtering restricts candidates to the k most likely tokens, while top-p (nucleus sampling) dynamically adjusts the candidate set based on cumulative probability.</p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Best Use Case</th>\n<th>Quality Characteristics</th>\n<th>Failure Mode</th>\n<th>Parameter Tuning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Greedy</td>\n<td>Factual text, code generation</td>\n<td>Deterministic, locally optimal</td>\n<td>Repetitive, conservative</td>\n<td>N/A - no parameters</td>\n</tr>\n<tr>\n<td>Temperature</td>\n<td>Creative writing, dialogue</td>\n<td>Controllable randomness</td>\n<td>Incoherent at high temps</td>\n<td>0.7-0.9 for creativity, 0.1-0.3 for focus</td>\n</tr>\n<tr>\n<td>Top-k</td>\n<td>Balanced quality/diversity</td>\n<td>Consistent candidate pool</td>\n<td>Fixed pool may be too large/small</td>\n<td>k=40-50 typical, adjust based on vocab</td>\n</tr>\n<tr>\n<td>Top-p</td>\n<td>Adaptive diversity control</td>\n<td>Dynamic candidate selection</td>\n<td>Complex parameter interaction</td>\n<td>p=0.9-0.95 typical, combine with temperature</td>\n</tr>\n<tr>\n<td>Combined</td>\n<td>Production systems</td>\n<td>Best quality-diversity trade-off</td>\n<td>Complex tuning required</td>\n<td>Start with temp=0.8, top-p=0.9, top-k=50</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: KV Cache Dimension Mismatches</strong></p>\n<p>KV caching optimization stores computed key and value tensors from previous generation steps to avoid redundant computation. A common error is incorrectly managing cache dimensions when concatenating new keys and values with cached tensors. The cache must maintain consistent batch, head, and feature dimensions while correctly extending the sequence dimension. Dimension mismatches in the cache can cause subtle bugs that only appear during longer generation sequences.</p>\n<p>⚠️ <strong>Pitfall: Temperature Zero Division</strong></p>\n<p>Setting temperature to exactly 0.0 for deterministic generation causes division by zero in the temperature scaling operation. While the intent is to make sampling deterministic, the correct approach is either using greedy decoding directly or setting temperature to a very small positive value (e.g., 1e-8). The division by zero error can crash generation or produce NaN values that propagate through subsequent computations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Debugging transformers effectively requires systematic approaches to identify, isolate, and resolve issues across the attention mechanism, training process, and text generation pipeline. The following guidance provides concrete tools and techniques for diagnosing common problems.</p>\n<p><strong>Technology Recommendations for Debugging</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Attention Visualization</td>\n<td><code>matplotlib</code> with heatmaps</td>\n<td><code>tensorboard</code> with custom scalars</td>\n</tr>\n<tr>\n<td>Gradient Monitoring</td>\n<td>Manual logging with <code>torch.nn.utils.clip_grad_norm_</code></td>\n<td><code>wandb</code> with automatic gradient tracking</td>\n</tr>\n<tr>\n<td>Loss Tracking</td>\n<td>Simple CSV logging</td>\n<td><code>tensorboard</code> or <code>wandb</code> with real-time plotting</td>\n</tr>\n<tr>\n<td>Memory Profiling</td>\n<td><code>torch.cuda.memory_summary()</code></td>\n<td><code>torch.profiler</code> with detailed analysis</td>\n</tr>\n<tr>\n<td>Model Inspection</td>\n<td>Custom print statements</td>\n<td><code>torchinfo</code> for detailed model summaries</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Debugging Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  debug/\n    attention_visualizer.py      ← attention weight plotting and analysis\n    gradient_monitor.py          ← gradient flow tracking and clipping\n    loss_analyzer.py            ← training loss pattern analysis\n    generation_tester.py        ← text generation quality assessment\n    numerical_stability.py      ← overflow/underflow detection\n  tests/\n    test_attention_shapes.py    ← dimension consistency tests\n    test_training_flow.py       ← end-to-end training validation\n    test_generation_quality.py  ← automated quality checks\n  utils/\n    debug_helpers.py           ← common debugging utilities</code></pre></div>\n\n<p><strong>Attention Debugging Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> matplotlib.pyplot </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> plt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Tuple, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AttentionDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive attention mechanism debugging utilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model_config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model_config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.attention_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> plot_attention_weights</span><span style=\"color:#E1E4E8\">(self, weights: torch.Tensor, tokens: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             layer: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, head: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, save_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Visualize attention weights as a heatmap for debugging attention patterns.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            weights: Attention weights tensor [seq_len, seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            tokens: List of token strings for axis labels</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            layer: Layer index for title</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            head: Head index for title</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            save_path: Optional path to save the plot</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert weights tensor to numpy and ensure 2D shape</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create matplotlib figure with appropriate size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate heatmap with token labels on both axes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add colorbar and title indicating layer/head</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Save plot if path provided, otherwise display</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_attention_shapes</span><span style=\"color:#E1E4E8\">(self, q: torch.Tensor, k: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                v: torch.Tensor, mask: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Comprehensive shape validation for attention inputs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary mapping validation check names to pass/fail status</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check Q, K, V have matching batch and sequence dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify head dimension consistency across Q, K, V</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate mask shape matches attention score dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure all tensors are on same device</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return detailed validation results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> diagnose_attention_patterns</span><span style=\"color:#E1E4E8\">(self, weights: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze attention weight patterns for common issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary of diagnostic metrics and their values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if attention is properly normalized (rows sum to 1)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Measure attention entropy to detect uniform vs focused patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate causal mask effectiveness (upper triangle should be zero)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute attention concentration metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive diagnostics dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Training Debugging Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrainingDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitor training dynamics and diagnose convergence issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TrainingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.loss_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.gradient_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_gradient_flow</span><span style=\"color:#E1E4E8\">(self, model: torch.nn.Module, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          loss: torch.Tensor) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze gradient magnitudes throughout the model after backward pass.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary mapping layer names to gradient statistics</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Ensure loss.backward() has been called</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Iterate through named parameters with gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute gradient norms for each layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify layers with vanishing or exploding gradients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive gradient analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> monitor_training_stability</span><span style=\"color:#E1E4E8\">(self, current_loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 gradient_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Detect training instability patterns and suggest fixes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary of stability issues and recommended actions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for loss spikes indicating gradient explosion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Detect loss plateaus suggesting learning rate issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Monitor gradient norm trends for stability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compare current metrics to historical patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate specific recommendations for detected issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_loss_computation</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                targets: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                computed_loss: torch.Tensor) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Verify cross-entropy loss computation correctness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if loss computation is correct, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute reference loss using torch.nn.functional.cross_entropy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for proper label shifting in language modeling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify padding tokens are properly masked in loss</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compare computed loss to reference within tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return validation result with detailed error info</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Generation Quality Assessment</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> GenerationDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Assess and debug text generation quality issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokenizer: SimpleTokenizer):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.generation_samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_repetition_patterns</span><span style=\"color:#E1E4E8\">(self, generated_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Quantify different types of repetition in generated text.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary with repetition metrics and severity scores</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize generated text for analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Detect immediate token repetitions (n-gram analysis)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify phrase-level repetitions using sliding windows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Measure semantic repetition using embedding similarity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive repetition analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> assess_coherence_quality</span><span style=\"color:#E1E4E8\">(self, prompt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               generated_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Evaluate multiple dimensions of text coherence.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary of coherence scores across different dimensions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Analyze grammatical correctness using basic heuristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check logical consistency between sentences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Measure context preservation relative to prompt</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Evaluate topic drift over generation length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return multi-dimensional coherence assessment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> optimize_sampling_parameters</span><span style=\"color:#E1E4E8\">(self, prompt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   num_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Test different sampling parameter combinations to find optimal settings.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary mapping parameter combinations to quality scores</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Define grid of temperature, top_k, and top_p values to test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate multiple samples with each parameter combination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Score each sample using coherence and diversity metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify parameter combinations with best quality/diversity balance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return ranked parameter recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints for Debugging</strong></p>\n<p>After implementing each debugging component, verify functionality with these specific tests:</p>\n<p><strong>Attention Debugging Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> debug.attention_visualizer</span><span style=\"color:#79B8FF\"> --model</span><span style=\"color:#9ECBFF\"> checkpoint.pt</span><span style=\"color:#79B8FF\"> --text</span><span style=\"color:#9ECBFF\"> \"The quick brown fox\"</span><span style=\"color:#79B8FF\"> --layer</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#79B8FF\"> --head</span><span style=\"color:#79B8FF\"> 0</span></span></code></pre></div>\n<p>Expected: Attention heatmap showing causal mask pattern with meaningful attention weights. Lower triangular matrix with focused attention on relevant tokens.</p>\n<p><strong>Training Debugging Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> debug.gradient_monitor</span><span style=\"color:#79B8FF\"> --config</span><span style=\"color:#9ECBFF\"> config.yaml</span><span style=\"color:#79B8FF\"> --steps</span><span style=\"color:#79B8FF\"> 100</span></span></code></pre></div>\n<p>Expected: Gradient flow analysis showing decreasing but non-zero gradients from output to input layers. No warnings about vanishing or exploding gradients.</p>\n<p><strong>Generation Debugging Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> debug.generation_tester</span><span style=\"color:#79B8FF\"> --model</span><span style=\"color:#9ECBFF\"> checkpoint.pt</span><span style=\"color:#79B8FF\"> --prompt</span><span style=\"color:#9ECBFF\"> \"Once upon a time\"</span><span style=\"color:#79B8FF\"> --samples</span><span style=\"color:#79B8FF\"> 5</span></span></code></pre></div>\n<p>Expected: Quality assessment showing low repetition scores, high coherence metrics, and diverse outputs across sampling strategies.</p>\n<p><strong>Debugging Tips by Symptom</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Attention weights all zeros</td>\n<td>Mask applied incorrectly</td>\n<td>Check mask values and application order</td>\n<td>Apply mask before softmax with -inf</td>\n</tr>\n<tr>\n<td>Loss becomes NaN</td>\n<td>Numerical overflow</td>\n<td>Log intermediate values in forward pass</td>\n<td>Add gradient clipping and stability checks</td>\n</tr>\n<tr>\n<td>Generated text is gibberish</td>\n<td>Insufficient training or wrong sampling</td>\n<td>Check training loss convergence</td>\n<td>Train longer or adjust sampling temperature</td>\n</tr>\n<tr>\n<td>Model predicts same token repeatedly</td>\n<td>Overconfident predictions</td>\n<td>Examine output probability distribution</td>\n<td>Apply repetition penalty or increase temperature</td>\n</tr>\n<tr>\n<td>Training loss oscillates wildly</td>\n<td>Learning rate too high</td>\n<td>Monitor gradient norms over time</td>\n<td>Reduce learning rate and add warmup</td>\n</tr>\n<tr>\n<td>Attention visualization shows uniform weights</td>\n<td>Poor attention learning</td>\n<td>Check attention score magnitudes</td>\n<td>Verify scaling factor and initialization</td>\n</tr>\n<tr>\n<td>Generation stops abruptly</td>\n<td>EOS token prediction issues</td>\n<td>Check tokenizer EOS handling</td>\n<td>Fix EOS token processing in generation loop</td>\n</tr>\n<tr>\n<td>Memory usage grows during generation</td>\n<td>KV cache not managed properly</td>\n<td>Profile memory usage during generation</td>\n<td>Implement proper cache clearing and limits</td>\n</tr>\n</tbody></table>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - building upon completed self-attention (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4) with advanced enhancements</p>\n</blockquote>\n<p>After successfully implementing the core transformer architecture through all four milestones, numerous opportunities exist to enhance both the model&#39;s capabilities and training efficiency. These extensions represent the natural evolution from a working prototype to a production-ready system, incorporating techniques that have emerged from extensive research and practical deployment experience in the transformer ecosystem.</p>\n<p>The extension landscape divides into two primary categories: <strong>architectural improvements</strong> that enhance the model&#39;s representational power and computational efficiency, and <strong>training enhancements</strong> that accelerate convergence, improve stability, and enable scaling to larger datasets and model sizes. Understanding these extensions provides insight into how modern transformer implementations achieve their impressive performance while maintaining computational tractability.</p>\n<h3 id=\"mental-model-the-performance-optimization-pyramid\">Mental Model: The Performance Optimization Pyramid</h3>\n<p>Think of transformer extensions as optimizing a high-performance race car. The base implementation we&#39;ve built through the four milestones represents a functional vehicle that can complete the race. Architectural improvements are like upgrading the engine, aerodynamics, and suspension - they fundamentally enhance the car&#39;s capabilities and efficiency. Training enhancements are like optimizing the pit crew, fuel strategy, and race tactics - they maximize performance within the existing hardware constraints while enabling longer races with larger teams.</p>\n<p>Just as race car improvements must be carefully balanced (a more powerful engine requires better brakes and cooling), transformer extensions introduce trade-offs between computational cost, implementation complexity, and performance gains. Each enhancement addresses specific bottlenecks or limitations discovered in the base implementation.</p>\n<h3 id=\"architectural-improvements\">Architectural Improvements</h3>\n<p>The architectural dimension focuses on enhancing the transformer&#39;s core computational mechanisms. These improvements typically modify how the model processes information rather than how it learns, though the distinction sometimes blurs. The key areas include positional encoding schemes that better capture sequential relationships, attention variants that improve efficiency or expressiveness, and normalization techniques that enhance training stability and convergence.</p>\n<h4 id=\"advanced-positional-encodings\">Advanced Positional Encodings</h4>\n<p>Our base implementation likely uses simple learned positional embeddings added to token embeddings. While effective for sequences within the training distribution, this approach has significant limitations for longer sequences or positions not seen during training. Advanced positional encoding schemes address these extrapolation challenges while providing richer positional information.</p>\n<blockquote>\n<p><strong>Decision: Rotary Position Embedding (RoPE)</strong></p>\n<ul>\n<li><strong>Context</strong>: Learned positional embeddings don&#39;t generalize well beyond training sequence lengths and provide weak inductive biases about relative positions</li>\n<li><strong>Options Considered</strong>: Sinusoidal encodings, ALiBi (Attention with Linear Biases), RoPE</li>\n<li><strong>Decision</strong>: Implement Rotary Position Embedding as the primary upgrade</li>\n<li><strong>Rationale</strong>: RoPE provides excellent length extrapolation, encodes relative positions naturally, and integrates cleanly into existing attention computation without parameter overhead</li>\n<li><strong>Consequences</strong>: Requires modifying attention computation to apply rotation matrices, but enables training on shorter sequences and inference on longer ones</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Positional Encoding Method</th>\n<th>Length Generalization</th>\n<th>Relative Position Awareness</th>\n<th>Parameter Overhead</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Learned Embeddings</td>\n<td>Poor</td>\n<td>No</td>\n<td>High (vocab_size × d_model)</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Sinusoidal</td>\n<td>Good</td>\n<td>Weak</td>\n<td>None</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>ALiBi</td>\n<td>Excellent</td>\n<td>Strong</td>\n<td>None</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>RoPE</td>\n<td>Excellent</td>\n<td>Strong</td>\n<td>None</td>\n<td>Medium-High</td>\n</tr>\n</tbody></table>\n<p><strong>Rotary Position Embedding Implementation Strategy</strong></p>\n<p>RoPE modifies the attention computation by rotating query and key vectors based on their absolute positions in a way that naturally encodes relative position information. The rotation angles follow a geometric progression across embedding dimensions, creating a rich positional representation.</p>\n<p>The core insight is that rotating queries and keys by position-dependent angles causes their dot product to depend on the relative distance between positions. For positions i and j, the attention score becomes a function of (i-j), providing the relative position awareness that simple additive embeddings lack.</p>\n<p>Implementation requires precomputing rotation matrices for each position and dimension pair, then applying these rotations within the attention mechanism. The rotation operation replaces the simple addition of positional embeddings to input tokens, moving position encoding directly into the attention computation.</p>\n<table>\n<thead>\n<tr>\n<th>RoPE Component</th>\n<th>Purpose</th>\n<th>Computational Cost</th>\n<th>Memory Overhead</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Angle Computation</td>\n<td>Generate rotation frequencies</td>\n<td>O(d_model)</td>\n<td>O(d_model)</td>\n</tr>\n<tr>\n<td>Rotation Matrix Application</td>\n<td>Apply position rotations</td>\n<td>O(seq_length × d_model)</td>\n<td>O(seq_length × d_model)</td>\n</tr>\n<tr>\n<td>Query/Key Transformation</td>\n<td>Rotate Q,K vectors</td>\n<td>O(seq_length × num_heads × d_k)</td>\n<td>None</td>\n</tr>\n</tbody></table>\n<p><strong>ALiBi as Lightweight Alternative</strong></p>\n<p>ALiBi (Attention with Linear Biases) offers a simpler alternative that adds position-dependent biases directly to attention scores. Rather than modifying input embeddings or attention computation, ALiBi subtracts a linear penalty based on distance from attention scores before the softmax operation.</p>\n<p>The penalty takes the form <code>-m × |i - j|</code> where m is a head-specific slope value and |i - j| is the distance between query and key positions. Different attention heads use different slope values, allowing them to specialize in different distance ranges - some heads focus on local relationships while others capture longer-range dependencies.</p>\n<p>ALiBi&#39;s primary advantage is implementation simplicity and computational efficiency. It requires no additional parameters or complex mathematical operations, just a position-dependent bias matrix applied during attention computation. However, it provides less rich positional information than RoPE and may not extrapolate as effectively to very long sequences.</p>\n<h4 id=\"attention-mechanism-variants\">Attention Mechanism Variants</h4>\n<p>The scaled dot-product attention implemented in Milestone 1 provides the foundation for more sophisticated attention mechanisms. These variants address specific limitations: computational complexity scaling, attention pattern diversity, and information bottlenecks in the standard attention formulation.</p>\n<blockquote>\n<p><strong>Decision: Implementing Flash Attention for Memory Efficiency</strong></p>\n<ul>\n<li><strong>Context</strong>: Standard attention computation requires materializing the full attention matrix (seq_length × seq_length), creating O(n²) memory complexity that limits maximum sequence length</li>\n<li><strong>Options Considered</strong>: Flash Attention, Linear Attention, Sparse Attention patterns</li>\n<li><strong>Decision</strong>: Implement Flash Attention as the primary memory optimization</li>\n<li><strong>Rationale</strong>: Flash Attention provides identical outputs to standard attention while reducing memory usage through tile-based computation and kernel fusion</li>\n<li><strong>Consequences</strong>: Requires careful implementation of memory-efficient attention kernels but enables processing much longer sequences</li>\n</ul>\n</blockquote>\n<p><strong>Flash Attention Architecture</strong></p>\n<p>Flash Attention reorganizes the attention computation to minimize memory transfers between GPU high-bandwidth memory (HBM) and on-chip SRAM. Rather than computing the full attention matrix and then applying it to values, Flash Attention processes attention in tiles that fit within SRAM capacity.</p>\n<p>The algorithm maintains running statistics (maximum values and normalization factors) to correctly compute softmax across tiles without materializing the full attention matrix. Each tile computation loads query, key, and value blocks into SRAM, computes attention for that block, updates the running statistics, and accumulates results directly into the output buffer.</p>\n<p>This approach reduces memory complexity from O(seq_length²) to O(seq_length), enabling much longer sequence processing. However, it requires careful implementation of the tiling algorithm and efficient kernel code to achieve the promised speedups.</p>\n<table>\n<thead>\n<tr>\n<th>Flash Attention Component</th>\n<th>Standard Attention</th>\n<th>Flash Attention</th>\n<th>Memory Reduction</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Attention Matrix Storage</td>\n<td>O(n²)</td>\n<td>O(tile_size²)</td>\n<td>~100-1000x</td>\n</tr>\n<tr>\n<td>Intermediate Activations</td>\n<td>O(n × d_model)</td>\n<td>O(tile_size × d_model)</td>\n<td>~10-50x</td>\n</tr>\n<tr>\n<td>Output Computation</td>\n<td>Single pass</td>\n<td>Tiled accumulation</td>\n<td>Same final result</td>\n</tr>\n</tbody></table>\n<p><strong>Multi-Query and Grouped-Query Attention</strong></p>\n<p>Multi-Query Attention (MQA) and its generalization Grouped-Query Attention (GQA) reduce the memory and computational overhead of the key-value cache during autoregressive generation. While standard multi-head attention maintains separate key and value projections for each head, MQA shares a single key-value pair across all attention heads.</p>\n<p>This modification significantly reduces the KV cache size during inference, which becomes the primary memory bottleneck for large models serving long sequences. Each attention head retains its own query projection, preserving the model&#39;s ability to attend to different aspects of the input, but shares the same key-value representations.</p>\n<p>GQA provides a middle ground by grouping attention heads and sharing key-value projections within each group. For example, with 8 attention heads and 2 groups, heads 1-4 share one set of key-value projections while heads 5-8 share another. This reduces cache size while maintaining more representational diversity than pure MQA.</p>\n<table>\n<thead>\n<tr>\n<th>Attention Variant</th>\n<th>Query Heads</th>\n<th>Key-Value Heads</th>\n<th>Cache Size Ratio</th>\n<th>Quality Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-Head (MHA)</td>\n<td>num_heads</td>\n<td>num_heads</td>\n<td>1.0x</td>\n<td>Baseline</td>\n</tr>\n<tr>\n<td>Grouped-Query (GQA)</td>\n<td>num_heads</td>\n<td>num_heads/group_size</td>\n<td>1/group_size</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>Multi-Query (MQA)</td>\n<td>num_heads</td>\n<td>1</td>\n<td>1/num_heads</td>\n<td>Small</td>\n</tr>\n</tbody></table>\n<h4 id=\"normalization-strategy-evolution\">Normalization Strategy Evolution</h4>\n<p>Layer normalization placement and variants significantly impact training dynamics and final model performance. Our Milestone 2 implementation likely uses pre-normalization (applying LayerNorm before each sublayer), but several alternatives offer compelling advantages for specific scenarios.</p>\n<blockquote>\n<p><strong>Decision: RMSNorm for Computational Efficiency</strong></p>\n<ul>\n<li><strong>Context</strong>: Standard LayerNorm computes both mean and variance, requiring two passes through the data and additional computational overhead</li>\n<li><strong>Options Considered</strong>: LayerNorm, RMSNorm, GroupNorm variants</li>\n<li><strong>Decision</strong>: Implement RMSNorm as an optional replacement for LayerNorm</li>\n<li><strong>Rationale</strong>: RMSNorm provides similar normalization benefits with ~15% computational savings by omitting mean centering, and recent large models show it performs equally well</li>\n<li><strong>Consequences</strong>: Slight implementation change with measurable speed improvement, especially beneficial for large models</li>\n</ul>\n</blockquote>\n<p><strong>RMSNorm Implementation Strategy</strong></p>\n<p>Root Mean Square Normalization (RMSNorm) simplifies the normalization computation by omitting the mean centering step. Instead of normalizing to zero mean and unit variance, RMSNorm only ensures unit RMS (root mean square) magnitude, scaling each vector by the inverse of its RMS value.</p>\n<p>The computational simplification reduces the normalization from two statistical moments (mean and variance) to one (RMS), eliminating one pass through the data. For large models where normalization overhead becomes significant, this reduction provides measurable speedups with minimal quality impact.</p>\n<p>Implementation replaces the standard LayerNorm computation <code>(x - mean) / sqrt(var + eps) * scale + bias</code> with the simpler RMSNorm formula <code>x / sqrt(mean_square + eps) * scale</code>. The bias term is typically omitted entirely, further reducing parameters and computation.</p>\n<table>\n<thead>\n<tr>\n<th>Normalization Method</th>\n<th>Computational Steps</th>\n<th>Memory Overhead</th>\n<th>Parameter Count</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LayerNorm</td>\n<td>Mean, Variance, Normalize</td>\n<td>2x intermediate storage</td>\n<td>2 × d_model</td>\n</tr>\n<tr>\n<td>RMSNorm</td>\n<td>RMS, Normalize</td>\n<td>1x intermediate storage</td>\n<td>1 × d_model</td>\n</tr>\n<tr>\n<td>GroupNorm</td>\n<td>Group statistics</td>\n<td>Group-dependent</td>\n<td>2 × d_model</td>\n</tr>\n</tbody></table>\n<p><strong>Normalization Placement Strategies</strong></p>\n<p>The placement of normalization layers within transformer blocks affects both training stability and final performance. Pre-normalization (applying LayerNorm before each sublayer) generally provides better training stability, while post-normalization (applying LayerNorm after each sublayer) can achieve slightly better final performance with careful tuning.</p>\n<p>Recent research has explored hybrid approaches that apply different normalization strategies to different parts of the network, or adaptive normalization schemes that adjust their behavior based on training progress. These advanced techniques require more sophisticated implementation but can provide benefits for specific model scales or training scenarios.</p>\n<p>Some implementations also experiment with normalization-free architectures that achieve stable training through careful weight initialization and activation scaling, avoiding normalization overhead entirely. However, these approaches require expert tuning and may not generalize across different model sizes.</p>\n<h3 id=\"training-enhancements\">Training Enhancements</h3>\n<p>Training enhancement focuses on improving the optimization process itself: how quickly the model learns, how stably it trains, and how efficiently it utilizes computational resources. These improvements often have larger practical impact than architectural changes, as they directly affect development iteration speed and resource costs.</p>\n<h4 id=\"learning-rate-scheduling-evolution\">Learning Rate Scheduling Evolution</h4>\n<p>Our Milestone 3 implementation likely uses a constant learning rate or simple decay schedule. Advanced learning rate scheduling can dramatically improve convergence speed and final performance by adapting the learning rate based on training progress and model characteristics.</p>\n<blockquote>\n<p><strong>Decision: Cosine Annealing with Warmup</strong></p>\n<ul>\n<li><strong>Context</strong>: Constant learning rates converge slowly, while simple decay schedules may reduce learning rate too aggressively early in training</li>\n<li><strong>Options Considered</strong>: Linear decay, exponential decay, cosine annealing, cyclical schedules</li>\n<li><strong>Decision</strong>: Implement cosine annealing with linear warmup as the standard schedule</li>\n<li><strong>Rationale</strong>: Cosine annealing provides smooth learning rate reduction that maintains exploration capability late in training, while warmup prevents early training instability</li>\n<li><strong>Consequences</strong>: Requires schedule computation and tracking training progress, but typically improves convergence speed by 20-30%</li>\n</ul>\n</blockquote>\n<p><strong>Cosine Annealing Implementation</strong></p>\n<p>Cosine annealing schedules the learning rate to follow a cosine curve from the maximum learning rate down to a minimum value (often zero or a small fraction of the maximum). This provides aggressive learning rate reduction in the middle of training while maintaining higher rates early and late in the schedule.</p>\n<p>The mathematical form is <code>lr = lr_min + (lr_max - lr_min) * (1 + cos(π * step / total_steps)) / 2</code>, creating a smooth curve that starts high, decreases rapidly in the middle, and approaches the minimum value asymptotically. This schedule often outperforms linear or exponential decay by maintaining some learning capacity throughout training.</p>\n<p>Linear warmup precedes the cosine schedule, gradually increasing the learning rate from zero to the maximum value over the first few thousand steps. This prevents the large gradient updates that can destabilize training when starting with a high learning rate and random weights.</p>\n<table>\n<thead>\n<tr>\n<th>Schedule Component</th>\n<th>Purpose</th>\n<th>Duration</th>\n<th>Learning Rate Range</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Linear Warmup</td>\n<td>Stabilize early training</td>\n<td>warmup_steps (typically 2000)</td>\n<td>0 → lr_max</td>\n</tr>\n<tr>\n<td>Cosine Annealing</td>\n<td>Smooth convergence</td>\n<td>total_steps - warmup_steps</td>\n<td>lr_max → lr_min</td>\n</tr>\n<tr>\n<td>Optional Restart</td>\n<td>Escape local minima</td>\n<td>Cyclical</td>\n<td>Reset to lr_max</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive Learning Rate Methods</strong></p>\n<p>Beyond schedule-based approaches, adaptive optimizers like AdamW automatically adjust learning rates based on gradient statistics. AdamW combines the adaptive learning rates of Adam with proper weight decay regularization, addressing the L2 regularization issues in the original Adam optimizer.</p>\n<p>The adaptive mechanism maintains exponential moving averages of both gradient values and squared gradients, using these statistics to compute per-parameter learning rate adjustments. Parameters with consistently large gradients receive smaller effective learning rates, while parameters with small or infrequent gradients receive larger effective rates.</p>\n<p>Implementation requires tracking the momentum and variance estimates for each parameter, increasing memory overhead but often improving convergence robustness. The weight decay term is applied directly to parameters rather than being added to gradients, providing more effective regularization.</p>\n<table>\n<thead>\n<tr>\n<th>Optimizer Component</th>\n<th>Purpose</th>\n<th>Memory Overhead</th>\n<th>Hyperparameters</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradient Momentum</td>\n<td>Smooth optimization path</td>\n<td>1x parameters</td>\n<td>beta1 (typically 0.9)</td>\n</tr>\n<tr>\n<td>Gradient Variance</td>\n<td>Adaptive learning rates</td>\n<td>1x parameters</td>\n<td>beta2 (typically 0.999)</td>\n</tr>\n<tr>\n<td>Weight Decay</td>\n<td>Regularization</td>\n<td>None</td>\n<td>weight_decay (typically 0.01)</td>\n</tr>\n</tbody></table>\n<h4 id=\"mixed-precision-training\">Mixed Precision Training</h4>\n<p>Mixed precision training uses 16-bit floating point for most operations while maintaining 32-bit precision for critical computations. This approach can nearly double training speed on modern GPUs while reducing memory usage, enabling larger models or batch sizes within the same hardware constraints.</p>\n<blockquote>\n<p><strong>Decision: Automatic Mixed Precision (AMP)</strong></p>\n<ul>\n<li><strong>Context</strong>: Training large transformers is computationally expensive and memory-intensive, limiting model size and training speed</li>\n<li><strong>Options Considered</strong>: Full FP32, manual mixed precision, automatic mixed precision, FP16 throughout</li>\n<li><strong>Decision</strong>: Implement Automatic Mixed Precision with gradient scaling</li>\n<li><strong>Rationale</strong>: AMP provides most benefits of mixed precision with minimal code changes and automatic handling of numerical stability issues</li>\n<li><strong>Consequences</strong>: Requires careful gradient scaling and loss computation, but typically provides 1.5-2x training speedup</li>\n</ul>\n</blockquote>\n<p><strong>AMP Implementation Strategy</strong></p>\n<p>Automatic Mixed Precision automatically casts operations to the appropriate precision level: FP16 for matrix multiplications and convolutions where speed matters, FP32 for normalization and loss computation where precision matters. The framework handles these decisions based on operation types and numerical stability requirements.</p>\n<p>Gradient scaling addresses the primary challenge of mixed precision training: gradient underflow. FP16&#39;s limited dynamic range can cause small gradients to round to zero, stalling training. Gradient scaling multiplies the loss by a large factor before backpropagation, keeping gradients in the representable FP16 range, then scales gradients back down before applying updates.</p>\n<p>The scaling factor adapts dynamically based on gradient overflow detection. When gradients overflow (become infinite or NaN), the scaler reduces the scaling factor and skips the parameter update. When training proceeds stably, the scaler gradually increases the scaling factor to maximize gradient precision.</p>\n<table>\n<thead>\n<tr>\n<th>AMP Component</th>\n<th>Precision</th>\n<th>Purpose</th>\n<th>Stability Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Forward Pass</td>\n<td>FP16</td>\n<td>Speed and memory</td>\n<td>Automatic casting</td>\n</tr>\n<tr>\n<td>Loss Computation</td>\n<td>FP32</td>\n<td>Numerical accuracy</td>\n<td>Prevent underflow</td>\n</tr>\n<tr>\n<td>Gradient Scaling</td>\n<td>Dynamic</td>\n<td>Prevent gradient underflow</td>\n<td>Overflow detection</td>\n</tr>\n<tr>\n<td>Parameter Updates</td>\n<td>FP32</td>\n<td>Maintain parameter precision</td>\n<td>Scale correction</td>\n</tr>\n</tbody></table>\n<p><strong>Memory Optimization Techniques</strong></p>\n<p>Beyond mixed precision, several memory optimization techniques enable training larger models or using larger batch sizes. Gradient checkpointing trades computation for memory by recomputing some intermediate activations during backpropagation rather than storing them during the forward pass.</p>\n<p>The technique identifies strategic points in the computation graph where activations are discarded and recomputed when needed for gradient computation. For transformers, checkpointing typically occurs at transformer block boundaries, reducing memory usage roughly proportional to the number of layers while increasing computation time by ~20%.</p>\n<p>ZeRO (Zero Redundancy Optimizer) techniques partition optimizer states, gradients, and even parameters across multiple GPUs, reducing per-device memory requirements. Combined with gradient checkpointing and mixed precision, these techniques enable training models that would otherwise exceed memory limits.</p>\n<table>\n<thead>\n<tr>\n<th>Optimization Technique</th>\n<th>Memory Reduction</th>\n<th>Computation Overhead</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradient Checkpointing</td>\n<td>~num_layers reduction</td>\n<td>~20% increase</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>ZeRO Stage 1 (optimizer states)</td>\n<td>4x with AdamW</td>\n<td>Minimal</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>ZeRO Stage 2 (+ gradients)</td>\n<td>8x with AdamW</td>\n<td>Small communication</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>ZeRO Stage 3 (+ parameters)</td>\n<td>Linear with devices</td>\n<td>Significant communication</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<h4 id=\"distributed-training-strategies\">Distributed Training Strategies</h4>\n<p>As models grow beyond single-GPU capacity, distributed training becomes essential. The distribution strategy depends on model size, available hardware, and performance requirements, with different approaches optimal for different scenarios.</p>\n<blockquote>\n<p><strong>Decision: Data Parallel with Gradient Synchronization</strong></p>\n<ul>\n<li><strong>Context</strong>: Model fits on single GPU but training would benefit from larger batch sizes or faster iteration</li>\n<li><strong>Options Considered</strong>: Data parallel, model parallel, pipeline parallel, hybrid approaches</li>\n<li><strong>Decision</strong>: Implement synchronized data parallel training as the primary scaling method</li>\n<li><strong>Rationale</strong>: Data parallel provides linear speedup with minimal code changes when model fits on single device, and handles most training scenarios effectively</li>\n<li><strong>Consequences</strong>: Requires gradient synchronization and careful batch size scaling, but enables straightforward scaling to multiple GPUs</li>\n</ul>\n</blockquote>\n<p><strong>Data Parallel Implementation</strong></p>\n<p>Data parallel training replicates the entire model on each GPU while splitting the training batch across devices. Each device processes its portion of the batch independently during the forward pass, then synchronizes gradients across all devices before applying parameter updates.</p>\n<p>The synchronization step typically uses AllReduce communication patterns that efficiently average gradients across all participating devices. Modern frameworks implement optimized AllReduce algorithms that overlap communication with computation, minimizing the synchronization overhead.</p>\n<p>Effective batch size scales linearly with the number of devices, which can improve training efficiency but may require adjusting learning rate and other hyperparameters. The linear scaling rule suggests increasing learning rate proportionally with batch size, though this relationship breaks down at very large batch sizes.</p>\n<table>\n<thead>\n<tr>\n<th>Distributed Component</th>\n<th>Purpose</th>\n<th>Communication Pattern</th>\n<th>Scaling Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Forward Pass</td>\n<td>Independent computation</td>\n<td>None</td>\n<td>Linear speedup</td>\n</tr>\n<tr>\n<td>Gradient Computation</td>\n<td>Independent computation</td>\n<td>None</td>\n<td>Linear speedup</td>\n</tr>\n<tr>\n<td>Gradient Synchronization</td>\n<td>Maintain model consistency</td>\n<td>AllReduce</td>\n<td>Communication overhead</td>\n</tr>\n<tr>\n<td>Parameter Updates</td>\n<td>Synchronized updates</td>\n<td>Broadcast</td>\n<td>Minimal overhead</td>\n</tr>\n</tbody></table>\n<p><strong>Model Parallel Alternatives</strong></p>\n<p>When models exceed single-device memory capacity, model parallel techniques become necessary. Tensor parallel splits individual operations (like matrix multiplications) across multiple devices, while pipeline parallel partitions the model into stages that process different parts of the batch simultaneously.</p>\n<p>Tensor parallel requires careful analysis of operation dependencies and communication patterns, as it introduces synchronization points within individual layers. Pipeline parallel provides cleaner abstractions but requires careful batch scheduling to maintain high device utilization and handle variable execution times.</p>\n<p>The choice between these approaches depends on model architecture, hardware topology, and performance requirements. Tensor parallel works well for very large transformer blocks, while pipeline parallel suits models with clear layer boundaries and uniform computation per layer.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The extensions described above require careful integration with the existing transformer implementation built through the four milestones. The following guidance provides concrete steps for implementing the most impactful enhancements while maintaining code organization and testability.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Positional Encoding</td>\n<td>Sinusoidal encoding replacement</td>\n<td>RoPE with precomputed rotations</td>\n</tr>\n<tr>\n<td>Memory Optimization</td>\n<td>Gradient checkpointing</td>\n<td>Flash Attention implementation</td>\n</tr>\n<tr>\n<td>Training Schedule</td>\n<td>Cosine annealing</td>\n<td>Adaptive schedules with validation monitoring</td>\n</tr>\n<tr>\n<td>Mixed Precision</td>\n<td>Manual FP16 casting</td>\n<td>Framework-integrated AMP</td>\n</tr>\n<tr>\n<td>Distributed Training</td>\n<td>Data parallel with framework support</td>\n<td>Custom communication optimization</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure-extension\">Recommended File Structure Extension</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>transformer_project/\n  src/\n    transformer/\n      extensions/                    ← new extension modules\n        positional/\n          __init__.py\n          rope.py                   ← RoPE implementation\n          alibi.py                  ← ALiBi implementation\n          sinusoidal.py            ← improved sinusoidal\n        attention/\n          __init__.py\n          flash_attention.py        ← memory-efficient attention\n          multi_query.py           ← MQA/GQA variants\n        normalization/\n          __init__.py\n          rmsnorm.py               ← RMSNorm implementation\n        training/\n          __init__.py\n          schedulers.py            ← learning rate schedules\n          amp_utils.py             ← mixed precision utilities\n          distributed.py           ← distributed training helpers\n      core/                         ← existing core modules\n        attention.py               ← enhanced with extension support\n        transformer_block.py       ← configurable normalization\n        training.py                ← extended training loop\n    configs/\n      extension_configs.py          ← configuration for extensions\n    experiments/\n      benchmark_extensions.py       ← performance comparisons\n      ablation_studies.py          ← extension impact analysis</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Learning Rate Scheduler Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CosineAnnealingScheduler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Cosine annealing learning rate scheduler with linear warmup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 optimizer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 max_lr: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 total_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 warmup_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 min_lr: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_lr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_lr</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.min_lr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> min_lr</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.total_steps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_steps</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.warmup_steps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> warmup_steps</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_step </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> step</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update learning rate for current training step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_step </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lr </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._compute_lr()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> param_group </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.optimizer.param_groups:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            param_group[</span><span style=\"color:#9ECBFF\">'lr'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> lr</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_lr</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_step </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.warmup_steps:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Linear warmup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_lr </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_step </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.warmup_steps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Cosine annealing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            progress </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_step </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.warmup_steps) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.total_steps </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.warmup_steps)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            progress </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(progress, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Clamp to [0, 1]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cosine_factor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> math.cos(math.pi </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> progress))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.min_lr </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_lr </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.min_lr) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> cosine_factor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_last_lr</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current learning rate.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._compute_lr()</span></span></code></pre></div>\n\n<p><strong>Mixed Precision Training Wrapper</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.cuda.amp </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> GradScaler, autocast</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AMPTrainer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Automatic Mixed Precision training wrapper.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model, optimizer, scheduler</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scheduler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scheduler</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GradScaler()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.amp_enabled </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> train_step</span><span style=\"color:#E1E4E8\">(self, batch: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute single training step with mixed precision.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model.train()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_ids </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch[</span><span style=\"color:#9ECBFF\">'input_ids'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        targets </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch[</span><span style=\"color:#9ECBFF\">'targets'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Forward pass with autocast</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> autocast(</span><span style=\"color:#FFAB70\">enabled</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.amp_enabled):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model(input_ids)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._compute_loss(logits, targets)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Backward pass with gradient scaling</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaler.scale(loss).backward()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Unscale gradients for clipping</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaler.unscale_(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.optimizer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        grad_norm </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.nn.utils.clip_grad_norm_(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.model.parameters(), </span><span style=\"color:#FFAB70\">max_norm</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Optimizer step with scaling</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaler.step(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.optimizer)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaler.update()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.scheduler:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.scheduler.step()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'loss'</span><span style=\"color:#E1E4E8\">: loss.item(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'grad_norm'</span><span style=\"color:#E1E4E8\">: grad_norm.item(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'lr'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.scheduler.get_last_lr() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.scheduler </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.optimizer.param_groups[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'lr'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'scale'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.scaler.get_scale()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_loss</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute cross-entropy loss with proper mixed precision handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure loss computation happens in FP32 for numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> torch.nn.functional.cross_entropy(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logits.view(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, logits.size(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            targets.view(</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            ignore_index</span><span style=\"color:#F97583\">=-</span><span style=\"color:#79B8FF\">1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>RoPE Integration Skeleton</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RotaryPositionalEmbedding</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Rotary Position Embedding implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, max_seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8192</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_seq_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_seq_length</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute frequency values for each dimension pair</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use geometric progression with base 10000</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # freq[i] = 1.0 / (10000 ** (2 * i / d_model)) for i in range(d_model // 2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Precompute position encodings for all positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For each position pos, compute cos(pos * freq) and sin(pos * freq)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Register buffers for cos and sin tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use self.register_buffer to ensure proper device placement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, q: torch.Tensor, k: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                start_pos: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> Tuple[torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply rotary embeddings to query and key tensors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seq_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> q.shape[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Extract cos and sin values for current sequence positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle start_pos for KV caching during generation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply rotation to query tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Split q into even/odd dimensions and apply rotation formula</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # q_rot = q_even * cos - q_odd * sin, q_even * sin + q_odd * cos</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply same rotation to key tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use identical rotation as query to preserve relative position encoding</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return rotated query and key tensors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure output shapes match input shapes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove when implementing</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultiHeadAttentionWithRoPE</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MultiHeadAttention</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enhanced multi-head attention with RoPE support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(config)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rope </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RotaryPositionalEmbedding(config.d_k, config.seq_length </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: torch.Tensor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                kv_cache: KVCache </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, start_pos: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Forward pass with rotary position embeddings.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batch_size, seq_len, d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> x.shape</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Standard Q, K, V projections</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        q </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.query_projection(x)  </span><span style=\"color:#6A737D\"># [batch, seq_len, num_heads * d_k]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        k </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.key_projection(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        v </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.value_projection(x)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Reshape for multi-head attention</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        q </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> q.view(batch_size, seq_len, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_heads, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.d_k).transpose(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        k </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> k.view(batch_size, seq_len, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_heads, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.d_k).transpose(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        v </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v.view(batch_size, seq_len, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.num_heads, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.d_v).transpose(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Apply RoPE to queries and keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # q_rot, k_rot = self.rope(q, k, start_pos)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Handle KV cache update if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update cache with new k_rot, v values and get full context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Compute attention with rotated queries and keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use existing scaled dot-product attention implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Apply output projection and return</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure proper tensor reshaping for output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span><span style=\"color:#6A737D\">  # Remove when implementing</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Extension Integration Checkpoint</strong></p>\n<p>After implementing each extension category, verify functionality with these checkpoints:</p>\n<ol>\n<li><p><strong>Positional Encoding Extensions</strong></p>\n<ul>\n<li>Test sequence length extrapolation: train on sequences of length N, generate text with length 1.5N</li>\n<li>Compare attention patterns between different positional encoding methods</li>\n<li>Verify mathematical correctness of RoPE rotation formulas with unit tests</li>\n</ul>\n</li>\n<li><p><strong>Training Enhancement Integration</strong></p>\n<ul>\n<li>Monitor training curves with new schedulers - loss should converge 20-30% faster</li>\n<li>Verify mixed precision maintains numerical stability - gradients should not underflow</li>\n<li>Test distributed training with gradient synchronization across multiple GPUs</li>\n</ul>\n</li>\n<li><p><strong>Performance Benchmarking</strong></p>\n<ul>\n<li>Measure memory usage reduction with Flash Attention or gradient checkpointing</li>\n<li>Time training iterations with different optimization combinations</li>\n<li>Profile computational overhead of each extension</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"debugging-tips-for-extensions\">Debugging Tips for Extensions</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Loss explodes with RoPE</td>\n<td>Incorrect rotation matrix computation</td>\n<td>Check cos/sin table values, verify rotation formula</td>\n<td>Debug frequency computation, ensure proper tensor dimensions</td>\n</tr>\n<tr>\n<td>Mixed precision training stalls</td>\n<td>Gradient underflow from insufficient scaling</td>\n<td>Monitor gradient magnitudes and scaler values</td>\n<td>Increase initial gradient scale, check loss computation precision</td>\n</tr>\n<tr>\n<td>Distributed training hangs</td>\n<td>Gradient synchronization deadlock</td>\n<td>Check AllReduce communication patterns</td>\n<td>Ensure all processes participate in gradient sync, verify device placement</td>\n</tr>\n<tr>\n<td>Flash Attention wrong outputs</td>\n<td>Tiling boundary errors or softmax normalization</td>\n<td>Compare outputs with standard attention on small examples</td>\n<td>Debug tile boundaries, verify running statistics for softmax</td>\n</tr>\n<tr>\n<td>Memory usage doesn&#39;t decrease</td>\n<td>Extension not properly integrated</td>\n<td>Profile memory usage before/after enabling extensions</td>\n<td>Check that extensions are actually being used, not bypassed</td>\n</tr>\n</tbody></table>\n<p>These extensions transform the basic transformer implementation into a production-capable system. Each enhancement addresses specific limitations discovered in practical deployment, and their combination enables training and deploying much larger, more capable models than the baseline implementation supports.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation of transformer extensions requires careful integration with existing code while maintaining backwards compatibility and testing coverage. The following guidance provides concrete implementation strategies and code organization patterns.</p>\n<h4 id=\"technology-recommendations-table\">Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Positional Encoding</td>\n<td>Replace learned embeddings with sinusoidal</td>\n<td>Full RoPE implementation with precomputed tables</td>\n</tr>\n<tr>\n<td>Attention Optimization</td>\n<td>Gradient checkpointing for memory</td>\n<td>Flash Attention kernel implementation</td>\n</tr>\n<tr>\n<td>Training Efficiency</td>\n<td>PyTorch native AMP with GradScaler</td>\n<td>Custom mixed precision with optimal casting</td>\n</tr>\n<tr>\n<td>Model Scaling</td>\n<td>DistributedDataParallel wrapper</td>\n<td>Pipeline parallel with custom scheduling</td>\n</tr>\n<tr>\n<td>Learning Rate</td>\n<td>Cosine annealing with warmup</td>\n<td>Adaptive schedules based on validation metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-extension-structure\">Recommended Extension Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/transformer/extensions/\n  __init__.py                      ← extension registry and factory functions\n  positional/\n    __init__.py\n    base_encoding.py               ← abstract base class for positional encodings\n    rope_encoding.py              ← RoPE implementation\n    alibi_encoding.py             ← ALiBi implementation\n    sinusoidal_encoding.py        ← improved sinusoidal\n  attention/\n    __init__.py\n    flash_attention.py            ← memory-efficient attention kernels\n    multi_query_attention.py      ← MQA and GQA implementations\n    sparse_attention.py           ← sparse attention patterns\n  optimization/\n    __init__.py\n    schedulers.py                 ← learning rate scheduling implementations\n    amp_trainer.py                ← mixed precision training utilities\n    gradient_utils.py             ← gradient clipping and monitoring\n  distributed/\n    __init__.py\n    data_parallel.py              ← enhanced data parallel training\n    model_parallel.py             ← tensor and pipeline parallel utilities\n    communication.py              ← optimized gradient synchronization\n  normalization/\n    __init__.py\n    rmsnorm.py                    ← RMSNorm implementation\n    adaptive_norm.py              ← adaptive normalization techniques</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Extension Configuration System</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Union, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for transformer extensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Positional encoding options</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    positional_encoding: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"learned\"</span><span style=\"color:#6A737D\">  # \"learned\", \"sinusoidal\", \"rope\", \"alibi\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rope_base: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alibi_slopes: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Attention optimization options</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    use_flash_attention: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attention_variant: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"multi_head\"</span><span style=\"color:#6A737D\">  # \"multi_head\", \"multi_query\", \"grouped_query\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    query_groups: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#6A737D\">  # for grouped-query attention</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Normalization options</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalization_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"layer_norm\"</span><span style=\"color:#6A737D\">  # \"layer_norm\", \"rms_norm\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalization_placement: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pre\"</span><span style=\"color:#6A737D\">  # \"pre\", \"post\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Training enhancements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    use_mixed_precision: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_checkpointing: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    learning_rate_schedule: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"constant\"</span><span style=\"color:#6A737D\">  # \"constant\", \"cosine\", \"linear\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warmup_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Distributed training</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    distributed_backend: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"nccl\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gradient_sync_frequency: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate configuration after initialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.positional_encoding </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"learned\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"sinusoidal\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"rope\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"alibi\"</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unsupported positional encoding: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.positional_encoding</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.attention_variant </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"grouped_query\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.query_groups </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Grouped-query attention requires query_groups > 1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Registry for dynamically loading transformer extensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _positional_encodings </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _attention_variants </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _normalization_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _schedulers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_positional_encoding</span><span style=\"color:#E1E4E8\">(cls, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, implementation_class):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a positional encoding implementation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        cls</span><span style=\"color:#E1E4E8\">._positional_encodings[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> implementation_class</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_positional_encoding</span><span style=\"color:#E1E4E8\">(cls, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: ExtensionConfig, model_config: TransformerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create positional encoding instance from configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._positional_encodings:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unknown positional encoding: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._positional_encodings[name](model_config, config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Similar methods for other extension types...</span></span></code></pre></div>\n\n<p><strong>Performance Monitoring Utilities</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitor training performance metrics for extension evaluation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics_history: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> timer</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for timing code sections.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            elapsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.timers[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> elapsed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_memory_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record current GPU and system memory usage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> torch.cuda.is_available():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            memory_stats.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_memory_allocated'</span><span style=\"color:#E1E4E8\">: torch.cuda.memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># GB</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_memory_cached'</span><span style=\"color:#E1E4E8\">: torch.cuda.memory_reserved() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gpu_max_memory'</span><span style=\"color:#E1E4E8\">: torch.cuda.max_memory_allocated() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_stats.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'system_memory_percent'</span><span style=\"color:#E1E4E8\">: psutil.virtual_memory().percent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'system_memory_available'</span><span style=\"color:#E1E4E8\">: psutil.virtual_memory().available </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> memory_stats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_training_step</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           learning_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           batch_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           sequence_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record metrics for a single training step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        step_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'step'</span><span style=\"color:#E1E4E8\">: step,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'loss'</span><span style=\"color:#E1E4E8\">: loss,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'learning_rate'</span><span style=\"color:#E1E4E8\">: learning_rate,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'batch_size'</span><span style=\"color:#E1E4E8\">: batch_size,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'sequence_length'</span><span style=\"color:#E1E4E8\">: sequence_length,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            **</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.record_memory_usage(),</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            **</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.timers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics_history.append(step_metrics)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timers.clear()  </span><span style=\"color:#6A737D\"># Reset timers for next step</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_performance_summary</span><span style=\"color:#E1E4E8\">(self, last_n_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate performance summary for recent training steps.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.metrics_history:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        recent_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.metrics_history[</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">last_n_steps:]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute averages and trends</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_loss </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">'loss'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> recent_metrics) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(recent_metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_lr </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(m[</span><span style=\"color:#9ECBFF\">'learning_rate'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> recent_metrics) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(recent_metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        step_times </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [m.get(</span><span style=\"color:#9ECBFF\">'forward_pass'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> m.get(</span><span style=\"color:#9ECBFF\">'backward_pass'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                     for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> recent_metrics </span><span style=\"color:#F97583\">if</span><span style=\"color:#9ECBFF\"> 'forward_pass'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> m]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_step_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(step_times) </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(step_times) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> step_times </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'avg_loss'</span><span style=\"color:#E1E4E8\">: avg_loss,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'avg_learning_rate'</span><span style=\"color:#E1E4E8\">: avg_lr,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'avg_step_time'</span><span style=\"color:#E1E4E8\">: avg_step_time,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'steps_per_second'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> avg_step_time </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> avg_step_time </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'tokens_per_second'</span><span style=\"color:#E1E4E8\">: (recent_metrics[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'batch_size'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                recent_metrics[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'sequence_length'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> avg_step_time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                                if</span><span style=\"color:#E1E4E8\"> avg_step_time </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<h4 id=\"core-extension-implementation-skeletons\">Core Extension Implementation Skeletons</h4>\n<p><strong>Rotary Position Embedding Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch.nn </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> nn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RotaryPositionalEmbedding</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Rotary Position Embedding for improved sequence length extrapolation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, max_seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8192</span><span style=\"color:#E1E4E8\">, base: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> d_model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_seq_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_seq_length</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> base</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute inverse frequency values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create tensor of shape [d_model // 2] with geometric progression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # inv_freq[i] = 1.0 / (base ** (2 * i / d_model)) for i in range(d_model // 2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Precompute cosine and sine tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # For positions 0 to max_seq_length-1, compute:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # cos_table[pos, i] = cos(pos * inv_freq[i])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # sin_table[pos, i] = sin(pos * inv_freq[i])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Register as buffers for automatic device placement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # self.register_buffer('cos_table', cos_table, persistent=False)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # self.register_buffer('sin_table', sin_table, persistent=False)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _rotate_half</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Rotate half the dimensions negatively for RoPE computation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Split tensor into two halves along last dimension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Apply rotation: concatenate [-x2, x1] where x = [x1, x2]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, q: torch.Tensor, k: torch.Tensor, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                start_pos: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> Tuple[torch.Tensor, torch.Tensor]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply rotary embeddings to query and key tensors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seq_len </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> q.shape[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Extract cos and sin values for current positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle start_pos offset for KV caching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # cos_vals = self.cos_table[start_pos:start_pos + seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # sin_vals = self.sin_table[start_pos:start_pos + seq_len]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply rotation to query tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # q_rot = q * cos_vals + self._rotate_half(q) * sin_vals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure broadcasting works correctly for multi-head dimensions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Apply identical rotation to key tensor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # k_rot = k * cos_vals + self._rotate_half(k) * sin_vals</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return rotated tensors with same shapes as inputs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> q_rot, k_rot</span></span></code></pre></div>\n\n<p><strong>Mixed Precision Training Integration</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch.cuda.amp </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> GradScaler, autocast</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EnhancedAMPTrainer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Advanced mixed precision trainer with extension support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 model: nn.Module, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 optimizer: torch.optim.Optimizer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 config: ExtensionConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scheduler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> scheduler</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GradScaler(</span><span style=\"color:#FFAB70\">enabled</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.use_mixed_precision)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.performance_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PerformanceMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> train_step</span><span style=\"color:#E1E4E8\">(self, batch: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, torch.Tensor], step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute training step with all enabled optimizations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model.train()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimizer.zero_grad()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.performance_monitor.timer(</span><span style=\"color:#9ECBFF\">'forward_pass'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Implement forward pass with conditional autocast</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Use autocast only if mixed precision is enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Handle gradient checkpointing if enabled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.performance_monitor.timer(</span><span style=\"color:#9ECBFF\">'backward_pass'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Implement backward pass with gradient scaling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Scale loss before backward if using mixed precision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Apply gradient clipping after unscaling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.performance_monitor.timer(</span><span style=\"color:#9ECBFF\">'optimizer_step'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 11: Implement optimizer step with scaling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Step optimizer and update gradient scaler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Update learning rate scheduler if provided</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 12: Record performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use performance_monitor to track training progress</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 13: Return comprehensive training metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Include loss, learning rate, gradient norms, timing info</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_loss_with_precision</span><span style=\"color:#E1E4E8\">(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute loss with appropriate precision handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 14: Ensure loss computation happens in FP32</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Cast logits to FP32 if using mixed precision for numerical stability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _apply_gradient_clipping</span><span style=\"color:#E1E4E8\">(self, max_norm: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply gradient clipping and return gradient norm.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 15: Unscale gradients before clipping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compute and return gradient norm for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_checkpoint</span><span style=\"color:#E1E4E8\">(self, filepath: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, validation_loss: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save training checkpoint with extension state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 16: Save comprehensive checkpoint including:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Model state dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Optimizer state dict  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Scheduler state dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Gradient scaler state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Extension configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Performance metrics history</span></span></code></pre></div>\n\n<h4 id=\"milestone-verification-checkpoints\">Milestone Verification Checkpoints</h4>\n<p><strong>Extension Integration Verification</strong></p>\n<ol>\n<li><strong>Positional Encoding Extension Checkpoint</strong></li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Test length extrapolation capability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> test_positional_encoding_extrapolation</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Train model on sequences of length 512</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate coherent text on sequences up to length 1024</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify attention patterns remain reasonable at longer lengths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # Expected: Attention should focus on relevant tokens without degradation</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Mixed Precision Training Checkpoint</strong></li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Verify numerical stability and performance gains</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> test_mixed_precision_stability</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare loss curves between FP32 and mixed precision training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure training speed improvement (expect 1.5-2x speedup)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Monitor gradient scaler behavior (should adapt dynamically)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # Expected: Similar final loss with significantly faster training</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Memory Optimization Checkpoint</strong></li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Verify memory usage reduction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   def</span><span style=\"color:#B392F0\"> test_memory_optimization</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure peak memory usage with and without optimizations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify model can train with larger batch sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test gradient checkpointing computational overhead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">       # Expected: 20-50% memory reduction with &#x3C;20% speed penalty</span></span></code></pre></div>\n\n<h4 id=\"performance-benchmarking-framework\">Performance Benchmarking Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Callable</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionBenchmark</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Benchmark framework for comparing extension performance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model_configs: List[TransformerConfig]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model_configs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model_configs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.benchmark_results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> benchmark_attention_variants</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   sequence_lengths: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   batch_sizes: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Benchmark different attention implementations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.model_configs:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> seq_len </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> sequence_lengths:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> batch_size </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> batch_sizes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    test_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"seq_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">seq_len</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_batch_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">batch_size</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 17: Benchmark standard multi-head attention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Create model with standard attention, measure forward/backward time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 18: Benchmark Flash Attention if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Compare memory usage and computation time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 19: Benchmark Multi-Query Attention variants</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Measure KV cache size reduction and inference speed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    results[test_key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        'standard_attention'</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">'time'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'memory'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        'flash_attention'</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">'time'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'memory'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        'multi_query_attention'</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">'time'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'memory'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> benchmark_training_optimizations</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       num_steps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Benchmark training optimization techniques.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 20: Compare training speed with different optimizations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Baseline FP32 training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Mixed precision training</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Gradient checkpointing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # - Combined optimizations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 21: Measure convergence speed differences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Track loss reduction rate with different configurations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 22: Profile memory usage patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Record peak memory usage throughout training</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'fp32_baseline'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'mixed_precision'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'gradient_checkpointing'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'combined_optimizations'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p>These extensions provide the foundation for scaling transformer implementations from educational prototypes to production-capable systems. Each extension addresses specific bottlenecks and limitations discovered in practical deployment, enabling much larger models and more efficient training than the baseline implementation supports.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - comprehensive reference for terminology, mathematical notation, and technical concepts used throughout transformer implementation (Milestones 1-4)</p>\n</blockquote>\n<p>The transformer architecture introduces a rich vocabulary of specialized terminology spanning mathematical concepts, architectural components, and implementation techniques. This glossary provides comprehensive definitions for all terms used throughout the transformer implementation, serving as both a learning resource for newcomers and a reference for experienced practitioners. Understanding these concepts deeply is crucial for successful implementation and debugging of transformer models.</p>\n<p>The terminology is organized into thematic categories to facilitate learning and reference. Each definition includes not only the formal meaning but also contextual usage within transformer implementations, common variations in terminology across different frameworks and papers, and connections to related concepts. Special attention is given to terms where confusion commonly arises, such as the distinction between different normalization approaches or the various attention mechanism variants.</p>\n<h3 id=\"mathematical-notation-and-operations\">Mathematical Notation and Operations</h3>\n<p><strong>Scaled dot-product attention</strong> represents the core mathematical operation of transformer models, computing attention weights through the formula <code>Q @ K^T / sqrt(d_k)</code> followed by softmax normalization. The scaling by the square root of the key dimension prevents the dot products from growing too large, which would cause the softmax to saturate and produce overly sharp attention distributions. This mathematical foundation underlies all attention mechanisms in transformers.</p>\n<p><strong>Multi-head attention</strong> extends scaled dot-product attention by computing multiple parallel attention functions with different learned projections. Each attention head processes the same input through different query, key, and value transformations, allowing the model to attend to information from different representation subspaces simultaneously. The outputs from all heads are concatenated and projected to produce the final attention output.</p>\n<p><strong>Cross-entropy loss</strong> serves as the standard loss function for language modeling, measuring the difference between predicted token probability distributions and the true next-token labels. The loss computation involves taking the negative log-likelihood of the correct token under the model&#39;s predicted distribution, providing a learning signal that encourages the model to assign higher probability to correct continuations.</p>\n<p><strong>Causal mask</strong> implements the constraint that tokens can only attend to previous positions in the sequence, preventing information leakage from future tokens during training. The mask is typically implemented as a lower triangular matrix where future positions are set to negative infinity before applying softmax, ensuring their attention weights become zero.</p>\n<h3 id=\"architecture-components-and-design-patterns\">Architecture Components and Design Patterns</h3>\n<p><strong>Self-attention</strong> describes attention mechanisms where the queries, keys, and values all derive from the same input sequence, allowing tokens to selectively focus on other positions within the same context. This contrasts with cross-attention mechanisms found in encoder-decoder architectures, where queries come from one sequence and keys/values from another.</p>\n<p><strong>Decoder-only</strong> architectures, exemplified by GPT models, consist entirely of transformer decoder blocks without any encoder component. This architectural choice optimizes for autoregressive generation tasks where the model processes and generates text sequentially from left to right.</p>\n<p><strong>Feed-forward network</strong> refers to the position-wise multilayer perceptron within each transformer block, typically expanding the hidden dimension by a factor of 4 before projecting back to the original size. This component provides non-linear transformation capacity that complements the linear operations of attention mechanisms.</p>\n<p><strong>Residual connections</strong> are skip connections that add the input to each sublayer&#39;s output, enabling direct gradient flow to earlier layers and facilitating training of very deep networks. In transformers, residual connections wrap both the attention and feed-forward sublayers within each block.</p>\n<p><strong>Layer normalization</strong> normalizes activations across the feature dimension for each token independently, computing zero mean and unit variance statistics. This normalization technique stabilizes training dynamics and enables the use of higher learning rates compared to unnormalized networks.</p>\n<h3 id=\"training-and-optimization-terminology\">Training and Optimization Terminology</h3>\n<p><strong>Language modeling objective</strong> defines the training task of predicting the next token in a sequence given all previous tokens. This objective transforms the problem of learning language into a standard supervised learning setup where the model learns to minimize prediction error on next-token targets.</p>\n<p><strong>Teacher forcing</strong> describes the training strategy where the model receives the actual previous tokens as input rather than its own predictions. This approach accelerates training by providing correct context at each position, though it can create exposure bias where training and inference conditions differ.</p>\n<p><strong>Label shifting</strong> refers to the offset between input sequences and target labels necessary for next-token prediction. Input sequences contain tokens 0 through n-1, while target sequences contain tokens 1 through n, creating the supervisory signal for learning token continuation patterns.</p>\n<p><strong>Gradient clipping</strong> limits the magnitude of gradients during backpropagation to prevent training instability. Gradient norms are computed across all model parameters and scaled down if they exceed a threshold, preventing the explosive gradient growth that can destabilize transformer training.</p>\n<p><strong>Learning rate scheduling</strong> adapts the learning rate throughout training to improve convergence and final performance. Common schedules include warmup periods with gradually increasing rates followed by decay phases, helping models escape local minima early while achieving stable convergence later.</p>\n<p><strong>Mixed precision training</strong> uses 16-bit floating point arithmetic for most operations while maintaining 32-bit precision for critical computations like loss calculation and parameter updates. This approach significantly reduces memory usage and accelerates training on modern hardware while maintaining numerical stability.</p>\n<h3 id=\"generation-and-sampling-methods\">Generation and Sampling Methods</h3>\n<p><strong>Autoregressive generation</strong> describes the sequential process of generating text one token at a time, where each new token depends on all previously generated tokens in the sequence. This approach enables coherent long-form generation but requires multiple forward passes through the model.</p>\n<p><strong>Greedy decoding</strong> selects the highest probability token at each generation step, producing deterministic outputs that maximize local likelihood. While simple and fast, greedy decoding can lead to repetitive or low-quality text due to its myopic optimization approach.</p>\n<p><strong>Temperature sampling</strong> introduces controlled randomness into generation by scaling the logits before applying softmax. Lower temperatures produce more focused, deterministic outputs, while higher temperatures increase randomness and diversity at the cost of coherence.</p>\n<p><strong>Top-k sampling</strong> restricts token selection to the k most likely candidates at each step, filtering out low-probability options that could lead to incoherent generations. This approach balances diversity with quality by maintaining reasonable candidates while eliminating extreme outliers.</p>\n<p><strong>Top-p sampling</strong> (also called <strong>nucleus sampling</strong>) dynamically selects from the smallest set of tokens whose cumulative probability exceeds a threshold p. This method adapts the candidate set size based on the prediction confidence, using fewer tokens when the model is confident and more when uncertainty is high.</p>\n<p><strong>KV caching</strong> optimizes autoregressive generation by storing computed key and value tensors from previous positions, avoiding redundant computation in subsequent forward passes. This optimization significantly accelerates generation speed, especially for longer sequences.</p>\n<h3 id=\"training-dynamics-and-stability\">Training Dynamics and Stability</h3>\n<p><strong>Vanishing gradients</strong> occur when backpropagated gradients become exponentially small through deep layer chains, preventing effective learning in early layers. Transformers mitigate this through residual connections and layer normalization, though the problem can still arise in very deep models.</p>\n<p><strong>Gradient explosion</strong> represents the opposite problem where gradients grow exponentially large, causing parameter updates that destabilize training. Gradient clipping serves as the primary defense against this issue in transformer training.</p>\n<p><strong>Numerical stability</strong> encompasses techniques for preventing overflow and underflow in floating-point computations. Key strategies include the LogSumExp trick for stable softmax computation, gradient scaling in mixed precision training, and careful initialization of model parameters.</p>\n<p><strong>Softmax overflow</strong> occurs when input values to the softmax function are too large, causing exponential computations to produce infinity. The LogSumExp trick addresses this by subtracting the maximum input value before computing exponentials, maintaining numerical stability without changing the final result.</p>\n<h3 id=\"model-architecture-variants-and-extensions\">Model Architecture Variants and Extensions</h3>\n<p><strong>Rotary position embedding</strong> (RoPE) encodes positional information by applying rotation matrices to query and key vectors, providing better length extrapolation than learned positional embeddings. This approach enables models to handle sequences longer than those seen during training.</p>\n<p><strong>Flash attention</strong> implements memory-efficient attention computation using tiling techniques that reduce memory usage from quadratic to linear in sequence length. This optimization enables processing of much longer sequences within memory constraints.</p>\n<p><strong>Multi-query attention</strong> shares key and value projections across all attention heads while maintaining separate query projections, reducing parameter count and memory usage with minimal impact on model quality.</p>\n<p><strong>Grouped-query attention</strong> represents a middle ground between standard multi-head attention and multi-query attention, grouping heads to share key-value projections while maintaining some parameter diversity.</p>\n<p><strong>ALiBi</strong> (Attention with Linear Biases) replaces positional embeddings with linear biases applied directly to attention scores, providing strong length extrapolation capabilities with minimal computational overhead.</p>\n<h3 id=\"advanced-training-techniques\">Advanced Training Techniques</h3>\n<p><strong>Automatic mixed precision</strong> provides framework-integrated mixed precision training with automatic casting between 16-bit and 32-bit precision based on operation requirements. This approach simplifies mixed precision adoption while maintaining the performance benefits.</p>\n<p><strong>Gradient checkpointing</strong> trades computation for memory by discarding intermediate activations during forward passes and recomputing them as needed during backpropagation. This technique enables training of larger models within memory constraints.</p>\n<p><strong>Distributed training</strong> scales transformer training across multiple devices or machines using various parallelization strategies. Common approaches include data parallel training with batch splitting and model parallel training with component distribution.</p>\n<p><strong>Data parallel</strong> training replicates the full model on each device while splitting training batches across devices, synchronizing gradients after each training step to maintain consistent parameter updates.</p>\n<p><strong>Model parallel</strong> training splits model components across different devices, requiring careful coordination of forward and backward passes to maintain training efficiency.</p>\n<h3 id=\"performance-and-monitoring\">Performance and Monitoring</h3>\n<p><strong>Performance monitoring</strong> tracks training metrics, resource usage, and model behavior throughout the training process. Key metrics include loss curves, gradient norms, learning rates, memory usage, and throughput measurements.</p>\n<p><strong>Extension registry</strong> provides a system for dynamically loading and configuring transformer enhancements like alternative attention mechanisms, positional encodings, and normalization techniques.</p>\n<p><strong>Length extrapolation</strong> refers to a model&#39;s ability to process sequences longer than those encountered during training, a crucial capability for practical deployment of transformer models.</p>\n<h3 id=\"configuration-and-hyperparameters\">Configuration and Hyperparameters</h3>\n<p>The glossary includes numerous configuration parameters that control model behavior and training dynamics. Key dimensional parameters include <code>d_model</code> (embedding dimension), <code>num_heads</code> (attention heads), <code>seq_length</code> (maximum sequence length), and <code>vocab_size</code> (vocabulary size). Training hyperparameters encompass <code>learning_rate</code>, <code>batch_size</code>, <code>gradient_clip_norm</code>, and <code>warmup_steps</code>. Generation parameters include <code>temperature</code>, <code>top_k</code>, <code>top_p</code>, and <code>repetition_penalty</code>.</p>\n<h3 id=\"common-implementation-patterns\">Common Implementation Patterns</h3>\n<p><strong>Information refinement pipeline</strong> serves as a mental model for understanding how transformer blocks iteratively improve token representations. Each block processes the input through attention (gathering relevant context) and feed-forward layers (applying non-linear transformations) to produce refined representations.</p>\n<p><strong>Iterative prediction</strong> describes the mental model for text generation where the model builds sequences incrementally, using each newly generated token to inform subsequent predictions in a chain of dependent decisions.</p>\n<p><strong>Selective focus</strong> provides intuition for attention mechanisms, where each token selectively focuses on relevant context positions to compute context-aware representations rather than processing all positions equally.</p>\n<h3 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h3>\n<p>Understanding failure modes and edge cases is crucial for robust transformer implementation. <strong>Sequence length limits</strong> define maximum token sequence lengths that models can process, <strong>token vocabulary bounds</strong> specify valid ranges of token IDs, and <strong>shape consistency checking</strong> validates tensor dimensions throughout the computation pipeline.</p>\n<p><strong>Input validation</strong> encompasses checks for sequence lengths, token vocabulary bounds, and tensor shape consistency to prevent runtime errors and ensure correct model behavior.</p>\n<p>This comprehensive glossary serves as both a learning resource and ongoing reference throughout transformer implementation. Each term connects to broader concepts within the architecture, and mastering this vocabulary is essential for understanding research papers, debugging implementations, and extending transformer models with new capabilities.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation of transformer terminology requires careful attention to naming consistency and documentation standards. This section provides guidance for maintaining terminological clarity throughout your codebase.</p>\n<h4 id=\"terminology-consistency-standards\">Terminology Consistency Standards</h4>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Convention</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Class Names</td>\n<td>PascalCase with descriptive suffixes</td>\n<td><code>MultiHeadAttention</code>, <code>TransformerBlock</code></td>\n</tr>\n<tr>\n<td>Method Names</td>\n<td>snake_case with action verbs</td>\n<td><code>forward()</code>, <code>compute_attention_weights()</code></td>\n</tr>\n<tr>\n<td>Configuration Fields</td>\n<td>snake_case with clear units</td>\n<td><code>d_model</code>, <code>learning_rate</code>, <code>max_seq_length</code></td>\n</tr>\n<tr>\n<td>Constants</td>\n<td>UPPER_SNAKE_CASE</td>\n<td><code>PAD_TOKEN_ID</code>, <code>EOS_TOKEN_ID</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"documentation-template\">Documentation Template</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultiHeadAttention</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">nn</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Module</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-head attention mechanism with scaled dot-product attention.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Implements parallel attention heads with different learned projections,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    enabling the model to attend to information from different representation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    subspaces simultaneously.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d_model: Model embedding dimension</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        num_heads: Number of parallel attention heads</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        dropout_rate: Dropout probability for attention weights</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Attributes:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d_k: Key dimension per attention head (d_model // num_heads)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d_v: Value dimension per attention head (d_model // num_heads)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        query_projection: Linear layer for query transformation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        key_projection: Linear layer for key transformation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        value_projection: Linear layer for value transformation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        output_projection: Linear layer for final output transformation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize projection layers and attention parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> forward</span><span style=\"color:#E1E4E8\">(self, x: torch.Tensor, mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> torch.Tensor:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply multi-head attention to input sequence.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            x: Input tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            mask: Optional causal mask of shape (seq_length, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Output tensor of shape (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement multi-head attention computation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"configuration-documentation-standards\">Configuration Documentation Standards</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TransformerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for transformer model architecture and training.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Attributes:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d_model: Model embedding dimension (typically 512, 768, or 1024)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        num_heads: Number of parallel attention heads (must divide d_model evenly)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d_k: Key dimension per head (computed as d_model // num_heads)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        d_v: Value dimension per head (computed as d_model // num_heads)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        seq_length: Maximum sequence length for positional encoding</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        vocab_size: Size of token vocabulary</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        num_layers: Number of transformer blocks in the model</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        dropout_rate: Dropout probability for regularization (typically 0.1)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ffn_expansion: Feed-forward network expansion ratio (typically 4)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_model: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_heads: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_k: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">init</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    d_v: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">init</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seq_length: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    vocab_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    num_layers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ffn_expansion: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute derived dimensions and validate configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"d_model must be divisible by num_heads\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_k </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.d_v </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.d_model </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.num_heads</span></span></code></pre></div>\n\n<h4 id=\"glossary-integration-utilities\">Glossary Integration Utilities</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TerminologyValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates consistent use of transformer terminology in code.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PREFERRED_TERMS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'self_attention'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'self-attention'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'selfattention'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'multi_head_attention'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'multihead_attention'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'multi_head_attn'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'feed_forward_network'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'ffn'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'mlp'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'position_wise_ffn'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'layer_normalization'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'layer_norm'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ln'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'causal_mask'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'attention_mask'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'sequence_mask'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_method_names</span><span style=\"color:#E1E4E8\">(self, class_methods: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check method names against preferred terminology.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        warnings </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> method </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> class_methods:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> preferred, alternatives </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">PREFERRED_TERMS</span><span style=\"color:#E1E4E8\">.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(alt </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> method.lower() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> alt </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> alternatives):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    warnings.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Consider using '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">preferred</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' instead of variant in '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">method</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> warnings</span></span></code></pre></div>\n\n<h4 id=\"type-annotation-standards\">Type Annotation Standards</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, Dict, Any, List, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> torch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> torch </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> nn, Tensor</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Standard type aliases for transformer implementations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">AttentionWeights </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.Tensor  </span><span style=\"color:#6A737D\"># Shape: (batch_size, num_heads, seq_length, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">TokenSequence </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.Tensor     </span><span style=\"color:#6A737D\"># Shape: (batch_size, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">Embeddings </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.Tensor        </span><span style=\"color:#6A737D\"># Shape: (batch_size, seq_length, d_model)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">Logits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> torch.Tensor           </span><span style=\"color:#6A737D\"># Shape: (batch_size, seq_length, vocab_size)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> scaled_dot_product_attention</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    query: Embeddings,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key: Embeddings, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Embeddings,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mask: Optional[torch.Tensor] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dropout_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">) -> Tuple[Embeddings, AttentionWeights]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compute scaled dot-product attention with optional masking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        query: Query tensor of shape (batch_size, seq_length, d_k)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        key: Key tensor of shape (batch_size, seq_length, d_k)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        value: Value tensor of shape (batch_size, seq_length, d_v)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        mask: Optional mask of shape (seq_length, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        dropout_rate: Attention dropout probability</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tuple of (attention_output, attention_weights)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - attention_output: Weighted value sum of shape (batch_size, seq_length, d_v)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - attention_weights: Attention probabilities of shape (batch_size, seq_length, seq_length)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement scaled dot-product attention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"debugging-terminology-helpers\">Debugging Terminology Helpers</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> print_tensor_info</span><span style=\"color:#E1E4E8\">(tensor: torch.Tensor, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_shape: Optional[Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Print comprehensive tensor information with standardized terminology.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        tensor: PyTorch tensor to analyze</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        name: Descriptive name using standard terminology</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        expected_shape: Optional expected shape for validation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> Tensor Analysis:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Shape: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tensor.shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Device: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tensor.device</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Dtype: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tensor.dtype</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Requires Grad: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tensor.requires_grad</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Memory Usage: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tensor.numel() </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> tensor.element_size() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> KB\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> expected_shape:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shape_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tensor.shape </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> torch.Size(expected_shape)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Shape Match: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">shape_match</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> shape_match:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Expected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_shape</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Got: </span><span style=\"color:#79B8FF\">{tuple</span><span style=\"color:#E1E4E8\">(tensor.shape)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage example:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># print_tensor_info(attention_weights, \"Multi-Head Attention Weights\", (batch_size, num_heads, seq_length, seq_length))</span></span></code></pre></div>\n\n<h4 id=\"milestone-terminology-checkpoints\">Milestone Terminology Checkpoints</h4>\n<p>After implementing each milestone, verify terminology consistency:</p>\n<p><strong>Milestone 1 - Self-Attention Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check that attention-related terms are used consistently</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">grep</span><span style=\"color:#79B8FF\"> -r</span><span style=\"color:#9ECBFF\"> \"attention\"</span><span style=\"color:#9ECBFF\"> src/</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> \"self.attention\\|multi_head_attention\\|scaled_dot_product_attention\"</span></span></code></pre></div>\n\n<p><strong>Milestone 2 - Transformer Block Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Verify transformer block terminology</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import inspect</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.transformer_block import TransformerBlock</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">methods = [m for m in dir(TransformerBlock) if not m.startswith('_')]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('TransformerBlock methods:', methods)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Should see: forward, get_attention_weights</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Milestone 3 - Training Pipeline Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check training-related terminology consistency  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.trainer import TransformerTrainer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.config import TrainingConfig</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">config_fields = TrainingConfig.__dataclass_fields__.keys()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('TrainingConfig fields:', list(config_fields))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Should match exactly: learning_rate, batch_size, num_epochs, gradient_clip_norm, etc.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Milestone 4 - Generation Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Verify generation terminology</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from src.generator import TextGenerator, SamplingStrategies</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">sampling_methods = [m for m in dir(SamplingStrategies) if not m.startswith('_')]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Sampling methods:', sampling_methods)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Should see: greedy_sample, temperature_sample, top_k_sample, top_p_sample</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p>This implementation guidance ensures consistent terminology usage throughout your transformer implementation, making your code more readable, maintainable, and aligned with standard conventions in the field.</p>\n","toc":[{"level":1,"text":"Build Your Own Transformer: Design Document","id":"build-your-own-transformer-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Cocktail Party","id":"mental-model-the-cocktail-party"},{"level":3,"text":"Pre-Transformer Approaches","id":"pre-transformer-approaches"},{"level":4,"text":"Recurrent Neural Networks: The Sequential Bottleneck","id":"recurrent-neural-networks-the-sequential-bottleneck"},{"level":4,"text":"Convolutional Neural Networks: Local Patterns Only","id":"convolutional-neural-networks-local-patterns-only"},{"level":4,"text":"The Transformer Revolution: Parallel Attention","id":"the-transformer-revolution-parallel-attention"},{"level":3,"text":"Implementation Challenges","id":"implementation-challenges"},{"level":4,"text":"Mathematical Complexity: The Attention Computation","id":"mathematical-complexity-the-attention-computation"},{"level":4,"text":"Multi-Head Attention Coordination","id":"multi-head-attention-coordination"},{"level":4,"text":"Numerical Stability Issues","id":"numerical-stability-issues"},{"level":4,"text":"Memory and Computational Scalability","id":"memory-and-computational-scalability"},{"level":4,"text":"Integration and Testing Complexity","id":"integration-and-testing-complexity"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"What We Will Build","id":"what-we-will-build"},{"level":4,"text":"Core Architecture Components","id":"core-architecture-components"},{"level":4,"text":"Training Infrastructure","id":"training-infrastructure"},{"level":4,"text":"Text Generation System","id":"text-generation-system"},{"level":4,"text":"Development and Debugging Tools","id":"development-and-debugging-tools"},{"level":3,"text":"What We Won&#39;t Build","id":"what-we-won39t-build"},{"level":4,"text":"Performance Optimizations","id":"performance-optimizations"},{"level":4,"text":"Advanced Architectural Features","id":"advanced-architectural-features"},{"level":4,"text":"Production Engineering Requirements","id":"production-engineering-requirements"},{"level":4,"text":"Advanced Training Techniques","id":"advanced-training-techniques"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Mental Model: The Literary Editor&#39;s Process","id":"mental-model-the-literary-editor39s-process"},{"level":3,"text":"Component Hierarchy","id":"component-hierarchy"},{"level":4,"text":"Input Processing Layer","id":"input-processing-layer"},{"level":4,"text":"Representation Learning Layer","id":"representation-learning-layer"},{"level":4,"text":"Generation Layer","id":"generation-layer"},{"level":4,"text":"System Integration Patterns","id":"system-integration-patterns"},{"level":3,"text":"Recommended Code Organization","id":"recommended-code-organization"},{"level":4,"text":"Module Structure Overview","id":"module-structure-overview"},{"level":4,"text":"Configuration Management Strategy","id":"configuration-management-strategy"},{"level":4,"text":"Component Interface Design","id":"component-interface-design"},{"level":4,"text":"Debugging and Visualization Support","id":"debugging-and-visualization-support"},{"level":4,"text":"Testing Strategy Integration","id":"testing-strategy-integration"},{"level":4,"text":"Performance Optimization Considerations","id":"performance-optimization-considerations"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Data Model and Types","id":"data-model-and-types"},{"level":3,"text":"Tensor Dimensions","id":"tensor-dimensions"},{"level":3,"text":"Configuration Structures","id":"configuration-structures"},{"level":4,"text":"Model Architecture Configuration","id":"model-architecture-configuration"},{"level":4,"text":"Training Configuration","id":"training-configuration"},{"level":4,"text":"Generation Configuration","id":"generation-configuration"},{"level":4,"text":"Configuration Relationships and Dependencies","id":"configuration-relationships-and-dependencies"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Configuration Infrastructure (Complete Implementation)","id":"configuration-infrastructure-complete-implementation"},{"level":4,"text":"Tensor Utilities (Complete Implementation)","id":"tensor-utilities-complete-implementation"},{"level":4,"text":"Core Configuration Skeletons","id":"core-configuration-skeletons"},{"level":4,"text":"Configuration Examples","id":"configuration-examples"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Self-Attention Mechanism","id":"self-attention-mechanism"},{"level":3,"text":"Mental Model: Selective Focus","id":"mental-model-selective-focus"},{"level":3,"text":"Scaled Dot-Product Attention","id":"scaled-dot-product-attention"},{"level":3,"text":"Multi-Head Attention","id":"multi-head-attention"},{"level":3,"text":"Causal Attention Masking","id":"causal-attention-masking"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Transformer Block Design","id":"transformer-block-design"},{"level":3,"text":"Mental Model: Information Refinement Pipeline","id":"mental-model-information-refinement-pipeline"},{"level":3,"text":"Feed-Forward Network","id":"feed-forward-network"},{"level":3,"text":"Layer Normalization Strategy","id":"layer-normalization-strategy"},{"level":3,"text":"Residual Connections","id":"residual-connections"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations","id":"a-technology-recommendations"},{"level":4,"text":"B. Recommended File Structure","id":"b-recommended-file-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"Training Pipeline","id":"training-pipeline"},{"level":3,"text":"Tokenization Approach","id":"tokenization-approach"},{"level":3,"text":"Data Loading and Batching","id":"data-loading-and-batching"},{"level":3,"text":"Language Modeling Objective","id":"language-modeling-objective"},{"level":3,"text":"Training Loop Design","id":"training-loop-design"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Training Loop Skeleton","id":"core-training-loop-skeleton"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Text Generation","id":"text-generation"},{"level":3,"text":"Mental Model: Iterative Prediction","id":"mental-model-iterative-prediction"},{"level":3,"text":"Sampling Strategy Design","id":"sampling-strategy-design"},{"level":4,"text":"Greedy Decoding","id":"greedy-decoding"},{"level":4,"text":"Temperature-Based Sampling","id":"temperature-based-sampling"},{"level":4,"text":"Top-k Sampling","id":"top-k-sampling"},{"level":4,"text":"Top-p (Nucleus) Sampling","id":"top-p-nucleus-sampling"},{"level":3,"text":"KV Cache Optimization","id":"kv-cache-optimization"},{"level":4,"text":"Cache Structure and Management","id":"cache-structure-and-management"},{"level":4,"text":"Cache Update Process","id":"cache-update-process"},{"level":4,"text":"Memory Management Considerations","id":"memory-management-considerations"},{"level":4,"text":"Cache Consistency and Debugging","id":"cache-consistency-and-debugging"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Core Generation Infrastructure","id":"core-generation-infrastructure"},{"level":4,"text":"Core Generation Logic (Skeleton for Implementation)","id":"core-generation-logic-skeleton-for-implementation"},{"level":4,"text":"Integration with Transformer Model","id":"integration-with-transformer-model"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Organization","id":"file-organization"},{"level":4,"text":"Complete KV Cache Infrastructure","id":"complete-kv-cache-infrastructure"},{"level":4,"text":"Core Generation Logic Implementation Skeleton","id":"core-generation-logic-implementation-skeleton"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Language-Specific Implementation Notes","id":"language-specific-implementation-notes"},{"level":4,"text":"Debugging Common Generation Issues","id":"debugging-common-generation-issues"},{"level":2,"text":"Component Interactions and Data Flow","id":"component-interactions-and-data-flow"},{"level":3,"text":"Mental Model: The Assembly Line","id":"mental-model-the-assembly-line"},{"level":3,"text":"Forward Pass Sequence","id":"forward-pass-sequence"},{"level":4,"text":"Token Input and Embedding","id":"token-input-and-embedding"},{"level":4,"text":"Positional Information Integration","id":"positional-information-integration"},{"level":4,"text":"Multi-Layer Transformer Processing","id":"multi-layer-transformer-processing"},{"level":4,"text":"Attention Computation Flow","id":"attention-computation-flow"},{"level":4,"text":"Output Head and Logit Generation","id":"output-head-and-logit-generation"},{"level":4,"text":"Data Flow Dependencies","id":"data-flow-dependencies"},{"level":4,"text":"Common Forward Pass Pitfalls","id":"common-forward-pass-pitfalls"},{"level":3,"text":"Training vs Inference Flows","id":"training-vs-inference-flows"},{"level":4,"text":"Training Flow Characteristics","id":"training-flow-characteristics"},{"level":4,"text":"Inference Flow Characteristics","id":"inference-flow-characteristics"},{"level":4,"text":"Mode-Specific Component Behavior","id":"mode-specific-component-behavior"},{"level":4,"text":"Memory and Computational Trade-offs","id":"memory-and-computational-trade-offs"},{"level":4,"text":"Error Handling Differences","id":"error-handling-differences"},{"level":4,"text":"Common Mode-Specific Pitfalls","id":"common-mode-specific-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Model Orchestration Code","id":"core-model-orchestration-code"},{"level":4,"text":"Training Flow Implementation","id":"training-flow-implementation"},{"level":4,"text":"Inference Flow Implementation","id":"inference-flow-implementation"},{"level":4,"text":"KV Cache Implementation","id":"kv-cache-implementation"},{"level":4,"text":"Data Flow Debugging Utilities","id":"data-flow-debugging-utilities"},{"level":4,"text":"Mode-Specific Context Managers","id":"mode-specific-context-managers"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Common Implementation Issues","id":"common-implementation-issues"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Numerical Stability","id":"numerical-stability"},{"level":3,"text":"Input Validation","id":"input-validation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Testing Strategy and Milestones","id":"testing-strategy-and-milestones"},{"level":3,"text":"Component Unit Tests","id":"component-unit-tests"},{"level":3,"text":"End-to-End Integration","id":"end-to-end-integration"},{"level":3,"text":"Milestone Verification","id":"milestone-verification"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: The Debugging Detective","id":"mental-model-the-debugging-detective"},{"level":3,"text":"Attention Mechanism Debugging","id":"attention-mechanism-debugging"},{"level":3,"text":"Training Issues","id":"training-issues"},{"level":3,"text":"Generation Quality Issues","id":"generation-quality-issues"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Mental Model: The Performance Optimization Pyramid","id":"mental-model-the-performance-optimization-pyramid"},{"level":3,"text":"Architectural Improvements","id":"architectural-improvements"},{"level":4,"text":"Advanced Positional Encodings","id":"advanced-positional-encodings"},{"level":4,"text":"Attention Mechanism Variants","id":"attention-mechanism-variants"},{"level":4,"text":"Normalization Strategy Evolution","id":"normalization-strategy-evolution"},{"level":3,"text":"Training Enhancements","id":"training-enhancements"},{"level":4,"text":"Learning Rate Scheduling Evolution","id":"learning-rate-scheduling-evolution"},{"level":4,"text":"Mixed Precision Training","id":"mixed-precision-training"},{"level":4,"text":"Distributed Training Strategies","id":"distributed-training-strategies"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure Extension","id":"recommended-file-structure-extension"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips for Extensions","id":"debugging-tips-for-extensions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations Table","id":"technology-recommendations-table"},{"level":4,"text":"Recommended Extension Structure","id":"recommended-extension-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Extension Implementation Skeletons","id":"core-extension-implementation-skeletons"},{"level":4,"text":"Milestone Verification Checkpoints","id":"milestone-verification-checkpoints"},{"level":4,"text":"Performance Benchmarking Framework","id":"performance-benchmarking-framework"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mathematical Notation and Operations","id":"mathematical-notation-and-operations"},{"level":3,"text":"Architecture Components and Design Patterns","id":"architecture-components-and-design-patterns"},{"level":3,"text":"Training and Optimization Terminology","id":"training-and-optimization-terminology"},{"level":3,"text":"Generation and Sampling Methods","id":"generation-and-sampling-methods"},{"level":3,"text":"Training Dynamics and Stability","id":"training-dynamics-and-stability"},{"level":3,"text":"Model Architecture Variants and Extensions","id":"model-architecture-variants-and-extensions"},{"level":3,"text":"Advanced Training Techniques","id":"advanced-training-techniques"},{"level":3,"text":"Performance and Monitoring","id":"performance-and-monitoring"},{"level":3,"text":"Configuration and Hyperparameters","id":"configuration-and-hyperparameters"},{"level":3,"text":"Common Implementation Patterns","id":"common-implementation-patterns"},{"level":3,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Terminology Consistency Standards","id":"terminology-consistency-standards"},{"level":4,"text":"Documentation Template","id":"documentation-template"},{"level":4,"text":"Configuration Documentation Standards","id":"configuration-documentation-standards"},{"level":4,"text":"Glossary Integration Utilities","id":"glossary-integration-utilities"},{"level":4,"text":"Type Annotation Standards","id":"type-annotation-standards"},{"level":4,"text":"Debugging Terminology Helpers","id":"debugging-terminology-helpers"},{"level":4,"text":"Milestone Terminology Checkpoints","id":"milestone-terminology-checkpoints"}],"title":"Build Your Own Transformer: Design Document","markdown":"# Build Your Own Transformer: Design Document\n\n\n## Overview\n\nThis system implements a GPT-style transformer from scratch for autoregressive text generation. The key architectural challenge is building the multi-layer attention mechanism that allows each token to attend to all previous tokens while efficiently processing sequences in parallel during training.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** Foundation for all milestones - understanding why transformers exist and the challenges in implementing them\n\nThe transformer architecture fundamentally changed how we think about sequence modeling in natural language processing. Before diving into implementation details, we need to understand why transformers were revolutionary, what problems they solved, and the specific challenges we'll face when building one from scratch. This section establishes the conceptual foundation and practical motivation for our implementation journey.\n\n### Mental Model: The Cocktail Party\n\nImagine you're at a bustling cocktail party with dozens of conversations happening simultaneously. As a human, you have a remarkable ability to focus your attention selectively - you can tune into the conversation with your colleague while filtering out background chatter, or suddenly shift focus when you hear your name mentioned across the room. Moreover, you can dynamically adjust your attention based on context: when someone mentions a topic you're passionate about, you lean in and focus intently, but when the conversation turns to something irrelevant, your attention naturally drifts.\n\n**Attention mechanisms in transformers work remarkably similarly.** Each word (or token) in a sentence is like a person at the cocktail party. When processing the word \"bank\" in the sentence \"I went to the bank to deposit money,\" the attention mechanism allows the word \"bank\" to \"listen\" to all the other words in the sentence. It pays more attention to words like \"deposit\" and \"money\" (which suggest a financial institution) while giving less attention to words like \"went\" or \"to\" (which are less informative for disambiguation).\n\nThe key insight is that this attention is **learned, not programmed.** Just as you've learned through experience that certain conversational cues indicate important information, the transformer learns through training which words should pay attention to which other words. The attention weights emerge naturally from the data, allowing the model to discover linguistic patterns like:\n- Pronouns attending to their antecedents (\"The cat sat on its bed\" - \"its\" strongly attends to \"cat\")\n- Modifiers attending to what they modify (\"The red car\" - \"red\" attends to \"car\")\n- Syntactic relationships (\"The book that I read\" - \"book\" and \"read\" attend to each other)\n\n> **Key Insight:** Unlike rule-based systems that explicitly encode grammatical relationships, transformers discover these relationships implicitly through attention patterns that emerge during training.\n\nThis cocktail party analogy captures three crucial aspects of transformer attention:\n1. **Selective focus**: Not all word pairs are equally important\n2. **Dynamic adjustment**: Attention patterns change based on context\n3. **Learned behavior**: The model discovers what to pay attention to through training data\n\n### Pre-Transformer Approaches\n\nBefore transformers revolutionized NLP in 2017, researchers relied primarily on recurrent neural networks (RNNs) and convolutional neural networks (CNNs) for sequence modeling. Understanding their limitations illuminates why the transformer architecture was so groundbreaking.\n\n#### Recurrent Neural Networks: The Sequential Bottleneck\n\nRNNs, including their more sophisticated variants like LSTMs and GRUs, process sequences one token at a time in strict left-to-right order. Think of an RNN as a person reading a book who can only look at one word at a time and must remember everything important from previous words in their limited working memory.\n\n**The fundamental limitation** is that information must pass through a sequential bottleneck. To understand the relationship between \"cat\" and \"its\" in \"The cat walked across the room and sat on its bed,\" the RNN must:\n1. Process \"cat\" and encode it in hidden state h₁\n2. Process \"walked\" and update the hidden state to h₂ (potentially diluting information about \"cat\")\n3. Continue through \"across,\" \"the,\" \"room,\" \"and,\" \"sat,\" \"on\" - each step potentially losing more information about the original \"cat\"\n4. Finally reach \"its\" with a hidden state h₉ that hopefully still contains enough information about \"cat\" to make the connection\n\nThis creates several problems:\n\n| Problem | Description | Impact on Performance |\n|---------|-------------|----------------------|\n| **Vanishing Gradients** | Information from early tokens gets exponentially diluted through sequential processing | Long-range dependencies are poorly captured |\n| **Sequential Processing** | Cannot parallelize computation across sequence positions | Training is slow, especially on long sequences |\n| **Fixed Context Window** | Hidden state has limited capacity to store information | Complex relationships in long texts are lost |\n| **Recency Bias** | Recent tokens have disproportionate influence on the hidden state | Earlier context gets forgotten |\n\n#### Convolutional Neural Networks: Local Patterns Only\n\nCNNs excel at capturing local patterns through sliding windows of fixed size. In NLP, this translates to detecting n-gram patterns like \"not good\" (negative sentiment) or \"New York\" (named entity). However, CNNs face their own limitations for sequence modeling:\n\n**Limited receptive field**: A CNN with kernel size 3 can only directly relate words that are within 3 positions of each other. To capture longer dependencies, you need either:\n- Very deep networks (many layers) which causes vanishing gradients and increases computational cost\n- Very large kernels which become parameter-intensive and lose the inductive bias of local patterns\n\n**No positional flexibility**: CNNs assume that patterns are translation-invariant (the same pattern matters regardless of position), but language is highly position-dependent. The phrase \"not happy\" has different meaning than \"happy, not sad.\"\n\n#### The Transformer Revolution: Parallel Attention\n\nTransformers solve these fundamental limitations through a radically different approach: instead of processing sequences sequentially or locally, they compute attention between all pairs of positions simultaneously.\n\n> **Architecture Decision: Self-Attention Over Sequential Processing**\n> - **Context**: Need to capture long-range dependencies without sequential processing bottlenecks\n> - **Options Considered**: \n>   1. Deeper RNNs with better memory mechanisms\n>   2. CNN architectures with dilated convolutions for larger receptive fields  \n>   3. Self-attention mechanisms that directly model all pairwise relationships\n> - **Decision**: Self-attention with parallel computation across all sequence positions\n> - **Rationale**: Eliminates sequential bottleneck, enables parallelization, provides direct paths for information flow between any two positions regardless of distance\n> - **Consequences**: Enables efficient training on long sequences and better capture of long-range dependencies, but introduces quadratic memory complexity in sequence length\n\nThe transformer's self-attention mechanism creates direct connections between every pair of tokens. In our \"cat...its\" example, the attention mechanism can directly compute how much \"its\" should attend to \"cat\" without any intermediate processing steps. This eliminates the vanishing gradient problem for long-range dependencies and enables parallel computation.\n\n| Architecture | Dependency Path Length | Parallel Operations | Memory Complexity |\n|--------------|------------------------|--------------------|--------------------|\n| **RNN** | O(sequence_length) | O(1) per timestep | O(sequence_length × hidden_size) |\n| **CNN** | O(log(sequence_length)) with dilated convolutions | O(sequence_length) | O(sequence_length × hidden_size) |\n| **Transformer** | O(1) - direct connections | O(sequence_length²) | O(sequence_length² + sequence_length × hidden_size) |\n\nThe transformer trades increased memory usage (quadratic in sequence length) for dramatically improved modeling capability and training efficiency. This trade-off has proven worthwhile for most NLP applications, leading to the current dominance of transformer-based models.\n\n### Implementation Challenges\n\nBuilding a transformer from scratch involves several interconnected mathematical and computational challenges. Unlike simpler neural network architectures where you can implement and test components in isolation, transformers require careful coordination between multiple sophisticated mechanisms.\n\n#### Mathematical Complexity: The Attention Computation\n\nThe core mathematical challenge lies in implementing scaled dot-product attention correctly. The computation appears deceptively simple:\n\n**Attention(Q, K, V) = softmax(QK^T / √d_k)V**\n\nHowever, this simple equation conceals several implementation pitfalls:\n\n⚠️ **Pitfall: Dimension Mismatches**\nThe matrix multiplication QK^T requires Q to have shape (batch_size, seq_len, d_k) and K to have shape (batch_size, seq_len, d_k), producing an attention matrix of shape (batch_size, seq_len, seq_len). Many implementations fail because they transpose the wrong dimensions or assume K should be transposed before the operation. The correct implementation transposes K during the multiplication: `torch.matmul(Q, K.transpose(-2, -1))`.\n\n⚠️ **Pitfall: Missing Scale Factor**  \nThe division by √d_k is crucial for numerical stability. Without this scaling, the dot products can become very large (especially for large d_k), causing the softmax to produce extremely sharp distributions with gradients close to zero. This leads to training instability and poor convergence. The scale factor keeps the variance of the dot products approximately constant regardless of the key dimension.\n\n⚠️ **Pitfall: Incorrect Masking**\nCausal attention masking must set future positions to negative infinity BEFORE applying softmax, not after. Setting masked positions to zero after softmax breaks the probability distribution (the remaining probabilities don't sum to 1) and provides misleading attention patterns during debugging.\n\n#### Multi-Head Attention Coordination\n\nMulti-head attention requires splitting the model dimension across multiple attention heads and then recombining their outputs. This creates several coordination challenges:\n\n**Dimension partitioning**: With model dimension d_model = 512 and num_heads = 8, each head operates on d_k = d_v = 64 dimensions. The implementation must:\n1. Split the input embeddings into 8 chunks of size 64\n2. Apply separate Q, K, V projections to each chunk  \n3. Compute attention independently for each head\n4. Concatenate the 8 outputs back into a 512-dimensional vector\n5. Apply a final linear projection\n\n**Memory layout optimization**: Naive implementations might use loops over attention heads, but efficient implementations reshape tensors to compute all heads in parallel. This requires careful tensor manipulation:\n- Input shape: (batch_size, seq_len, d_model)\n- Reshaped for multi-head: (batch_size, seq_len, num_heads, d_k)  \n- Transposed for parallel computation: (batch_size, num_heads, seq_len, d_k)\n- After attention: (batch_size, num_heads, seq_len, d_v)\n- Concatenated output: (batch_size, seq_len, d_model)\n\n#### Numerical Stability Issues\n\nTransformers are particularly susceptible to numerical instability due to the combination of softmax operations, layer normalization, and deep architectures.\n\n**Softmax overflow**: For large attention scores, softmax can overflow or underflow. Consider attention scores [10, 8, 12] - after exponential, these become [22026, 2981, 162754], which may exceed float32 precision. The standard solution is to subtract the maximum value before exponential: [10-12, 8-12, 12-12] = [-2, -4, 0], giving stable exponentials [0.135, 0.018, 1.0].\n\n**Gradient explosion**: Deep transformer stacks can suffer from exploding gradients, especially in the early stages of training. This is typically addressed through:\n- Gradient clipping to limit the norm of gradients\n- Careful weight initialization (Xavier/Glorot or He initialization)  \n- Layer normalization to stabilize activations\n- Residual connections to provide stable gradient paths\n\n**Accumulating precision errors**: During autoregressive generation, small numerical errors can accumulate over many generation steps. This is particularly problematic when using mixed precision training (float16 for speed, float32 for accuracy) where careful casting between precisions is required.\n\n#### Memory and Computational Scalability\n\nThe quadratic memory complexity of self-attention creates significant scalability challenges:\n\n**Memory usage**: For a sequence of length L with model dimension d, self-attention requires O(L²) memory for storing attention weights, plus O(L × d) for embeddings. For L=512 and batch size 32, the attention matrix alone requires 32 × 512 × 512 × 4 bytes = 33MB per layer. A 12-layer model uses nearly 400MB just for attention matrices.\n\n**Computational complexity**: Each attention computation requires O(L² × d) operations. For long sequences, this dominates training time. Modern implementations use several optimizations:\n- **KV caching**: During generation, cache key and value computations for previous tokens to avoid recomputation\n- **Attention chunking**: Process attention in blocks to reduce peak memory usage\n- **Flash attention**: Fused attention kernels that minimize memory transfers\n\n#### Integration and Testing Complexity\n\nUnlike simpler architectures where you can test each layer independently, transformer components are tightly coupled:\n\n**Forward pass dependencies**: Testing the transformer block requires working implementations of multi-head attention, layer normalization, feed-forward networks, and residual connections. A bug in any component can cause failures throughout the system.\n\n**Training loop complexity**: The language modeling objective requires careful coordination between tokenization, batch preparation, loss computation, and gradient updates. Label shifting (input tokens vs. target tokens) is a common source of bugs.\n\n**Generation complexity**: Autoregressive text generation involves different code paths than training, with additional complexity from sampling strategies, KV caching, and stopping criteria.\n\n> **Implementation Strategy**: Given these interdependencies, we'll build and test components in carefully ordered milestones. Each milestone produces a testable artifact that validates the correctness of previous components while building toward the complete system.\n\nThe combination of mathematical sophistication, numerical stability requirements, and tight component coupling makes transformer implementation significantly more challenging than simpler neural architectures. However, understanding these challenges upfront allows us to structure our implementation to minimize debugging complexity and ensure correctness at each step.\n\n### Implementation Guidance\n\nThis implementation guidance provides the practical foundation for building your transformer, with technology choices optimized for learning and debugging rather than production performance.\n\n#### Technology Recommendations\n\n| Component | Simple Option (Recommended for Learning) | Advanced Option |\n|-----------|------------------------------------------|-----------------|\n| **Deep Learning Framework** | PyTorch with manual tensor operations | PyTorch Lightning with automated training loops |\n| **Tensor Operations** | Explicit `torch.matmul`, `torch.softmax` calls | `torch.nn.functional` high-level operations |\n| **Model Definition** | Custom `nn.Module` classes with explicit forward methods | `nn.Sequential` or config-driven model builders |\n| **Data Loading** | Simple character-level tokenization with manual batching | HuggingFace tokenizers with DataLoader optimizations |\n| **Training Loop** | Manual gradient computation and optimization steps | PyTorch Lightning Trainer with callbacks |\n| **Debugging** | Print statements and tensor shape inspection | TensorBoard integration with attention visualizations |\n\n**Rationale for simple options**: When learning transformers, explicit implementations help you understand what each operation does. High-level abstractions hide important details about tensor shapes, attention computations, and gradient flow that are crucial for debugging and intuition building.\n\n#### Recommended Project Structure\n\nOrganize your code to mirror the conceptual hierarchy and enable incremental testing:\n\n```\ntransformer-project/\n├── src/\n│   ├── __init__.py\n│   ├── attention.py          ← Self-attention mechanism (Milestone 1)\n│   ├── transformer_block.py  ← Complete transformer block (Milestone 2)  \n│   ├── model.py             ← Full transformer stack and embedding\n│   ├── training.py          ← Training pipeline (Milestone 3)\n│   ├── generation.py        ← Text generation (Milestone 4)\n│   └── utils.py             ← Helper functions and utilities\n├── tests/\n│   ├── test_attention.py    ← Unit tests for attention mechanisms\n│   ├── test_transformer_block.py  ← Unit tests for complete blocks\n│   ├── test_integration.py  ← End-to-end pipeline tests\n│   └── test_data/           ← Small datasets for testing\n├── data/\n│   ├── tiny_shakespeare.txt ← Small training corpus\n│   └── processed/           ← Tokenized and batched data\n├── notebooks/\n│   ├── debug_attention.ipynb    ← Attention weight visualization\n│   ├── training_analysis.ipynb ← Loss curves and training metrics\n│   └── generation_examples.ipynb ← Sample outputs and analysis\n├── configs/\n│   └── model_config.py      ← Hyperparameter configurations\n└── requirements.txt         ← Python dependencies\n```\n\nThis structure separates concerns cleanly while maintaining clear dependencies: `attention.py` has no internal dependencies, `transformer_block.py` depends only on `attention.py`, and so forth.\n\n#### Infrastructure Starter Code\n\n**Configuration Management** (`configs/model_config.py`):\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass TransformerConfig:\n    \"\"\"Complete configuration for transformer model and training.\"\"\"\n    # Model architecture\n    vocab_size: int = 65        # Number of unique tokens (characters for tiny dataset)\n    max_seq_length: int = 256   # Maximum sequence length for training\n    d_model: int = 384          # Model dimension (embedding size)\n    num_heads: int = 6          # Number of attention heads (d_model must be divisible)\n    num_layers: int = 6         # Number of transformer blocks\n    d_ff: int = 1536           # Feed-forward dimension (typically 4 * d_model)\n    dropout_rate: float = 0.1   # Dropout probability\n    \n    # Training hyperparameters\n    learning_rate: float = 3e-4\n    batch_size: int = 64\n    num_epochs: int = 10\n    warmup_steps: int = 1000\n    max_grad_norm: float = 1.0  # Gradient clipping threshold\n    \n    # Generation settings\n    temperature: float = 1.0\n    top_k: Optional[int] = None\n    top_p: Optional[float] = None\n    max_generation_length: int = 100\n    \n    def __post_init__(self):\n        \"\"\"Validate configuration parameters.\"\"\"\n        assert self.d_model % self.num_heads == 0, f\"d_model ({self.d_model}) must be divisible by num_heads ({self.num_heads})\"\n        assert self.d_ff > 0, \"Feed-forward dimension must be positive\"\n        assert 0.0 <= self.dropout_rate < 1.0, \"Dropout rate must be in [0, 1)\"\n```\n\n**Utility Functions** (`src/utils.py`):\n```python\nimport torch\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, List\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_causal_mask(seq_length: int, device: torch.device) -> torch.Tensor:\n    \"\"\"\n    Create lower triangular causal mask for self-attention.\n    \n    Args:\n        seq_length: Length of sequence\n        device: Device to create tensor on\n        \n    Returns:\n        Boolean mask of shape (seq_length, seq_length) where True means \"allow attention\"\n    \"\"\"\n    # TODO 1: Create lower triangular matrix using torch.tril\n    # TODO 2: Convert to boolean (1s become True, 0s become False)  \n    # TODO 3: Move to specified device\n    pass\n\ndef apply_causal_mask(attention_scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Apply causal mask to attention scores before softmax.\n    \n    Args:\n        attention_scores: Raw attention scores of shape (batch, num_heads, seq_len, seq_len)\n        mask: Boolean mask of shape (seq_len, seq_len)\n        \n    Returns:\n        Masked attention scores with -inf at masked positions\n    \"\"\"\n    # TODO 1: Use torch.where to set masked positions to -inf\n    # TODO 2: Ensure mask is broadcasted correctly across batch and head dimensions\n    # Hint: Use float('-inf') for masked positions to ensure softmax gives 0 probability\n    pass\n\ndef count_parameters(model: torch.nn.Module) -> int:\n    \"\"\"Count total number of trainable parameters in model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef plot_attention_weights(attention_weights: torch.Tensor, tokens: List[str], \n                          layer: int = 0, head: int = 0, save_path: Optional[str] = None):\n    \"\"\"\n    Visualize attention weights as heatmap.\n    \n    Args:\n        attention_weights: Attention weights of shape (num_layers, num_heads, seq_len, seq_len)\n        tokens: List of token strings for axis labels\n        layer: Which layer to visualize  \n        head: Which attention head to visualize\n        save_path: Optional path to save the plot\n    \"\"\"\n    # TODO 1: Extract weights for specified layer and head\n    # TODO 2: Create matplotlib heatmap with token labels\n    # TODO 3: Add colorbar and title\n    # TODO 4: Save or display the plot\n    pass\n\nclass SimpleTokenizer:\n    \"\"\"Character-level tokenizer for educational purposes.\"\"\"\n    \n    def __init__(self, vocab: Optional[List[str]] = None):\n        \"\"\"Initialize tokenizer with vocabulary.\"\"\"\n        # TODO 1: If vocab is None, create default vocab with printable ASCII chars\n        # TODO 2: Create char_to_idx and idx_to_char mappings\n        # TODO 3: Store special token indices (if any)\n        pass\n    \n    def encode(self, text: str) -> List[int]:\n        \"\"\"Convert text string to list of token indices.\"\"\"\n        # TODO 1: Convert each character to its vocabulary index\n        # TODO 2: Handle unknown characters (either skip or use special UNK token)\n        pass\n    \n    def decode(self, token_ids: List[int]) -> str:\n        \"\"\"Convert list of token indices back to text string.\"\"\"\n        # TODO 1: Convert each index back to character\n        # TODO 2: Join characters into string\n        # TODO 3: Handle invalid indices gracefully\n        pass\n    \n    @classmethod\n    def from_text(cls, text: str) -> 'SimpleTokenizer':\n        \"\"\"Create tokenizer by extracting vocabulary from text corpus.\"\"\"\n        # TODO 1: Find all unique characters in text\n        # TODO 2: Sort characters for consistent ordering\n        # TODO 3: Return new tokenizer instance with this vocabulary\n        pass\n```\n\n**Basic Data Loading** (`src/data_loader.py`):\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Tuple, Optional\nfrom .utils import SimpleTokenizer\n\nclass TextDataset(Dataset):\n    \"\"\"Dataset for character-level language modeling.\"\"\"\n    \n    def __init__(self, text: str, tokenizer: SimpleTokenizer, \n                 seq_length: int, stride: Optional[int] = None):\n        \"\"\"\n        Create dataset from text string.\n        \n        Args:\n            text: Raw text content\n            tokenizer: Tokenizer to convert text to indices  \n            seq_length: Length of each training sequence\n            stride: Step size between sequences (default: seq_length for no overlap)\n        \"\"\"\n        # TODO 1: Tokenize the entire text\n        # TODO 2: Create overlapping sequences of length seq_length + 1 \n        #         (extra token for target)\n        # TODO 3: Store sequences as list of (input, target) pairs\n        # Hint: Input is tokens[i:i+seq_length], target is tokens[i+1:i+seq_length+1]\n        pass\n    \n    def __len__(self) -> int:\n        \"\"\"Return number of sequences in dataset.\"\"\"\n        # TODO: Return length of stored sequences\n        pass\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Return input and target tensors for given index.\"\"\"\n        # TODO 1: Get the idx-th sequence pair\n        # TODO 2: Convert to PyTorch tensors with dtype=torch.long\n        pass\n\ndef create_data_loaders(text: str, config, train_split: float = 0.9) -> Tuple[DataLoader, DataLoader, SimpleTokenizer]:\n    \"\"\"\n    Create training and validation data loaders from text.\n    \n    Args:\n        text: Raw text content\n        config: TransformerConfig with batch_size and seq_length\n        train_split: Fraction of data to use for training\n        \n    Returns:\n        Tuple of (train_loader, val_loader, tokenizer)\n    \"\"\"\n    # TODO 1: Create tokenizer from text vocabulary\n    # TODO 2: Split text into train and validation portions\n    # TODO 3: Create datasets for each split\n    # TODO 4: Create DataLoader instances with specified batch_size\n    # TODO 5: Return loaders and tokenizer\n    pass\n```\n\n#### Core Logic Skeleton Code\n\n**Self-Attention Implementation** (`src/attention.py`):\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nfrom .utils import create_causal_mask, apply_causal_mask\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head self-attention mechanism.\"\"\"\n    \n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float = 0.1):\n        \"\"\"\n        Initialize multi-head attention.\n        \n        Args:\n            d_model: Model dimension (must be divisible by num_heads)\n            num_heads: Number of parallel attention heads\n            dropout_rate: Dropout probability for attention weights\n        \"\"\"\n        super().__init__()\n        # TODO 1: Store configuration parameters\n        # TODO 2: Calculate d_k = d_v = d_model // num_heads\n        # TODO 3: Create linear projections for Q, K, V (single linear layer that outputs d_model dimensions)\n        # TODO 4: Create output projection layer  \n        # TODO 5: Create dropout layer\n        # TODO 6: Initialize scale factor for attention scores\n        \n        # Hint: Use single linear layer for efficiency: self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n        pass\n    \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Apply multi-head self-attention to input sequence.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, d_model)\n            mask: Optional causal mask of shape (seq_length, seq_length)\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_length, d_model)\n        \"\"\"\n        batch_size, seq_length, d_model = x.shape\n        \n        # TODO 1: Apply QKV projection and split into Q, K, V\n        # TODO 2: Reshape Q, K, V for multi-head attention\n        #         From: (batch_size, seq_length, d_model)  \n        #         To: (batch_size, num_heads, seq_length, d_k)\n        # TODO 3: Compute attention scores: Q @ K.transpose(-2, -1)\n        # TODO 4: Scale attention scores by sqrt(d_k)\n        # TODO 5: Apply causal mask if provided (set masked positions to -inf)\n        # TODO 6: Apply softmax to get attention weights\n        # TODO 7: Apply dropout to attention weights\n        # TODO 8: Apply attention weights to values: attention_weights @ V\n        # TODO 9: Reshape output back to (batch_size, seq_length, d_model)\n        # TODO 10: Apply final output projection\n        \n        # Hint: Use torch.matmul for batch matrix multiplication\n        # Hint: For reshaping, use .view() and .contiguous() as needed\n        pass\n```\n\n#### Language-Specific Hints\n\n**PyTorch Tensor Operations**:\n- Use `tensor.view()` to reshape tensors, but call `.contiguous()` first if the tensor is not contiguous in memory\n- `torch.matmul()` handles batch matrix multiplication automatically - it multiplies the last two dimensions and broadcasts over earlier dimensions\n- For numerical stability in softmax, PyTorch's `F.softmax()` automatically handles overflow by subtracting the max value\n- Use `tensor.transpose(-2, -1)` to swap the last two dimensions regardless of total tensor dimensionality\n\n**Memory Management**:\n- Use `torch.no_grad()` context manager during inference to save memory and speed up computation\n- Call `optimizer.zero_grad()` before each backward pass to clear accumulated gradients\n- Use `del` to explicitly free large tensors when done with them during debugging\n\n**Device Handling**:\n```python\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\ndata = data.to(device)\n```\n\n**Gradient Debugging**:\n- Use `torch.autograd.grad_mode.set_grad_enabled(True)` to ensure gradients are being computed\n- Check if gradients are flowing: `print(f\"Parameter grad norm: {param.grad.norm().item()}\")` after backward pass\n- Use `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)` for gradient clipping\n\n#### Milestone Checkpoints\n\n**Milestone 1 Checkpoint - Self-Attention**:\nAfter implementing the attention mechanism, verify correct behavior:\n```bash\ncd tests/\npython test_attention.py\n```\n\nExpected output:\n- Attention weights sum to 1.0 for each query position\n- Causal mask prevents attending to future positions (upper triangular of attention matrix should be zeros)\n- Output shape matches input shape: (batch_size, seq_length, d_model)\n- Gradient flows backward through attention computation\n\nManual verification:\n```python\n# Create small test input\nx = torch.randn(2, 4, 64)  # batch=2, seq_len=4, d_model=64\nattention = MultiHeadAttention(d_model=64, num_heads=4)\noutput = attention(x)\nprint(f\"Input shape: {x.shape}, Output shape: {output.shape}\")\n```\n\n**Signs of problems**:\n- NaN values in output → Check for division by zero in scaling or softmax overflow\n- Wrong output shape → Verify tensor reshaping and transposition operations  \n- No gradient flow → Ensure all operations are differentiable and tensors require_grad=True\n- Attention weights don't sum to 1 → Check softmax application and mask timing\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** Foundation for all milestones - defining the scope and boundaries of our transformer implementation\n\nBuilding a transformer from scratch is an ambitious undertaking that could easily expand into months of work if we tried to implement every optimization and feature found in production systems. Like an architect who must choose between building a sturdy house versus a sprawling mansion, we need to clearly define what we will and won't build to maintain focus on the core learning objectives.\n\nThe primary challenge in scoping a transformer implementation lies in balancing educational value with practical constraints. We want to build something substantial enough to understand the fundamental mechanisms, yet focused enough to complete within a reasonable timeframe. This means making deliberate trade-offs between completeness and clarity, between performance and simplicity.\n\nOur approach follows the principle of **progressive complexity**: we'll implement the essential components that make a transformer work, ensuring each piece is solid and well-understood, rather than attempting to build a production-ready system with all the bells and whistles. Think of this as building a working bicycle before attempting to construct a Formula 1 race car.\n\n### What We Will Build\n\nOur transformer implementation will focus on the **core mechanisms that define the architecture**, providing a solid foundation for understanding how these models work under the hood. Every component we build serves a direct educational purpose in understanding attention, autoregression, or neural language modeling.\n\n#### Core Architecture Components\n\nWe will implement a complete **decoder-only transformer** similar to the GPT architecture. This includes building every layer of the transformer stack from the ground up, starting with the mathematical primitives and working up to the full model.\n\n| Component | Purpose | Implementation Scope |\n|-----------|---------|---------------------|\n| `MultiHeadAttention` | Core attention mechanism with Q/K/V projections | Scaled dot-product attention with configurable heads |\n| `TransformerBlock` | Complete transformer layer | Attention + FFN + normalization + residuals |\n| `TransformerModel` | Full model stack | Embedding + multiple transformer blocks + output head |\n| `SimpleTokenizer` | Text-to-token conversion | Character-level tokenization with encode/decode |\n| `TextDataset` | Training data handling | Sequence batching with proper label shifting |\n\nThe attention mechanism will be implemented with **full mathematical transparency**. Rather than using high-level library functions, we'll compute the scaled dot-product attention manually, implement the multi-head splitting and concatenation, and handle the causal masking ourselves. This hands-on approach ensures deep understanding of how attention weights are computed and applied.\n\n> **Decision: Decoder-Only Architecture**\n> - **Context**: We could implement encoder-decoder (like original Transformer), encoder-only (like BERT), or decoder-only (like GPT)\n> - **Options Considered**: \n>   1. Full encoder-decoder for translation tasks\n>   2. Encoder-only for understanding/classification  \n>   3. Decoder-only for text generation\n> - **Decision**: Decoder-only transformer (GPT-style)\n> - **Rationale**: Autoregressive generation provides the clearest demonstration of attention mechanisms, and the training objective (next-token prediction) is straightforward to implement and debug\n> - **Consequences**: We focus on generation tasks rather than understanding tasks, but gain simplicity in architecture and training pipeline\n\n#### Training Infrastructure\n\nOur training pipeline will implement the fundamental components needed for **language modeling** with next-token prediction. This includes proper data loading, loss computation, and optimization, but keeps the training loop simple and transparent.\n\n| Training Component | Functionality | Educational Focus |\n|-------------------|---------------|-------------------|\n| Language Modeling Objective | Cross-entropy loss for next-token prediction | Understanding autoregressive training |\n| Gradient Computation | Backpropagation through transformer blocks | How gradients flow through attention |\n| Basic Optimization | Adam optimizer with configurable learning rate | Standard neural network training |\n| Loss Monitoring | Training loss logging and convergence tracking | Diagnosing training progress |\n\nThe training loop will be implemented as a straightforward iteration over batches, computing forward passes, calculating losses, and updating parameters. We'll include essential monitoring to track convergence, but avoid complex learning rate schedules or advanced optimization techniques that obscure the core learning process.\n\n#### Text Generation System\n\nOur generation system will implement multiple **sampling strategies** to demonstrate how different decoding methods affect output quality. This provides hands-on experience with the trade-offs between deterministic and stochastic generation.\n\n| Generation Method | Behavior | Learning Objective |\n|------------------|----------|-------------------|\n| Greedy Decoding | Always select highest probability token | Deterministic baseline generation |\n| Temperature Sampling | Scale logits before sampling | Control randomness vs coherence |\n| Top-k Sampling | Sample from k most likely tokens | Truncated probability distributions |\n| Top-p (Nucleus) Sampling | Sample from cumulative probability mass p | Dynamic vocabulary filtering |\n\nEach sampling strategy will be implemented as a separate function that takes raw logits and returns token selections, making the differences between approaches explicit and easy to experiment with.\n\n#### Development and Debugging Tools\n\nTo support the learning process, we'll build essential tools for **understanding and debugging** the transformer's behavior. These tools make the model's internal state visible and help diagnose common implementation issues.\n\n| Tool | Purpose | Implementation |\n|------|---------|----------------|\n| `plot_attention_weights()` | Visualize attention patterns | Heatmap of attention scores across heads/layers |\n| `count_parameters()` | Model size analysis | Count trainable parameters by component |\n| Gradient monitoring | Training stability diagnosis | Track gradient norms and detect vanishing/exploding |\n| Loss curve plotting | Training progress visualization | Simple matplotlib charts of training metrics |\n\nThese debugging tools will be integrated into the training and inference pipelines, providing immediate feedback on model behavior and making it easier to spot implementation bugs or training issues.\n\n### What We Won't Build\n\nWhile our transformer will be functionally complete, we'll deliberately omit numerous **production optimizations and advanced features** that would complicate the implementation without proportional educational benefit. These omissions are strategic choices to maintain focus on the core mechanisms.\n\n#### Performance Optimizations\n\nWe will skip the complex optimizations that make production transformers efficient but obscure their fundamental operation. Our implementation prioritizes clarity over speed, accepting performance trade-offs in service of educational goals.\n\n| Optimization | Why We Skip It | Production Impact |\n|--------------|---------------|-------------------|\n| KV caching during generation | Complex memory management obscures generation logic | 10-100x speedup for long sequences |\n| Flash Attention | Requires deep CUDA knowledge and specialized kernels | Significant memory reduction |\n| Mixed precision training | Adds complexity to gradient handling | 2x memory savings, faster training |\n| Gradient checkpointing | Complex memory/compute trade-off implementation | Enables training much larger models |\n| Model parallelism | Distributed systems complexity | Required for models that don't fit on one GPU |\n\nWhile KV caching provides dramatic speedups during autoregressive generation, implementing it correctly requires careful management of tensor slicing, concatenation, and device placement. The educational value doesn't justify the implementation complexity for our purposes.\n\n> **Decision: No KV Cache Optimization**\n> - **Context**: Autoregressive generation repeatedly computes attention over previously generated tokens\n> - **Options Considered**:\n>   1. Implement full KV caching with proper memory management\n>   2. Simple caching without memory optimization\n>   3. No caching - recompute everything each step\n> - **Decision**: No caching - recompute attention weights each generation step  \n> - **Rationale**: KV caching requires complex tensor manipulation and memory management that obscures the core attention mechanism. The performance cost is acceptable for educational sequences\n> - **Consequences**: Generation will be slower (O(n²) instead of O(n) per token), but attention computation remains transparent and debuggable\n\n#### Advanced Architectural Features\n\nModern transformers incorporate numerous architectural refinements that improve performance but add implementation complexity. We'll use the original transformer design principles rather than chasing the latest architectural innovations.\n\n| Feature | Educational Value | Implementation Complexity |\n|---------|------------------|---------------------------|\n| Rotary Positional Embedding (RoPE) | Minimal - core attention still the same | High - complex trigonometric operations |\n| Layer-wise learning rates | Moderate - optimization insight | Medium - requires per-parameter learning rates |\n| Sparse attention patterns | Moderate - attention efficiency | High - custom attention masks and indexing |\n| Mixture of Experts | Low - doesn't teach core transformer concepts | Very High - routing and load balancing |\n| Advanced normalization (RMSNorm, etc.) | Low - layer normalization concepts transfer | Medium - different mathematical formulations |\n\nThese features represent active areas of research and engineering optimization, but they build upon the fundamental attention mechanisms we'll implement. Understanding our basic transformer provides the foundation needed to comprehend these advanced techniques later.\n\n#### Production Engineering Requirements\n\nReal-world transformer deployments require extensive engineering infrastructure that falls outside our educational scope. These systems are complex enough to be projects in their own right.\n\n| System Component | Complexity | Why We Skip |\n|------------------|------------|-------------|\n| Distributed training | Very High | Requires deep understanding of distributed systems |\n| Model serving infrastructure | High | Focus is on model internals, not deployment |\n| Efficient tokenization (BPE, SentencePiece) | Medium | Character-level tokenization sufficient for learning |\n| Checkpointing and model versioning | Medium | Standard deep learning infrastructure |\n| Production monitoring and logging | Medium | Standard MLOps practices |\n| ONNX/TensorRT optimization | High | Specialized inference optimization |\n\nOur `SimpleTokenizer` will use character-level tokenization rather than implementing subword algorithms like Byte Pair Encoding. While BPE provides better modeling efficiency, character tokenization eliminates the complexity of building and managing subword vocabularies, allowing us to focus on the transformer architecture itself.\n\n> **Decision: Character-Level Tokenization**\n> - **Context**: Need tokenization for text processing, with options ranging from word-level to subword to character-level\n> - **Options Considered**:\n>   1. Byte Pair Encoding (BPE) for efficiency\n>   2. Word-level tokenization for simplicity\n>   3. Character-level tokenization for minimal complexity\n> - **Decision**: Character-level tokenization via `SimpleTokenizer`\n> - **Rationale**: Character tokenization eliminates vocabulary management complexity and subword algorithm implementation, while still demonstrating the core concept of text-to-token conversion\n> - **Consequences**: Less efficient token usage (longer sequences for same text), but dramatically simpler implementation and debugging\n\n#### Advanced Training Techniques\n\nModern transformer training employs sophisticated techniques for stability, efficiency, and performance. We'll use standard gradient descent optimization rather than implementing these advanced methods.\n\n| Training Technique | Benefit | Implementation Burden |\n|-------------------|---------|----------------------|\n| Learning rate scheduling (cosine, linear warmup) | Better convergence | Complex scheduling logic |\n| Gradient clipping with adaptive norms | Training stability | Gradient norm computation and clipping |\n| Loss scaling for mixed precision | Numerical stability | Dynamic loss scale adjustment |\n| Curriculum learning | Faster convergence | Sophisticated data ordering |\n| Regularization techniques (weight decay, dropout scheduling) | Better generalization | Multiple regularization implementations |\n\nOur training loop will use a fixed learning rate and standard dropout, providing a clear baseline for understanding how transformer training works. Once this foundation is solid, learners can experiment with advanced techniques as extensions.\n\nThe scope we've defined creates a **complete yet manageable** transformer implementation that demonstrates every essential concept while remaining achievable for a dedicated learner. Each component we include directly contributes to understanding attention mechanisms, autoregressive modeling, or neural language generation.\n\n### Implementation Guidance\n\nThis implementation guidance provides the practical foundation for building our scoped transformer, with clear technology choices and file organization that support the educational goals we've defined.\n\n#### Technology Recommendations\n\nOur technology stack prioritizes **learning accessibility** over cutting-edge performance, choosing tools that are widely available and well-documented rather than specialized frameworks that require expert knowledge.\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Deep Learning Framework | PyTorch with standard layers | PyTorch with custom CUDA kernels |\n| Tokenization | Character-level with Python strings | SentencePiece or Hugging Face tokenizers |\n| Visualization | Matplotlib for attention heatmaps | TensorBoard or Weights & Biases |\n| Data Loading | PyTorch DataLoader with simple batching | Custom data pipeline with preprocessing |\n| Model Serialization | PyTorch state_dict save/load | ONNX export for production deployment |\n| Development Environment | Local GPU or Google Colab | Multi-GPU cluster or cloud instances |\n\nThe simple options provide everything needed for understanding transformers while keeping dependency management and setup straightforward. Advanced options offer better performance but require additional expertise that distracts from the core learning objectives.\n\n#### Recommended File Structure\n\nOrganizing code clearly from the start prevents the common mistake of implementing everything in a single monolithic file. This structure separates concerns while keeping related components together.\n\n```\ntransformer-from-scratch/\n├── src/\n│   ├── __init__.py\n│   ├── config.py              # TransformerConfig and hyperparameters\n│   ├── tokenizer.py           # SimpleTokenizer implementation  \n│   ├── attention.py           # MultiHeadAttention and masking utilities\n│   ├── transformer.py         # TransformerBlock and full model\n│   ├── training.py            # Training loop and optimization\n│   ├── generation.py          # Text generation and sampling strategies\n│   └── utils.py               # Helper functions and debugging tools\n├── data/\n│   └── sample_text.txt        # Training data\n├── notebooks/\n│   ├── attention_visualization.ipynb  # Interactive attention analysis\n│   └── training_demo.ipynb    # Step-by-step training walkthrough\n├── tests/\n│   ├── test_attention.py      # Unit tests for attention mechanism\n│   ├── test_transformer.py    # Integration tests for full model\n│   └── test_generation.py     # Generation quality and correctness tests\n├── requirements.txt           # Python dependencies\n└── README.md                  # Setup and usage instructions\n```\n\nThis structure scales well as the implementation grows, with clear separation between core components (`attention.py`, `transformer.py`) and supporting infrastructure (`training.py`, `utils.py`). The notebooks provide interactive environments for experimentation and visualization.\n\n#### Infrastructure Starter Code\n\nThese complete utility functions handle the foundational operations that support transformer implementation but aren't the primary learning focus. Copy these directly and build the core transformer components on top of them.\n\n**Configuration Management** (`config.py`):\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass TransformerConfig:\n    \"\"\"Complete configuration for transformer model and training.\"\"\"\n    # Model architecture\n    d_model: int = 512              # embedding dimension\n    num_heads: int = 8              # number of attention heads  \n    d_k: int = 64                   # key/query dimension per head\n    d_v: int = 64                   # value dimension per head\n    seq_length: int = 512           # maximum sequence length\n    vocab_size: int = 128           # vocabulary size (for character-level)\n    num_layers: int = 6             # number of transformer blocks\n    d_ff: int = 2048               # feed-forward hidden dimension\n    dropout_rate: float = 0.1       # dropout probability\n    \n    # Training parameters\n    batch_size: int = 32\n    learning_rate: float = 0.0001\n    num_epochs: int = 10\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    def __post_init__(self):\n        \"\"\"Validate configuration and set derived values.\"\"\"\n        assert self.d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_k = self.d_model // self.num_heads\n        self.d_v = self.d_model // self.num_heads\n```\n\n**Utility Functions** (`utils.py`):\n```python\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List\n\ndef count_parameters(model: torch.nn.Module) -> int:\n    \"\"\"Count total trainable parameters in model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_causal_mask(seq_length: int, device: torch.device) -> torch.Tensor:\n    \"\"\"Create lower triangular causal attention mask.\"\"\"\n    # TODO: Create seq_length x seq_length matrix of ones\n    # TODO: Use torch.tril to make it lower triangular  \n    # TODO: Convert to boolean mask (1 = attend, 0 = mask)\n    # TODO: Move to specified device\n    pass\n\ndef apply_causal_mask(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"Apply causal mask to attention scores before softmax.\"\"\"\n    # TODO: Replace masked positions with large negative value (-1e9)\n    # TODO: Ensure mask broadcast correctly across batch and head dimensions\n    pass\n\ndef plot_attention_weights(weights: torch.Tensor, tokens: List[str], \n                          layer: int, head: int, save_path: Optional[str] = None):\n    \"\"\"Visualize attention weights as heatmap.\"\"\"\n    # TODO: Extract specific layer and head from weights tensor\n    # TODO: Create matplotlib heatmap with token labels\n    # TODO: Add title indicating layer and head number  \n    # TODO: Save or display plot based on save_path parameter\n    pass\n```\n\n#### Core Logic Skeleton Code\n\nThese signatures define the interfaces for the main transformer components that learners should implement themselves. Each TODO maps directly to the algorithm steps described in the design sections.\n\n**Multi-Head Attention** (`attention.py`):\n```python\nimport torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head self-attention with causal masking.\"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        # TODO: Initialize W_q, W_k, W_v projection matrices\n        # TODO: Initialize W_o output projection matrix  \n        # TODO: Store config values (d_model, num_heads, etc.)\n        # TODO: Initialize dropout layer\n        \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass through multi-head attention.\n        \n        Args:\n            x: Input embeddings (batch_size, seq_length, d_model)\n            mask: Causal attention mask (seq_length, seq_length)\n            \n        Returns:\n            Attended embeddings (batch_size, seq_length, d_model)\n        \"\"\"\n        # TODO: Apply Q, K, V projections to input\n        # TODO: Reshape projections for multi-head computation  \n        # TODO: Compute scaled dot-product attention scores\n        # TODO: Apply causal mask if provided\n        # TODO: Apply softmax to get attention weights\n        # TODO: Apply dropout to attention weights\n        # TODO: Compute weighted sum of values\n        # TODO: Concatenate heads and apply output projection\n        pass\n```\n\n**Transformer Block** (`transformer.py`):\n```python\nclass TransformerBlock(nn.Module):\n    \"\"\"Single transformer block with attention and feed-forward layers.\"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        # TODO: Initialize MultiHeadAttention layer\n        # TODO: Initialize feed-forward network (two linear layers with ReLU)\n        # TODO: Initialize layer normalization layers\n        # TODO: Initialize dropout layers\n        \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass through transformer block.\n        \n        Args:\n            x: Input embeddings (batch_size, seq_length, d_model)  \n            mask: Causal attention mask\n            \n        Returns:\n            Transformed embeddings (batch_size, seq_length, d_model)\n        \"\"\"\n        # TODO: Apply attention sub-layer with residual connection\n        # TODO: Apply layer normalization after attention\n        # TODO: Apply feed-forward sub-layer with residual connection  \n        # TODO: Apply layer normalization after feed-forward\n        pass\n```\n\n#### Milestone Checkpoints\n\nAfter implementing each major component, verify correct behavior with these concrete tests and expected outputs.\n\n**Milestone 1 Checkpoint - Self-Attention**:\n```bash\npython -m pytest tests/test_attention.py -v\n```\nExpected behavior:\n- Attention weights sum to 1.0 across sequence dimension for each query position\n- Causal mask prevents attention to future positions (weights are 0 for masked positions)  \n- Multi-head outputs have correct dimensions after concatenation\n- Gradient flows correctly through attention computation\n\n**Milestone 2 Checkpoint - Transformer Block**:\n```bash\npython tests/test_transformer.py\n```\nExpected behavior:\n- Input and output tensors have identical shapes (residual connections preserve dimensions)\n- Layer normalization produces zero mean, unit variance activations\n- Feed-forward network expands to 4x dimension internally then projects back\n- Dropout is applied during training but not during evaluation\n\n**Training Verification**:\n```python\n# Simple overfitting test - model should memorize small dataset\nconfig = TransformerConfig(seq_length=64, batch_size=8, num_layers=2)\n# Train on 10 short sequences, loss should approach zero\n```\n\n#### Language-Specific Implementation Hints\n\n**PyTorch-Specific Guidance**:\n- Use `torch.nn.Linear` for all projection matrices rather than manual weight initialization\n- Apply `torch.nn.functional.softmax(scores, dim=-1)` for attention weight normalization  \n- Use `scores.masked_fill(mask == 0, -1e9)` for efficient causal masking\n- Call `model.train()` before training loops and `model.eval()` before inference\n- Use `torch.no_grad()` context manager during evaluation to disable gradient computation\n\n**Debugging Tips**:\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|------------------|-----|\n| Attention weights all equal | Missing causal mask or wrong softmax dimension | Print attention matrix shape and values | Apply mask before softmax, verify dim=-1 |\n| Loss stays constant | Wrong label shifting in data loader | Check if targets are shifted by one position | Shift targets: `targets = inputs[:, 1:]` |\n| Memory errors during training | Gradient accumulation without clearing | Check if gradients are zeroed each step | Call `optimizer.zero_grad()` before each batch |\n| Model generates repetitive text | No sampling diversity | Check if using greedy decoding only | Implement temperature or top-k sampling |\n\nThis implementation guidance provides the scaffolding needed to build the scoped transformer we've defined, with clear separation between infrastructure code (copy directly) and core learning components (implement with provided skeletons).\n\n\n## High-Level Architecture\n\n> **Milestone(s):** Foundation for all milestones - understanding how transformer components compose together and organizing code for implementation\n\nUnderstanding the transformer architecture requires grasping how multiple sophisticated components work together to transform input text into coherent generated output. The key insight is that transformers operate as a **hierarchical refinement system** - starting with raw tokens, progressively building richer representations through multiple layers of attention and processing, and finally producing probability distributions over the vocabulary for text generation.\n\n### Mental Model: The Literary Editor's Process\n\nThink of a transformer like a team of literary editors working on a manuscript. Each editor (transformer block) reads through the entire document, paying **selective attention** to different parts based on context, and makes refinements to improve clarity and coherence. The first editor might focus on basic grammar and word choice, while later editors handle more sophisticated aspects like narrative flow and thematic consistency. Each editor passes their refined version to the next, and the final editor produces suggestions for what word should come next to continue the story most naturally.\n\nJust as each editor has access to the entire manuscript but focuses on different aspects, each transformer block processes all tokens in the sequence but learns to attend to different patterns and relationships. The hierarchical structure allows the model to build increasingly sophisticated understanding - from simple token relationships in early layers to complex semantic and syntactic patterns in deeper layers.\n\n### Component Hierarchy\n\nThe transformer system consists of four primary hierarchical layers, each with distinct responsibilities and data transformations. Understanding this hierarchy is crucial for implementation because it determines how information flows through the system and how gradients propagate during training.\n\n![Transformer System Components](./diagrams/system-components.svg)\n\n#### Input Processing Layer\n\nThe input processing layer converts raw text into numerical representations that the transformer can manipulate mathematically. This layer handles the critical transition from discrete symbolic information (text) to continuous vector representations.\n\n| Component | Input Type | Output Type | Primary Responsibility |\n|-----------|------------|-------------|----------------------|\n| `SimpleTokenizer` | Raw text string | List of token IDs | Convert text to integer sequences |\n| Token Embedding | Token ID integers | Dense vectors (d_model) | Map discrete tokens to continuous space |\n| Positional Encoding | Sequence position | Position vectors (d_model) | Inject sequence order information |\n\nThe tokenizer serves as the bridge between the human-readable text domain and the numerical computation domain. Character-level tokenization, while simpler to implement, creates longer sequences but ensures no out-of-vocabulary issues. The token embeddings transform these discrete identifiers into dense `d_model`-dimensional vectors that can encode rich semantic information through gradient-based learning.\n\nPositional encoding addresses a fundamental limitation of the attention mechanism - unlike recurrent networks, attention has no inherent notion of sequence order. Without positional information, the transformer would treat \"The cat sat on the mat\" identically to \"Mat the on sat cat the.\" The positional encoding adds learned or computed position-dependent vectors to the token embeddings, allowing the model to distinguish between tokens based on their sequence positions.\n\n#### Representation Learning Layer\n\nThe representation learning layer contains the core transformer blocks that iteratively refine token representations through self-attention and feed-forward processing. This is where the model learns complex patterns and relationships in the data.\n\n| Component | Input Shape | Output Shape | Key Operations |\n|-----------|-------------|--------------|----------------|\n| `MultiHeadAttention` | (batch, seq_len, d_model) | (batch, seq_len, d_model) | Compute attention weights, aggregate values |\n| Feed-Forward Network | (batch, seq_len, d_model) | (batch, seq_len, d_model) | Non-linear transformation with 4x expansion |\n| Layer Normalization | (batch, seq_len, d_model) | (batch, seq_len, d_model) | Normalize activations per token |\n| Residual Connections | Input + Sublayer Output | (batch, seq_len, d_model) | Preserve gradient flow |\n\nEach transformer block applies the same architectural pattern: multi-head self-attention followed by a feed-forward network, with layer normalization and residual connections around each sublayer. This design creates a powerful inductive bias for learning hierarchical representations while maintaining stable gradient flow during training.\n\nThe multi-head attention mechanism allows each token to selectively focus on relevant parts of the sequence, learning different types of relationships in parallel across multiple attention heads. Some heads might learn syntactic relationships (subject-verb agreement), while others capture semantic associations (pronoun-antecedent relationships) or long-range dependencies (opening and closing of parenthetical statements).\n\n#### Generation Layer\n\nThe generation layer converts the final transformer block outputs into probability distributions over the vocabulary and implements sampling strategies for autoregressive text generation.\n\n| Component | Input Shape | Output Shape | Purpose |\n|-----------|-------------|--------------|---------|\n| Output Projection | (batch, seq_len, d_model) | (batch, seq_len, vocab_size) | Map representations to vocabulary logits |\n| Sampling Strategy | Vocabulary logits | Next token ID | Select next token using various strategies |\n| Generation Loop | Prompt tokens | Generated sequence | Orchestrate autoregressive generation |\n\nThe output projection typically shares weights with the input token embedding matrix, creating a tied embedding structure that reduces parameters and often improves performance. The projection produces logits (unnormalized log probabilities) for each vocabulary token, which are then processed by sampling strategies to select the next token.\n\nThe generation layer implements multiple sampling strategies to balance coherence and creativity. Greedy decoding always selects the most probable token, producing deterministic but potentially repetitive output. Temperature-based sampling introduces controlled randomness, while top-k and top-p (nucleus) sampling restrict the candidate set to maintain quality while preserving diversity.\n\n#### System Integration Patterns\n\nThe components interact through well-defined interfaces that maintain tensor shape consistency and enable efficient computation. Understanding these interaction patterns is essential for debugging and extending the system.\n\n> **Design Insight**: The transformer's power comes from the combination of three key architectural innovations: (1) self-attention for flexible sequence modeling, (2) residual connections for deep network training, and (3) layer normalization for stable optimization. Each component is necessary but not sufficient - their combination creates emergent capabilities.\n\n**Decision: Decoder-Only Architecture**\n- **Context**: We need to choose between encoder-decoder, encoder-only, or decoder-only transformer architectures for text generation\n- **Options Considered**: \n  - Encoder-decoder (like original Transformer): Separate encoding and decoding stacks\n  - Encoder-only (like BERT): Bidirectional attention for representation learning\n  - Decoder-only (like GPT): Unidirectional attention for autoregressive generation\n- **Decision**: Implement decoder-only architecture with causal masking\n- **Rationale**: Decoder-only architectures are simpler to implement, require fewer components, and have proven highly effective for text generation tasks. The causal masking naturally enforces the autoregressive constraint without additional architectural complexity.\n- **Consequences**: Enables straightforward text generation but cannot perform bidirectional tasks like fill-in-the-blank without modification. Simplifies implementation by eliminating encoder-decoder attention complexity.\n\n### Recommended Code Organization\n\nOrganizing transformer code requires balancing modularity, reusability, and clarity. The recommended structure separates concerns while maintaining clear dependencies and enabling incremental development through the milestones.\n\n#### Module Structure Overview\n\n```\ntransformer/\n├── __init__.py                 # Package initialization and public API\n├── config.py                   # Configuration dataclasses and constants\n├── tokenizer.py               # Text tokenization and vocabulary management\n├── embeddings.py              # Token and positional embeddings\n├── attention.py               # Self-attention mechanism implementation\n├── transformer.py             # Complete transformer model and blocks\n├── training.py                # Training loop, loss, and optimization\n├── generation.py              # Text generation and sampling strategies\n├── utils.py                   # Utility functions and debugging tools\n└── tests/                     # Unit tests for each component\n    ├── test_attention.py\n    ├── test_transformer.py\n    ├── test_training.py\n    └── test_generation.py\n```\n\nThis organization follows the **dependency layering principle** - lower-level modules (config, tokenizer, embeddings) have no dependencies on higher-level modules, while higher-level modules (training, generation) can import from all lower levels. This structure prevents circular dependencies and enables clean unit testing.\n\n#### Configuration Management Strategy\n\nAll model hyperparameters and training configuration should be centralized in `config.py` to enable easy experimentation and reproducible results. The configuration system should support both programmatic usage and command-line parameter specification.\n\n| Configuration Class | Responsibility | Key Parameters |\n|---------------------|----------------|----------------|\n| `TransformerConfig` | Model architecture parameters | `d_model`, `num_heads`, `seq_length`, `vocab_size` |\n| `TrainingConfig` | Training procedure parameters | Learning rate, batch size, epochs, gradient clipping |\n| `GenerationConfig` | Text generation parameters | Temperature, top_k, top_p, max_length |\n\nThe configuration system should validate parameter consistency (e.g., `d_model` must be divisible by `num_heads`) and provide sensible defaults for experimentation. Consider using dataclasses with type hints to enable automatic validation and IDE support.\n\n**Decision: Separate Configuration Classes**\n- **Context**: Need to organize the numerous hyperparameters and configuration options for the transformer system\n- **Options Considered**:\n  - Single monolithic config class with all parameters\n  - Separate classes for model, training, and generation configs\n  - Configuration file-based approach (YAML/JSON)\n- **Decision**: Use separate dataclasses for different configuration domains\n- **Rationale**: Separating concerns makes it easier to reuse model configs with different training configs, enables cleaner testing, and reduces the likelihood of accidentally modifying unrelated parameters during experiments\n- **Consequences**: Slightly more complex to pass multiple config objects but much cleaner organization and better maintainability\n\n#### Component Interface Design\n\nEach module should expose clean, minimal interfaces that hide implementation complexity while providing necessary flexibility for experimentation and debugging. The interface design should follow PyTorch conventions for neural network modules.\n\n| Module | Primary Classes | Key Methods | Public Interface Principle |\n|--------|-----------------|-------------|---------------------------|\n| `attention.py` | `MultiHeadAttention` | `forward(x, mask)` | Standard PyTorch nn.Module interface |\n| `transformer.py` | `TransformerBlock`, `GPTModel` | `forward(x)`, `generate(prompt)` | Composable building blocks |\n| `tokenizer.py` | `SimpleTokenizer` | `encode(text)`, `decode(tokens)` | Stateless text processing |\n| `training.py` | `Trainer` | `train_epoch(model, data)` | Encapsulate training logic |\n\nThe interface design should prioritize **composability** - each component should work independently and combine naturally with others. For example, the `MultiHeadAttention` module should not know about transformer blocks, and the `TransformerBlock` should not know about the complete model architecture.\n\n#### Debugging and Visualization Support\n\nInclude debugging utilities from the beginning rather than adding them retroactively. Debugging transformer implementations requires specialized tools for visualizing attention patterns, monitoring gradient flow, and analyzing generation quality.\n\n| Utility Function | Purpose | Usage Context |\n|------------------|---------|---------------|\n| `plot_attention_weights(weights, tokens, layer, head)` | Visualize attention patterns | Understanding learned relationships |\n| `count_parameters(model)` | Monitor model size | Architecture comparison |\n| `check_gradient_flow(model)` | Detect vanishing/exploding gradients | Training diagnosis |\n| `analyze_generation_quality(samples)` | Measure generation metrics | Generation evaluation |\n\nThese utilities should be designed for interactive use in Jupyter notebooks as well as programmatic use in training scripts. Consider providing both high-level convenience functions and lower-level access to intermediate computations for advanced debugging.\n\n#### Testing Strategy Integration\n\nThe code organization should facilitate comprehensive testing at multiple levels. Unit tests should cover individual components, integration tests should verify component interactions, and end-to-end tests should validate complete workflows.\n\n| Test Category | Scope | Example Test Cases |\n|---------------|-------|-------------------|\n| Unit Tests | Individual functions/classes | Attention weight computation, mask application |\n| Integration Tests | Component interactions | Transformer block forward/backward pass |\n| End-to-End Tests | Complete workflows | Training convergence, generation quality |\n| Property Tests | Invariant verification | Shape consistency, gradient computation |\n\nConsider using property-based testing for verifying mathematical invariants like attention weight normalization and tensor shape consistency across different batch sizes and sequence lengths. These tests can catch subtle bugs that are difficult to find with example-based testing.\n\n#### Performance Optimization Considerations\n\nWhile the primary goal is educational clarity, the code organization should not preclude future performance optimizations. Design interfaces that can accommodate optimizations like KV caching, gradient checkpointing, and mixed precision training.\n\n> **Design Principle**: Optimize for learning first, performance second. The code should be clear enough for educational purposes but structured to enable future optimizations without major refactoring.\n\n**Decision: PyTorch as Primary Framework**\n- **Context**: Need to choose a deep learning framework that balances educational clarity with practical utility\n- **Options Considered**:\n  - Pure NumPy implementation for maximum educational transparency\n  - PyTorch for its dynamic computation graphs and educational adoption\n  - TensorFlow/Keras for its high-level APIs and production readiness\n- **Decision**: Use PyTorch with clear separation between mathematical concepts and framework-specific details\n- **Rationale**: PyTorch's dynamic computation graphs make debugging easier, its nn.Module system provides clean abstractions for components, and it's widely adopted in educational settings. The framework handles automatic differentiation and GPU acceleration while remaining close to the mathematical formulation.\n- **Consequences**: Slightly steeper learning curve than pure NumPy but much more practical for experimentation and extension to larger models\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Tightly Coupled Components**\nMany implementations create circular dependencies between components, making testing and debugging extremely difficult. For example, embedding the tokenizer logic directly in the model class or having the attention mechanism know about the complete transformer architecture. This makes it impossible to test individual components in isolation and creates fragile code that breaks when any component changes. **Fix**: Design each component with a single responsibility and clean interfaces. The attention mechanism should only know about queries, keys, and values - not about transformer blocks or models.\n\n⚠️ **Pitfall: Configuration Parameter Scattering**\nHardcoding hyperparameters throughout the codebase or passing individual parameters to every function makes experimentation and debugging nearly impossible. When `d_model=512` is scattered across ten different files, changing the model size becomes an error-prone search-and-replace operation. **Fix**: Centralize all configuration in a single module and pass config objects to components. Use type hints and validation to catch configuration errors early.\n\n⚠️ **Pitfall: Inconsistent Tensor Shape Conventions**\nMixing different tensor shape conventions (batch-first vs sequence-first, or inconsistent dimension ordering) causes subtle bugs that are difficult to track down. These bugs often manifest as training instability or incorrect attention patterns rather than obvious errors. **Fix**: Establish consistent shape conventions from the beginning and document them clearly. Use shape assertions in forward passes during development to catch dimension mismatches immediately.\n\n⚠️ **Pitfall: Missing Intermediate Value Access**\nImplementing components as black boxes without access to intermediate computations makes debugging and analysis impossible. When attention isn't working correctly, you need to inspect attention weights, but if they're not accessible from outside the component, debugging becomes guesswork. **Fix**: Design components to optionally return intermediate values for debugging and visualization. Consider using hooks or debug modes that expose internal states.\n\n⚠️ **Pitfall: Inadequate Error Handling**\nTransformer implementations often fail silently when given invalid inputs, making it difficult to identify problems during development. For example, sequence lengths exceeding the model's maximum or attention masks with incorrect shapes might cause training instability rather than clear error messages. **Fix**: Add comprehensive input validation with clear error messages. Check tensor shapes, value ranges, and consistency constraints at component boundaries.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| Deep Learning Framework | PyTorch with nn.Module | PyTorch with custom CUDA kernels | PyTorch provides excellent educational clarity and debugging capabilities |\n| Tokenization | Character-level with Python dict | SentencePiece or Hugging Face tokenizers | Character-level is simpler to implement and debug for learning purposes |\n| Data Loading | Simple list-based batching | PyTorch DataLoader with custom collate | Start simple for understanding, upgrade for performance |\n| Visualization | Matplotlib for attention heatmaps | Weights & Biases or TensorBoard | Matplotlib provides immediate feedback during development |\n| Testing Framework | pytest with parametrize | pytest with hypothesis property testing | Parametrized tests cover multiple configurations efficiently |\n\n#### Recommended File Structure\n\n```\ntransformer_project/\n├── transformer/                # Main package\n│   ├── __init__.py\n│   ├── config.py              # TransformerConfig, TrainingConfig\n│   ├── tokenizer.py           # SimpleTokenizer implementation\n│   ├── embeddings.py          # Token and positional embeddings\n│   ├── attention.py           # MultiHeadAttention core logic\n│   ├── transformer.py         # TransformerBlock, GPTModel\n│   ├── training.py            # Trainer class, loss computation\n│   ├── generation.py          # Sampling strategies, generation loop\n│   └── utils.py               # count_parameters, visualization\n├── tests/                     # Comprehensive test suite\n│   ├── __init__.py\n│   ├── test_attention.py      # Attention mechanism tests\n│   ├── test_transformer.py    # Block and model tests\n│   ├── test_training.py       # Training pipeline tests\n│   ├── test_generation.py     # Generation quality tests\n│   └── conftest.py            # Shared test fixtures\n├── experiments/               # Training scripts and notebooks\n│   ├── train_model.py         # Main training script\n│   ├── generate_text.py       # Text generation script\n│   └── analysis.ipynb         # Interactive analysis notebook\n├── data/                      # Training data and outputs\n│   ├── text_data.txt          # Training corpus\n│   └── models/                # Saved model checkpoints\n├── requirements.txt           # Project dependencies\n└── README.md                  # Project documentation\n```\n\nThis structure supports incremental development - you can implement and test each component independently before integrating them into the complete system.\n\n#### Infrastructure Starter Code\n\n**Configuration Management** (config.py):\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\n\n@dataclass\nclass TransformerConfig:\n    \"\"\"Configuration for transformer model architecture.\"\"\"\n    d_model: int = 512              # Model embedding dimension\n    num_heads: int = 8              # Number of attention heads\n    num_layers: int = 6             # Number of transformer blocks\n    d_ff: int = 2048               # Feed-forward hidden dimension\n    seq_length: int = 1024         # Maximum sequence length\n    vocab_size: int = 50000        # Vocabulary size\n    dropout_rate: float = 0.1      # Dropout probability\n    \n    def __post_init__(self):\n        \"\"\"Validate configuration consistency.\"\"\"\n        if self.d_model % self.num_heads != 0:\n            raise ValueError(f\"d_model ({self.d_model}) must be divisible by num_heads ({self.num_heads})\")\n        \n        # Derived parameters\n        self.d_k = self.d_model // self.num_heads  # Key/query dimension per head\n        self.d_v = self.d_model // self.num_heads  # Value dimension per head\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Configuration for model training.\"\"\"\n    learning_rate: float = 1e-4\n    batch_size: int = 32\n    num_epochs: int = 10\n    gradient_clip_norm: float = 1.0\n    warmup_steps: int = 4000\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n@dataclass\nclass GenerationConfig:\n    \"\"\"Configuration for text generation.\"\"\"\n    temperature: float = 1.0\n    top_k: Optional[int] = 50\n    top_p: Optional[float] = 0.9\n    max_length: int = 100\n    do_sample: bool = True\n```\n\n**Utility Functions** (utils.py):\n```python\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List\n\ndef count_parameters(model: torch.nn.Module) -> int:\n    \"\"\"Count trainable parameters in model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_causal_mask(seq_length: int, device: torch.device) -> torch.Tensor:\n    \"\"\"Create lower triangular causal mask for autoregressive attention.\"\"\"\n    mask = torch.tril(torch.ones(seq_length, seq_length, device=device))\n    return mask.bool()\n\ndef plot_attention_weights(weights: torch.Tensor, tokens: List[str], \n                         layer: int, head: int, save_path: str = None):\n    \"\"\"Visualize attention weights as heatmap.\"\"\"\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(weights.detach().cpu().numpy(), \n                xticklabels=tokens, yticklabels=tokens,\n                cmap='Blues', cbar=True)\n    plt.title(f'Attention Weights - Layer {layer}, Head {head}')\n    plt.xlabel('Key Position')\n    plt.ylabel('Query Position')\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\ndef apply_causal_mask(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"Apply causal mask to attention scores before softmax.\"\"\"\n    return scores.masked_fill(~mask, float('-inf'))\n```\n\n#### Core Logic Skeleton Code\n\n**Multi-Head Attention** (attention.py):\n```python\nimport torch\nimport torch.nn as nn\nimport math\nfrom .config import TransformerConfig\nfrom .utils import create_causal_mask, apply_causal_mask\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head self-attention mechanism with causal masking.\"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        self.config = config\n        \n        # TODO 1: Initialize query, key, value projection matrices\n        # Hint: Use nn.Linear(d_model, d_model) for each projection\n        # TODO 2: Initialize output projection matrix\n        # TODO 3: Initialize dropout layer with config.dropout_rate\n        # TODO 4: Register causal mask as buffer (doesn't require gradients)\n        \n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Apply multi-head self-attention to input sequence.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, d_model)\n            mask: Optional attention mask\n            \n        Returns:\n            Output tensor of same shape as input\n        \"\"\"\n        batch_size, seq_length, d_model = x.shape\n        \n        # TODO 1: Project input to queries, keys, values using linear layers\n        # TODO 2: Reshape Q, K, V to (batch, num_heads, seq_len, d_k)\n        # Hint: Use .view() and .transpose() to rearrange dimensions\n        \n        # TODO 3: Compute scaled dot-product attention scores\n        # scores = Q @ K^T / sqrt(d_k)\n        \n        # TODO 4: Apply causal mask to prevent attending to future positions\n        # Use apply_causal_mask utility function\n        \n        # TODO 5: Apply softmax to get attention weights\n        # TODO 6: Apply dropout to attention weights (during training only)\n        \n        # TODO 7: Compute weighted sum of values\n        # attention_output = attention_weights @ V\n        \n        # TODO 8: Concatenate heads and apply output projection\n        # Reshape back to (batch, seq_len, d_model)\n        \n        pass  # Remove this when implementing\n```\n\n**Transformer Block** (transformer.py):\n```python\nimport torch\nimport torch.nn as nn\nfrom .attention import MultiHeadAttention\nfrom .config import TransformerConfig\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Single transformer block with attention and feed-forward layers.\"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        \n        # TODO 1: Initialize multi-head attention layer\n        # TODO 2: Initialize first layer normalization\n        # TODO 3: Initialize feed-forward network (two linear layers with ReLU)\n        # First layer: d_model -> d_ff, Second layer: d_ff -> d_model\n        # TODO 4: Initialize second layer normalization\n        # TODO 5: Initialize dropout layers\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply transformer block processing to input.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, d_model)\n            \n        Returns:\n            Output tensor of same shape\n        \"\"\"\n        # TODO 1: Apply first sub-layer (multi-head attention with residual connection)\n        # Use pre-normalization: LayerNorm -> Attention -> Dropout -> Residual\n        \n        # TODO 2: Apply second sub-layer (feed-forward with residual connection)\n        # Use pre-normalization: LayerNorm -> FFN -> Dropout -> Residual\n        \n        pass  # Remove this when implementing\n```\n\n#### Language-Specific Hints\n\n**PyTorch-Specific Implementation Tips:**\n- Use `torch.nn.functional.scaled_dot_product_attention` for optimized attention computation (PyTorch 2.0+)\n- Register masks and positional encodings as buffers with `self.register_buffer()` to automatically handle device placement\n- Use `torch.nn.utils.clip_grad_norm_()` for gradient clipping during training\n- Enable mixed precision training with `torch.cuda.amp.GradScaler` for faster training on modern GPUs\n- Use `model.eval()` vs `model.train()` to control dropout behavior during inference vs training\n\n**Memory and Performance Optimization:**\n- Use `torch.no_grad()` context manager during inference to reduce memory usage\n- Implement gradient checkpointing with `torch.utils.checkpoint.checkpoint()` for training larger models\n- Consider using `torch.compile()` (PyTorch 2.0+) for automatic optimization of the forward pass\n- Use appropriate data types: `torch.float16` for mixed precision, `torch.int64` for token IDs\n\n#### Milestone Checkpoints\n\n**After implementing attention mechanism (Milestone 1):**\n```bash\npython -m pytest tests/test_attention.py -v\npython -c \"\nfrom transformer.attention import MultiHeadAttention\nfrom transformer.config import TransformerConfig\nimport torch\n\nconfig = TransformerConfig(d_model=512, num_heads=8, seq_length=128)\nattention = MultiHeadAttention(config)\nx = torch.randn(2, 64, 512)  # batch_size=2, seq_length=64\noutput = attention(x)\nprint(f'Input shape: {x.shape}')\nprint(f'Output shape: {output.shape}')\nprint('✓ Attention mechanism working correctly')\n\"\n```\n\n**Expected behavior:** Attention layer should process input tensors without shape errors, produce output of identical shape to input, and generate reasonable attention weight patterns when visualized.\n\n**After implementing transformer block (Milestone 2):**\n```bash\npython -c \"\nfrom transformer.transformer import TransformerBlock\nfrom transformer.config import TransformerConfig\nimport torch\n\nconfig = TransformerConfig()\nblock = TransformerBlock(config)\nx = torch.randn(4, 128, 512)\noutput = block(x)\nprint(f'Parameters: {sum(p.numel() for p in block.parameters()):,}')\nprint(f'Output statistics: mean={output.mean():.3f}, std={output.std():.3f}')\nprint('✓ Transformer block implemented correctly')\n\"\n```\n\n**Expected behavior:** Transformer block should maintain reasonable output statistics (mean near 0, std around 1) due to layer normalization, and gradients should flow cleanly through residual connections.\n\n\n## Data Model and Types\n\n> **Milestone(s):** Foundation for all milestones - data structures and types that support attention mechanism (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)\n\nThe foundation of any transformer implementation lies in carefully designed data structures and type definitions that capture the mathematical relationships between embeddings, attention mechanisms, and generation parameters. Getting these definitions right from the start prevents subtle bugs and makes the codebase maintainable as we build increasingly complex components.\n\nThink of the data model as the architectural blueprint for a skyscraper - every beam, joint, and connection must be precisely specified before construction begins. Just as structural engineers define load-bearing capacities and material specifications, we must define tensor dimensions, configuration parameters, and their relationships to ensure our transformer can handle the computational loads of attention mechanisms and gradient flows.\n\nThe complexity of transformers stems from their multi-dimensional nature: sequences of tokens become matrices of embeddings, which are processed by multi-head attention across multiple layers. Each transformation must preserve certain dimensional invariants while allowing information to flow freely between positions. This requires a careful balance between flexibility for different model sizes and rigidity to prevent dimension mismatches that would cause runtime failures.\n\n> **Critical Insight:** The transformer's power comes from its ability to process sequences in parallel while maintaining positional relationships. This parallelism demands that all tensor dimensions are known at model construction time, making careful type design essential for both correctness and performance.\n\n### Tensor Dimensions\n\nThe transformer operates on multi-dimensional tensors with specific shape relationships that must be maintained throughout the forward pass. Understanding these dimensional constraints is crucial for implementing attention mechanisms and avoiding the most common source of transformer bugs: tensor shape mismatches.\n\n**Mental Model: The Assembly Line**\n\nImagine a transformer as a sophisticated assembly line where each workstation (layer) processes batches of products (token sequences) in parallel. Each product has specific dimensions - length, width, height - that determine which machines it can pass through. Just as an assembly line would break if we tried to force a 10-foot beam through a 5-foot opening, our transformer will crash if we try to multiply tensors with incompatible dimensions.\n\nThe key insight is that while the batch size and sequence length can vary dynamically, the embedding dimension (`d_model`) acts as a universal \"width\" that must match at every layer. This allows us to stack transformer blocks like standardized assembly line segments, each expecting inputs and producing outputs of the same embedding dimension.\n\nThe fundamental tensor shapes flow through the transformer in a predictable pattern:\n\n| Tensor Name | Shape | Description |\n|-------------|--------|-------------|\n| Input Tokens | `[batch_size, seq_length]` | Integer token IDs before embedding |\n| Token Embeddings | `[batch_size, seq_length, d_model]` | Dense vector representations of tokens |\n| Query Matrix | `[batch_size, seq_length, d_model]` | Attention queries for all positions |\n| Key Matrix | `[batch_size, seq_length, d_model]` | Attention keys for all positions |\n| Value Matrix | `[batch_size, seq_length, d_model]` | Attention values for all positions |\n| Attention Scores | `[batch_size, num_heads, seq_length, seq_length]` | Raw attention weights before masking |\n| Attention Weights | `[batch_size, num_heads, seq_length, seq_length]` | Normalized attention weights after softmax |\n| Head Output | `[batch_size, seq_length, d_k]` | Single attention head output |\n| Multi-Head Output | `[batch_size, seq_length, d_model]` | Concatenated and projected head outputs |\n| FFN Hidden | `[batch_size, seq_length, 4 * d_model]` | Feed-forward network intermediate layer |\n| Final Logits | `[batch_size, seq_length, vocab_size]` | Unnormalized probabilities over vocabulary |\n\nThe attention mechanism introduces additional dimensional complexity through multi-head computation. Each attention head operates on a smaller slice of the embedding dimension to allow parallel processing of different representation subspaces:\n\n| Dimension Name | Formula | Typical Value | Description |\n|----------------|---------|---------------|-------------|\n| `d_model` | Model hyperparameter | 512, 768, 1024 | Primary embedding dimension |\n| `d_k` | `d_model // num_heads` | 64, 96, 128 | Key/query dimension per head |\n| `d_v` | `d_model // num_heads` | 64, 96, 128 | Value dimension per head |\n| `num_heads` | Model hyperparameter | 8, 12, 16 | Number of parallel attention heads |\n| `seq_length` | Input dependent | 128, 512, 2048 | Maximum sequence length |\n| `vocab_size` | Dataset dependent | 10000, 30000, 50000 | Number of unique tokens |\n\n> **Architecture Decision: Equal Head Dimensions**\n> - **Context**: We need to divide the embedding dimension across multiple attention heads\n> - **Options Considered**: \n>   1. Equal dimensions per head (`d_k = d_v = d_model // num_heads`)\n>   2. Varying head dimensions with learned allocation\n>   3. Fixed small head dimensions independent of `d_model`\n> - **Decision**: Use equal dimensions per head with `d_k = d_v = d_model // num_heads`\n> - **Rationale**: This ensures the concatenated head outputs exactly reconstruct the original embedding dimension without requiring additional padding or projection. It also simplifies the math and aligns with the original Transformer paper.\n> - **Consequences**: Requires `d_model` to be divisible by `num_heads`, but provides clean dimensional relationships and efficient computation.\n\nThe dimensional relationships create important constraints that must be validated at model construction time:\n\n| Constraint | Mathematical Relationship | Validation Check |\n|------------|-------------------------|------------------|\n| Head Division | `d_model % num_heads == 0` | Embedding dimension evenly divisible |\n| Head Reconstruction | `num_heads * d_k == d_model` | Concatenated heads match embedding size |\n| Sequence Bounds | `input_length <= seq_length` | Input fits within position embeddings |\n| Vocabulary Bounds | `max(token_ids) < vocab_size` | All tokens have valid embeddings |\n| Batch Consistency | All sequences in batch have same length (after padding) | Efficient tensor operations |\n\nThe careful design of these dimensional relationships enables the transformer's key capability: processing variable-length sequences in fixed-size tensor operations. During training, sequences are padded to a common length within each batch, while during generation, we can process sequences of any length up to `seq_length` by simply slicing the appropriate tensor dimensions.\n\n### Configuration Structures\n\nConfiguration management in transformers requires balancing flexibility with type safety. We need structures that can express the full range of model architectures (from small experimental models to large production systems) while preventing invalid combinations of hyperparameters that would cause training failures or numerical instabilities.\n\n**Mental Model: The Recipe Card**\n\nThink of configuration structures as detailed recipe cards for baking a transformer. Just as a recipe specifies ingredients (model dimensions), cooking methods (training procedures), and serving suggestions (generation parameters), our configurations encode all the decisions needed to create a working model. Like a professional baker who maintains separate recipe cards for different occasions, we separate model architecture, training procedures, and generation behavior into distinct configuration structures.\n\nThe separation prevents common mistakes like accidentally using training-specific parameters during inference or mixing up model dimensions when switching between architectures. Each configuration structure has a single responsibility and clear validation rules.\n\n#### Model Architecture Configuration\n\nThe `TransformerConfig` structure captures all architectural decisions that determine the model's capacity and computational requirements. These parameters are typically set once during model design and remain fixed throughout training and deployment:\n\n| Field | Type | Description | Typical Values |\n|-------|------|-------------|----------------|\n| `d_model` | `int` | Primary embedding dimension for all layers | 512, 768, 1024, 1536 |\n| `num_heads` | `int` | Number of parallel attention heads per layer | 8, 12, 16, 24 |\n| `d_k` | `int` | Key and query dimension per attention head | 64, 96, 128 (computed as d_model // num_heads) |\n| `d_v` | `int` | Value dimension per attention head | 64, 96, 128 (typically equals d_k) |\n| `seq_length` | `int` | Maximum sequence length for positional embeddings | 128, 512, 1024, 2048 |\n| `vocab_size` | `int` | Size of token vocabulary for embeddings and output projection | 10000, 30000, 50000 |\n| `num_layers` | `int` | Number of transformer blocks in the stack | 6, 12, 24, 48 |\n| `dropout_rate` | `float` | Dropout probability for regularization (0.0 to 1.0) | 0.1, 0.2, 0.3 |\n| `ffn_expansion` | `int` | Multiplier for feed-forward network hidden dimension | 4 (gives 4 * d_model) |\n\nThe `TransformerConfig` includes built-in validation to catch common configuration errors before they cause cryptic runtime failures:\n\n| Validation Rule | Check | Error Message |\n|-----------------|-------|---------------|\n| Head Division | `d_model % num_heads == 0` | \"d_model must be divisible by num_heads\" |\n| Positive Dimensions | `d_model > 0 and num_heads > 0` | \"All dimensions must be positive\" |\n| Dropout Range | `0.0 <= dropout_rate <= 1.0` | \"Dropout rate must be between 0.0 and 1.0\" |\n| Sequence Length | `seq_length >= 1` | \"Sequence length must be at least 1\" |\n| Vocabulary Size | `vocab_size >= 2` | \"Vocabulary must contain at least 2 tokens\" |\n\n> **Architecture Decision: Computed vs Explicit Head Dimensions**\n> - **Context**: We could either compute `d_k` and `d_v` from `d_model` and `num_heads`, or require them to be specified explicitly\n> - **Options Considered**:\n>   1. Always compute `d_k = d_v = d_model // num_heads` automatically\n>   2. Require explicit specification but validate consistency\n>   3. Allow arbitrary head dimensions independent of `d_model`\n> - **Decision**: Require explicit specification with validation that `d_k * num_heads == d_model`\n> - **Rationale**: Makes the dimensional relationships explicit in configuration files, easier to debug when dimensions don't align, and allows for future flexibility if we want to experiment with varying head sizes\n> - **Consequences**: Slightly more verbose configuration files, but much clearer error messages when dimensions are mismatched\n\n#### Training Configuration\n\nThe `TrainingConfig` structure encapsulates all hyperparameters that control the optimization process. These parameters can be adjusted between training runs without changing the model architecture:\n\n| Field | Type | Description | Typical Values |\n|-------|------|-------------|----------------|\n| `learning_rate` | `float` | Initial learning rate for optimizer | 1e-3, 5e-4, 1e-4, 3e-5 |\n| `batch_size` | `int` | Number of sequences processed in parallel | 16, 32, 64, 128 |\n| `num_epochs` | `int` | Number of complete passes through training data | 10, 50, 100, 200 |\n| `gradient_clip_norm` | `float` | Maximum L2 norm for gradient clipping | 1.0, 0.5, 0.25 |\n| `weight_decay` | `float` | L2 regularization coefficient | 0.01, 0.001, 0.0001 |\n| `warmup_steps` | `int` | Number of steps for learning rate warmup | 1000, 4000, 10000 |\n| `save_interval` | `int` | Save checkpoint every N epochs | 5, 10, 25 |\n| `log_interval` | `int` | Log metrics every N training steps | 100, 500, 1000 |\n| `eval_interval` | `int` | Run validation every N training steps | 1000, 2500, 5000 |\n\nThe training configuration includes validation to prevent common training mistakes:\n\n| Validation Rule | Check | Error Message |\n|-----------------|-------|---------------|\n| Positive Learning Rate | `learning_rate > 0.0` | \"Learning rate must be positive\" |\n| Reasonable Batch Size | `1 <= batch_size <= 1024` | \"Batch size must be between 1 and 1024\" |\n| Gradient Clipping | `gradient_clip_norm > 0.0` | \"Gradient clip norm must be positive\" |\n| Weight Decay Range | `0.0 <= weight_decay <= 1.0` | \"Weight decay must be between 0.0 and 1.0\" |\n| Interval Consistency | `log_interval <= eval_interval` | \"Log interval should not exceed evaluation interval\" |\n\n#### Generation Configuration\n\nThe `GenerationConfig` structure controls the behavior of autoregressive text generation. These parameters can be adjusted at inference time to produce different styles of generated text:\n\n| Field | Type | Description | Typical Values |\n|-------|------|-------------|----------------|\n| `temperature` | `float` | Scaling factor for output logits (higher = more random) | 0.7, 1.0, 1.2, 1.5 |\n| `top_k` | `int` | Number of highest-probability tokens to consider | 0 (disabled), 10, 50, 100 |\n| `top_p` | `float` | Cumulative probability threshold for nucleus sampling | 0.0 (disabled), 0.9, 0.95, 0.99 |\n| `max_length` | `int` | Maximum number of tokens to generate | 50, 100, 200, 500 |\n| `pad_token_id` | `int` | Token ID used for padding sequences | 0, vocab_size - 1 |\n| `eos_token_id` | `int` | Token ID that signals end of sequence | 1, vocab_size - 2 |\n| `repetition_penalty` | `float` | Penalty applied to recently generated tokens | 1.0 (disabled), 1.1, 1.2 |\n| `length_penalty` | `float` | Penalty applied to longer sequences | 1.0 (disabled), 0.8, 1.2 |\n\nGeneration configuration validation focuses on preventing parameter combinations that would produce degenerate text:\n\n| Validation Rule | Check | Error Message |\n|-----------------|-------|---------------|\n| Temperature Range | `temperature > 0.0` | \"Temperature must be positive (use small values like 0.01 instead of 0.0)\" |\n| Top-k Bounds | `top_k >= 0` | \"Top-k must be non-negative (0 disables top-k sampling)\" |\n| Top-p Range | `0.0 <= top_p <= 1.0` | \"Top-p must be between 0.0 and 1.0\" |\n| Length Bounds | `max_length > 0 and max_length <= seq_length` | \"Max length must be positive and within model sequence limit\" |\n| Token ID Validity | `pad_token_id < vocab_size and eos_token_id < vocab_size` | \"Special token IDs must be within vocabulary\" |\n\n> **Architecture Decision: Temperature Zero Handling**\n> - **Context**: Setting temperature to 0.0 would cause division by zero in the softmax temperature scaling\n> - **Options Considered**:\n>   1. Allow temperature = 0.0 and implement special case for greedy decoding\n>   2. Require temperature > 0.0 and suggest small values like 0.01 for near-greedy behavior\n>   3. Automatically convert temperature = 0.0 to greedy decoding mode\n> - **Decision**: Require temperature > 0.0 with clear error message suggesting alternatives\n> - **Rationale**: Explicit is better than implicit - users should consciously choose between deterministic and probabilistic sampling rather than triggering special cases accidentally\n> - **Consequences**: Users must set temperature to small positive values like 0.01 for nearly deterministic generation, but this makes the sampling behavior explicit\n\n#### Configuration Relationships and Dependencies\n\nThe three configuration structures have important relationships that must be maintained for correct operation:\n\n| Relationship | Description | Validation |\n|--------------|-------------|------------|\n| Sequence Length Consistency | Training sequences must fit within model's maximum | `training_seq_len <= model.seq_length` |\n| Batch Size Memory Scaling | Larger batches with longer sequences require more GPU memory | `batch_size * seq_length^2 < memory_limit` (approximate) |\n| Generation Length Bounds | Generated sequences cannot exceed model capacity | `generation.max_length <= model.seq_length` |\n| Vocabulary Consistency | Special tokens must exist in model vocabulary | `generation.eos_token_id < model.vocab_size` |\n| Learning Rate Scaling | Larger batch sizes typically require higher learning rates | No automatic validation, but worth noting |\n\nThe configuration design supports common workflows like hyperparameter sweeps and model scaling:\n\n1. **Hyperparameter Sweeps**: Training configuration can be varied independently while keeping model architecture fixed\n2. **Model Scaling**: Architecture configuration supports scaling from small experimental models to large production systems\n3. **Generation Tuning**: Generation parameters can be adjusted at inference time without reloading the model\n4. **Distributed Training**: Batch size and learning rate can be scaled together for multi-GPU training\n\n### Implementation Guidance\n\nThe data model implementation requires careful attention to type safety, validation, and integration with deep learning frameworks. We'll use Python with PyTorch as our primary implementation language, leveraging dataclasses for clean configuration management and PyTorch's tensor operations for efficient computation.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Configuration Management | Python dataclasses with basic validation | Pydantic with automatic validation and JSON schema |\n| Type Hints | Basic Python typing | MyPy with strict mode for compile-time checking |\n| Tensor Operations | PyTorch tensors with manual shape checking | TorchScript for optimized inference |\n| Configuration Files | JSON or YAML with manual parsing | Hydra for hierarchical configuration management |\n| Parameter Counting | Simple loop over model.parameters() | PyTorch Lightning for automatic logging |\n\n#### Recommended File Structure\n\n```\ntransformer/\n├── config/\n│   ├── __init__.py\n│   ├── model_config.py      ← TransformerConfig definition\n│   ├── training_config.py   ← TrainingConfig definition\n│   ├── generation_config.py ← GenerationConfig definition\n│   └── validation.py        ← Configuration validation utilities\n├── data/\n│   ├── __init__.py\n│   ├── tokenizer.py        ← SimpleTokenizer implementation\n│   └── dataset.py          ← TextDataset implementation\n├── models/\n│   ├── __init__.py\n│   ├── attention.py        ← MultiHeadAttention implementation\n│   ├── transformer.py     ← Main transformer model\n│   └── utils.py           ← count_parameters, create_causal_mask\n└── examples/\n    ├── small_model.json    ← Example configurations\n    ├── medium_model.json\n    └── large_model.json\n```\n\n#### Configuration Infrastructure (Complete Implementation)\n\n```python\n# config/model_config.py\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport json\n\n@dataclass\nclass TransformerConfig:\n    \"\"\"Configuration for transformer model architecture.\n    \n    All architectural parameters that determine model capacity and\n    computational requirements. These are typically set once and\n    remain fixed throughout training and deployment.\n    \"\"\"\n    d_model: int = 512           # Primary embedding dimension\n    num_heads: int = 8           # Number of attention heads\n    d_k: int = 64               # Key/query dimension per head  \n    d_v: int = 64               # Value dimension per head\n    seq_length: int = 512        # Maximum sequence length\n    vocab_size: int = 10000      # Token vocabulary size\n    num_layers: int = 6          # Number of transformer blocks\n    dropout_rate: float = 0.1    # Dropout probability\n    ffn_expansion: int = 4       # FFN hidden dim multiplier\n    \n    def __post_init__(self):\n        \"\"\"Validate configuration parameters after initialization.\"\"\"\n        self.validate()\n    \n    def validate(self) -> None:\n        \"\"\"Validate configuration parameters for consistency.\"\"\"\n        if self.d_model <= 0:\n            raise ValueError(\"d_model must be positive\")\n        if self.num_heads <= 0:\n            raise ValueError(\"num_heads must be positive\") \n        if self.d_model % self.num_heads != 0:\n            raise ValueError(\"d_model must be divisible by num_heads\")\n        if self.d_k != self.d_model // self.num_heads:\n            raise ValueError(f\"d_k must equal d_model // num_heads = {self.d_model // self.num_heads}\")\n        if self.d_v != self.d_model // self.num_heads:\n            raise ValueError(f\"d_v must equal d_model // num_heads = {self.d_model // self.num_heads}\")\n        if self.seq_length <= 0:\n            raise ValueError(\"seq_length must be positive\")\n        if self.vocab_size < 2:\n            raise ValueError(\"vocab_size must be at least 2\")\n        if not (0.0 <= self.dropout_rate <= 1.0):\n            raise ValueError(\"dropout_rate must be between 0.0 and 1.0\")\n        if self.ffn_expansion <= 0:\n            raise ValueError(\"ffn_expansion must be positive\")\n    \n    @classmethod\n    def from_json(cls, json_path: str) -> 'TransformerConfig':\n        \"\"\"Load configuration from JSON file.\"\"\"\n        with open(json_path, 'r') as f:\n            config_dict = json.load(f)\n        return cls(**config_dict)\n    \n    def to_json(self, json_path: str) -> None:\n        \"\"\"Save configuration to JSON file.\"\"\"\n        with open(json_path, 'w') as f:\n            json.dump(self.__dict__, f, indent=2)\n\n@dataclass \nclass TrainingConfig:\n    \"\"\"Configuration for training hyperparameters.\"\"\"\n    learning_rate: float = 1e-4\n    batch_size: int = 32\n    num_epochs: int = 100\n    gradient_clip_norm: float = 1.0\n    weight_decay: float = 0.01\n    warmup_steps: int = 1000\n    save_interval: int = 10\n    log_interval: int = 100\n    eval_interval: int = 1000\n    \n    def __post_init__(self):\n        self.validate()\n        \n    def validate(self) -> None:\n        \"\"\"Validate training configuration parameters.\"\"\"\n        if self.learning_rate <= 0.0:\n            raise ValueError(\"learning_rate must be positive\")\n        if not (1 <= self.batch_size <= 1024):\n            raise ValueError(\"batch_size must be between 1 and 1024\")\n        if self.num_epochs <= 0:\n            raise ValueError(\"num_epochs must be positive\")\n        if self.gradient_clip_norm <= 0.0:\n            raise ValueError(\"gradient_clip_norm must be positive\")\n        if not (0.0 <= self.weight_decay <= 1.0):\n            raise ValueError(\"weight_decay must be between 0.0 and 1.0\")\n\n@dataclass\nclass GenerationConfig:\n    \"\"\"Configuration for text generation parameters.\"\"\" \n    temperature: float = 1.0\n    top_k: int = 0              # 0 disables top-k\n    top_p: float = 0.0          # 0.0 disables top-p  \n    max_length: int = 100\n    pad_token_id: int = 0\n    eos_token_id: int = 1\n    repetition_penalty: float = 1.0\n    length_penalty: float = 1.0\n    \n    def __post_init__(self):\n        self.validate()\n        \n    def validate(self) -> None:\n        \"\"\"Validate generation configuration parameters.\"\"\"\n        if self.temperature <= 0.0:\n            raise ValueError(\"temperature must be positive (use small values like 0.01 instead of 0.0)\")\n        if self.top_k < 0:\n            raise ValueError(\"top_k must be non-negative (0 disables top-k sampling)\")\n        if not (0.0 <= self.top_p <= 1.0):\n            raise ValueError(\"top_p must be between 0.0 and 1.0\")\n        if self.max_length <= 0:\n            raise ValueError(\"max_length must be positive\")\n```\n\n#### Tensor Utilities (Complete Implementation)\n\n```python\n# models/utils.py  \nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef create_causal_mask(seq_length: int, device: torch.device) -> torch.Tensor:\n    \"\"\"Create lower triangular causal mask for autoregressive attention.\n    \n    Args:\n        seq_length: Length of the sequence\n        device: Device to create tensor on\n        \n    Returns:\n        Boolean mask of shape [seq_length, seq_length] where True indicates\n        positions that should be masked (set to -inf in attention scores)\n    \"\"\"\n    # Create upper triangular matrix (above diagonal = True = masked)\n    mask = torch.triu(torch.ones(seq_length, seq_length, dtype=torch.bool, device=device), diagonal=1)\n    return mask\n\ndef apply_causal_mask(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"Apply causal mask to attention scores.\n    \n    Args:\n        scores: Attention scores of shape [batch_size, num_heads, seq_len, seq_len]\n        mask: Boolean mask of shape [seq_len, seq_len]\n        \n    Returns:\n        Masked scores with -inf at masked positions\n    \"\"\"\n    # Use large negative value instead of -inf to avoid numerical issues\n    masked_scores = scores.masked_fill(mask, -1e9)\n    return masked_scores\n\ndef count_parameters(model: nn.Module) -> int:\n    \"\"\"Count the total number of trainable parameters in a model.\n    \n    Args:\n        model: PyTorch model\n        \n    Returns:\n        Total number of trainable parameters\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef plot_attention_weights(weights: torch.Tensor, tokens: list, layer: int, head: int) -> None:\n    \"\"\"Visualize attention weights as a heatmap.\n    \n    Args:\n        weights: Attention weights of shape [batch_size, num_heads, seq_len, seq_len]\n        tokens: List of token strings for axis labels\n        layer: Layer index for title\n        head: Head index for title\n    \"\"\"\n    # Extract single head weights [seq_len, seq_len]\n    head_weights = weights[0, head].detach().cpu().numpy()\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(head_weights, \n                xticklabels=tokens, \n                yticklabels=tokens,\n                cmap='Blues',\n                cbar=True)\n    plt.title(f'Attention Weights - Layer {layer}, Head {head}')\n    plt.xlabel('Key Positions') \n    plt.ylabel('Query Positions')\n    plt.show()\n```\n\n#### Core Configuration Skeletons\n\n```python\n# config/validation.py\nfrom typing import Tuple, List, Dict, Any\nfrom .model_config import TransformerConfig\nfrom .training_config import TrainingConfig  \nfrom .generation_config import GenerationConfig\n\ndef validate_config_compatibility(model_config: TransformerConfig, \n                                training_config: TrainingConfig,\n                                generation_config: GenerationConfig) -> List[str]:\n    \"\"\"Validate compatibility between different configuration structures.\n    \n    Returns:\n        List of validation error messages (empty if all valid)\n    \"\"\"\n    errors = []\n    \n    # TODO 1: Check that generation max_length doesn't exceed model seq_length\n    # TODO 2: Verify that special token IDs are within model vocabulary\n    # TODO 3: Validate memory requirements don't exceed reasonable limits\n    # TODO 4: Check that batch_size and seq_length combination is feasible\n    # TODO 5: Warn about learning rate scaling for large batch sizes\n    \n    return errors\n\ndef estimate_memory_usage(config: TransformerConfig, batch_size: int) -> Dict[str, float]:\n    \"\"\"Estimate GPU memory usage in MB for given configuration.\n    \n    Returns:\n        Dictionary with memory breakdown by component\n    \"\"\"\n    # TODO 1: Calculate embedding table memory: vocab_size * d_model * 4 bytes\n    # TODO 2: Calculate attention memory: batch * heads * seq_len^2 * 4 bytes  \n    # TODO 3: Calculate activation memory: batch * seq_len * d_model * layers * 4 bytes\n    # TODO 4: Calculate parameter memory: count_parameters(model) * 4 bytes\n    # TODO 5: Add optimizer state memory (2x parameters for Adam)\n    \n    return {\n        'embeddings_mb': 0.0,\n        'attention_mb': 0.0, \n        'activations_mb': 0.0,\n        'parameters_mb': 0.0,\n        'optimizer_mb': 0.0,\n        'total_mb': 0.0\n    }\n```\n\n#### Configuration Examples\n\n```json\n// examples/small_model.json\n{\n  \"d_model\": 256,\n  \"num_heads\": 4,\n  \"d_k\": 64,\n  \"d_v\": 64,\n  \"seq_length\": 128,\n  \"vocab_size\": 5000,\n  \"num_layers\": 4,\n  \"dropout_rate\": 0.1,\n  \"ffn_expansion\": 4\n}\n\n// examples/medium_model.json  \n{\n  \"d_model\": 512,\n  \"num_heads\": 8,\n  \"d_k\": 64,\n  \"d_v\": 64,\n  \"seq_length\": 512,\n  \"vocab_size\": 10000,\n  \"num_layers\": 6,\n  \"dropout_rate\": 0.1,\n  \"ffn_expansion\": 4\n}\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the data model structures, verify the following behavior:\n\n**Test Configuration Loading:**\n```python\n# Should successfully load and validate\nconfig = TransformerConfig.from_json('examples/small_model.json')\nprint(f\"Model has {config.d_model} embedding dimension\")\nprint(f\"Each head processes {config.d_k} dimensions\")\n```\n\n**Test Configuration Validation:**\n```python\n# Should raise ValueError about head division\ntry:\n    bad_config = TransformerConfig(d_model=100, num_heads=8, d_k=12, d_v=12)\nexcept ValueError as e:\n    print(f\"Caught expected error: {e}\")\n```\n\n**Test Tensor Utilities:**\n```python \n# Should create proper triangular mask\nmask = create_causal_mask(4, torch.device('cpu'))\nprint(\"Causal mask (True = masked):\")\nprint(mask)\n# Expected output:\n# [[False, True,  True,  True ],\n#  [False, False, True,  True ],  \n#  [False, False, False, True ],\n#  [False, False, False, False]]\n```\n\nSigns that something is wrong:\n- Configuration validation doesn't catch obvious errors (like negative dimensions)\n- Causal mask has True values below the diagonal  \n- `count_parameters` returns 0 for a model with weights\n- JSON loading/saving fails with basic configuration files\n\nThe next step is implementing the self-attention mechanism using these data structures as the foundation for tensor operations.\n\n![Tensor Shapes and Data Types](./diagrams/data-model.svg)\n\n\n## Self-Attention Mechanism\n\n> **Milestone(s):** Milestone 1 - Self-Attention (implementing scaled dot-product attention, multi-head attention, and causal masking)\n\nThe self-attention mechanism represents the revolutionary core of transformer architecture, fundamentally changing how neural networks process sequential data. Unlike recurrent neural networks that process tokens sequentially, self-attention allows every token in a sequence to directly interact with every other token in parallel. This section implements the complete self-attention system, including the scaled dot-product attention computation, multi-head attention with parallel processing heads, and causal masking that enables autoregressive text generation.\n\n![Multi-Head Attention Architecture](./diagrams/attention-mechanism.svg)\n\nUnderstanding self-attention requires building intuition about how relevance-based information aggregation works in practice. We'll start with mental models that make the mathematical formulation intuitive, then dive deep into each component's implementation details, architectural decisions, and common pitfalls that derail many transformer implementations.\n\n### Mental Model: Selective Focus\n\nThink of self-attention as a sophisticated **cocktail party listener** who can dynamically focus on different conversations simultaneously. Imagine you're at a crowded party where multiple conversations happen around you. Your brain naturally focuses more attention on certain speakers based on context, relevance, and your current interests. When someone mentions your name, you instantly shift more attention to that conversation. When discussing a technical topic, you might simultaneously listen to multiple related discussions and synthesize information from all of them.\n\nSelf-attention works similarly with tokens in a text sequence. Each token (like you at the party) needs to decide how much attention to pay to every other token (like each speaker) in the sequence. The attention mechanism computes **relevance scores** between tokens, then uses these scores to create a weighted summary of information from all positions.\n\nConsider the sentence: \"The cat sat on the mat because it was comfortable.\" When processing the word \"it,\" the attention mechanism should focus heavily on \"cat\" (high attention weight) while paying minimal attention to words like \"the\" or \"on\" (low attention weights). This relevance-based focusing allows the model to capture long-range dependencies and resolve ambiguities that would be impossible with purely local context windows.\n\nThe mathematical beauty of self-attention lies in how it formalizes this intuitive process. Each token creates three representations of itself:\n- **Query (Q)**: \"What am I looking for?\" - represents what information this token needs\n- **Key (K)**: \"What information do I offer?\" - represents what this token can provide to others  \n- **Value (V)**: \"Here's my actual content\" - represents the information this token contributes\n\nThe attention computation measures compatibility between queries and keys (relevance), then uses these scores to weight the values (content aggregation). This creates a context-aware representation where each token incorporates relevant information from across the entire sequence.\n\n### Scaled Dot-Product Attention\n\nThe scaled dot-product attention mechanism forms the mathematical foundation of transformer models. This computation takes three matrices - queries, keys, and values - and produces attention-weighted outputs through a carefully designed sequence of linear algebra operations.\n\n**Attention Computation Formula:**\nThe core computation follows this mathematical sequence:\n1. Compute compatibility scores between queries and keys using matrix multiplication\n2. Scale the scores by the square root of the key dimension to maintain stable gradients\n3. Apply softmax normalization to convert scores into probability distributions\n4. Use attention weights to compute weighted averages of value vectors\n\nThe mathematical formulation captures this as: `Attention(Q, K, V) = softmax(Q @ K.T / sqrt(d_k)) @ V`\n\nLet's examine each step in detail with concrete tensor operations:\n\n**Step 1: Score Computation**\nThe raw attention scores measure compatibility between each query vector and every key vector. Given input sequences of length `seq_length` with embedding dimension `d_model`, the query matrix Q and key matrix K both have shape `[batch_size, seq_length, d_k]`. The score computation `Q @ K.T` produces a matrix of shape `[batch_size, seq_length, seq_length]` where entry (i,j) represents how much token i should attend to token j.\n\nThis compatibility scoring captures semantic relationships between positions. Words with similar meanings or strong contextual relationships produce higher dot-product scores, while unrelated tokens produce scores closer to zero. The dot-product operation efficiently computes these pairwise interactions in parallel across all positions.\n\n**Step 2: Score Scaling**\nThe scaling factor `1/sqrt(d_k)` prevents attention scores from growing too large as the model dimension increases. Without scaling, dot-products between high-dimensional vectors tend to produce extreme values that push the softmax function into saturation regions where gradients vanish. The square root scaling maintains reasonable score magnitudes regardless of dimension.\n\nThis scaling decision represents a critical stability consideration. Large unscaled scores cause the softmax function to produce near-one attention weights for the highest-scoring position and near-zero weights everywhere else. Such extreme attention distributions eliminate the model's ability to attend to multiple relevant positions simultaneously and create gradient flow problems during training.\n\n**Step 3: Softmax Normalization**\nThe softmax operation converts raw scores into a probability distribution over positions. For each query position, the softmax ensures attention weights sum to 1.0 while maintaining relative ordering from the raw scores. This normalization step is crucial for interpreting attention weights as relevance probabilities.\n\nThe softmax computation `exp(score_i) / sum(exp(score_j))` amplifies differences between scores while ensuring valid probability distributions. Higher-scoring positions receive proportionally more attention weight, but the probabilistic interpretation allows the model to attend to multiple positions when appropriate.\n\n**Step 4: Value Aggregation**\nThe final matrix multiplication combines attention weights with value vectors to produce context-aware outputs. Each output position receives a weighted combination of value vectors from all positions, with weights determined by the attention distribution. This aggregation step creates representations that incorporate relevant information from across the sequence.\n\nThe value matrix V has shape `[batch_size, seq_length, d_v]`, and the attention weights have shape `[batch_size, seq_length, seq_length]`. The multiplication produces output vectors of shape `[batch_size, seq_length, d_v]`, where each output vector represents a context-aware encoding of its corresponding input position.\n\n> **Decision: Separate Key and Value Dimensions**\n> - **Context**: The original attention formulation could use the same dimension for keys and values, or allow them to differ\n> - **Options Considered**: \n>   1. Unified dimension where `d_k = d_v = d_model`\n>   2. Separate dimensions where `d_k` optimizes similarity computation and `d_v` optimizes content representation\n>   3. Fixed small dimensions regardless of model size\n> - **Decision**: Use separate configurable dimensions with typical values `d_k = d_v = d_model / num_heads`\n> - **Rationale**: Separate dimensions provide flexibility to optimize similarity computation (keys/queries) independently from content representation (values), while the per-head scaling maintains reasonable parameter counts\n> - **Consequences**: Adds configuration complexity but enables better parameter allocation between attention computation and content transformation\n\n![Attention Computation States](./diagrams/attention-states.svg)\n\n**Attention Score Interpretation**\nUnderstanding attention weights requires recognizing them as learned similarity measures. High attention weights between positions indicate the model has learned these positions provide mutually relevant information. The specific patterns depend on the task and training data, but common patterns emerge:\n\n| Attention Pattern | Description | Example Context |\n|------------------|-------------|-----------------|\n| Local Dependencies | High weights between adjacent tokens | Syntactic relationships, phrases |\n| Long-Range Dependencies | High weights across distant positions | Pronoun resolution, discourse coherence |\n| Content-Based Similarity | High weights between semantically similar tokens | Synonym relationships, topic coherence |\n| Positional Bias | Consistent attention to specific relative positions | Sentence beginnings, punctuation |\n\n### Multi-Head Attention\n\nMulti-head attention extends the single attention mechanism by running multiple attention computations in parallel, each with different learned projection matrices. This architectural choice allows the model to capture different types of relationships simultaneously - some heads might focus on syntactic relationships while others capture semantic similarities or positional patterns.\n\n**Parallel Attention Heads**\nEach attention head operates independently with its own query, key, and value projection matrices. Given input embeddings of dimension `d_model`, each head projects to smaller dimensions `d_k` and `d_v`, typically `d_model / num_heads`. This dimensionality reduction allows multiple heads to operate within the same computational budget as a single large head.\n\nThe parallel computation creates `num_heads` different attention patterns over the same input sequence. Head h computes its attention as: `head_h = Attention(X @ W_Q_h, X @ W_K_h, X @ W_V_h)` where `W_Q_h`, `W_K_h`, and `W_V_h` are the learned projection matrices specific to head h.\n\n**Head Specialization and Diversity**\nDifferent attention heads naturally specialize during training to capture complementary aspects of the input relationships. This emergent specialization provides several advantages:\n\nTraining dynamics naturally encourage head diversity through the model's objective to minimize loss. Heads that focus on redundant patterns provide less marginal benefit than heads discovering new relevant relationships. This creates pressure for each head to find unique useful patterns in the data.\n\nResearch has identified several common head specialization patterns that emerge across different transformer models and tasks. Some heads consistently focus on local syntactic relationships like part-of-speech patterns or phrase boundaries. Other heads specialize in long-range semantic relationships, connecting entities mentioned far apart in the text. Positional heads learn to attend to specific relative positions regardless of content, useful for tasks with strong structural patterns.\n\n**Head Dimension Scaling**\nThe choice of per-head dimensions involves balancing expressiveness against computational efficiency. Smaller head dimensions force each head to focus on the most important relationships, potentially improving interpretability and reducing overfitting. Larger head dimensions provide more representational capacity but may lead to redundant computations across heads.\n\n> **Decision: Per-Head Dimension Scaling**\n> - **Context**: Multi-head attention requires deciding how to allocate the total model dimension across attention heads\n> - **Options Considered**: \n>   1. Fixed small dimensions (e.g., d_k=64) regardless of model size\n>   2. Proportional scaling where `d_k = d_model / num_heads` \n>   3. Hybrid approach with minimum dimension limits\n> - **Decision**: Use proportional scaling with `d_k = d_v = d_model / num_heads`\n> - **Rationale**: Proportional scaling maintains constant total parameters while distributing capacity evenly across heads; ensures each head has sufficient capacity as model size grows\n> - **Consequences**: Larger models get more expressive individual heads, but very small models might have insufficient per-head capacity\n\n**Multi-Head Projection Matrices**\nEach attention head requires three projection matrices to transform input embeddings into queries, keys, and values. These matrices are learned parameters that allow each head to focus on different aspects of the input representation.\n\n| Projection Type | Matrix Shape | Purpose | Learning Objective |\n|----------------|---------------|---------|-------------------|\n| Query Projection (`W_Q_h`) | `[d_model, d_k]` | Transform input to \"what I'm looking for\" representation | Learn to identify information needs |\n| Key Projection (`W_K_h`) | `[d_model, d_k]` | Transform input to \"what I can provide\" representation | Learn to advertise available information |\n| Value Projection (`W_V_h`) | `[d_model, d_v]` | Transform input to \"actual content\" representation | Learn to extract and format useful content |\n\nThe projection matrices enable each head to create specialized views of the input data. A head focusing on syntactic relationships might learn projections that emphasize part-of-speech information, while a head capturing semantic similarity might project tokens into a space where synonyms cluster together.\n\n**Output Concatenation and Final Projection**\nAfter computing attention outputs from all heads, multi-head attention concatenates the results and applies a final linear projection. This concatenation combines the specialized representations from each head into a unified output that preserves information from all attention patterns.\n\nThe concatenation operation stacks head outputs along the feature dimension: if each head produces output shape `[batch_size, seq_length, d_v]`, concatenating `num_heads` heads yields shape `[batch_size, seq_length, num_heads * d_v]`. When `d_v = d_model / num_heads`, the concatenated output has the original `d_model` dimension.\n\nThe final output projection `W_O` has shape `[num_heads * d_v, d_model]` and serves several important purposes. It allows the model to learn how to combine information from different heads optimally. It can resolve potential conflicts when heads produce contradictory information. It provides a final transformation to match the residual connection requirements in the transformer block.\n\n**Multi-Head Implementation Strategy**\nEfficient multi-head attention implementation requires careful consideration of tensor operations and memory layout. Two main approaches exist for organizing the computation:\n\n**Sequential Head Processing** computes each head independently in a loop, concatenating results at the end. This approach is conceptually simple and matches the mathematical definition directly, but may not utilize modern GPU parallelism optimally.\n\n**Batched Head Processing** reshapes projection matrices to compute all heads simultaneously. The key insight is reorganizing dimensions so that head computation becomes an additional batch dimension that GPUs can parallelize naturally.\n\n| Implementation Approach | Memory Layout | Computational Efficiency | Code Complexity |\n|------------------------|---------------|-------------------------|-----------------|\n| Sequential Processing | Head-by-head computation | Moderate, limited parallelism | Low, matches math directly |\n| Batched Processing | All heads in single tensor | High, full GPU utilization | Moderate, requires reshaping |\n| Fused Operations | Combined QKV projection | Highest, minimal memory transfers | Higher, hardware-specific optimization |\n\n### Causal Attention Masking\n\nCausal attention masking represents a crucial architectural constraint that enables autoregressive text generation. The causal mask prevents tokens from attending to future positions in the sequence, ensuring that each position can only use information from previous and current positions when making predictions. This constraint transforms the general bidirectional attention mechanism into a unidirectional system suitable for language modeling.\n\n**The Causality Constraint**\nIn autoregressive language modeling, each token must predict the next token using only the tokens that have been generated so far. If tokens could attend to future positions during training, the model would learn to \"cheat\" by looking ahead at the targets, creating a mismatch between training and generation conditions where future tokens are unavailable.\n\nThe causal constraint requires that attention weights `A[i,j]` be zero whenever `j > i`, meaning position i cannot attend to any position j that comes after it in the sequence. This creates a lower triangular attention pattern where each position can attend to itself and all previous positions, but never to future positions.\n\nConsider processing the sequence \"The cat sat on\" during training. When computing representations for \"cat\" (position 1), the model can attend to \"The\" (position 0) and \"cat\" itself, but not to \"sat\" (position 2) or \"on\" (position 3). This constraint ensures the model learns to predict \"sat\" based only on the available context \"The cat\", matching the generation scenario where future tokens don't exist yet.\n\n**Mask Implementation Strategy**\nImplementing causal masking requires careful consideration of numerical stability and computational efficiency. The standard approach applies the mask to attention scores before the softmax operation, setting future positions to negative infinity so they receive zero attention weight after normalization.\n\nThe causal mask is a binary matrix of shape `[seq_length, seq_length]` where entry (i,j) is 1 if position i can attend to position j, and 0 otherwise. For causal attention, this creates a lower triangular matrix with ones on and below the diagonal, zeros above the diagonal.\n\nApplying the mask involves replacing masked positions with large negative values (typically -1e9 or -inf) before softmax. The softmax function transforms these extreme negative values into near-zero probabilities, effectively eliminating attention to future positions. Using negative infinity guarantees exactly zero attention weights, but may cause numerical issues in some implementations.\n\n**Mask Generation and Caching**\nGenerating causal masks efficiently requires understanding their structure and reusability properties. Since causal masks depend only on sequence length and not on actual content, they can be pre-computed and reused across different inputs and training steps.\n\nThe mask generation process creates a lower triangular matrix using efficient tensor operations. Most deep learning frameworks provide utilities like `triu` (upper triangular) that can be inverted to create lower triangular masks. Alternatively, comparing position indices generates the mask: `mask[i,j] = (j <= i)`.\n\nCaching causal masks provides significant computational savings since mask generation involves expensive tensor operations that would be repeated for every attention computation. A well-designed implementation caches masks for common sequence lengths and reuses them across batches and training steps.\n\n> **Decision: Mask Application Strategy**\n> - **Context**: Causal masks can be applied at different stages of attention computation with different numerical properties\n> - **Options Considered**: \n>   1. Pre-softmax masking with negative infinity replacement\n>   2. Pre-softmax masking with large negative finite values (-1e9)\n>   3. Post-softmax masking by directly zeroing attention weights\n> - **Decision**: Use pre-softmax masking with large negative finite values (-1e4 to -1e9)\n> - **Rationale**: Negative infinity can cause NaN gradients in some frameworks; large negative finite values achieve near-zero attention weights while maintaining numerical stability; post-softmax masking breaks the probability distribution property\n> - **Consequences**: Requires choosing appropriate negative value magnitude; maintains stable gradients; preserves softmax probability interpretation\n\n**Attention Pattern Analysis**\nUnderstanding the effect of causal masking on attention patterns provides insights into how autoregressive models process sequences. The mask creates a characteristic triangular attention pattern where early tokens have limited context (only previous positions) while later tokens can attend to increasingly larger context windows.\n\nThis triangular attention pattern has several important implications for model behavior. Tokens at the beginning of sequences must be predicted with minimal context, often leading to higher uncertainty and loss for early positions. Later tokens benefit from full left-context, typically achieving lower loss and higher confidence predictions.\n\nThe causal structure also influences how information propagates through the sequence. Information can only flow forward in the sequence, creating a directed acyclic graph of dependencies. This constrains the types of relationships the model can capture compared to bidirectional attention, but matches the fundamental constraint of text generation where future tokens are unavailable.\n\n| Position in Sequence | Available Context | Typical Prediction Difficulty | Information Flow |\n|---------------------|-------------------|------------------------------|------------------|\n| Early Positions | Very limited context | High difficulty, high loss | Receives little information |\n| Middle Positions | Moderate left context | Moderate difficulty | Accumulates forward-flowing information |\n| Later Positions | Full left context | Lower difficulty | Benefits from full sequence context |\n\n**Mask Broadcasting and Batching**\nImplementing causal masking in batched settings requires understanding tensor broadcasting and memory efficiency. The same causal mask applies to all sequences in a batch, so the mask should broadcast across the batch dimension to avoid memory duplication.\n\nThe causal mask has shape `[seq_length, seq_length]` but attention scores have shape `[batch_size, num_heads, seq_length, seq_length]`. Broadcasting rules allow the mask to expand automatically across batch and head dimensions when applied to attention scores. This broadcasting saves memory by storing only one mask copy regardless of batch size.\n\nAdvanced implementations may combine causal masking with padding masks for variable-length sequences, creating compound masks that handle both causality and sequence boundaries. These compound masks require careful logical operations to ensure both constraints are enforced simultaneously.\n\n### Common Pitfalls\n\nImplementing self-attention involves numerous subtle details where small mistakes can completely break the mechanism or cause silent failures that are difficult to debug. Understanding these common pitfalls helps avoid weeks of frustrating debugging.\n\n⚠️ **Pitfall: Incorrect Attention Score Scaling**\nMany implementations forget the scaling factor or apply it incorrectly. Without scaling by `1/sqrt(d_k)`, attention scores grow proportionally with the key dimension, pushing softmax into saturation regions where gradients vanish. This causes training to stall with very sharp attention distributions that prevent the model from learning to attend to multiple positions.\n\nThe scaling must be applied to the raw dot-product scores before softmax, not after. Scaling the attention weights after softmax changes their probabilistic interpretation and doesn't address the gradient saturation problem. The scaling factor must use the actual key dimension per head, not the full model dimension.\n\n**Symptoms**: Training loss plateaus early, attention weights are extremely peaked (one position gets >0.99 weight), gradients are very small or zero for query/key projections.\n**Fix**: Verify scaling factor is `1/sqrt(d_k)` where `d_k` is the per-head key dimension, applied to scores before softmax.\n\n⚠️ **Pitfall: Causal Mask Applied After Softmax**\nApplying the causal mask to attention weights after softmax breaks the probability distribution property and creates incorrect attention patterns. Post-softmax masking zeros out some attention weights but doesn't renormalize the remaining weights, so they no longer sum to 1.0.\n\nThe mask must be applied to raw attention scores before softmax. Setting future positions to large negative values (-1e9) ensures they receive near-zero probability after softmax normalization, while preserving the probability distribution over valid positions.\n\n**Symptoms**: Attention weights don't sum to 1.0, model generates text that seems to use future context during training, training/generation performance mismatch.\n**Fix**: Apply causal mask by setting `scores[mask == 0] = -1e9` before softmax, not by zeroing attention weights after softmax.\n\n⚠️ **Pitfall: Dimension Mismatches in Multi-Head Attention**\nMulti-head attention involves numerous tensor reshaping operations where dimension mismatches can cause subtle bugs. Common mistakes include using `d_model` instead of `d_k` for per-head dimensions, incorrect concatenation ordering, or wrong final projection dimensions.\n\nThe per-head dimensions must be `d_k = d_v = d_model / num_heads` for standard implementations. Query and key projections output `d_k` dimensions, value projections output `d_v` dimensions. After concatenating all heads, the final projection must map from `num_heads * d_v` back to `d_model`.\n\n**Symptoms**: Runtime shape errors, attention outputs have wrong dimensions, final projection fails, concatenated head outputs don't match expected dimensions.\n**Fix**: Carefully track tensor shapes through each operation, verify `d_k * num_heads = d_model`, ensure projection matrices have correct input/output dimensions.\n\n⚠️ **Pitfall: Incorrect Key Transpose in Score Computation**\nThe attention score computation requires transposing the key matrix: `scores = Q @ K.transpose()`. Forgetting the transpose or transposing the wrong matrix completely breaks the attention mechanism by computing meaningless dot products between incompatible dimensions.\n\nThe transpose must be applied to the last two dimensions of the key tensor to align query and key dimensions for dot product computation. For batched multi-head attention with shape `[batch_size, num_heads, seq_length, d_k]`, the transpose should swap the last two dimensions.\n\n**Symptoms**: Runtime shape errors during score computation, attention patterns that don't make sense, very poor model performance despite correct loss computation.\n**Fix**: Ensure score computation uses `Q @ K.transpose(-2, -1)` where the transpose swaps sequence length and key dimensions.\n\n⚠️ **Pitfall: Shared Projection Matrices Across Heads**\nUsing the same projection matrices for all attention heads eliminates the benefit of multi-head attention. Each head must have its own learned query, key, and value projections to capture different relationships.\n\nA common implementation mistake creates projection matrices of shape `[d_model, d_k]` and reuses them across heads, rather than creating separate matrices `[d_model, num_heads * d_k]` that can be split per head or using individual matrices per head.\n\n**Symptoms**: All attention heads produce identical patterns, model performance doesn't improve with more heads, attention visualizations show redundant head behaviors.\n**Fix**: Ensure each head has separate projection matrices, either as individual parameters or as slices of larger concatenated parameter matrices.\n\n⚠️ **Pitfall: Gradient Flow Problems from Extreme Attention Weights**\nVery sharp attention distributions can cause gradient flow problems where most positions receive zero gradients. This happens when attention scores are too large (insufficient scaling), when temperature is too low, or when causal masking uses inappropriate negative values.\n\nExtreme attention weights (one position >0.99, others <0.01) prevent the model from learning distributed representations and can cause training instability. The gradients for low-attention positions become negligible, effectively removing them from training.\n\n**Symptoms**: Very peaked attention distributions, some token embeddings don't update during training, unstable training loss, poor generalization to different sequence lengths.\n**Fix**: Verify proper score scaling, check causal mask values aren't too extreme (use -1e4 to -1e9), consider attention temperature or dropout for regularization.\n\n### Implementation Guidance\n\nThis section provides concrete code implementations and practical guidance for building the self-attention mechanism in PyTorch. The implementation balances clarity with efficiency, providing complete working code for supporting components and detailed skeleton code for the core attention logic.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Tensor Operations | PyTorch with basic operations | PyTorch with fused CUDA kernels |\n| Attention Implementation | Separate Q, K, V projections | Fused QKV projection for efficiency |\n| Masking Strategy | Boolean masks with broadcasting | In-place masking with memory views |\n| Multi-Head Processing | Sequential head computation | Batched reshape for parallel processing |\n| Numerical Precision | Float32 throughout | Mixed precision (float16 forward, float32 backward) |\n\n**B. Recommended File/Module Structure**\n```\ntransformer/\n├── attention/\n│   ├── __init__.py\n│   ├── multi_head_attention.py     ← Core MultiHeadAttention class\n│   ├── attention_utils.py          ← Mask creation and utility functions  \n│   └── attention_test.py           ← Unit tests for attention mechanisms\n├── config/\n│   ├── __init__.py\n│   └── model_config.py             ← TransformerConfig definition\n├── utils/\n│   ├── __init__.py\n│   └── tensor_utils.py             ← Shape checking and debugging utilities\n└── tests/\n    └── test_attention_integration.py ← End-to-end attention tests\n```\n\n**C. Infrastructure Starter Code (COMPLETE)**\n\n```python\n# attention/attention_utils.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import Optional, Tuple\n\ndef create_causal_mask(seq_length: int, device: torch.device) -> torch.Tensor:\n    \"\"\"\n    Creates a causal (lower triangular) attention mask.\n    \n    Args:\n        seq_length: Maximum sequence length\n        device: Device to create tensor on\n        \n    Returns:\n        Boolean mask of shape [seq_length, seq_length] where True indicates\n        positions that can be attended to (lower triangle including diagonal)\n    \"\"\"\n    mask = torch.tril(torch.ones(seq_length, seq_length, device=device))\n    return mask.bool()\n\ndef apply_causal_mask(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Applies causal mask to attention scores by setting masked positions to -1e9.\n    \n    Args:\n        scores: Attention scores of shape [batch_size, num_heads, seq_len, seq_len]\n        mask: Boolean mask of shape [seq_len, seq_len] where True = can attend\n        \n    Returns:\n        Masked attention scores with same shape as input\n    \"\"\"\n    # Invert mask so False positions (cannot attend) become True for masking\n    mask_value = -1e9\n    inverted_mask = ~mask\n    \n    # Broadcast mask across batch and head dimensions\n    scores = scores.masked_fill(inverted_mask, mask_value)\n    return scores\n\ndef count_parameters(model: nn.Module) -> int:\n    \"\"\"Count total trainable parameters in model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef plot_attention_weights(weights: torch.Tensor, \n                         tokens: list, \n                         layer: int, \n                         head: int,\n                         save_path: Optional[str] = None) -> None:\n    \"\"\"\n    Visualizes attention weights as a heatmap.\n    \n    Args:\n        weights: Attention weights [num_heads, seq_len, seq_len] or [seq_len, seq_len]\n        tokens: List of token strings for axis labels\n        layer: Layer index for title\n        head: Head index for title  \n        save_path: Optional path to save figure\n    \"\"\"\n    if weights.dim() == 3:\n        # Extract specific head\n        attn = weights[head].detach().cpu().numpy()\n    else:\n        attn = weights.detach().cpu().numpy()\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(attn, \n                xticklabels=tokens, \n                yticklabels=tokens,\n                cmap='Blues',\n                cbar_kws={'label': 'Attention Weight'})\n    \n    plt.title(f'Attention Weights - Layer {layer}, Head {head}')\n    plt.xlabel('Key Positions') \n    plt.ylabel('Query Positions')\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n\n# config/model_config.py\nfrom dataclasses import dataclass\n\n@dataclass\nclass TransformerConfig:\n    \"\"\"Configuration for transformer model architecture.\"\"\"\n    d_model: int = 512              # Model embedding dimension\n    num_heads: int = 8              # Number of parallel attention heads  \n    d_k: int = 64                   # Key/query dimension per head\n    d_v: int = 64                   # Value dimension per head\n    seq_length: int = 1024          # Maximum sequence length\n    vocab_size: int = 50257         # Size of token vocabulary\n    num_layers: int = 6             # Number of transformer blocks\n    dropout_rate: float = 0.1       # Probability for dropout regularization\n    ffn_expansion: int = 4          # Feed-forward network expansion ratio\n    \n    def __post_init__(self):\n        \"\"\"Validate configuration and set derived values.\"\"\"\n        # Ensure head dimensions divide evenly\n        assert self.d_model % self.num_heads == 0, \\\n            f\"d_model ({self.d_model}) must be divisible by num_heads ({self.num_heads})\"\n        \n        # Set per-head dimensions if not explicitly provided\n        if self.d_k is None:\n            self.d_k = self.d_model // self.num_heads\n        if self.d_v is None:\n            self.d_v = self.d_model // self.num_heads\n            \n        # Validate dimensions\n        assert self.d_k * self.num_heads <= self.d_model, \\\n            \"Total key dimension exceeds model dimension\"\n        assert self.d_v * self.num_heads <= self.d_model, \\\n            \"Total value dimension exceeds model dimension\"\n\n# utils/tensor_utils.py\nimport torch\nfrom typing import Tuple, List\n\ndef check_attention_shapes(queries: torch.Tensor,\n                          keys: torch.Tensor, \n                          values: torch.Tensor,\n                          expected_batch: int,\n                          expected_heads: int,\n                          expected_seq_len: int,\n                          expected_d_k: int,\n                          expected_d_v: int) -> None:\n    \"\"\"\n    Validates attention tensor shapes for debugging.\n    \n    Args:\n        queries: Query tensor\n        keys: Key tensor  \n        values: Value tensor\n        expected_*: Expected dimensions for validation\n        \n    Raises:\n        AssertionError: If any tensor has incorrect shape\n    \"\"\"\n    expected_q_shape = (expected_batch, expected_heads, expected_seq_len, expected_d_k)\n    expected_k_shape = (expected_batch, expected_heads, expected_seq_len, expected_d_k)  \n    expected_v_shape = (expected_batch, expected_heads, expected_seq_len, expected_d_v)\n    \n    assert queries.shape == expected_q_shape, \\\n        f\"Query shape {queries.shape} != expected {expected_q_shape}\"\n    assert keys.shape == expected_k_shape, \\\n        f\"Key shape {keys.shape} != expected {expected_k_shape}\"\n    assert values.shape == expected_v_shape, \\\n        f\"Value shape {values.shape} != expected {expected_v_shape}\"\n\ndef print_attention_debug_info(queries: torch.Tensor,\n                              keys: torch.Tensor,\n                              values: torch.Tensor,\n                              scores: torch.Tensor,\n                              attention_weights: torch.Tensor,\n                              output: torch.Tensor) -> None:\n    \"\"\"Prints detailed shape and statistics for attention debugging.\"\"\"\n    print(\"=== Attention Debug Info ===\")\n    print(f\"Queries: {queries.shape}, mean={queries.mean():.4f}, std={queries.std():.4f}\")\n    print(f\"Keys: {keys.shape}, mean={keys.mean():.4f}, std={keys.std():.4f}\")  \n    print(f\"Values: {values.shape}, mean={values.mean():.4f}, std={values.std():.4f}\")\n    print(f\"Scores: {scores.shape}, mean={scores.mean():.4f}, std={scores.std():.4f}\")\n    print(f\"Attention Weights: {attention_weights.shape}, mean={attention_weights.mean():.4f}\")\n    print(f\"Output: {output.shape}, mean={output.mean():.4f}, std={output.std():.4f}\")\n    \n    # Check for common issues\n    if torch.isnan(queries).any():\n        print(\"⚠️  WARNING: NaN values in queries\")\n    if torch.isnan(attention_weights).any():\n        print(\"⚠️  WARNING: NaN values in attention weights\")\n    if scores.max() > 50:\n        print(f\"⚠️  WARNING: Very large attention scores (max: {scores.max():.2f})\")\n    if attention_weights.max() > 0.99:\n        print(f\"⚠️  WARNING: Very peaked attention (max weight: {attention_weights.max():.4f})\")\n```\n\n**D. Core Logic Skeleton Code (Multi-Head Attention)**\n\n```python\n# attention/multi_head_attention.py\nimport torch\nimport torch.nn as nn\nimport math\nfrom typing import Optional, Tuple\nfrom ..config.model_config import TransformerConfig\nfrom .attention_utils import create_causal_mask, apply_causal_mask\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head self-attention mechanism with causal masking support.\n    \n    Implements parallel attention heads that capture different types of \n    relationships simultaneously, with optional causal masking for \n    autoregressive generation.\n    \"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        self.d_model = config.d_model\n        self.num_heads = config.num_heads\n        self.d_k = config.d_k\n        self.d_v = config.d_v\n        self.seq_length = config.seq_length\n        self.dropout_rate = config.dropout_rate\n        \n        # TODO 1: Create query projection matrix W_Q \n        # Shape: [d_model, num_heads * d_k]\n        # Hint: Use nn.Linear(d_model, num_heads * d_k, bias=False)\n        self.query_projection = None\n        \n        # TODO 2: Create key projection matrix W_K\n        # Shape: [d_model, num_heads * d_k] \n        # Hint: Same as query projection\n        self.key_projection = None\n        \n        # TODO 3: Create value projection matrix W_V\n        # Shape: [d_model, num_heads * d_v]\n        # Hint: Use nn.Linear(d_model, num_heads * d_v, bias=False)\n        self.value_projection = None\n        \n        # TODO 4: Create final output projection W_O\n        # Shape: [num_heads * d_v, d_model]\n        # Hint: Combines multi-head outputs back to d_model dimension\n        self.output_projection = None\n        \n        # TODO 5: Create dropout layer for attention weights\n        # Hint: Use nn.Dropout(dropout_rate)\n        self.attention_dropout = None\n        \n        # Register causal mask as buffer (not a learnable parameter)\n        # This creates the mask once and reuses it across forward passes\n        causal_mask = create_causal_mask(config.seq_length, torch.device('cpu'))\n        self.register_buffer('causal_mask', causal_mask)\n        \n        # Initialize projection weights using Xavier/Glorot initialization\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        \"\"\"Initialize projection matrices using Xavier initialization.\"\"\"\n        # TODO 6: Initialize all projection matrices with Xavier uniform\n        # Hint: Use nn.init.xavier_uniform_(module.weight) for each projection\n        pass\n    \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass through multi-head attention.\n        \n        Args:\n            x: Input embeddings [batch_size, seq_len, d_model]\n            mask: Optional attention mask [seq_len, seq_len]\n            \n        Returns:\n            Attention output [batch_size, seq_len, d_model]\n        \"\"\"\n        batch_size, seq_len, d_model = x.shape\n        \n        # TODO 7: Project input to queries, keys, values\n        # Apply query_projection, key_projection, value_projection to x\n        # Hint: queries = self.query_projection(x), etc.\n        queries = None  # Shape: [batch_size, seq_len, num_heads * d_k]\n        keys = None     # Shape: [batch_size, seq_len, num_heads * d_k]  \n        values = None   # Shape: [batch_size, seq_len, num_heads * d_v]\n        \n        # TODO 8: Reshape projections for multi-head processing\n        # Reshape from [batch_size, seq_len, num_heads * d_k] \n        # to [batch_size, num_heads, seq_len, d_k]\n        # Hint: Use .view() and .transpose() operations\n        queries = None  # Target: [batch_size, num_heads, seq_len, d_k]\n        keys = None     # Target: [batch_size, num_heads, seq_len, d_k]\n        values = None   # Target: [batch_size, num_heads, seq_len, d_v]\n        \n        # TODO 9: Compute scaled dot-product attention scores\n        # Formula: scores = queries @ keys.transpose(-2, -1) / sqrt(d_k)\n        # Hint: Use torch.matmul() and math.sqrt()\n        attention_scores = None  # Shape: [batch_size, num_heads, seq_len, seq_len]\n        \n        # TODO 10: Apply causal mask to attention scores\n        # Use the causal_mask buffer and apply_causal_mask utility\n        # Hint: Slice causal mask to current sequence length\n        if mask is None:\n            # Use default causal mask\n            current_mask = self.causal_mask[:seq_len, :seq_len]\n        else:\n            current_mask = mask\n        \n        # Apply mask to scores\n        masked_scores = None\n        \n        # TODO 11: Compute attention weights using softmax\n        # Apply softmax along the last dimension (key positions)\n        # Hint: Use torch.softmax(masked_scores, dim=-1)\n        attention_weights = None  # Shape: [batch_size, num_heads, seq_len, seq_len]\n        \n        # TODO 12: Apply dropout to attention weights\n        # Hint: Use self.attention_dropout during training\n        attention_weights = None\n        \n        # TODO 13: Compute attention output by weighting values\n        # Formula: output = attention_weights @ values\n        # Hint: Use torch.matmul()\n        attention_output = None  # Shape: [batch_size, num_heads, seq_len, d_v]\n        \n        # TODO 14: Concatenate multi-head outputs\n        # Reshape from [batch_size, num_heads, seq_len, d_v]\n        # to [batch_size, seq_len, num_heads * d_v]\n        # Hint: Use .transpose() and .contiguous().view()\n        concatenated_output = None\n        \n        # TODO 15: Apply final output projection\n        # Project from num_heads * d_v back to d_model\n        # Hint: Use self.output_projection()\n        final_output = None  # Shape: [batch_size, seq_len, d_model]\n        \n        return final_output\n    \n    def get_attention_weights(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Returns attention weights for visualization (without dropout).\n        \n        Args:\n            x: Input embeddings [batch_size, seq_len, d_model]\n            \n        Returns:\n            Attention weights [batch_size, num_heads, seq_len, seq_len]\n        \"\"\"\n        # TODO 16: Implement attention weight extraction for visualization\n        # Similar to forward() but return attention_weights before applying to values\n        # Don't apply dropout for visualization\n        with torch.no_grad():\n            # Implement same steps as forward() through attention weight computation\n            pass\n```\n\n**E. Language-Specific Hints**\n\n**PyTorch Tensor Operations:**\n- Use `torch.matmul()` or `@` operator for matrix multiplication\n- Use `tensor.transpose(-2, -1)` to swap last two dimensions for key transpose\n- Use `tensor.view()` for reshaping when memory layout allows, `.contiguous().view()` when needed\n- Use `F.softmax(tensor, dim=-1)` for softmax along last dimension\n- Use `tensor.masked_fill(mask, value)` for efficient masking\n\n**Memory and Performance:**\n- Register masks as buffers with `self.register_buffer()` to avoid recomputation\n- Use in-place operations like `tensor.masked_fill_()` when possible\n- Consider `torch.backends.cuda.enable_flash_sdp(True)` for optimized attention on compatible hardware\n- Use `torch.no_grad()` for visualization functions to save memory\n\n**Numerical Stability:**\n- Use `math.sqrt(self.d_k)` for scaling factor, not `self.d_k ** 0.5` \n- Set masked positions to -1e9, not -float('inf'), to avoid NaN gradients\n- Initialize projection matrices with Xavier initialization: `nn.init.xavier_uniform_()`\n- Consider gradient clipping if attention scores become very large\n\n**F. Milestone Checkpoint**\n\nAfter implementing the multi-head attention mechanism, verify correctness with these checkpoints:\n\n**Unit Tests to Run:**\n```bash\npython -m pytest tests/test_attention_integration.py -v\npython -c \"from attention.multi_head_attention import MultiHeadAttention; print('Import successful')\"\n```\n\n**Expected Behavior Verification:**\n1. **Shape Consistency**: Create a `MultiHeadAttention` layer with `d_model=512`, `num_heads=8`. Pass input tensor of shape `[2, 10, 512]`. Output should have shape `[2, 10, 512]`.\n\n2. **Causal Masking**: Generate attention weights for input sequence \"The cat sat\". Position 0 should only attend to position 0. Position 1 should attend to positions 0-1. Position 2 should attend to positions 0-2.\n\n3. **Attention Weight Properties**: Attention weights should sum to 1.0 along the last dimension (key positions). No NaN values should appear. Weights should be non-negative.\n\n4. **Multi-Head Diversity**: Different attention heads should produce different attention patterns. Visualize attention weights for multiple heads - they should not be identical.\n\n**Manual Verification Commands:**\n```python\nconfig = TransformerConfig(d_model=64, num_heads=4, seq_length=8)\nattention = MultiHeadAttention(config)\nx = torch.randn(1, 8, 64)  # [batch=1, seq_len=8, d_model=64]\noutput = attention(x)\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nassert output.shape == x.shape, \"Shape mismatch!\"\n\n# Check attention weights\nweights = attention.get_attention_weights(x)\nprint(f\"Attention weights shape: {weights.shape}\")  # Should be [1, 4, 8, 8]\nprint(f\"Weights sum along keys: {weights.sum(dim=-1)}\")  # Should be all 1.0\nprint(f\"Causal pattern check - pos 0 can only see pos 0: {weights[0, 0, 0, 1:].sum() < 1e-6}\")\n```\n\n**Signs of Problems and Debugging:**\n- **Runtime Errors**: Check tensor shapes at each step, especially after reshape operations\n- **NaN Values**: Usually caused by extreme attention scores; verify scaling factor and mask values\n- **Identical Heads**: Check that projection matrices are different across heads\n- **Non-Causal Attention**: Verify mask is applied before softmax and has correct triangular pattern\n- **Poor Performance**: May indicate initialization problems or incorrect gradient flow\n\n\n## Transformer Block Design\n\n> **Milestone(s):** Milestone 2 - Transformer Block (building complete transformer blocks with feed-forward networks, layer normalization, and residual connections)\n\nThe transformer block represents the fundamental computational unit that makes transformers so powerful for sequence modeling. While the self-attention mechanism from Milestone 1 allows tokens to communicate and share information, the complete transformer block creates a sophisticated **information refinement pipeline** that iteratively processes and enhances token representations through multiple complementary operations.\n\n### Mental Model: Information Refinement Pipeline\n\nThink of a transformer block like a **sophisticated document editing workflow** in a professional writing environment. Imagine you're working on an important document with a team of editors, each with different specializations:\n\n1. **The Communication Phase (Self-Attention)**: First, all team members read the entire document and discuss which parts are most relevant to each section. They share insights about how different paragraphs relate to each other, ensuring that each section incorporates the most relevant information from other parts of the document.\n\n2. **The Individual Processing Phase (Feed-Forward Network)**: Next, each team member works independently on their assigned section, applying their specialized knowledge to enhance the content. They expand on ideas, add details, and refine the language without communicating with others during this phase.\n\n3. **The Quality Control Phase (Layer Normalization)**: Throughout both phases, a quality controller ensures that all changes maintain consistent style, tone, and format across the document, preventing any section from becoming too different from the others.\n\n4. **The Preservation Phase (Residual Connections)**: Finally, the team ensures that the original essence of each section is preserved by explicitly comparing the revised version with the original and making sure important information isn't lost in the editing process.\n\nThis analogy captures the key insight that transformer blocks don't just process information—they **refine** it through complementary operations that alternate between global communication (attention) and local processing (feed-forward), while maintaining stability and coherence through normalization and residual connections.\n\n![Transformer Block Internal Structure](./diagrams/transformer-block.svg)\n\n### Feed-Forward Network\n\nThe **feed-forward network (FFN)** within each transformer block serves as the \"individual processing\" component in our mental model. While self-attention enables global communication between tokens, the FFN provides each token position with dedicated computational capacity to process and transform its representation independently.\n\nThe FFN implements a simple but powerful two-layer architecture that expands the representation to a higher-dimensional space, applies a non-linear activation function, then projects back to the original dimension. This expansion-contraction pattern is crucial for the transformer's expressiveness.\n\n> **Decision: 4x Hidden Dimension Expansion**\n> - **Context**: The FFN needs sufficient capacity to process token representations, but computational cost grows quadratically with hidden dimension\n> - **Options Considered**: \n>   - 2x expansion (used in some efficient models)\n>   - 4x expansion (standard in most transformers)  \n>   - 8x expansion (used in very large models)\n> - **Decision**: Use 4x expansion ratio as the standard\n> - **Rationale**: The 4x expansion provides a good balance between model capacity and computational efficiency. Empirically, this ratio has proven effective across many transformer variants and scales well from small to large models\n> - **Consequences**: Each FFN layer will require 8x the parameters of the attention layer (2 linear layers × 4x expansion), making FFN the dominant parameter consumer in transformer blocks\n\n| Expansion Ratio | Parameters | Computational Cost | Model Capacity | Empirical Performance |\n|-----------------|------------|-------------------|----------------|---------------------|\n| 2x | 4 × d_model² | Low | Limited | Adequate for simple tasks |\n| 4x | 8 × d_model² | Moderate | High | Strong across diverse tasks |\n| 8x | 16 × d_model² | High | Very High | Marginal gains for most tasks |\n\nThe FFN architecture consists of exactly two linear transformations with a non-linear activation function between them:\n\n1. **Expansion Layer**: Projects input from `d_model` dimensions to `ffn_expansion × d_model` dimensions (typically 4 × d_model)\n2. **Activation Function**: Applies ReLU, GELU, or SwiGLU activation to introduce non-linearity\n3. **Projection Layer**: Projects back from expanded dimension to original `d_model` dimensions\n4. **Dropout**: Applied after the projection layer for regularization during training\n\n| Component | Input Shape | Output Shape | Parameters | Purpose |\n|-----------|-------------|--------------|------------|---------|\n| Expansion Linear | (batch, seq_len, d_model) | (batch, seq_len, 4×d_model) | d_model × 4×d_model | Expand representation space |\n| Activation | (batch, seq_len, 4×d_model) | (batch, seq_len, 4×d_model) | 0 | Add non-linearity |\n| Projection Linear | (batch, seq_len, 4×d_model) | (batch, seq_len, d_model) | 4×d_model × d_model | Contract to original dimension |\n| Dropout | (batch, seq_len, d_model) | (batch, seq_len, d_model) | 0 | Regularization |\n\n> **Decision: GELU Activation Function**\n> - **Context**: Need non-linear activation function for the FFN hidden layer\n> - **Options Considered**: \n>   - ReLU (fast, simple, but can cause dead neurons)\n>   - GELU (smooth, probabilistic, better gradients)\n>   - SwiGLU (state-of-the-art but more complex)\n> - **Decision**: Use GELU as the standard activation\n> - **Rationale**: GELU provides smoother gradients than ReLU and performs better empirically on language tasks. The smooth activation helps with gradient flow during training, and the probabilistic interpretation (input × P(X ≤ input)) aligns well with the stochastic nature of language\n> - **Consequences**: Slightly more computational cost than ReLU, but significantly better training dynamics and final performance\n\nThe mathematical formulation of the complete FFN is:\n\n```\nFFN(x) = dropout(W₂ · GELU(W₁ · x + b₁) + b₂)\n```\n\nWhere:\n- W₁ has shape (d_model, 4×d_model) - expansion weights\n- W₂ has shape (4×d_model, d_model) - projection weights  \n- b₁ and b₂ are bias vectors\n- GELU(x) = x · P(X ≤ x) where X ~ N(0,1)\n\n**Why the FFN is Essential**: The FFN serves several critical purposes that attention alone cannot provide:\n\n1. **Non-linear Processing**: While attention is fundamentally a linear operation (weighted averages), the FFN introduces non-linearity that enables complex transformations\n2. **Position-wise Processing**: Each token position is processed independently, allowing the model to apply learned transformations specific to the content at that position\n3. **Memory and Storage**: The FFN weights act as a form of memory, storing patterns and transformations learned from the training data\n4. **Computational Bottleneck**: The expanded dimension provides additional computational capacity for complex reasoning\n\n### Layer Normalization Strategy\n\n**Layer normalization** serves as the \"quality control\" mechanism in our transformer block, ensuring that activations maintain stable statistics throughout the forward pass. Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes across the feature dimension for each individual token, making it particularly well-suited for sequence models where batch and sequence dimensions have different semantic meanings.\n\nThe core layer normalization operation normalizes each token's feature vector to have zero mean and unit variance, then applies learned scale and shift parameters:\n\n```\nLayerNorm(x) = γ ⊙ (x - μ) / σ + β\n```\n\nWhere:\n- μ = mean(x) across the feature dimension\n- σ = std(x) across the feature dimension  \n- γ = learned scale parameter (initialized to 1)\n- β = learned bias parameter (initialized to 0)\n- ⊙ = element-wise multiplication\n\n| Parameter | Shape | Initialization | Purpose |\n|-----------|-------|----------------|---------|\n| γ (gamma) | (d_model,) | ones | Learned scale factors |\n| β (beta) | (d_model,) | zeros | Learned bias terms |\n| ε (epsilon) | scalar | 1e-5 | Numerical stability constant |\n\n> **Decision: Pre-Normalization Architecture**\n> - **Context**: Layer normalization can be applied before or after the sublayers (attention/FFN), significantly impacting training dynamics\n> - **Options Considered**: \n>   - Post-normalization (original Transformer paper): LayerNorm(x + Sublayer(x))\n>   - Pre-normalization (modern practice): x + Sublayer(LayerNorm(x))\n>   - Sandwich normalization: LayerNorm(x + Sublayer(LayerNorm(x)))\n> - **Decision**: Use pre-normalization architecture\n> - **Rationale**: Pre-norm provides better gradient flow and training stability, especially for deep models. The normalization operation occurs in the residual branch rather than the main path, preventing gradient scaling issues. Modern transformer implementations consistently use pre-norm for improved convergence\n> - **Consequences**: Training converges faster and is more stable, but requires careful initialization since the residual path carries the primary gradient signal\n\n| Normalization Placement | Gradient Flow | Training Stability | Convergence Speed | Modern Usage |\n|-------------------------|---------------|-------------------|------------------|--------------|\n| Post-norm | Poor for deep models | Requires warm-up | Slow initial training | Legacy only |\n| Pre-norm | Excellent | High stability | Fast convergence | Standard practice |\n| Sandwich | Best theoretical | Highest stability | Fastest but complex | Research only |\n\nThe pre-normalization transformer block applies layer normalization before each sublayer rather than after:\n\n1. **Attention Sublayer**: `x = x + MultiHeadAttention(LayerNorm(x))`\n2. **FFN Sublayer**: `x = x + FFN(LayerNorm(x))`\n\nThis arrangement creates a \"clean\" residual path where gradients can flow directly from the output back to the input without passing through normalization operations that might scale or distort the gradient signal.\n\n**Why Layer Normalization Works**: The effectiveness of layer normalization in transformers stems from several factors:\n\n1. **Activation Stabilization**: Prevents activations from growing too large or small, maintaining healthy gradient magnitudes\n2. **Reduced Internal Covariate Shift**: Normalizing inputs to each layer reduces the change in input distributions during training\n3. **Improved Gradient Flow**: Stable activations lead to more consistent gradients throughout the network\n4. **Regularization Effect**: The normalization operation acts as a mild regularizer, reducing overfitting\n\n**Numerical Considerations**: Layer normalization requires careful handling of numerical stability, particularly when computing the standard deviation:\n\n```\nvariance = mean((x - mean)²) + ε\nstd = sqrt(variance)\nnormalized = (x - mean) / std\n```\n\nThe epsilon term (typically 1e-5) prevents division by zero and numerical instability when the variance is very small.\n\n### Residual Connections\n\n**Residual connections** implement the \"preservation\" aspect of our information refinement pipeline, ensuring that the original token representations are explicitly preserved even as they undergo complex transformations. These skip connections, popularized by ResNet and adapted for transformers, address the fundamental challenge of training deep neural networks: the vanishing gradient problem.\n\nIn the transformer block context, residual connections create direct paths from the input to the output that bypass the complex transformations of attention and feed-forward layers. This allows gradients to flow directly backwards during training and ensures that the model can always fall back to the identity transformation if the learned transformations prove harmful.\n\nThe mathematical formulation for each sublayer becomes:\n\n```\noutput = input + Sublayer(input)\n```\n\nWhere `Sublayer(input)` represents either the multi-head attention operation or the feed-forward network. This simple addition has profound implications for training dynamics and model expressiveness.\n\n| Sublayer | Transformation | Residual Connection |\n|----------|---------------|-------------------|\n| Multi-Head Attention | `MultiHeadAttention(LayerNorm(x))` | `x + MultiHeadAttention(LayerNorm(x))` |\n| Feed-Forward Network | `FFN(LayerNorm(x))` | `x + FFN(LayerNorm(x))` |\n\n> **Decision: Addition-Based Residual Connections**\n> - **Context**: Multiple ways exist to combine the residual connection with the sublayer output\n> - **Options Considered**: \n>   - Element-wise addition: x + f(x)\n>   - Concatenation followed by linear projection: Linear(concat(x, f(x)))\n>   - Weighted combination: αx + (1-α)f(x) with learned α\n> - **Decision**: Use simple element-wise addition\n> - **Rationale**: Addition preserves gradient flow exactly, requires no additional parameters, and has proven effective across millions of transformer deployments. More complex combinations add parameters without consistent performance gains\n> - **Consequences**: Requires that residual and sublayer outputs have identical dimensions, but provides optimal gradient flow and parameter efficiency\n\n**Why Residual Connections are Critical**: The importance of residual connections in transformers extends beyond simple gradient flow:\n\n1. **Gradient Flow Preservation**: During backpropagation, gradients can flow directly through the residual connections without being modified by sublayer computations, preventing vanishing gradients in deep networks\n\n2. **Identity Mapping Capability**: If a sublayer learns to output zeros, the overall transformation becomes the identity function, allowing the network to preserve information when complex transformations aren't beneficial\n\n3. **Training Stability**: Residual connections reduce the risk of training instability by ensuring that even poorly initialized or temporarily poorly performing sublayers cannot completely disrupt the forward pass\n\n4. **Incremental Learning**: The network can learn incremental refinements to token representations rather than completely new representations, leading to more stable training dynamics\n\n**Gradient Flow Analysis**: Consider the backward pass through a transformer block with residual connections:\n\n1. **Output Gradient**: ∂L/∂output flows from the loss function\n2. **Residual Branch**: ∂L/∂input includes a direct copy of ∂L/∂output due to the addition operation\n3. **Sublayer Branch**: ∂L/∂input also includes ∂L/∂sublayer_output × ∂sublayer_output/∂input\n4. **Combined Flow**: The total gradient is the sum of both branches, ensuring strong gradient signal\n\nThis dual-path gradient flow is the key insight that makes deep transformers trainable without the gradient decay that plagued earlier deep architectures.\n\n**Residual Connection Requirements**: For residual connections to work properly, several conditions must be met:\n\n| Requirement | Specification | Purpose |\n|-------------|---------------|---------|\n| Dimension Matching | input.shape == sublayer_output.shape | Enable element-wise addition |\n| Initialization | Sublayer outputs near zero initially | Start near identity transformation |\n| Normalization | Apply before sublayers in pre-norm | Maintain stable residual path |\n| Dropout | Applied to sublayer outputs | Regularize transformations, not residuals |\n\n### Common Pitfalls\n\nUnderstanding the most frequent implementation mistakes helps developers avoid debugging nightmares and ensures correct transformer block behavior from the start.\n\n⚠️ **Pitfall: Pre-norm vs Post-norm Confusion**\n\nMany developers implement post-normalization architecture because it appears in the original \"Attention Is All You Need\" paper, not realizing that modern practice has shifted to pre-normalization for better training dynamics.\n\n**Incorrect Implementation (Post-norm)**:\n```python\n# Wrong: applying normalization after the residual connection\ndef forward(self, x):\n    x = self.norm1(x + self.attention(x))\n    x = self.norm2(x + self.ffn(x))\n    return x\n```\n\n**Correct Implementation (Pre-norm)**:\n```python\n# Right: applying normalization before the sublayer\ndef forward(self, x):\n    x = x + self.attention(self.norm1(x))\n    x = x + self.ffn(self.norm2(x))\n    return x\n```\n\n**Why Post-norm Causes Problems**: In post-norm architecture, gradients must flow through the normalization operation, which can scale and distort gradient magnitudes. This leads to training instability, especially in deep models, and often requires careful learning rate warm-up schedules.\n\n**How to Detect**: If training loss oscillates wildly or requires extremely small learning rates to converge, check your normalization placement.\n\n⚠️ **Pitfall: Missing or Incorrect Residual Connections**\n\nResidual connections are easy to forget or implement incorrectly, leading to models that fail to train or converge to poor solutions.\n\n**Common Mistakes**:\n1. Forgetting residual connections entirely: `x = self.attention(x)` instead of `x = x + self.attention(x)`\n2. Applying residuals at wrong locations: Adding input to normalized output instead of sublayer output\n3. Dimension mismatches preventing addition: Sublayer changes dimensions without accounting for residuals\n\n**Detection**: Model trains extremely slowly, loss decreases very gradually, or training becomes unstable with deeper models.\n\n**Fix**: Ensure every sublayer has a corresponding residual connection with matching input/output dimensions.\n\n⚠️ **Pitfall: FFN Dimension Errors**\n\nThe feed-forward network's expansion and projection dimensions are frequently implemented incorrectly, leading to shape mismatches or suboptimal capacity.\n\n**Common Dimension Errors**:\n\n| Error Type | Incorrect Dimension | Correct Dimension | Consequence |\n|------------|-------------------|------------------|-------------|\n| No expansion | d_model → d_model | d_model → 4×d_model | Severely limited capacity |\n| Wrong expansion ratio | d_model → 2×d_model | d_model → 4×d_model | Reduced capacity |\n| Missing projection | 4×d_model → 4×d_model | 4×d_model → d_model | Dimension mismatch |\n| Swapped dimensions | (4×d_model, d_model) | (d_model, 4×d_model) | Runtime shape errors |\n\n**Detection**: PyTorch will typically throw shape mismatch errors during forward pass, or the model will have far fewer parameters than expected.\n\n**Fix**: Double-check that the FFN expands to 4×d_model then projects back to d_model, with weight matrices shaped correctly for matrix multiplication.\n\n⚠️ **Pitfall: Dropout Applied in Wrong Locations**\n\nDropout placement significantly affects training dynamics, and applying dropout incorrectly can harm performance or training stability.\n\n**Incorrect Dropout Applications**:\n1. Applying dropout to residual connections: breaks gradient flow\n2. Forgetting to disable dropout during inference: corrupts generation quality\n3. Applying dropout before layer normalization: destabilizes training\n\n**Correct Dropout Placement**:\n- Apply dropout to attention outputs before residual addition\n- Apply dropout to FFN outputs before residual addition  \n- Never apply dropout to the residual path itself\n- Ensure dropout is disabled during evaluation/generation\n\n⚠️ **Pitfall: Layer Normalization Parameter Initialization**\n\nIncorrect initialization of layer normalization parameters can prevent the model from training effectively.\n\n**Common Initialization Errors**:\n\n| Parameter | Wrong Init | Correct Init | Problem if Wrong |\n|-----------|------------|--------------|------------------|\n| γ (scale) | zeros | ones | Kills gradient signal |\n| β (bias) | random | zeros | Biases initial distribution |\n| ε (epsilon) | 1e-8 | 1e-5 | Numerical instability |\n\n**Detection**: Model fails to train from the beginning, loss remains constant, or numerical instabilities (NaN values) appear early in training.\n\n**Fix**: Initialize γ to ones and β to zeros, use epsilon around 1e-5 for numerical stability.\n\n⚠️ **Pitfall: Inconsistent Training vs Inference Behavior**\n\nThe transformer block behaves differently during training and inference due to dropout and layer normalization statistics, but developers sometimes fail to properly manage these modes.\n\n**Common Mode Errors**:\n1. Forgetting to call `model.eval()` during generation\n2. Manually managing dropout instead of using PyTorch's training mode\n3. Not understanding that layer normalization statistics are computed per forward pass (no stored statistics like batch norm)\n\n**Detection**: Generated text quality is poor despite good training loss, or model produces inconsistent outputs for identical inputs during inference.\n\n**Fix**: Always call `model.train()` during training and `model.eval()` during inference, and let PyTorch handle dropout automatically based on the model's mode.\n\n### Implementation Guidance\n\nThis implementation guidance provides complete, working code for transformer blocks along with the organizational structure and debugging tools needed to successfully complete Milestone 2.\n\n#### A. Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Framework | PyTorch with nn.Module | PyTorch with custom autograd functions |\n| Activation | F.gelu (built-in) | Custom GELU implementation |\n| Initialization | Default PyTorch init | Xavier/Kaiming uniform initialization |\n| Debugging | Print statements | TensorBoard logging with histograms |\n| Testing | Manual tensor checks | Automated shape and gradient tests |\n\n#### B. Recommended File Structure\n\n```\ntransformer_project/\n├── src/\n│   ├── model/\n│   │   ├── __init__.py\n│   │   ├── attention.py           ← MultiHeadAttention from Milestone 1\n│   │   ├── transformer_block.py   ← This milestone's core implementation  \n│   │   ├── ffn.py                 ← Feed-forward network component\n│   │   ├── layer_norm.py          ← Layer normalization utilities\n│   │   └── config.py              ← TransformerConfig definition\n│   ├── utils/\n│   │   ├── __init__.py\n│   │   └── model_utils.py         ← count_parameters, initialization helpers\n│   └── tests/\n│       ├── test_transformer_block.py\n│       ├── test_ffn.py\n│       └── test_layer_norm.py\n├── notebooks/\n│   └── transformer_block_analysis.ipynb  ← Visualization and debugging\n└── requirements.txt\n```\n\n#### C. Infrastructure Starter Code\n\n**File: `src/model/layer_norm.py`** (Complete implementation - copy and use):\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass LayerNorm(nn.Module):\n    \"\"\"\n    Layer normalization with learnable scale and bias parameters.\n    Normalizes across the feature dimension for each token independently.\n    \"\"\"\n    \n    def __init__(self, d_model: int, eps: float = 1e-5):\n        super().__init__()\n        self.d_model = d_model\n        self.eps = eps\n        \n        # Learnable parameters\n        self.gamma = nn.Parameter(torch.ones(d_model))  # Scale\n        self.beta = nn.Parameter(torch.zeros(d_model))  # Bias\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply layer normalization.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, d_model)\n            \n        Returns:\n            Normalized tensor with same shape as input\n        \"\"\"\n        # Compute mean and variance across the feature dimension (last dim)\n        mean = x.mean(dim=-1, keepdim=True)  # (batch_size, seq_length, 1)\n        variance = x.var(dim=-1, keepdim=True, unbiased=False)  # (batch_size, seq_length, 1)\n        \n        # Normalize\n        normalized = (x - mean) / torch.sqrt(variance + self.eps)\n        \n        # Apply learnable scale and bias\n        return self.gamma * normalized + self.beta\n\ndef test_layer_norm():\n    \"\"\"Test LayerNorm implementation with known inputs.\"\"\"\n    batch_size, seq_length, d_model = 2, 4, 6\n    layer_norm = LayerNorm(d_model)\n    \n    # Create test input with known statistics\n    x = torch.randn(batch_size, seq_length, d_model) * 5 + 10  # Mean≈10, std≈5\n    \n    # Apply layer norm\n    normalized = layer_norm(x)\n    \n    # Check that output has approximately zero mean and unit variance per token\n    token_means = normalized.mean(dim=-1)  # Should be ≈ 0\n    token_vars = normalized.var(dim=-1)    # Should be ≈ 1\n    \n    print(f\"Input mean: {x.mean(dim=-1)[0, 0]:.3f}, std: {x.std(dim=-1)[0, 0]:.3f}\")\n    print(f\"Output mean: {token_means[0, 0]:.6f}, std: {torch.sqrt(token_vars[0, 0]):.6f}\")\n    assert torch.allclose(token_means, torch.zeros_like(token_means), atol=1e-6)\n    assert torch.allclose(token_vars, torch.ones_like(token_vars), atol=1e-6)\n    print(\"LayerNorm test passed!\")\n\nif __name__ == \"__main__\":\n    test_layer_norm()\n```\n\n**File: `src/utils/model_utils.py`** (Complete utilities):\n\n```python\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Any\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef count_parameters(model: nn.Module) -> Dict[str, int]:\n    \"\"\"\n    Count trainable and total parameters in a model.\n    \n    Args:\n        model: PyTorch model\n        \n    Returns:\n        Dictionary with parameter counts\n    \"\"\"\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n    \n    return {\n        'trainable': trainable_params,\n        'total': total_params,\n        'non_trainable': total_params - trainable_params\n    }\n\ndef initialize_transformer_weights(module: nn.Module):\n    \"\"\"\n    Initialize transformer weights using best practices.\n    \n    Args:\n        module: PyTorch module to initialize\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        # Xavier uniform initialization for linear layers\n        torch.nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            torch.nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.LayerNorm):\n        # Standard layer norm initialization\n        torch.nn.init.ones_(module.weight)\n        torch.nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        # Normal initialization for embeddings\n        torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n\ndef visualize_attention_patterns(attention_weights: torch.Tensor, \n                                tokens: list, \n                                layer: int, \n                                head: int):\n    \"\"\"\n    Visualize attention weights as a heatmap.\n    \n    Args:\n        attention_weights: Attention weights of shape (num_heads, seq_len, seq_len)\n        tokens: List of token strings\n        layer: Layer number for title\n        head: Head number to visualize\n    \"\"\"\n    # Extract single head's attention\n    head_attention = attention_weights[head].detach().cpu().numpy()\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(head_attention, \n                xticklabels=tokens, \n                yticklabels=tokens,\n                cmap='Blues',\n                annot=True,\n                fmt='.3f',\n                cbar_kws={'label': 'Attention Weight'})\n    \n    plt.title(f'Layer {layer}, Head {head} Attention Pattern')\n    plt.xlabel('Key Position')\n    plt.ylabel('Query Position')\n    plt.tight_layout()\n    plt.show()\n\ndef check_gradient_flow(model: nn.Module, loss: torch.Tensor) -> Dict[str, float]:\n    \"\"\"\n    Check gradient flow through model parameters.\n    \n    Args:\n        model: PyTorch model after backward pass\n        loss: Loss tensor\n        \n    Returns:\n        Dictionary with gradient statistics per layer\n    \"\"\"\n    gradient_stats = {}\n    \n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.data.norm(2).item()\n            gradient_stats[name] = grad_norm\n        else:\n            gradient_stats[name] = 0.0\n    \n    return gradient_stats\n\ndef test_shape_consistency(model: nn.Module, input_shape: tuple, device: str = 'cpu'):\n    \"\"\"\n    Test that model maintains shape consistency through forward pass.\n    \n    Args:\n        model: PyTorch model to test\n        input_shape: Expected input shape (batch_size, seq_length, d_model)\n        device: Device to run test on\n    \"\"\"\n    model.eval()\n    \n    # Create random input\n    test_input = torch.randn(input_shape, device=device)\n    \n    print(f\"Testing model with input shape: {test_input.shape}\")\n    \n    # Forward pass\n    with torch.no_grad():\n        output = model(test_input)\n    \n    print(f\"Output shape: {output.shape}\")\n    \n    # Check that batch and sequence dimensions are preserved\n    assert output.shape[0] == input_shape[0], f\"Batch size mismatch: {output.shape[0]} vs {input_shape[0]}\"\n    assert output.shape[1] == input_shape[1], f\"Sequence length mismatch: {output.shape[1]} vs {input_shape[1]}\"\n    \n    print(\"Shape consistency test passed!\")\n    \n    return output\n```\n\n#### D. Core Logic Skeleton Code\n\n**File: `src/model/ffn.py`** (Skeleton for learner implementation):\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .config import TransformerConfig\n\nclass FeedForwardNetwork(nn.Module):\n    \"\"\"\n    Position-wise feed-forward network with 4x hidden dimension expansion.\n    Implements: FFN(x) = dropout(W2 * GELU(W1 * x + b1) + b2)\n    \"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        self.config = config\n        \n        # TODO 1: Initialize expansion layer (d_model -> 4 * d_model)\n        # Hint: Use nn.Linear(config.d_model, config.ffn_expansion * config.d_model)\n        self.expansion = None\n        \n        # TODO 2: Initialize projection layer (4 * d_model -> d_model)  \n        # Hint: Use nn.Linear(config.ffn_expansion * config.d_model, config.d_model)\n        self.projection = None\n        \n        # TODO 3: Initialize dropout layer with config.dropout_rate\n        self.dropout = None\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply feed-forward network transformation.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, d_model)\n            \n        Returns:\n            Transformed tensor with same shape as input\n        \"\"\"\n        # TODO 4: Apply expansion layer to input x\n        # Store result in variable called 'expanded'\n        expanded = None\n        \n        # TODO 5: Apply GELU activation to expanded tensor\n        # Hint: Use F.gelu(expanded)\n        activated = None\n        \n        # TODO 6: Apply projection layer to bring back to d_model dimension\n        projected = None\n        \n        # TODO 7: Apply dropout to final output (only during training)\n        # Hint: self.dropout(projected)\n        output = None\n        \n        return output\n```\n\n**File: `src/model/transformer_block.py`** (Core skeleton):\n\n```python\nimport torch\nimport torch.nn as nn\nfrom .attention import MultiHeadAttention\nfrom .ffn import FeedForwardNetwork\nfrom .layer_norm import LayerNorm\nfrom .config import TransformerConfig\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\n    Complete transformer block with multi-head attention and feed-forward network.\n    Uses pre-normalization architecture for better training stability.\n    \"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        self.config = config\n        \n        # TODO 1: Initialize multi-head attention layer\n        # Hint: Use MultiHeadAttention(config) from previous milestone\n        self.attention = None\n        \n        # TODO 2: Initialize feed-forward network\n        self.ffn = None\n        \n        # TODO 3: Initialize layer normalization for attention sublayer\n        # Hint: LayerNorm(config.d_model)\n        self.norm1 = None\n        \n        # TODO 4: Initialize layer normalization for FFN sublayer  \n        self.norm2 = None\n    \n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass through transformer block.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, d_model)\n            mask: Optional causal mask for attention\n            \n        Returns:\n            Output tensor with same shape as input\n        \"\"\"\n        # TODO 5: Implement attention sublayer with pre-norm and residual connection\n        # Pattern: x = x + attention(layer_norm(x))\n        # Store intermediate result for debugging\n        normed_x1 = None\n        attention_output = None\n        x_after_attention = None\n        \n        # TODO 6: Implement FFN sublayer with pre-norm and residual connection  \n        # Pattern: x = x + ffn(layer_norm(x))\n        normed_x2 = None\n        ffn_output = None\n        x_after_ffn = None\n        \n        return x_after_ffn\n    \n    def get_attention_weights(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"\n        Extract attention weights for visualization.\n        \n        Args:\n            x: Input tensor\n            mask: Optional causal mask\n            \n        Returns:\n            Attention weights of shape (batch_size, num_heads, seq_length, seq_length)\n        \"\"\"\n        # TODO 7: Call attention layer's get_attention_weights method\n        # Hint: Apply layer norm first, then get attention weights\n        normed_x = self.norm1(x)\n        return self.attention.get_attention_weights(normed_x, mask)\n```\n\n#### E. Language-Specific Hints\n\n**PyTorch Specific Tips:**\n\n1. **Module Registration**: All `nn.Module` subcomponents are automatically registered when assigned as attributes in `__init__`, enabling proper parameter tracking and device movement.\n\n2. **Training Mode Management**: Use `self.training` to check if model is in training mode, but prefer letting PyTorch handle dropout automatically via `F.dropout(x, training=self.training)`.\n\n3. **Memory Efficiency**: For large models, consider using `torch.utils.checkpoint.checkpoint()` to trade computation for memory by recomputing activations during backward pass.\n\n4. **Gradient Clipping**: Apply gradient clipping before optimizer step with `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`.\n\n5. **Device Handling**: Use `x.device` to ensure all tensors are on the same device, especially when creating masks or constants.\n\n#### F. Milestone Checkpoint\n\nAfter implementing the transformer block, verify correct behavior with these checkpoints:\n\n**Test 1: Shape Consistency**\n```bash\npython -c \"\nfrom src.model.transformer_block import TransformerBlock\nfrom src.model.config import TransformerConfig\nfrom src.utils.model_utils import test_shape_consistency\nimport torch\n\nconfig = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=10000)\nblock = TransformerBlock(config)\ntest_shape_consistency(block, (2, 10, 512))\n\"\n```\n\nExpected output: \"Shape consistency test passed!\" with matching input/output shapes.\n\n**Test 2: Parameter Count Verification**\n```bash\npython -c \"\nfrom src.model.transformer_block import TransformerBlock\nfrom src.model.config import TransformerConfig  \nfrom src.utils.model_utils import count_parameters\n\nconfig = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=10000)\nblock = TransformerBlock(config)\nparams = count_parameters(block)\nprint(f'Parameters: {params}')\n\n# Expected: ~2.1M parameters for d_model=512\n# FFN: 2 * (512 * 2048) = 2,097,152 parameters  \n# Attention: ~1M parameters\n# Layer norms: ~2K parameters\n\"\n```\n\n**Test 3: Gradient Flow Check**\n```bash\npython -c \"\nimport torch\nfrom src.model.transformer_block import TransformerBlock\nfrom src.model.config import TransformerConfig\nfrom src.utils.model_utils import check_gradient_flow\n\nconfig = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=10000)  \nblock = TransformerBlock(config)\n\n# Forward and backward pass\nx = torch.randn(2, 10, 512, requires_grad=True)\noutput = block(x)\nloss = output.mean()\nloss.backward()\n\ngrad_stats = check_gradient_flow(block, loss)\nfor name, grad_norm in grad_stats.items():\n    print(f'{name}: {grad_norm:.6f}')\n\"\n```\n\nExpected: All parameters should have non-zero gradients, with reasonable magnitudes (typically 1e-6 to 1e-2).\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Shape mismatch errors | FFN dimension errors | Print tensor shapes at each step | Check expansion/projection layer dimensions |\n| Training loss not decreasing | Missing residual connections | Check if loss oscillates vs stays flat | Add residual connections to each sublayer |\n| Gradient explosion | No gradient clipping | Print gradient norms | Add gradient clipping with max_norm=1.0 |\n| Very slow convergence | Post-norm instead of pre-norm | Check layer norm placement | Move layer norm before sublayers |\n| NaN values appearing | Layer norm epsilon too small | Check for division by zero | Increase epsilon to 1e-5 |\n| Poor generation quality | Dropout active during inference | Check model.training flag | Call model.eval() before generation |\n| Memory usage too high | No gradient checkpointing | Monitor GPU memory | Use torch.utils.checkpoint for large models |\n\n\n## Training Pipeline\n\n> **Milestone(s):** Milestone 3 - Training Pipeline (implementing tokenization, data loading, and language modeling objective with next-token prediction)\n\nThe training pipeline transforms our transformer from a mathematical abstraction into a learning system that can acquire language understanding from raw text. This pipeline bridges the gap between static model architecture and dynamic learning, orchestrating the complex dance between data preprocessing, batch formation, loss computation, and parameter optimization.\n\nThink of the training pipeline as a sophisticated factory assembly line for language understanding. Raw text enters at one end, gets broken down into standardized components (tokens), assembled into training batches, fed through our transformer, and emerges as gradient updates that incrementally improve the model's language capabilities. Like any efficient factory, each stage has specific responsibilities and quality controls to ensure the final product meets our standards.\n\nThe training pipeline encompasses four critical stages: tokenization converts raw text into discrete tokens our model can process, data loading organizes these tokens into efficient training batches, the language modeling objective defines what constitutes \"correct\" predictions, and the training loop orchestrates the entire learning process. Each stage presents unique challenges that can make or break the entire training effort.\n\n![Training Pipeline Sequence](./diagrams/training-flow.svg)\n\nThe complexity lies not just in implementing each component correctly, but in ensuring they work harmoniously together. A subtle bug in tokenization can propagate through the entire pipeline, manifesting as mysterious training failures hours later. Similarly, improper label shifting or gradient clipping can transform a promising training run into a frustrating plateau.\n\n### Tokenization Approach\n\nThe tokenization strategy fundamentally shapes how our transformer perceives and processes language. This seemingly simple preprocessing step—converting text strings into integer sequences—involves profound trade-offs that affect model capabilities, training efficiency, and final performance.\n\nConsider tokenization as the process of choosing a vocabulary for communication between humans and our transformer. Just as different human languages carve up the space of possible meanings differently, different tokenization strategies carve up the space of text differently. Some approaches favor fine-grained control with character-level tokens, while others prioritize efficiency with subword units.\n\n> **Decision: Character-Level vs Subword Tokenization**\n> - **Context**: Our transformer needs a way to convert arbitrary text into fixed-vocabulary token sequences. We must choose between character-level tokenization (each character becomes a token) and subword tokenization (common character sequences become tokens).\n> - **Options Considered**: Character-level tokenization, Byte-Pair Encoding (BPE), SentencePiece subword tokenization\n> - **Decision**: Character-level tokenization for initial implementation\n> - **Rationale**: Character-level tokenization provides simplicity for learning purposes—no complex subword training, deterministic encoding/decoding, and universal applicability to any language. While less efficient than subword approaches, it eliminates an entire class of preprocessing complexity.\n> - **Consequences**: Longer sequences for the same text (affecting memory usage), but simpler implementation and debugging. Can upgrade to subword tokenization later without changing model architecture.\n\n| Tokenization Approach | Vocabulary Size | Sequence Length | Implementation Complexity | Language Coverage |\n|----------------------|-----------------|-----------------|---------------------------|-------------------|\n| Character-level | ~100-500 | Long (5-10x words) | Simple | Universal |\n| Byte-Pair Encoding | ~10k-50k | Medium (1-2x words) | Complex training | Language-specific |\n| SentencePiece | ~10k-50k | Medium (1-2x words) | Complex training | Configurable |\n\nThe `SimpleTokenizer` serves as our interface to the tokenization system, providing clean separation between text processing and model training. This abstraction allows us to experiment with different tokenization strategies without modifying downstream components.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `encode` | text: str | List[int] | Converts text string to integer token sequence |\n| `decode` | tokens: List[int] | str | Converts integer token sequence back to text |\n| `vocab_size` | None | int | Returns the size of the token vocabulary |\n| `pad_token_id` | None | int | Returns the token ID used for padding sequences |\n| `eos_token_id` | None | int | Returns the token ID used to mark end of sequence |\n\nCharacter-level tokenization operates through a straightforward character-to-integer mapping. We construct a vocabulary containing all unique characters in our training corpus, plus special tokens for padding, end-of-sequence, and unknown characters. Each character maps to a unique integer ID, and tokenization becomes a simple lookup operation.\n\nThe encoding process follows these steps:\n1. Initialize vocabulary with special tokens (PAD=0, EOS=1, UNK=2)\n2. Scan training corpus to identify all unique characters\n3. Assign each character a unique integer ID starting from 3\n4. Store character-to-ID and ID-to-character mappings\n5. For encoding: map each character to its ID, handling unknowns as UNK token\n6. For decoding: map each ID back to its character, concatenating to form text\n\n> Character-level tokenization's greatest strength is its universality—given enough training data, a character-level model can theoretically represent any text in its character set. However, this comes at the cost of longer sequences and potentially slower learning of word-level patterns.\n\nSpecial token handling requires careful consideration. The padding token (`pad_token_id`) fills shorter sequences to match batch dimensions, the end-of-sequence token (`eos_token_id`) signals natural text boundaries, and the unknown token handles characters absent from the vocabulary. These special tokens need reserved positions in the vocabulary and special handling during training and generation.\n\nFor robust tokenization, we must handle edge cases: empty strings, strings containing only whitespace, strings with characters outside our vocabulary, and very long strings that exceed our maximum sequence length. Each case requires specific handling to prevent downstream errors.\n\n![Model Training States](./diagrams/training-states.svg)\n\n### Data Loading and Batching\n\nEfficient data loading transforms our tokenized text into training batches that maximize GPU utilization while preserving the sequential structure necessary for language modeling. This stage must balance memory efficiency, computational throughput, and proper sequence alignment for next-token prediction.\n\nThink of data loading as organizing a library for optimal research efficiency. Rather than reading books one word at a time, we want to arrange text into convenient chunks that researchers (our model) can process in parallel. However, unlike a physical library, our digital library must maintain the sequential flow of language while enabling parallel processing across multiple sequences.\n\nThe `TextDataset` serves as our primary data structure, implementing PyTorch's `Dataset` interface to integrate seamlessly with PyTorch's data loading ecosystem. This dataset handles the complexities of sequence windowing, overlap management, and proper label creation.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `__init__` | text_data: List[int], seq_length: int, stride: int | None | Initialize dataset with tokenized text and windowing parameters |\n| `__len__` | None | int | Return number of sequences available in dataset |\n| `__getitem__` | idx: int | Tuple[torch.Tensor, torch.Tensor] | Return input sequence and target labels for given index |\n| `create_windows` | None | List[Tuple[torch.Tensor, torch.Tensor]] | Split text into overlapping sequence windows |\n\nThe windowing strategy determines how we extract fixed-length training sequences from variable-length text. We slide a window of size `seq_length` across our tokenized text, creating overlapping sequences that maximize data utilization while maintaining sequential coherence.\n\nSequence windowing algorithm:\n1. Start with concatenated tokenized text from all training documents\n2. Initialize window start position at index 0\n3. Extract sequence from current position to position + seq_length\n4. Create input sequence from tokens[pos:pos+seq_length]\n5. Create target sequence from tokens[pos+1:pos+seq_length+1] (shifted by 1)\n6. Add (input, target) pair to dataset\n7. Advance window start position by stride amount\n8. Repeat until insufficient tokens remain for full sequence\n\nThe stride parameter controls overlap between consecutive windows. A stride equal to `seq_length` creates non-overlapping windows, maximizing data efficiency but potentially losing context at boundaries. A smaller stride creates overlapping windows, improving context continuity but increasing computational cost due to repeated processing.\n\n> **Decision: Sequence Windowing Stride**\n> - **Context**: We need to decide how much to advance the window between consecutive training sequences. This affects both data efficiency and context preservation.\n> - **Options Considered**: Stride = seq_length (non-overlapping), stride = seq_length//2 (50% overlap), stride = 1 (maximum overlap)\n> - **Decision**: Stride = seq_length//4 (75% overlap)\n> - **Rationale**: Provides good balance between context preservation and computational efficiency. Overlapping windows help the model see how sequences transition while not being excessively redundant.\n> - **Consequences**: 4x more training sequences than non-overlapping, but better context learning. Increases training time but improves model quality.\n\nLabel shifting represents the most critical aspect of sequence preparation for language modeling. The fundamental insight is that for next-token prediction, the target for each input token is the following token in the sequence. This creates an off-by-one relationship between inputs and targets.\n\nProper label shifting works as follows:\n- Input sequence: [token₁, token₂, token₃, ..., tokenₙ]\n- Target sequence: [token₂, token₃, token₄, ..., tokenₙ₊₁]\n- The model predicts token₂ given token₁, token₃ given [token₁, token₂], and so forth\n\nBatching multiple sequences together requires careful attention to padding and masking. When sequences in a batch have different lengths, we pad shorter sequences with the `pad_token_id`. However, we must ensure the loss function ignores padded positions to prevent the model from learning spurious patterns from padding tokens.\n\nThe `DataLoader` configuration affects both training efficiency and model performance. Key parameters include batch size (affecting GPU memory usage and gradient noise), shuffling (preventing overfitting to sequence order), and the number of worker processes (affecting data loading speed).\n\n| DataLoader Parameter | Value | Rationale |\n|---------------------|--------|-----------|\n| `batch_size` | 16-32 | Balance between GPU memory and gradient stability |\n| `shuffle` | True | Prevent overfitting to document order |\n| `num_workers` | 4-8 | Parallel data loading to keep GPU busy |\n| `drop_last` | True | Ensure consistent batch sizes for training |\n| `pin_memory` | True | Faster GPU transfer on CUDA systems |\n\nBatch composition requires attention to sequence boundaries. Ideally, we want each batch to contain sequences from different documents to maximize diversity. However, for simplicity, we can randomly sample sequences regardless of their source document, relying on shuffling to provide adequate mixing.\n\n### Language Modeling Objective\n\nThe language modeling objective transforms our transformer's raw output logits into a learning signal that drives parameter optimization. This objective function defines what constitutes \"correct\" behavior and shapes how our model learns to predict the next token in a sequence.\n\nConsider the language modeling objective as the grading system for our transformer student. Just as a teacher evaluates student responses against correct answers, our objective function evaluates the transformer's token predictions against the actual next tokens in our training sequences. The \"grades\" (losses) guide the optimization process toward better predictions.\n\nLanguage modeling treats each position in a sequence as an independent prediction task. Given tokens [token₁, token₂, ..., tokenᵢ], the model must predict tokenᵢ₊₁. This creates multiple supervised learning tasks within each sequence, dramatically increasing the learning signal from limited data.\n\nThe **cross-entropy loss** serves as our primary objective function, measuring the difference between the model's predicted probability distribution over tokens and the true next token. Cross-entropy loss penalizes confident wrong predictions more heavily than uncertain wrong predictions, encouraging the model to express appropriate confidence levels.\n\nCross-entropy computation proceeds as follows:\n1. Model outputs logits of shape [batch_size, seq_length, vocab_size]\n2. Apply softmax to logits to get probability distributions: P(token) = exp(logit) / Σ exp(logits)\n3. For each position, extract the probability assigned to the correct next token\n4. Compute negative log probability: loss = -log(P(correct_token))\n5. Average losses across all positions and sequences in the batch\n\n| Loss Component | Mathematical Formula | Intuitive Meaning |\n|----------------|---------------------|-------------------|\n| Position Loss | -log(P(correct_token)) | Penalty for assigning low probability to correct token |\n| Sequence Loss | Σ position_losses / seq_length | Average penalty across all positions in sequence |\n| Batch Loss | Σ sequence_losses / batch_size | Average penalty across all sequences in batch |\n\nThe logarithmic nature of cross-entropy loss creates desirable properties for optimization. When the model assigns high probability (close to 1.0) to the correct token, the loss approaches zero. When the model assigns low probability to the correct token, the loss grows large, creating strong gradients that drive learning.\n\nMasking padded positions becomes crucial when computing loss over batched sequences. Padded tokens don't represent real language and shouldn't contribute to the loss computation. We achieve this by setting the loss to zero for padded positions or by excluding them from the loss computation entirely.\n\nLoss masking implementation:\n1. Create boolean mask identifying non-padded positions: mask = (targets != pad_token_id)\n2. Compute raw cross-entropy loss for all positions\n3. Apply mask to zero out losses for padded positions: masked_loss = loss * mask\n4. Sum masked losses and divide by number of non-padded positions for proper averaging\n\n> The language modeling objective's elegance lies in its simplicity—by learning to predict the next token, the model implicitly learns grammar, semantics, world knowledge, and reasoning patterns present in the training data. This simple objective enables the emergence of complex language understanding.\n\n**Teacher forcing** represents a key training strategy where we feed the model the actual previous tokens rather than its own predictions during training. This accelerates learning by providing stable, correct context for each prediction task. During generation, however, the model must use its own predictions (autoregressive generation), creating a train-test mismatch that can affect performance.\n\nThe objective function must handle special tokens appropriately. End-of-sequence tokens provide natural boundaries for text completion tasks, while padding tokens should be masked out of loss computation. Some implementations also mask certain special tokens to prevent the model from over-relying on them.\n\nRegularization through the objective function can help prevent overfitting. Label smoothing replaces hard targets (probability 1.0 for correct token, 0.0 for others) with soft targets (probability 0.9 for correct token, 0.1/vocab_size for others), encouraging the model to be less confident and more robust.\n\n### Training Loop Design\n\nThe training loop orchestrates the entire learning process, coordinating forward passes, loss computation, backward passes, and parameter updates across thousands of iterations. This loop must balance computational efficiency, numerical stability, and training progress monitoring while gracefully handling various failure modes.\n\nThink of the training loop as the conductor of a complex orchestra. Each component—data loading, model forward pass, loss computation, backpropagation, and optimization—must execute in precise timing and coordination. The conductor (training loop) ensures smooth transitions between movements while monitoring the overall performance and making adjustments as needed.\n\nThe training loop operates as a nested iteration structure: epochs contain batches, and each batch requires a complete forward-backward cycle. This structure allows for progress tracking at multiple granularities and provides natural checkpoints for saving model state and evaluating performance.\n\nTraining loop algorithm:\n1. Initialize model with random weights using `initialize_transformer_weights`\n2. Initialize optimizer (AdamW) with learning rate and weight decay\n3. Initialize learning rate scheduler for warmup and decay\n4. For each epoch from 1 to num_epochs:\n   a. Set model to training mode (enables dropout and batch norm)\n   b. Shuffle dataset to ensure different batch composition\n   c. For each batch in DataLoader:\n      i. Move batch to GPU device if available\n      ii. Zero gradients from previous batch: optimizer.zero_grad()\n      iii. Forward pass: logits = model(input_sequences)\n      iv. Compute cross-entropy loss with proper masking\n      v. Backward pass: loss.backward()\n      vi. Gradient clipping: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n      vii. Optimizer step: optimizer.step()\n      viii. Learning rate scheduler step: scheduler.step()\n      ix. Log metrics if at logging interval\n   d. Evaluate on validation set if at evaluation interval\n   e. Save model checkpoint if at save interval\n\nThe `TrainingConfig` centralizes all training hyperparameters, making experiments reproducible and configuration management straightforward. This configuration object contains both optimization parameters and training logistics.\n\n| TrainingConfig Field | Type | Typical Value | Purpose |\n|---------------------|------|---------------|---------|\n| `learning_rate` | float | 3e-4 | Base learning rate for optimizer |\n| `batch_size` | int | 32 | Number of sequences per training batch |\n| `num_epochs` | int | 10-100 | Number of complete passes through dataset |\n| `gradient_clip_norm` | float | 1.0 | Maximum gradient norm before clipping |\n| `weight_decay` | float | 0.1 | L2 regularization strength |\n| `warmup_steps` | int | 2000 | Steps for learning rate warmup |\n| `save_interval` | int | 1000 | Steps between model checkpoints |\n| `log_interval` | int | 100 | Steps between progress logging |\n| `eval_interval` | int | 500 | Steps between validation evaluations |\n\n**Gradient clipping** prevents exploding gradients that can destabilize training. When gradients become too large (indicating the optimization step would be too aggressive), we scale them down to a maximum norm. This maintains the gradient direction while limiting step size.\n\nThe gradient clipping process:\n1. Compute the L2 norm of all gradients: grad_norm = sqrt(Σ grad²)\n2. If grad_norm > gradient_clip_norm:\n   a. Compute scaling factor: scale = gradient_clip_norm / grad_norm\n   b. Scale all gradients: grad = grad * scale\n3. Proceed with optimizer step using clipped gradients\n\n**Learning rate scheduling** adapts the learning rate during training to improve convergence. The warmup phase starts with a very low learning rate and gradually increases it to the base learning rate over the first few thousand steps. This prevents early training instability when gradients are large and unpredictable.\n\nPopular learning rate schedules include:\n- Linear warmup followed by cosine decay\n- Linear warmup followed by linear decay\n- Linear warmup followed by constant learning rate\n- Exponential decay with warmup\n\n> **Decision: Learning Rate Schedule**\n> - **Context**: We need to choose how the learning rate evolves during training. Different schedules affect convergence speed and final performance.\n> - **Options Considered**: Constant learning rate, linear warmup + cosine decay, linear warmup + linear decay\n> - **Decision**: Linear warmup (2000 steps) followed by cosine decay\n> - **Rationale**: Warmup prevents early instability, cosine decay provides smooth convergence to near-zero learning rate, widely successful in transformer training\n> - **Consequences**: More complex scheduling logic, but better training dynamics and potentially higher final performance\n\n**Checkpointing** saves model state at regular intervals, providing insurance against training interruptions and enabling model evaluation at different training stages. Effective checkpointing saves not just model weights but also optimizer state, learning rate scheduler state, and training progress metrics.\n\nCheckpoint content should include:\n- Model state dictionary (all model parameters)\n- Optimizer state dictionary (momentum terms, etc.)\n- Learning rate scheduler state\n- Current epoch and step count\n- Training loss history\n- Validation metrics history\n- Random number generator states for reproducibility\n\n**Mixed precision training** can significantly speed up training and reduce memory usage on modern GPUs. By using 16-bit floating point for most computations while keeping 32-bit precision for critical operations like loss computation, we can achieve faster training with minimal accuracy impact.\n\n**Distributed training** considerations become important for larger models and datasets. While our initial implementation targets single-GPU training, designing the training loop with clean interfaces makes future scaling easier. Key considerations include gradient synchronization, batch size scaling, and checkpoint coordination across multiple processes.\n\nProgress monitoring requires careful attention to which metrics to track and how frequently to evaluate them. Training loss should be logged frequently (every 100 steps) as it's computationally cheap. Validation loss requires a full forward pass through the validation set, so it's computed less frequently (every 500-1000 steps). Generation quality evaluation is expensive and might only be performed once per epoch.\n\n| Monitoring Metric | Frequency | Purpose |\n|-------------------|-----------|---------|\n| Training Loss | Every 100 steps | Track learning progress |\n| Training Perplexity | Every 100 steps | Interpretable loss measure |\n| Validation Loss | Every 500 steps | Detect overfitting |\n| Validation Perplexity | Every 500 steps | Comparable across models |\n| Gradient Norms | Every 100 steps | Monitor training stability |\n| Learning Rate | Every 100 steps | Verify schedule correctness |\n| Generation Samples | Every 1000 steps | Qualitative assessment |\n\n### Common Pitfalls\n\nTraining pipelines present numerous opportunities for subtle bugs that can sabotage the entire learning process. These pitfalls often manifest as mysterious training failures, poor convergence, or degraded model quality, making them particularly frustrating to debug.\n\n⚠️ **Pitfall: Incorrect Label Shifting**\nThe most common and destructive error in language model training is incorrect label shifting. This occurs when the target tokens don't properly correspond to the \"next\" tokens for each input position. For example, using the same sequence for both inputs and targets (no shift), shifting in the wrong direction, or shifting by the wrong amount. This completely breaks the learning signal, as the model learns to predict irrelevant tokens. To fix this, ensure targets are always inputs shifted forward by exactly one position: targets = inputs[1:] with appropriate handling of sequence boundaries.\n\n⚠️ **Pitfall: Padding Tokens in Loss Computation**\nIncluding padding tokens in loss computation teaches the model to predict padding tokens as likely next tokens in real text, degrading generation quality. This happens when using sequences of different lengths without proper masking in the loss function. The model learns that padding tokens are common \"words,\" leading to generations filled with padding characters. Fix this by creating a boolean mask identifying non-padding positions and applying it before averaging the loss: `loss = loss * mask` followed by `loss = loss.sum() / mask.sum()`.\n\n⚠️ **Pitfall: Missing Gradient Clipping**\nLarge gradients can destabilize training, causing loss to explode or oscillate wildly. This often occurs early in training when the model's predictions are random and gradients are consequently large and unpredictable. Without gradient clipping, a single bad batch can push parameters to extreme values that take many steps to recover from. Implement gradient clipping with `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)` before every optimizer step.\n\n⚠️ **Pitfall: Incorrect Loss Averaging**\nComputing loss incorrectly by averaging over wrong dimensions or including padded positions in the average leads to biased training signals. This commonly occurs when using PyTorch's default reduction behavior without accounting for variable sequence lengths or padding. The model may learn to favor shorter sequences or give undue weight to padding tokens. Ensure loss averaging only includes valid (non-padded) tokens by manually computing the average with proper masking.\n\n⚠️ **Pitfall: Learning Rate Too High or Too Low**\nAn inappropriate learning rate can prevent convergence entirely. Too high causes training loss to oscillate or explode, while too low results in extremely slow convergence or getting stuck in poor local minima. Start with the established learning rate of 3e-4 for transformer models, and use learning rate scheduling with warmup to ensure stable early training. Monitor training loss—if it's not decreasing after several hundred steps, the learning rate may be too low.\n\n⚠️ **Pitfall: Inconsistent Train/Eval Modes**\nForgetting to set model.train() during training or model.eval() during evaluation can lead to inconsistent behavior due to dropout and layer normalization differences. During training, dropout should be active and layer norm should use batch statistics. During evaluation, dropout should be disabled and layer norm should use running statistics. This affects both training dynamics and evaluation accuracy.\n\n⚠️ **Pitfall: Batch Size and Memory Issues**\nSetting batch size too large causes out-of-memory errors, while too small leads to noisy gradients and poor convergence. GPU memory usage scales with batch_size × seq_length × d_model, so all three factors affect memory consumption. Start with smaller batch sizes (8-16) and increase until you approach memory limits. Consider gradient accumulation if you need larger effective batch sizes than GPU memory allows.\n\n⚠️ **Pitfall: Tokenizer Vocabulary Mismatch**\nUsing a tokenizer vocabulary size that doesn't match the model's configured vocab_size parameter leads to dimension mismatches and training failures. This occurs when changing tokenization strategies without updating model configuration, or when loading pretrained tokenizers with different vocabulary sizes. Ensure the model's vocab_size exactly matches the tokenizer's vocabulary size, including all special tokens.\n\n⚠️ **Pitfall: Checkpoint Corruption**\nSaving checkpoints at inappropriate times (during gradient updates) or with incomplete state can corrupt training runs. This leads to training resumption failures or subtle performance degradation. Save checkpoints only after complete training steps, include optimizer and scheduler state for proper resumption, and verify checkpoint integrity by loading immediately after saving.\n\n⚠️ **Pitfall: Data Loading Bottlenecks**\nInsufficient data loading parallelization causes GPU utilization to drop as the model waits for new batches. This dramatically increases training time and can hide other performance issues. Use multiple DataLoader workers (num_workers=4-8), enable pin_memory for faster GPU transfer, and consider prefetching to keep the GPU consistently busy.\n\n### Implementation Guidance\n\nThe training pipeline requires careful orchestration of multiple components working in harmony. For transformer training, we'll build upon PyTorch's data loading utilities while implementing custom logic for tokenization, loss computation, and training coordination.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Tokenization | Character-level with dict mapping | HuggingFace Tokenizers with BPE |\n| Data Loading | PyTorch DataLoader with custom Dataset | WebDataset with streaming |\n| Loss Function | PyTorch CrossEntropyLoss with masking | Label smoothing + focal loss |\n| Optimization | AdamW with cosine scheduling | AdamW with custom warmup schedules |\n| Monitoring | Print statements with loss tracking | Weights & Biases or TensorBoard |\n| Checkpointing | Simple state dict saving | HuggingFace transformers format |\n\n#### Recommended File Structure\n\n```\nproject-root/\n  src/\n    data/\n      tokenizer.py              ← SimpleTokenizer implementation\n      dataset.py               ← TextDataset and data loading utilities\n      preprocessing.py         ← Text cleaning and preparation\n    training/\n      trainer.py               ← Main training loop and coordination\n      loss.py                  ← Loss computation with masking\n      optimization.py          ← Optimizer and scheduler setup\n      checkpointing.py         ← Model saving and loading\n    models/\n      transformer.py           ← Core transformer architecture (from previous sections)\n      config.py               ← Configuration classes\n    utils/\n      monitoring.py           ← Training metrics and logging\n      reproducibility.py     ← Random seed management\n  data/\n    raw/                      ← Original text files\n    processed/                ← Tokenized and prepared datasets\n  checkpoints/                ← Saved model states\n  logs/                       ← Training logs and metrics\n```\n\n#### Infrastructure Starter Code\n\nHere's complete, ready-to-use tokenizer implementation:\n\n```python\n# src/data/tokenizer.py\nimport json\nimport pickle\nfrom typing import List, Dict, Optional\nfrom collections import Counter\nimport re\n\nclass SimpleTokenizer:\n    \"\"\"Character-level tokenizer for transformer training.\n    \n    Provides clean interface for encoding/decoding text while handling\n    special tokens and unknown characters gracefully.\n    \"\"\"\n    \n    def __init__(self, pad_token: str = \"<PAD>\", eos_token: str = \"<EOS>\", \n                 unk_token: str = \"<UNK>\"):\n        self.pad_token = pad_token\n        self.eos_token = eos_token\n        self.unk_token = unk_token\n        \n        # Initialize special token mappings\n        self.char_to_id = {\n            pad_token: 0,\n            eos_token: 1,\n            unk_token: 2\n        }\n        self.id_to_char = {0: pad_token, 1: eos_token, 2: unk_token}\n        self._vocab_size = 3\n        \n    def build_vocab(self, texts: List[str]) -> None:\n        \"\"\"Build character vocabulary from training texts.\"\"\"\n        char_counts = Counter()\n        for text in texts:\n            char_counts.update(text)\n            \n        # Add characters in frequency order for consistent vocab\n        for char, _ in char_counts.most_common():\n            if char not in self.char_to_id:\n                self.char_to_id[char] = self._vocab_size\n                self.id_to_char[self._vocab_size] = char\n                self._vocab_size += 1\n                \n    def encode(self, text: str, add_eos: bool = False) -> List[int]:\n        \"\"\"Convert text to list of token IDs.\"\"\"\n        tokens = []\n        for char in text:\n            tokens.append(self.char_to_id.get(char, self.char_to_id[self.unk_token]))\n            \n        if add_eos:\n            tokens.append(self.char_to_id[self.eos_token])\n            \n        return tokens\n        \n    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:\n        \"\"\"Convert token IDs back to text.\"\"\"\n        chars = []\n        special_ids = {self.char_to_id[self.pad_token], \n                      self.char_to_id[self.eos_token]}\n        \n        for token_id in token_ids:\n            if skip_special_tokens and token_id in special_ids:\n                continue\n            chars.append(self.id_to_char.get(token_id, self.unk_token))\n            \n        return ''.join(chars)\n        \n    @property\n    def vocab_size(self) -> int:\n        return self._vocab_size\n        \n    @property\n    def pad_token_id(self) -> int:\n        return self.char_to_id[self.pad_token]\n        \n    @property\n    def eos_token_id(self) -> int:\n        return self.char_to_id[self.eos_token]\n        \n    def save(self, path: str) -> None:\n        \"\"\"Save tokenizer state.\"\"\"\n        state = {\n            'char_to_id': self.char_to_id,\n            'id_to_char': self.id_to_char,\n            'vocab_size': self._vocab_size,\n            'special_tokens': {\n                'pad_token': self.pad_token,\n                'eos_token': self.eos_token,\n                'unk_token': self.unk_token\n            }\n        }\n        with open(path, 'w') as f:\n            json.dump(state, f)\n            \n    @classmethod\n    def load(cls, path: str) -> 'SimpleTokenizer':\n        \"\"\"Load tokenizer from saved state.\"\"\"\n        with open(path, 'r') as f:\n            state = json.load(f)\n            \n        tokenizer = cls(**state['special_tokens'])\n        tokenizer.char_to_id = state['char_to_id']\n        tokenizer.id_to_char = {int(k): v for k, v in state['id_to_char'].items()}\n        tokenizer._vocab_size = state['vocab_size']\n        return tokenizer\n```\n\nHere's the complete dataset implementation:\n\n```python\n# src/data/dataset.py\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Tuple, Optional\nimport numpy as np\n\nclass TextDataset(Dataset):\n    \"\"\"Dataset for autoregressive language modeling.\n    \n    Handles sequence windowing, label shifting, and proper batching\n    for next-token prediction training.\n    \"\"\"\n    \n    def __init__(self, token_ids: List[int], seq_length: int, stride: Optional[int] = None):\n        \"\"\"\n        Args:\n            token_ids: Flattened list of all training tokens\n            seq_length: Length of each training sequence\n            stride: Step size between sequences (default: seq_length // 4)\n        \"\"\"\n        self.token_ids = token_ids\n        self.seq_length = seq_length\n        self.stride = stride or max(1, seq_length // 4)  # 75% overlap by default\n        \n        # Pre-compute all valid sequence start positions\n        self.sequence_starts = []\n        for i in range(0, len(token_ids) - seq_length, self.stride):\n            if i + seq_length + 1 <= len(token_ids):  # Need +1 for target\n                self.sequence_starts.append(i)\n                \n        if len(self.sequence_starts) == 0:\n            raise ValueError(f\"Text too short for seq_length={seq_length}\")\n            \n    def __len__(self) -> int:\n        return len(self.sequence_starts)\n        \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Return (input_sequence, target_sequence) pair.\"\"\"\n        start_pos = self.sequence_starts[idx]\n        \n        # Extract input and target sequences with proper shifting\n        input_seq = self.token_ids[start_pos:start_pos + self.seq_length]\n        target_seq = self.token_ids[start_pos + 1:start_pos + self.seq_length + 1]\n        \n        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n\ndef create_data_loaders(\n    train_tokens: List[int], \n    val_tokens: List[int],\n    seq_length: int,\n    batch_size: int,\n    num_workers: int = 4\n) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"Create train and validation data loaders.\"\"\"\n    \n    train_dataset = TextDataset(train_tokens, seq_length)\n    val_dataset = TextDataset(val_tokens, seq_length, stride=seq_length)  # No overlap for validation\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        drop_last=True,\n        pin_memory=torch.cuda.is_available()\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        drop_last=False,\n        pin_memory=torch.cuda.is_available()\n    )\n    \n    return train_loader, val_loader\n```\n\n#### Core Training Loop Skeleton\n\nThis skeleton provides the training coordination logic you'll need to implement:\n\n```python\n# src/training/trainer.py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom typing import Dict, Any, Optional\nimport math\nimport time\nfrom pathlib import Path\n\nclass TransformerTrainer:\n    \"\"\"Coordinates transformer training with proper gradient handling and monitoring.\"\"\"\n    \n    def __init__(self, model: nn.Module, config: 'TrainingConfig', \n                 tokenizer: 'SimpleTokenizer'):\n        self.model = model\n        self.config = config  \n        self.tokenizer = tokenizer\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Move model to device\n        self.model.to(self.device)\n        \n        # Initialize training components\n        self.optimizer = self._create_optimizer()\n        self.scheduler = self._create_scheduler()\n        self.criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='none')\n        \n        # Training state\n        self.step = 0\n        self.epoch = 0\n        self.best_val_loss = float('inf')\n        \n    def train(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, Any]:\n        \"\"\"Main training loop with proper error handling and monitoring.\"\"\"\n        \n        # TODO 1: Set model to training mode and initialize metrics tracking\n        # TODO 2: For each epoch in range(config.num_epochs):\n        # TODO 3: For each batch in train_loader:\n        # TODO 4: Move batch to device and extract input/target sequences  \n        # TODO 5: Zero gradients: optimizer.zero_grad()\n        # TODO 6: Forward pass: logits = model(input_sequences)\n        # TODO 7: Compute masked loss using _compute_loss helper\n        # TODO 8: Backward pass: loss.backward()\n        # TODO 9: Clip gradients: torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)\n        # TODO 10: Optimizer and scheduler steps\n        # TODO 11: Log metrics at config.log_interval\n        # TODO 12: Validate at config.eval_interval  \n        # TODO 13: Save checkpoint at config.save_interval\n        # TODO 14: Update self.step counter\n        # Hint: Use _compute_loss, _validate, _save_checkpoint helper methods\n        pass\n        \n    def _compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute cross-entropy loss with proper padding mask.\"\"\"\n        # TODO 1: Reshape logits to [batch_size * seq_length, vocab_size]\n        # TODO 2: Reshape targets to [batch_size * seq_length]\n        # TODO 3: Compute per-position losses using self.criterion\n        # TODO 4: Create mask for non-padded positions\n        # TODO 5: Apply mask and average over valid positions only\n        # Hint: targets != self.tokenizer.pad_token_id creates the mask\n        pass\n        \n    def _validate(self, val_loader: DataLoader) -> float:\n        \"\"\"Compute validation loss without gradient computation.\"\"\"\n        # TODO 1: Set model to eval mode and disable gradients\n        # TODO 2: Iterate through validation batches\n        # TODO 3: Compute losses and accumulate\n        # TODO 4: Return average validation loss\n        # TODO 5: Remember to set model back to train mode\n        # Hint: Use torch.no_grad() context manager\n        pass\n        \n    def _create_optimizer(self) -> torch.optim.Optimizer:\n        \"\"\"Create AdamW optimizer with weight decay.\"\"\"\n        return torch.optim.AdamW(\n            self.model.parameters(),\n            lr=self.config.learning_rate,\n            weight_decay=self.config.weight_decay,\n            betas=(0.9, 0.95),  # Standard transformer betas\n            eps=1e-8\n        )\n        \n    def _create_scheduler(self) -> torch.optim.lr_scheduler._LRScheduler:\n        \"\"\"Create learning rate scheduler with warmup.\"\"\"\n        # TODO 1: Implement linear warmup for config.warmup_steps\n        # TODO 2: Follow with cosine decay to training end  \n        # TODO 3: Use torch.optim.lr_scheduler.LambdaLR with custom lambda\n        # Hint: Combine linear warmup (lr = step / warmup_steps * base_lr) \n        # with cosine decay (lr = 0.5 * (1 + cos(π * progress)))\n        pass\n        \n    def _save_checkpoint(self, val_loss: float, filepath: Optional[str] = None) -> None:\n        \"\"\"Save complete training state.\"\"\"\n        # TODO 1: Create checkpoint dictionary with model, optimizer, scheduler state\n        # TODO 2: Include current step, epoch, and best validation loss\n        # TODO 3: Save tokenizer state alongside model\n        # TODO 4: Use torch.save to write checkpoint\n        # Hint: Include everything needed to resume training exactly\n        pass\n        \n    def generate_sample(self, prompt: str, max_length: int = 100) -> str:\n        \"\"\"Generate text sample for qualitative evaluation.\"\"\"\n        # TODO 1: Tokenize prompt and move to device\n        # TODO 2: Set model to eval mode  \n        # TODO 3: Generate tokens autoregressively\n        # TODO 4: Decode and return generated text\n        # Hint: Use torch.no_grad() and model.eval()\n        pass\n```\n\n#### Language-Specific Hints\n\n**PyTorch Training Best Practices:**\n- Use `torch.cuda.amp.autocast()` for automatic mixed precision training to speed up training and reduce memory usage\n- Set `torch.backends.cudnn.benchmark = True` if input sizes are consistent to optimize CUDA operations\n- Use `model.train()` before training loops and `model.eval()` before validation/generation\n- Always use `torch.no_grad()` context manager during validation and generation to save memory\n\n**Memory Management:**\n- Monitor GPU memory with `torch.cuda.memory_allocated()` and `torch.cuda.memory_reserved()`\n- Use gradient accumulation if desired batch size exceeds memory: accumulate gradients over multiple mini-batches before optimizer step\n- Clear unnecessary variables with `del variable` or let them go out of scope in tight loops\n- Consider using `torch.utils.checkpoint.checkpoint()` for memory-efficient gradient computation in very deep models\n\n**Numerical Stability:**\n- Initialize transformer weights using scaled random initialization: `nn.init.normal_(tensor, mean=0, std=0.02)`\n- Use `torch.nn.utils.clip_grad_norm_()` with max_norm=1.0 to prevent gradient explosions\n- Monitor gradient norms: `torch.nn.utils.clip_grad_norm_()` returns the computed norm before clipping\n- Consider using `torch.nn.functional.cross_entropy()` instead of manual softmax + log for better numerical stability\n\n#### Milestone Checkpoint\n\nAfter implementing the training pipeline, verify correct behavior:\n\n**Step 1: Test Tokenizer**\n```bash\npython -c \"\nfrom src.data.tokenizer import SimpleTokenizer\ntokenizer = SimpleTokenizer()\ntokenizer.build_vocab(['hello world', 'test text'])\ntokens = tokenizer.encode('hello')\nprint(f'Tokens: {tokens}')\nprint(f'Decoded: {tokenizer.decode(tokens)}')\n\"\n```\nExpected: Tokens should be list of integers, decoded text should match input.\n\n**Step 2: Test Dataset**\n```bash\npython -c \"\nfrom src.data.dataset import TextDataset\ntokens = list(range(100))  # Simple test data\ndataset = TextDataset(tokens, seq_length=10)\ninput_seq, target_seq = dataset[0]\nprint(f'Input: {input_seq.tolist()[:5]}...')\nprint(f'Target: {target_seq.tolist()[:5]}...')\nprint(f'Correct shift: {target_seq[0] == input_seq[1]}')\n\"\n```\nExpected: Target should be input shifted by 1 position.\n\n**Step 3: Training Loop Test**\nRun training for a few steps on tiny dataset:\n```bash\npython train.py --seq_length 32 --batch_size 4 --num_epochs 1 --save_interval 50\n```\nExpected behaviors:\n- Loss should be computed and printed (initial loss around ln(vocab_size) ≈ 4-6 for character-level)\n- No CUDA out of memory errors\n- Gradient norms should be reasonable (1.0-10.0 range initially)\n- Model should save checkpoint after 50 steps\n\n**Signs of Problems:**\n- Loss = NaN: Check for division by zero in loss computation or exploding gradients\n- Loss not decreasing after 500+ steps: Learning rate too low, or incorrect label shifting\n- CUDA OOM: Reduce batch_size or seq_length\n- Very slow training: Increase num_workers in DataLoader, check data loading bottleneck\n\n\n## Text Generation\n\n> **Milestone(s):** Milestone 4 - Text Generation (implementing autoregressive generation with various sampling strategies and KV caching optimization)\n\nText generation represents the culmination of our transformer implementation—the point where our carefully crafted attention mechanisms, transformer blocks, and training pipeline converge to produce coherent text sequences. Unlike training, where we process entire sequences in parallel, text generation operates in an autoregressive manner, predicting one token at a time and using each prediction to inform the next. This fundamental shift from parallel processing to sequential generation introduces unique challenges around computational efficiency, sampling strategies, and maintaining coherence over long sequences.\n\nThe text generation process transforms our transformer from a pattern recognition system into a creative engine capable of extending prompts, completing thoughts, and generating novel content. However, this transformation requires careful consideration of how we sample from probability distributions, how we cache intermediate computations for efficiency, and how we balance creativity with coherence in the generated output.\n\n### Mental Model: Iterative Prediction\n\nThink of text generation like a careful storyteller who must decide each word one at a time, considering all the words that came before but never knowing what comes next until the decision is made. The storyteller has learned patterns from thousands of stories and can predict which word is most likely to come next, but still has creative freedom in making each choice.\n\nAt each step, the transformer examines the entire sequence generated so far—from the initial prompt through all previously generated tokens—and produces a probability distribution over the entire vocabulary. This distribution represents the model's confidence about what should come next: perhaps 30% chance of \"the\", 15% chance of \"a\", 10% chance of \"and\", and thousands of other possibilities with smaller probabilities. The generation process then samples from this distribution to select the actual next token.\n\nThis process creates a feedback loop where each generated token influences all subsequent predictions. If we generate \"The cat\", the model might strongly favor \"sat\" or \"ran\" as the next token. But if we had generated \"The dog\" instead, those same positions would favor different continuations. This dependency chain explains why text generation is inherently sequential—we cannot parallelize the generation of multiple tokens because each one depends on the concrete choices made for all previous positions.\n\nThe iterative nature also explains why generation can sometimes produce repetitive or incoherent text. Early poor choices compound through the sequence, leading the model down paths that become increasingly difficult to recover from. Advanced sampling strategies help mitigate these issues by introducing controlled randomness that prevents the model from getting stuck in repetitive loops while maintaining overall coherence.\n\n**Decision: Autoregressive Generation Strategy**\n- **Context**: Transformers can theoretically generate multiple tokens in parallel, but maintaining coherence requires each token to depend on concrete previous choices\n- **Options Considered**: \n  1. Fully parallel generation with iterative refinement\n  2. Autoregressive generation with caching optimizations\n  3. Hybrid approach with parallel generation for independent positions\n- **Decision**: Pure autoregressive generation with KV caching\n- **Rationale**: Autoregressive generation provides the strongest guarantees of coherence and is the standard approach used by successful language models. The sequential dependency ensures each token is conditioned on the actual previous tokens rather than parallel predictions that might conflict\n- **Consequences**: Generation is slower than parallel approaches but produces more coherent text. Requires careful optimization through caching to achieve acceptable performance\n\n| Generation Approach | Coherence | Speed | Implementation Complexity | Chosen? |\n|---------------------|-----------|--------|---------------------------|---------|\n| Parallel with refinement | Medium | High | High | No |\n| Autoregressive with caching | High | Medium | Medium | **Yes** |\n| Hybrid parallel/sequential | Medium | Medium | Very High | No |\n\nThe autoregressive approach treats text generation as a sequential decision process where each token is sampled independently from a distribution conditioned on all previous tokens. This ensures maximum coherence but requires optimization strategies like KV caching to achieve practical performance.\n\n### Sampling Strategy Design\n\nOnce our transformer produces a probability distribution over the vocabulary, we must decide how to select the next token from this distribution. This choice fundamentally shapes the character of the generated text—deterministic selection produces predictable but potentially repetitive text, while random sampling introduces creativity at the cost of potential incoherence. The art of text generation lies in balancing these competing objectives through sophisticated sampling strategies.\n\nThe probability distribution produced by the transformer's final linear layer contains the raw logits for each vocabulary token. These logits represent the model's unnormalized confidence in each token, but they require transformation into proper probabilities through the softmax function. However, we can manipulate these logits before applying softmax to influence the resulting probability distribution and, consequently, the sampling behavior.\n\n**Decision: Multiple Sampling Strategy Support**\n- **Context**: Different applications require different trade-offs between creativity and predictability in generated text\n- **Options Considered**:\n  1. Single greedy decoding for simplicity\n  2. Temperature sampling only for controlled randomness\n  3. Multiple strategies (greedy, temperature, top-k, top-p) for flexibility\n- **Decision**: Implement multiple complementary sampling strategies\n- **Rationale**: Different use cases demand different sampling behaviors. Code completion benefits from greedy decoding, creative writing needs temperature sampling, and chatbots work well with top-p sampling. Supporting multiple strategies allows users to choose the appropriate method\n- **Consequences**: Increased implementation complexity but much greater flexibility for different applications\n\n#### Greedy Decoding\n\nGreedy decoding represents the simplest sampling strategy: always select the token with the highest probability at each step. This deterministic approach produces the same output given identical inputs, making it valuable for applications requiring predictable behavior like code completion or factual question answering.\n\nThe greedy strategy operates by applying `torch.argmax()` to the logits after softmax normalization, identifying the single most likely token according to the model's learned patterns. This approach maximizes the likelihood of each individual token choice, though it does not necessarily maximize the likelihood of the entire generated sequence due to the sequential dependencies in autoregressive generation.\n\n| Greedy Decoding Characteristics | Value |\n|--------------------------------|-------|\n| Determinism | Fully deterministic |\n| Creativity | Low - always most likely choice |\n| Coherence | High - follows strongest patterns |\n| Repetition Risk | High - can get stuck in loops |\n| Best Use Cases | Code completion, factual answers |\n| Implementation Complexity | Very low |\n\nGreedy decoding often produces highly coherent short sequences but struggles with longer generations where the deterministic nature can lead to repetitive patterns. The model may generate the same phrases repeatedly because the highest-probability continuation often cycles back to similar contexts.\n\n#### Temperature-Based Sampling\n\nTemperature sampling introduces controlled randomness by scaling the logits before applying softmax normalization. The temperature parameter τ (tau) controls how \"sharp\" or \"flat\" the probability distribution becomes: lower temperatures make the distribution more peaked around high-probability tokens, while higher temperatures flatten the distribution to make all tokens more equally likely.\n\nThe mathematical transformation applies the temperature by dividing all logits by τ before softmax: `P(token) = softmax(logits / temperature)`. When temperature equals 1.0, the distribution remains unchanged. Values below 1.0 (like 0.7) sharpen the distribution, making high-probability tokens even more likely. Values above 1.0 (like 1.5) flatten the distribution, increasing the likelihood of selecting lower-probability tokens.\n\n| Temperature Value | Effect | Typical Use Case |\n|------------------|--------|------------------|\n| 0.1 - 0.5 | Very focused, near-deterministic | Technical writing, factual content |\n| 0.6 - 0.8 | Moderately focused, some variation | General-purpose text generation |\n| 0.9 - 1.1 | Balanced creativity and coherence | Creative writing, dialogue |\n| 1.2 - 2.0 | High creativity, less coherent | Brainstorming, experimental text |\n| 2.0+ | Very random, potentially incoherent | Debugging, exploration |\n\nTemperature sampling provides a simple but effective way to control the creativity-coherence trade-off. The strategy samples from the temperature-adjusted distribution using `torch.multinomial()`, ensuring that even low-probability tokens have a chance of selection when temperatures are higher.\n\n#### Top-k Sampling\n\nTop-k sampling addresses a key limitation of temperature sampling: it still considers the entire vocabulary, including tokens that are contextually inappropriate or nonsensical. By restricting sampling to only the k most likely tokens, top-k sampling prevents the selection of highly improbable tokens while maintaining diversity among reasonable choices.\n\nThe algorithm first identifies the k tokens with highest logits, sets all other logits to negative infinity (effectively zero probability), then applies temperature and samples from this restricted distribution. This approach ensures that sampling never selects tokens outside the top-k most likely options, regardless of the temperature setting.\n\n| Top-k Value | Vocabulary Restriction | Generation Character |\n|-------------|----------------------|---------------------|\n| k = 1 | Single token (greedy) | Deterministic |\n| k = 5-10 | Very restricted | Conservative, highly coherent |\n| k = 20-50 | Moderately restricted | Balanced creativity and coherence |\n| k = 100-200 | Lightly restricted | High creativity, filters extreme outliers |\n| k = vocab_size | No restriction | Equivalent to pure temperature sampling |\n\nTop-k sampling works particularly well for technical domains where terminology is specific and most vocabulary tokens would be inappropriate in context. However, the fixed k value can be problematic when the confidence distribution varies significantly across different contexts—sometimes the model is very confident and only needs k=5 options, while other times it's uncertain and benefits from k=50 choices.\n\n#### Top-p (Nucleus) Sampling\n\nTop-p sampling, also known as nucleus sampling, provides an adaptive solution to top-k's fixed restriction problem. Instead of selecting a fixed number of tokens, top-p dynamically determines how many tokens to include based on their cumulative probability mass. The algorithm includes the minimum number of tokens needed to reach a cumulative probability of p.\n\nThe process sorts tokens by probability in descending order, then includes tokens until their cumulative probability reaches the threshold p. If the most likely token has probability 0.8 and p=0.9, only one additional token is needed to cross the threshold. But if probabilities are more evenly distributed, many more tokens might be included to reach the same cumulative probability.\n\n| Top-p Value | Probability Mass | Typical Behavior |\n|-------------|------------------|------------------|\n| p = 0.1-0.3 | Very conservative | Near-deterministic, high confidence only |\n| p = 0.5-0.7 | Moderately selective | Good balance of safety and creativity |\n| p = 0.8-0.9 | Standard setting | Most common choice for general use |\n| p = 0.95-0.99 | Very inclusive | High creativity, includes low-probability options |\n| p = 1.0 | No restriction | Equivalent to pure temperature sampling |\n\nTop-p sampling adapts naturally to the model's confidence level. When the model is highly confident about the next token, the nucleus remains small and generation stays focused. When the model is uncertain, the nucleus expands to include more options, allowing for greater creativity precisely when the model lacks strong preferences.\n\n**The `GenerationConfig` structure captures all sampling parameters:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `temperature` | float | Temperature scaling factor (0.1 = focused, 2.0 = creative) |\n| `top_k` | int | Maximum number of tokens to consider (0 = disabled) |\n| `top_p` | float | Cumulative probability threshold for nucleus sampling |\n| `max_length` | int | Maximum sequence length including prompt |\n| `pad_token_id` | int | Token ID used for padding in batched generation |\n| `eos_token_id` | int | Token ID that signals end of sequence |\n| `repetition_penalty` | float | Penalty factor for recently used tokens |\n| `length_penalty` | float | Penalty factor favoring longer or shorter sequences |\n\nThe sampling strategies can be combined effectively: temperature adjustment shapes the overall distribution, top-k or top-p restricts the candidate set, and repetition penalties discourage cycling. This layered approach provides fine-grained control over generation behavior.\n\n### KV Cache Optimization\n\nThe autoregressive nature of text generation creates a significant computational inefficiency: at each step, we must recompute attention for all previous tokens, even though their key and value representations haven't changed. For a sequence of length n, this means the first token's key and value are computed n times, the second token's n-1 times, and so on. This quadratic redundancy makes naive autoregressive generation prohibitively expensive for longer sequences.\n\nKV caching solves this problem by storing the computed key and value tensors for all previous tokens, reusing them across generation steps. When generating the next token, we only compute keys and values for the new token, then concatenate them with the cached values from previous steps. This optimization transforms the computational complexity from O(n²) to O(n) for the key-value computations.\n\n**Decision: KV Cache Implementation Strategy**\n- **Context**: Autoregressive generation recomputes the same key-value pairs repeatedly, creating significant computational waste\n- **Options Considered**:\n  1. No caching - recompute everything each step for simplicity\n  2. Full KV caching with careful tensor management\n  3. Sliding window cache with fixed memory usage\n- **Decision**: Full KV caching with dynamic tensor growth\n- **Rationale**: The performance benefits of KV caching are too significant to ignore—often 5-10x speedup for longer sequences. Full caching provides maximum performance and the implementation complexity is manageable with proper tensor operations\n- **Consequences**: Significantly faster generation at the cost of increased memory usage and more complex cache management code\n\n#### Cache Structure and Management\n\nThe KV cache maintains separate storage for keys and values at each transformer layer, since multi-layer models need to cache intermediate states throughout the entire stack. Each cache entry stores tensors with shape `[batch_size, num_heads, seq_length, d_k]` for keys and `[batch_size, num_heads, seq_length, d_v]` for values.\n\n| Cache Component | Shape | Purpose |\n|----------------|-------|---------|\n| Layer key cache | `[batch_size, num_heads, cached_length, d_k]` | Stores computed key projections for reuse |\n| Layer value cache | `[batch_size, num_heads, cached_length, d_v]` | Stores computed value projections for reuse |\n| Position tracker | `int` | Current sequence length for indexing new tokens |\n| Attention mask | `[1, 1, total_length, total_length]` | Causal mask expanded for cached + new tokens |\n\nThe cache grows dynamically as new tokens are generated. At step t, we concatenate the new token's keys and values with the cached tensors from steps 1 through t-1. This concatenation happens along the sequence length dimension, preserving the batch and head dimensions.\n\n#### Cache Update Process\n\nThe cache update process follows a precise sequence to maintain consistency between cached values and current computations:\n\n1. **Extract New Token Keys and Values**: Compute key and value projections for the current input token using the learned projection matrices. These new tensors have shape `[batch_size, num_heads, 1, d_k/d_v]` since we're processing a single new token.\n\n2. **Concatenate with Cache**: Combine the new key and value tensors with their respective caches along the sequence dimension. If the cache contains keys for positions 0 through t-1, concatenation produces tensors spanning positions 0 through t.\n\n3. **Update Attention Mask**: Expand the causal attention mask to accommodate the new sequence length. The mask must prevent position t from attending to any position beyond t, maintaining the autoregressive property.\n\n4. **Compute Attention**: Perform scaled dot-product attention using the full concatenated keys and values but only computing outputs for the new token position. This requires careful indexing to avoid recomputing attention for cached positions.\n\n5. **Update Cache Storage**: Store the concatenated key and value tensors back into the cache, replacing the previous cached values. Update the position tracker to reflect the new sequence length.\n\nThe attention computation during cached generation requires special handling because we only need the attention output for the new token, but the attention scores must be computed using all previous keys. The query tensor for the new token attends to keys from all previous positions, but we only need to compute and return the weighted sum for the current position.\n\n#### Memory Management Considerations\n\nKV caching trades memory for computational efficiency, and this trade-off becomes more pronounced for longer sequences or larger models. The memory requirements scale linearly with sequence length, number of layers, model dimension, and batch size. For a model with `num_layers` layers, maximum sequence length `max_length`, and batch size `batch_size`, the total cache memory is approximately:\n\n```\nTotal Cache Memory ≈ 2 × num_layers × batch_size × num_heads × max_length × (d_k + d_v) × sizeof(float)\n```\n\n| Memory Optimization Strategy | Memory Impact | Performance Impact | Implementation Complexity |\n|-----------------------------|---------------|-------------------|---------------------------|\n| No optimization | High memory usage | Maximum performance | Low |\n| Sliding window cache | Fixed memory | Moderate performance loss | Medium |\n| Gradient checkpointing | Low memory | Significant performance loss | High |\n| Mixed precision caching | 50% memory reduction | Minimal performance impact | Low |\n\nFor most applications, the memory cost is justified by the dramatic performance improvement. However, for very long sequences or memory-constrained environments, sliding window caching can maintain a fixed-size cache containing only the most recent tokens, though this may impact generation quality for tasks requiring long-range dependencies.\n\n#### Cache Consistency and Debugging\n\nKV cache implementations are prone to subtle bugs that can be difficult to diagnose because they often manifest as slight quality degradations rather than obvious failures. The most common issues involve dimension mismatches, incorrect concatenation operations, or mask inconsistencies.\n\n| Common Cache Bug | Symptom | Diagnostic | Fix |\n|------------------|---------|------------|-----|\n| Wrong concatenation dimension | Shape errors or gibberish text | Check tensor shapes before/after concat | Concatenate along seq_length dimension (dim=2) |\n| Stale cache between sequences | First token of new sequence is wrong | Clear cache completely between sequences | Reset cache to None or empty tensors |\n| Mask-cache length mismatch | Attention errors or NaN values | Verify mask shape matches cached sequence length | Update mask size when updating cache |\n| Incorrect position indexing | Repetitive or inconsistent text | Log cache positions and verify monotonic increase | Use cache size to determine next position |\n\nCache validation can be implemented by occasionally running generation both with and without caching and comparing the outputs. Identical results confirm cache correctness, while differences indicate bugs in the caching logic.\n\n### Common Pitfalls\n\nText generation combines multiple complex systems—attention mechanisms, sampling strategies, and cache management—creating numerous opportunities for subtle errors. These pitfalls often produce plausible but incorrect behavior, making them particularly challenging to diagnose and fix.\n\n⚠️ **Pitfall: Repetitive Text Generation**\n\nThe most common generation quality issue is repetitive text where the model generates the same phrases or tokens in loops. This typically occurs when using greedy decoding or very low temperatures, where the deterministic selection process leads the model into cycles.\n\n**Why it happens**: Deterministic sampling strategies can create feedback loops where a particular token sequence makes itself the most likely continuation. For example, generating \"the the the\" makes \"the\" highly likely as the next token, perpetuating the repetition.\n\n**How to fix**: Implement repetition penalty mechanisms that temporarily reduce the probability of recently generated tokens. A typical approach multiplies the logits of recent tokens by a penalty factor (like 0.9) before sampling. Alternatively, switch to stochastic sampling methods like top-p sampling that introduce enough randomness to break repetitive patterns.\n\n⚠️ **Pitfall: Temperature Edge Cases**\n\nTemperature scaling can create numerical instability or unexpected behavior at extreme values. Setting temperature to exactly 0.0 causes division by zero, while very high temperatures can make logits so small that softmax produces uniform distributions.\n\n**Why it happens**: The temperature transformation `logits / temperature` becomes undefined when temperature equals zero, and very small temperatures can cause logit overflow when large values are divided by tiny denominators.\n\n**How to fix**: Clamp temperature to a minimum value (like 1e-6) to prevent division by zero, and implement special handling for very low temperatures by using greedy decoding directly instead of temperature sampling.\n\n⚠️ **Pitfall: KV Cache Dimension Mismatches**\n\nCache concatenation often fails due to dimension mismatches between new keys/values and cached tensors. This typically happens when batch sizes change between generation steps or when the cache isn't properly initialized.\n\n**Why it happens**: Tensor concatenation requires exact shape matching in all dimensions except the concatenation dimension. If the cache was initialized with a different batch size or head configuration, concatenation fails with cryptic error messages.\n\n**How to fix**: Always validate tensor shapes before concatenation operations. Clear and reinitialize caches when generation parameters change. Use assertion statements to verify shape consistency: `assert new_keys.shape[:-2] == cached_keys.shape[:-2]`.\n\n⚠️ **Pitfall: Incorrect Label Shifting in Generation Loop**\n\nUnlike training where we have ground-truth target tokens, generation must use the model's own previous predictions as input for the next step. Failing to properly append generated tokens to the input sequence breaks the autoregressive dependency chain.\n\n**Why it happens**: Training uses teacher forcing with ground-truth previous tokens, but generation must use model predictions. Code written for training often doesn't properly accumulate generated tokens into the input sequence.\n\n**How to fix**: Maintain an explicit generated sequence that grows with each prediction. At each step, append the newly generated token to this sequence and use the entire accumulated sequence as input for the next prediction.\n\n⚠️ **Pitfall: Ignoring Special Tokens During Generation**\n\nGeneration systems must handle special tokens like `pad_token_id` and `eos_token_id` correctly. Failing to stop generation when encountering end-of-sequence tokens leads to continued generation beyond natural stopping points.\n\n**Why it happens**: The model learns to predict end-of-sequence tokens at natural completion points, but generation code often ignores these predictions and continues generating until reaching maximum length.\n\n**How to fix**: Check each generated token against `eos_token_id` and terminate generation immediately when encountered. For batched generation, track completion status per sequence and mask completed sequences from further processing.\n\n⚠️ **Pitfall: Softmax Numerical Instability**\n\nLarge logit values can cause softmax computation to overflow or underflow, producing NaN values that corrupt the entire generation process. This is particularly problematic with very low temperatures that amplify logit magnitudes.\n\n**Why it happens**: Softmax involves exponential operations that can produce very large or very small values when logits are extreme. The numerical precision of floating-point arithmetic cannot handle the full range of possible exponential values.\n\n**How to fix**: Implement numerically stable softmax by subtracting the maximum logit value before exponentiation: `softmax(x) = softmax(x - max(x))`. Modern frameworks like PyTorch handle this automatically, but custom implementations must include this normalization.\n\n⚠️ **Pitfall: Inconsistent Device Placement**\n\nGeneration involves multiple tensors (model parameters, input sequences, caches, masks) that must all reside on the same device (CPU or GPU). Device mismatches cause runtime errors that can be difficult to trace.\n\n**Why it happens**: Tensor operations require all operands to be on the same device, but generation code often creates new tensors (like attention masks) without explicitly placing them on the model's device.\n\n**How to fix**: Always specify device placement explicitly when creating new tensors during generation. Use the model's device as the reference: `new_tensor = torch.tensor(data, device=model.device)`. Implement device checking assertions to catch mismatches early.\n\n### Implementation Guidance\n\nThe text generation system requires careful orchestration of multiple components—the trained transformer model, sampling strategies, cache management, and sequence building logic. The implementation must balance simplicity for basic use cases with flexibility for advanced sampling techniques and performance optimizations.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Sampling | Temperature + greedy fallback | Full top-k/top-p with temperature |\n| Caching | Manual tensor concatenation | Optimized KV cache class with memory pooling |\n| Batching | Single sequence generation | Dynamic batching with padding |\n| Device Management | CPU-only generation | CUDA with automatic device placement |\n| Memory Management | Unlimited cache growth | Memory-bounded cache with eviction |\n\n#### File Structure\n\n```\ntransformer/\n├── generation/\n│   ├── __init__.py              ← Export main generation classes\n│   ├── generator.py             ← Main TextGenerator class\n│   ├── sampling.py              ← Sampling strategy implementations\n│   ├── cache.py                 ← KV cache management\n│   └── utils.py                 ← Generation utilities and helpers\n├── models/\n│   └── transformer.py          ← Core transformer model (from previous sections)\n└── training/\n    └── tokenizer.py             ← SimpleTokenizer (from previous sections)\n```\n\n#### Core Generation Infrastructure\n\n**Complete KV Cache Implementation:**\n\n```python\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple, List, Dict, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass GenerationConfig:\n    \"\"\"Configuration for text generation parameters.\"\"\"\n    temperature: float = 1.0\n    top_k: int = 0  # 0 means disabled\n    top_p: float = 1.0  # 1.0 means disabled\n    max_length: int = 100\n    pad_token_id: int = 0\n    eos_token_id: int = 1\n    repetition_penalty: float = 1.0\n    length_penalty: float = 1.0\n\nclass KVCache:\n    \"\"\"Manages key-value caching for efficient autoregressive generation.\"\"\"\n    \n    def __init__(self, num_layers: int, num_heads: int, d_k: int, d_v: int, device: torch.device):\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.d_k = d_k\n        self.d_v = d_v\n        self.device = device\n        \n        # Initialize empty cache storage\n        self.key_caches: List[Optional[torch.Tensor]] = [None] * num_layers\n        self.value_caches: List[Optional[torch.Tensor]] = [None] * num_layers\n        self.current_length = 0\n    \n    def clear(self):\n        \"\"\"Clear all cached values and reset length counter.\"\"\"\n        self.key_caches = [None] * self.num_layers\n        self.value_caches = [None] * self.num_layers\n        self.current_length = 0\n    \n    def update(self, layer_idx: int, new_keys: torch.Tensor, new_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Update cache with new keys and values, returning full concatenated tensors.\n        \n        Args:\n            layer_idx: Which transformer layer this cache belongs to\n            new_keys: New key tensor [batch_size, num_heads, 1, d_k]\n            new_values: New value tensor [batch_size, num_heads, 1, d_v]\n        \n        Returns:\n            Tuple of (full_keys, full_values) after concatenation\n        \"\"\"\n        if self.key_caches[layer_idx] is None:\n            # First generation step - initialize cache with new tensors\n            self.key_caches[layer_idx] = new_keys.clone()\n            self.value_caches[layer_idx] = new_values.clone()\n        else:\n            # Subsequent steps - concatenate with existing cache\n            self.key_caches[layer_idx] = torch.cat([self.key_caches[layer_idx], new_keys], dim=2)\n            self.value_caches[layer_idx] = torch.cat([self.value_caches[layer_idx], new_values], dim=2)\n        \n        # Update length after first layer (avoid counting multiple times)\n        if layer_idx == 0:\n            self.current_length += 1\n        \n        return self.key_caches[layer_idx], self.value_caches[layer_idx]\n    \n    def get_cache_info(self) -> Dict[str, Any]:\n        \"\"\"Return cache statistics for debugging and monitoring.\"\"\"\n        memory_usage = 0\n        for layer_idx in range(self.num_layers):\n            if self.key_caches[layer_idx] is not None:\n                memory_usage += self.key_caches[layer_idx].numel() * 4  # 4 bytes per float32\n                memory_usage += self.value_caches[layer_idx].numel() * 4\n        \n        return {\n            \"current_length\": self.current_length,\n            \"memory_usage_bytes\": memory_usage,\n            \"memory_usage_mb\": memory_usage / (1024 * 1024),\n            \"cached_layers\": sum(1 for cache in self.key_caches if cache is not None)\n        }\n\nclass SamplingStrategies:\n    \"\"\"Collection of token sampling methods for generation.\"\"\"\n    \n    @staticmethod\n    def greedy_sample(logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"Select token with highest probability.\"\"\"\n        return torch.argmax(logits, dim=-1)\n    \n    @staticmethod\n    def temperature_sample(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n        \"\"\"Sample with temperature scaling for controlled randomness.\"\"\"\n        if temperature < 1e-6:\n            return SamplingStrategies.greedy_sample(logits)\n        \n        scaled_logits = logits / temperature\n        probabilities = torch.softmax(scaled_logits, dim=-1)\n        return torch.multinomial(probabilities, num_samples=1).squeeze(-1)\n    \n    @staticmethod\n    def top_k_sample(logits: torch.Tensor, k: int, temperature: float = 1.0) -> torch.Tensor:\n        \"\"\"Sample from top-k most likely tokens.\"\"\"\n        if k <= 0 or k >= logits.size(-1):\n            return SamplingStrategies.temperature_sample(logits, temperature)\n        \n        # Get top-k values and indices\n        top_k_values, top_k_indices = torch.topk(logits, k, dim=-1)\n        \n        # Create mask for non-top-k tokens\n        mask = torch.full_like(logits, float('-inf'))\n        mask.scatter_(-1, top_k_indices, top_k_values)\n        \n        return SamplingStrategies.temperature_sample(mask, temperature)\n    \n    @staticmethod\n    def top_p_sample(logits: torch.Tensor, p: float, temperature: float = 1.0) -> torch.Tensor:\n        \"\"\"Sample from nucleus of tokens with cumulative probability p.\"\"\"\n        if p >= 1.0:\n            return SamplingStrategies.temperature_sample(logits, temperature)\n        \n        # Sort logits in descending order\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n        \n        # Compute cumulative probabilities\n        sorted_probs = torch.softmax(sorted_logits / temperature, dim=-1)\n        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n        \n        # Create mask for tokens beyond nucleus\n        nucleus_mask = cumulative_probs <= p\n        \n        # Always include at least the top token\n        nucleus_mask[..., 0] = True\n        \n        # Apply mask to sorted logits\n        filtered_logits = sorted_logits.clone()\n        filtered_logits[~nucleus_mask] = float('-inf')\n        \n        # Sample from filtered distribution\n        if temperature < 1e-6:\n            selected_indices = torch.zeros_like(sorted_indices[..., :1])\n        else:\n            filtered_probs = torch.softmax(filtered_logits / temperature, dim=-1)\n            selected_indices = torch.multinomial(filtered_probs, num_samples=1)\n        \n        # Map back to original vocabulary indices\n        return torch.gather(sorted_indices, -1, selected_indices).squeeze(-1)\n```\n\n#### Core Generation Logic (Skeleton for Implementation)\n\n```python\nclass TextGenerator:\n    \"\"\"Main class for autoregressive text generation with transformer models.\"\"\"\n    \n    def __init__(self, model: nn.Module, tokenizer: 'SimpleTokenizer', config: GenerationConfig):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.config = config\n        self.device = next(model.parameters()).device\n        \n        # Initialize KV cache based on model configuration\n        model_config = model.config  # Assumes model has config attribute\n        self.kv_cache = KVCache(\n            num_layers=model_config.num_layers,\n            num_heads=model_config.num_heads,\n            d_k=model_config.d_k,\n            d_v=model_config.d_v,\n            device=self.device\n        )\n    \n    def generate(self, prompt: str, max_new_tokens: int = None) -> str:\n        \"\"\"\n        Generate text continuation for given prompt using autoregressive sampling.\n        \n        Args:\n            prompt: Initial text to continue\n            max_new_tokens: Maximum number of tokens to generate (overrides config)\n        \n        Returns:\n            Generated text including original prompt\n        \"\"\"\n        # TODO 1: Encode prompt to token sequence and move to device\n        # TODO 2: Clear KV cache from any previous generation\n        # TODO 3: Set model to evaluation mode and disable gradients\n        # TODO 4: Initialize generation loop with encoded prompt as current sequence\n        # TODO 5: For each generation step up to max_length:\n        #   - Get model logits for current sequence (use KV caching if available)\n        #   - Apply repetition penalty to recently generated tokens\n        #   - Sample next token using configured sampling strategy\n        #   - Check if next token is EOS and break if so\n        #   - Append next token to current sequence\n        # TODO 6: Decode final token sequence back to text string\n        # TODO 7: Return generated text with proper handling of special tokens\n        \n        pass  # Implementation goes here\n    \n    def _apply_repetition_penalty(self, logits: torch.Tensor, generated_tokens: torch.Tensor, penalty: float) -> torch.Tensor:\n        \"\"\"\n        Apply repetition penalty to reduce probability of recently generated tokens.\n        \n        Args:\n            logits: Current token logits [batch_size, vocab_size]\n            generated_tokens: Previously generated token sequence [batch_size, seq_len]\n            penalty: Penalty multiplier for repeated tokens (< 1.0 reduces probability)\n        \n        Returns:\n            Modified logits with repetition penalty applied\n        \"\"\"\n        # TODO 1: Create penalty mask for tokens that appear in generated sequence\n        # TODO 2: Apply penalty multiplier to logits of repeated tokens\n        # TODO 3: Consider recency weighting (recent tokens get higher penalty)\n        # Hint: Use torch.gather and torch.scatter to modify specific token logits\n        \n        pass  # Implementation goes here\n    \n    def _sample_next_token(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Sample next token using configured sampling strategy.\n        \n        Args:\n            logits: Token logits from model [batch_size, vocab_size]\n        \n        Returns:\n            Selected token indices [batch_size]\n        \"\"\"\n        # TODO 1: Extract sampling parameters from self.config\n        # TODO 2: Apply appropriate sampling strategy based on configuration\n        # TODO 3: Handle edge cases (temperature=0, top_k=1, etc.)\n        # TODO 4: Return sampled token indices\n        # Hint: Use the SamplingStrategies static methods implemented above\n        \n        pass  # Implementation goes here\n    \n    def _update_kv_cache(self, layer_outputs: List[Tuple[torch.Tensor, torch.Tensor]]) -> None:\n        \"\"\"\n        Update KV cache with new key-value pairs from model forward pass.\n        \n        Args:\n            layer_outputs: List of (keys, values) tuples from each transformer layer\n        \"\"\"\n        # TODO 1: Iterate through layer outputs and update cache for each layer\n        # TODO 2: Handle cache initialization on first generation step\n        # TODO 3: Ensure proper tensor concatenation along sequence dimension\n        # TODO 4: Update cache length tracking\n        \n        pass  # Implementation goes here\n    \n    def generate_batch(self, prompts: List[str], max_new_tokens: int = None) -> List[str]:\n        \"\"\"\n        Generate text for multiple prompts in parallel (advanced feature).\n        \n        Args:\n            prompts: List of input prompts\n            max_new_tokens: Maximum tokens to generate per prompt\n        \n        Returns:\n            List of generated text strings\n        \"\"\"\n        # TODO 1: Tokenize and pad all prompts to same length\n        # TODO 2: Create batch tensor with proper padding masks\n        # TODO 3: Implement batched generation loop with per-sequence EOS handling\n        # TODO 4: Decode each sequence separately, handling padding appropriately\n        # TODO 5: Return list of generated strings in same order as input prompts\n        \n        pass  # Implementation goes here\n```\n\n#### Integration with Transformer Model\n\nThe generation system must integrate cleanly with the transformer model implemented in previous sections. The model needs slight modifications to support KV caching during generation:\n\n```python\n# Addition to MultiHeadAttention class to support KV caching\ndef forward_with_cache(self, x: torch.Tensor, mask: torch.Tensor = None, \n                      kv_cache: Optional[KVCache] = None, layer_idx: int = 0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass with optional KV caching for efficient generation.\n    \n    Returns:\n        Tuple of (attention_output, keys, values) for cache management\n    \"\"\"\n    # TODO 1: Compute Q, K, V projections for input tokens\n    # TODO 2: If using cache, only compute K, V for new tokens and concatenate with cache\n    # TODO 3: Perform attention computation with full K, V but return output for new tokens only\n    # TODO 4: Return attention output along with K, V tensors for cache updates\n    \n    pass  # Implementation goes here\n```\n\n#### Milestone Checkpoint\n\nAfter implementing text generation, verify functionality with these tests:\n\n**Basic Generation Test:**\n```python\ndef test_basic_generation():\n    # Load trained model and tokenizer\n    model = torch.load('trained_transformer.pt')\n    tokenizer = SimpleTokenizer.load('tokenizer.json')\n    \n    config = GenerationConfig(temperature=0.8, max_length=50)\n    generator = TextGenerator(model, tokenizer, config)\n    \n    result = generator.generate(\"The quick brown fox\")\n    print(f\"Generated: {result}\")\n    \n    # Should produce coherent continuation\n    assert len(result) > len(\"The quick brown fox\")\n    assert result.startswith(\"The quick brown fox\")\n```\n\n**Sampling Strategy Test:**\n```python\ndef test_sampling_strategies():\n    logits = torch.tensor([[1.0, 2.0, 0.5, 3.0, 1.5]])  # Batch size 1\n    \n    # Test greedy (should always select index 3)\n    greedy_token = SamplingStrategies.greedy_sample(logits)\n    assert greedy_token.item() == 3\n    \n    # Test temperature (should vary across runs)\n    temp_tokens = [SamplingStrategies.temperature_sample(logits, 1.0).item() for _ in range(10)]\n    assert len(set(temp_tokens)) > 1  # Should have some variety\n    \n    print(\"All sampling strategy tests passed!\")\n```\n\n**KV Cache Test:**\n```python\ndef test_kv_cache():\n    cache = KVCache(num_layers=2, num_heads=4, d_k=32, d_v=32, device=torch.device('cpu'))\n    \n    # Simulate cache updates\n    keys1 = torch.randn(1, 4, 1, 32)\n    values1 = torch.randn(1, 4, 1, 32)\n    \n    full_keys, full_values = cache.update(0, keys1, values1)\n    assert full_keys.shape == (1, 4, 1, 32)\n    assert cache.current_length == 1\n    \n    # Add second token\n    keys2 = torch.randn(1, 4, 1, 32)\n    values2 = torch.randn(1, 4, 1, 32)\n    \n    full_keys, full_values = cache.update(0, keys2, values2)\n    assert full_keys.shape == (1, 4, 2, 32)\n    assert cache.current_length == 2\n    \n    print(\"KV cache tests passed!\")\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|-------------|-----------|-----|\n| Generation produces only padding tokens | EOS token generated immediately | Check if prompt encodes to empty sequence | Verify tokenizer handles prompt correctly |\n| Repetitive text loops | Greedy decoding with deterministic patterns | Try higher temperature or top-p sampling | Add repetition penalty or use stochastic sampling |\n| Generation is very slow | No KV caching or inefficient implementation | Profile generation loop timing | Implement KV caching and batch operations |\n| CUDA out of memory during generation | Cache memory grows too large | Monitor cache memory usage | Implement cache size limits or use smaller batch sizes |\n| Generated text ends abruptly | EOS token sampled unexpectedly | Check if EOS appears in top-k/top-p candidates | Filter EOS from sampling or adjust sampling parameters |\n| Incoherent text with high perplexity | Model not properly trained or wrong sampling | Evaluate model perplexity on validation set | Verify training convergence and adjust sampling strategy |\n\nThe text generation system represents the final piece of the transformer implementation puzzle. When working correctly, it should produce coherent continuations of input prompts, demonstrate sensitivity to sampling parameters, and generate text significantly faster with KV caching enabled. The combination of proper sampling strategies and efficient caching transforms the transformer from a training artifact into a practical text generation tool.\n\n![Autoregressive Generation Process](./diagrams/generation-flow.svg)\n\n### Implementation Guidance\n\nThe text generation implementation requires careful coordination between sampling strategies, cache management, and the core transformer model. Focus on building a robust foundation with simple greedy generation first, then add sophisticated sampling and caching optimizations.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Basic Generation | Greedy decoding with temperature fallback | Multi-strategy sampling with all options |\n| Performance | CPU generation with manual loops | GPU acceleration with batched operations |\n| Memory Management | Unlimited cache growth | Memory-bounded cache with sliding windows |\n| Sampling | Temperature and top-k only | Full nucleus sampling with repetition penalties |\n| Error Handling | Basic shape validation | Comprehensive error recovery and graceful degradation |\n\n#### File Organization\n\n```\ntransformer/\n├── generation/\n│   ├── __init__.py              # Export TextGenerator and GenerationConfig\n│   ├── generator.py             # Main TextGenerator class implementation\n│   ├── sampling.py              # All sampling strategy implementations\n│   ├── cache.py                 # KVCache class and memory management\n│   └── utils.py                 # Generation utilities and helper functions\n├── models/\n│   ├── transformer.py          # Core transformer (modified for cache support)\n│   └── attention.py             # MultiHeadAttention with caching methods\n├── training/\n│   └── tokenizer.py             # SimpleTokenizer for encoding/decoding\n└── examples/\n    └── generate_text.py         # Complete generation example script\n```\n\n#### Complete KV Cache Infrastructure\n\n```python\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple, List, Dict, Any, Union\nfrom dataclasses import dataclass\nimport warnings\n\n@dataclass\nclass GenerationConfig:\n    \"\"\"Configuration parameters for text generation.\"\"\"\n    temperature: float = 1.0\n    top_k: int = 0  # 0 disables top-k\n    top_p: float = 1.0  # 1.0 disables top-p\n    max_length: int = 100\n    pad_token_id: int = 0\n    eos_token_id: int = 1\n    repetition_penalty: float = 1.0\n    length_penalty: float = 1.0\n\nclass KVCache:\n    \"\"\"Efficient key-value caching for autoregressive generation.\"\"\"\n    \n    def __init__(self, num_layers: int, num_heads: int, d_k: int, d_v: int, device: torch.device, max_length: int = 1024):\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.d_k = d_k\n        self.d_v = d_v\n        self.device = device\n        self.max_length = max_length\n        \n        # Pre-allocate cache tensors for efficiency\n        self.key_caches: List[Optional[torch.Tensor]] = [None] * num_layers\n        self.value_caches: List[Optional[torch.Tensor]] = [None] * num_layers\n        self.current_length = 0\n        self._cache_initialized = False\n    \n    def initialize_cache(self, batch_size: int):\n        \"\"\"Pre-allocate cache tensors to avoid repeated allocations.\"\"\"\n        self.key_caches = [\n            torch.zeros(batch_size, self.num_heads, self.max_length, self.d_k, device=self.device)\n            for _ in range(self.num_layers)\n        ]\n        self.value_caches = [\n            torch.zeros(batch_size, self.num_heads, self.max_length, self.d_v, device=self.device)\n            for _ in range(self.num_layers)\n        ]\n        self._cache_initialized = True\n        self.current_length = 0\n    \n    def clear(self):\n        \"\"\"Reset cache state for new generation sequence.\"\"\"\n        self.current_length = 0\n        self._cache_initialized = False\n        # Don't deallocate tensors - just reset length for reuse\n    \n    def update(self, layer_idx: int, new_keys: torch.Tensor, new_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Update cache with new key-value pairs and return current cached values.\"\"\"\n        batch_size, num_heads, seq_len, d_k = new_keys.shape\n        \n        if not self._cache_initialized:\n            self.initialize_cache(batch_size)\n        \n        if self.current_length + seq_len > self.max_length:\n            warnings.warn(f\"Sequence length {self.current_length + seq_len} exceeds cache capacity {self.max_length}\")\n            # Implement sliding window or raise error based on requirements\n            return new_keys, new_values\n        \n        # Update cache tensors in-place\n        end_pos = self.current_length + seq_len\n        self.key_caches[layer_idx][:, :, self.current_length:end_pos, :] = new_keys\n        self.value_caches[layer_idx][:, :, self.current_length:end_pos, :] = new_values\n        \n        # Return view of cached data up to current position\n        cached_keys = self.key_caches[layer_idx][:, :, :end_pos, :]\n        cached_values = self.value_caches[layer_idx][:, :, :end_pos, :]\n        \n        # Update length only after first layer to avoid double counting\n        if layer_idx == 0:\n            self.current_length = end_pos\n        \n        return cached_keys, cached_values\n    \n    def get_memory_info(self) -> Dict[str, float]:\n        \"\"\"Return detailed memory usage statistics.\"\"\"\n        if not self._cache_initialized:\n            return {\"memory_mb\": 0.0, \"utilization\": 0.0}\n        \n        total_elements = 0\n        for cache in self.key_caches + self.value_caches:\n            if cache is not None:\n                total_elements += cache.numel()\n        \n        memory_bytes = total_elements * 4  # Assume float32\n        memory_mb = memory_bytes / (1024 * 1024)\n        utilization = self.current_length / self.max_length if self.max_length > 0 else 0.0\n        \n        return {\n            \"memory_mb\": memory_mb,\n            \"utilization\": utilization,\n            \"current_length\": self.current_length,\n            \"max_length\": self.max_length\n        }\n\nclass SamplingStrategies:\n    \"\"\"Comprehensive collection of token sampling methods.\"\"\"\n    \n    @staticmethod\n    def greedy_sample(logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"Deterministic selection of highest probability token.\"\"\"\n        return torch.argmax(logits, dim=-1, keepdim=True)\n    \n    @staticmethod\n    def temperature_sample(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n        \"\"\"Temperature-controlled stochastic sampling.\"\"\"\n        # Handle edge case of zero temperature\n        if temperature < 1e-6:\n            return SamplingStrategies.greedy_sample(logits)\n        \n        # Apply temperature scaling\n        scaled_logits = logits / temperature\n        \n        # Compute probabilities with numerical stability\n        max_logits = torch.max(scaled_logits, dim=-1, keepdim=True)[0]\n        stable_logits = scaled_logits - max_logits\n        probabilities = torch.softmax(stable_logits, dim=-1)\n        \n        # Sample from distribution\n        return torch.multinomial(probabilities, num_samples=1)\n    \n    @staticmethod\n    def top_k_sample(logits: torch.Tensor, k: int, temperature: float = 1.0) -> torch.Tensor:\n        \"\"\"Sample from top-k highest probability tokens.\"\"\"\n        if k <= 0 or k >= logits.size(-1):\n            return SamplingStrategies.temperature_sample(logits, temperature)\n        \n        # Get top-k values and their indices\n        top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n        \n        # Create filtered logit tensor\n        filtered_logits = torch.full_like(logits, float('-inf'))\n        filtered_logits.scatter_(-1, top_k_indices, top_k_logits)\n        \n        return SamplingStrategies.temperature_sample(filtered_logits, temperature)\n    \n    @staticmethod\n    def top_p_sample(logits: torch.Tensor, p: float, temperature: float = 1.0) -> torch.Tensor:\n        \"\"\"Nucleus sampling with cumulative probability threshold.\"\"\"\n        if p >= 1.0:\n            return SamplingStrategies.temperature_sample(logits, temperature)\n        \n        # Sort in descending order\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n        \n        # Apply temperature to sorted logits\n        if temperature != 1.0:\n            sorted_logits = sorted_logits / temperature\n        \n        # Compute cumulative probabilities\n        sorted_probs = torch.softmax(sorted_logits, dim=-1)\n        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n        \n        # Create nucleus mask (always include first token)\n        nucleus_mask = cumulative_probs <= p\n        nucleus_mask[..., 0] = True  # Always include top token\n        \n        # Apply mask\n        masked_logits = sorted_logits.clone()\n        masked_logits[~nucleus_mask] = float('-inf')\n        \n        # Sample from nucleus\n        nucleus_probs = torch.softmax(masked_logits, dim=-1)\n        selected_sorted_indices = torch.multinomial(nucleus_probs, num_samples=1)\n        \n        # Map back to original indices\n        return torch.gather(sorted_indices, -1, selected_sorted_indices)\n```\n\n#### Core Generation Logic Implementation Skeleton\n\n```python\nclass TextGenerator:\n    \"\"\"Main autoregressive text generation system.\"\"\"\n    \n    def __init__(self, model: nn.Module, tokenizer: 'SimpleTokenizer', config: GenerationConfig):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.config = config\n        self.device = next(model.parameters()).device\n        \n        # Extract model configuration for cache initialization\n        self.model_config = getattr(model, 'config', None)\n        if self.model_config is None:\n            raise ValueError(\"Model must have 'config' attribute with layer and attention parameters\")\n        \n        self.kv_cache = KVCache(\n            num_layers=self.model_config.num_layers,\n            num_heads=self.model_config.num_heads,\n            d_k=self.model_config.d_k,\n            d_v=self.model_config.d_v,\n            device=self.device\n        )\n    \n    def generate(self, prompt: str, max_new_tokens: Optional[int] = None) -> str:\n        \"\"\"Generate text continuation from prompt using autoregressive sampling.\"\"\"\n        # TODO 1: Handle empty prompt edge case\n        # TODO 2: Encode prompt using tokenizer.encode() and convert to tensor\n        # TODO 3: Move input tensor to model device\n        # TODO 4: Clear KV cache and set model to eval mode\n        # TODO 5: Initialize generation variables (generated_tokens, current_length)\n        # TODO 6: Main generation loop:\n        #   - Forward pass through model (handle caching)\n        #   - Extract logits for last position\n        #   - Apply repetition penalty if configured\n        #   - Sample next token using _sample_next_token()\n        #   - Check for EOS token and break if found\n        #   - Append new token to sequence\n        #   - Check length limits\n        # TODO 7: Decode final sequence back to text\n        # TODO 8: Handle special token filtering in output\n        \n        pass  # Your implementation here\n    \n    def _sample_next_token(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"Apply configured sampling strategy to select next token.\"\"\"\n        # TODO 1: Extract current sampling configuration\n        # TODO 2: Handle multiple sampling strategies in priority order:\n        #   - If top_k > 0: use top_k_sample\n        #   - Else if top_p < 1.0: use top_p_sample  \n        #   - Else: use temperature_sample\n        # TODO 3: Handle edge cases (temperature=0, invalid parameters)\n        # TODO 4: Return sampled token indices\n        \n        pass  # Your implementation here\n    \n    def _apply_repetition_penalty(self, logits: torch.Tensor, recent_tokens: torch.Tensor) -> torch.Tensor:\n        \"\"\"Reduce probability of recently generated tokens to prevent repetition.\"\"\"\n        # TODO 1: Check if repetition penalty is enabled (penalty != 1.0)\n        # TODO 2: Extract unique tokens from recent generation window\n        # TODO 3: Apply penalty by dividing/multiplying logits at those positions\n        # TODO 4: Consider implementing recency weighting (recent tokens penalized more)\n        # TODO 5: Return modified logits tensor\n        \n        pass  # Your implementation here\n    \n    def _model_forward_with_cache(self, input_ids: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass through model with KV caching support.\"\"\"\n        # TODO 1: Check if model supports caching (has forward_with_cache method)\n        # TODO 2: If caching supported:\n        #   - Call model.forward_with_cache() with current cache\n        #   - Update cache with returned key-value pairs\n        #   - Return logits for new tokens only\n        # TODO 3: If no caching:\n        #   - Standard forward pass with full input sequence\n        #   - Return logits for all positions\n        # TODO 4: Handle device placement and gradient context\n        \n        pass  # Your implementation here\n\ndef create_generation_example():\n    \"\"\"Complete example demonstrating text generation usage.\"\"\"\n    # This would normally load a trained model\n    # For testing, create a minimal model structure\n    \n    from models.transformer import TransformerModel  # Your transformer implementation\n    from training.tokenizer import SimpleTokenizer\n    \n    # Initialize tokenizer and model\n    tokenizer = SimpleTokenizer(vocab_size=1000)  # Your tokenizer\n    model = TransformerModel(config=your_config)  # Your trained model\n    \n    # Create generation configuration\n    gen_config = GenerationConfig(\n        temperature=0.8,\n        top_k=50,\n        top_p=0.9,\n        max_length=100,\n        repetition_penalty=1.1\n    )\n    \n    # Initialize generator\n    generator = TextGenerator(model, tokenizer, gen_config)\n    \n    # Generate text\n    prompt = \"The future of artificial intelligence\"\n    generated_text = generator.generate(prompt, max_new_tokens=50)\n    \n    print(f\"Prompt: {prompt}\")\n    print(f\"Generated: {generated_text}\")\n    \n    return generated_text\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    # Test sampling strategies\n    test_logits = torch.tensor([[1.0, 3.0, 2.0, 0.5, 2.5]])\n    \n    print(\"Testing sampling strategies:\")\n    print(f\"Greedy: {SamplingStrategies.greedy_sample(test_logits)}\")\n    print(f\"Temperature (0.5): {SamplingStrategies.temperature_sample(test_logits, 0.5)}\")\n    print(f\"Top-k (3): {SamplingStrategies.top_k_sample(test_logits, 3)}\")\n    print(f\"Top-p (0.8): {SamplingStrategies.top_p_sample(test_logits, 0.8)}\")\n    \n    # Test cache functionality\n    cache = KVCache(num_layers=2, num_heads=4, d_k=64, d_v=64, device=torch.device('cpu'))\n    print(f\"Initial cache memory: {cache.get_memory_info()}\")\n    \n    # Would continue with full generation example...\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the text generation system, verify functionality with these comprehensive tests:\n\n**Test 1: Basic Generation Functionality**\n```bash\npython -c \"\nfrom generation.generator import TextGenerator, GenerationConfig\n# Load your trained model and tokenizer\n# Test basic generation works and produces reasonable output\n\"\n```\n\nExpected behavior: Generator produces coherent text continuations of reasonable length, respects max_length parameter, and handles EOS tokens correctly.\n\n**Test 2: Sampling Strategy Verification**\n```bash\npython test_sampling.py\n```\n\nExpected behavior: Different sampling strategies produce different outputs, temperature affects randomness level, top-k/top-p filtering works as expected, and edge cases (temperature=0) handle gracefully.\n\n**Test 3: KV Cache Performance**\n```bash\npython benchmark_generation.py --with-cache --without-cache\n```\n\nExpected behavior: Generation with caching is significantly faster (3-10x for longer sequences), produces identical outputs to non-cached generation, and memory usage scales reasonably with sequence length.\n\n#### Language-Specific Implementation Notes\n\n**PyTorch-specific optimizations:**\n- Use `torch.no_grad()` context during generation to prevent gradient computation\n- Implement `model.eval()` to disable dropout and batch normalization training behavior  \n- Utilize `torch.multinomial()` for efficient sampling from probability distributions\n- Apply `tensor.to(device)` consistently for proper GPU utilization\n\n**Memory management strategies:**\n- Pre-allocate cache tensors to avoid repeated memory allocation during generation\n- Use `torch.cat()` efficiently by pre-sizing tensors when possible\n- Monitor GPU memory usage and implement cache size limits for long sequences\n- Consider using `torch.cuda.empty_cache()` between generation sessions\n\n#### Debugging Common Generation Issues\n\n| Problem | Symptoms | Diagnostic Steps | Solution |\n|---------|----------|------------------|----------|\n| Repetitive text loops | Same phrase repeated infinitely | Check repetition penalty, try higher temperature | Implement repetition penalty or use stochastic sampling |\n| Generation stops immediately | Output equals input prompt exactly | Check if EOS token generated on first step | Verify model training, check prompt encoding |\n| Slow generation speed | Each token takes several seconds | Profile forward pass timing | Implement KV caching, use GPU acceleration |\n| Incoherent gibberish output | Random characters or broken words | Check model training loss convergence | Verify model training completed successfully |\n| CUDA memory errors | Generation crashes with OOM | Monitor memory usage during generation | Reduce batch size or implement cache memory limits |\n| Inconsistent output quality | Some prompts work well, others poorly | Test various prompts and sampling settings | Adjust sampling parameters, check training data coverage |\n\n\n## Component Interactions and Data Flow\n\n> **Milestone(s):** All milestones - understanding how data flows through the transformer architecture from tokens to generated text, with different patterns for training (Milestone 3) and inference (Milestone 4)\n\nUnderstanding how components communicate and data flows through a transformer is essential for successful implementation. Unlike traditional neural networks with simple sequential processing, transformers involve complex interactions between attention mechanisms, normalization layers, and residual connections that must be orchestrated precisely. The data flow patterns differ significantly between training and inference modes, requiring careful design of component interfaces and state management.\n\n### Mental Model: The Assembly Line\n\nThink of the transformer as a sophisticated assembly line in a factory. Raw materials (tokens) enter at one end and flow through multiple workstations (transformer blocks). At each workstation, specialized workers (attention heads) examine the materials from different perspectives, while quality control inspectors (layer normalization) ensure consistency. Conveyor belts (residual connections) carry materials forward while allowing workers to compare current work with the original materials. The assembly line operates differently during training (processing many products simultaneously with feedback from the end result) versus production (creating one product at a time based on customer specifications).\n\nThis mental model captures the key aspects of transformer data flow: parallel processing within layers, sequential processing across layers, quality control at each stage, and different operational modes for training versus inference.\n\n### Forward Pass Sequence\n\nThe forward pass represents the core data transformation pipeline that converts input tokens into probability distributions over the vocabulary. Understanding this sequence is crucial because every component must maintain tensor shape consistency while performing its specialized function.\n\n#### Token Input and Embedding\n\nThe forward pass begins when integer token IDs enter the transformer through the embedding layer. The `SimpleTokenizer` has already converted raw text into a sequence of integers, each representing a token in the vocabulary. These token IDs have shape `[batch_size, seq_length]` and contain values in the range `[0, vocab_size-1]`.\n\nThe embedding layer performs the first critical transformation by converting discrete token IDs into dense vector representations. This learnable lookup table has dimensions `[vocab_size, d_model]`, allowing each token to be represented as a `d_model`-dimensional vector. After embedding lookup, the tensor shape becomes `[batch_size, seq_length, d_model]`, which remains constant throughout most of the transformer.\n\n> **Decision: Learnable vs Fixed Embeddings**\n> - **Context**: Token representations can use either randomly initialized learnable embeddings or pre-trained fixed vectors\n> - **Options Considered**: Random initialization, pre-trained GloVe/Word2Vec, learned from scratch\n> - **Decision**: Learnable embeddings trained from scratch\n> - **Rationale**: Allows the model to learn optimal token representations for the specific task and vocabulary, avoiding distribution mismatch with pre-trained embeddings\n> - **Consequences**: Requires more training data and time but provides better task-specific representations\n\n#### Positional Information Integration\n\nAfter token embedding, positional information must be added because the self-attention mechanism is inherently position-agnostic. The transformer needs to understand token order to generate coherent text. Positional encodings are added directly to the token embeddings, maintaining the `[batch_size, seq_length, d_model]` shape.\n\n| Position Integration Method | Description | Shape Preservation | Learning Required |\n|----------------------------|-------------|-------------------|-------------------|\n| Sinusoidal Encodings | Fixed mathematical patterns based on position | Yes | No |\n| Learnable Positional Embeddings | Trained position-specific vectors | Yes | Yes |\n| Relative Position Encodings | Position differences in attention computation | Yes | Yes |\n\nThe combined token and positional representations now contain both semantic and structural information needed for the transformer blocks.\n\n#### Multi-Layer Transformer Processing\n\nThe heart of the forward pass occurs as data flows through the stack of transformer blocks. Each `TransformerBlock` performs identical operations but with different learned parameters. The processing within each block follows a precise sequence that maintains gradient flow and numerical stability.\n\n**Within Each Transformer Block:**\n\n1. **Pre-Normalization**: Input tensors first pass through `LayerNorm`, which normalizes activations across the `d_model` dimension for each token independently. This stabilizes training by ensuring activations maintain reasonable magnitudes.\n\n2. **Multi-Head Attention**: The normalized input flows into the `MultiHeadAttention` module, where it undergoes the most computationally intensive transformation. The attention mechanism computes relevance-weighted summaries of the entire sequence for each token position.\n\n3. **First Residual Connection**: The attention output is added to the original block input (before normalization), allowing gradient flow and preserving information from earlier layers.\n\n4. **Second Pre-Normalization**: The result passes through another `LayerNorm` layer, preparing for the feed-forward processing.\n\n5. **Feed-Forward Network**: The `FeedForwardNetwork` applies position-wise transformations, expanding to `ffn_expansion * d_model` dimensions internally before projecting back to `d_model`.\n\n6. **Second Residual Connection**: The FFN output is added to the input of the FFN sublayer, completing the transformer block processing.\n\nThroughout this sequence, the tensor shape remains `[batch_size, seq_length, d_model]`, ensuring compatibility between all components.\n\n#### Attention Computation Flow\n\nWithin the attention mechanism, data undergoes several transformations that deserve detailed explanation due to their complexity and importance for understanding transformer behavior.\n\n| Attention Step | Input Shape | Output Shape | Operation |\n|----------------|-------------|--------------|-----------|\n| Q/K/V Projection | `[batch_size, seq_length, d_model]` | `[batch_size, seq_length, d_model]` | Linear transformations |\n| Head Reshaping | `[batch_size, seq_length, d_model]` | `[batch_size, num_heads, seq_length, d_k]` | Tensor reshaping |\n| Attention Scores | `[batch_size, num_heads, seq_length, d_k]` | `[batch_size, num_heads, seq_length, seq_length]` | Q @ K^T / sqrt(d_k) |\n| Causal Masking | `[batch_size, num_heads, seq_length, seq_length]` | `[batch_size, num_heads, seq_length, seq_length]` | Apply mask before softmax |\n| Attention Weights | `[batch_size, num_heads, seq_length, seq_length]` | `[batch_size, num_heads, seq_length, seq_length]` | Softmax normalization |\n| Weighted Values | `[batch_size, num_heads, seq_length, seq_length]` | `[batch_size, num_heads, seq_length, d_v]` | Attention @ V |\n| Head Concatenation | `[batch_size, num_heads, seq_length, d_v]` | `[batch_size, seq_length, d_model]` | Reshape and concatenate |\n| Output Projection | `[batch_size, seq_length, d_model]` | `[batch_size, seq_length, d_model]` | Final linear transformation |\n\nThe `create_causal_mask` function generates a lower triangular mask that prevents tokens from attending to future positions, maintaining the autoregressive property essential for language modeling.\n\n#### Output Head and Logit Generation\n\nAfter processing through all transformer blocks, the final hidden states have shape `[batch_size, seq_length, d_model]`. These rich contextual representations must be converted into probability distributions over the vocabulary for next-token prediction.\n\nThe output head consists of a final `LayerNorm` followed by a linear projection layer with weight matrix shape `[d_model, vocab_size]`. This transformation produces logits with shape `[batch_size, seq_length, vocab_size]`, where each position contains unnormalized scores for every token in the vocabulary.\n\nDuring training, these logits are compared against target tokens using cross-entropy loss. During inference, only the logits for the final sequence position are used for next-token prediction, as previous positions already contain known tokens.\n\n#### Data Flow Dependencies\n\nThe forward pass involves several critical dependencies that must be respected for correct implementation:\n\n| Dependency Type | Source Component | Target Component | Requirement |\n|----------------|------------------|------------------|-------------|\n| Shape Consistency | All layers | Sequential layers | Maintain `[batch_size, seq_length, d_model]` |\n| Causal Ordering | Attention mask | Attention computation | Prevent future information leakage |\n| Gradient Flow | Residual connections | Backward pass | Preserve gradients across deep layers |\n| Numerical Stability | Layer normalization | All computations | Prevent activation explosion/vanishing |\n| Memory Efficiency | Attention computation | Training/inference | Quadratic memory scaling with sequence length |\n\nUnderstanding these dependencies helps identify potential implementation issues and optimization opportunities.\n\n#### Common Forward Pass Pitfalls\n\n⚠️ **Pitfall: Inconsistent Tensor Shapes**\nMany implementations fail because tensor shapes don't match between components. The most common issue is incorrectly computing attention head dimensions where `d_k * num_heads` must equal `d_model`. Debug by printing tensor shapes after each major operation and verifying they match expected dimensions from the configuration.\n\n⚠️ **Pitfall: Missing Causal Mask Application**\nForgetting to apply the causal mask or applying it after softmax instead of before breaks the autoregressive property. The mask must set future position scores to negative infinity before the softmax operation. Verify by checking that attention weights for position `i` sum to zero for all positions `j > i`.\n\n⚠️ **Pitfall: Incorrect Residual Connection Placement**\nResidual connections must add the input to each sublayer (before normalization) to its output. Adding normalized inputs breaks gradient flow and hurts training stability. The correct pattern is: `output = sublayer(norm(x)) + x`, not `output = sublayer(x) + norm(x)`.\n\n⚠️ **Pitfall: Attention Score Scaling Missing**\nOmitting the `1/sqrt(d_k)` scaling factor in attention computation causes training instability as sequence length or model size increases. Without scaling, attention weights become too sharp, leading to vanishing gradients. Always verify that scores are divided by `sqrt(d_k)` before softmax.\n\n### Training vs Inference Flows\n\nThe transformer architecture supports two fundamentally different operational modes that require distinct data flow patterns and component interactions. Understanding these differences is crucial for implementing both training loops and text generation systems effectively.\n\n#### Training Flow Characteristics\n\nDuring training, the transformer processes complete sequences in parallel using **teacher forcing**, where the model learns to predict each token given all previous tokens in the ground truth sequence. This parallel processing enables efficient training on modern hardware but requires careful handling of the causal mask and label shifting.\n\n**Teacher Forcing Data Flow:**\n\nTraining begins with complete input sequences of shape `[batch_size, seq_length]` containing actual tokens from the training dataset. The `TextDataset` provides these sequences along with corresponding target sequences that are shifted by one position for next-token prediction. The key insight is that during training, the model can process all positions simultaneously because we know all the correct previous tokens.\n\nThe forward pass processes the entire sequence through all transformer blocks in parallel. Each token at position `i` attends only to positions `0` through `i-1` due to the causal mask, but computationally, all positions are processed together. This parallel processing dramatically reduces training time compared to sequential generation.\n\n**Training-Specific Component Interactions:**\n\n| Component | Training Behavior | Key Characteristics |\n|-----------|------------------|-------------------|\n| `TextDataset` | Returns `(input_tensor, target_tensor)` pairs | Handles sequence padding and batch creation |\n| `MultiHeadAttention` | Processes full sequences with causal masking | Computes attention for all positions simultaneously |\n| Cross-entropy Loss | Computes loss across all sequence positions | Uses label smoothing and padding mask |\n| Optimizer | Updates parameters based on averaged gradients | Includes gradient clipping and weight decay |\n| `LayerNorm` | Uses batch statistics for normalization | Running statistics updated during training |\n\nThe loss computation deserves special attention in training flow. The model produces logits for every position in the sequence, but the loss is only computed for positions where we have valid target tokens (excluding padding). The `_compute_loss` method masks padded positions to prevent them from contributing to gradient updates.\n\n**Gradient Flow in Training:**\n\nTraining involves backward pass computation where gradients flow from the loss through all transformer blocks back to the embeddings. The residual connections play a crucial role here, allowing gradients to flow both through the transformer blocks and around them via skip connections. This prevents vanishing gradients that would otherwise make training deep transformers impossible.\n\n> The residual connections create multiple gradient pathways through the network. Without them, gradients must pass through many matrix multiplications and nonlinearities, causing them to vanish exponentially. With residuals, gradients can take shorter paths directly to earlier layers, maintaining training signal throughout the network depth.\n\n#### Inference Flow Characteristics\n\nInference operates fundamentally differently because the model must generate tokens sequentially without access to future ground truth tokens. This **autoregressive generation** requires the model to use its own predictions as input for subsequent token predictions, creating a sequential dependency that cannot be parallelized.\n\n**Sequential Generation Data Flow:**\n\nInference begins with a prompt sequence and generates new tokens one at a time. At each generation step, the model processes the entire sequence up to the current position to predict the next token. This creates a pattern where the sequence length grows with each iteration, requiring careful memory management and optimization.\n\nThe `TextGenerator` orchestrates this process by maintaining the growing sequence and calling the model repeatedly. Each forward pass processes a longer sequence than the previous one, making naive implementation computationally expensive due to redundant attention computations for previously processed tokens.\n\n**Inference-Specific Optimizations:**\n\n| Optimization | Purpose | Implementation |\n|-------------|---------|----------------|\n| KV Caching | Avoid recomputing attention for previous tokens | Store key/value tensors from previous steps |\n| Batch Inference | Generate multiple sequences simultaneously | Process multiple prompts in parallel |\n| Memory Management | Handle growing sequence lengths efficiently | Limit maximum generation length |\n| Sampling Strategies | Control randomness and quality of generation | Temperature, top-k, top-p sampling |\n\nThe `KVCache` optimization is particularly important for inference efficiency. Instead of recomputing attention keys and values for all previous tokens at each generation step, the cache stores these tensors and only computes them for the new token. This reduces the computational complexity from quadratic to linear in the sequence length.\n\n**KV Cache Data Flow:**\n\nDuring the first generation step, the model computes and caches key/value tensors for all attention heads and layers. For subsequent steps, the cache provides stored tensors for previous positions and only computes new tensors for the current position. The `update` method handles concatenating cached and new tensors to form complete key/value matrices for attention computation.\n\n#### Mode-Specific Component Behavior\n\nComponents must adapt their behavior based on whether they're operating in training or inference mode. This adaptation affects both computational patterns and memory usage.\n\n| Component | Training Mode | Inference Mode |\n|-----------|---------------|----------------|\n| `MultiHeadAttention` | Processes full sequences in parallel | Processes one new token, uses KV cache |\n| `LayerNorm` | Uses current batch statistics | Uses running statistics from training |\n| Dropout | Randomly zeros activations | Disabled (no randomness) |\n| Causal Mask | Applied to full attention matrix | Only masks new token position |\n| Memory Usage | Fixed for batch and sequence length | Grows with generated sequence length |\n\n**Training Loop vs Generation Loop:**\n\nThe training loop and generation loop have different control flow patterns that reflect their distinct objectives:\n\n**Training Loop Pattern:**\n1. Load batch of sequences from `DataLoader`\n2. Forward pass through entire transformer stack\n3. Compute loss against target tokens\n4. Backward pass to compute gradients\n5. Optimizer step to update parameters\n6. Repeat for all batches in epoch\n\n**Generation Loop Pattern:**\n1. Initialize sequence with prompt tokens\n2. Forward pass to get logits for next position\n3. Apply sampling strategy to select next token\n4. Append token to sequence\n5. Update KV cache with new key/value tensors\n6. Repeat until end condition (max length or EOS token)\n\n#### Memory and Computational Trade-offs\n\nThe different flow patterns create distinct resource usage profiles that must be considered during implementation:\n\n| Resource | Training | Inference | Implication |\n|----------|----------|-----------|-------------|\n| Memory | Fixed per batch | Grows with sequence | Need generation length limits |\n| Computation | Parallel across positions | Sequential token generation | Training is more hardware-efficient |\n| Latency | Batch processing delay | Per-token delay | Inference optimized for individual requests |\n| Throughput | High (parallel processing) | Lower (sequential dependency) | Training can process more tokens/second |\n\n#### Error Handling Differences\n\nTraining and inference require different error handling strategies due to their distinct failure modes:\n\n**Training Error Patterns:**\n- Gradient explosion (handled by gradient clipping)\n- Loss spikes (handled by learning rate scheduling)\n- Memory overflow (handled by batch size adjustment)\n- Convergence issues (handled by validation monitoring)\n\n**Inference Error Patterns:**\n- Repetitive generation (handled by repetition penalty)\n- Incoherent output (handled by sampling strategy tuning)\n- Memory growth (handled by maximum length limits)\n- Cache inconsistency (handled by cache validation)\n\nUnderstanding these operational differences ensures robust implementation of both training and inference systems.\n\n#### Common Mode-Specific Pitfalls\n\n⚠️ **Pitfall: Inconsistent Model State Between Modes**\nForgetting to call `model.eval()` during inference or `model.train()` during training causes components like dropout and layer normalization to behave incorrectly. Always explicitly set the model mode and verify that components respond appropriately to the training/evaluation state.\n\n⚠️ **Pitfall: KV Cache Shape Mismatches**\nDuring inference, the KV cache must maintain consistent tensor shapes as sequences grow. Common errors include incorrect concatenation dimensions or failing to account for batch size in cache tensors. Debug by logging cache tensor shapes after each update and verifying they match expected attention input shapes.\n\n⚠️ **Pitfall: Label Shifting Confusion in Training**\nThe training data loader must shift labels by one position relative to inputs for next-token prediction. Input sequence `[A, B, C, D]` should have target sequence `[B, C, D, EOS]`. Incorrect shifting causes the model to learn identity mapping instead of next-token prediction.\n\n⚠️ **Pitfall: Memory Leaks in Long Generation**\nInference with very long sequences can cause memory leaks if intermediate tensors aren't properly released. The KV cache and attention computations create large temporary tensors that must be explicitly cleared. Implement maximum generation length limits and monitor memory usage during long sequences.\n\n### Implementation Guidance\n\nThe component interaction patterns require careful orchestration of data flow and state management. This implementation guidance provides concrete code structure for managing the complex interactions between training and inference modes.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Model State Management | Manual mode switching with `model.train()/eval()` | Context managers for automatic mode handling |\n| Memory Profiling | Manual tensor shape logging | PyTorch profiler with memory tracking |\n| KV Cache Implementation | Simple dictionary storage | Optimized circular buffers |\n| Batch Processing | Fixed batch sizes | Dynamic batching with padding |\n\n#### Recommended File Structure\n\nUnderstanding component interactions requires organizing code to clearly separate concerns while enabling efficient data flow:\n\n```\ntransformer/\n  core/\n    transformer.py          ← Main model orchestrating all components\n    attention.py           ← MultiHeadAttention with KV cache support\n    transformer_block.py   ← TransformerBlock with mode-aware processing\n    generation.py          ← TextGenerator with sampling strategies\n  training/\n    trainer.py            ← TransformerTrainer managing training flow\n    data.py              ← TextDataset and data loading utilities\n  utils/\n    cache.py             ← KVCache implementation\n    flow_utils.py        ← Data flow debugging utilities\n```\n\n#### Core Model Orchestration Code\n\n```python\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple, Dict, Any\n\nclass TransformerModel(nn.Module):\n    \"\"\"Main transformer model orchestrating component interactions.\"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        self.config = config\n        \n        # Core components with proper initialization order\n        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n        self.position_embedding = nn.Embedding(config.seq_length, config.d_model)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        \n        # Transformer block stack\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(config) for _ in range(config.num_layers)\n        ])\n        \n        # Output head\n        self.layer_norm = LayerNorm(config.d_model)\n        self.output_projection = nn.Linear(config.d_model, config.vocab_size, bias=False)\n        \n        # Initialize weights\n        self.apply(initialize_transformer_weights)\n    \n    def forward(self, \n                input_ids: torch.Tensor, \n                attention_mask: Optional[torch.Tensor] = None,\n                kv_cache: Optional[KVCache] = None,\n                use_cache: bool = False) -> Tuple[torch.Tensor, Optional[KVCache]]:\n        \"\"\"\n        Forward pass supporting both training and inference modes.\n        \n        Training mode: processes full sequences in parallel\n        Inference mode: supports KV caching for efficient generation\n        \"\"\"\n        # TODO 1: Extract batch_size and seq_length from input_ids shape\n        # TODO 2: Generate position indices for current sequence\n        # TODO 3: Compute token embeddings and position embeddings\n        # TODO 4: Add embeddings and apply dropout\n        # TODO 5: Create causal mask if attention_mask not provided\n        # TODO 6: Process through transformer blocks with optional caching\n        # TODO 7: Apply final layer norm and output projection\n        # TODO 8: Return logits and updated cache if requested\n        pass\n```\n\n#### Training Flow Implementation\n\n```python\nclass TransformerTrainer:\n    \"\"\"Manages training data flow and component coordination.\"\"\"\n    \n    def __init__(self, model: TransformerModel, config: TrainingConfig):\n        self.model = model\n        self.config = config\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(), \n            lr=config.learning_rate,\n            weight_decay=config.weight_decay\n        )\n        \n    def train(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, Any]:\n        \"\"\"Main training loop with proper mode management.\"\"\"\n        # TODO 1: Set model to training mode\n        # TODO 2: Initialize tracking variables (loss, step count)\n        # TODO 3: Iterate through epochs and batches\n        # TODO 4: For each batch: forward pass, loss computation, backward pass\n        # TODO 5: Apply gradient clipping before optimizer step\n        # TODO 6: Log training metrics at specified intervals\n        # TODO 7: Run validation at specified intervals\n        # TODO 8: Save checkpoints based on validation performance\n        # TODO 9: Return training history and metrics\n        pass\n    \n    def _compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute cross-entropy loss with proper label shifting and masking.\"\"\"\n        # TODO 1: Reshape logits to [batch_size * seq_length, vocab_size]\n        # TODO 2: Reshape targets to [batch_size * seq_length]\n        # TODO 3: Create padding mask to exclude pad tokens from loss\n        # TODO 4: Compute cross-entropy loss with reduction='none'\n        # TODO 5: Apply padding mask and compute mean over valid tokens\n        # Hint: Use F.cross_entropy with ignore_index for padding tokens\n        pass\n```\n\n#### Inference Flow Implementation\n\n```python\nclass TextGenerator:\n    \"\"\"Handles inference data flow with KV cache optimization.\"\"\"\n    \n    def __init__(self, model: TransformerModel, tokenizer: SimpleTokenizer, config: GenerationConfig):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.config = config\n        \n    def generate(self, prompt: str, max_new_tokens: int) -> str:\n        \"\"\"Main generation loop with efficient caching.\"\"\"\n        # TODO 1: Set model to evaluation mode\n        # TODO 2: Encode prompt to token sequence\n        # TODO 3: Initialize KV cache for efficient generation\n        # TODO 4: Generate tokens one by one in loop\n        # TODO 5: For each step: forward pass, sampling, cache update\n        # TODO 6: Check for early stopping conditions (EOS token, max length)\n        # TODO 7: Decode final token sequence to text\n        # TODO 8: Clean up cache and return generated text\n        pass\n    \n    def _sample_next_token(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"Apply sampling strategy to select next token.\"\"\"\n        # TODO 1: Extract logits for last position only\n        # TODO 2: Apply temperature scaling if specified\n        # TODO 3: Apply repetition penalty for recently generated tokens\n        # TODO 4: Use configured sampling strategy (greedy, top-k, top-p)\n        # TODO 5: Return selected token ID as tensor\n        # Hint: Use SamplingStrategies static methods for different approaches\n        pass\n```\n\n#### KV Cache Implementation\n\n```python\nclass KVCache:\n    \"\"\"Efficient key-value caching for autoregressive generation.\"\"\"\n    \n    def __init__(self, num_layers: int, num_heads: int, d_k: int, d_v: int, device: str):\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.d_k = d_k\n        self.d_v = d_v\n        self.device = device\n        \n        # Initialize cache storage\n        self.key_cache = {}  # layer_idx -> [batch_size, num_heads, seq_len, d_k]\n        self.value_cache = {}  # layer_idx -> [batch_size, num_heads, seq_len, d_v]\n        \n    def update(self, layer_idx: int, new_keys: torch.Tensor, new_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Update cache and return concatenated keys/values.\"\"\"\n        # TODO 1: Check if this is first update for layer (cache empty)\n        # TODO 2: If first update, store tensors directly\n        # TODO 3: If subsequent update, concatenate with cached tensors\n        # TODO 4: Update cache with concatenated tensors\n        # TODO 5: Return concatenated keys and values for attention computation\n        # Hint: Use torch.cat along sequence dimension (dim=2)\n        pass\n    \n    def clear(self) -> None:\n        \"\"\"Reset cache for new generation sequence.\"\"\"\n        # TODO 1: Clear key_cache dictionary\n        # TODO 2: Clear value_cache dictionary\n        # TODO 3: Optionally trigger garbage collection for memory cleanup\n        pass\n```\n\n#### Data Flow Debugging Utilities\n\n```python\ndef trace_tensor_flow(model: TransformerModel, input_ids: torch.Tensor) -> Dict[str, torch.Size]:\n    \"\"\"Trace tensor shapes through forward pass for debugging.\"\"\"\n    # TODO 1: Register forward hooks on all model components\n    # TODO 2: Run forward pass and capture intermediate tensor shapes\n    # TODO 3: Return dictionary mapping component names to output shapes\n    # TODO 4: Verify all shapes match expected dimensions from config\n    # Hint: Use model.named_modules() to iterate through components\n    pass\n\ndef validate_attention_patterns(attention_weights: torch.Tensor, seq_length: int) -> bool:\n    \"\"\"Validate that attention weights respect causal masking.\"\"\"\n    # TODO 1: Check that attention_weights has correct shape\n    # TODO 2: Verify that weights[i, j] == 0 for all j > i (causal property)\n    # TODO 3: Verify that weights sum to 1.0 along last dimension\n    # TODO 4: Return True if all validations pass\n    # Hint: Use torch.triu to check upper triangular zeros\n    pass\n```\n\n#### Mode-Specific Context Managers\n\n```python\nclass InferenceMode:\n    \"\"\"Context manager for inference-specific model configuration.\"\"\"\n    \n    def __init__(self, model: TransformerModel):\n        self.model = model\n        self.training_mode = None\n        \n    def __enter__(self):\n        # TODO 1: Store current training mode\n        # TODO 2: Set model to evaluation mode\n        # TODO 3: Disable gradient computation\n        # TODO 4: Clear any existing caches\n        return self\n        \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # TODO 1: Restore original training mode\n        # TODO 2: Re-enable gradients if originally enabled\n        # TODO 3: Clean up any resources\n        pass\n\n# Usage example:\n# with InferenceMode(model):\n#     generated_text = generator.generate(prompt, max_tokens=100)\n```\n\n#### Milestone Checkpoints\n\n**Forward Pass Verification:**\nAfter implementing the forward pass, run these checks:\n```bash\npython -c \"\nfrom transformer import TransformerModel, TransformerConfig\nimport torch\n\nconfig = TransformerConfig(d_model=512, num_heads=8, seq_length=128, vocab_size=1000, num_layers=6)\nmodel = TransformerModel(config)\ninput_ids = torch.randint(0, config.vocab_size, (2, 64))  # batch_size=2, seq_length=64\n\n# Should output logits with correct shape\nlogits, _ = model(input_ids)\nassert logits.shape == (2, 64, 1000), f'Expected (2, 64, 1000), got {logits.shape}'\nprint('✓ Forward pass shape verification passed')\n\"\n```\n\n**Training Flow Verification:**\nAfter implementing training components:\n```bash\npython -c \"\n# Create small dataset and train for a few steps\n# Loss should decrease over iterations\n# Gradients should flow to all parameters\nprint('✓ Training flow verification - check logs for decreasing loss')\n\"\n```\n\n**KV Cache Verification:**\nAfter implementing caching:\n```bash\npython -c \"\n# Generate same sequence with and without cache\n# Results should be identical but cache version faster\n# Cache memory usage should be reasonable\nprint('✓ KV cache verification - check generation consistency')\n\"\n```\n\n#### Common Implementation Issues\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Forward pass shape errors | Incorrect tensor reshaping | Print shapes after each operation | Fix attention head dimension calculations |\n| Training loss not decreasing | Wrong label shifting or learning rate | Check loss computation and gradients | Verify target sequences are shifted correctly |\n| Generation produces repetitive text | Missing sampling randomness | Check sampling strategy application | Add temperature > 0 or top-k/top-p sampling |\n| Memory usage grows unbounded | KV cache not clearing properly | Monitor cache size during generation | Implement proper cache cleanup and limits |\n| Attention weights sum > 1.0 | Causal mask applied after softmax | Verify mask timing in attention computation | Apply mask to scores before softmax operation |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones - ensuring robust implementation across self-attention (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)\n\nError handling in transformer implementations goes far beyond simple try-catch blocks. The numerical computations involved in self-attention, the massive parameter spaces of transformer models, and the sequential nature of text generation create unique failure modes that can silently corrupt results or cause catastrophic training failures. Understanding these edge cases and implementing robust handling strategies is crucial for building reliable transformer systems.\n\nThe complexity stems from the mathematical nature of attention mechanisms, where small numerical instabilities can cascade through multiple layers, and the autoregressive generation process, where a single incorrect token can derail an entire sequence. Unlike traditional software where failures are often obvious, transformer failures can manifest as subtle degradations in text quality, training instabilities that emerge after thousands of iterations, or seemingly random generation artifacts that only appear under specific conditions.\n\n### Numerical Stability\n\n> **Mental Model: The Numerical Tightrope Walk**\n> Think of transformer computations as walking a tightrope between numerical extremes. On one side lies the abyss of vanishing gradients where your model learns nothing, and on the other side the cliff of exploding values that crash your training. Layer normalization, gradient clipping, and careful attention scaling act as your balance pole, helping you navigate safely between these extremes while maintaining the delicate precision needed for coherent text generation.\n\nNumerical stability in transformers presents unique challenges due to the combination of softmax operations in attention mechanisms, deep layer stacks that can amplify small errors, and the need to maintain precision across widely varying scales of embeddings, attention weights, and gradients. The softmax operation in particular creates a critical vulnerability point where input values that are too large can cause overflow, while values that are too small can underflow to zero.\n\nThe **scaled dot-product attention** computation creates the primary numerical stability challenge through its softmax operation. When attention scores become too large, the softmax function produces probabilities that are numerically indistinguishable from hard attention (all weight on one token), eliminating the model's ability to attend to multiple relevant tokens. Conversely, when scores are too small, all attention weights become nearly uniform, preventing the model from focusing on important information.\n\n**Softmax Overflow Prevention** requires careful management of the input range before the softmax operation. The fundamental issue occurs when the maximum attention score exceeds the floating-point representation limit, typically around 88 for float32. When this happens, the exponential function returns infinity, and the resulting probability distribution contains NaN values that corrupt all downstream computations.\n\n> **Decision: Attention Score Clipping Strategy**\n> - **Context**: Attention scores can grow unboundedly during training, especially in early iterations when weights are randomly initialized, leading to softmax overflow and NaN gradients\n> - **Options Considered**: Pre-softmax clipping, temperature scaling, attention score normalization\n> - **Decision**: Implement pre-softmax clipping with learnable temperature parameter\n> - **Rationale**: Clipping preserves the relative ordering of scores while preventing overflow, and learnable temperature allows the model to adjust the attention sharpness during training\n> - **Consequences**: Adds one parameter per attention head but provides robust protection against numerical instability without compromising model expressiveness\n\n| Stability Technique | Implementation | Numerical Range | Gradient Impact |\n|---------------------|----------------|-----------------|-----------------|\n| Pre-softmax clipping | `torch.clamp(scores, max=20.0)` | [-inf, 20.0] | Gradient flow preserved |\n| Temperature scaling | `scores / max(temperature, 1e-4)` | Scaled by temperature | Temperature gets gradients |\n| LogSumExp trick | `scores - scores.max(dim=-1)` | Relative to maximum | No additional parameters |\n| Attention dropout | Random masking before softmax | Original range | Prevents overfitting |\n\nThe **LogSumExp numerical trick** provides the most robust solution for softmax stability. Instead of computing softmax directly, we subtract the maximum score from all scores before applying the exponential function. This mathematical identity ensures that the maximum input to the exponential is always zero, preventing overflow while maintaining exact mathematical equivalence to the original softmax computation.\n\n**Gradient explosion and vanishing** present complementary challenges in transformer training. Gradient explosion occurs when the chain rule multiplication through many layers produces gradients with exponentially growing magnitude, while vanishing gradients result from repeated multiplication of small values. The residual connections in transformer blocks specifically address vanishing gradients, but gradient explosion requires explicit clipping strategies.\n\nThe gradient clipping implementation must occur after computing gradients for all parameters but before the optimizer step. The total gradient norm is computed across all model parameters, and if it exceeds the specified threshold, all gradients are scaled down proportionally. This preserves the relative gradient directions while preventing the parameter updates from being too large.\n\n| Gradient Issue | Detection Method | Typical Range | Recovery Strategy |\n|----------------|------------------|---------------|-------------------|\n| Explosion | Gradient norm > 10.0 | 100-1000+ | Clip to max norm 1.0 |\n| Vanishing | Gradient norm < 1e-6 | 1e-8 to 1e-4 | Increase learning rate |\n| NaN gradients | `torch.isnan(grad).any()` | N/A | Reset to checkpoint |\n| Inf gradients | `torch.isinf(grad).any()` | N/A | Skip optimizer step |\n\n**Precision issues** arise from the limited precision of floating-point arithmetic, particularly when mixing operations that produce vastly different scales. The attention mechanism combines embedding vectors (typically in range [-1, 1]) with position encodings, then scales by the square root of the dimension, creating opportunities for precision loss when different components have mismatched scales.\n\nMixed precision training introduces additional complexity by using 16-bit floats for forward passes and 32-bit floats for gradient accumulation. While this provides significant memory and speed benefits, it requires careful management of the loss scaling to prevent gradient underflow. The loss scaling factor must be large enough to move small gradients into the representable range of 16-bit floats, but small enough to avoid overflow during backpropagation.\n\n> **Decision: Automatic Mixed Precision Configuration**\n> - **Context**: Manual mixed precision management is error-prone and requires careful tuning of loss scaling factors for different model sizes and learning rates\n> - **Options Considered**: Manual FP16 casting, automatic mixed precision (AMP), full FP32 training\n> - **Decision**: Use PyTorch's automatic mixed precision with dynamic loss scaling\n> - **Rationale**: AMP automatically handles precision decisions and dynamic loss scaling adapts to the training dynamics, reducing manual tuning while maintaining numerical stability\n> - **Consequences**: Requires PyTorch 1.6+ and modern GPUs but provides significant memory savings and training speedup with minimal stability risk\n\nThe **layer normalization** operation introduces its own numerical stability considerations. The variance computation requires careful handling when the variance approaches zero, which can occur when all activations in a layer become very similar. The standard implementation adds a small epsilon value (typically 1e-5) to the variance before taking the square root, preventing division by zero while maintaining numerical stability.\n\n**Common Numerical Stability Pitfalls:**\n\n⚠️ **Pitfall: Ignoring Attention Score Magnitude**\nMany implementations fail to monitor attention scores before softmax, allowing them to grow unboundedly during training. This manifests as sudden spikes in training loss or NaN values appearing after several stable epochs. The fix requires implementing attention score monitoring and either clipping or temperature scaling to keep scores in a reasonable range.\n\n⚠️ **Pitfall: Inconsistent Gradient Accumulation**\nWhen implementing gradient accumulation for large effective batch sizes, failing to scale the loss correctly leads to gradient magnitudes that depend on the accumulation steps rather than the actual loss value. Each accumulated gradient should be divided by the number of accumulation steps before adding to the total gradient.\n\n⚠️ **Pitfall: Mixed Precision Loss Scaling Errors**\nUsing static loss scaling in mixed precision training often leads to either gradient underflow (scaling too small) or overflow (scaling too large). The scaling factor needs dynamic adjustment based on whether gradients contain inf/NaN values, requiring careful monitoring and adjustment logic.\n\n### Input Validation\n\nInput validation in transformer systems extends beyond simple type checking to include semantic constraints that ensure mathematical operations remain well-defined and computationally feasible. The sequential nature of transformer processing means that invalid inputs can corrupt not just individual computations but entire generation sequences, making comprehensive validation essential for robust operation.\n\nThe validation strategy must operate at multiple levels: token-level validation ensuring individual tokens are within vocabulary bounds, sequence-level validation maintaining length constraints and proper formatting, batch-level validation ensuring consistent dimensions across sequences, and model-level validation verifying that input dimensions match the model's expected configuration.\n\n**Sequence Length Limits** represent the most critical constraint in transformer systems. The quadratic memory complexity of attention means that sequence length directly impacts both memory usage and computational requirements. The maximum sequence length is typically set during model initialization and cannot be exceeded without retraining the model, as the positional embeddings and causal attention masks are pre-allocated for the maximum length.\n\nThe sequence length validation must occur at multiple points in the processing pipeline. During tokenization, the raw text must be checked to ensure the resulting token sequence won't exceed limits. During data loading, batched sequences must be validated to ensure consistent lengths within batches. During generation, the accumulated sequence length must be monitored to prevent infinite generation loops.\n\n| Validation Point | Length Check | Action on Violation | Recovery Strategy |\n|------------------|--------------|-------------------|-------------------|\n| Tokenization | `len(tokens) <= seq_length` | Truncate or split | Preserve important tokens |\n| Data loading | `max(batch_lengths) <= seq_length` | Filter or pad | Use sliding window |\n| Generation | `current_length < max_length` | Stop generation | Return partial sequence |\n| Attention computation | `seq_len <= model.seq_length` | Raise exception | Cannot recover |\n\nThe **sliding window approach** provides a robust strategy for handling sequences that exceed the maximum length. Instead of simply truncating long sequences, the sliding window maintains context by processing overlapping segments of the sequence. This is particularly important for generation tasks where losing early context can significantly impact coherence.\n\n**Token Vocabulary Bounds** validation ensures that all token IDs fall within the valid range defined by the tokenizer's vocabulary. Out-of-vocabulary tokens typically manifest as IDs that are negative, exceed the vocabulary size, or correspond to undefined token positions. The validation must handle both individual token validation during processing and batch validation during data loading.\n\nThe token validation strategy must account for special tokens that have specific semantic meaning. The `pad_token_id` is used for sequence padding and should not appear in the middle of sequences. The `eos_token_id` indicates sequence termination and should trigger special handling during generation. Unknown tokens or corrupted token IDs should be either corrected through fallback strategies or cause graceful failure with informative error messages.\n\n> **Decision: Token Validation Timing Strategy**\n> - **Context**: Token validation can occur at tokenization time, data loading time, or model forward pass time, each with different performance and error detection trade-offs\n> - **Options Considered**: Early validation at tokenization, lazy validation at model input, comprehensive validation at each step\n> - **Decision**: Implement validation at tokenization with spot checks during data loading and generation\n> - **Rationale**: Early validation catches errors close to their source while spot checks provide safety nets without significant performance overhead\n> - **Consequences**: Requires tokenizer to maintain vocabulary bounds checking but provides fast error detection and clear error attribution\n\n**Shape Consistency Checking** validates that tensor dimensions align correctly throughout the transformer pipeline. The attention mechanism requires that the sequence length dimension matches across query, key, and value tensors, while the batch dimension must be consistent across all inputs. The embedding dimension must match the model's configured `d_model` parameter, and the attention head dimensions must divide evenly into the total model dimension.\n\nThe shape validation becomes particularly complex during generation, where the sequence length grows dynamically, and during training with variable-length sequences in batches. The KV cache mechanism adds additional shape constraints, as cached key and value tensors must maintain consistency with newly computed values.\n\n| Tensor | Expected Shape | Validation Rule | Error Indication |\n|--------|----------------|-----------------|-------------------|\n| Input embeddings | `[batch_size, seq_len, d_model]` | All dims > 0, `d_model` matches config | Shape mismatch error |\n| Attention mask | `[seq_len, seq_len]` or `[batch, seq_len, seq_len]` | Square matrix, compatible with input | Broadcast error |\n| Query/Key/Value | `[batch_size, seq_len, d_model]` | Match input embeddings | Attention computation fails |\n| KV cache | `[batch_size, num_heads, cached_len, d_k]` | Consistent with new keys/values | Cache concatenation fails |\n\n**Batch Consistency Validation** ensures that all sequences in a batch have compatible properties for efficient processing. While sequences can have different actual lengths (handled through padding and masking), they must all fit within the model's maximum sequence length and use the same vocabulary. The batch validation also checks that attention masks have the correct shape and that any provided position IDs are within valid ranges.\n\nThe batch validation strategy must balance thoroughness with performance, as validation overhead can become significant for large batches or high-throughput scenarios. Critical validations that prevent crashes or silent failures should always be performed, while optional validations that catch rare edge cases can be enabled through debug flags.\n\n> **Decision: Validation Granularity and Performance Trade-off**\n> - **Context**: Comprehensive validation provides safety but adds computational overhead that can impact training and inference performance\n> - **Options Considered**: Full validation always, validation only in debug mode, critical validations always with optional detailed checking\n> - **Decision**: Implement tiered validation with critical checks always enabled and detailed validation controllable via configuration\n> - **Rationale**: Critical failures (crashes, NaN propagation) must always be prevented, while performance-impacting detailed validation can be disabled in production\n> - **Consequences**: Requires careful categorization of validation checks but provides both safety and performance when needed\n\n**Memory Constraint Validation** monitors system resources to prevent out-of-memory failures that can crash training or generation processes. The validation must account for the quadratic memory growth of attention mechanisms, the linear growth of KV caches during generation, and the temporary memory spikes during gradient computation and optimizer updates.\n\nThe memory validation strategy includes pre-computation estimation of memory requirements based on model configuration and input sizes, runtime monitoring of actual memory usage with warnings before critical thresholds, and graceful degradation strategies when memory limits are approached.\n\n**Common Input Validation Pitfalls:**\n\n⚠️ **Pitfall: Trusting Tokenizer Output**\nMany implementations assume that tokenizers always produce valid output, but tokenizers can generate out-of-vocabulary IDs, exceed length limits, or produce inconsistent special token placement. Always validate tokenizer output before passing to the model, checking for ID bounds, length limits, and proper special token usage.\n\n⚠️ **Pitfall: Ignoring Batch Size Variations**\nVariable batch sizes during training or inference can cause shape mismatches in attention computations or gradient accumulation. Implement explicit batch size checking and ensure that all tensor operations can handle the actual batch size rather than assuming a fixed size from configuration.\n\n⚠️ **Pitfall: Missing Edge Case Handling**\nEmpty sequences, single-token sequences, or sequences containing only special tokens can cause mathematical operations to produce undefined results. Implement explicit handling for these edge cases, either through early returns with appropriate default values or graceful error messages explaining the limitation.\n\n### Implementation Guidance\n\nThe implementation of robust error handling and edge case management in transformers requires careful attention to both the mathematical foundations and the practical realities of deep learning systems. The following guidance provides concrete approaches for implementing the stability and validation strategies described above.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Numerical Stability | Manual gradient clipping + basic overflow checks | PyTorch AMP + custom stability monitors |\n| Input Validation | Simple assertion-based checking | Comprehensive schema validation with detailed error messages |\n| Error Logging | Python logging + print statements | Structured logging (loguru) + metric collection |\n| Testing | Basic unit tests for edge cases | Property-based testing (hypothesis) + fuzzing |\n\n**Recommended File Structure:**\n```\ntransformer/\n  validation/\n    __init__.py                     ← validation utilities\n    input_validators.py             ← input validation functions\n    numerical_stability.py          ← stability monitoring and fixes\n    error_handlers.py               ← error recovery strategies\n  utils/\n    logging_config.py               ← logging setup\n    monitoring.py                   ← runtime monitoring utilities\n  tests/\n    test_validation.py              ← validation unit tests\n    test_edge_cases.py              ← edge case integration tests\n```\n\n**Infrastructure Starter Code:**\n\n```python\n# validation/numerical_stability.py - Complete stability utilities\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Dict, Any\nimport logging\n\nclass NumericalStabilityMonitor:\n    \"\"\"Complete monitoring and fixing utilities for numerical stability issues.\"\"\"\n    \n    def __init__(self, gradient_clip_norm: float = 1.0, attention_clip_value: float = 20.0):\n        self.gradient_clip_norm = gradient_clip_norm\n        self.attention_clip_value = attention_clip_value\n        self.logger = logging.getLogger(__name__)\n        \n    def stabilize_attention_scores(self, scores: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n        \"\"\"Apply multiple stability techniques to attention scores.\"\"\"\n        # Clamp extreme values\n        scores = torch.clamp(scores, max=self.attention_clip_value)\n        \n        # Apply temperature scaling with minimum temperature\n        temperature = max(temperature, 1e-4)\n        scores = scores / temperature\n        \n        # LogSumExp trick for softmax stability\n        scores_max = scores.max(dim=-1, keepdim=True)[0]\n        scores = scores - scores_max\n        \n        return scores\n    \n    def check_and_clip_gradients(self, model: nn.Module) -> Dict[str, float]:\n        \"\"\"Clip gradients and return diagnostic information.\"\"\"\n        total_norm = 0.0\n        param_count = 0\n        \n        for param in model.parameters():\n            if param.grad is not None:\n                # Check for NaN or Inf gradients\n                if torch.isnan(param.grad).any():\n                    self.logger.warning(\"NaN gradients detected\")\n                    param.grad.zero_()\n                    continue\n                    \n                if torch.isinf(param.grad).any():\n                    self.logger.warning(\"Inf gradients detected\")\n                    param.grad.zero_()\n                    continue\n                \n                param_norm = param.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n                param_count += 1\n        \n        total_norm = total_norm ** (1. / 2)\n        \n        # Clip gradients if needed\n        if total_norm > self.gradient_clip_norm:\n            clip_coef = self.gradient_clip_norm / (total_norm + 1e-6)\n            for param in model.parameters():\n                if param.grad is not None:\n                    param.grad.data.mul_(clip_coef)\n        \n        return {\n            'total_norm': total_norm,\n            'param_count': param_count,\n            'clipped': total_norm > self.gradient_clip_norm\n        }\n\n# validation/input_validators.py - Complete validation utilities\nclass TransformerInputValidator:\n    \"\"\"Complete input validation for transformer components.\"\"\"\n    \n    def __init__(self, config: 'TransformerConfig'):\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n    \n    def validate_token_sequence(self, tokens: torch.Tensor, sequence_name: str = \"sequence\") -> bool:\n        \"\"\"Validate a token sequence tensor.\"\"\"\n        # Shape validation\n        if tokens.dim() != 2:\n            raise ValueError(f\"{sequence_name} must be 2D tensor (batch_size, seq_len), got {tokens.shape}\")\n        \n        batch_size, seq_len = tokens.shape\n        \n        # Length validation\n        if seq_len > self.config.seq_length:\n            raise ValueError(f\"{sequence_name} length {seq_len} exceeds maximum {self.config.seq_length}\")\n        \n        # Vocabulary bounds validation\n        if tokens.min() < 0:\n            raise ValueError(f\"{sequence_name} contains negative token IDs\")\n        \n        if tokens.max() >= self.config.vocab_size:\n            raise ValueError(f\"{sequence_name} contains token IDs >= vocab_size {self.config.vocab_size}\")\n        \n        return True\n    \n    def validate_attention_inputs(self, query: torch.Tensor, key: torch.Tensor, \n                                 value: torch.Tensor, mask: Optional[torch.Tensor] = None) -> bool:\n        \"\"\"Validate attention mechanism inputs.\"\"\"\n        # Shape consistency\n        if query.shape != key.shape:\n            raise ValueError(f\"Query shape {query.shape} != key shape {key.shape}\")\n        \n        if query.shape != value.shape:\n            raise ValueError(f\"Query shape {query.shape} != value shape {value.shape}\")\n        \n        # Dimension validation\n        if query.shape[-1] != self.config.d_model:\n            raise ValueError(f\"Input dimension {query.shape[-1]} != d_model {self.config.d_model}\")\n        \n        # Mask validation\n        if mask is not None:\n            seq_len = query.shape[1]\n            expected_mask_shape = (seq_len, seq_len)\n            if mask.shape[-2:] != expected_mask_shape:\n                raise ValueError(f\"Mask shape {mask.shape} incompatible with sequence length {seq_len}\")\n        \n        return True\n\n# utils/monitoring.py - Runtime monitoring utilities\nclass RuntimeMonitor:\n    \"\"\"Monitor system resources and model behavior during training/inference.\"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n    \n    def check_memory_usage(self, threshold_gb: float = 0.9) -> Dict[str, Any]:\n        \"\"\"Check GPU memory usage and warn if approaching limits.\"\"\"\n        if not torch.cuda.is_available():\n            return {\"gpu_available\": False}\n        \n        allocated = torch.cuda.memory_allocated() / 1e9\n        cached = torch.cuda.memory_reserved() / 1e9\n        max_memory = torch.cuda.max_memory_allocated() / 1e9\n        \n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        usage_ratio = cached / total_memory\n        \n        if usage_ratio > threshold_gb:\n            self.logger.warning(f\"GPU memory usage {usage_ratio:.2%} exceeds threshold {threshold_gb:.2%}\")\n        \n        return {\n            \"gpu_available\": True,\n            \"allocated_gb\": allocated,\n            \"cached_gb\": cached,\n            \"max_gb\": max_memory,\n            \"total_gb\": total_memory,\n            \"usage_ratio\": usage_ratio,\n            \"threshold_exceeded\": usage_ratio > threshold_gb\n        }\n```\n\n**Core Logic Skeleton Code:**\n\n```python\n# In your MultiHeadAttention class\ndef forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"Forward pass with comprehensive error handling and numerical stability.\"\"\"\n    # TODO 1: Validate input tensor shapes and dimensions using input validator\n    # TODO 2: Apply Q, K, V projections and check for numerical issues\n    # TODO 3: Compute attention scores with stability monitoring\n    # TODO 4: Apply causal mask and validate mask compatibility\n    # TODO 5: Apply softmax with LogSumExp trick for numerical stability\n    # TODO 6: Check attention weights for NaN/Inf and handle gracefully\n    # TODO 7: Apply attention dropout only during training\n    # TODO 8: Compute weighted values and validate output shapes\n    # TODO 9: Apply output projection and final validation\n    # Hint: Use stability_monitor.stabilize_attention_scores() before softmax\n    # Hint: Always validate shapes before mathematical operations\n    # Hint: Log warnings for numerical issues but continue processing when possible\n\n# In your TransformerTrainer class  \ndef _handle_training_step_errors(self, batch_idx: int, error: Exception) -> bool:\n    \"\"\"Handle errors during training steps with recovery strategies.\"\"\"\n    # TODO 1: Identify error type (numerical instability, OOM, data corruption)\n    # TODO 2: Log detailed error information including batch characteristics\n    # TODO 3: For numerical errors, reset gradients and continue with next batch\n    # TODO 4: For OOM errors, reduce batch size temporarily if possible\n    # TODO 5: For data errors, skip batch and log problematic data indices\n    # TODO 6: Update error counters and decide whether to continue training\n    # TODO 7: Return True to continue training, False to abort\n    # Hint: torch.cuda.empty_cache() can help recover from OOM\n    # Hint: Keep error counters to detect systematic issues\n    # Hint: Different error types require different recovery strategies\n\n# In your TextGenerator class\ndef _validate_generation_state(self, tokens: torch.Tensor, step: int) -> bool:\n    \"\"\"Validate generation state and detect potential issues.\"\"\"\n    # TODO 1: Check if sequence length exceeds configured maximum\n    # TODO 2: Validate all token IDs are within vocabulary bounds\n    # TODO 3: Check for repetitive patterns that indicate generation issues\n    # TODO 4: Monitor memory usage for KV cache growth\n    # TODO 5: Detect infinite generation loops (no EOS token progress)\n    # TODO 6: Validate that probability distributions are well-formed\n    # TODO 7: Return False if generation should be terminated early\n    # Hint: Track recent tokens to detect repetition loops\n    # Hint: Set maximum steps to prevent infinite generation\n    # Hint: Check that probability distributions sum to approximately 1.0\n```\n\n**Language-Specific Hints:**\n\n- **PyTorch Stability**: Use `torch.nn.utils.clip_grad_norm_()` for gradient clipping and `torch.cuda.amp.autocast()` for automatic mixed precision\n- **Memory Management**: Call `torch.cuda.empty_cache()` after handling OOM errors, and use `torch.cuda.memory_summary()` for detailed memory debugging\n- **Numerical Checking**: Use `torch.isfinite()` to check for both NaN and Inf simultaneously, and `torch.finfo(dtype).max` to get safe numerical limits\n- **Error Recovery**: Implement exponential backoff for retrying failed operations, and maintain error budgets to prevent infinite retry loops\n\n**Milestone Checkpoints:**\n\n**After Implementing Numerical Stability (All Milestones):**\n- Run: `python -m pytest tests/test_numerical_stability.py -v`\n- Expected: All tests pass, including edge cases with extreme attention scores\n- Manual verification: Train a small model with intentionally large learning rates - should see gradient clipping warnings but stable training loss\n- Warning signs: NaN losses, exploding gradients that aren't caught by clipping, or attention weights that are all zeros or ones\n\n**After Implementing Input Validation (All Milestones):**\n- Run: `python -m pytest tests/test_input_validation.py -v`\n- Expected: Validation catches all malformed inputs with informative error messages\n- Manual verification: Try feeding invalid inputs (wrong shapes, out-of-bounds tokens) - should get clear error messages, not crashes\n- Warning signs: Cryptic error messages from deep in PyTorch, silent failures with wrong outputs, or crashes without informative errors\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Training loss becomes NaN | Gradient explosion or numerical instability | Check gradient norms, attention score ranges | Reduce learning rate, add gradient clipping |\n| Attention weights all uniform | Attention scores too small or numerical underflow | Print attention scores before softmax | Increase temperature, check score computation |\n| Generation produces repetitive text | Sampling strategy issues or numerical precision | Check probability distributions, sampling logic | Adjust temperature, implement repetition penalty |\n| Memory usage grows unexpectedly | KV cache not cleared between sequences | Monitor cache size, check clear() calls | Implement proper cache management |\n| Model outputs become deterministic | Dropout not disabled during inference | Check model.eval() calls, dropout behavior | Ensure model.eval() before generation |\n\n\n## Testing Strategy and Milestones\n\n> **Milestone(s):** All milestones - systematic verification approaches for self-attention mechanism (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)\n\nTesting a transformer implementation requires a comprehensive strategy that validates both individual components and their integration. The challenge lies in verifying mathematical correctness, architectural consistency, and end-to-end behavior across multiple interconnected systems. Unlike traditional software testing that focuses primarily on business logic, transformer testing must validate tensor operations, gradient flows, attention patterns, and generative capabilities.\n\n**Mental Model: The Quality Assurance Pyramid**\n\nThink of transformer testing as a quality assurance pyramid in a manufacturing plant. At the base, we have component unit tests that verify each individual part meets specifications - like testing that each gear has the right number of teeth and rotates smoothly. In the middle, we have integration tests that verify assembled modules work together - like testing that the gear assembly transfers power correctly. At the top, we have end-to-end tests that verify the complete machine produces the desired output - like testing that the entire assembly line produces finished products to specification. Each level catches different types of defects, and failures at higher levels often trace back to issues at lower levels.\n\nThe testing pyramid for transformers follows this same structure. Unit tests catch mathematical errors in individual attention heads or layer normalization computations. Integration tests catch architectural issues like dimension mismatches between transformer blocks or incorrect data flow through the training pipeline. End-to-end tests catch behavioral issues like repetitive text generation or failure to learn coherent patterns from training data.\n\n### Component Unit Tests\n\nComponent unit tests form the foundation of transformer validation, focusing on the mathematical correctness and architectural consistency of individual modules. Each component must be tested in isolation to ensure it implements the expected mathematical operations with correct tensor shapes and numerical stability.\n\n**Attention Mechanism Testing Strategy**\n\nThe attention mechanism requires particularly thorough testing due to its mathematical complexity and central role in transformer performance. Testing must validate the scaled dot-product computation, multi-head parallelization, causal masking behavior, and numerical stability under various input conditions.\n\nThe core attention computation involves matrix multiplications, scaling operations, masking applications, and softmax normalizations that must maintain precise mathematical relationships. Testing verifies that query-key similarity scores are computed correctly, that scaling by the square root of the key dimension prevents gradient explosion, that causal masks properly zero out future positions, and that attention weights sum to unity after softmax normalization.\n\n| Test Category | Focus Area | Key Validations | Expected Behavior |\n|---------------|------------|------------------|-------------------|\n| Shape Consistency | Tensor dimensions | Input/output tensor shapes match specifications | Q, K, V projections produce correct head dimensions |\n| Mathematical Correctness | Attention computation | Scaled dot-product follows formula exactly | Attention scores = softmax(Q@K^T / sqrt(d_k)) |\n| Masking Behavior | Causal attention | Future positions masked to negative infinity | Upper triangular attention matrix is zero |\n| Multi-head Parallelization | Head computation | Each head operates on correct dimension slice | Concatenated heads restore original d_model dimension |\n| Numerical Stability | Softmax overflow | Large attention scores don't cause NaN values | LogSumExp trick prevents numerical overflow |\n| Gradient Flow | Backpropagation | Gradients flow correctly through attention layers | Non-zero gradients reach input embeddings |\n\n**Feed-Forward Network Testing Strategy**\n\nThe feed-forward network testing focuses on the two-layer MLP structure with expansion and projection operations. Testing must validate dimension transformations, activation function application, and dropout behavior during training and inference modes.\n\nThe feed-forward network expands input representations to four times the model dimension, applies a non-linear activation, then projects back to the original dimension. This expansion-contraction pattern must preserve batch and sequence dimensions while transforming the feature dimension according to the specified expansion ratio.\n\n| Test Component | Validation Focus | Test Cases | Expected Results |\n|----------------|------------------|------------|------------------|\n| Expansion Layer | Dimension transformation | Input (batch, seq, d_model) → Hidden (batch, seq, 4*d_model) | Correct 4x expansion ratio |\n| Activation Function | Non-linearity | ReLU/GELU application to expanded representations | Proper activation of positive values |\n| Projection Layer | Dimension restoration | Hidden (batch, seq, 4*d_model) → Output (batch, seq, d_model) | Return to original dimensions |\n| Dropout Behavior | Regularization | Different behavior in train vs eval mode | Random zeroing in training, identity in inference |\n| Parameter Initialization | Weight values | Proper initialization prevents vanishing/exploding gradients | Weights follow recommended initialization scheme |\n| Residual Addition | Skip connections | Output added to input maintains gradient flow | Input + FFN(LayerNorm(Input)) pattern |\n\n**Layer Normalization Testing Strategy**\n\nLayer normalization testing validates the stabilization of activations across the feature dimension for each token independently. Testing must verify mean centering, variance scaling, learnable parameter application, and numerical stability across different input distributions.\n\nLayer normalization computes statistics across the feature dimension for each token in each sequence independently, then applies learned scale and shift parameters. This per-token normalization differs from batch normalization and requires specific validation of the statistical computations and parameter applications.\n\n| Normalization Aspect | Test Focus | Validation Method | Success Criteria |\n|----------------------|------------|-------------------|------------------|\n| Mean Centering | Zero mean per token | Compute mean across d_model dimension | Mean ≈ 0 within numerical precision |\n| Variance Scaling | Unit variance per token | Compute variance across d_model dimension | Variance ≈ 1 within numerical precision |\n| Parameter Application | Scale and shift | Apply learned gamma and beta parameters | Output = gamma * normalized + beta |\n| Epsilon Handling | Numerical stability | Very small variance inputs | No division by zero or NaN values |\n| Gradient Computation | Parameter updates | Gradients flow to gamma and beta | Non-zero gradients for learnable parameters |\n| Batch Independence | Per-token operation | Different sequences don't affect each other | Normalization statistics computed independently |\n\n**Tokenization and Data Loading Testing**\n\nTokenization and data loading components require validation of text-to-integer conversion, vocabulary handling, special token management, and batch construction. Testing must verify correct encoding and decoding, proper sequence length handling, and appropriate padding or truncation behavior.\n\nThe tokenization system converts between text strings and integer sequences that feed into the transformer. This conversion must be deterministic, reversible, and handle edge cases like unknown characters, empty strings, and sequences exceeding maximum length limits.\n\n| Component | Test Category | Validation Points | Expected Behavior |\n|-----------|---------------|-------------------|-------------------|\n| Text Encoding | String to tokens | Consistent tokenization of same text | Deterministic token ID sequences |\n| Text Decoding | Tokens to string | Reconstruction of original text | Exact or semantically equivalent text recovery |\n| Vocabulary Handling | Token ID bounds | All tokens within valid vocabulary range | Token IDs in [0, vocab_size) range |\n| Special Tokens | Padding and EOS | Correct insertion of special tokens | Proper pad_token_id and eos_token_id usage |\n| Sequence Length | Truncation and padding | Handling of variable-length sequences | Fixed-length sequences for batching |\n| Batch Construction | Data loader output | Correct input-target pair construction | Proper label shifting for next-token prediction |\n\n### End-to-End Integration\n\nEnd-to-end integration testing validates the complete transformer pipeline from text input to generated output, ensuring that all components work together correctly and produce expected behaviors. Integration testing catches issues that component unit tests miss, particularly around data flow, shape consistency, and behavioral correctness across component boundaries.\n\n**Training Pipeline Integration**\n\nTraining pipeline integration testing validates the complete learning process from tokenized input through gradient computation and parameter updates. Testing must verify that the model can overfit on small datasets, that loss decreases consistently, and that gradients flow properly through all components.\n\nThe training integration test starts with a minimal dataset that the model should easily memorize, then verifies that training loss decreases to near zero over multiple epochs. This overfitting test confirms that the model architecture can learn and that the training loop correctly computes gradients and updates parameters.\n\n> **Decision: Overfitting Test as Primary Integration Validation**\n> - **Context**: Need to verify complete training pipeline works correctly before testing on realistic datasets\n> - **Options Considered**: Large dataset convergence test, synthetic data pattern learning, small dataset overfitting test\n> - **Decision**: Use small dataset overfitting test as primary integration validation\n> - **Rationale**: Overfitting test isolates architectural issues from data complexity, provides fast feedback, and guarantees expected outcome if implementation is correct\n> - **Consequences**: Enables rapid validation of training correctness but doesn't test generalization capabilities until later testing phases\n\n| Integration Test | Dataset Characteristics | Success Criteria | Failure Indicators |\n|------------------|-------------------------|-------------------|---------------------|\n| Overfitting Test | 10-20 short sequences, repeated patterns | Training loss < 0.01 within 100 epochs | Loss plateaus above 1.0, gradients vanish |\n| Shape Consistency | Various sequence lengths and batch sizes | No dimension mismatch errors during training | Runtime errors about tensor shapes |\n| Gradient Flow | Deep transformer stack | Non-zero gradients reach embedding layer | Zero gradients in early layers |\n| Memory Management | Large batch sizes | Stable memory usage throughout training | Out-of-memory errors or memory leaks |\n| Checkpoint Saving | Training interruption and resumption | Identical loss trajectory after restart | Different loss values after checkpoint reload |\n| Validation Loop | Separate validation dataset | Validation loss computed without gradients | Validation loss changes during eval mode |\n\n**Generation Pipeline Integration**\n\nGeneration pipeline integration testing validates the autoregressive text generation process from input prompt to coherent output text. Testing must verify that generation produces valid token sequences, respects length limits, and applies sampling strategies correctly.\n\nThe generation integration test evaluates both the mechanical correctness of the generation loop and the quality of generated text. Mechanical correctness includes proper token prediction, sampling strategy application, and sequence termination. Text quality assessment focuses on coherence, repetition avoidance, and adherence to the input prompt.\n\n| Generation Aspect | Test Approach | Validation Method | Quality Indicators |\n|-------------------|---------------|-------------------|-------------------|\n| Token Prediction | Simple prompts | Verify logits shape and vocabulary bounds | Output logits have shape (1, vocab_size) |\n| Sampling Strategies | Deterministic vs stochastic | Compare greedy, temperature, top-k, top-p outputs | Different strategies produce different texts |\n| Sequence Termination | EOS token generation | Verify generation stops at EOS or max length | Generation respects configured length limits |\n| Prompt Conditioning | Specific input prompts | Generated text continues prompt appropriately | Output relates semantically to input prompt |\n| Repetition Handling | Repetition penalty application | Generated text avoids excessive repetition | Repetition penalty reduces token probability |\n| KV Cache Correctness | Cache vs non-cache generation | Identical outputs with and without caching | Cache optimization doesn't change generation |\n\n**Memory and Performance Integration**\n\nMemory and performance integration testing validates resource usage patterns, computational efficiency, and scalability characteristics across different model sizes and sequence lengths. Testing must verify that memory usage remains within expected bounds and that performance scales appropriately with model parameters.\n\nThe transformer architecture has specific memory and computational scaling characteristics that must be validated during integration testing. Attention mechanisms scale quadratically with sequence length, while feed-forward networks scale linearly with model dimension. These scaling patterns must be verified to match theoretical expectations.\n\n| Resource Aspect | Scaling Pattern | Measurement Method | Expected Behavior |\n|-----------------|-----------------|-------------------|-------------------|\n| Training Memory | Linear with batch size | Monitor GPU memory during training | Memory usage = base + batch_size * per_sample |\n| Attention Memory | Quadratic with sequence length | Measure attention matrix memory | Memory ∝ seq_length² * num_heads |\n| Generation Memory | Linear with sequence length | KV cache memory growth during generation | Memory grows linearly with generated tokens |\n| Training Speed | Linear with model parameters | Time per training step measurement | Step time ∝ parameter count |\n| Generation Speed | Constant per token | Time per generated token | Consistent token generation speed |\n| Parameter Scaling | Expected parameter count | Count trainable parameters | Parameters match theoretical calculation |\n\n### Milestone Verification\n\nMilestone verification provides structured checkpoints for validating progress through the transformer implementation. Each milestone has specific acceptance criteria that must be met before proceeding to the next phase, ensuring that foundational components work correctly before building dependent functionality.\n\n**Milestone 1: Self-Attention Verification**\n\nThe self-attention milestone verification focuses on the mathematical correctness and architectural consistency of the attention mechanism. Verification must confirm that attention computations follow the scaled dot-product formula, that multi-head attention operates correctly, and that causal masking prevents information leakage from future positions.\n\nAttention verification requires both mathematical validation and behavioral testing. Mathematical validation confirms that attention weights are computed according to the specified formula and that tensor dimensions match architectural requirements. Behavioral testing verifies that attention patterns make intuitive sense for language modeling tasks.\n\n| Verification Component | Test Method | Success Criteria | Validation Approach |\n|------------------------|-------------|------------------|---------------------|\n| Q, K, V Projections | Shape inspection | Projections produce (batch, seq, num_heads, d_k) tensors | Assert tensor shapes after linear transformations |\n| Attention Score Computation | Mathematical validation | Scores = Q@K^T / sqrt(d_k) exactly | Compare computed scores with manual calculation |\n| Causal Mask Application | Upper triangle verification | Future positions have attention weight 0 | Verify attention matrix upper triangle is zero |\n| Multi-head Concatenation | Dimension restoration | Concatenated heads restore d_model dimension | Assert output shape matches input embedding shape |\n| Attention Pattern Visualization | Heatmap inspection | Attention focuses on relevant previous tokens | Visual inspection of attention weight matrices |\n| Gradient Flow Validation | Backpropagation test | Gradients propagate to Q, K, V projections | Verify non-zero gradients after loss.backward() |\n\n**Milestone 2: Transformer Block Verification**\n\nThe transformer block milestone verification validates the complete transformer layer including attention, feed-forward network, layer normalization, and residual connections. Verification must confirm that all sub-components integrate correctly and that information flows properly through the block architecture.\n\nTransformer block verification tests the interaction between attention and feed-forward sublayers, proper placement of layer normalization, and effectiveness of residual connections for gradient flow. The verification must also confirm that dropout operates correctly during training and is disabled during inference.\n\n| Block Component | Verification Focus | Test Procedure | Expected Results |\n|-----------------|-------------------|----------------|------------------|\n| Sublayer Integration | Attention + FFN composition | Forward pass through complete block | Output shape matches input shape |\n| Residual Connections | Skip connection functionality | Compare with and without residual connections | Residual connections improve gradient flow |\n| Layer Normalization Placement | Pre-norm architecture | Verify LayerNorm applied before sublayers | LayerNorm(x) fed to attention and FFN |\n| Dropout Behavior | Training vs inference modes | Test dropout in both modes | Dropout active in training, disabled in inference |\n| Parameter Count | Expected parameter total | Count block parameters | Parameters match theoretical calculation |\n| Information Flow | Input to output transformation | End-to-end block processing | Block processes information without errors |\n\n**Milestone 3: Training Pipeline Verification**\n\nThe training pipeline milestone verification validates the complete learning system including tokenization, data loading, loss computation, and parameter optimization. Verification must confirm that the model can learn from training data and that all training components operate correctly.\n\nTraining pipeline verification requires both mechanical and learning validation. Mechanical validation confirms that data flows correctly through the training loop without errors. Learning validation confirms that the model actually learns patterns from training data by demonstrating decreasing loss and improved performance.\n\n| Pipeline Component | Verification Method | Success Indicators | Troubleshooting Signs |\n|--------------------|--------------------|--------------------|----------------------|\n| Tokenization Accuracy | Encode-decode test | Perfect reconstruction of input text | Text corruption indicates tokenization errors |\n| Data Loader Functionality | Batch inspection | Correct input-target pair construction | Shape mismatches indicate data loading issues |\n| Loss Computation | Cross-entropy validation | Loss decreases monotonically on training data | Increasing loss indicates gradient or learning rate issues |\n| Parameter Updates | Gradient inspection | Parameters change after optimizer step | Unchanged parameters indicate optimization issues |\n| Overfitting Capability | Small dataset memorization | Training loss approaches zero | Inability to overfit indicates architectural problems |\n| Validation Integration | Separate validation loop | Validation loss computed without affecting training | Validation affecting training indicates mode switching issues |\n\n**Milestone 4: Text Generation Verification**\n\nThe text generation milestone verification validates the autoregressive generation capability including sampling strategies, sequence management, and text quality. Verification must confirm that generation produces coherent text that follows the input prompt and respects generation constraints.\n\nGeneration verification encompasses both mechanical correctness and output quality assessment. Mechanical verification confirms that generation mechanics work without errors and respect configured constraints. Quality assessment evaluates whether generated text demonstrates learning and produces reasonable continuations of input prompts.\n\n| Generation Aspect | Verification Approach | Quality Metrics | Failure Indicators |\n|-------------------|----------------------|------------------|-------------------|\n| Basic Generation | Simple prompt completion | Generated text continues prompt logically | Immediate repetition or incoherent output |\n| Sampling Strategy Variety | Compare different sampling methods | Different strategies produce different outputs | All strategies produce identical text |\n| Length Control | Maximum length constraints | Generation respects max_length parameter | Generation exceeds specified length limits |\n| EOS Token Handling | End-of-sequence termination | Generation stops appropriately at EOS | Generation continues past EOS token |\n| Repetition Avoidance | Repetition penalty effectiveness | Generated text avoids excessive repetition | Pathological repetition patterns |\n| Prompt Adherence | Semantic continuation | Generated text relates to input prompt | Generated text ignores prompt context |\n\n### Implementation Guidance\n\nThe testing implementation requires a systematic approach to component validation, integration verification, and milestone checkpoints. The testing framework should provide clear feedback about implementation correctness and guide developers toward fixing identified issues.\n\n**Technology Recommendations for Testing**\n\n| Testing Component | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Unit Testing Framework | pytest with basic assertions | pytest with fixtures and parameterization |\n| Tensor Validation | Manual shape assertions | torch.testing.assert_close for numerical comparison |\n| Visualization Tools | matplotlib for attention heatmaps | wandb for experiment tracking and visualization |\n| Performance Monitoring | time.time() for basic timing | torch.profiler for detailed performance analysis |\n| Memory Tracking | torch.cuda.memory_allocated() | torch.cuda.profiler for memory timeline |\n| Integration Testing | Simple overfitting test | Comprehensive benchmark suite |\n\n**Recommended File Structure for Testing**\n\n```\nproject-root/\n  tests/\n    unit/\n      test_attention.py           ← attention mechanism unit tests\n      test_transformer_block.py   ← transformer block unit tests  \n      test_tokenizer.py           ← tokenization unit tests\n      test_data_loading.py        ← data loading unit tests\n    integration/\n      test_training_pipeline.py   ← end-to-end training tests\n      test_generation_pipeline.py ← end-to-end generation tests\n      test_memory_performance.py  ← resource usage tests\n    fixtures/\n      sample_data.txt             ← test data for integration tests\n      expected_outputs.json       ← expected results for validation\n    utils/\n      test_helpers.py             ← common testing utilities\n      visualization.py            ← attention visualization helpers\n  src/\n    transformer/\n      attention.py                ← attention implementation\n      transformer_block.py        ← transformer block implementation\n      training.py                 ← training pipeline implementation\n      generation.py               ← generation implementation\n```\n\n**Component Testing Infrastructure**\n\n```python\nimport torch\nimport pytest\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Dict, Any\nimport numpy as np\n\nclass TransformerTestSuite:\n    \"\"\"Comprehensive testing utilities for transformer components.\"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        self.config = config\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def create_test_inputs(self, batch_size: int = 2, seq_length: int = 10) -> torch.Tensor:\n        \"\"\"Create test input tensors with valid token IDs.\"\"\"\n        return torch.randint(0, self.config.vocab_size, \n                           (batch_size, seq_length), device=self.device)\n    \n    def validate_attention_output_shapes(self, attention: MultiHeadAttention, \n                                       input_tensor: torch.Tensor) -> bool:\n        \"\"\"Validate that attention produces correct output shapes.\"\"\"\n        # TODO: Create causal mask for input sequence\n        # TODO: Run forward pass through attention layer\n        # TODO: Assert output shape matches input shape\n        # TODO: Verify attention weights have correct dimensions\n        # TODO: Check that attention weights sum to 1 across key dimension\n        pass\n    \n    def test_attention_causality(self, attention: MultiHeadAttention,\n                               input_tensor: torch.Tensor) -> bool:\n        \"\"\"Verify that attention respects causal masking constraints.\"\"\"\n        # TODO: Extract attention weights from forward pass\n        # TODO: Verify upper triangular portion is zero (future masking)\n        # TODO: Check that each position only attends to previous positions\n        # TODO: Validate that diagonal and lower triangle have positive weights\n        pass\n    \n    def measure_gradient_flow(self, model: torch.nn.Module, \n                            loss: torch.Tensor) -> Dict[str, float]:\n        \"\"\"Analyze gradient magnitudes throughout the model.\"\"\"\n        # TODO: Perform backward pass on loss\n        # TODO: Extract gradients from each named parameter\n        # TODO: Compute gradient norms for each layer\n        # TODO: Return dictionary mapping layer names to gradient magnitudes\n        # TODO: Identify layers with vanishing gradients (norm < 1e-6)\n        pass\n\ndef plot_attention_weights(weights: torch.Tensor, tokens: List[str], \n                         layer: int, head: int) -> None:\n    \"\"\"Visualize attention patterns as heatmap.\"\"\"\n    # Complete implementation for attention visualization\n    weights_np = weights[layer, head].detach().cpu().numpy()\n    plt.figure(figsize=(10, 8))\n    plt.imshow(weights_np, cmap='Blues', interpolation='nearest')\n    plt.colorbar(label='Attention Weight')\n    plt.xticks(range(len(tokens)), tokens, rotation=45)\n    plt.yticks(range(len(tokens)), tokens)\n    plt.xlabel('Key Position')\n    plt.ylabel('Query Position')\n    plt.title(f'Attention Weights - Layer {layer}, Head {head}')\n    plt.tight_layout()\n    plt.show()\n\ndef count_parameters(model: torch.nn.Module) -> int:\n    \"\"\"Count total trainable parameters in model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef test_shape_consistency(model: torch.nn.Module, input_shape: Tuple[int, ...], \n                         device: torch.device) -> bool:\n    \"\"\"Verify model maintains correct tensor shapes throughout forward pass.\"\"\"\n    # Complete implementation for shape validation\n    model.eval()\n    test_input = torch.randint(0, 1000, input_shape, device=device)\n    \n    try:\n        with torch.no_grad():\n            output = model(test_input)\n        \n        expected_output_shape = (*input_shape, model.config.vocab_size)\n        assert output.shape == expected_output_shape, \\\n            f\"Expected output shape {expected_output_shape}, got {output.shape}\"\n        \n        return True\n    except Exception as e:\n        print(f\"Shape consistency test failed: {e}\")\n        return False\n\ndef check_gradient_flow(model: torch.nn.Module, loss: torch.Tensor) -> Dict[str, Any]:\n    \"\"\"Analyze gradient magnitudes after backward pass.\"\"\"\n    # Complete implementation for gradient analysis\n    gradient_info = {}\n    \n    loss.backward()\n    \n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.data.norm().item()\n            gradient_info[name] = {\n                'grad_norm': grad_norm,\n                'param_norm': param.data.norm().item(),\n                'grad_param_ratio': grad_norm / (param.data.norm().item() + 1e-8)\n            }\n        else:\n            gradient_info[name] = {'grad_norm': 0.0, 'param_norm': 0.0, 'grad_param_ratio': 0.0}\n    \n    return gradient_info\n```\n\n**Integration Testing Infrastructure**\n\n```python\nclass TransformerIntegrationTester:\n    \"\"\"End-to-end integration testing for transformer pipeline.\"\"\"\n    \n    def __init__(self, config: TransformerConfig, training_config: TrainingConfig):\n        self.config = config\n        self.training_config = training_config\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def create_overfitting_dataset(self) -> Tuple[List[str], List[str]]:\n        \"\"\"Create small dataset for overfitting test.\"\"\"\n        # Complete implementation with small, repetitive dataset\n        train_texts = [\n            \"The cat sat on the mat.\",\n            \"The dog ran in the park.\",\n            \"The bird flew in the sky.\",\n            \"The fish swam in the water.\",\n            \"The cat sat on the mat.\"  # Repeat for memorization\n        ] * 4  # Replicate for sufficient training data\n        \n        val_texts = train_texts[:2]  # Small validation set\n        return train_texts, val_texts\n    \n    def test_training_overfitting(self, model: torch.nn.Module, \n                                tokenizer: SimpleTokenizer) -> Dict[str, Any]:\n        \"\"\"Test that model can overfit on small dataset.\"\"\"\n        # TODO: Create small training dataset with repetitive patterns\n        # TODO: Set up training loop with high learning rate\n        # TODO: Train for multiple epochs until loss < 0.01\n        # TODO: Record loss trajectory and final loss value\n        # TODO: Verify that model memorizes training sequences exactly\n        # TODO: Return training metrics and success indicators\n        pass\n    \n    def test_generation_quality(self, model: torch.nn.Module, \n                              tokenizer: SimpleTokenizer,\n                              generator: TextGenerator) -> Dict[str, Any]:\n        \"\"\"Evaluate generation quality on various prompts.\"\"\"\n        # TODO: Define test prompts covering different scenarios\n        # TODO: Generate text using different sampling strategies\n        # TODO: Evaluate coherence, repetition, and prompt adherence\n        # TODO: Check for pathological patterns (infinite loops, repetition)\n        # TODO: Measure generation speed and memory usage\n        # TODO: Return quality metrics and generated samples\n        pass\n    \n    def validate_kv_cache_correctness(self, model: torch.nn.Module,\n                                    generator: TextGenerator) -> bool:\n        \"\"\"Verify KV cache optimization produces identical results.\"\"\"\n        # TODO: Generate text with KV cache enabled\n        # TODO: Generate same text with KV cache disabled  \n        # TODO: Compare outputs token by token for exact match\n        # TODO: Measure speed improvement from caching\n        # TODO: Verify memory usage patterns are as expected\n        pass\n\ndef create_data_loaders(train_tokens: List[int], val_tokens: List[int], \n                       seq_length: int, batch_size: int, \n                       num_workers: int = 0) -> Tuple[torch.utils.data.DataLoader, \n                                                     torch.utils.data.DataLoader]:\n    \"\"\"Create train and validation data loaders for testing.\"\"\"\n    # Complete implementation for data loader creation\n    train_dataset = TextDataset(train_tokens, seq_length)\n    val_dataset = TextDataset(val_tokens, seq_length)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, \n        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n    )\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n    )\n    \n    return train_loader, val_loader\n```\n\n**Milestone Checkpoint Implementation**\n\n```python\nclass MilestoneValidator:\n    \"\"\"Automated validation for transformer implementation milestones.\"\"\"\n    \n    def validate_milestone_1_attention(self, attention: MultiHeadAttention,\n                                     config: TransformerConfig) -> Dict[str, bool]:\n        \"\"\"Validate self-attention implementation (Milestone 1).\"\"\"\n        results = {}\n        \n        # TODO: Test Q, K, V projection dimensions\n        # Create test input: (batch=2, seq=8, d_model)\n        # Verify projections produce (batch=2, seq=8, num_heads, d_k) tensors\n        # Check that d_k * num_heads == d_model for dimension consistency\n        \n        # TODO: Test attention score computation\n        # Manually compute Q@K^T/sqrt(d_k) and compare with attention output\n        # Verify scaling factor is applied correctly\n        # Check numerical stability with large input values\n        \n        # TODO: Test causal mask application  \n        # Verify upper triangle of attention weights is exactly zero\n        # Check that mask is applied before softmax, not after\n        # Ensure diagonal and lower triangle have positive attention weights\n        \n        # TODO: Test multi-head concatenation\n        # Verify concatenated output has shape (batch, seq, d_model)\n        # Check that final projection restores original embedding dimension\n        # Ensure all heads contribute to final output\n        \n        return results\n    \n    def validate_milestone_2_transformer_block(self, block: TransformerBlock,\n                                             config: TransformerConfig) -> Dict[str, bool]:\n        \"\"\"Validate transformer block implementation (Milestone 2).\"\"\"\n        results = {}\n        \n        # TODO: Test feed-forward network expansion\n        # Verify FFN expands to 4x hidden dimension then projects back\n        # Check that activation function is applied between layers\n        # Ensure output dimension matches input dimension\n        \n        # TODO: Test layer normalization behavior\n        # Verify mean ≈ 0 and variance ≈ 1 across feature dimension per token\n        # Check that learned gamma and beta parameters are applied\n        # Ensure normalization is applied before sublayers (pre-norm)\n        \n        # TODO: Test residual connections\n        # Verify input is added to sublayer output: output = input + sublayer(norm(input))\n        # Check gradient flow improvement with residuals vs without\n        # Ensure residual paths preserve gradient magnitudes\n        \n        # TODO: Test dropout functionality\n        # Verify dropout is active during training mode\n        # Check that dropout is disabled during eval mode\n        # Ensure dropout rate matches configuration\n        \n        return results\n    \n    def validate_milestone_3_training_pipeline(self, trainer: TransformerTrainer,\n                                             tokenizer: SimpleTokenizer) -> Dict[str, bool]:\n        \"\"\"Validate training pipeline implementation (Milestone 3).\"\"\"\n        results = {}\n        \n        # TODO: Test tokenization accuracy\n        # Encode and decode text strings, verify perfect reconstruction\n        # Check special token handling (pad_token_id, eos_token_id)\n        # Ensure vocabulary bounds are respected\n        \n        # TODO: Test data loader functionality\n        # Verify batch construction with correct input-target pairs\n        # Check label shifting for next-token prediction\n        # Ensure sequence length handling and padding behavior\n        \n        # TODO: Test loss computation\n        # Verify cross-entropy loss decreases on training data\n        # Check that loss ignores padding tokens in computation\n        # Ensure gradients are computed correctly\n        \n        # TODO: Test parameter updates\n        # Verify parameters change after optimizer step\n        # Check learning rate scheduling if implemented\n        # Ensure gradient clipping prevents explosion\n        \n        return results\n    \n    def validate_milestone_4_text_generation(self, generator: TextGenerator,\n                                           config: GenerationConfig) -> Dict[str, bool]:\n        \"\"\"Validate text generation implementation (Milestone 4).\"\"\"\n        results = {}\n        \n        # TODO: Test greedy decoding\n        # Verify deterministic selection of highest probability token\n        # Check that same prompt always produces same output\n        # Ensure greedy decoding respects length limits\n        \n        # TODO: Test temperature sampling\n        # Verify temperature scaling affects output randomness\n        # Check that temperature=0 approaches greedy decoding\n        # Ensure higher temperatures increase diversity\n        \n        # TODO: Test top-k and top-p sampling\n        # Verify top-k restricts to k most likely tokens\n        # Check top-p restricts to nucleus of cumulative probability p\n        # Ensure both methods respect temperature parameter\n        \n        # TODO: Test sequence management\n        # Verify generation stops at EOS token or max length\n        # Check that generated tokens are within vocabulary bounds\n        # Ensure repetition penalty reduces repetitive patterns\n        \n        return results\n\n# Debugging utilities for systematic issue diagnosis\ndef diagnose_attention_issues(attention: MultiHeadAttention, \n                            input_tensor: torch.Tensor) -> Dict[str, Any]:\n    \"\"\"Systematic diagnosis of attention mechanism problems.\"\"\"\n    diagnostics = {}\n    \n    # TODO: Check input tensor properties\n    # Verify input shape, data type, device placement\n    # Check for NaN or infinite values in input\n    # Ensure input values are within reasonable range\n    \n    # TODO: Analyze attention weight patterns\n    # Extract and visualize attention weight matrices\n    # Check for degenerate patterns (all attention on first/last token)\n    # Verify attention weights sum to 1 across key dimension\n    \n    # TODO: Test gradient flow through attention\n    # Compute gradients with respect to Q, K, V projections\n    # Check for vanishing gradients in deep attention stacks\n    # Verify gradient magnitudes are reasonable\n    \n    return diagnostics\n\ndef diagnose_training_issues(trainer: TransformerTrainer, \n                           train_loader: torch.utils.data.DataLoader) -> Dict[str, Any]:\n    \"\"\"Systematic diagnosis of training pipeline problems.\"\"\"\n    diagnostics = {}\n    \n    # TODO: Analyze loss trajectory\n    # Check if loss is decreasing, increasing, or plateauing\n    # Identify potential learning rate issues\n    # Detect gradient explosion or vanishing symptoms\n    \n    # TODO: Examine data loading\n    # Verify batch construction and label shifting\n    # Check for data corruption or preprocessing errors\n    # Ensure consistent data loader behavior across epochs\n    \n    # TODO: Monitor resource usage\n    # Track memory usage patterns during training\n    # Check for memory leaks or excessive allocations\n    # Monitor GPU utilization and potential bottlenecks\n    \n    return diagnostics\n```\n\nThe testing strategy provides comprehensive validation across all transformer components and milestones. Component unit tests ensure mathematical correctness and architectural consistency at the individual module level. Integration tests validate end-to-end behavior and catch issues that emerge from component interactions. Milestone verification provides structured checkpoints that guide systematic implementation progress.\n\nThe testing infrastructure includes both automated validation and manual inspection tools. Automated tests catch regression errors and verify correctness properties that can be mathematically validated. Manual inspection tools like attention visualization and gradient flow analysis help developers understand model behavior and diagnose subtle issues that automated tests might miss.\n\nSuccessful completion of all milestone validations ensures that the transformer implementation meets the specified requirements and demonstrates expected behavior across self-attention mechanisms, transformer blocks, training pipelines, and text generation capabilities.\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones - systematic diagnosis and resolution of implementation bugs across self-attention mechanism (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4)\n\nDebugging transformer implementations requires a systematic approach that addresses the unique challenges of attention mechanisms, gradient-based training, and autoregressive generation. Unlike simpler neural networks, transformers involve complex matrix operations with specific dimension requirements, numerical stability concerns, and interdependent components where bugs in one layer can cascade through the entire architecture.\n\n### Mental Model: The Debugging Detective\n\nThink of debugging a transformer like being a detective investigating a crime scene. You have multiple suspects (attention weights, gradient flows, sampling strategies), various pieces of evidence (loss curves, attention heatmaps, generated text quality), and you need to systematically eliminate possibilities to find the root cause. Just as a detective follows a methodical process - securing the scene, gathering evidence, forming hypotheses, testing theories - transformer debugging requires structured investigation rather than random code changes.\n\nThe key insight is that transformer bugs often manifest in predictable patterns. A dimension mismatch in attention computation creates specific error signatures. Gradient explosion produces characteristic loss spikes. Poor generation quality has distinct patterns based on the underlying cause. By learning to recognize these patterns and following systematic diagnostic procedures, you can quickly isolate and fix even complex bugs.\n\n### Attention Mechanism Debugging\n\nThe self-attention mechanism represents the most mathematically complex component of transformers, making it a frequent source of implementation bugs. Attention debugging requires understanding both the mathematical relationships between queries, keys, and values, and the expected patterns in attention weight distributions.\n\n**Attention Weight Pattern Analysis**\n\nHealthy attention patterns exhibit specific characteristics that reveal whether the mechanism is functioning correctly. In causal self-attention, each position should only attend to previous positions, creating a lower-triangular pattern when visualized. The attention weights for each query position should sum to 1.0 after softmax normalization, and the distribution should show meaningful focus on relevant tokens rather than uniform attention across all positions.\n\n| Pattern Type | Expected Behavior | Problematic Signs | Diagnostic Action |\n|--------------|------------------|-------------------|-------------------|\n| Causal Masking | Lower triangular attention matrix | Upper triangular has non-zero values | Check mask application order |\n| Weight Normalization | Each row sums to 1.0 | Row sums deviate significantly | Verify softmax implementation |\n| Attention Focus | Concentrated weights on relevant tokens | Uniform distribution across all positions | Check scaling and temperature |\n| Multi-Head Diversity | Different heads show different patterns | All heads identical or random | Verify head initialization |\n| Position Encoding | Positional patterns in early layers | No positional structure visible | Check positional embedding addition |\n\n**Dimension Mismatch Diagnosis**\n\nAttention mechanisms involve multiple matrix operations with strict dimension requirements. The most common dimension errors occur during the scaled dot-product computation, multi-head concatenation, and output projection. These errors often produce cryptic PyTorch error messages that don't clearly indicate the source of the problem.\n\nThe query, key, and value tensors must maintain consistent dimensions throughout the attention computation. For batch size `B`, sequence length `S`, number of heads `H`, and head dimension `D_K`, the expected shapes are: Q, K, V should be `[B, H, S, D_K]` after head projection and reshaping. The attention scores matrix should be `[B, H, S, S]`, and the final output should be `[B, S, d_model]` after concatenation and projection.\n\n| Operation Stage | Input Shapes | Output Shape | Common Errors |\n|----------------|--------------|--------------|---------------|\n| Initial Projection | `[B, S, d_model]` → QKV | `[B, S, d_model]` each | Wrong projection dimensions |\n| Head Reshaping | `[B, S, d_model]` | `[B, H, S, d_k]` | Incorrect reshape calculation |\n| Score Computation | Q:`[B, H, S, d_k]`, K:`[B, H, S, d_k]` | `[B, H, S, S]` | Transpose applied incorrectly |\n| Score Masking | Scores:`[B, H, S, S]`, Mask:`[S, S]` | `[B, H, S, S]` | Broadcasting dimension mismatch |\n| Value Weighting | Weights:`[B, H, S, S]`, V:`[B, H, S, d_v]` | `[B, H, S, d_v]` | Value dimension inconsistency |\n| Head Concatenation | `[B, H, S, d_v]` | `[B, S, H*d_v]` | Concatenation axis wrong |\n| Output Projection | `[B, S, H*d_v]` | `[B, S, d_model]` | Final projection size mismatch |\n\n**Attention Score Stability Issues**\n\nNumerical instability in attention computation manifests as exploding or vanishing gradients, NaN values in attention weights, or attention collapse where all weights concentrate on a single position. The scaled dot-product attention is particularly sensitive to the magnitude of query and key vectors, requiring proper scaling by the square root of the key dimension.\n\nTemperature control becomes critical when attention scores grow too large. Without proper scaling, the softmax function can produce extremely peaked distributions that create vanishing gradients for non-selected positions. Conversely, insufficient scaling can lead to overly uniform attention that fails to focus on relevant information.\n\n| Stability Issue | Symptoms | Root Cause | Diagnostic Check | Solution |\n|-----------------|----------|------------|------------------|----------|\n| Attention Collapse | All weights focus on one position | Unscaled dot products too large | Check score magnitudes before softmax | Apply temperature scaling |\n| Uniform Attention | Weights spread equally across positions | Scores too small relative to softmax | Examine score variance | Reduce temperature or check scaling |\n| NaN in Weights | Attention matrix contains NaN values | Softmax overflow from large scores | Log maximum score values | Implement LogSumExp trick |\n| Gradient Explosion | Loss spikes during training | Attention gradients too large | Monitor attention gradient norms | Apply gradient clipping |\n| Vanishing Focus | Model ignores relevant context | Poor initialization or learning rate | Visualize attention evolution | Adjust learning rate schedule |\n\n⚠️ **Pitfall: Applying Causal Mask After Softmax**\n\nA common mistake is applying the causal mask after the softmax operation instead of before. This produces attention weights that don't sum to 1.0 and breaks the fundamental assumption of attention as a probability distribution. The mask should set future positions to negative infinity before softmax, allowing the exponential function to naturally zero out these positions. Applying the mask after softmax by simply setting values to zero destroys the normalization property and can cause numerical instabilities in gradient computation.\n\n⚠️ **Pitfall: Incorrect Multi-Head Dimension Calculations**\n\nMany implementations incorrectly calculate the per-head dimension by dividing `d_model` by `num_heads` without ensuring the division is exact. When `d_model` is not evenly divisible by `num_heads`, this creates dimension mismatches during concatenation. The head dimension `d_k` should be explicitly defined and verified that `num_heads * d_k == d_model` before beginning attention computation.\n\n### Training Issues\n\nTraining transformer models involves complex interactions between gradient computation, parameter updates, and numerical stability. Training issues often manifest gradually, making them challenging to diagnose without systematic monitoring of training dynamics.\n\n**Loss Convergence Problems**\n\nTransformer training loss should follow a predictable pattern: initial rapid decrease as the model learns basic token distributions, followed by slower convergence as it learns longer-range dependencies. Deviations from this pattern indicate specific implementation problems or hyperparameter issues.\n\nNon-decreasing loss often results from learning rate problems, gradient computation errors, or data preprocessing issues. The language modeling objective should show consistent improvement when the model architecture and training loop are implemented correctly. Oscillating loss indicates instability in the optimization process, while sudden loss spikes suggest gradient explosion or numerical overflow.\n\n| Loss Pattern | Probable Cause | Diagnostic Steps | Resolution Strategy |\n|-------------|----------------|------------------|-------------------|\n| Flat/Non-decreasing | Learning rate too low or data issues | Check gradient magnitudes, verify data loading | Increase learning rate, inspect training data |\n| Oscillating Wildly | Learning rate too high or gradient instability | Monitor gradient norms, check loss smoothing | Reduce learning rate, add gradient clipping |\n| Sudden Spikes | Gradient explosion or numerical overflow | Log gradient norms, check for NaN/inf values | Implement gradient clipping, reduce learning rate |\n| Rapid Initial Drop Then Plateau | Model memorizing instead of generalizing | Compare train/validation loss, check overfitting | Add dropout, reduce model size, increase data |\n| Negative Loss Values | Incorrect loss computation | Verify cross-entropy implementation | Fix loss calculation, check label preprocessing |\n\n**Gradient Flow Analysis**\n\nHealthy gradient flow is essential for transformer training success. Gradients should decrease in magnitude from the output layer toward the input layer, but not vanish completely in early layers. The residual connections and layer normalization are specifically designed to maintain gradient flow through the deep transformer stack.\n\nVanishing gradients manifest as extremely small gradient magnitudes in early transformer layers, preventing these layers from learning meaningful representations. Exploding gradients appear as exponentially increasing gradient norms that cause parameter updates to overshoot optimal values. Both conditions can be diagnosed by monitoring gradient statistics throughout the network.\n\n| Layer Position | Expected Gradient Magnitude | Vanishing Signs | Exploding Signs | Mitigation |\n|----------------|---------------------------|-----------------|-----------------|------------|\n| Output Layer | Largest gradients (1e-2 to 1e-1) | Gradients below 1e-6 | Gradients above 1e2 | Adjust loss scaling |\n| Middle Layers | Moderate gradients (1e-3 to 1e-2) | Exponential decrease toward input | Exponential increase from output | Check residual connections |\n| Input Layers | Smallest but non-zero (1e-4 to 1e-3) | Near-zero gradients | Unstable oscillations | Verify layer norm placement |\n| Embedding Layer | Small gradients (1e-5 to 1e-4) | Zero gradients | Large embedding updates | Consider embedding freezing |\n\n**Learning Rate Schedule Problems**\n\nTransformer training benefits from learning rate warmup followed by decay, but incorrect scheduling can prevent convergence or cause instability. The warmup phase allows the model to stabilize before applying full learning rates, while the decay phase enables fine-tuning of learned representations.\n\nInsufficient warmup can cause early training instability, particularly in attention mechanisms that are sensitive to initialization. Excessive warmup delays learning and wastes computational resources. Similarly, aggressive decay can prematurely stop learning, while insufficient decay can prevent final convergence.\n\n> **Decision: Pre-Layer Normalization with Warmup Scheduling**\n> - **Context**: Transformers require careful coordination between normalization placement and learning rate scheduling to achieve stable training\n> - **Options Considered**: Post-norm with standard scheduling, pre-norm with warmup, mixed normalization strategies\n> - **Decision**: Pre-layer normalization with linear warmup over 2000 steps followed by cosine decay\n> - **Rationale**: Pre-norm provides better gradient flow and stability, while warmup prevents early training instability from random initialization\n> - **Consequences**: Enables stable training of deeper models but requires tuning warmup steps for different model sizes\n\n⚠️ **Pitfall: Incorrect Label Shifting**\n\nLanguage modeling requires shifting the target sequence by one position relative to the input sequence, so the model predicts the next token at each position. A common error is failing to implement this shift correctly, either by not shifting at all or shifting in the wrong direction. The input sequence should be `[token_0, token_1, ..., token_n-1]` while the target sequence should be `[token_1, token_2, ..., token_n]`, ensuring each input position predicts the subsequent token.\n\n⚠️ **Pitfall: Missing Gradient Accumulation in Large Batch Training**\n\nWhen memory constraints prevent using large batch sizes directly, gradient accumulation simulates larger batches by accumulating gradients over multiple forward passes before updating parameters. A common mistake is forgetting to scale the accumulated gradients by the number of accumulation steps, leading to effectively larger learning rates than intended. The accumulated gradients should be divided by the accumulation factor before the optimizer step.\n\n### Generation Quality Issues\n\nText generation problems often stem from improper sampling strategies, KV cache implementation errors, or insufficient training. Unlike training bugs that produce clear error messages, generation issues manifest as poor text quality that requires qualitative assessment.\n\n**Repetitive Text Patterns**\n\nRepetitive generation is one of the most common quality issues in transformer text generation. The model may repeat individual tokens, phrases, or entire sequences. This behavior often results from overconfident predictions, poor sampling strategies, or inadequate training diversity.\n\nToken-level repetition occurs when the model assigns extremely high probability to recently generated tokens, creating loops where the same token is selected repeatedly. Phrase-level repetition happens when the model learns strong sequential patterns during training but lacks diversity mechanisms during generation. Sequence-level repetition indicates that the model has memorized training examples and reproduces them during generation.\n\n| Repetition Type | Pattern Example | Probable Cause | Sampling Fix | Training Fix |\n|----------------|-----------------|----------------|--------------|---------------|\n| Token Repetition | \"the the the the...\" | Overconfident predictions | Apply repetition penalty | Increase training diversity |\n| Phrase Repetition | \"in the world in the world...\" | Poor sampling diversity | Use top-p sampling | Add dropout during training |\n| Sequence Repetition | Reproducing training examples verbatim | Model memorization | Increase temperature | Larger training dataset |\n| Pattern Loops | Cycling through fixed patterns | Insufficient context modeling | Implement cycle detection | Longer sequence training |\n| Semantic Repetition | Rephrasing same idea repeatedly | Lack of semantic diversity | Semantic similarity penalty | Diverse training objectives |\n\n**Incoherent Output Generation**\n\nIncoherent text manifests as grammatically incorrect sentences, logical inconsistencies, or failure to maintain context over longer sequences. These issues often indicate problems with attention mechanism training, insufficient model capacity, or inappropriate sampling parameters.\n\nGrammatical incoherence suggests the model hasn't learned proper language structure, often due to insufficient training or poor tokenization. Logical incoherence indicates failure to maintain semantic consistency, typically resulting from attention mechanisms that don't properly model long-range dependencies. Contextual incoherence shows that the model loses track of conversation context or narrative flow.\n\n| Coherence Issue | Symptoms | Attention Problem | Context Problem | Sampling Problem |\n|----------------|----------|------------------|-----------------|------------------|\n| Grammar Errors | Incorrect verb tense, word order | Poor local attention focus | N/A | Temperature too high |\n| Logic Breaks | Contradictory statements | Failure to attend to relevant context | Context window too short | Insufficient top-k filtering |\n| Context Loss | Forgetting previous information | Attention weights too uniform | Exceeding trained sequence length | Greedy decoding too rigid |\n| Topic Drift | Gradual shift away from subject | No attention on topic-relevant tokens | Insufficient topic reinforcement | Need topic-aware sampling |\n| Factual Errors | Incorrect information generation | Training data quality issues | Limited factual training | Confidence-based filtering needed |\n\n**Sampling Strategy Optimization**\n\nDifferent sampling strategies produce distinct quality characteristics and failure modes. Greedy decoding provides deterministic, locally optimal choices but can lead to repetitive or overly conservative text. Temperature sampling introduces randomness but may produce incoherent results with poor temperature tuning. Top-k and top-p sampling balance quality and diversity but require careful parameter selection.\n\nUnderstanding the interaction between sampling parameters is crucial for achieving desired generation characteristics. Temperature affects the sharpness of the probability distribution, with lower values producing more focused selections and higher values increasing randomness. Top-k filtering restricts candidates to the k most likely tokens, while top-p (nucleus sampling) dynamically adjusts the candidate set based on cumulative probability.\n\n| Strategy | Best Use Case | Quality Characteristics | Failure Mode | Parameter Tuning |\n|----------|---------------|------------------------|--------------|------------------|\n| Greedy | Factual text, code generation | Deterministic, locally optimal | Repetitive, conservative | N/A - no parameters |\n| Temperature | Creative writing, dialogue | Controllable randomness | Incoherent at high temps | 0.7-0.9 for creativity, 0.1-0.3 for focus |\n| Top-k | Balanced quality/diversity | Consistent candidate pool | Fixed pool may be too large/small | k=40-50 typical, adjust based on vocab |\n| Top-p | Adaptive diversity control | Dynamic candidate selection | Complex parameter interaction | p=0.9-0.95 typical, combine with temperature |\n| Combined | Production systems | Best quality-diversity trade-off | Complex tuning required | Start with temp=0.8, top-p=0.9, top-k=50 |\n\n⚠️ **Pitfall: KV Cache Dimension Mismatches**\n\nKV caching optimization stores computed key and value tensors from previous generation steps to avoid redundant computation. A common error is incorrectly managing cache dimensions when concatenating new keys and values with cached tensors. The cache must maintain consistent batch, head, and feature dimensions while correctly extending the sequence dimension. Dimension mismatches in the cache can cause subtle bugs that only appear during longer generation sequences.\n\n⚠️ **Pitfall: Temperature Zero Division**\n\nSetting temperature to exactly 0.0 for deterministic generation causes division by zero in the temperature scaling operation. While the intent is to make sampling deterministic, the correct approach is either using greedy decoding directly or setting temperature to a very small positive value (e.g., 1e-8). The division by zero error can crash generation or produce NaN values that propagate through subsequent computations.\n\n### Implementation Guidance\n\nDebugging transformers effectively requires systematic approaches to identify, isolate, and resolve issues across the attention mechanism, training process, and text generation pipeline. The following guidance provides concrete tools and techniques for diagnosing common problems.\n\n**Technology Recommendations for Debugging**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Attention Visualization | `matplotlib` with heatmaps | `tensorboard` with custom scalars |\n| Gradient Monitoring | Manual logging with `torch.nn.utils.clip_grad_norm_` | `wandb` with automatic gradient tracking |\n| Loss Tracking | Simple CSV logging | `tensorboard` or `wandb` with real-time plotting |\n| Memory Profiling | `torch.cuda.memory_summary()` | `torch.profiler` with detailed analysis |\n| Model Inspection | Custom print statements | `torchinfo` for detailed model summaries |\n\n**Recommended Debugging Module Structure**\n\n```\nproject-root/\n  debug/\n    attention_visualizer.py      ← attention weight plotting and analysis\n    gradient_monitor.py          ← gradient flow tracking and clipping\n    loss_analyzer.py            ← training loss pattern analysis\n    generation_tester.py        ← text generation quality assessment\n    numerical_stability.py      ← overflow/underflow detection\n  tests/\n    test_attention_shapes.py    ← dimension consistency tests\n    test_training_flow.py       ← end-to-end training validation\n    test_generation_quality.py  ← automated quality checks\n  utils/\n    debug_helpers.py           ← common debugging utilities\n```\n\n**Attention Debugging Infrastructure**\n\n```python\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\n\nclass AttentionDebugger:\n    \"\"\"Comprehensive attention mechanism debugging utilities.\"\"\"\n    \n    def __init__(self, model_config: TransformerConfig):\n        self.config = model_config\n        self.attention_history = []\n        \n    def plot_attention_weights(self, weights: torch.Tensor, tokens: List[str], \n                             layer: int, head: int, save_path: Optional[str] = None):\n        \"\"\"\n        Visualize attention weights as a heatmap for debugging attention patterns.\n        \n        Args:\n            weights: Attention weights tensor [seq_len, seq_len]\n            tokens: List of token strings for axis labels\n            layer: Layer index for title\n            head: Head index for title\n            save_path: Optional path to save the plot\n        \"\"\"\n        # TODO 1: Convert weights tensor to numpy and ensure 2D shape\n        # TODO 2: Create matplotlib figure with appropriate size\n        # TODO 3: Generate heatmap with token labels on both axes\n        # TODO 4: Add colorbar and title indicating layer/head\n        # TODO 5: Save plot if path provided, otherwise display\n        pass\n        \n    def validate_attention_shapes(self, q: torch.Tensor, k: torch.Tensor, \n                                v: torch.Tensor, mask: torch.Tensor) -> Dict[str, bool]:\n        \"\"\"\n        Comprehensive shape validation for attention inputs.\n        \n        Returns:\n            Dictionary mapping validation check names to pass/fail status\n        \"\"\"\n        # TODO 1: Check Q, K, V have matching batch and sequence dimensions\n        # TODO 2: Verify head dimension consistency across Q, K, V\n        # TODO 3: Validate mask shape matches attention score dimensions\n        # TODO 4: Ensure all tensors are on same device\n        # TODO 5: Return detailed validation results\n        pass\n        \n    def diagnose_attention_patterns(self, weights: torch.Tensor) -> Dict[str, float]:\n        \"\"\"\n        Analyze attention weight patterns for common issues.\n        \n        Returns:\n            Dictionary of diagnostic metrics and their values\n        \"\"\"\n        # TODO 1: Check if attention is properly normalized (rows sum to 1)\n        # TODO 2: Measure attention entropy to detect uniform vs focused patterns\n        # TODO 3: Validate causal mask effectiveness (upper triangle should be zero)\n        # TODO 4: Compute attention concentration metrics\n        # TODO 5: Return comprehensive diagnostics dictionary\n        pass\n```\n\n**Training Debugging Infrastructure**\n\n```python\nclass TrainingDebugger:\n    \"\"\"Monitor training dynamics and diagnose convergence issues.\"\"\"\n    \n    def __init__(self, config: TrainingConfig):\n        self.config = config\n        self.loss_history = []\n        self.gradient_history = []\n        \n    def check_gradient_flow(self, model: torch.nn.Module, \n                          loss: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Analyze gradient magnitudes throughout the model after backward pass.\n        \n        Returns:\n            Dictionary mapping layer names to gradient statistics\n        \"\"\"\n        # TODO 1: Ensure loss.backward() has been called\n        # TODO 2: Iterate through named parameters with gradients\n        # TODO 3: Compute gradient norms for each layer\n        # TODO 4: Identify layers with vanishing or exploding gradients\n        # TODO 5: Return comprehensive gradient analysis\n        pass\n        \n    def monitor_training_stability(self, current_loss: float, \n                                 gradient_norm: float) -> Dict[str, str]:\n        \"\"\"\n        Detect training instability patterns and suggest fixes.\n        \n        Returns:\n            Dictionary of stability issues and recommended actions\n        \"\"\"\n        # TODO 1: Check for loss spikes indicating gradient explosion\n        # TODO 2: Detect loss plateaus suggesting learning rate issues\n        # TODO 3: Monitor gradient norm trends for stability\n        # TODO 4: Compare current metrics to historical patterns\n        # TODO 5: Generate specific recommendations for detected issues\n        pass\n        \n    def validate_loss_computation(self, logits: torch.Tensor, \n                                targets: torch.Tensor, \n                                computed_loss: torch.Tensor) -> bool:\n        \"\"\"\n        Verify cross-entropy loss computation correctness.\n        \n        Returns:\n            True if loss computation is correct, False otherwise\n        \"\"\"\n        # TODO 1: Compute reference loss using torch.nn.functional.cross_entropy\n        # TODO 2: Check for proper label shifting in language modeling\n        # TODO 3: Verify padding tokens are properly masked in loss\n        # TODO 4: Compare computed loss to reference within tolerance\n        # TODO 5: Return validation result with detailed error info\n        pass\n```\n\n**Generation Quality Assessment**\n\n```python\nclass GenerationDebugger:\n    \"\"\"Assess and debug text generation quality issues.\"\"\"\n    \n    def __init__(self, tokenizer: SimpleTokenizer):\n        self.tokenizer = tokenizer\n        self.generation_samples = []\n        \n    def analyze_repetition_patterns(self, generated_text: str) -> Dict[str, float]:\n        \"\"\"\n        Quantify different types of repetition in generated text.\n        \n        Returns:\n            Dictionary with repetition metrics and severity scores\n        \"\"\"\n        # TODO 1: Tokenize generated text for analysis\n        # TODO 2: Detect immediate token repetitions (n-gram analysis)\n        # TODO 3: Identify phrase-level repetitions using sliding windows\n        # TODO 4: Measure semantic repetition using embedding similarity\n        # TODO 5: Return comprehensive repetition analysis\n        pass\n        \n    def assess_coherence_quality(self, prompt: str, \n                               generated_text: str) -> Dict[str, float]:\n        \"\"\"\n        Evaluate multiple dimensions of text coherence.\n        \n        Returns:\n            Dictionary of coherence scores across different dimensions\n        \"\"\"\n        # TODO 1: Analyze grammatical correctness using basic heuristics\n        # TODO 2: Check logical consistency between sentences\n        # TODO 3: Measure context preservation relative to prompt\n        # TODO 4: Evaluate topic drift over generation length\n        # TODO 5: Return multi-dimensional coherence assessment\n        pass\n        \n    def optimize_sampling_parameters(self, prompt: str, \n                                   num_samples: int = 10) -> Dict[str, float]:\n        \"\"\"\n        Test different sampling parameter combinations to find optimal settings.\n        \n        Returns:\n            Dictionary mapping parameter combinations to quality scores\n        \"\"\"\n        # TODO 1: Define grid of temperature, top_k, and top_p values to test\n        # TODO 2: Generate multiple samples with each parameter combination\n        # TODO 3: Score each sample using coherence and diversity metrics\n        # TODO 4: Identify parameter combinations with best quality/diversity balance\n        # TODO 5: Return ranked parameter recommendations\n        pass\n```\n\n**Milestone Checkpoints for Debugging**\n\nAfter implementing each debugging component, verify functionality with these specific tests:\n\n**Attention Debugging Checkpoint:**\n```bash\npython -m debug.attention_visualizer --model checkpoint.pt --text \"The quick brown fox\" --layer 0 --head 0\n```\nExpected: Attention heatmap showing causal mask pattern with meaningful attention weights. Lower triangular matrix with focused attention on relevant tokens.\n\n**Training Debugging Checkpoint:**\n```bash\npython -m debug.gradient_monitor --config config.yaml --steps 100\n```\nExpected: Gradient flow analysis showing decreasing but non-zero gradients from output to input layers. No warnings about vanishing or exploding gradients.\n\n**Generation Debugging Checkpoint:**\n```bash\npython -m debug.generation_tester --model checkpoint.pt --prompt \"Once upon a time\" --samples 5\n```\nExpected: Quality assessment showing low repetition scores, high coherence metrics, and diverse outputs across sampling strategies.\n\n**Debugging Tips by Symptom**\n\n| Symptom | Likely Cause | How to Diagnose | Fix Strategy |\n|---------|--------------|-----------------|--------------|\n| Attention weights all zeros | Mask applied incorrectly | Check mask values and application order | Apply mask before softmax with -inf |\n| Loss becomes NaN | Numerical overflow | Log intermediate values in forward pass | Add gradient clipping and stability checks |\n| Generated text is gibberish | Insufficient training or wrong sampling | Check training loss convergence | Train longer or adjust sampling temperature |\n| Model predicts same token repeatedly | Overconfident predictions | Examine output probability distribution | Apply repetition penalty or increase temperature |\n| Training loss oscillates wildly | Learning rate too high | Monitor gradient norms over time | Reduce learning rate and add warmup |\n| Attention visualization shows uniform weights | Poor attention learning | Check attention score magnitudes | Verify scaling factor and initialization |\n| Generation stops abruptly | EOS token prediction issues | Check tokenizer EOS handling | Fix EOS token processing in generation loop |\n| Memory usage grows during generation | KV cache not managed properly | Profile memory usage during generation | Implement proper cache clearing and limits |\n\n\n## Future Extensions\n\n> **Milestone(s):** All milestones - building upon completed self-attention (Milestone 1), transformer blocks (Milestone 2), training pipeline (Milestone 3), and text generation (Milestone 4) with advanced enhancements\n\nAfter successfully implementing the core transformer architecture through all four milestones, numerous opportunities exist to enhance both the model's capabilities and training efficiency. These extensions represent the natural evolution from a working prototype to a production-ready system, incorporating techniques that have emerged from extensive research and practical deployment experience in the transformer ecosystem.\n\nThe extension landscape divides into two primary categories: **architectural improvements** that enhance the model's representational power and computational efficiency, and **training enhancements** that accelerate convergence, improve stability, and enable scaling to larger datasets and model sizes. Understanding these extensions provides insight into how modern transformer implementations achieve their impressive performance while maintaining computational tractability.\n\n### Mental Model: The Performance Optimization Pyramid\n\nThink of transformer extensions as optimizing a high-performance race car. The base implementation we've built through the four milestones represents a functional vehicle that can complete the race. Architectural improvements are like upgrading the engine, aerodynamics, and suspension - they fundamentally enhance the car's capabilities and efficiency. Training enhancements are like optimizing the pit crew, fuel strategy, and race tactics - they maximize performance within the existing hardware constraints while enabling longer races with larger teams.\n\nJust as race car improvements must be carefully balanced (a more powerful engine requires better brakes and cooling), transformer extensions introduce trade-offs between computational cost, implementation complexity, and performance gains. Each enhancement addresses specific bottlenecks or limitations discovered in the base implementation.\n\n### Architectural Improvements\n\nThe architectural dimension focuses on enhancing the transformer's core computational mechanisms. These improvements typically modify how the model processes information rather than how it learns, though the distinction sometimes blurs. The key areas include positional encoding schemes that better capture sequential relationships, attention variants that improve efficiency or expressiveness, and normalization techniques that enhance training stability and convergence.\n\n#### Advanced Positional Encodings\n\nOur base implementation likely uses simple learned positional embeddings added to token embeddings. While effective for sequences within the training distribution, this approach has significant limitations for longer sequences or positions not seen during training. Advanced positional encoding schemes address these extrapolation challenges while providing richer positional information.\n\n> **Decision: Rotary Position Embedding (RoPE)**\n> - **Context**: Learned positional embeddings don't generalize well beyond training sequence lengths and provide weak inductive biases about relative positions\n> - **Options Considered**: Sinusoidal encodings, ALiBi (Attention with Linear Biases), RoPE\n> - **Decision**: Implement Rotary Position Embedding as the primary upgrade\n> - **Rationale**: RoPE provides excellent length extrapolation, encodes relative positions naturally, and integrates cleanly into existing attention computation without parameter overhead\n> - **Consequences**: Requires modifying attention computation to apply rotation matrices, but enables training on shorter sequences and inference on longer ones\n\n| Positional Encoding Method | Length Generalization | Relative Position Awareness | Parameter Overhead | Implementation Complexity |\n|----------------------------|----------------------|----------------------------|-------------------|------------------------|\n| Learned Embeddings | Poor | No | High (vocab_size × d_model) | Low |\n| Sinusoidal | Good | Weak | None | Low |\n| ALiBi | Excellent | Strong | None | Medium |\n| RoPE | Excellent | Strong | None | Medium-High |\n\n**Rotary Position Embedding Implementation Strategy**\n\nRoPE modifies the attention computation by rotating query and key vectors based on their absolute positions in a way that naturally encodes relative position information. The rotation angles follow a geometric progression across embedding dimensions, creating a rich positional representation.\n\nThe core insight is that rotating queries and keys by position-dependent angles causes their dot product to depend on the relative distance between positions. For positions i and j, the attention score becomes a function of (i-j), providing the relative position awareness that simple additive embeddings lack.\n\nImplementation requires precomputing rotation matrices for each position and dimension pair, then applying these rotations within the attention mechanism. The rotation operation replaces the simple addition of positional embeddings to input tokens, moving position encoding directly into the attention computation.\n\n| RoPE Component | Purpose | Computational Cost | Memory Overhead |\n|----------------|---------|-------------------|-----------------|\n| Angle Computation | Generate rotation frequencies | O(d_model) | O(d_model) |\n| Rotation Matrix Application | Apply position rotations | O(seq_length × d_model) | O(seq_length × d_model) |\n| Query/Key Transformation | Rotate Q,K vectors | O(seq_length × num_heads × d_k) | None |\n\n**ALiBi as Lightweight Alternative**\n\nALiBi (Attention with Linear Biases) offers a simpler alternative that adds position-dependent biases directly to attention scores. Rather than modifying input embeddings or attention computation, ALiBi subtracts a linear penalty based on distance from attention scores before the softmax operation.\n\nThe penalty takes the form `-m × |i - j|` where m is a head-specific slope value and |i - j| is the distance between query and key positions. Different attention heads use different slope values, allowing them to specialize in different distance ranges - some heads focus on local relationships while others capture longer-range dependencies.\n\nALiBi's primary advantage is implementation simplicity and computational efficiency. It requires no additional parameters or complex mathematical operations, just a position-dependent bias matrix applied during attention computation. However, it provides less rich positional information than RoPE and may not extrapolate as effectively to very long sequences.\n\n#### Attention Mechanism Variants\n\nThe scaled dot-product attention implemented in Milestone 1 provides the foundation for more sophisticated attention mechanisms. These variants address specific limitations: computational complexity scaling, attention pattern diversity, and information bottlenecks in the standard attention formulation.\n\n> **Decision: Implementing Flash Attention for Memory Efficiency**\n> - **Context**: Standard attention computation requires materializing the full attention matrix (seq_length × seq_length), creating O(n²) memory complexity that limits maximum sequence length\n> - **Options Considered**: Flash Attention, Linear Attention, Sparse Attention patterns\n> - **Decision**: Implement Flash Attention as the primary memory optimization\n> - **Rationale**: Flash Attention provides identical outputs to standard attention while reducing memory usage through tile-based computation and kernel fusion\n> - **Consequences**: Requires careful implementation of memory-efficient attention kernels but enables processing much longer sequences\n\n**Flash Attention Architecture**\n\nFlash Attention reorganizes the attention computation to minimize memory transfers between GPU high-bandwidth memory (HBM) and on-chip SRAM. Rather than computing the full attention matrix and then applying it to values, Flash Attention processes attention in tiles that fit within SRAM capacity.\n\nThe algorithm maintains running statistics (maximum values and normalization factors) to correctly compute softmax across tiles without materializing the full attention matrix. Each tile computation loads query, key, and value blocks into SRAM, computes attention for that block, updates the running statistics, and accumulates results directly into the output buffer.\n\nThis approach reduces memory complexity from O(seq_length²) to O(seq_length), enabling much longer sequence processing. However, it requires careful implementation of the tiling algorithm and efficient kernel code to achieve the promised speedups.\n\n| Flash Attention Component | Standard Attention | Flash Attention | Memory Reduction |\n|---------------------------|-------------------|-----------------|------------------|\n| Attention Matrix Storage | O(n²) | O(tile_size²) | ~100-1000x |\n| Intermediate Activations | O(n × d_model) | O(tile_size × d_model) | ~10-50x |\n| Output Computation | Single pass | Tiled accumulation | Same final result |\n\n**Multi-Query and Grouped-Query Attention**\n\nMulti-Query Attention (MQA) and its generalization Grouped-Query Attention (GQA) reduce the memory and computational overhead of the key-value cache during autoregressive generation. While standard multi-head attention maintains separate key and value projections for each head, MQA shares a single key-value pair across all attention heads.\n\nThis modification significantly reduces the KV cache size during inference, which becomes the primary memory bottleneck for large models serving long sequences. Each attention head retains its own query projection, preserving the model's ability to attend to different aspects of the input, but shares the same key-value representations.\n\nGQA provides a middle ground by grouping attention heads and sharing key-value projections within each group. For example, with 8 attention heads and 2 groups, heads 1-4 share one set of key-value projections while heads 5-8 share another. This reduces cache size while maintaining more representational diversity than pure MQA.\n\n| Attention Variant | Query Heads | Key-Value Heads | Cache Size Ratio | Quality Impact |\n|-------------------|-------------|-----------------|------------------|----------------|\n| Multi-Head (MHA) | num_heads | num_heads | 1.0x | Baseline |\n| Grouped-Query (GQA) | num_heads | num_heads/group_size | 1/group_size | Minimal |\n| Multi-Query (MQA) | num_heads | 1 | 1/num_heads | Small |\n\n#### Normalization Strategy Evolution\n\nLayer normalization placement and variants significantly impact training dynamics and final model performance. Our Milestone 2 implementation likely uses pre-normalization (applying LayerNorm before each sublayer), but several alternatives offer compelling advantages for specific scenarios.\n\n> **Decision: RMSNorm for Computational Efficiency**\n> - **Context**: Standard LayerNorm computes both mean and variance, requiring two passes through the data and additional computational overhead\n> - **Options Considered**: LayerNorm, RMSNorm, GroupNorm variants\n> - **Decision**: Implement RMSNorm as an optional replacement for LayerNorm\n> - **Rationale**: RMSNorm provides similar normalization benefits with ~15% computational savings by omitting mean centering, and recent large models show it performs equally well\n> - **Consequences**: Slight implementation change with measurable speed improvement, especially beneficial for large models\n\n**RMSNorm Implementation Strategy**\n\nRoot Mean Square Normalization (RMSNorm) simplifies the normalization computation by omitting the mean centering step. Instead of normalizing to zero mean and unit variance, RMSNorm only ensures unit RMS (root mean square) magnitude, scaling each vector by the inverse of its RMS value.\n\nThe computational simplification reduces the normalization from two statistical moments (mean and variance) to one (RMS), eliminating one pass through the data. For large models where normalization overhead becomes significant, this reduction provides measurable speedups with minimal quality impact.\n\nImplementation replaces the standard LayerNorm computation `(x - mean) / sqrt(var + eps) * scale + bias` with the simpler RMSNorm formula `x / sqrt(mean_square + eps) * scale`. The bias term is typically omitted entirely, further reducing parameters and computation.\n\n| Normalization Method | Computational Steps | Memory Overhead | Parameter Count |\n|---------------------|-------------------|-----------------|-----------------|\n| LayerNorm | Mean, Variance, Normalize | 2x intermediate storage | 2 × d_model |\n| RMSNorm | RMS, Normalize | 1x intermediate storage | 1 × d_model |\n| GroupNorm | Group statistics | Group-dependent | 2 × d_model |\n\n**Normalization Placement Strategies**\n\nThe placement of normalization layers within transformer blocks affects both training stability and final performance. Pre-normalization (applying LayerNorm before each sublayer) generally provides better training stability, while post-normalization (applying LayerNorm after each sublayer) can achieve slightly better final performance with careful tuning.\n\nRecent research has explored hybrid approaches that apply different normalization strategies to different parts of the network, or adaptive normalization schemes that adjust their behavior based on training progress. These advanced techniques require more sophisticated implementation but can provide benefits for specific model scales or training scenarios.\n\nSome implementations also experiment with normalization-free architectures that achieve stable training through careful weight initialization and activation scaling, avoiding normalization overhead entirely. However, these approaches require expert tuning and may not generalize across different model sizes.\n\n### Training Enhancements\n\nTraining enhancement focuses on improving the optimization process itself: how quickly the model learns, how stably it trains, and how efficiently it utilizes computational resources. These improvements often have larger practical impact than architectural changes, as they directly affect development iteration speed and resource costs.\n\n#### Learning Rate Scheduling Evolution\n\nOur Milestone 3 implementation likely uses a constant learning rate or simple decay schedule. Advanced learning rate scheduling can dramatically improve convergence speed and final performance by adapting the learning rate based on training progress and model characteristics.\n\n> **Decision: Cosine Annealing with Warmup**\n> - **Context**: Constant learning rates converge slowly, while simple decay schedules may reduce learning rate too aggressively early in training\n> - **Options Considered**: Linear decay, exponential decay, cosine annealing, cyclical schedules\n> - **Decision**: Implement cosine annealing with linear warmup as the standard schedule\n> - **Rationale**: Cosine annealing provides smooth learning rate reduction that maintains exploration capability late in training, while warmup prevents early training instability\n> - **Consequences**: Requires schedule computation and tracking training progress, but typically improves convergence speed by 20-30%\n\n**Cosine Annealing Implementation**\n\nCosine annealing schedules the learning rate to follow a cosine curve from the maximum learning rate down to a minimum value (often zero or a small fraction of the maximum). This provides aggressive learning rate reduction in the middle of training while maintaining higher rates early and late in the schedule.\n\nThe mathematical form is `lr = lr_min + (lr_max - lr_min) * (1 + cos(π * step / total_steps)) / 2`, creating a smooth curve that starts high, decreases rapidly in the middle, and approaches the minimum value asymptotically. This schedule often outperforms linear or exponential decay by maintaining some learning capacity throughout training.\n\nLinear warmup precedes the cosine schedule, gradually increasing the learning rate from zero to the maximum value over the first few thousand steps. This prevents the large gradient updates that can destabilize training when starting with a high learning rate and random weights.\n\n| Schedule Component | Purpose | Duration | Learning Rate Range |\n|-------------------|---------|----------|-------------------|\n| Linear Warmup | Stabilize early training | warmup_steps (typically 2000) | 0 → lr_max |\n| Cosine Annealing | Smooth convergence | total_steps - warmup_steps | lr_max → lr_min |\n| Optional Restart | Escape local minima | Cyclical | Reset to lr_max |\n\n**Adaptive Learning Rate Methods**\n\nBeyond schedule-based approaches, adaptive optimizers like AdamW automatically adjust learning rates based on gradient statistics. AdamW combines the adaptive learning rates of Adam with proper weight decay regularization, addressing the L2 regularization issues in the original Adam optimizer.\n\nThe adaptive mechanism maintains exponential moving averages of both gradient values and squared gradients, using these statistics to compute per-parameter learning rate adjustments. Parameters with consistently large gradients receive smaller effective learning rates, while parameters with small or infrequent gradients receive larger effective rates.\n\nImplementation requires tracking the momentum and variance estimates for each parameter, increasing memory overhead but often improving convergence robustness. The weight decay term is applied directly to parameters rather than being added to gradients, providing more effective regularization.\n\n| Optimizer Component | Purpose | Memory Overhead | Hyperparameters |\n|--------------------|---------|-----------------|-----------------|\n| Gradient Momentum | Smooth optimization path | 1x parameters | beta1 (typically 0.9) |\n| Gradient Variance | Adaptive learning rates | 1x parameters | beta2 (typically 0.999) |\n| Weight Decay | Regularization | None | weight_decay (typically 0.01) |\n\n#### Mixed Precision Training\n\nMixed precision training uses 16-bit floating point for most operations while maintaining 32-bit precision for critical computations. This approach can nearly double training speed on modern GPUs while reducing memory usage, enabling larger models or batch sizes within the same hardware constraints.\n\n> **Decision: Automatic Mixed Precision (AMP)**\n> - **Context**: Training large transformers is computationally expensive and memory-intensive, limiting model size and training speed\n> - **Options Considered**: Full FP32, manual mixed precision, automatic mixed precision, FP16 throughout\n> - **Decision**: Implement Automatic Mixed Precision with gradient scaling\n> - **Rationale**: AMP provides most benefits of mixed precision with minimal code changes and automatic handling of numerical stability issues\n> - **Consequences**: Requires careful gradient scaling and loss computation, but typically provides 1.5-2x training speedup\n\n**AMP Implementation Strategy**\n\nAutomatic Mixed Precision automatically casts operations to the appropriate precision level: FP16 for matrix multiplications and convolutions where speed matters, FP32 for normalization and loss computation where precision matters. The framework handles these decisions based on operation types and numerical stability requirements.\n\nGradient scaling addresses the primary challenge of mixed precision training: gradient underflow. FP16's limited dynamic range can cause small gradients to round to zero, stalling training. Gradient scaling multiplies the loss by a large factor before backpropagation, keeping gradients in the representable FP16 range, then scales gradients back down before applying updates.\n\nThe scaling factor adapts dynamically based on gradient overflow detection. When gradients overflow (become infinite or NaN), the scaler reduces the scaling factor and skips the parameter update. When training proceeds stably, the scaler gradually increases the scaling factor to maximize gradient precision.\n\n| AMP Component | Precision | Purpose | Stability Considerations |\n|---------------|-----------|---------|-------------------------|\n| Forward Pass | FP16 | Speed and memory | Automatic casting |\n| Loss Computation | FP32 | Numerical accuracy | Prevent underflow |\n| Gradient Scaling | Dynamic | Prevent gradient underflow | Overflow detection |\n| Parameter Updates | FP32 | Maintain parameter precision | Scale correction |\n\n**Memory Optimization Techniques**\n\nBeyond mixed precision, several memory optimization techniques enable training larger models or using larger batch sizes. Gradient checkpointing trades computation for memory by recomputing some intermediate activations during backpropagation rather than storing them during the forward pass.\n\nThe technique identifies strategic points in the computation graph where activations are discarded and recomputed when needed for gradient computation. For transformers, checkpointing typically occurs at transformer block boundaries, reducing memory usage roughly proportional to the number of layers while increasing computation time by ~20%.\n\nZeRO (Zero Redundancy Optimizer) techniques partition optimizer states, gradients, and even parameters across multiple GPUs, reducing per-device memory requirements. Combined with gradient checkpointing and mixed precision, these techniques enable training models that would otherwise exceed memory limits.\n\n| Optimization Technique | Memory Reduction | Computation Overhead | Implementation Complexity |\n|------------------------|------------------|---------------------|-------------------------|\n| Gradient Checkpointing | ~num_layers reduction | ~20% increase | Medium |\n| ZeRO Stage 1 (optimizer states) | 4x with AdamW | Minimal | Low |\n| ZeRO Stage 2 (+ gradients) | 8x with AdamW | Small communication | Medium |\n| ZeRO Stage 3 (+ parameters) | Linear with devices | Significant communication | High |\n\n#### Distributed Training Strategies\n\nAs models grow beyond single-GPU capacity, distributed training becomes essential. The distribution strategy depends on model size, available hardware, and performance requirements, with different approaches optimal for different scenarios.\n\n> **Decision: Data Parallel with Gradient Synchronization**\n> - **Context**: Model fits on single GPU but training would benefit from larger batch sizes or faster iteration\n> - **Options Considered**: Data parallel, model parallel, pipeline parallel, hybrid approaches\n> - **Decision**: Implement synchronized data parallel training as the primary scaling method\n> - **Rationale**: Data parallel provides linear speedup with minimal code changes when model fits on single device, and handles most training scenarios effectively\n> - **Consequences**: Requires gradient synchronization and careful batch size scaling, but enables straightforward scaling to multiple GPUs\n\n**Data Parallel Implementation**\n\nData parallel training replicates the entire model on each GPU while splitting the training batch across devices. Each device processes its portion of the batch independently during the forward pass, then synchronizes gradients across all devices before applying parameter updates.\n\nThe synchronization step typically uses AllReduce communication patterns that efficiently average gradients across all participating devices. Modern frameworks implement optimized AllReduce algorithms that overlap communication with computation, minimizing the synchronization overhead.\n\nEffective batch size scales linearly with the number of devices, which can improve training efficiency but may require adjusting learning rate and other hyperparameters. The linear scaling rule suggests increasing learning rate proportionally with batch size, though this relationship breaks down at very large batch sizes.\n\n| Distributed Component | Purpose | Communication Pattern | Scaling Characteristics |\n|-----------------------|---------|----------------------|------------------------|\n| Forward Pass | Independent computation | None | Linear speedup |\n| Gradient Computation | Independent computation | None | Linear speedup |\n| Gradient Synchronization | Maintain model consistency | AllReduce | Communication overhead |\n| Parameter Updates | Synchronized updates | Broadcast | Minimal overhead |\n\n**Model Parallel Alternatives**\n\nWhen models exceed single-device memory capacity, model parallel techniques become necessary. Tensor parallel splits individual operations (like matrix multiplications) across multiple devices, while pipeline parallel partitions the model into stages that process different parts of the batch simultaneously.\n\nTensor parallel requires careful analysis of operation dependencies and communication patterns, as it introduces synchronization points within individual layers. Pipeline parallel provides cleaner abstractions but requires careful batch scheduling to maintain high device utilization and handle variable execution times.\n\nThe choice between these approaches depends on model architecture, hardware topology, and performance requirements. Tensor parallel works well for very large transformer blocks, while pipeline parallel suits models with clear layer boundaries and uniform computation per layer.\n\n### Implementation Guidance\n\nThe extensions described above require careful integration with the existing transformer implementation built through the four milestones. The following guidance provides concrete steps for implementing the most impactful enhancements while maintaining code organization and testability.\n\n#### Technology Recommendations\n\n| Extension Category | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Positional Encoding | Sinusoidal encoding replacement | RoPE with precomputed rotations |\n| Memory Optimization | Gradient checkpointing | Flash Attention implementation |\n| Training Schedule | Cosine annealing | Adaptive schedules with validation monitoring |\n| Mixed Precision | Manual FP16 casting | Framework-integrated AMP |\n| Distributed Training | Data parallel with framework support | Custom communication optimization |\n\n#### Recommended File Structure Extension\n\n```\ntransformer_project/\n  src/\n    transformer/\n      extensions/                    ← new extension modules\n        positional/\n          __init__.py\n          rope.py                   ← RoPE implementation\n          alibi.py                  ← ALiBi implementation\n          sinusoidal.py            ← improved sinusoidal\n        attention/\n          __init__.py\n          flash_attention.py        ← memory-efficient attention\n          multi_query.py           ← MQA/GQA variants\n        normalization/\n          __init__.py\n          rmsnorm.py               ← RMSNorm implementation\n        training/\n          __init__.py\n          schedulers.py            ← learning rate schedules\n          amp_utils.py             ← mixed precision utilities\n          distributed.py           ← distributed training helpers\n      core/                         ← existing core modules\n        attention.py               ← enhanced with extension support\n        transformer_block.py       ← configurable normalization\n        training.py                ← extended training loop\n    configs/\n      extension_configs.py          ← configuration for extensions\n    experiments/\n      benchmark_extensions.py       ← performance comparisons\n      ablation_studies.py          ← extension impact analysis\n```\n\n#### Infrastructure Starter Code\n\n**Learning Rate Scheduler Implementation**\n\n```python\nimport math\nfrom typing import Optional\n\nclass CosineAnnealingScheduler:\n    \"\"\"Cosine annealing learning rate scheduler with linear warmup.\"\"\"\n    \n    def __init__(self, \n                 optimizer,\n                 max_lr: float,\n                 total_steps: int,\n                 warmup_steps: int = 2000,\n                 min_lr: float = 0.0):\n        self.optimizer = optimizer\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.total_steps = total_steps\n        self.warmup_steps = warmup_steps\n        self.current_step = 0\n        \n    def step(self):\n        \"\"\"Update learning rate for current training step.\"\"\"\n        self.current_step += 1\n        lr = self._compute_lr()\n        \n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n            \n    def _compute_lr(self) -> float:\n        if self.current_step < self.warmup_steps:\n            # Linear warmup\n            return self.max_lr * self.current_step / self.warmup_steps\n        else:\n            # Cosine annealing\n            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n            progress = min(progress, 1.0)  # Clamp to [0, 1]\n            cosine_factor = 0.5 * (1 + math.cos(math.pi * progress))\n            return self.min_lr + (self.max_lr - self.min_lr) * cosine_factor\n            \n    def get_last_lr(self) -> float:\n        \"\"\"Get current learning rate.\"\"\"\n        return self._compute_lr()\n```\n\n**Mixed Precision Training Wrapper**\n\n```python\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\nfrom typing import Dict, Any\n\nclass AMPTrainer:\n    \"\"\"Automatic Mixed Precision training wrapper.\"\"\"\n    \n    def __init__(self, model, optimizer, scheduler=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.scaler = GradScaler()\n        self.amp_enabled = torch.cuda.is_available()\n        \n    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Execute single training step with mixed precision.\"\"\"\n        self.model.train()\n        self.optimizer.zero_grad()\n        \n        input_ids = batch['input_ids']\n        targets = batch['targets']\n        \n        # Forward pass with autocast\n        with autocast(enabled=self.amp_enabled):\n            logits = self.model(input_ids)\n            loss = self._compute_loss(logits, targets)\n        \n        # Backward pass with gradient scaling\n        self.scaler.scale(loss).backward()\n        \n        # Unscale gradients for clipping\n        self.scaler.unscale_(self.optimizer)\n        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        \n        # Optimizer step with scaling\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        \n        if self.scheduler:\n            self.scheduler.step()\n            \n        return {\n            'loss': loss.item(),\n            'grad_norm': grad_norm.item(),\n            'lr': self.scheduler.get_last_lr() if self.scheduler else self.optimizer.param_groups[0]['lr'],\n            'scale': self.scaler.get_scale()\n        }\n        \n    def _compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute cross-entropy loss with proper mixed precision handling.\"\"\"\n        # Ensure loss computation happens in FP32 for numerical stability\n        return torch.nn.functional.cross_entropy(\n            logits.view(-1, logits.size(-1)), \n            targets.view(-1), \n            ignore_index=-1\n        )\n```\n\n#### Core Logic Skeleton Code\n\n**RoPE Integration Skeleton**\n\n```python\nimport torch\nimport torch.nn as nn\nfrom typing import Tuple\n\nclass RotaryPositionalEmbedding(nn.Module):\n    \"\"\"Rotary Position Embedding implementation.\"\"\"\n    \n    def __init__(self, d_model: int, max_seq_length: int = 8192):\n        super().__init__()\n        self.d_model = d_model\n        self.max_seq_length = max_seq_length\n        \n        # TODO 1: Compute frequency values for each dimension pair\n        # Hint: Use geometric progression with base 10000\n        # freq[i] = 1.0 / (10000 ** (2 * i / d_model)) for i in range(d_model // 2)\n        \n        # TODO 2: Precompute position encodings for all positions\n        # Hint: For each position pos, compute cos(pos * freq) and sin(pos * freq)\n        \n        # TODO 3: Register buffers for cos and sin tables\n        # Use self.register_buffer to ensure proper device placement\n        \n    def forward(self, q: torch.Tensor, k: torch.Tensor, \n                start_pos: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Apply rotary embeddings to query and key tensors.\"\"\"\n        seq_len = q.shape[-2]\n        \n        # TODO 4: Extract cos and sin values for current sequence positions\n        # Handle start_pos for KV caching during generation\n        \n        # TODO 5: Apply rotation to query tensor\n        # Split q into even/odd dimensions and apply rotation formula\n        # q_rot = q_even * cos - q_odd * sin, q_even * sin + q_odd * cos\n        \n        # TODO 6: Apply same rotation to key tensor\n        # Use identical rotation as query to preserve relative position encoding\n        \n        # TODO 7: Return rotated query and key tensors\n        # Ensure output shapes match input shapes\n        \n        pass  # Remove when implementing\n\nclass MultiHeadAttentionWithRoPE(MultiHeadAttention):\n    \"\"\"Enhanced multi-head attention with RoPE support.\"\"\"\n    \n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.rope = RotaryPositionalEmbedding(config.d_k, config.seq_length * 2)\n        \n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None, \n                kv_cache: KVCache = None, start_pos: int = 0) -> torch.Tensor:\n        \"\"\"Forward pass with rotary position embeddings.\"\"\"\n        batch_size, seq_len, d_model = x.shape\n        \n        # Standard Q, K, V projections\n        q = self.query_projection(x)  # [batch, seq_len, num_heads * d_k]\n        k = self.key_projection(x)\n        v = self.value_projection(x)\n        \n        # Reshape for multi-head attention\n        q = q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        k = k.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n        v = v.view(batch_size, seq_len, self.num_heads, self.d_v).transpose(1, 2)\n        \n        # TODO 8: Apply RoPE to queries and keys\n        # q_rot, k_rot = self.rope(q, k, start_pos)\n        \n        # TODO 9: Handle KV cache update if provided\n        # Update cache with new k_rot, v values and get full context\n        \n        # TODO 10: Compute attention with rotated queries and keys\n        # Use existing scaled dot-product attention implementation\n        \n        # TODO 11: Apply output projection and return\n        # Ensure proper tensor reshaping for output\n        \n        pass  # Remove when implementing\n```\n\n#### Milestone Checkpoints\n\n**Extension Integration Checkpoint**\n\nAfter implementing each extension category, verify functionality with these checkpoints:\n\n1. **Positional Encoding Extensions**\n   - Test sequence length extrapolation: train on sequences of length N, generate text with length 1.5N\n   - Compare attention patterns between different positional encoding methods\n   - Verify mathematical correctness of RoPE rotation formulas with unit tests\n\n2. **Training Enhancement Integration**\n   - Monitor training curves with new schedulers - loss should converge 20-30% faster\n   - Verify mixed precision maintains numerical stability - gradients should not underflow\n   - Test distributed training with gradient synchronization across multiple GPUs\n\n3. **Performance Benchmarking**\n   - Measure memory usage reduction with Flash Attention or gradient checkpointing\n   - Time training iterations with different optimization combinations\n   - Profile computational overhead of each extension\n\n#### Debugging Tips for Extensions\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Loss explodes with RoPE | Incorrect rotation matrix computation | Check cos/sin table values, verify rotation formula | Debug frequency computation, ensure proper tensor dimensions |\n| Mixed precision training stalls | Gradient underflow from insufficient scaling | Monitor gradient magnitudes and scaler values | Increase initial gradient scale, check loss computation precision |\n| Distributed training hangs | Gradient synchronization deadlock | Check AllReduce communication patterns | Ensure all processes participate in gradient sync, verify device placement |\n| Flash Attention wrong outputs | Tiling boundary errors or softmax normalization | Compare outputs with standard attention on small examples | Debug tile boundaries, verify running statistics for softmax |\n| Memory usage doesn't decrease | Extension not properly integrated | Profile memory usage before/after enabling extensions | Check that extensions are actually being used, not bypassed |\n\nThese extensions transform the basic transformer implementation into a production-capable system. Each enhancement addresses specific limitations discovered in practical deployment, and their combination enables training and deploying much larger, more capable models than the baseline implementation supports.\n\n### Implementation Guidance\n\nThe implementation of transformer extensions requires careful integration with existing code while maintaining backwards compatibility and testing coverage. The following guidance provides concrete implementation strategies and code organization patterns.\n\n#### Technology Recommendations Table\n\n| Extension Category | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Positional Encoding | Replace learned embeddings with sinusoidal | Full RoPE implementation with precomputed tables |\n| Attention Optimization | Gradient checkpointing for memory | Flash Attention kernel implementation |\n| Training Efficiency | PyTorch native AMP with GradScaler | Custom mixed precision with optimal casting |\n| Model Scaling | DistributedDataParallel wrapper | Pipeline parallel with custom scheduling |\n| Learning Rate | Cosine annealing with warmup | Adaptive schedules based on validation metrics |\n\n#### Recommended Extension Structure\n\n```\nsrc/transformer/extensions/\n  __init__.py                      ← extension registry and factory functions\n  positional/\n    __init__.py\n    base_encoding.py               ← abstract base class for positional encodings\n    rope_encoding.py              ← RoPE implementation\n    alibi_encoding.py             ← ALiBi implementation\n    sinusoidal_encoding.py        ← improved sinusoidal\n  attention/\n    __init__.py\n    flash_attention.py            ← memory-efficient attention kernels\n    multi_query_attention.py      ← MQA and GQA implementations\n    sparse_attention.py           ← sparse attention patterns\n  optimization/\n    __init__.py\n    schedulers.py                 ← learning rate scheduling implementations\n    amp_trainer.py                ← mixed precision training utilities\n    gradient_utils.py             ← gradient clipping and monitoring\n  distributed/\n    __init__.py\n    data_parallel.py              ← enhanced data parallel training\n    model_parallel.py             ← tensor and pipeline parallel utilities\n    communication.py              ← optimized gradient synchronization\n  normalization/\n    __init__.py\n    rmsnorm.py                    ← RMSNorm implementation\n    adaptive_norm.py              ← adaptive normalization techniques\n```\n\n#### Infrastructure Starter Code\n\n**Extension Configuration System**\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional, Union, Dict, Any\nimport torch\n\n@dataclass\nclass ExtensionConfig:\n    \"\"\"Configuration for transformer extensions.\"\"\"\n    \n    # Positional encoding options\n    positional_encoding: str = \"learned\"  # \"learned\", \"sinusoidal\", \"rope\", \"alibi\"\n    rope_base: float = 10000.0\n    alibi_slopes: Optional[torch.Tensor] = None\n    \n    # Attention optimization options\n    use_flash_attention: bool = False\n    attention_variant: str = \"multi_head\"  # \"multi_head\", \"multi_query\", \"grouped_query\"\n    query_groups: int = 1  # for grouped-query attention\n    \n    # Normalization options\n    normalization_type: str = \"layer_norm\"  # \"layer_norm\", \"rms_norm\"\n    normalization_placement: str = \"pre\"  # \"pre\", \"post\"\n    \n    # Training enhancements\n    use_mixed_precision: bool = False\n    gradient_checkpointing: bool = False\n    learning_rate_schedule: str = \"constant\"  # \"constant\", \"cosine\", \"linear\"\n    warmup_steps: int = 2000\n    \n    # Distributed training\n    distributed_backend: str = \"nccl\"\n    gradient_sync_frequency: int = 1\n    \n    def __post_init__(self):\n        \"\"\"Validate configuration after initialization.\"\"\"\n        if self.positional_encoding not in [\"learned\", \"sinusoidal\", \"rope\", \"alibi\"]:\n            raise ValueError(f\"Unsupported positional encoding: {self.positional_encoding}\")\n        \n        if self.attention_variant == \"grouped_query\" and self.query_groups <= 1:\n            raise ValueError(\"Grouped-query attention requires query_groups > 1\")\n\nclass ExtensionRegistry:\n    \"\"\"Registry for dynamically loading transformer extensions.\"\"\"\n    \n    _positional_encodings = {}\n    _attention_variants = {}\n    _normalization_types = {}\n    _schedulers = {}\n    \n    @classmethod\n    def register_positional_encoding(cls, name: str, implementation_class):\n        \"\"\"Register a positional encoding implementation.\"\"\"\n        cls._positional_encodings[name] = implementation_class\n    \n    @classmethod\n    def create_positional_encoding(cls, name: str, config: ExtensionConfig, model_config: TransformerConfig):\n        \"\"\"Create positional encoding instance from configuration.\"\"\"\n        if name not in cls._positional_encodings:\n            raise ValueError(f\"Unknown positional encoding: {name}\")\n        return cls._positional_encodings[name](model_config, config)\n    \n    # Similar methods for other extension types...\n```\n\n**Performance Monitoring Utilities**\n\n```python\nimport time\nimport torch\nimport psutil\nfrom typing import Dict, Any, List, Optional\nfrom contextlib import contextmanager\n\nclass PerformanceMonitor:\n    \"\"\"Monitor training performance metrics for extension evaluation.\"\"\"\n    \n    def __init__(self):\n        self.metrics_history: List[Dict[str, Any]] = []\n        self.current_metrics: Dict[str, Any] = {}\n        self.timers: Dict[str, float] = {}\n        \n    @contextmanager\n    def timer(self, name: str):\n        \"\"\"Context manager for timing code sections.\"\"\"\n        start_time = time.time()\n        try:\n            yield\n        finally:\n            elapsed = time.time() - start_time\n            self.timers[name] = elapsed\n            \n    def record_memory_usage(self) -> Dict[str, float]:\n        \"\"\"Record current GPU and system memory usage.\"\"\"\n        memory_stats = {}\n        \n        if torch.cuda.is_available():\n            memory_stats.update({\n                'gpu_memory_allocated': torch.cuda.memory_allocated() / 1024**3,  # GB\n                'gpu_memory_cached': torch.cuda.memory_reserved() / 1024**3,\n                'gpu_max_memory': torch.cuda.max_memory_allocated() / 1024**3\n            })\n        \n        memory_stats.update({\n            'system_memory_percent': psutil.virtual_memory().percent,\n            'system_memory_available': psutil.virtual_memory().available / 1024**3\n        })\n        \n        return memory_stats\n        \n    def record_training_step(self, \n                           step: int, \n                           loss: float, \n                           learning_rate: float,\n                           batch_size: int,\n                           sequence_length: int) -> None:\n        \"\"\"Record metrics for a single training step.\"\"\"\n        step_metrics = {\n            'step': step,\n            'loss': loss,\n            'learning_rate': learning_rate,\n            'batch_size': batch_size,\n            'sequence_length': sequence_length,\n            'timestamp': time.time(),\n            **self.record_memory_usage(),\n            **self.timers\n        }\n        \n        self.metrics_history.append(step_metrics)\n        self.timers.clear()  # Reset timers for next step\n        \n    def get_performance_summary(self, last_n_steps: int = 100) -> Dict[str, float]:\n        \"\"\"Generate performance summary for recent training steps.\"\"\"\n        if not self.metrics_history:\n            return {}\n            \n        recent_metrics = self.metrics_history[-last_n_steps:]\n        \n        # Compute averages and trends\n        avg_loss = sum(m['loss'] for m in recent_metrics) / len(recent_metrics)\n        avg_lr = sum(m['learning_rate'] for m in recent_metrics) / len(recent_metrics)\n        \n        step_times = [m.get('forward_pass', 0) + m.get('backward_pass', 0) \n                     for m in recent_metrics if 'forward_pass' in m]\n        avg_step_time = sum(step_times) / len(step_times) if step_times else 0\n        \n        return {\n            'avg_loss': avg_loss,\n            'avg_learning_rate': avg_lr,\n            'avg_step_time': avg_step_time,\n            'steps_per_second': 1.0 / avg_step_time if avg_step_time > 0 else 0,\n            'tokens_per_second': (recent_metrics[-1]['batch_size'] * \n                                recent_metrics[-1]['sequence_length'] / avg_step_time\n                                if avg_step_time > 0 else 0)\n        }\n```\n\n#### Core Extension Implementation Skeletons\n\n**Rotary Position Embedding Implementation**\n\n```python\nimport torch\nimport torch.nn as nn\nimport math\nfrom typing import Tuple, Optional\n\nclass RotaryPositionalEmbedding(nn.Module):\n    \"\"\"Rotary Position Embedding for improved sequence length extrapolation.\"\"\"\n    \n    def __init__(self, d_model: int, max_seq_length: int = 8192, base: float = 10000.0):\n        super().__init__()\n        self.d_model = d_model\n        self.max_seq_length = max_seq_length\n        self.base = base\n        \n        # TODO 1: Compute inverse frequency values\n        # Create tensor of shape [d_model // 2] with geometric progression\n        # inv_freq[i] = 1.0 / (base ** (2 * i / d_model)) for i in range(d_model // 2)\n        \n        # TODO 2: Precompute cosine and sine tables\n        # For positions 0 to max_seq_length-1, compute:\n        # cos_table[pos, i] = cos(pos * inv_freq[i])\n        # sin_table[pos, i] = sin(pos * inv_freq[i])\n        \n        # TODO 3: Register as buffers for automatic device placement\n        # self.register_buffer('cos_table', cos_table, persistent=False)\n        # self.register_buffer('sin_table', sin_table, persistent=False)\n        \n    def _rotate_half(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Rotate half the dimensions negatively for RoPE computation.\"\"\"\n        # TODO 4: Split tensor into two halves along last dimension\n        # Apply rotation: concatenate [-x2, x1] where x = [x1, x2]\n        \n    def forward(self, q: torch.Tensor, k: torch.Tensor, \n                start_pos: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Apply rotary embeddings to query and key tensors.\"\"\"\n        seq_len = q.shape[-2]\n        \n        # TODO 5: Extract cos and sin values for current positions\n        # Handle start_pos offset for KV caching\n        # cos_vals = self.cos_table[start_pos:start_pos + seq_len]\n        # sin_vals = self.sin_table[start_pos:start_pos + seq_len]\n        \n        # TODO 6: Apply rotation to query tensor\n        # q_rot = q * cos_vals + self._rotate_half(q) * sin_vals\n        # Ensure broadcasting works correctly for multi-head dimensions\n        \n        # TODO 7: Apply identical rotation to key tensor\n        # k_rot = k * cos_vals + self._rotate_half(k) * sin_vals\n        \n        # TODO 8: Return rotated tensors with same shapes as inputs\n        return q_rot, k_rot\n```\n\n**Mixed Precision Training Integration**\n\n```python\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\nfrom typing import Dict, Any, Optional\n\nclass EnhancedAMPTrainer:\n    \"\"\"Advanced mixed precision trainer with extension support.\"\"\"\n    \n    def __init__(self, \n                 model: nn.Module, \n                 optimizer: torch.optim.Optimizer,\n                 config: ExtensionConfig,\n                 scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.config = config\n        self.scaler = GradScaler(enabled=config.use_mixed_precision)\n        self.performance_monitor = PerformanceMonitor()\n        \n    def train_step(self, batch: Dict[str, torch.Tensor], step: int) -> Dict[str, Any]:\n        \"\"\"Execute training step with all enabled optimizations.\"\"\"\n        self.model.train()\n        self.optimizer.zero_grad()\n        \n        with self.performance_monitor.timer('forward_pass'):\n            # TODO 9: Implement forward pass with conditional autocast\n            # Use autocast only if mixed precision is enabled\n            # Handle gradient checkpointing if enabled\n            \n        with self.performance_monitor.timer('backward_pass'):\n            # TODO 10: Implement backward pass with gradient scaling\n            # Scale loss before backward if using mixed precision\n            # Apply gradient clipping after unscaling\n            \n        with self.performance_monitor.timer('optimizer_step'):\n            # TODO 11: Implement optimizer step with scaling\n            # Step optimizer and update gradient scaler\n            # Update learning rate scheduler if provided\n            \n        # TODO 12: Record performance metrics\n        # Use performance_monitor to track training progress\n        \n        # TODO 13: Return comprehensive training metrics\n        # Include loss, learning rate, gradient norms, timing info\n        \n    def _compute_loss_with_precision(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute loss with appropriate precision handling.\"\"\"\n        # TODO 14: Ensure loss computation happens in FP32\n        # Cast logits to FP32 if using mixed precision for numerical stability\n        \n    def _apply_gradient_clipping(self, max_norm: float = 1.0) -> float:\n        \"\"\"Apply gradient clipping and return gradient norm.\"\"\"\n        # TODO 15: Unscale gradients before clipping\n        # Compute and return gradient norm for monitoring\n        \n    def save_checkpoint(self, filepath: str, step: int, validation_loss: float) -> None:\n        \"\"\"Save training checkpoint with extension state.\"\"\"\n        # TODO 16: Save comprehensive checkpoint including:\n        # - Model state dict\n        # - Optimizer state dict  \n        # - Scheduler state dict\n        # - Gradient scaler state\n        # - Extension configuration\n        # - Performance metrics history\n```\n\n#### Milestone Verification Checkpoints\n\n**Extension Integration Verification**\n\n1. **Positional Encoding Extension Checkpoint**\n   ```python\n   # Test length extrapolation capability\n   def test_positional_encoding_extrapolation():\n       # TODO: Train model on sequences of length 512\n       # TODO: Generate coherent text on sequences up to length 1024\n       # TODO: Verify attention patterns remain reasonable at longer lengths\n       # Expected: Attention should focus on relevant tokens without degradation\n   ```\n\n2. **Mixed Precision Training Checkpoint**\n   ```python\n   # Verify numerical stability and performance gains\n   def test_mixed_precision_stability():\n       # TODO: Compare loss curves between FP32 and mixed precision training\n       # TODO: Measure training speed improvement (expect 1.5-2x speedup)\n       # TODO: Monitor gradient scaler behavior (should adapt dynamically)\n       # Expected: Similar final loss with significantly faster training\n   ```\n\n3. **Memory Optimization Checkpoint**\n   ```python\n   # Verify memory usage reduction\n   def test_memory_optimization():\n       # TODO: Measure peak memory usage with and without optimizations\n       # TODO: Verify model can train with larger batch sizes\n       # TODO: Test gradient checkpointing computational overhead\n       # Expected: 20-50% memory reduction with <20% speed penalty\n   ```\n\n#### Performance Benchmarking Framework\n\n```python\nimport torch\nimport time\nfrom typing import List, Dict, Callable\n\nclass ExtensionBenchmark:\n    \"\"\"Benchmark framework for comparing extension performance.\"\"\"\n    \n    def __init__(self, model_configs: List[TransformerConfig]):\n        self.model_configs = model_configs\n        self.benchmark_results = {}\n        \n    def benchmark_attention_variants(self, \n                                   sequence_lengths: List[int],\n                                   batch_sizes: List[int]) -> Dict[str, Dict[str, float]]:\n        \"\"\"Benchmark different attention implementations.\"\"\"\n        results = {}\n        \n        for config in self.model_configs:\n            for seq_len in sequence_lengths:\n                for batch_size in batch_sizes:\n                    test_key = f\"seq_{seq_len}_batch_{batch_size}\"\n                    \n                    # TODO 17: Benchmark standard multi-head attention\n                    # Create model with standard attention, measure forward/backward time\n                    \n                    # TODO 18: Benchmark Flash Attention if available\n                    # Compare memory usage and computation time\n                    \n                    # TODO 19: Benchmark Multi-Query Attention variants\n                    # Measure KV cache size reduction and inference speed\n                    \n                    results[test_key] = {\n                        'standard_attention': {'time': 0, 'memory': 0},\n                        'flash_attention': {'time': 0, 'memory': 0},\n                        'multi_query_attention': {'time': 0, 'memory': 0}\n                    }\n                    \n        return results\n        \n    def benchmark_training_optimizations(self, \n                                       num_steps: int = 100) -> Dict[str, float]:\n        \"\"\"Benchmark training optimization techniques.\"\"\"\n        # TODO 20: Compare training speed with different optimizations\n        # - Baseline FP32 training\n        # - Mixed precision training\n        # - Gradient checkpointing\n        # - Combined optimizations\n        \n        # TODO 21: Measure convergence speed differences\n        # Track loss reduction rate with different configurations\n        \n        # TODO 22: Profile memory usage patterns\n        # Record peak memory usage throughout training\n        \n        return {\n            'fp32_baseline': 0.0,\n            'mixed_precision': 0.0,\n            'gradient_checkpointing': 0.0,\n            'combined_optimizations': 0.0\n        }\n```\n\nThese extensions provide the foundation for scaling transformer implementations from educational prototypes to production-capable systems. Each extension addresses specific bottlenecks and limitations discovered in practical deployment, enabling much larger models and more efficient training than the baseline implementation supports.\n\n\n## Glossary\n\n> **Milestone(s):** All milestones - comprehensive reference for terminology, mathematical notation, and technical concepts used throughout transformer implementation (Milestones 1-4)\n\nThe transformer architecture introduces a rich vocabulary of specialized terminology spanning mathematical concepts, architectural components, and implementation techniques. This glossary provides comprehensive definitions for all terms used throughout the transformer implementation, serving as both a learning resource for newcomers and a reference for experienced practitioners. Understanding these concepts deeply is crucial for successful implementation and debugging of transformer models.\n\nThe terminology is organized into thematic categories to facilitate learning and reference. Each definition includes not only the formal meaning but also contextual usage within transformer implementations, common variations in terminology across different frameworks and papers, and connections to related concepts. Special attention is given to terms where confusion commonly arises, such as the distinction between different normalization approaches or the various attention mechanism variants.\n\n### Mathematical Notation and Operations\n\n**Scaled dot-product attention** represents the core mathematical operation of transformer models, computing attention weights through the formula `Q @ K^T / sqrt(d_k)` followed by softmax normalization. The scaling by the square root of the key dimension prevents the dot products from growing too large, which would cause the softmax to saturate and produce overly sharp attention distributions. This mathematical foundation underlies all attention mechanisms in transformers.\n\n**Multi-head attention** extends scaled dot-product attention by computing multiple parallel attention functions with different learned projections. Each attention head processes the same input through different query, key, and value transformations, allowing the model to attend to information from different representation subspaces simultaneously. The outputs from all heads are concatenated and projected to produce the final attention output.\n\n**Cross-entropy loss** serves as the standard loss function for language modeling, measuring the difference between predicted token probability distributions and the true next-token labels. The loss computation involves taking the negative log-likelihood of the correct token under the model's predicted distribution, providing a learning signal that encourages the model to assign higher probability to correct continuations.\n\n**Causal mask** implements the constraint that tokens can only attend to previous positions in the sequence, preventing information leakage from future tokens during training. The mask is typically implemented as a lower triangular matrix where future positions are set to negative infinity before applying softmax, ensuring their attention weights become zero.\n\n### Architecture Components and Design Patterns\n\n**Self-attention** describes attention mechanisms where the queries, keys, and values all derive from the same input sequence, allowing tokens to selectively focus on other positions within the same context. This contrasts with cross-attention mechanisms found in encoder-decoder architectures, where queries come from one sequence and keys/values from another.\n\n**Decoder-only** architectures, exemplified by GPT models, consist entirely of transformer decoder blocks without any encoder component. This architectural choice optimizes for autoregressive generation tasks where the model processes and generates text sequentially from left to right.\n\n**Feed-forward network** refers to the position-wise multilayer perceptron within each transformer block, typically expanding the hidden dimension by a factor of 4 before projecting back to the original size. This component provides non-linear transformation capacity that complements the linear operations of attention mechanisms.\n\n**Residual connections** are skip connections that add the input to each sublayer's output, enabling direct gradient flow to earlier layers and facilitating training of very deep networks. In transformers, residual connections wrap both the attention and feed-forward sublayers within each block.\n\n**Layer normalization** normalizes activations across the feature dimension for each token independently, computing zero mean and unit variance statistics. This normalization technique stabilizes training dynamics and enables the use of higher learning rates compared to unnormalized networks.\n\n### Training and Optimization Terminology\n\n**Language modeling objective** defines the training task of predicting the next token in a sequence given all previous tokens. This objective transforms the problem of learning language into a standard supervised learning setup where the model learns to minimize prediction error on next-token targets.\n\n**Teacher forcing** describes the training strategy where the model receives the actual previous tokens as input rather than its own predictions. This approach accelerates training by providing correct context at each position, though it can create exposure bias where training and inference conditions differ.\n\n**Label shifting** refers to the offset between input sequences and target labels necessary for next-token prediction. Input sequences contain tokens 0 through n-1, while target sequences contain tokens 1 through n, creating the supervisory signal for learning token continuation patterns.\n\n**Gradient clipping** limits the magnitude of gradients during backpropagation to prevent training instability. Gradient norms are computed across all model parameters and scaled down if they exceed a threshold, preventing the explosive gradient growth that can destabilize transformer training.\n\n**Learning rate scheduling** adapts the learning rate throughout training to improve convergence and final performance. Common schedules include warmup periods with gradually increasing rates followed by decay phases, helping models escape local minima early while achieving stable convergence later.\n\n**Mixed precision training** uses 16-bit floating point arithmetic for most operations while maintaining 32-bit precision for critical computations like loss calculation and parameter updates. This approach significantly reduces memory usage and accelerates training on modern hardware while maintaining numerical stability.\n\n### Generation and Sampling Methods\n\n**Autoregressive generation** describes the sequential process of generating text one token at a time, where each new token depends on all previously generated tokens in the sequence. This approach enables coherent long-form generation but requires multiple forward passes through the model.\n\n**Greedy decoding** selects the highest probability token at each generation step, producing deterministic outputs that maximize local likelihood. While simple and fast, greedy decoding can lead to repetitive or low-quality text due to its myopic optimization approach.\n\n**Temperature sampling** introduces controlled randomness into generation by scaling the logits before applying softmax. Lower temperatures produce more focused, deterministic outputs, while higher temperatures increase randomness and diversity at the cost of coherence.\n\n**Top-k sampling** restricts token selection to the k most likely candidates at each step, filtering out low-probability options that could lead to incoherent generations. This approach balances diversity with quality by maintaining reasonable candidates while eliminating extreme outliers.\n\n**Top-p sampling** (also called **nucleus sampling**) dynamically selects from the smallest set of tokens whose cumulative probability exceeds a threshold p. This method adapts the candidate set size based on the prediction confidence, using fewer tokens when the model is confident and more when uncertainty is high.\n\n**KV caching** optimizes autoregressive generation by storing computed key and value tensors from previous positions, avoiding redundant computation in subsequent forward passes. This optimization significantly accelerates generation speed, especially for longer sequences.\n\n### Training Dynamics and Stability\n\n**Vanishing gradients** occur when backpropagated gradients become exponentially small through deep layer chains, preventing effective learning in early layers. Transformers mitigate this through residual connections and layer normalization, though the problem can still arise in very deep models.\n\n**Gradient explosion** represents the opposite problem where gradients grow exponentially large, causing parameter updates that destabilize training. Gradient clipping serves as the primary defense against this issue in transformer training.\n\n**Numerical stability** encompasses techniques for preventing overflow and underflow in floating-point computations. Key strategies include the LogSumExp trick for stable softmax computation, gradient scaling in mixed precision training, and careful initialization of model parameters.\n\n**Softmax overflow** occurs when input values to the softmax function are too large, causing exponential computations to produce infinity. The LogSumExp trick addresses this by subtracting the maximum input value before computing exponentials, maintaining numerical stability without changing the final result.\n\n### Model Architecture Variants and Extensions\n\n**Rotary position embedding** (RoPE) encodes positional information by applying rotation matrices to query and key vectors, providing better length extrapolation than learned positional embeddings. This approach enables models to handle sequences longer than those seen during training.\n\n**Flash attention** implements memory-efficient attention computation using tiling techniques that reduce memory usage from quadratic to linear in sequence length. This optimization enables processing of much longer sequences within memory constraints.\n\n**Multi-query attention** shares key and value projections across all attention heads while maintaining separate query projections, reducing parameter count and memory usage with minimal impact on model quality.\n\n**Grouped-query attention** represents a middle ground between standard multi-head attention and multi-query attention, grouping heads to share key-value projections while maintaining some parameter diversity.\n\n**ALiBi** (Attention with Linear Biases) replaces positional embeddings with linear biases applied directly to attention scores, providing strong length extrapolation capabilities with minimal computational overhead.\n\n### Advanced Training Techniques\n\n**Automatic mixed precision** provides framework-integrated mixed precision training with automatic casting between 16-bit and 32-bit precision based on operation requirements. This approach simplifies mixed precision adoption while maintaining the performance benefits.\n\n**Gradient checkpointing** trades computation for memory by discarding intermediate activations during forward passes and recomputing them as needed during backpropagation. This technique enables training of larger models within memory constraints.\n\n**Distributed training** scales transformer training across multiple devices or machines using various parallelization strategies. Common approaches include data parallel training with batch splitting and model parallel training with component distribution.\n\n**Data parallel** training replicates the full model on each device while splitting training batches across devices, synchronizing gradients after each training step to maintain consistent parameter updates.\n\n**Model parallel** training splits model components across different devices, requiring careful coordination of forward and backward passes to maintain training efficiency.\n\n### Performance and Monitoring\n\n**Performance monitoring** tracks training metrics, resource usage, and model behavior throughout the training process. Key metrics include loss curves, gradient norms, learning rates, memory usage, and throughput measurements.\n\n**Extension registry** provides a system for dynamically loading and configuring transformer enhancements like alternative attention mechanisms, positional encodings, and normalization techniques.\n\n**Length extrapolation** refers to a model's ability to process sequences longer than those encountered during training, a crucial capability for practical deployment of transformer models.\n\n### Configuration and Hyperparameters\n\nThe glossary includes numerous configuration parameters that control model behavior and training dynamics. Key dimensional parameters include `d_model` (embedding dimension), `num_heads` (attention heads), `seq_length` (maximum sequence length), and `vocab_size` (vocabulary size). Training hyperparameters encompass `learning_rate`, `batch_size`, `gradient_clip_norm`, and `warmup_steps`. Generation parameters include `temperature`, `top_k`, `top_p`, and `repetition_penalty`.\n\n### Common Implementation Patterns\n\n**Information refinement pipeline** serves as a mental model for understanding how transformer blocks iteratively improve token representations. Each block processes the input through attention (gathering relevant context) and feed-forward layers (applying non-linear transformations) to produce refined representations.\n\n**Iterative prediction** describes the mental model for text generation where the model builds sequences incrementally, using each newly generated token to inform subsequent predictions in a chain of dependent decisions.\n\n**Selective focus** provides intuition for attention mechanisms, where each token selectively focuses on relevant context positions to compute context-aware representations rather than processing all positions equally.\n\n### Error Handling and Edge Cases\n\nUnderstanding failure modes and edge cases is crucial for robust transformer implementation. **Sequence length limits** define maximum token sequence lengths that models can process, **token vocabulary bounds** specify valid ranges of token IDs, and **shape consistency checking** validates tensor dimensions throughout the computation pipeline.\n\n**Input validation** encompasses checks for sequence lengths, token vocabulary bounds, and tensor shape consistency to prevent runtime errors and ensure correct model behavior.\n\nThis comprehensive glossary serves as both a learning resource and ongoing reference throughout transformer implementation. Each term connects to broader concepts within the architecture, and mastering this vocabulary is essential for understanding research papers, debugging implementations, and extending transformer models with new capabilities.\n\n### Implementation Guidance\n\nThe implementation of transformer terminology requires careful attention to naming consistency and documentation standards. This section provides guidance for maintaining terminological clarity throughout your codebase.\n\n#### Terminology Consistency Standards\n\n| Category | Convention | Example |\n|----------|------------|---------|\n| Class Names | PascalCase with descriptive suffixes | `MultiHeadAttention`, `TransformerBlock` |\n| Method Names | snake_case with action verbs | `forward()`, `compute_attention_weights()` |\n| Configuration Fields | snake_case with clear units | `d_model`, `learning_rate`, `max_seq_length` |\n| Constants | UPPER_SNAKE_CASE | `PAD_TOKEN_ID`, `EOS_TOKEN_ID` |\n\n#### Documentation Template\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention mechanism with scaled dot-product attention.\n    \n    Implements parallel attention heads with different learned projections,\n    enabling the model to attend to information from different representation\n    subspaces simultaneously.\n    \n    Args:\n        d_model: Model embedding dimension\n        num_heads: Number of parallel attention heads\n        dropout_rate: Dropout probability for attention weights\n        \n    Attributes:\n        d_k: Key dimension per attention head (d_model // num_heads)\n        d_v: Value dimension per attention head (d_model // num_heads)\n        query_projection: Linear layer for query transformation\n        key_projection: Linear layer for key transformation\n        value_projection: Linear layer for value transformation\n        output_projection: Linear layer for final output transformation\n    \"\"\"\n    \n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float = 0.1):\n        # TODO: Initialize projection layers and attention parameters\n        pass\n    \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"Apply multi-head attention to input sequence.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, d_model)\n            mask: Optional causal mask of shape (seq_length, seq_length)\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_length, d_model)\n        \"\"\"\n        # TODO: Implement multi-head attention computation\n        pass\n```\n\n#### Configuration Documentation Standards\n\n```python\n@dataclass\nclass TransformerConfig:\n    \"\"\"Configuration for transformer model architecture and training.\n    \n    Attributes:\n        d_model: Model embedding dimension (typically 512, 768, or 1024)\n        num_heads: Number of parallel attention heads (must divide d_model evenly)\n        d_k: Key dimension per head (computed as d_model // num_heads)\n        d_v: Value dimension per head (computed as d_model // num_heads)\n        seq_length: Maximum sequence length for positional encoding\n        vocab_size: Size of token vocabulary\n        num_layers: Number of transformer blocks in the model\n        dropout_rate: Dropout probability for regularization (typically 0.1)\n        ffn_expansion: Feed-forward network expansion ratio (typically 4)\n    \"\"\"\n    d_model: int = 512\n    num_heads: int = 8\n    d_k: int = field(init=False)\n    d_v: int = field(init=False)\n    seq_length: int = 512\n    vocab_size: int = 10000\n    num_layers: int = 6\n    dropout_rate: float = 0.1\n    ffn_expansion: int = 4\n    \n    def __post_init__(self):\n        \"\"\"Compute derived dimensions and validate configuration.\"\"\"\n        assert self.d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_k = self.d_model // self.num_heads\n        self.d_v = self.d_model // self.num_heads\n```\n\n#### Glossary Integration Utilities\n\n```python\nclass TerminologyValidator:\n    \"\"\"Validates consistent use of transformer terminology in code.\"\"\"\n    \n    PREFERRED_TERMS = {\n        'self_attention': ['self-attention', 'selfattention'],\n        'multi_head_attention': ['multihead_attention', 'multi_head_attn'],\n        'feed_forward_network': ['ffn', 'mlp', 'position_wise_ffn'],\n        'layer_normalization': ['layer_norm', 'ln'],\n        'causal_mask': ['attention_mask', 'sequence_mask'],\n    }\n    \n    def validate_method_names(self, class_methods: List[str]) -> List[str]:\n        \"\"\"Check method names against preferred terminology.\"\"\"\n        warnings = []\n        for method in class_methods:\n            for preferred, alternatives in self.PREFERRED_TERMS.items():\n                if any(alt in method.lower() for alt in alternatives):\n                    warnings.append(f\"Consider using '{preferred}' instead of variant in '{method}'\")\n        return warnings\n```\n\n#### Type Annotation Standards\n\n```python\nfrom typing import Optional, Tuple, Dict, Any, List, Union\nimport torch\nfrom torch import nn, Tensor\n\n# Standard type aliases for transformer implementations\nAttentionWeights = torch.Tensor  # Shape: (batch_size, num_heads, seq_length, seq_length)\nTokenSequence = torch.Tensor     # Shape: (batch_size, seq_length)\nEmbeddings = torch.Tensor        # Shape: (batch_size, seq_length, d_model)\nLogits = torch.Tensor           # Shape: (batch_size, seq_length, vocab_size)\n\ndef scaled_dot_product_attention(\n    query: Embeddings,\n    key: Embeddings, \n    value: Embeddings,\n    mask: Optional[torch.Tensor] = None,\n    dropout_rate: float = 0.0\n) -> Tuple[Embeddings, AttentionWeights]:\n    \"\"\"Compute scaled dot-product attention with optional masking.\n    \n    Args:\n        query: Query tensor of shape (batch_size, seq_length, d_k)\n        key: Key tensor of shape (batch_size, seq_length, d_k)\n        value: Value tensor of shape (batch_size, seq_length, d_v)\n        mask: Optional mask of shape (seq_length, seq_length)\n        dropout_rate: Attention dropout probability\n        \n    Returns:\n        Tuple of (attention_output, attention_weights)\n        - attention_output: Weighted value sum of shape (batch_size, seq_length, d_v)\n        - attention_weights: Attention probabilities of shape (batch_size, seq_length, seq_length)\n    \"\"\"\n    # TODO: Implement scaled dot-product attention\n    pass\n```\n\n#### Debugging Terminology Helpers\n\n```python\ndef print_tensor_info(tensor: torch.Tensor, name: str, expected_shape: Optional[Tuple[int, ...]] = None):\n    \"\"\"Print comprehensive tensor information with standardized terminology.\n    \n    Args:\n        tensor: PyTorch tensor to analyze\n        name: Descriptive name using standard terminology\n        expected_shape: Optional expected shape for validation\n    \"\"\"\n    print(f\"\\n{name} Tensor Analysis:\")\n    print(f\"  Shape: {tensor.shape}\")\n    print(f\"  Device: {tensor.device}\")\n    print(f\"  Dtype: {tensor.dtype}\")\n    print(f\"  Requires Grad: {tensor.requires_grad}\")\n    print(f\"  Memory Usage: {tensor.numel() * tensor.element_size() / 1024:.2f} KB\")\n    \n    if expected_shape:\n        shape_match = tensor.shape == torch.Size(expected_shape)\n        print(f\"  Shape Match: {shape_match}\")\n        if not shape_match:\n            print(f\"  Expected: {expected_shape}\")\n            print(f\"  Got: {tuple(tensor.shape)}\")\n\n# Usage example:\n# print_tensor_info(attention_weights, \"Multi-Head Attention Weights\", (batch_size, num_heads, seq_length, seq_length))\n```\n\n#### Milestone Terminology Checkpoints\n\nAfter implementing each milestone, verify terminology consistency:\n\n**Milestone 1 - Self-Attention Verification:**\n```bash\n# Check that attention-related terms are used consistently\ngrep -r \"attention\" src/ | grep -v \"self.attention\\|multi_head_attention\\|scaled_dot_product_attention\"\n```\n\n**Milestone 2 - Transformer Block Verification:**\n```bash\n# Verify transformer block terminology\npython -c \"\nimport inspect\nfrom src.transformer_block import TransformerBlock\nmethods = [m for m in dir(TransformerBlock) if not m.startswith('_')]\nprint('TransformerBlock methods:', methods)\n# Should see: forward, get_attention_weights\n\"\n```\n\n**Milestone 3 - Training Pipeline Verification:**\n```bash\n# Check training-related terminology consistency  \npython -c \"\nfrom src.trainer import TransformerTrainer\nfrom src.config import TrainingConfig\nconfig_fields = TrainingConfig.__dataclass_fields__.keys()\nprint('TrainingConfig fields:', list(config_fields))\n# Should match exactly: learning_rate, batch_size, num_epochs, gradient_clip_norm, etc.\n\"\n```\n\n**Milestone 4 - Generation Verification:**\n```bash\n# Verify generation terminology\npython -c \"\nfrom src.generator import TextGenerator, SamplingStrategies\nsampling_methods = [m for m in dir(SamplingStrategies) if not m.startswith('_')]\nprint('Sampling methods:', sampling_methods)\n# Should see: greedy_sample, temperature_sample, top_k_sample, top_p_sample\n\"\n```\n\nThis implementation guidance ensures consistent terminology usage throughout your transformer implementation, making your code more readable, maintainable, and aligned with standard conventions in the field.\n"}