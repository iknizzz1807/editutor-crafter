{"html":"<h1 id=\"project-cdc-real-time-change-data-capture-system-design\">Project CDC: Real-Time Change Data Capture System Design</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This document outlines the design for a Change Data Capture (CDC) system that streams database changes (INSERT, UPDATE, DELETE) to downstream consumers in real-time. The core challenge is reliably reading low-level database transaction logs (like PostgreSQL WAL or MySQL binlog), transforming them into structured change events, and delivering them with strong ordering guarantees, all while handling schema changes and database-specific complexities without impacting the source database&#39;s performance.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (foundational concepts), Milestone 2 (delivery concepts), Milestone 3 (schema concepts)</p>\n</blockquote>\n<h2 id=\"1-context-and-problem-statement\">1. Context and Problem Statement</h2>\n<p>Modern applications increasingly rely on real-time data flows—when a user updates their profile, the search index should reflect it instantly; when an inventory item sells out, the e-commerce listing should update immediately; when a financial transaction occurs, fraud detection must analyze it in seconds. This creates a fundamental challenge: <strong>how do you reliably propagate database changes to downstream systems the moment they happen?</strong></p>\n<p>Traditional approaches like periodic polling or batch ETL jobs are fundamentally mismatched for this real-time requirement. They introduce significant latency (minutes to hours), waste resources by repeatedly querying unchanged data, and can&#39;t guarantee they capture every change. The core problem this design document addresses is building a system that captures database changes <strong>as they occur</strong> with <strong>minimal performance impact</strong> on the source database, <strong>strong ordering guarantees</strong>, and <strong>robust handling of schema evolution</strong>—all while being maintainable across different database technologies.</p>\n<h3 id=\"11-the-newspaper-analogy-for-cdc\">1.1 The Newspaper Analogy for CDC</h3>\n<p>Think of your application database as a town hall where official records are kept. Different departments (downstream systems) need the latest information: the tax office needs new business registrations, the postal service needs address changes, and the library needs updates to public records.</p>\n<p><strong>Polling-Based Approach (The Town Crier):</strong> Every hour, each department sends a clerk to the town hall to manually copy all records changed since their last visit. This is wasteful (clerks often find nothing changed), slow (changes aren&#39;t known until the next hourly visit), and error-prone (what if the clerk misses a page?).</p>\n<p><strong>Trigger-Based Approach (The Overzealous Clerk):</strong> The town hall hires a clerk who sits beside every record book. Whenever a record is modified, the clerk immediately makes a copy and rushes it to all departments. This ensures real-time delivery but causes chaos—the clerk&#39;s constant interruptions slow down the main record-keeping work, and if the clerk gets sick, all updates stop.</p>\n<p><strong>Log-Based CDC (The Newspaper Printer):</strong> The town hall maintains a <strong>printing press</strong> that automatically produces a copy of every change made to the records, in the exact order they occurred. This press operates in a separate room, not interfering with the main clerks. At 5 AM each morning, the press rolls, producing a <strong>newspaper</strong> (the transaction log) containing yesterday&#39;s changes. Downstream systems simply subscribe to the newspaper delivery. This approach is <strong>non-intrusive</strong> (doesn&#39;t slow record-keeping), <strong>complete</strong> (every change is printed), and <strong>ordered</strong> (changes appear in the sequence they happened).</p>\n<blockquote>\n<p><strong>Key Insight:</strong> Transaction logs (Write-Ahead Logs in PostgreSQL, binlogs in MySQL) are the database&#39;s internal &quot;newspaper printer.&quot; They already exist for crash recovery purposes. Log-based CDC reads this existing byproduct, making it the most efficient and reliable method for change capture.</p>\n</blockquote>\n<h3 id=\"12-the-technical-challenge-of-log-based-cdc\">1.2 The Technical Challenge of Log-Based CDC</h3>\n<p>While the newspaper analogy simplifies the concept, the engineering reality involves navigating a labyrinth of low-level complexities. Building a production-grade CDC system is challenging because you&#39;re interfacing with the database&#39;s most critical internal mechanism—its transaction log—which was designed for recovery, not for external consumption.</p>\n<p><strong>1. Database-Specific Binary Formats:</strong> Each database implements its transaction log differently. PostgreSQL&#39;s WAL is a low-level physical log recording page-level changes, while MySQL&#39;s binlog can be configured for statement-based (SQL commands) or row-based (actual row changes) logging. Parsing these requires deep understanding of each database&#39;s internal binary format, which can change between minor versions.</p>\n<p><strong>2. Transaction Boundary Reconstruction:</strong> Databases write to their logs during transactions, not just at commit. A single logical update might involve multiple log entries across different tables. The CDC system must <strong>reconstruct transaction boundaries</strong>—grouping all changes belonging to a single transaction and only emitting them when the commit record appears. This requires maintaining in-memory state for pending transactions, which introduces complexity around memory management and crash recovery.</p>\n<p><strong>3. Ordering and Duplication Guarantees:</strong> Downstream consumers often require that changes to the same database row are processed in the exact order they occurred. If a row is updated from value A→B→C, consumers must see A→B→C, never A→C→B or B→A→C. The CDC system must guarantee <strong>per-primary-key ordering</strong> even when reading from a log that might interleave changes from multiple concurrent transactions. Additionally, after a system restart or network failure, the CDC system must resume from exactly where it left off without <strong>skipping changes</strong> or <strong>reprocessing duplicates</strong>.</p>\n<p><strong>4. Schema Evolution Without Breaking Consumers:</strong> Databases evolve—columns are added, removed, or renamed; data types change. A CDC system emitting change events must handle these <strong>schema changes gracefully</strong>. If a consumer built to expect three columns suddenly receives events with four columns, it will fail. The system needs a <strong>versioned schema registry</strong> and <strong>compatibility rules</strong> to ensure old consumers can still read new events (backward compatibility) and new consumers can read old events (forward compatibility).</p>\n<p><strong>5. Performance Isolation and Backpressure:</strong> The CDC system must operate without degrading the source database&#39;s performance. This means:</p>\n<ul>\n<li><strong>Minimal read load:</strong> Efficiently tailing the log without expensive queries.</li>\n<li><strong>Backpressure handling:</strong> If downstream consumers slow down, the CDC system must <strong>pause log reading</strong> rather than buffer events infinitely in memory, which could lead to out-of-memory crashes.</li>\n<li><strong>Network resilience:</strong> Surviving temporary disconnections from the database or message broker without data loss.</li>\n</ul>\n<p>The following table summarizes these core challenges and their implications:</p>\n<table>\n<thead>\n<tr>\n<th>Challenge</th>\n<th>Technical Implication</th>\n<th>Consequence If Mishandled</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Binary Log Formats</strong></td>\n<td>Need separate parser for each database (PostgreSQL, MySQL, etc.)</td>\n<td>System only works with one database; breaks on version upgrades</td>\n</tr>\n<tr>\n<td><strong>Transaction Reconstruction</strong></td>\n<td>Must buffer uncommitted changes in memory</td>\n<td>Memory exhaustion on large transactions; lost changes on crash</td>\n</tr>\n<tr>\n<td><strong>Ordering Guarantees</strong></td>\n<td>Must track position per partition/key</td>\n<td>Consumers process changes out-of-order, causing data corruption</td>\n</tr>\n<tr>\n<td><strong>Schema Evolution</strong></td>\n<td>Need versioned schemas + compatibility checks</td>\n<td>Consumer crashes on schema changes; requires manual intervention</td>\n</tr>\n<tr>\n<td><strong>Backpressure</strong></td>\n<td>Must coordinate between log reader and event publisher</td>\n<td>Memory overflow; stalled log reading causing lag</td>\n</tr>\n</tbody></table>\n<h3 id=\"13-existing-approaches-and-trade-offs\">1.3 Existing Approaches and Trade-offs</h3>\n<p>Before committing to log-based CDC, we evaluated three primary architectural approaches, each with distinct trade-offs. The decision matrix below compares them across critical dimensions for our use case:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>How It Works</th>\n<th>Latency</th>\n<th>Performance Impact on Source DB</th>\n<th>Reliability/Completeness</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Application-Level Dual Writes</strong></td>\n<td>Application code writes to DB and simultaneously publishes to message queue</td>\n<td>Very Low (ms)</td>\n<td>High (doubles write latency, adds failure modes)</td>\n<td>Low (race conditions, partial failures cause inconsistency)</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Database Triggers</strong></td>\n<td>Database triggers fire on DML, calling stored procedure to publish changes</td>\n<td>Low (ms)</td>\n<td>Very High (serializes writes, adds trigger overhead)</td>\n<td>Medium (trigger failures silently lose changes)</td>\n<td>High (database-specific, hard to debug)</td>\n</tr>\n<tr>\n<td><strong>Polling with Timestamps/Version Columns</strong></td>\n<td>Periodic SELECT queries for rows where <code>updated_at &gt; last_check</code></td>\n<td>High (seconds-minutes)</td>\n<td>Medium (indexed queries still cause load)</td>\n<td>Low (misses deletions, concurrent updates)</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Log-Based CDC (Our Choice)</strong></td>\n<td>Read database transaction log (WAL/binlog) asynchronously</td>\n<td>Low (ms-seconds)</td>\n<td><strong>Very Low</strong> (reads existing log, non-intrusive)</td>\n<td><strong>High</strong> (captures every change, ordered)</td>\n<td><strong>Very High</strong> (complex parsing, state management)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Choosing Log-Based CDC Over Alternatives</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a production-grade CDC system that guarantees complete change capture with minimal performance impact on source databases, suitable for high-volume transactional systems.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Application-Level Dual Writes</strong>: Modify every application to write to both DB and message queue.</li>\n<li><strong>Database Triggers</strong>: Use database triggers to publish changes via stored procedures.</li>\n<li><strong>Polling with Change Tracking</strong>: Periodic queries using timestamps or SQL Server Change Tracking.</li>\n<li><strong>Log-Based CDC</strong>: Asynchronous reading of transaction logs.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement log-based CDC using database transaction logs (PostgreSQL WAL, MySQL binlog).</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Performance Isolation</strong>: Transaction logs are already written for crash recovery; reading them adds negligible load compared to triggers (which serialize writes) or dual writes (which double latency).</li>\n<li><strong>Completeness Guarantee</strong>: Logs capture EVERY committed change including deletions—polling approaches inherently miss deletions unless using tombstone flags.</li>\n<li><strong>Ordering Preservation</strong>: Logs preserve the exact commit sequence, enabling per-key ordering guarantees crucial for downstream consistency.</li>\n<li><strong>No Application Changes</strong>: Works with existing applications without code modifications—critical for legacy systems.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Increased Implementation Complexity</strong>: Must build/maintain database-specific log parsers that understand binary formats.</li>\n<li><strong>Database Version Coupling</strong>: Parser may need updates for new database versions.</li>\n<li><strong>Initial Snapshot Requirement</strong>: Need a mechanism to capture initial table state before beginning log streaming.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Deep Dive: Why Not Application-Level Dual Writes?</strong></p>\n<p>While dual writes seem simple—just add a <code>kafkaProducer.send()</code> after your database <code>INSERT</code>—they suffer from <strong>distributed consistency problems</strong>. Consider this scenario:</p>\n<ol>\n<li>Application writes to Database → SUCCESS</li>\n<li>Application writes to Kafka → NETWORK TIMEOUT</li>\n<li>What now? The database has the change but Kafka doesn&#39;t.</li>\n</ol>\n<p>The application must now decide: roll back the database transaction (impacting user experience) or retry the Kafka write (potentially causing duplicates). Even with retries, there&#39;s a window where the systems are inconsistent. This problem is formally known as the <strong>Two Generals&#39; Problem</strong>—there&#39;s no guaranteed way to coordinate two distributed systems to both commit or both rollback. Log-based CDC avoids this by making the database the <strong>single source of truth</strong> and treating change capture as an asynchronous follower.</p>\n<p><strong>Deep Dive: The Pitfalls of Trigger-Based CDC</strong></p>\n<p>Triggers seem appealing—they&#39;re database-native and guarantee capture. However, they introduce <strong>severe performance penalties</strong>:</p>\n<ul>\n<li><strong>Serial Execution</strong>: Triggers execute within the same transaction as the DML operation. If the trigger code (which publishes to Kafka) takes 100ms, every database write now takes 100ms longer.</li>\n<li><strong>Single Point of Failure</strong>: If the network to Kafka is down, the trigger fails, causing the entire database transaction to fail—blocking all writes.</li>\n<li><strong>Debugging Complexity</strong>: Trigger errors manifest as mysterious database errors, requiring deep database-specific expertise to troubleshoot.</li>\n</ul>\n<p>A real-world example: An e-commerce platform implemented trigger-based CDC. During Black Friday, write volume increased 50x. The trigger overhead caused transaction locks to be held longer, leading to <strong>deadlock storms</strong> that brought the entire checkout system to a halt.</p>\n<p><strong>Deep Dive: Limitations of Polling Approaches</strong></p>\n<p>Polling with <code>updated_at</code> columns or Change Tracking has fundamental gaps:</p>\n<ol>\n<li><strong>Misses Deletions</strong>: Unless you implement soft deletes with a <code>deleted_at</code> column, <code>DELETE</code> operations vanish without a trace.</li>\n<li><strong>Race Conditions</strong>: If two updates occur between polling intervals, you might only capture the second one.</li>\n<li><strong>Performance Scaling</strong>: As the table grows, <code>SELECT ... WHERE updated_at &gt; ?</code> queries become expensive even with indexes, competing with operational queries.</li>\n<li><strong>No Transaction Boundaries</strong>: You see individual row changes but lose the information that changes to Table A and Table B were part of the same atomic transaction.</li>\n</ol>\n<p>These limitations make polling unsuitable for financial systems, inventory management, or any use case requiring complete, ordered change capture.</p>\n<blockquote>\n<p><strong>Design Principle:</strong> A CDC system should be <strong>invisible to the source database</strong>—it shouldn&#39;t add latency to writes, consume significant CPU, or become a single point of failure. Log-based CDC, while complex to implement, is the only approach that satisfies this principle while providing the completeness and ordering guarantees required for mission-critical data pipelines.</p>\n</blockquote>\n<p>By embracing log-based CDC, we accept the upfront complexity of parsing binary logs in exchange for a robust, performant foundation that can scale with the database without impacting it. The remainder of this design document details how we navigate these complexities through careful component design, state management, and fault-tolerant streaming.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Since this section is purely conceptual (Context and Problem Statement), there is no implementation code required. However, we provide foundational technology recommendations that will inform the implementation in subsequent sections.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (For Learning/Prototyping)</th>\n<th>Advanced Option (Production-Grade)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Database Connection</strong></td>\n<td>JDBC for initial snapshot + direct TCP socket for log streaming</td>\n<td>Database-native libraries (PostgreSQL: <code>pgjdbc</code> with <code>PGReplicationStream</code>; MySQL: <code>mysql-connector-j</code> with binlog listener)</td>\n</tr>\n<tr>\n<td><strong>Log Parsing</strong></td>\n<td>Custom parser for a single database version using documented format</td>\n<td>Debezium Connector framework (pluggable parsers, handles version differences)</td>\n</tr>\n<tr>\n<td><strong>Event Streaming</strong></td>\n<td>Apache Kafka with Java Producer API</td>\n<td>Kafka with exactly-once semantics (Kafka Transactions) or Apache Pulsar</td>\n</tr>\n<tr>\n<td><strong>Schema Management</strong></td>\n<td>Simple file-based schema registry (JSON files)</td>\n<td>Confluent Schema Registry (REST API, Avro schemas, compatibility checks)</td>\n</tr>\n<tr>\n<td><strong>Monitoring</strong></td>\n<td>Log files + manual offset tracking</td>\n<td>Prometheus metrics + Grafana dashboards for consumer lag, parse errors</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended Project File Structure:</strong></p>\n<p>While the full architecture will be detailed in Section 3, we recommend starting with this structure to organize the codebase:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-system/\n├── docs/                           # Design documents, diagrams\n│   └── diagrams/                   # SVG diagram files\n├── lib/                            # Database-specific parser libraries (if not using Debezium)\n│   ├── postgres-wal-parser/\n│   └── mysql-binlog-parser/\n├── src/main/java/com/cdc/\n│   ├── core/                       # Core data models and interfaces\n│   │   ├── ChangeEvent.java\n│   │   ├── RawLogEntry.java\n│   │   └── SchemaVersion.java\n│   ├── connector/                  # Log Connector &amp; Parser (Milestone 1)\n│   │   ├── LogConnector.java\n│   │   ├── postgres/              # PostgreSQL-specific implementation\n│   │   └── mysql/                 # MySQL-specific implementation\n│   ├── builder/                    # Change Event Builder (Milestone 1)\n│   │   └── ChangeEventBuilder.java\n│   ├── streamer/                   # Event Streamer &amp; Delivery (Milestone 2)\n│   │   └── EventStreamer.java\n│   ├── schema/                     # Schema Registry &amp; Evolution (Milestone 3)\n│   │   └── SchemaRegistry.java\n│   └── util/                       # Utilities (serialization, configuration, metrics)\n├── config/                         # Configuration files\n│   ├── cdc-config.yaml\n│   └── log4j2.xml\n├── tests/                          # Test suites\n│   ├── unit/                       # Unit tests\n│   ├── integration/                # Integration tests with embedded DB\n│   └── e2e/                        # End-to-end tests\n└── README.md</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p>Before implementing log parsing, you&#39;ll need a configuration management utility. Here&#39;s a complete, ready-to-use configuration loader:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.util;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.databind.ObjectMapper;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.dataformat.yaml.YAMLFactory;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.File;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.IOException;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Configuration loader for CDC system. Loads YAML configuration files.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * This is a prerequisite utility - copy and use as-is.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ConfigLoader</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ObjectMapper mapper </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ObjectMapper</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">new</span><span style=\"color:#B392F0\"> YAMLFactory</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Load configuration from YAML file.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> configPath</span><span style=\"color:#6A737D\"> Path to YAML configuration file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> configClass</span><span style=\"color:#6A737D\"> Configuration class type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@return</span><span style=\"color:#6A737D\"> Populated configuration object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@throws</span><span style=\"color:#B392F0\"> IOException</span><span style=\"color:#6A737D\"> If file cannot be read or parsed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> T </span><span style=\"color:#B392F0\">loadConfig</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#E1E4E8\">, Class&#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">configClass</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> IOException {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        File configFile </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> File</span><span style=\"color:#E1E4E8\">(configPath);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">configFile.</span><span style=\"color:#B392F0\">exists</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> IOException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Configuration file not found: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> configPath);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> mapper.</span><span style=\"color:#B392F0\">readValue</span><span style=\"color:#E1E4E8\">(configFile, configClass);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Main CDC configuration class. Extend this with database-specific sections.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> CdcConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> DatabaseConfig database;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> StreamingConfig streaming;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> SchemaConfig schema;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> DatabaseConfig </span><span style=\"color:#B392F0\">getDatabase</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setDatabase</span><span style=\"color:#E1E4E8\">(DatabaseConfig </span><span style=\"color:#FFAB70\">database</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.database </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> DatabaseConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String type; </span><span style=\"color:#6A737D\">// \"postgresql\" or \"mysql\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String host;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> port;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String database;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String username;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String password;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String slotName; </span><span style=\"color:#6A737D\">// For PostgreSQL replication slot</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> StreamingConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String bootstrapServers; </span><span style=\"color:#6A737D\">// Kafka bootstrap servers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String topicPrefix </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"cdc.\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> maxInFlightRequests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> SchemaConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String registryUrl; </span><span style=\"color:#6A737D\">// Schema Registry URL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#E1E4E8\"> autoRegisterSchemas </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Language-Specific Hints (Java):</strong></p>\n<ul>\n<li><strong>Use try-with-resources</strong>: Always use <code>try-with-resources</code> for database connections, network sockets, and file streams to ensure proper cleanup.</li>\n<li><strong>Logging framework</strong>: Use SLF4J with Logback or Log4j2 for structured logging. Avoid <code>System.out.println()</code> in production code.</li>\n<li><strong>Concurrency</strong>: Use <code>java.util.concurrent</code> classes (<code>ExecutorService</code>, <code>ConcurrentHashMap</code>) rather than manual <code>synchronized</code> blocks where possible.</li>\n<li><strong>Error handling</strong>: Use specific exception types (<code>IOException</code>, <code>SQLException</code>) rather than generic <code>Exception</code> catches to handle different failure modes appropriately.</li>\n<li><strong>Configuration</strong>: Use environment variables for secrets (passwords, API keys) rather than hardcoding in configuration files.</li>\n</ul>\n<p><strong>E. Milestone Checkpoint (Conceptual Foundation):</strong></p>\n<p>Before proceeding to implementation, verify you understand these core concepts:</p>\n<ul>\n<li>✅ <strong>Understand the newspaper analogy</strong> and why log-based CDC is non-intrusive.</li>\n<li>✅ <strong>Identify the three main challenges</strong>: binary log parsing, transaction reconstruction, and schema evolution.</li>\n<li>✅ <strong>Articulate why dual writes and triggers are problematic</strong> for production systems.</li>\n<li>✅ <strong>Set up your development environment</strong> with Java 11+, Maven/Gradle, and access to a PostgreSQL or MySQL instance.</li>\n</ul>\n<p><strong>F. Debugging Tips (Conceptual Level):</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Conceptual Misunderstanding</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Why not just query the table every minute?&quot;</td>\n<td>Doesn&#39;t understand latency and completeness requirements</td>\n<td>Ask: &quot;What happens if a row is updated twice between queries? What about DELETE operations?&quot;</td>\n<td>Review polling limitations in section 1.3</td>\n</tr>\n<tr>\n<td>&quot;Can&#39;t we just add a Kafka send() in our DAO layer?&quot;</td>\n<td>Doesn&#39;t understand distributed consistency problem</td>\n<td>Diagram the dual write failure scenario with network timeout</td>\n<td>Review &quot;Why Not Application-Level Dual Writes?&quot;</td>\n</tr>\n<tr>\n<td>&quot;Triggers are built into the database, so they must be reliable&quot;</td>\n<td>Doesn&#39;t understand performance impact of serial execution</td>\n<td>Test trigger overhead with a simple stored procedure that sleeps for 100ms</td>\n<td>Review trigger pitfalls in section 1.3</td>\n</tr>\n</tbody></table>\n<hr>\n<h2 id=\"2-goals-and-non-goals\">2. Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3 (foundational requirements for all milestones)</p>\n</blockquote>\n<p>This section establishes a clear contract between the system designers and implementers by defining <strong>what the system must accomplish</strong> (Goals) and <strong>what it explicitly will not handle</strong> (Non-Goals). In a complex system like CDC where requirements could expand infinitely, these boundaries prevent scope creep and ensure focus on delivering core value. Think of this as the architectural constitution—it defines the principles that all subsequent design decisions must uphold.</p>\n<h3 id=\"21-goals-must-have\">2.1 Goals (Must Have)</h3>\n<p>The system must deliver <strong>real-time, reliable, and ordered</strong> change data capture from transactional databases to downstream consumers. These goals are categorized into functional requirements (what the system does) and non-functional requirements (how well it does it).</p>\n<h4 id=\"functional-goals\">Functional Goals</h4>\n<table>\n<thead>\n<tr>\n<th>Goal Category</th>\n<th>Specific Requirement</th>\n<th>Description &amp; Rationale</th>\n<th>Associated Milestone</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Log Reading &amp; Parsing</strong></td>\n<td>Connect to database transaction logs</td>\n<td>Establish a direct connection to PostgreSQL&#39;s Write-Ahead Log (WAL) or MySQL&#39;s binlog using their respective native protocols (logical decoding for PostgreSQL, binlog replication for MySQL). This is foundational—without reading the log, there is no CDC.</td>\n<td>1</td>\n</tr>\n<tr>\n<td></td>\n<td>Parse INSERT, UPDATE, DELETE operations</td>\n<td>Extract these three DML (Data Manipulation Language) operations from log entries. These represent the core data changes that downstream systems care about. DDL (Data Definition Language) operations like <code>ALTER TABLE</code> are handled separately under schema evolution.</td>\n<td>1</td>\n</tr>\n<tr>\n<td></td>\n<td>Capture before and after images for UPDATE</td>\n<td>For UPDATE operations, extract both the previous state (<code>beforeImage</code>) and the new state (<code>afterImage</code>) of the row. This enables consumers to understand what changed, not just the new value, which is critical for audit trails, incremental materialized views, and conflict resolution.</td>\n<td>1</td>\n</tr>\n<tr>\n<td></td>\n<td>Track and persist log position</td>\n<td>Record the <strong>log sequence number (LSN)</strong> for PostgreSQL or <strong>binlog position</strong> for MySQL after successfully processing each transaction. This state must be durably stored (e.g., in a separate state table or file) to enable resumption from the exact point after a restart, preventing data loss or duplication.</td>\n<td>1</td>\n</tr>\n<tr>\n<td><strong>Event Construction &amp; Delivery</strong></td>\n<td>Publish events to a message broker</td>\n<td>Stream <code>ChangeEvent</code> objects to Apache Kafka (or a similar message broker like Pulsar). The broker provides buffering, persistence, and fan-out capabilities that decouple the CDC system from consumers.</td>\n<td>2</td>\n</tr>\n<tr>\n<td></td>\n<td>Guarantee at-least-once delivery</td>\n<td>Ensure every <code>ChangeEvent</code> reaches the message broker at least once, even in the face of network failures or producer restarts. This is achieved through producer retries with idempotence enabled and acknowledgment (ack) from all in-sync replicas before considering an event &quot;sent.&quot;</td>\n<td>2</td>\n</tr>\n<tr>\n<td></td>\n<td>Maintain ordering per primary key</td>\n<td>Guarantee that all changes to the same database row (identified by its primary key) are delivered to consumers in the exact order they were committed in the source database. This preserves causality and prevents race conditions in consumer processing.</td>\n<td>2</td>\n</tr>\n<tr>\n<td></td>\n<td>Handle backpressure</td>\n<td>Monitor consumer lag (the difference between the latest event produced and the last event consumed). If lag exceeds a configurable threshold, the system should slow down or pause log reading to prevent overwhelming downstream systems and the broker.</td>\n<td>2</td>\n</tr>\n<tr>\n<td><strong>Schema Management</strong></td>\n<td>Store and version table schemas</td>\n<td>Maintain a history of <code>SchemaVersion</code> objects for each table in a <strong>schema registry</strong>. Each version includes the complete column definition (<code>columnDefinitions</code>) at that point in time.</td>\n<td>3</td>\n</tr>\n<tr>\n<td></td>\n<td>Enforce backward compatibility</td>\n<td>Validate that new schema versions are <strong>backward compatible</strong> with the previous version. This means consumers using the old schema can read data written with the new schema. In practice, this allows adding nullable columns or columns with default values but prohibits removing columns or narrowing column types.</td>\n<td>3</td>\n</tr>\n<tr>\n<td></td>\n<td>Propagate schema change events</td>\n<td>Emit a special <code>ChangeEvent</code> (with <code>operationType = &quot;SCHEMA_CHANGE&quot;</code>) to a dedicated topic when a compatible schema change occurs. This event includes the new <code>schemaVersionId</code> and metadata about the change, allowing consumers to refresh their local schema cache or trigger reprocessing.</td>\n<td>3</td>\n</tr>\n<tr>\n<td><strong>Operational &amp; Observability</strong></td>\n<td>Provide monitoring metrics</td>\n<td>Expose key metrics: number of events processed per second, consumer lag per partition, parse errors, schema validation failures, and database connection health. These should be available via a Prometheus endpoint or similar.</td>\n<td>2, 3</td>\n</tr>\n<tr>\n<td></td>\n<td>Support graceful shutdown and restart</td>\n<td>On shutdown signal, complete processing of the current log transaction, flush all pending events to the broker, commit the final log position, and then exit. On restart, reload the last committed position and resume reading from that exact point.</td>\n<td>1, 2</td>\n</tr>\n<tr>\n<td></td>\n<td>Configuration externalization</td>\n<td>All operational parameters (database connection details, broker addresses, thresholds) must be loadable from external configuration files or environment variables, not hardcoded. The <code>ConfigLoader.loadConfig</code> utility is provided for this purpose.</td>\n<td>All</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architectural Insight:</strong> The functional goals are intentionally scoped to <strong>logical decoding</strong> of committed transactions only. We avoid <strong>trigger-based</strong> CDC (which modifies application tables) and <strong>query-based</strong> CDC (which polls tables), as these impose unacceptable performance overhead on the source database and cannot capture deletes. Reading the transaction log is the only non-intrusive method for real-time change capture.</p>\n</blockquote>\n<h4 id=\"non-functional-goals-quality-attributes\">Non-Functional Goals (Quality Attributes)</h4>\n<table>\n<thead>\n<tr>\n<th>Quality Attribute</th>\n<th>Target</th>\n<th>Justification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Latency</strong></td>\n<td>End-to-end latency (database commit to event available to consumer) ≤ 1 second under normal load.</td>\n<td>Real-time use cases (e.g., fraud detection, live dashboards) require sub-second freshness. The primary latency contributors are log flush frequency on the database (controllable via configuration) and network round-trip to the broker.</td>\n</tr>\n<tr>\n<td><strong>Throughput</strong></td>\n<td>Sustain processing of at least 10,000 change events per second on a single CDC instance.</td>\n<td>Must handle peak write loads of typical online transaction processing (OLTP) systems. Throughput is primarily bounded by log parsing CPU, network bandwidth to the broker, and serialization overhead.</td>\n</tr>\n<tr>\n<td><strong>Availability</strong></td>\n<td>99.9% uptime (≈8.76 hours of downtime per year). The system should be restartable without data loss.</td>\n<td>While not mission-critical like the source database, the CDC system is a core data pipeline component. High availability is achieved through fast recovery (resuming from persisted position) and potentially multiple instances (though only one can read the log at a time).</td>\n</tr>\n<tr>\n<td><strong>Durability</strong></td>\n<td>Zero data loss under single-node failure of the CDC system.</td>\n<td>Achieved by <strong>only advancing the persisted log position after events are confirmed as durably stored in the message broker</strong> (acknowledged by all in-sync replicas). This creates a chain of durability from database log → broker → consumer.</td>\n</tr>\n<tr>\n<td><strong>Operational Simplicity</strong></td>\n<td>Single binary deployment with minimal runtime dependencies (besides the database and broker).</td>\n<td>Reduces deployment complexity and operational overhead. The CDC system should not require a separate cluster manager, distributed coordination service (like ZooKeeper), or complex orchestration to function in its basic form.</td>\n</tr>\n<tr>\n<td><strong>Extensibility</strong></td>\n<td>Clear interfaces for adding support for new database types (e.g., Oracle, SQL Server).</td>\n<td>The core CDC logic should be database-agnostic. Database-specific parsing is isolated behind a <code>LogConnector</code> interface, allowing new adapters to be plugged in with minimal changes to the event pipeline.</td>\n</tr>\n</tbody></table>\n<h3 id=\"22-non-goals-explicitly-out-of-scope\">2.2 Non-Goals (Explicitly Out of Scope)</h3>\n<p>Explicitly stating what the system <strong>will not do</strong> is equally important to prevent misaligned expectations and endless feature creep. These decisions allow us to build a focused, robust core system.</p>\n<blockquote>\n<p><strong>Decision: Focus on Core CDC Pipeline, Not Auxiliary Services</strong><br><strong>Context:</strong> A complete enterprise data platform might include many related services: data transformation, enrichment, dead-letter queues, complex event processing, and managed deployments.<br><strong>Options Considered:</strong>  </p>\n<ol>\n<li><strong>Build a comprehensive data pipeline platform</strong> with built-in transformation, enrichment, and complex routing.  </li>\n<li><strong>Build only the core CDC extraction and delivery pipeline</strong>, delegating other concerns to specialized downstream systems.<br><strong>Decision:</strong> Option 2—focus on being a reliable source of raw change events.<br><strong>Rationale:</strong> The Unix philosophy: &quot;Do one thing well.&quot; By outputting standardized <code>ChangeEvent</code> objects to a message broker, we enable any number of downstream systems (Stream processors like Flink, ETL tools, custom applications) to consume and transform the data according to their specific needs. This keeps the CDC system simple, maintainable, and focused on its hardest problem: reliable log reading.<br><strong>Consequences:</strong> Users must deploy additional systems for transformations, but they gain flexibility in choosing the right tool for each job. The CDC system remains stable and easier to reason about.</li>\n</ol>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Non-Goal Area</th>\n<th>Specific Exclusions</th>\n<th>Reasoning &amp; Recommended Alternative</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Data Transformation &amp; Enrichment</strong></td>\n<td>No in-flight modification of event payloads (e.g., joining with reference data, masking sensitive fields, computing derived columns).</td>\n<td>Transformation is a separate concern with different scalability and fault-tolerance requirements. <strong>Alternative:</strong> Stream processing frameworks (Apache Flink, Kafka Streams) or dedicated ETL tools should consume raw events and perform transformations.</td>\n</tr>\n<tr>\n<td><strong>Exactly-Once Delivery Semantics</strong></td>\n<td>The system will not guarantee exactly-once delivery to end consumers in the face of all possible failures.</td>\n<td>Achieving exactly-once semantics across the entire pipeline (database → CDC → broker → consumer) requires distributed transactions and heavy coordination, which adds immense complexity and performance overhead. <strong>Alternative:</strong> We guarantee <strong>at-least-once</strong> with idempotent producers to the broker, and consumers must implement <strong>idempotent processing</strong> (e.g., deduplication by <code>eventId</code> or by primary key + timestamp) to achieve effectively-once semantics.</td>\n</tr>\n<tr>\n<td><strong>Global Ordering Across All Events</strong></td>\n<td>Events across different tables or different primary keys within the same table are <strong>not</strong> guaranteed to be delivered in commit order relative to each other.</td>\n<td>Maintaining strict global ordering would require single-threaded processing or complex distributed sequencing, destroying throughput. <strong>Alternative:</strong> We guarantee <strong>partition-level ordering</strong> (per primary key). Consumers needing cross-key causality must infer it via timestamps or use more advanced techniques like distributed snapshots, which are out of scope.</td>\n</tr>\n<tr>\n<td><strong>DDL Change Automation</strong></td>\n<td>The system will not automatically apply DDL changes (like <code>ALTER TABLE</code>) to downstream data stores or data warehouses.</td>\n<td>Schema changes often require careful migration planning, data backfilling, and consumer coordination. Automating this is risky. <strong>Alternative:</strong> The system <strong>will detect and notify</strong> of schema changes via <code>SchemaChangeEvent</code>. A separate orchestration process or human operator should then manage the downstream migration.</td>\n</tr>\n<tr>\n<td><strong>Change Data Capture for Non-Supported Databases</strong></td>\n<td>Out-of-the-box support for databases other than PostgreSQL and MySQL (e.g., Oracle, SQL Server, MongoDB).</td>\n<td>Each database has unique log formats and replication protocols. Supporting all would make the codebase unwieldy. <strong>Alternative:</strong> The system is designed with a pluggable <code>LogConnector</code> interface. Support for additional databases can be added as extensions in future versions without modifying the core pipeline.</td>\n</tr>\n<tr>\n<td><strong>Historical Data Snapshotting</strong></td>\n<td>The system will not perform initial full-table snapshots (bulk load of existing data) before beginning incremental change capture.</td>\n<td>Snapshotting requires different mechanisms (e.g., table locks, consistent reads) and has different recovery characteristics than log reading. <strong>Alternative:</strong> A separate, one-time snapshot tool should be used to bootstrap consumers. The CDC system then streams changes from the snapshot point forward.</td>\n</tr>\n<tr>\n<td><strong>Long-Term Event Storage &amp; Archive</strong></td>\n<td>Events are not stored indefinitely within the CDC system. Retention is delegated to the message broker.</td>\n<td>Storage management is a separate concern. <strong>Alternative:</strong> Configure Kafka retention policies based on storage capacity and compliance requirements. For archival needs, use Kafka Connect to sink events to long-term storage (S3, HDFS).</td>\n</tr>\n<tr>\n<td><strong>Graphical User Interface (GUI)</strong></td>\n<td>No administrative web UI for monitoring, configuration, or control.</td>\n<td>Building a production-grade UI is a major undertaking that distracts from core data pipeline reliability. <strong>Alternative:</strong> Expose metrics via Prometheus (for dashboards with Grafana) and provide command-line tools for operational tasks.</td>\n</tr>\n<tr>\n<td><strong>Multi-Tenancy &amp; Access Control</strong></td>\n<td>No built-in user authentication, authorization, or tenant isolation within the CDC system itself.</td>\n<td>Security boundaries should be enforced at the infrastructure level (database permissions, network isolation, broker ACLs). <strong>Alternative:</strong> Run separate CDC instances per tenant, or rely on database and broker-level security features.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Design Principle:</strong> By explicitly excluding these areas, we commit to building a <strong>simple, composable component</strong> rather than a monolithic platform. This aligns with the modern data stack philosophy, where best-of-breed tools are connected via streaming platforms. The CDC system&#39;s job is to be the most reliable and performant bridge between the database and that ecosystem.</p>\n</blockquote>\n<p>The following table summarizes the architectural trade-offs implied by these non-goals:</p>\n<table>\n<thead>\n<tr>\n<th>Trade-off</th>\n<th>Benefit</th>\n<th>Cost</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>At-least-once vs Exactly-once</strong></td>\n<td>Simpler implementation, higher throughput, no need for distributed transaction coordinator.</td>\n<td>Consumers must implement idempotent processing; possible duplicate events during failure recovery.</td>\n</tr>\n<tr>\n<td><strong>Partition ordering vs Global ordering</strong></td>\n<td>Enables massive parallelism (multiple partitions processed concurrently), high throughput.</td>\n<td>Applications needing cross-key causality must implement more complex logic or accept eventual consistency.</td>\n</tr>\n<tr>\n<td><strong>No built-in transformation</strong></td>\n<td>Focused codebase, easier debugging, stability in core function.</td>\n<td>Requires additional stream processing component in the architecture, increasing system complexity overall.</td>\n</tr>\n<tr>\n<td><strong>Notification-only for DDL</strong></td>\n<td>Safe, avoids automatic actions that could break downstream systems.</td>\n<td>Requires operational processes to handle schema migrations, potentially slower response to changes.</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (For Learning/Dev)</th>\n<th>Advanced Option (For Production)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Log Parser Library</strong></td>\n<td>Direct JDBC for initial testing (simpler but higher latency)</td>\n<td><strong>Debezium Embedded Engine</strong> (mature, handles version-specific log formats, includes state management)</td>\n</tr>\n<tr>\n<td><strong>Message Broker</strong></td>\n<td>Apache Kafka (single broker in Docker)</td>\n<td><strong>Apache Kafka Cluster</strong> (multiple brokers with replication factor ≥ 2 for high availability)</td>\n</tr>\n<tr>\n<td><strong>Schema Registry</strong></td>\n<td>In-memory <code>ConcurrentHashMap</code>-based registry (for testing)</td>\n<td><strong>Confluent Schema Registry</strong> (RESTful, supports Avro, Protobuf, JSON Schema, compatibility checking)</td>\n</tr>\n<tr>\n<td><strong>Metrics &amp; Monitoring</strong></td>\n<td>Logging to stdout with structured JSON</td>\n<td><strong>Micrometer + Prometheus</strong> (exposes metrics endpoint) + <strong>Grafana</strong> (dashboards)</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>YAML file loaded via <code>ConfigLoader</code></td>\n<td><strong>HashiCorp Consul</strong> or <strong>Spring Cloud Config</strong> (dynamic configuration, secrets management)</td>\n</tr>\n<tr>\n<td><strong>State Persistence</strong></td>\n<td>File-based storage (JSON in local file)</td>\n<td><strong>Kafka <code>__consumer_offsets</code></strong> topic or a dedicated database table (more durable, supports multi-instance)</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-project-file-structure\">B. Recommended Project File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-system/\n├── config/\n│   └── application.yml              # Main configuration file\n├── src/main/java/com/cdc/\n│   ├── core/\n│   │   ├── ConfigLoader.java        # Provided utility\n│   │   ├── RawLogEntry.java         # Core data type\n│   │   ├── ChangeEvent.java         # Core data type\n│   │   └── SchemaVersion.java       # Core data type\n│   ├── connector/\n│   │   ├── LogConnector.java        # Interface for database-specific connectors\n│   │   ├── postgres/\n│   │   │   ├── PostgresWalConnector.java\n│   │   │   └── PostgresWalParser.java\n│   │   └── mysql/\n│   │       ├── MySqlBinlogConnector.java\n│   │       └── MySqlBinlogParser.java\n│   ├── builder/\n│   │   ├── ChangeEventBuilder.java\n│   │   └── TransactionState.java    # Tracks in-progress transactions\n│   ├── streamer/\n│   │   ├── EventStreamer.java\n│   │   ├── KafkaEventPublisher.java\n│   │   └── BackpressureController.java\n│   ├── schema/\n│   │   ├── SchemaRegistry.java\n│   │   ├── CompatibilityChecker.java\n│   │   └── InMemorySchemaRegistry.java # Simple implementation\n│   ├── state/\n│   │   └── PositionManager.java     # Persists and loads LSN/binlog position\n│   └── Main.java                    # Application entry point\n└── README.md</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Configuration Loader (Provided Utility):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.core;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.databind.ObjectMapper;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.dataformat.yaml.YAMLFactory;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.File;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.IOException;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ConfigLoader</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ObjectMapper mapper </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ObjectMapper</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">new</span><span style=\"color:#B392F0\"> YAMLFactory</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Loads configuration from a YAML file and maps it to the specified class.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> configPath</span><span style=\"color:#6A737D\"> Path to the YAML configuration file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> configClass</span><span style=\"color:#6A737D\"> Class of the configuration object (e.g., AppConfig.class)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@return</span><span style=\"color:#6A737D\"> Populated configuration object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@throws</span><span style=\"color:#B392F0\"> IOException</span><span style=\"color:#6A737D\"> If file cannot be read or parsed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> T </span><span style=\"color:#B392F0\">loadConfig</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#E1E4E8\">, Class&#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">configClass</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> IOException {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> mapper.</span><span style=\"color:#B392F0\">readValue</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">new</span><span style=\"color:#B392F0\"> File</span><span style=\"color:#E1E4E8\">(configPath), configClass);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Application Configuration Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.core;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> AppConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> DatabaseConfig database;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> KafkaConfig kafka;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> SchemaConfig schema;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> maxBatchSize;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> backpressureThresholdMs;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add getters and setters for all fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> DatabaseConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String type; </span><span style=\"color:#6A737D\">// \"postgresql\" or \"mysql\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String host;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> port;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String database;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String username;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String password;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String slotName; </span><span style=\"color:#6A737D\">// For PostgreSQL logical decoding slot</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Add getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> KafkaConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String bootstrapServers;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String topicPrefix;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> replicationFactor;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> partitionsPerTable;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String acks; </span><span style=\"color:#6A737D\">// \"all\" for at-least-once</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#E1E4E8\"> enableIdempotence;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Add getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> SchemaConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String registryType; </span><span style=\"color:#6A737D\">// \"inmemory\" or \"confluent\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#E1E4E8\"> String registryUrl; </span><span style=\"color:#6A737D\">// For Confluent Schema Registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Add getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Main Application Entry Point:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.core.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.core.ConfigLoader;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.LogConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.postgres.PostgresWalConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.mysql.MySqlBinlogConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.builder.ChangeEventBuilder;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.streamer.EventStreamer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.schema.SchemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.state.PositionManager;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> Main</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#FFAB70\">args</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Load configuration from \"config/application.yml\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Initialize PositionManager to load last saved log position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Initialize SchemaRegistry (in-memory or remote)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Create appropriate LogConnector based on database type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Initialize ChangeEventBuilder with SchemaRegistry and PositionManager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 6: Initialize EventStreamer with Kafka configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 7: Connect LogConnector to database and start reading from last position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 8: For each RawLogEntry parsed, pass to ChangeEventBuilder</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 9: For each ChangeEvent built, pass to EventStreamer for publishing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 10: Handle shutdown signal to gracefully stop all components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (Exception </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            System.err.</span><span style=\"color:#B392F0\">println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CDC System failed to start: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">getMessage</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            e.</span><span style=\"color:#B392F0\">printStackTrace</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            System.</span><span style=\"color:#B392F0\">exit</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-java\">E. Language-Specific Hints (Java)</h4>\n<ol>\n<li><strong>Use try-with-resources</strong> for all database and network connections to ensure proper cleanup.</li>\n<li><strong>For JSON serialization</strong> of configuration and state, use Jackson (<code>ObjectMapper</code>) as shown in <code>ConfigLoader</code>.</li>\n<li><strong>For Kafka integration</strong>, use the official <code>kafka-clients</code> library. Configure <code>ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG = true</code> and <code>ProducerConfig.ACKS_CONFIG = &quot;all&quot;</code> for at-least-once semantics.</li>\n<li><strong>For concurrent state management</strong> in <code>ChangeEventBuilder</code>, use <code>ConcurrentHashMap</code> for tracking transaction state, keyed by <code>transactionId</code>.</li>\n<li><strong>Use SLF4J with Logback</strong> for structured logging, not <code>System.out.println</code>.</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the foundational structure from this section (configuration loading, basic type definitions), verify:</p>\n<ol>\n<li><strong>Configuration Loading Works:</strong></li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   $ java -cp target/cdc-system.jar com.cdc.Main\n   Should print: &quot;CDC System starting with configuration from config/application.yml&quot;</code></pre></div>\n<p>   Create a minimal <code>config/application.yml</code>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#85E89D\">   database</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">     type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"postgresql\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">     host</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"localhost\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">     port</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">5432</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">     database</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"mydb\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">   kafka</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">     bootstrapServers</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"localhost:9092\"</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><p><strong>Type Definitions Compile:</strong> Ensure the three core types (<code>RawLogEntry</code>, <code>ChangeEvent</code>, <code>SchemaVersion</code>) compile without errors. You should be able to create instances of each in a simple test.</p>\n</li>\n<li><p><strong>Project Structure Validated:</strong> Confirm your project follows the recommended directory layout. This foundation will make subsequent milestone implementation much smoother.</p>\n</li>\n</ol>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Configuration file not found&quot; error on startup</td>\n<td>Wrong path or working directory</td>\n<td>Print <code>new File(&quot;.&quot;).getAbsolutePath()</code> to see current directory</td>\n<td>Use absolute path in <code>ConfigLoader</code> or run from project root</td>\n</tr>\n<tr>\n<td><code>RawLogEntry</code>/<code>ChangeEvent</code> fields missing or wrong type</td>\n<td>Type definitions don&#39;t match naming conventions</td>\n<td>Compare field names exactly with NAMING CONVENTIONS section</td>\n<td>Ensure fields match EXACTLY: <code>logSequenceNumber String</code>, <code>operationType String</code>, etc.</td>\n</tr>\n<tr>\n<td>Unable to import Jackson classes in <code>ConfigLoader</code></td>\n<td>Missing dependency in build file (pom.xml or build.gradle)</td>\n<td>Check if <code>jackson-dataformat-yaml</code> is in dependencies</td>\n<td>Add: <code>&lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt;&lt;artifactId&gt;jackson-dataformat-yaml&lt;/artifactId&gt;</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3</p>\n</blockquote>\n<h2 id=\"3-high-level-architecture\">3. High-Level Architecture</h2>\n<p>This section provides a comprehensive architectural blueprint of the Change Data Capture (CDC) system. Before diving into component-level details, we establish the &quot;big picture&quot; view of how the system transforms low-level database transaction logs into reliable, schema-aware event streams for downstream consumers. Think of the architecture as an <strong>industrial assembly line</strong>: raw database log entries enter at one end and, through a series of specialized processing stations, emerge as polished, packaged change events ready for delivery. Each station has a specific responsibility, maintains its own state, and communicates with others through well-defined interfaces.</p>\n<p>The architecture follows a <strong>pipeline pattern</strong> with clear separation of concerns, enabling independent development, testing, and scaling of each component. The system is designed to be <strong>database-agnostic</strong> at the pipeline level, with database-specific details encapsulated in pluggable adapters. This modular approach balances the need for consistent processing semantics across different database systems with the reality that each database implements its transaction logs differently.</p>\n<p><img src=\"/api/project/cdc-system/architecture-doc/asset?path=diagrams%2Fsystem-component.svg\" alt=\"High-Level System Component Diagram\"></p>\n<h3 id=\"31-component-overview-and-responsibilities\">3.1 Component Overview and Responsibilities</h3>\n<p>The system comprises four core internal components that work together, plus three external systems they interact with. The following table provides a complete reference of each component&#39;s role, ownership, and key responsibilities:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Role (Mental Model)</th>\n<th>What It Owns (State)</th>\n<th>Key Responsibilities</th>\n<th>Input → Output Transformation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Log Connector &amp; Parser</strong> (Milestone 1)</td>\n<td><strong>The Database Translator</strong> – reads the database&#39;s internal &quot;flight recorder&quot; and converts its binary format into structured data.</td>\n<td>- Database connection handle<br>- Last processed LSN/offset<br>- Parser state (connected, reading, paused)<br>- Database-specific parser implementation</td>\n<td>1. Establishes and maintains connection to database transaction log (WAL/binlog)<br>2. Reads binary log entries in streaming fashion<br>3. Parses binary format into structured <code>RawLogEntry</code> objects<br>4. Tracks position (LSN) for resumption after restart<br>5. Handles database-specific quirks and version differences</td>\n<td>Binary log entry → <code>RawLogEntry</code></td>\n</tr>\n<tr>\n<td><strong>Change Event Builder</strong> (Milestone 1)</td>\n<td><strong>The Event Assembler</strong> – groups related log entries into complete change events, ensuring transaction atomicity and deduplication.</td>\n<td>- In-progress transaction buffer<br>- Last seen event IDs for deduplication<br>- Mapping between database types and schema IDs</td>\n<td>1. Groups <code>RawLogEntry</code> objects by transaction boundaries<br>2. Constructs <code>ChangeEvent</code> with before/after images<br>3. Attaches schema version ID to each event<br>4. Performs deduplication based on primary keys<br>5. Orders events within same transaction by operation sequence</td>\n<td><code>RawLogEntry</code> (multiple) → <code>ChangeEvent</code></td>\n</tr>\n<tr>\n<td><strong>Event Streamer &amp; Delivery</strong> (Milestone 2)</td>\n<td><strong>The Reliable Postal Service</strong> – ensures change events reach their destination with delivery guarantees and proper ordering.</td>\n<td>- Kafka producer instances<br>- Producer configuration and state<br>- Delivery callbacks and error handlers<br>- Backpressure monitoring state</td>\n<td>1. Publishes <code>ChangeEvent</code> objects to Kafka topics<br>2. Implements at-least-once delivery semantics<br>3. Enforces ordering per primary key via partitioning<br>4. Monitors consumer lag and applies backpressure<br>5. Handles broker failures and network issues</td>\n<td><code>ChangeEvent</code> → Kafka message (persisted)</td>\n</tr>\n<tr>\n<td><strong>Schema Registry</strong> (Milestone 3)</td>\n<td><strong>The Contract Librarian</strong> – maintains the evolving &quot;data contracts&quot; between database tables and their consumers.</td>\n<td>- Versioned schema definitions<br>- Compatibility rules configuration<br>- Schema change history<br>- Client connection cache</td>\n<td>1. Stores and versions table schema definitions<br>2. Validates schema changes against compatibility rules<br>3. Provides schema lookup by version ID<br>4. Emits schema change notifications<br>5. Handles schema evolution migrations</td>\n<td>DDL statement → <code>SchemaVersion</code> + compatibility check result</td>\n</tr>\n<tr>\n<td><strong>Source Database</strong> (External)</td>\n<td><strong>The Event Source</strong> – the operational database whose changes we capture.</td>\n<td>Transaction logs (WAL/binlog), table schemas, and actual data.</td>\n<td>1. Generates transaction log entries for all data changes<br>2. Provides logical decoding/replication interfaces<br>3. Maintains log retention and rotation policies</td>\n<td>Data modifications → Transaction log entries</td>\n</tr>\n<tr>\n<td><strong>Message Broker (Kafka)</strong> (External)</td>\n<td><strong>The Event Highway</strong> – provides durable, ordered, partitioned event storage and distribution.</td>\n<td>Topics, partitions, offsets, and consumer groups.</td>\n<td>1. Persists change events with configurable retention<br>2. Distributes events to multiple consumer groups<br>3. Maintains ordering within partitions<br>4. Tracks consumer progress via offsets</td>\n<td>Producer messages → Consumer-accessible streams</td>\n</tr>\n<tr>\n<td><strong>Downstream Consumers</strong> (External)</td>\n<td><strong>The Event Destinations</strong> – applications that react to or analyze database changes.</td>\n<td>Their own processing state and business logic.</td>\n<td>1. Subscribe to Kafka topics<br>2. Process change events according to their business needs<br>3. Commit offsets to track progress<br>4. Handle schema evolution notifications</td>\n<td>Change events → Business actions/analytics</td>\n</tr>\n</tbody></table>\n<p><strong>Data Flow Through the Pipeline:</strong></p>\n<p>The end-to-end flow follows this sequence:</p>\n<ol>\n<li><p><strong>Capture</strong>: When a database transaction commits, the database writes corresponding entries to its transaction log (PostgreSQL WAL or MySQL binlog). The Log Connector &amp; Parser continuously reads these entries via the database&#39;s logical decoding interface (e.g., PostgreSQL&#39;s <code>pgoutput</code> or MySQL&#39;s binlog replication protocol).</p>\n</li>\n<li><p><strong>Parse &amp; Translate</strong>: Each binary log entry is parsed into a <code>RawLogEntry</code> containing the operation type, table name, affected row data (both old and new values for updates), and the log sequence number (LSN). This parsing is database-specific but produces a standardized intermediate format.</p>\n</li>\n<li><p><strong>Assemble &amp; Enrich</strong>: The Change Event Builder collects all <code>RawLogEntry</code> objects belonging to the same transaction. Once the transaction commit is detected, it assembles complete <code>ChangeEvent</code> objects, attaches the appropriate schema version ID by querying the Schema Registry, and ensures deduplication and ordering.</p>\n</li>\n<li><p><strong>Deliver &amp; Guarantee</strong>: The Event Streamer publishes each <code>ChangeEvent</code> to Kafka, using the table name and primary key to determine the target partition. It waits for acknowledgment from Kafka (configured for at-least-once semantics) before marking the event as delivered. If consumers fall behind, backpressure mechanisms slow down or pause the parser.</p>\n</li>\n<li><p><strong>Evolve &amp; Notify</strong>: When a DDL statement (like <code>ALTER TABLE</code>) is detected in the transaction log, the Schema Registry is consulted. The new schema is validated for compatibility, registered with a new version ID, and a special schema change event is emitted to notify consumers.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Key Design Insight</strong>: The pipeline is designed with <strong>idempotency</strong> in mind. Each component can be restarted and resume from its last known good state without creating duplicates or losing data. This is achieved through persistent position tracking (LSN offsets), idempotent Kafka producers, and idempotent schema registration.</p>\n</blockquote>\n<h3 id=\"32-recommended-project-file-structure\">3.2 Recommended Project File Structure</h3>\n<p>A well-organized codebase is critical for managing the complexity of a CDC system. The following structure separates concerns logically, isolates database-specific code, and creates clear boundaries for testing and deployment. This structure follows Maven/Gradle conventions for Java projects but can be adapted for other languages.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-system/\n├── README.md                           # Project overview and setup instructions\n├── pom.xml (or build.gradle)           # Build configuration and dependencies\n├── config/\n│   ├── application.yaml                # Main configuration file (loaded by ConfigLoader)\n│   ├── logback.xml                     # Logging configuration\n│   └── database-specific/\n│       ├── postgresql-parser.yaml      # PostgreSQL-specific parser settings\n│       └── mysql-parser.yaml           # MySQL-specific parser settings\n├── src/main/java/com/example/cdc/\n│   ├── Main.java                       # Entry point: Main.main(String[])\n│   ├── config/\n│   │   ├── AppConfig.java              # Root configuration class\n│   │   ├── DatabaseConfig.java         # Database connection settings\n│   │   ├── KafkaConfig.java            # Kafka producer settings\n│   │   ├── SchemaConfig.java           # Schema registry settings\n│   │   └── ConfigLoader.java           # Utility: ConfigLoader.loadConfig()\n│   ├── model/                          # Core data types (immutable value objects)\n│   │   ├── RawLogEntry.java            # Parsed log entry\n│   │   ├── ChangeEvent.java            # Final change event\n│   │   ├── SchemaVersion.java          # Schema version metadata\n│   │   └── OperationType.java          # Enum: INSERT, UPDATE, DELETE\n│   ├── connector/                      # Log Connector &amp; Parser component\n│   │   ├── LogConnector.java           # Main interface for log reading\n│   │   ├── state/\n│   │   │   ├── ConnectorState.java     # Enum: IDLE, CONNECTING, READING, etc.\n│   │   │   └── PositionTracker.java    # Manages LSN/offset persistence\n│   │   ├── parser/\n│   │   │   ├── LogParser.java          # Generic parser interface\n│   │   │   ├── postgresql/\n│   │   │   │   ├── PostgresParser.java # PostgreSQL WAL parser implementation\n│   │   │   │   └── PgOutputDecoder.java# Handles pgoutput logical decoding\n│   │   │   └── mysql/\n│   │   │       ├── MySqlParser.java    # MySQL binlog parser implementation\n│   │   │       └── BinlogDecoder.java  # Handles MySQL binlog format\n│   │   └── exception/\n│   │       └── ParseException.java     # Custom exception for parse failures\n│   ├── builder/                        # Change Event Builder component\n│   │   ├── ChangeEventBuilder.java     # Main builder interface\n│   │   ├── TransactionBuffer.java      # Buffers entries per transaction\n│   │   ├── Deduplicator.java           # Removes duplicate events\n│   │   └── EventOrderer.java           # Ensures causal ordering\n│   ├── streamer/                       # Event Streamer &amp; Delivery component\n│   │   ├── EventStreamer.java          # Main streaming interface\n│   │   ├── KafkaEventPublisher.java    # Kafka implementation\n│   │   ├── DeliveryGuarantee.java      # Implements at-least-once logic\n│   │   ├── BackpressureController.java # Monitors lag and applies backpressure\n│   │   └── partitioner/\n│   │       ├── EventPartitioner.java   # Interface for partitioning strategy\n│   │       └── TablePkPartitioner.java # Partitions by table + primary key hash\n│   ├── schema/                         # Schema Registry component\n│   │   ├── SchemaRegistry.java         # Main registry interface\n│   │   ├── InMemorySchemaRegistry.java # Simple in-memory implementation\n│   │   ├── SchemaValidator.java        # Validates compatibility\n│   │   ├── evolution/\n│   │   │   ├── CompatibilityMode.java  # Enum: BACKWARD, FORWARD, FULL\n│   │   │   └── SchemaMigrator.java     # Applies schema migrations\n│   │   └── event/\n│   │       └── SchemaChangeEvent.java  # Special event for schema changes\n│   ├── pipeline/                       # Orchestrates the entire pipeline\n│   │   ├── CdcPipeline.java            # Main pipeline coordinator\n│   │   └── PipelineStateManager.java   # Manages pipeline lifecycle and state\n│   └── util/                           # Shared utilities\n│       ├── JsonSerializer.java         # JSON serialization/deserialization\n│       ├── MetricsCollector.java       # Collects and exposes metrics\n│       └── RetryUtil.java              # Retry logic with exponential backoff\n└── src/test/java/com/example/cdc/      # Comprehensive test suite\n    ├── unit/                           # Unit tests for each component\n    ├── integration/                    # Integration tests with embedded DB/Kafka\n    └── e2e/                            # End-to-end pipeline tests</code></pre></div>\n\n<p><strong>Rationale for Key Organizational Decisions:</strong></p>\n<ol>\n<li><p><strong>Database-Specific Code Isolation</strong>: Placing PostgreSQL and MySQL parsers in separate subdirectories (<code>connector/parser/postgresql/</code>, <code>connector/parser/mysql/</code>) allows clean separation. New database adapters can be added without modifying core parsing logic.</p>\n</li>\n<li><p><strong>Configuration Centralization</strong>: The <code>config/</code> directory at the root holds all configuration files, with database-specific configurations separated. The <code>ConfigLoader</code> utility provides a single way to load any configuration class.</p>\n</li>\n<li><p><strong>Immutable Model Objects</strong>: All classes in the <code>model/</code> package are designed as immutable value objects with clear validation in their constructors. This prevents accidental mutation as events flow through the pipeline.</p>\n</li>\n<li><p><strong>Component Cohesion</strong>: Each major component (<code>connector/</code>, <code>builder/</code>, <code>streamer/</code>, <code>schema/</code>) contains all related classes, including their interfaces, implementations, and supporting utilities. This makes the codebase navigable and enforces architectural boundaries.</p>\n</li>\n<li><p><strong>Pipeline Orchestration</strong>: The <code>pipeline/</code> package contains the high-level coordinator that wires components together and manages the overall lifecycle. This separates coordination logic from component implementation details.</p>\n</li>\n<li><p><strong>Test Structure Mirroring</strong>: The test directory structure mirrors the main source structure, making it easy to locate tests for any component.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Important</strong>: This file structure assumes a single monolithic deployment. For production-scale systems, components like the Schema Registry might be deployed as separate services. The code organization still works well in such cases—each component package would become its own microservice project.</p>\n</blockquote>\n<h3 id=\"33-implementation-guidance\">3.3 Implementation Guidance</h3>\n<p>This section provides concrete starting points for implementing the architectural components described above. Since the core learning value is in implementing the CDC logic itself, we provide complete starter code for infrastructure (configuration, serialization) and skeletons for the core components that you&#39;ll fill in.</p>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Getting Started)</th>\n<th>Advanced Option (Production)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Log Parsing</strong></td>\n<td>Database-native JDBC + replication protocols (PostgreSQL JDBC with <code>PGReplicationStream</code>, MySQL Connector/J with binlog)</td>\n<td>Debezium Engine (embedded) for robust, battle-tested parsing</td>\n</tr>\n<tr>\n<td><strong>Event Streaming</strong></td>\n<td>Apache Kafka with plain Java producer</td>\n<td>Confluent Platform with KStreams for stream processing, Schema Registry integration</td>\n</tr>\n<tr>\n<td><strong>Schema Registry</strong></td>\n<td>In-memory registry (for development)</td>\n<td>Confluent Schema Registry or Apicurio Registry for production</td>\n</tr>\n<tr>\n<td><strong>Serialization</strong></td>\n<td>JSON (simple, human-readable)</td>\n<td>Avro with Schema Registry (compact, schema-enforced)</td>\n</tr>\n<tr>\n<td><strong>Metrics</strong></td>\n<td>SLF4J logging</td>\n<td>Micrometer/Prometheus metrics with Grafana dashboards</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>YAML files with SnakeYAML</td>\n<td>Spring Boot Configuration or TypeSafe Config</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-complete-starter-code-for-configuration-loading\">B. Complete Starter Code for Configuration Loading</h4>\n<p>Since configuration management is a prerequisite but not the core learning goal, here&#39;s complete, working code for the configuration system:</p>\n<p><strong>File: <code>src/main/java/com/example/cdc/config/ConfigLoader.java</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.example.cdc.config;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.databind.ObjectMapper;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.dataformat.yaml.YAMLFactory;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.Logger;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.LoggerFactory;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.File;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.IOException;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.InputStream;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Utility class to load configuration from YAML files.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Uses Jackson's YAML parser for deserialization.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ConfigLoader</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Logger LOG </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoggerFactory.</span><span style=\"color:#B392F0\">getLogger</span><span style=\"color:#E1E4E8\">(ConfigLoader.class);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ObjectMapper YAML_MAPPER </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ObjectMapper</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">new</span><span style=\"color:#B392F0\"> YAMLFactory</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    static</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        YAML_MAPPER.</span><span style=\"color:#B392F0\">findAndRegisterModules</span><span style=\"color:#E1E4E8\">(); </span><span style=\"color:#6A737D\">// Handles Java 8 date/time types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Loads configuration from a YAML file on the classpath or filesystem.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> configPath</span><span style=\"color:#6A737D\"> Path to the configuration file (e.g., \"config/application.yaml\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> configClass</span><span style=\"color:#6A737D\"> The class to deserialize into (e.g., AppConfig.class)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@return</span><span style=\"color:#6A737D\"> Fully populated configuration object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@throws</span><span style=\"color:#B392F0\"> IllegalArgumentException</span><span style=\"color:#6A737D\"> if configuration cannot be loaded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> T </span><span style=\"color:#B392F0\">loadConfig</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#E1E4E8\">, Class&#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">configClass</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // First try classpath (for packaged JARs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            InputStream classpathStream </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ConfigLoader.class.</span><span style=\"color:#B392F0\">getClassLoader</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    .</span><span style=\"color:#B392F0\">getResourceAsStream</span><span style=\"color:#E1E4E8\">(configPath);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (classpathStream </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Loading configuration from classpath: {}\"</span><span style=\"color:#E1E4E8\">, configPath);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> YAML_MAPPER.</span><span style=\"color:#B392F0\">readValue</span><span style=\"color:#E1E4E8\">(classpathStream, configClass);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Fall back to filesystem (for development)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            File configFile </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> File</span><span style=\"color:#E1E4E8\">(configPath);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (configFile.</span><span style=\"color:#B392F0\">exists</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Loading configuration from filesystem: {}\"</span><span style=\"color:#E1E4E8\">, configPath);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> YAML_MAPPER.</span><span style=\"color:#B392F0\">readValue</span><span style=\"color:#E1E4E8\">(configFile, configClass);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> IOException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Configuration file not found: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> configPath);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (IOException </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> IllegalArgumentException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to load configuration from \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> configPath, e);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>File: <code>src/main/java/com/example/cdc/config/AppConfig.java</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.example.cdc.config;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Root configuration class containing all subsystem configurations.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Maps directly to the top-level YAML structure.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> AppConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> DatabaseConfig database;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> KafkaConfig kafka;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> SchemaConfig schema;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> maxBatchSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> backpressureThresholdMs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5000</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters and setters (required for Jackson deserialization)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> DatabaseConfig </span><span style=\"color:#B392F0\">getDatabase</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setDatabase</span><span style=\"color:#E1E4E8\">(DatabaseConfig </span><span style=\"color:#FFAB70\">database</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.database </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> KafkaConfig </span><span style=\"color:#B392F0\">getKafka</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> kafka; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setKafka</span><span style=\"color:#E1E4E8\">(KafkaConfig </span><span style=\"color:#FFAB70\">kafka</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.kafka </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> kafka; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> SchemaConfig </span><span style=\"color:#B392F0\">getSchema</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> schema; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setSchema</span><span style=\"color:#E1E4E8\">(SchemaConfig </span><span style=\"color:#FFAB70\">schema</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.schema </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schema; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> int</span><span style=\"color:#B392F0\"> getMaxBatchSize</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> maxBatchSize; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setMaxBatchSize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> maxBatchSize</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.maxBatchSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> maxBatchSize; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> long</span><span style=\"color:#B392F0\"> getBackpressureThresholdMs</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> backpressureThresholdMs; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setBackpressureThresholdMs</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">long</span><span style=\"color:#FFAB70\"> backpressureThresholdMs</span><span style=\"color:#E1E4E8\">) { </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.backpressureThresholdMs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> backpressureThresholdMs; </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>File: <code>src/main/java/com/example/cdc/config/DatabaseConfig.java</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.example.cdc.config;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Database connection and replication configuration.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> DatabaseConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String type; </span><span style=\"color:#6A737D\">// \"postgresql\" or \"mysql\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String host;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> port;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String database;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String username;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String password;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String slotName; </span><span style=\"color:#6A737D\">// PostgreSQL replication slot name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters and setters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getType</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> type; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setType</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">type</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> type; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getHost</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> host; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setHost</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">host</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.host </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> host; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> int</span><span style=\"color:#B392F0\"> getPort</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> port; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setPort</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> port</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.port </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> port; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getDatabase</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setDatabase</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">database</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.database </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getUsername</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> username; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setUsername</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">username</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.username </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> username; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getPassword</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> password; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setPassword</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">password</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.password </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> password; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getSlotName</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> slotName; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setSlotName</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">slotName</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.slotName </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> slotName; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Sample Configuration File: <code>config/application.yaml</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#85E89D\">database</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"postgresql\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  host</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"localhost\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  port</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">5432</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  database</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"mydb\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  username</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"replicator\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  password</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"secretpassword\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  slotName</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"cdc_slot\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#85E89D\">kafka</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  bootstrapServers</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"localhost:9092\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  topicPrefix</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"cdc.\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  replicationFactor</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  partitionsPerTable</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">3</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  acks</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"all\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  enableIdempotence</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#85E89D\">schema</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  registryType</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"inmemory\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  registryUrl</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"http://localhost:8081\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#85E89D\">maxBatchSize</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">500</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">backpressureThresholdMs</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">3000</span></span></code></pre></div>\n\n<h4 id=\"c-core-component-skeleton-code\">C. Core Component Skeleton Code</h4>\n<p>Here&#39;s the skeleton for the main pipeline coordinator that you&#39;ll implement:</p>\n<p><strong>File: <code>src/main/java/com/example/cdc/pipeline/CdcPipeline.java</code></strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.example.cdc.pipeline;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.config.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.connector.LogConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.builder.ChangeEventBuilder;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.streamer.EventStreamer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.schema.SchemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.Logger;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.LoggerFactory;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Main pipeline coordinator that wires all components together and manages the</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * end-to-end flow from database logs to Kafka.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> CdcPipeline</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Logger LOG </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoggerFactory.</span><span style=\"color:#B392F0\">getLogger</span><span style=\"color:#E1E4E8\">(CdcPipeline.class);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> AppConfig config;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> LogConnector logConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> ChangeEventBuilder eventBuilder;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> EventStreamer eventStreamer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> SchemaRegistry schemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> volatile</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#E1E4E8\"> running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> CdcPipeline</span><span style=\"color:#E1E4E8\">(AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Initializes all components in the correct order.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * 1. Schema Registry (needed for event building)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * 2. Event Streamer (needed for backpressure signaling)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * 3. Change Event Builder (needs both registry and streamer)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * 4. Log Connector (needs builder for callbacks)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Initializing CDC pipeline...\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Create and initialize SchemaRegistry based on config.getSchema()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Create and initialize EventStreamer based on config.getKafka()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Create ChangeEventBuilder, passing the schema registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Create LogConnector, passing config.getDatabase() and the event builder</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Set up backpressure linkage: streamer -> connector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: Register shutdown hook for graceful termination</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CDC pipeline initialized successfully\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Starts the pipeline and begins processing database changes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * This method should not block; it starts processing in background threads.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (running) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            LOG.</span><span style=\"color:#B392F0\">warn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Pipeline already running\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Starting CDC pipeline...\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Start the event streamer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Start the log connector (begins reading and parsing)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Log startup completion with initial LSN/offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CDC pipeline started\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Stops the pipeline gracefully, ensuring all in-flight events are processed.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">running) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Stopping CDC pipeline gracefully...\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Stop the log connector (flush remaining entries)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Stop the event builder (complete pending transactions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Stop the event streamer (flush and close Kafka producer)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Stop the schema registry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"CDC pipeline stopped\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Returns the current health status of the pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> HealthStatus </span><span style=\"color:#B392F0\">getHealthStatus</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Check each component's health and return aggregated status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> running </span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\"> HealthStatus.HEALTHY </span><span style=\"color:#F97583\">:</span><span style=\"color:#E1E4E8\"> HealthStatus.STOPPED;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> enum</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        HEALTHY</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">DEGRADED</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">STOPPED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-language-specific-hints-for-java\">D. Language-Specific Hints for Java</h4>\n<ol>\n<li><strong>Use try-with-resources</strong> for all database and network connections to ensure proper cleanup.</li>\n<li><strong>Prefer CompletableFuture</strong> for asynchronous operations in the pipeline to avoid blocking threads.</li>\n<li><strong>Use SLF4J with parameterized logging</strong> (<code>LOG.debug(&quot;Processing event {}&quot;, eventId)</code>) instead of string concatenation for performance.</li>\n<li><strong>Make model classes immutable</strong> using <code>final</code> fields and builders (Lombok <code>@Builder</code> or manual).</li>\n<li><strong>Use Jackson&#39;s <code>ObjectMapper</code></strong> for YAML parsing—it&#39;s more robust than SnakeYAML for complex object graphs.</li>\n<li><strong>Configure Kafka producer with <code>enable.idempotence=true</code></strong> and <code>acks=all</code> for at-least-once semantics.</li>\n<li><strong>Store LSN/offset frequently</strong> (every few seconds or per batch) to minimize replay on restart.</li>\n</ol>\n<h4 id=\"e-milestone-checkpoint-for-architecture-implementation\">E. Milestone Checkpoint for Architecture Implementation</h4>\n<p>After setting up the project structure and configuration:</p>\n<ol>\n<li><strong>Run the configuration test</strong> to verify everything loads:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   mvn test -Dtest=ConfigLoaderTest</code></pre></div>\n<p>   Expected: All tests pass, showing configuration loads from YAML correctly.</p>\n<ol start=\"2\">\n<li><strong>Verify the pipeline skeleton compiles</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   mvn clean compile</code></pre></div>\n<p>   Expected: Build succeeds with no compilation errors.</p>\n<ol start=\"3\">\n<li><strong>Check the component wiring</strong> by creating a simple main class:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> Main</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">       public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#FFAB70\">args</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           AppConfig config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ConfigLoader.</span><span style=\"color:#B392F0\">loadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"config/application.yaml\"</span><span style=\"color:#E1E4E8\">, AppConfig.class);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           CdcPipeline pipeline </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> CdcPipeline</span><span style=\"color:#E1E4E8\">(config);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           pipeline.</span><span style=\"color:#B392F0\">initialize</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">           System.out.</span><span style=\"color:#B392F0\">println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Pipeline initialized successfully!\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   }</span></span></code></pre></div>\n<p>   Expected: Program runs and prints success message (components will be stubs initially).</p>\n<p><strong>Signs of Trouble:</strong></p>\n<ul>\n<li><code>ClassNotFoundException</code> for Jackson YAML: Add <code>jackson-dataformat-yaml</code> dependency.</li>\n<li>Configuration file not found: Ensure <code>config/application.yaml</code> is in your classpath or current directory.</li>\n<li>Null pointer in pipeline: You&#39;re missing component initialization—implement the TODOs in <code>CdcPipeline.initialize()</code>.</li>\n</ul>\n<hr>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (foundational), Milestone 2 (delivery), Milestone 3 (schema)</p>\n</blockquote>\n<h2 id=\"4-data-model\">4. Data Model</h2>\n<p>This section defines the fundamental data structures that constitute the <strong>vocabulary</strong> of our CDC system. Just as a postal system needs standardized envelopes, addresses, and tracking numbers to function reliably, our CDC pipeline requires precisely defined data types to ensure consistent communication between components. These types represent the contract that governs how change information is captured, transformed, and delivered throughout the system.</p>\n<p><img src=\"/api/project/cdc-system/architecture-doc/asset?path=diagrams%2Fdata-model-class.svg\" alt=\"Data Model Class Diagram\"></p>\n<p>The diagram above illustrates the relationships between our core data types. Think of <code>RawLogEntry</code> as the <strong>raw material</strong> extracted from the database&#39;s transaction log, <code>ChangeEvent</code> as the <strong>finished product</strong> packaged for delivery to consumers, and <code>SchemaVersion</code> as the <strong>blueprint</strong> that defines how to interpret the data inside each event. The configuration types (<code>AppConfig</code>, <code>DatabaseConfig</code>, etc.) serve as the <strong>operating instructions</strong> that tell each component how to behave in a specific deployment environment.</p>\n<h3 id=\"41-core-type-definitions\">4.1 Core Type Definitions</h3>\n<p>This subsection details the exact structure of each data type that flows through the CDC pipeline. Each type serves a distinct purpose in the transformation journey from low-level database log entries to consumable change notifications.</p>\n<h4 id=\"the-foundation-rawlogentry\">The Foundation: RawLogEntry</h4>\n<p>Think of a <code>RawLogEntry</code> as a <strong>verbatim transcript of a database&#39;s internal diary</strong>. When the database records a change in its Write-Ahead Log (WAL) or binlog, it writes in a compact, database-specific binary format optimized for crash recovery, not for external consumption. The <code>RawLogEntry</code> is our first translation of that raw binary into a structured, but still database-specific, representation. It contains all the raw data we need, but hasn&#39;t yet been normalized into our system&#39;s universal format.</p>\n<p>The <code>RawLogEntry</code> is the output of the Log Connector &amp; Parser (Milestone 1) and serves as the input to the Change Event Builder. Its fields capture the essential facts from a single log entry:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>logSequenceNumber</code></td>\n<td>String</td>\n<td>The <strong>unique pointer</strong> to this exact position in the transaction log. For PostgreSQL, this is a Log Sequence Number (LSN) like <code>0/18A3C58</code>. For MySQL, it&#39;s a binlog position and file name. This is the critical bookmark that allows the system to resume processing after a restart without missing or duplicating events.</td>\n</tr>\n<tr>\n<td><code>databaseType</code></td>\n<td>String</td>\n<td>The <strong>source database flavor</strong> (e.g., <code>&quot;POSTGRESQL&quot;</code>, <code>&quot;MYSQL&quot;</code>). This is necessary because the interpretation of the <code>rowData</code> map and the exact semantics of the <code>logSequenceNumber</code> can vary between databases. The Event Builder uses this to apply any database-specific normalization logic.</td>\n</tr>\n<tr>\n<td><code>tableName</code></td>\n<td>String</td>\n<td>The fully-qualified name of the database table that was modified (e.g., <code>&quot;public.users&quot;</code> or <code>&quot;inventory.products&quot;</code>). This identifies the target of the change.</td>\n</tr>\n<tr>\n<td><code>operationType</code></td>\n<td>String</td>\n<td>The <strong>type of database operation</strong> that occurred. Must be one of the constants: <code>OPERATION_INSERT</code> (&quot;INSERT&quot;), <code>OPERATION_UPDATE</code> (&quot;UPDATE&quot;), or <code>OPERATION_DELETE</code> (&quot;DELETE&quot;). This tells the Event Builder how to populate the <code>beforeImage</code> and <code>afterImage</code> in the final <code>ChangeEvent</code>.</td>\n</tr>\n<tr>\n<td><code>rowData</code></td>\n<td>Map&lt;String, Object&gt;</td>\n<td>The <strong>raw column values</strong> as read directly from the log. This is a map where keys are column names and values are the database-native Java objects (e.g., <code>String</code>, <code>Long</code>, <code>java.sql.Timestamp</code>, <code>byte[]</code>). For an INSERT, this map contains the new row values. For a DELETE, it contains the old row values. For an UPDATE, the contents depend on the database&#39;s log format—it might contain only the new values, only the changed values, or both. The Event Builder&#39;s job is to reconcile this into consistent before/after images.</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>Long</td>\n<td>The <strong>database server&#39;s timestamp</strong> (in milliseconds since epoch) when this log entry was written. This is the closest proxy we have to when the change <em>actually occurred</em> at the source, which is crucial for maintaining causal ordering and for consumer applications that care about event time.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The <code>RawLogEntry</code> is intentionally <em>database-aware</em>. It doesn&#39;t try to abstract away database-specific details immediately. This separation of concerns allows the Log Parser to focus solely on the complex task of interpreting native log formats, while the Event Builder handles the normalization. If we later add support for Oracle or SQL Server, we would create new parser implementations that all produce this same <code>RawLogEntry</code> type, keeping the rest of the pipeline unchanged.</p>\n</blockquote>\n<h4 id=\"the-deliverable-changeevent\">The Deliverable: ChangeEvent</h4>\n<p>A <code>ChangeEvent</code> is the <strong>canonical, packaged notification of a single row change</strong> that our system guarantees to deliver to consumers. If <code>RawLogEntry</code> is a raw ingredient, <code>ChangeEvent</code> is the plated meal ready for service. It is the output of the Change Event Builder and the input to the Event Streamer (Milestone 2).</p>\n<p>This type embodies the core value proposition of CDC: a reliable, ordered, self-contained record of &quot;what changed.&quot; Every field serves a specific purpose for downstream consumers:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>eventId</code></td>\n<td>String</td>\n<td>A <strong>globally unique identifier</strong> for this specific change event, typically a UUID (e.g., <code>&quot;123e4567-e89b-12d3-a456-426614174000&quot;</code>). This is critical for idempotent processing by consumers. If a consumer receives the same event twice (due to at-least-once delivery), it can use this ID to deduplicate.</td>\n</tr>\n<tr>\n<td><code>sourceTable</code></td>\n<td>String</td>\n<td>The fully-qualified table name (e.g., <code>&quot;public.orders&quot;</code>). Carried over from <code>RawLogEntry</code> but now in a normalized form (e.g., lowercased).</td>\n</tr>\n<tr>\n<td><code>operationType</code></td>\n<td>String</td>\n<td>The type of operation (<code>OPERATION_INSERT</code>, <code>OPERATION_UPDATE</code>, <code>OPERATION_DELETE</code>).</td>\n</tr>\n<tr>\n<td><code>beforeImage</code></td>\n<td>Map&lt;String, Object&gt;</td>\n<td>A <strong>snapshot of the entire row <em>before</em> the change</strong>. For an INSERT, this is <code>null</code> (or an empty map). For a DELETE, this contains all column values of the deleted row. For an UPDATE, this contains the complete row state prior to the update. This &quot;full before image&quot; is essential for many use cases like audit trails, undo operations, or synchronizing to systems that need to know what was removed.</td>\n</tr>\n<tr>\n<td><code>afterImage</code></td>\n<td>Map&lt;String, Object&gt;</td>\n<td>A <strong>snapshot of the entire row <em>after</em> the change</strong>. For an INSERT, this is the new row. For a DELETE, this is <code>null</code> (or an empty map). For an UPDATE, this contains the complete row state after the update. Together with <code>beforeImage</code>, this gives consumers a complete picture of the change.</td>\n</tr>\n<tr>\n<td><code>schemaVersionId</code></td>\n<td>String</td>\n<td>A <strong>reference to the exact schema version</strong> that defines the structure of <code>beforeImage</code> and <code>afterImage</code>. This is typically a compound key like <code>&quot;public.orders:v2&quot;</code>. Consumers use this ID to fetch the correct <code>SchemaVersion</code> from the Schema Registry (Milestone 3) to properly deserialize and interpret the column values. This field is what enables safe schema evolution.</td>\n</tr>\n<tr>\n<td><code>transactionId</code></td>\n<td>String</td>\n<td>The <strong>database transaction identifier</strong> that grouped this change with others. In PostgreSQL, this might be the XID; in MySQL, it might be the GTID or a logical grouping ID. This allows consumers to reason about atomicity—they know which changes were committed together, which is important for maintaining data consistency in the target system.</td>\n</tr>\n<tr>\n<td><code>commitTimestamp</code></td>\n<td>Long</td>\n<td>The <strong>exact moment the transaction was committed</strong> on the source database (milliseconds since epoch). This is more authoritative than the log write timestamp in <code>RawLogEntry</code> because it reflects the point at which the change became visible to other database sessions. It&#39;s the definitive &quot;event time&quot; for the change.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Behavior:</strong> For UPDATE operations, our system guarantees to provide <strong>both full before and after images</strong>. This is a deliberate design choice that simplifies consumer logic at the cost of slightly larger message sizes. Some CDC systems only send the changed columns (deltas), but that forces consumers to maintain state to reconstruct full rows. Our approach favors completeness and stateless consumption.</p>\n</blockquote>\n<h4 id=\"the-contract-schemaversion\">The Contract: SchemaVersion</h4>\n<p>The <code>SchemaVersion</code> acts as the <strong>evolving dictionary</strong> that defines how to read the data within a <code>ChangeEvent</code>. Imagine you&#39;re receiving a document written in a language that occasionally adds new words or changes grammar rules. The <code>SchemaVersion</code> is the versioned dictionary you consult to understand each document correctly. It is stored and managed by the Schema Registry component (Milestone 3).</p>\n<p>Each version captures the exact column structure of a table at a point in time:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>schemaId</code></td>\n<td>String</td>\n<td>A <strong>unique, stable identifier</strong> for this specific schema version. Usually follows a pattern like <code>&quot;{tableName}:v{version}&quot;</code> (e.g., <code>&quot;public.users:v3&quot;</code>). This is the value that appears in the <code>ChangeEvent.schemaVersionId</code> field.</td>\n</tr>\n<tr>\n<td><code>tableName</code></td>\n<td>String</td>\n<td>The fully-qualified table name this schema defines.</td>\n</tr>\n<tr>\n<td><code>version</code></td>\n<td>Integer</td>\n<td>A monotonically increasing integer (starting at 1) representing the version of the schema for this table. Increments each time a backward-compatible change is made.</td>\n</tr>\n<tr>\n<td><code>columnDefinitions</code></td>\n<td>Map&lt;String, ColumnType&gt;</td>\n<td>The <strong>complete column specification</strong>. The key is the column name; the value is a <code>ColumnType</code> object (a custom type we define) that includes the Java/SQL type (e.g., <code>VARCHAR</code>, <code>BIGINT</code>), nullability, default value, and any type-specific parameters (like length for strings, precision for decimals). This map defines the contract for the keys and value types in the <code>ChangeEvent</code>&#39;s <code>beforeImage</code> and <code>afterImage</code> maps.</td>\n</tr>\n<tr>\n<td><code>compatibilityMode</code></td>\n<td>String</td>\n<td>The <strong>compatibility rule</strong> that was in effect when this schema version was registered. Common values: <code>&quot;BACKWARD&quot;</code> (default—new schema can read data written by old schema), <code>&quot;FORWARD&quot;</code> (old schema can read data written by new schema), <code>&quot;FULL&quot;</code> (both). This is stored for audit and to understand the evolution constraints applied.</td>\n</tr>\n</tbody></table>\n<p><strong>About ColumnType:</strong> While not in the explicit naming conventions, <code>ColumnType</code> is a necessary auxiliary type we must define. It&#39;s a simple value object with fields like <code>sqlType</code> (String), <code>javaClass</code> (Class&lt;?&gt;), <code>nullable</code> (boolean), and <code>defaultValue</code> (Object). It allows the Schema Registry to perform precise compatibility checks (e.g., &quot;can data of type <code>BIGINT</code> be safely read as <code>INTEGER</code>?&quot;).</p>\n<h4 id=\"the-configuration-appconfig-and-sub-configs\">The Configuration: AppConfig and Sub-Configs</h4>\n<p>Configuration types are the <strong>deployment manifests</strong> that parameterize the CDC pipeline for a specific environment. They tell each component <em>where</em> to connect, <em>how</em> to behave, and <em>what</em> limits to respect. We use a hierarchical structure where <code>AppConfig</code> is the root, containing nested configurations for each major subsystem.</p>\n<p><strong>DatabaseConfig</strong> — Directs the Log Connector:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>type</code></td>\n<td>String</td>\n<td>Database type: <code>&quot;POSTGRESQL&quot;</code> or <code>&quot;MYSQL&quot;</code>. Determines which parser implementation to use.</td>\n</tr>\n<tr>\n<td><code>host</code>, <code>port</code></td>\n<td>String, int</td>\n<td>Network location of the source database.</td>\n</tr>\n<tr>\n<td><code>database</code>, <code>username</code>, <code>password</code></td>\n<td>String</td>\n<td>Credentials and target database name.</td>\n</tr>\n<tr>\n<td><code>slotName</code></td>\n<td>String</td>\n<td>(PostgreSQL-specific) The logical replication slot name. Essential for persistent WAL tracking.</td>\n</tr>\n</tbody></table>\n<p><strong>KafkaConfig</strong> — Controls the Event Streamer:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>bootstrapServers</code></td>\n<td>String</td>\n<td>Comma-separated list of Kafka broker addresses (e.g., <code>&quot;broker1:9092,broker2:9092&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>topicPrefix</code></td>\n<td>String</td>\n<td>Prefix for auto-created topics (e.g., <code>&quot;cdc.&quot;</code> results in topics like <code>cdc.public.users</code>).</td>\n</tr>\n<tr>\n<td><code>replicationFactor</code>, <code>partitionsPerTable</code></td>\n<td>int</td>\n<td>Topic creation parameters controlling durability and parallelism.</td>\n</tr>\n<tr>\n<td><code>acks</code></td>\n<td>String</td>\n<td>Producer acknowledgment setting: <code>&quot;all&quot;</code> for strongest durability (required for at-least-once).</td>\n</tr>\n<tr>\n<td><code>enableIdempotence</code></td>\n<td>boolean</td>\n<td>Must be <code>true</code> to prevent duplicate message production on retries.</td>\n</tr>\n</tbody></table>\n<p><strong>SchemaConfig</strong> — Guides Schema Registry integration:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>registryType</code></td>\n<td>String</td>\n<td>Registry implementation: <code>&quot;MEMORY&quot;</code> for simple testing, <code>&quot;KAFKA&quot;</code> for Confluent Schema Registry, or <code>&quot;CUSTOM&quot;</code>.</td>\n</tr>\n<tr>\n<td><code>registryUrl</code></td>\n<td>String</td>\n<td>The HTTP endpoint of the schema registry service.</td>\n</tr>\n</tbody></table>\n<p><strong>AppConfig</strong> — The root container:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>database</code></td>\n<td>DatabaseConfig</td>\n<td>Database connection parameters.</td>\n</tr>\n<tr>\n<td><code>kafka</code></td>\n<td>KafkaConfig</td>\n<td>Message broker parameters.</td>\n</tr>\n<tr>\n<td><code>schema</code></td>\n<td>SchemaConfig</td>\n<td>Schema registry parameters.</td>\n</tr>\n<tr>\n<td><code>maxBatchSize</code></td>\n<td>int</td>\n<td>Maximum number of <code>ChangeEvent</code>s to batch before publishing to Kafka. Balances latency and throughput.</td>\n</tr>\n<tr>\n<td><code>backpressureThresholdMs</code></td>\n<td>long</td>\n<td>If consumer lag exceeds this many milliseconds, the pipeline will pause reading from the database log to apply backpressure.</td>\n</tr>\n</tbody></table>\n<h4 id=\"the-health-indicator-healthstatus-enum\">The Health Indicator: HealthStatus Enum</h4>\n<p>This simple enum provides a <strong>standardized status signal</strong> for the entire pipeline, used by monitoring systems and orchestration tools (like Kubernetes).</p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HEALTHY</code></td>\n<td>All components are operating normally, connected to their respective sources (database, Kafka), and processing events.</td>\n</tr>\n<tr>\n<td><code>DEGRADED</code></td>\n<td>The pipeline is running but experiencing issues—for example, the database connection is intermittent, consumer lag is above threshold, or the schema registry is unreachable. Events may still be flowing, but the system requires attention.</td>\n</tr>\n<tr>\n<td><code>STOPPED</code></td>\n<td>The pipeline is not processing events, either due to a fatal error, a manual stop, or a failure to initialize.</td>\n</tr>\n</tbody></table>\n<h3 id=\"42-serialization-format-for-events\">4.2 Serialization Format for Events</h3>\n<p>Serialization is the process of converting our in-memory <code>ChangeEvent</code> objects into a byte stream for transmission over the network (to Kafka) and for persistent storage. The choice of serialization format is a critical architectural decision with profound implications for performance, compatibility, and system evolution.</p>\n<blockquote>\n<p><strong>Mental Model:</strong> Think of serialization as the <strong>packaging material</strong> for your shipped product. You could use bulky, human-readable cardboard (JSON), compact and efficient but rigid plastic (Java Serialization), or a smart, self-describing vacuum-sealed bag (Avro/Protobuf). The right packaging protects the contents, minimizes space, and includes a label that explains what&#39;s inside even if the product design changes slightly.</p>\n</blockquote>\n<h4 id=\"adr-choosing-a-serialization-format-for-change-events\">ADR: Choosing a Serialization Format for Change Events</h4>\n<p><strong>Decision: Use Apache Avro as the primary serialization format for <code>ChangeEvent</code>s, with JSON as a fallback for debugging and simple deployments.</strong></p>\n<ul>\n<li><p><strong>Context:</strong> <code>ChangeEvent</code>s must be serialized for publishing to Kafka and for consumers to read. The format must support high throughput (low CPU/memory overhead), compact representation (to reduce network and storage costs), and most importantly, <strong>robust schema evolution</strong> to handle database schema changes without breaking existing consumers.</p>\n</li>\n<li><p><strong>Options Considered:</strong></p>\n<ol>\n<li><strong>JSON (Plain Text):</strong> Human-readable, language-agnostic, and simple to implement. No schema required for deserialization.</li>\n<li><strong>Java Serialization:</strong> Built into Java, but produces bulky output, is Java-specific, and handles schema evolution poorly.</li>\n<li><strong>Protocol Buffers (Protobuf):</strong> Binary, efficient, and supports backward/forward compatibility through field numbers. Requires <code>.proto</code> schema definitions.</li>\n<li><strong>Apache Avro:</strong> Binary, highly efficient, uses JSON for schema definition, and has excellent schema evolution capabilities with a centralized <strong>Schema Registry</strong> pattern. Supports &quot;schema-less&quot; reading if the reader&#39;s schema is provided.</li>\n</ol>\n</li>\n<li><p><strong>Decision Rationale:</strong>\nAvro is chosen because its design aligns perfectly with the requirements of a CDC system. Its <strong>schema evolution rules</strong> are well-defined and match the compatibility modes (BACKWARD, FORWARD, FULL) we need for Milestone 3. The integration with a Schema Registry (where schemas are stored by ID) means the serialized data is incredibly compact—it only includes the data bytes, not field names or metadata. Consumers fetch the schema from the registry using the <code>schemaVersionId</code> included in the message (or in message headers). This enables:</p>\n<ul>\n<li><strong>Backward Compatibility:</strong> Adding a nullable column to a table generates a new Avro schema. Old consumers (with the old schema) can still read new events because Avro ignores the unknown new field.</li>\n<li><strong>Forward Compatibility:</strong> Removing a column is trickier, but if done with a default value in the schema, new consumers can read old data.</li>\n<li><strong>Efficiency:</strong> Avro binary encoding is significantly smaller and faster to parse than JSON.</li>\n<li><strong>Ecosystem Integration:</strong> Kafka&#39;s Confluent Platform has first-class Avro and Schema Registry support, which simplifies operational tooling.</li>\n</ul>\n</li>\n<li><p><strong>Consequences:</strong></p>\n<ul>\n<li>We must integrate and operate a Schema Registry service (an additional component).</li>\n<li>Consumers must use Avro deserializers and have network access to the registry (or cache schemas locally).</li>\n<li>Debugging requires tooling to decode Avro binaries (though we can also produce JSON for debugging topics).</li>\n</ul>\n</li>\n</ul>\n<p><strong>Comparison of Serialization Options:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>JSON</strong></td>\n<td>Human-readable, universal support, no schema needed, easy debugging.</td>\n<td>Verbose (high overhead), no built-in schema evolution support, slower parsing.</td>\n<td>The overhead and lack of robust schema evolution are disqualifying for a high-volume CDC system.</td>\n</tr>\n<tr>\n<td><strong>Java Serialization</strong></td>\n<td>Built-in, requires no configuration.</td>\n<td>Java-only, extremely verbose, fragile to class changes, terrible performance.</td>\n<td>Not cross-platform and performs poorly.</td>\n</tr>\n<tr>\n<td><strong>Protocol Buffers</strong></td>\n<td>Efficient, robust cross-language support, good backward compatibility.</td>\n<td>Forward compatibility less straightforward than Avro, requires code generation, <code>.proto</code> files must be distributed.</td>\n<td>Slightly less ideal for the &quot;schema registry&quot; pattern where the schema is fetched at runtime. Avro&#39;s dynamic deserialization fits better.</td>\n</tr>\n<tr>\n<td><strong>Apache Avro</strong></td>\n<td>Excellent schema evolution, very efficient, designed for Schema Registry pattern, dynamic deserialization possible.</td>\n<td>Requires a Schema Registry, slightly steeper learning curve.</td>\n<td><strong>CHOSEN:</strong> Best aligns with our need for schema evolution and operational integration with Kafka.</td>\n</tr>\n</tbody></table>\n<h4 id=\"serialized-message-structure\">Serialized Message Structure</h4>\n<p>When a <code>ChangeEvent</code> is published to Kafka, it is wrapped in a <strong>envelope structure</strong> that includes metadata alongside the actual Avro-encoded payload. This envelope is what the Kafka producer sends and the consumer receives.</p>\n<p>The following table describes the structure of the Kafka message value (the main payload):</p>\n<table>\n<thead>\n<tr>\n<th>Field in Serialized Form</th>\n<th>Avro Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>schemaId</code></td>\n<td>string</td>\n<td>The <code>schemaVersionId</code> (e.g., <code>&quot;public.orders:v2&quot;</code>). This may alternatively be placed in the Kafka message <em>headers</em> (as <code>&quot;schema.id&quot;</code>) for even more efficient consumer deserialization, where the deserializer fetches the schema by this ID.</td>\n</tr>\n<tr>\n<td><code>eventId</code></td>\n<td>string</td>\n<td>UUID of the event, for deduplication.</td>\n</tr>\n<tr>\n<td><code>sourceTable</code></td>\n<td>string</td>\n<td>Source table name.</td>\n</tr>\n<tr>\n<td><code>operationType</code></td>\n<td>string</td>\n<td>&quot;INSERT&quot;, &quot;UPDATE&quot;, &quot;DELETE&quot;.</td>\n</tr>\n<tr>\n<td><code>beforeImage</code></td>\n<td>map&lt;string, bytes&gt;</td>\n<td><strong>Avro-encoded map.</strong> The keys are column names. Each value is the <strong>Avro-encoded bytes</strong> of that column&#39;s value, according to the column&#39;s type in the schema. This nested encoding allows each field to be typed precisely. Alternatively, the entire <code>beforeImage</code> could be a single Avro record; the map approach is more flexible for sparse updates (though we always send full images).</td>\n</tr>\n<tr>\n<td><code>afterImage</code></td>\n<td>map&lt;string, bytes&gt;</td>\n<td>Same as <code>beforeImage</code>, for the after state.</td>\n</tr>\n<tr>\n<td><code>transactionId</code></td>\n<td>string</td>\n<td>Database transaction identifier.</td>\n</tr>\n<tr>\n<td><code>commitTimestamp</code></td>\n<td>long</td>\n<td>Commit timestamp in milliseconds.</td>\n</tr>\n</tbody></table>\n<p><strong>Alternative: Embedded Schema Approach:</strong> For simplicity in early milestones or testing, we can use a <strong>JSON serialization with an embedded schema version</strong>. The message would be a JSON object containing all <code>ChangeEvent</code> fields, with <code>beforeImage</code> and <code>afterImage</code> as JSON objects. The <code>schemaVersionId</code> would be a field within the JSON. This is less efficient but removes the Schema Registry dependency for initial development.</p>\n<p>We will support both modes via configuration: an <code>AvroWithRegistry</code> mode and a <code>JsonEmbedded</code> mode. The <code>JsonEmbedded</code> mode is useful for Milestone 1 and 2 completion before implementing the full Schema Registry in Milestone 3.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (for Milestone 1/2)</th>\n<th>Advanced Option (for Milestone 3 &amp; Production)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Serialization</strong></td>\n<td>JSON using Jackson library. Embed <code>schemaVersionId</code> in the JSON structure.</td>\n<td>Apache Avro with Confluent Schema Registry. Use <code>KafkaAvroSerializer</code> and <code>KafkaAvroDeserializer</code>.</td>\n</tr>\n<tr>\n<td><strong>Schema Registry</strong></td>\n<td>In-memory map storing <code>SchemaVersion</code> objects. No persistence.</td>\n<td>Confluent Schema Registry (HTTP service) or Apicurio Registry. Provides persistence, REST API, and compatibility enforcement.</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>YAML file parsed with Jackson <code>ObjectMapper</code>.</td>\n<td>YAML file with environment variable overrides (using <code>${ENV_VAR}</code> substitution) and validation with Java Bean Validation.</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-file-structure\">B. Recommended File Structure</h4>\n<p>Place data model types in a dedicated package to keep them organized and separate from component logic.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── config/\n│   │   ├── AppConfig.java          ← Root configuration\n│   │   ├── DatabaseConfig.java\n│   │   ├── KafkaConfig.java\n│   │   ├── SchemaConfig.java\n│   │   └── ConfigLoader.java       ← loadConfig method\n│   ├── model/                      ← Core data types\n│   │   ├── RawLogEntry.java\n│   │   ├── ChangeEvent.java\n│   │   ├── SchemaVersion.java\n│   │   ├── ColumnType.java         ← Auxiliary type\n│   │   └── HealthStatus.java       ← Enum\n│   ├── pipeline/\n│   │   └── CdcPipeline.java        ← Main orchestrator class\n│   └── Main.java                   ← main method\n├── config/\n│   └── cdc-config.yaml             ← YAML configuration file\n└── pom.xml or build.gradle</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p>Here is a complete, ready-to-use implementation of the data model types and configuration loader. This is foundational code that learners can use directly.</p>\n<p><strong>ColumnType.java (auxiliary type for SchemaVersion):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.model;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Objects;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Represents the type and constraints of a single database column.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Used within SchemaVersion.columnDefinitions.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ColumnType</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String sqlType;        </span><span style=\"color:#6A737D\">// e.g., \"VARCHAR\", \"BIGINT\", \"TIMESTAMP\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Class&#x3C;</span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\">> javaClass;    </span><span style=\"color:#6A737D\">// e.g., String.class, Long.class</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#E1E4E8\"> nullable;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Object defaultValue;   </span><span style=\"color:#6A737D\">// Can be null</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> ColumnType</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">sqlType</span><span style=\"color:#E1E4E8\">, Class&#x3C;</span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">javaClass</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">boolean</span><span style=\"color:#FFAB70\"> nullable</span><span style=\"color:#E1E4E8\">, Object </span><span style=\"color:#FFAB70\">defaultValue</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.sqlType </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sqlType;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.javaClass </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> javaClass;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.nullable </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nullable;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.defaultValue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultValue;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters, equals, hashCode, toString omitted for brevity but should be implemented.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>RawLogEntry.java:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.model;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Map;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.HashMap;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Represents a single parsed entry from a database transaction log.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * This is a raw, database-specific representation before normalization.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> RawLogEntry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String logSequenceNumber;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String databaseType;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String tableName;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String operationType;  </span><span style=\"color:#6A737D\">// Use constants OPERATION_INSERT, etc.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> rowData;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> timestamp;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> RawLogEntry</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">logSequenceNumber</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">databaseType</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       String </span><span style=\"color:#FFAB70\">operationType</span><span style=\"color:#E1E4E8\">, Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">rowData</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">long</span><span style=\"color:#FFAB70\"> timestamp</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.logSequenceNumber </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logSequenceNumber;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.databaseType </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> databaseType;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.tableName </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tableName;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.operationType </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operationType;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.rowData </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>(rowData); </span><span style=\"color:#6A737D\">// Defensive copy</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> timestamp;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters, equals, hashCode, toString omitted for brevity.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>ChangeEvent.java:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.model;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Map;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.HashMap;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * The canonical representation of a single row change, ready for delivery to consumers.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ChangeEvent</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String eventId;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String sourceTable;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String operationType;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> beforeImage;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> afterImage;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String schemaVersionId;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String transactionId;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> commitTimestamp;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> ChangeEvent</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">eventId</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">sourceTable</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">operationType</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">beforeImage</span><span style=\"color:#E1E4E8\">, Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">afterImage</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       String </span><span style=\"color:#FFAB70\">schemaVersionId</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">transactionId</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">long</span><span style=\"color:#FFAB70\"> commitTimestamp</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.eventId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> eventId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.sourceTable </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sourceTable;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.operationType </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operationType;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.beforeImage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> beforeImage </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> ?</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> :</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>(beforeImage);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.afterImage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> afterImage </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> ?</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> :</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>(afterImage);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.schemaVersionId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schemaVersionId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.transactionId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> transactionId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.commitTimestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> commitTimestamp;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters, equals, hashCode, toString omitted.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>SchemaVersion.java:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.model;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Map;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.HashMap;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Represents a versioned schema for a database table.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> SchemaVersion</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String schemaId;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String tableName;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> version;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ColumnType</span><span style=\"color:#E1E4E8\">> columnDefinitions;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String compatibilityMode;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> SchemaVersion</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">schemaId</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> version</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ColumnType</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">columnDefinitions</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">compatibilityMode</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.schemaId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schemaId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.tableName </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tableName;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.version </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> version;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.columnDefinitions </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>(columnDefinitions);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.compatibilityMode </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> compatibilityMode;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters, equals, hashCode, toString omitted.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Configuration Classes (AppConfig.java as example):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.config;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.annotation.JsonProperty;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> AppConfig</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> DatabaseConfig database;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> KafkaConfig kafka;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> SchemaConfig schema;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> maxBatchSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> backpressureThresholdMs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5000</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // No-arg constructor for Jackson</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> AppConfig</span><span style=\"color:#E1E4E8\">() {}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters and setters for each field (required for Jackson binding).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> DatabaseConfig </span><span style=\"color:#B392F0\">getDatabase</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> setDatabase</span><span style=\"color:#E1E4E8\">(DatabaseConfig </span><span style=\"color:#FFAB70\">database</span><span style=\"color:#E1E4E8\">) { </span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">.database </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> database; }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... same for other fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>ConfigLoader.java (complete utility):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.config;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.databind.ObjectMapper;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.dataformat.yaml.YAMLFactory;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.File;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.IOException;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ConfigLoader</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ObjectMapper MAPPER </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ObjectMapper</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">new</span><span style=\"color:#B392F0\"> YAMLFactory</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Loads configuration from a YAML file.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> configPath</span><span style=\"color:#6A737D\"> Path to the YAML file.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> clazz</span><span style=\"color:#6A737D\"> The configuration class (e.g., AppConfig.class).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@return</span><span style=\"color:#6A737D\"> The populated configuration object.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@throws</span><span style=\"color:#B392F0\"> IOException</span><span style=\"color:#6A737D\"> If the file cannot be read or parsed.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> &#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> T </span><span style=\"color:#B392F0\">loadConfig</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#E1E4E8\">, Class&#x3C;</span><span style=\"color:#F97583\">T</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">clazz</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> IOException {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        File file </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> File</span><span style=\"color:#E1E4E8\">(configPath);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">file.</span><span style=\"color:#B392F0\">exists</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> IOException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Configuration file not found: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> configPath);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> MAPPER.</span><span style=\"color:#B392F0\">readValue</span><span style=\"color:#E1E4E8\">(file, clazz);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p>For the <strong>serialization logic</strong> that learners will implement in later milestones, here is a skeleton interface. The actual Avro/JSON implementation will be filled in during the Event Streamer and Schema Registry components.</p>\n<p><strong>EventSerializer.java (interface to be implemented):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.serialization;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.model.ChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.model.SchemaVersion;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Defines how to serialize and deserialize ChangeEvent objects.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#B392F0\"> EventSerializer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Serializes a ChangeEvent into bytes for transmission.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> event</span><span style=\"color:#6A737D\"> The event to serialize.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> schema</span><span style=\"color:#6A737D\"> The SchemaVersion that describes the event's structure.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@return</span><span style=\"color:#6A737D\"> Byte array representing the serialized event.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    byte</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#B392F0\">serialize</span><span style=\"color:#E1E4E8\">(ChangeEvent </span><span style=\"color:#FFAB70\">event</span><span style=\"color:#E1E4E8\">, SchemaVersion </span><span style=\"color:#FFAB70\">schema</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Deserializes bytes back into a ChangeEvent.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> data</span><span style=\"color:#6A737D\"> The serialized byte array.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> schema</span><span style=\"color:#6A737D\"> The SchemaVersion to use for deserialization.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@return</span><span style=\"color:#6A737D\"> The reconstructed ChangeEvent.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChangeEvent </span><span style=\"color:#B392F0\">deserialize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\">, SchemaVersion </span><span style=\"color:#FFAB70\">schema</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>JsonEventSerializer.java (simple implementation for early milestones):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.serialization;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.model.ChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.model.SchemaVersion;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.databind.ObjectMapper;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.IOException;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * A simple JSON serializer for testing and early development.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * WARNING: This does not handle schema evolution properly.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> JsonEventSerializer</span><span style=\"color:#F97583\"> implements</span><span style=\"color:#B392F0\"> EventSerializer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ObjectMapper objectMapper </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ObjectMapper</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> byte</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#B392F0\">serialize</span><span style=\"color:#E1E4E8\">(ChangeEvent </span><span style=\"color:#FFAB70\">event</span><span style=\"color:#E1E4E8\">, SchemaVersion </span><span style=\"color:#FFAB70\">schema</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Convert the ChangeEvent to a JSON byte array using objectMapper.writeValueAsBytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Consider including the schemaVersionId as a field in the JSON if not already present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Return the byte array</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> new</span><span style=\"color:#F97583\"> byte</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]; </span><span style=\"color:#6A737D\">// Replace with actual implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (Exception </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> RuntimeException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to serialize ChangeEvent\"</span><span style=\"color:#E1E4E8\">, e);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> ChangeEvent </span><span style=\"color:#B392F0\">deserialize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\">, SchemaVersion </span><span style=\"color:#FFAB70\">schema</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Use objectMapper.readValue to parse the byte array into a ChangeEvent object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Validate that the event's schemaVersionId matches the provided schema's ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Return the ChangeEvent</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// Replace with actual implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (IOException </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> RuntimeException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to deserialize ChangeEvent\"</span><span style=\"color:#E1E4E8\">, e);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-java\">E. Language-Specific Hints (Java)</h4>\n<ul>\n<li>Use <strong>Jackson</strong> (<code>com.fasterxml.jackson</code>) for YAML and JSON processing. Include <code>jackson-databind</code> and <code>jackson-dataformat-yaml</code> dependencies.</li>\n<li>For <strong>Avro</strong>, use <code>org.apache.avro</code> and <code>io.confluent:kafka-avro-serializer</code> if using Confluent&#39;s Schema Registry.</li>\n<li>Always make data model classes <strong>immutable</strong> (final fields, no setters) where possible. This prevents accidental mutation as events flow through the pipeline.</li>\n<li>Implement <code>equals()</code>, <code>hashCode()</code>, and <code>toString()</code> for all model classes—this greatly aids debugging and testing.</li>\n<li>For the <code>rowData</code> and image maps, use <strong>defensive copying</strong> in constructors and getters to prevent external modification of internal state.</li>\n</ul>\n<h4 id=\"f-milestone-checkpoint\">F. Milestone Checkpoint</h4>\n<p>After implementing the data model classes and configuration loader:</p>\n<ol>\n<li><strong>Compilation:</strong> Run <code>mvn compile</code> or <code>gradle compileJava</code>. It should succeed without errors.</li>\n<li><strong>Unit Test:</strong> Write a simple test in <code>src/test/java</code> that:<ul>\n<li>Creates a <code>RawLogEntry</code> with sample data.</li>\n<li>Creates a <code>ChangeEvent</code> from that raw entry (manually, for now).</li>\n<li>Serializes the <code>ChangeEvent</code> to JSON using the <code>JsonEventSerializer</code>.</li>\n<li>Deserializes it back and asserts equality.</li>\n</ul>\n</li>\n<li><strong>Configuration Loading:</strong> Write a test that loads a sample <code>cdc-config.yaml</code> file using <code>ConfigLoader.loadConfig</code> and asserts the values are correctly populated into <code>AppConfig</code>.</li>\n<li><strong>Expected Behavior:</strong> You should be able to run these tests and see them pass. This confirms your data structures are correctly defined and can be serialized, which is foundational for all subsequent milestones.</li>\n</ol>\n<h4 id=\"g-debugging-tips\">G. Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>NullPointerException</code> when accessing map fields in <code>ChangeEvent</code>.</td>\n<td>The <code>beforeImage</code> or <code>afterImage</code> map is null for INSERT/DELETE operations, and code doesn&#39;t handle null.</td>\n<td>Check the operation type and ensure your logic handles null images appropriately.</td>\n<td>Use <code>Optional.ofNullable(event.getBeforeImage())</code> or explicit null checks before iterating.</td>\n</tr>\n<tr>\n<td>Configuration values are not loaded (remain default).</td>\n<td>YAML file field names don&#39;t match Java bean property names, or file is not in the expected path.</td>\n<td>Print the loaded <code>AppConfig</code> object. Check that YAML keys match the Java field names (case-sensitive).</td>\n<td>Ensure YAML uses <code>camelCase</code> keys matching the Java getter/setter names (e.g., <code>maxBatchSize</code>).</td>\n</tr>\n<tr>\n<td>Serialized JSON is extremely large.</td>\n<td>The <code>rowData</code> or image maps contain large binary objects (e.g., <code>byte[]</code> of a BLOB column).</td>\n<td>Inspect the JSON output; look for base64-encoded strings (Jackson&#39;s default for <code>byte[]</code>).</td>\n<td>Consider excluding large binary columns from CDC or using a binary serialization format (Avro) which is more efficient.</td>\n</tr>\n<tr>\n<td><code>ClassCastException</code> when putting values into <code>rowData</code> map.</td>\n<td>Database driver returns a database-specific type (e.g., <code>PGobject</code>) that cannot be cast to expected Java type.</td>\n<td>Log the actual class of the value from the database log parser.</td>\n<td>Convert database-specific types to standard Java types in the Log Parser (e.g., convert PostgreSQL <code>PGobject</code> to <code>String</code> for JSONB).</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Log Parsing &amp; Change Events)</p>\n</blockquote>\n<h2 id=\"5-component-log-connector-amp-parser\">5. Component: Log Connector &amp; Parser</h2>\n<p>This component is the <strong>foundational data ingestion layer</strong> of the CDC pipeline. Its sole responsibility is to establish a persistent, stateful connection to the source database&#39;s transaction log and translate raw, binary log entries into structured <code>RawLogEntry</code> objects. It is the &quot;eyes&quot; of the CDC system, continuously watching the database&#39;s internal journal of changes. Think of this component as a specialized, low-level translator that understands the database&#39;s private, binary language (the Write-Ahead Log or binlog) and converts it into a standardized internal dialect that the rest of the system can understand.</p>\n<h3 id=\"51-mental-model-the-database-translator\">5.1 Mental Model: The Database Translator</h3>\n<p>Imagine you&#39;re tasked with translating a live, never-ending speech given in a complex, technical dialect (PostgreSQL&#39;s WAL or MySQL&#39;s binlog) into plain English for a group of reporters (downstream components). The speech is delivered at high speed, sometimes with interruptions and corrections, and you must keep your place meticulously. You have two main tools: a <strong>dictionary</strong> (the parser&#39;s logic) to translate each word and sentence structure, and a <strong>bookmark</strong> (the log sequence number) to note exactly where you stopped reading, so you can resume seamlessly after a coffee break (a system restart). This translator must be fluent in multiple dialects (different database engines) but maintain the same output format.</p>\n<p>More technically, the component acts as a <strong>stateful, pull-based iterator</strong> over the database&#39;s logical replication stream. It does not interpret transaction boundaries or deduplicate entries—it simply provides a reliable stream of &quot;raw facts&quot; about individual row-level changes as they are durably recorded by the database engine itself. Its core challenge is dealing with the <strong>opaque, vendor-specific, and version-dependent</strong> binary formats while maintaining a persistent read position that survives crashes.</p>\n<h3 id=\"52-interface-and-state\">5.2 Interface and State</h3>\n<p>The Log Connector &amp; Parser exposes a simple, iterator-like interface for the downstream Change Event Builder to consume raw log entries. It manages significant internal state to track its position and connection health.</p>\n<h4 id=\"interface\">Interface</h4>\n<p>The primary interaction is through a <code>LogConnector</code> interface that provides methods for lifecycle management and data retrieval.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>initialize</code></td>\n<td><code>AppConfig</code></td>\n<td><code>void</code></td>\n<td>Prepares the connector using application configuration. Establishes the initial connection or replication slot and sets the starting LSN from persisted state.</td>\n</tr>\n<tr>\n<td><code>start</code></td>\n<td>—</td>\n<td><code>void</code></td>\n<td>Begins the continuous reading and parsing loop. This is typically a blocking or long-running operation that publishes parsed <code>RawLogEntry</code> objects to an internal buffer or queue.</td>\n</tr>\n<tr>\n<td><code>stop</code></td>\n<td>—</td>\n<td><code>void</code></td>\n<td>Gracefully stops the reading loop, closes the database connection, and ensures the last processed LSN is safely persisted.</td>\n</tr>\n<tr>\n<td><code>getNextBatch</code></td>\n<td><code>int maxBatchSize</code></td>\n<td><code>List&lt;RawLogEntry&gt;</code></td>\n<td><strong>Primary consumption method.</strong> Fetches the next batch of parsed log entries from an internal buffer. Blocks if the buffer is empty and the connector is still running. Used by the Event Builder.</td>\n</tr>\n<tr>\n<td><code>getLastProcessedLSN</code></td>\n<td>—</td>\n<td><code>String</code></td>\n<td>Returns the most recent Log Sequence Number that has been <em>successfully parsed and made available</em> via <code>getNextBatch</code>. This is the position that would be used for recovery.</td>\n</tr>\n<tr>\n<td><code>getHealthStatus</code></td>\n<td>—</td>\n<td><code>HealthStatus</code></td>\n<td>Reports the current health: <code>HEALTHY</code> (reading), <code>DEGRADED</code> (connected but lagging, or buffer full), <code>STOPPED</code> (disconnected or error).</td>\n</tr>\n</tbody></table>\n<h4 id=\"core-data-structure-rawlogentry\">Core Data Structure: <code>RawLogEntry</code></h4>\n<p>This is the standardized output of the parser. It represents a single, atomic change to a row, extracted directly from the transaction log.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>logSequenceNumber</code></td>\n<td><code>String</code></td>\n<td><strong>The critical bookmark.</strong> The database-specific, monotonically increasing identifier for this exact position in the transaction log (e.g., PostgreSQL LSN like <code>0/15E8E10</code>, MySQL binlog position <code>471</code>). This must be persisted for recovery.</td>\n</tr>\n<tr>\n<td><code>databaseType</code></td>\n<td><code>String</code></td>\n<td>Identifies the source database engine (e.g., <code>&quot;POSTGRESQL&quot;</code>, <code>&quot;MYSQL&quot;</code>). Needed because downstream logic may handle certain edge cases differently per database.</td>\n</tr>\n<tr>\n<td><code>tableName</code></td>\n<td><code>String</code></td>\n<td>The fully-qualified name of the table that was modified (e.g., <code>&quot;public.users&quot;</code>).</td>\n</tr>\n<tr>\n<td><code>operationType</code></td>\n<td><code>String</code></td>\n<td>The type of operation. Must be one of the constants: <code>OPERATION_INSERT</code>, <code>OPERATION_UPDATE</code>, or <code>OPERATION_DELETE</code>.</td>\n</tr>\n<tr>\n<td><code>rowData</code></td>\n<td><code>Map&lt;String, Object&gt;</code></td>\n<td>A map representing the <strong>full row</strong> involved in the change. For <code>INSERT</code>, this is the new row values. For <code>DELETE</code>, this is the old row values. For <code>UPDATE</code>, this map contains the <strong>new values (after-image)</strong>. The parser extracts this directly from the log, with column names as keys and Java-object representations of the column values.</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td><code>Long</code></td>\n<td>The database server&#39;s timestamp (epoch milliseconds) when this specific change was written to the transaction log. This is often the commit time, but may be the statement execution time depending on the database.</td>\n</tr>\n</tbody></table>\n<h4 id=\"internal-state\">Internal State</h4>\n<p>The component maintains several key pieces of state, which can be visualized in the state machine diagram:\n<img src=\"/api/project/cdc-system/architecture-doc/asset?path=diagrams%2Fparser-state-machine.svg\" alt=\"Log Parser State Machine\"></p>\n<table>\n<thead>\n<tr>\n<th>State Variable</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Persistence</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>currentLSN</code></td>\n<td><code>String</code></td>\n<td>The LSN of the last log entry that was <em>read from the source</em>. This advances as we read, even before parsing is complete.</td>\n<td>Volatile (in-memory).</td>\n</tr>\n<tr>\n<td><code>lastProcessedLSN</code></td>\n<td><code>String</code></td>\n<td>The LSN of the last entry <em>successfully parsed and published</em> to the internal buffer (via <code>getNextBatch</code>). This is the recovery point.</td>\n<td><strong>Must be durably stored</strong> (e.g., in a Kafka topic <code>__cdc_offsets</code>, or a local file).</td>\n</tr>\n<tr>\n<td><code>replicationSlotName</code></td>\n<td><code>String</code></td>\n<td>(PostgreSQL-specific) The name of the logical replication slot. This ensures the database retains WAL segments until we have consumed them.</td>\n<td>Stored in configuration and in the database system catalog.</td>\n</tr>\n<tr>\n<td><code>connectionHandle</code></td>\n<td><code>Object</code> (DB-specific)</td>\n<td>The active connection or session to the database&#39;s log streaming interface (e.g., PostgreSQL&#39;s <code>PGReplicationConnection</code>, MySQL&#39;s <code>BinlogClient</code>).</td>\n<td>Volatile.</td>\n</tr>\n<tr>\n<td><code>internalBuffer</code></td>\n<td><code>BlockingQueue&lt;RawLogEntry&gt;</code></td>\n<td>A bounded queue that holds parsed entries waiting to be consumed by <code>getNextBatch</code>. Its size is a key backpressure control.</td>\n<td>Volatile.</td>\n</tr>\n<tr>\n<td><code>state</code></td>\n<td><code>Enum</code></td>\n<td>The current state of the connector (e.g., <code>IDLE</code>, <code>CONNECTING</code>, <code>READING_LOG</code>, <code>PAUSED</code>, <code>ERROR</code>).</td>\n<td>Volatile.</td>\n</tr>\n</tbody></table>\n<h3 id=\"53-internal-behavior-and-algorithm\">5.3 Internal Behavior and Algorithm</h3>\n<p>The core of the parser is a continuous loop that runs after <code>start()</code> is called. Here is the step-by-step algorithm, which is implemented in the <code>start</code> method or a dedicated background thread it launches.</p>\n<ol>\n<li><strong>Initialization &amp; Recovery</strong>: Load the <code>lastProcessedLSN</code> from the persistent offset store. This is the LSN from which to start (or resume) reading. If no offset exists (first run), start from the current database&#39;s LSN or from a specific timestamp defined in the configuration.</li>\n<li><strong>Establish Connection</strong>: Open a low-level, streaming connection to the database&#39;s logical replication interface. For PostgreSQL, this involves creating a replication connection and starting logical decoding for the chosen slot. For MySQL, it involves connecting as a replica and requesting binlog events starting from the recovered position.</li>\n<li><strong>Enter Main Loop</strong>: While the connector state is <code>READING_LOG</code>, continuously perform the following sub-steps:<ol>\n<li><strong>Read Raw Log Chunk</strong>: Blockingly read the next chunk of bytes from the database stream. The database will send data as it&#39;s written to the log. Handle network timeouts and keep-alives.</li>\n<li><strong>Parse Chunk into Logical Updates</strong>: Pass the binary chunk to a database-specific parser. This parser understands the log&#39;s format (e.g., PostgreSQL&#39;s XLog, MySQL&#39;s Binlog Event) and breaks it into a sequence of logical row change events. This step is complex and vendor-specific.</li>\n<li><strong>Iterate Over Parsed Events</strong>: For each logical row change event within the chunk:\na.  <strong>Extract Metadata</strong>: Identify the table, operation type (<code>INSERT</code>/<code>UPDATE</code>/<code>DELETE</code>), and the LSN for this specific event.\nb.  <strong>Extract Row Data</strong>: Deserialize the column values from the log&#39;s binary format into Java objects (e.g., <code>Integer</code>, <code>String</code>, <code>java.sql.Timestamp</code>). Build the <code>rowData</code> map.\nc.  <strong>Construct RawLogEntry</strong>: Create a <code>RawLogEntry</code> object with the extracted data, the current <code>databaseType</code>, and the server <code>timestamp</code>.\nd.  <strong>Update currentLSN</strong>: Set <code>currentLSN</code> to the event&#39;s LSN.\ne.  <strong>Apply Backpressure Check</strong>: Check if the <code>internalBuffer</code> has available capacity. If it&#39;s full, transition to the <code>PAUSED</code> state, which should pause reading from the database stream (often by stopping the read loop or not consuming the stream), applying backpressure at the source.\nf.  <strong>Buffer Entry</strong>: Place the constructed <code>RawLogEntry</code> into the <code>internalBuffer</code>.</li>\n<li><strong>Advance Persistent Offset</strong>: After a configured batch size or time interval, or when the buffer is flushed to consumers, update the <code>lastProcessedLSN</code> in the durable store to the <code>currentLSN</code>. This is a critical durability operation—it should happen <em>after</em> entries are in the buffer but <em>before</em> acknowledging to the database that they are consumed (if the database protocol requires it, like PostgreSQL&#39;s <code>confirmReceive</code>).</li>\n</ol>\n</li>\n<li><strong>Handle Stop Signal</strong>: When <code>stop()</code> is called, set state to <code>STOPPING</code>, flush any remaining parsed entries, perform a final update of <code>lastProcessedLSN</code>, close the database connection gracefully, and update state to <code>STOPPED</code>.</li>\n<li><strong>Error Handling</strong>: If a parse error, network failure, or database disconnection occurs, transition to the <code>ERROR</code> state. Log the error comprehensively (including the problematic LSN and raw bytes if possible). The recovery strategy (see Section 10) should attempt to reconnect from the last successfully processed LSN.</li>\n</ol>\n<blockquote>\n<p><strong>Key Design Insight:</strong> The parser&#39;s main loop is deliberately simple—it transforms binary data to structured data. It does <strong>not</strong> handle transaction grouping, deduplication, or schema matching. Those responsibilities belong to the downstream Change Event Builder. This separation of concerns keeps the parsing component focused, fast, and resilient to changes in business logic.</p>\n</blockquote>\n<h3 id=\"54-adr-database-specific-vs-generic-parser\">5.4 ADR: Database-Specific vs. Generic Parser</h3>\n<blockquote>\n<p><strong>Decision: Use a Pluggable Parser Architecture with Database-Specific Implementations</strong></p>\n</blockquote>\n<ul>\n<li><strong>Context</strong>: Our CDC system must support multiple database engines (PostgreSQL, MySQL), each with a completely different, complex, and evolving binary log format. We need to decide how to structure the parsing logic to manage this complexity while maintaining a clean codebase.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Monolithic Generic Parser</strong>: A single parser module that contains conditional logic (<code>if databaseType == POSTGRESQL</code>) to handle all supported formats.</li>\n<li><strong>Pluggable Parser Interface</strong>: Define a common <code>LogParser</code> interface (e.g., <code>parseChunk(byte[]): List&lt;RawLogEntry&gt;</code>, <code>getRequiredConfig()</code>). Implement this interface in separate, isolated modules for each database (e.g., <code>PostgresWalParser</code>, <code>MysqlBinlogParser</code>). The main <code>LogConnector</code> selects and loads the appropriate implementation at runtime based on configuration.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: We chose <strong>Option 2: Pluggable Parser Interface</strong>.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Separation of Concerns</strong>: Each database&#39;s parsing logic is isolated, making the code easier to understand, test, and maintain. Changes to MySQL&#39;s binlog format only affect the <code>MysqlBinlogParser</code> class.</li>\n<li><strong>Independent Evolution</strong>: New database support (e.g., Oracle, SQL Server) can be added by implementing a new plugin without touching any core connector code. This aligns with the Open/Closed Principle.</li>\n<li><strong>Runtime Flexibility</strong>: The correct parser is loaded based on <code>DatabaseConfig.type</code>, allowing a single binary to support multiple source databases.</li>\n<li><strong>Mitigates Complexity</strong>: The binary formats are so divergent that a generic parser would become a tangled, unmaintainable &quot;big ball of mud.&quot; A pluggable architecture forces clean abstraction boundaries.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Enables</strong>: Clean code structure, easier testing, and straightforward addition of new database vendors.</li>\n<li><strong>Introduces</strong>: Additional overhead of defining and maintaining an interface. Requires a mechanism to discover/load plugins (can be simple classpath scanning or Spring-style dependency injection).</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Monolithic Generic Parser</td>\n<td>Slightly simpler initial structure; no plugin loading logic.</td>\n<td>Code becomes highly complex and entangled; a change for one database risks breaking another; difficult to test in isolation.</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>Pluggable Parser Interface</strong></td>\n<td><strong>Clean separation; easier testing and maintenance; scalable to new databases.</strong></td>\n<td>Requires careful interface design and a plugin loading mechanism.</td>\n<td>✅</td>\n</tr>\n</tbody></table>\n<h3 id=\"55-common-pitfalls-in-log-parsing\">5.5 Common Pitfalls in Log Parsing</h3>\n<p>⚠️ <strong>Pitfall 1: Forgetting to Persist the LSN Before Acknowledging to the Database</strong></p>\n<ul>\n<li><strong>Description</strong>: In databases like PostgreSQL, after consuming WAL data, you must send an acknowledgement (via <code>confirmReceive(LSN)</code>). If you acknowledge an LSN but then crash before persisting your <code>lastProcessedLSN</code>, upon restart you will have lost track of those events. The database may discard the acknowledged WAL segments, making the data unrecoverable.</li>\n<li><strong>Why it&#39;s wrong</strong>: This causes <strong>permanent data loss</strong>. The CDC pipeline will resume from an older LSN, missing all changes between the old LSN and the acknowledged one.</li>\n<li><strong>How to fix</strong>: <strong>Always persist the <code>lastProcessedLSN</code> to durable storage <em>before</em> sending the acknowledgement to the database</strong>. Treat this as a transactional operation: write the offset, then acknowledge. Better yet, use a transactional offset store if the database supports it.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 2: Ignoring DDL Events in the Log</strong></p>\n<ul>\n<li><strong>Description</strong>: Transaction logs contain not only <code>INSERT</code>/<code>UPDATE</code>/<code>DELETE</code> (DML) but also <code>ALTER TABLE</code>, <code>DROP TABLE</code> (DDL) events. A naive parser might skip these because they don&#39;t produce a <code>RawLogEntry</code> with <code>rowData</code>.</li>\n<li><strong>Why it&#39;s wrong</strong>: Schema changes are critical. If a column is added and the parser continues to parse old-format log entries for new changes, it will fail or produce incorrect data because its internal understanding of the table layout is stale.</li>\n<li><strong>How to fix</strong>: Detect DDL events in the log stream. When one is encountered, <strong>pause the parsing stream</strong>, notify the Schema Registry component (see Section 8) to update its schema definition, and potentially invalidate the current parser&#39;s cached table metadata. Only resume parsing after the schema is updated.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 3: Not Handling Large Transactions (Huge Binlog Events)</strong></p>\n<ul>\n<li><strong>Description</strong>: A single transaction can modify millions of rows (e.g., a bulk <code>UPDATE</code>). The database may write this as a single, massive log entry or event. Loading this entire entry into memory for parsing can cause an OutOfMemoryError.</li>\n<li><strong>Why it&#39;s wrong</strong>: It crashes the CDC process, requiring manual intervention and potentially causing significant lag.</li>\n<li><strong>How to fix</strong>: Implement <strong>streaming parsing within a single log event</strong>. Process rows incrementally as they are parsed, flushing <code>RawLogEntry</code> objects to the buffer in batches. Alternatively, configure the source database to limit the size of WAL/binglog events (not always possible). Ensure the parser can handle partial reads and resume.</li>\n</ul>\n<p>⚠️ <strong>Pitfall 4: Assuming Log Format Stability</strong></p>\n<ul>\n<li><strong>Description</strong>: Writing parsing logic tightly coupled to a specific minor version of a database&#39;s log format (e.g., MySQL 8.0.25&#39;s binlog event flags).</li>\n<li><strong>Why it&#39;s wrong</strong>: Database upgrades can change the log format, breaking the parser silently (producing garbled data) or with a clear error. This creates operational fragility during maintenance.</li>\n<li><strong>How to fix</strong>: <strong>Use a well-maintained, community-vetted library for the heavy lifting of binary log parsing</strong> where possible (e.g., Debezium&#39;s embedded parsers, <code>mysql-binlog-connector-java</code>). If writing your own, clearly document the supported database versions and include version checks at connector startup. Write robust parsing logic that can fail fast with a clear error message on format mismatches.</li>\n</ul>\n<h3 id=\"56-implementation-guidance\">5.6 Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option</th>\n<th align=\"left\">Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>PostgreSQL Connector</strong></td>\n<td align=\"left\">Use the official PostgreSQL JDBC driver&#39;s <code>PGReplicationConnection</code> API for logical decoding.</td>\n<td align=\"left\">Use the <strong>Debezium PostgreSQL connector</strong> library (<code>io.debezium:debezium-connector-postgres</code>) which handles slot management, parsing, and schema history.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>MySQL Connector</strong></td>\n<td align=\"left\">Use the <code>mysql-binlog-connector-java</code> library for raw binlog streaming.</td>\n<td align=\"left\">Use the <strong>Debezium MySQL connector</strong> library (<code>io.debezium:debezium-connector-mysql</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Offset/Persistence Store</strong></td>\n<td align=\"left\">A simple file on local disk (e.g., <code>offset.properties</code>).</td>\n<td align=\"left\">A dedicated Kafka topic (<code>__cdc_offsets</code>) for centralized, fault-tolerant offset storage.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Internal Buffering</strong></td>\n<td align=\"left\"><code>java.util.concurrent.ArrayBlockingQueue&lt;RawLogEntry&gt;</code></td>\n<td align=\"left\">A reactive stream (Project Reactor, RxJava) with configurable backpressure strategies.</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── config/\n│   │   ├── AppConfig.java          # Your provided config classes\n│   │   └── ConfigLoader.java\n│   ├── connector/\n│   │   ├── LogConnector.java       # Main interface and abstract class\n│   │   ├── parser/\n│   │   │   ├── LogParser.java      # Common parser interface\n│   │   │   ├── postgres/\n│   │   │   │   ├── PostgresWalParser.java\n│   │   │   │   └── PostgresConnector.java # Implements LogConnector for PG\n│   │   │   └── mysql/\n│   │   │       ├── MysqlBinlogParser.java\n│   │   │       └── MysqlConnector.java    # Implements LogConnector for MySQL\n│   │   ├── offset/\n│   │   │   ├── OffsetStore.java\n│   │   │   ├── FileOffsetStore.java\n│   │   │   └── KafkaOffsetStore.java\n│   │   └── model/\n│   │       └── RawLogEntry.java    # Your provided data model\n│   └── Main.java\n└── config/\n    └── application.yaml</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here is a complete, ready-to-use file-based <code>OffsetStore</code>. The Log Connector will depend on this to save and load its position.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/main/java/com/cdc/connector/offset/FileOffsetStore.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.connector.offset;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Properties;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Simple durable store for the last processed Log Sequence Number.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Persists to a file in key=value format.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> FileOffsetStore</span><span style=\"color:#F97583\"> implements</span><span style=\"color:#B392F0\"> OffsetStore</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> File offsetFile;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Properties properties;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> FileOffsetStore</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">filePath</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.offsetFile </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> File</span><span style=\"color:#E1E4E8\">(filePath);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.properties </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> Properties</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        load</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">offsetFile.</span><span style=\"color:#B392F0\">exists</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> (InputStream input </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> FileInputStream</span><span style=\"color:#E1E4E8\">(offsetFile)) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            properties.</span><span style=\"color:#B392F0\">load</span><span style=\"color:#E1E4E8\">(input);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (IOException </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> RuntimeException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to load offset file: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> offsetFile, e);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> save</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">slotName</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">lastProcessedLSN</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        properties.</span><span style=\"color:#B392F0\">setProperty</span><span style=\"color:#E1E4E8\">(slotName, lastProcessedLSN);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> (OutputStream output </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> FileOutputStream</span><span style=\"color:#E1E4E8\">(offsetFile)) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            properties.</span><span style=\"color:#B392F0\">store</span><span style=\"color:#E1E4E8\">(output, </span><span style=\"color:#9ECBFF\">\"CDC Offset Store\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (IOException </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> RuntimeException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to save offset to file: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> offsetFile, e);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">load</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">slotName</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> properties.</span><span style=\"color:#B392F0\">getProperty</span><span style=\"color:#E1E4E8\">(slotName);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// File: src/main/java/com/cdc/connector/offset/OffsetStore.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.connector.offset;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#B392F0\"> OffsetStore</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    void</span><span style=\"color:#B392F0\"> save</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">slotName</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">lastProcessedLSN</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    String </span><span style=\"color:#B392F0\">load</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">slotName</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>Here is the skeleton for the primary loop of a generic <code>LogConnector</code>. Database-specific implementations (like <code>PostgresConnector</code>) will extend this.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/main/java/com/cdc/connector/LogConnector.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.connector;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.config.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.model.RawLogEntry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.offset.OffsetStore;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.parser.LogParser;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.List;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.ArrayBlockingQueue;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.BlockingQueue;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Abstract base class for database-specific log connectors.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Manages the lifecycle, offset persistence, and buffering.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Subclasses implement the database-specific connection and raw byte reading.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> abstract</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> LogConnector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> AppConfig config;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> OffsetStore offsetStore;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> LogParser parser;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> BlockingQueue&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> buffer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> volatile</span><span style=\"color:#E1E4E8\"> String lastProcessedLSN;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> volatile</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#E1E4E8\"> running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#E1E4E8\"> Thread workerThread;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> LogConnector</span><span style=\"color:#E1E4E8\">(AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">, OffsetStore </span><span style=\"color:#FFAB70\">offsetStore</span><span style=\"color:#E1E4E8\">, LogParser </span><span style=\"color:#FFAB70\">parser</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.offsetStore </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> offsetStore;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.buffer </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayBlockingQueue&#x3C;>(config.</span><span style=\"color:#B392F0\">getMaxBatchSize</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">); </span><span style=\"color:#6A737D\">// Buffer 10x batch size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Load the last processed LSN from the OffsetStore using the configured slot name.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: If no offset exists, determine a safe starting LSN (e.g., current database LSN or based on config).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Set the internal `lastProcessedLSN` variable to the loaded/calculated value.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Perform any database-specific initialization (e.g., create replication slot if needed).</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.workerThread </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> Thread</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">this</span><span style=\"color:#F97583\">::</span><span style=\"color:#E1E4E8\">runLoop, </span><span style=\"color:#9ECBFF\">\"log-connector-worker\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.workerThread.</span><span style=\"color:#B392F0\">start</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> runLoop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Establish a low-level, streaming connection to the database log.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: While `running` is true, perform the main read-parse-buffer loop:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   a. Read a chunk of raw bytes from the database stream.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   b. If the buffer is full, pause reading (maybe sleep briefly) to apply backpressure.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   c. Parse the raw bytes using the injected `parser` to get a List&#x3C;RawLogEntry>.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   d. For each RawLogEntry in the list:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        i. Update the in-memory `lastProcessedLSN` to this entry's LSN.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        ii. Put the entry into the `buffer` (this may block if buffer is full).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   e. Periodically (e.g., after each batch or time interval), call `persistOffset()`.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: If an unrecoverable error occurs, set `running` to false and log the error.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Upon exit (when `running` becomes false), close the database connection gracefully.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getNextBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> maxBatchSize</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Create an empty list to hold the batch.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Poll the `buffer` for up to `maxBatchSize` entries, adding them to the list.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: If the buffer is empty and the connector is still running, optionally block for a short time to wait for data.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Return the list (may be empty if no data is available).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> persistOffset</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Save the current `lastProcessedLSN` to the OffsetStore.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: (Database-specific) If required by the database protocol (e.g., PostgreSQL), send an acknowledgement for this LSN.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Set `running` to false to signal the worker thread to stop.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Interrupt the worker thread if it's blocked on reading or buffering.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Wait for the worker thread to terminate (with a timeout).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Call `persistOffset()` one final time to flush the most recent LSN.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Perform any database-specific cleanup (e.g., close replication connection).</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getLastProcessedLSN</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> lastProcessedLSN;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Abstract methods to be implemented by database-specific subclasses</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> abstract</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> connectToLogStream</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> abstract</span><span style=\"color:#F97583\"> byte</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#B392F0\">readLogChunk</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> abstract</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> acknowledgeLSN</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">lsn</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#F97583\"> abstract</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> closeConnection</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints (Java)</strong></p>\n<ul>\n<li><strong>Use <code>Thread.interrupt()</code> for graceful shutdown</strong>: In the <code>stop()</code> method, call <code>workerThread.interrupt()</code>. Ensure your <code>readLogChunk</code> and <code>buffer.put()</code> operations are interruptible or check the <code>running</code> flag periodically.</li>\n<li><strong>Mind the blocking queue capacity</strong>: The <code>ArrayBlockingQueue</code> size is a critical tuning parameter. Too small, and you throttle throughput unnecessarily. Too large, and you risk excessive memory usage during consumer lag.</li>\n<li><strong>Prefer <code>java.time</code> types</strong>: When parsing database timestamps, convert them to <code>java.time.Instant</code> or <code>Long</code> (epoch millis) immediately for consistency within your <code>RawLogEntry</code>.</li>\n<li><strong>Resource Management</strong>: Use try-with-resources for any database connections or streams opened in the <code>connectToLogStream</code> and <code>readLogChunk</code> implementations.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong>\nAfter implementing the Log Connector &amp; Parser for one database (e.g., PostgreSQL), you should be able to verify Milestone 1 acceptance criteria:</p>\n<ol>\n<li><strong>Run a Simple Test</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">    # Start your CDC application with a config pointing to a test PostgreSQL database.</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    java</span><span style=\"color:#79B8FF\"> -jar</span><span style=\"color:#9ECBFF\"> cdc-pipeline.jar</span><span style=\"color:#9ECBFF\"> config/application.yaml</span></span></code></pre></div>\n<ol start=\"2\">\n<li><strong>Expected Behavior</strong>:<ul>\n<li>The application should start without errors, create a replication slot in PostgreSQL (check with <code>SELECT * FROM pg_replication_slots;</code>).</li>\n<li>Make an <code>INSERT</code> into a monitored table in your test database.</li>\n<li>In your application logs, you should see a debug message indicating a <code>RawLogEntry</code> was parsed and buffered.</li>\n<li>You can write a simple test consumer that calls <code>connector.getNextBatch(10)</code> and prints the received <code>RawLogEntry</code> objects, verifying the <code>tableName</code>, <code>operationType</code>, and <code>rowData</code> are correct.</li>\n</ul>\n</li>\n<li><strong>Check Offset Persistence</strong>: Stop the application. Check the offset file (e.g., <code>offset.properties</code>). It should contain an LSN. Restart the application; it should resume from that LSN without re-reading the old change.</li>\n<li><strong>Signs of Trouble</strong>:<ul>\n<li><strong>&quot;No pgoutput plugin installed&quot;</strong>: Ensure <code>wal_level=logical</code> in <code>postgresql.conf</code> and the <code>pgoutput</code> plugin is available.</li>\n<li><strong>&quot;Replication slot already exists&quot;</strong>: Your connector should handle this gracefully (re-use it). Ensure your slot management logic is correct.</li>\n<li><strong>No events are parsed</strong>: Verify your table has <code>REPLICA IDENTITY FULL</code> (for full before/after images in updates/deletes) and that you are reading from the correct slot.</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Change Events construction, transaction boundaries), Milestone 2 (ordering guarantees), Milestone 3 (schema attachment)</p>\n</blockquote>\n<h2 id=\"6-component-change-event-builder\">6. Component: Change Event Builder</h2>\n<p>The <strong>Change Event Builder</strong> is the central transformation engine of the CDC pipeline, responsible for converting low-level, database-specific log entries into standardized, semantically rich change events ready for downstream consumption. While the Log Connector extracts raw data from transaction logs, this component assembles complete business-level change narratives with proper transaction context, deduplication, and schema attachment.</p>\n<h3 id=\"61-mental-model-the-event-assembler\">6.1 Mental Model: The Event Assembler</h3>\n<p><strong>Think of the Change Event Builder as a film editor assembling raw footage into a coherent movie.</strong> </p>\n<p>The Log Connector provides individual film frames (<code>RawLogEntry</code> objects) – isolated snapshots of database operations. However, these frames lack crucial context:</p>\n<ul>\n<li>Which frames belong to the same &quot;scene&quot; (transaction)?</li>\n<li>When did the scene start and end (commit boundaries)?</li>\n<li>What objects appear before and after changes (before/after images)?</li>\n<li>What&#39;s the storyline for each main character (primary key ordering)?</li>\n</ul>\n<p>The Change Event Builder performs the editorial work:</p>\n<ol>\n<li><strong>Grouping</strong>: It collects all log entries belonging to the same database transaction (using transaction IDs or commit boundaries)</li>\n<li><strong>Storytelling</strong>: It arranges changes in the order they occurred within each transaction</li>\n<li><strong>Character Development</strong>: For UPDATE operations, it pairs &quot;before&quot; and &quot;after&quot; images to show exactly what changed</li>\n<li><strong>Quality Control</strong>: It removes duplicate frames (idempotent processing) and ensures continuity (no missing frames)</li>\n<li><strong>Metadata Addition</strong>: It attaches schema version information so consumers understand the data format</li>\n<li><strong>Final Cut</strong>: It outputs complete, self-contained <code>ChangeEvent</code> objects – the finished scenes ready for distribution</li>\n</ol>\n<p>This mental model emphasizes that raw log parsing is just the beginning – the real value comes from assembling isolated database operations into coherent, transactionally consistent change stories that downstream systems can reliably consume.</p>\n<h3 id=\"62-interface-and-transaction-state\">6.2 Interface and Transaction State</h3>\n<p>The Change Event Builder exposes a simple streaming interface while maintaining complex internal state about ongoing transactions. Its primary responsibility is transforming sequences of <code>RawLogEntry</code> objects into sequences of <code>ChangeEvent</code> objects, handling all transaction-level logic internally.</p>\n<h4 id=\"621-public-interface\">6.2.1 Public Interface</h4>\n<p>The component provides two operational modes: batch processing for throughput and streaming for low latency. Both share the same internal transaction state management.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>processEntry(RawLogEntry)</code></td>\n<td><code>entry</code>: A single parsed log entry</td>\n<td><code>List&lt;ChangeEvent&gt;</code></td>\n<td>Processes a single log entry through the transaction state machine. May return 0-n events (0 when accumulating transaction state, 1+ when transaction commits). This is the streaming API.</td>\n</tr>\n<tr>\n<td><code>processBatch(List&lt;RawLogEntry&gt;)</code></td>\n<td><code>entries</code>: Batch of parsed entries</td>\n<td><code>List&lt;ChangeEvent&gt;</code></td>\n<td>Processes multiple log entries efficiently. Maintains same guarantees as streaming API but with better throughput due to batched state transitions.</td>\n</tr>\n<tr>\n<td><code>flushPendingTransactions()</code></td>\n<td>None</td>\n<td><code>List&lt;ChangeEvent&gt;</code></td>\n<td>Forces all pending (uncommitted) transactions to be emitted as events with a special &quot;transaction rolled back&quot; marker. Used during shutdown or when switching database connections to prevent state leaks.</td>\n</tr>\n<tr>\n<td><code>getTransactionCount()</code></td>\n<td>None</td>\n<td><code>int</code></td>\n<td>Returns the number of currently active (uncommitted) transactions being tracked. Used for monitoring and health checks.</td>\n</tr>\n<tr>\n<td><code>reset()</code></td>\n<td>None</td>\n<td><code>void</code></td>\n<td>Clears all internal transaction state. Used after connection loss or position reset to start fresh.</td>\n</tr>\n</tbody></table>\n<h4 id=\"622-internal-transaction-state-machine\">6.2.2 Internal Transaction State Machine</h4>\n<p>The builder maintains a state machine for each database transaction it encounters. This is the core complexity of the component – tracking which operations belong to which transaction and when to emit them.</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Action Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>NO_TRANSACTION</strong></td>\n<td>Receive <code>RawLogEntry</code> with <code>BEGIN</code> marker</td>\n<td><code>TRANSACTION_OPEN</code></td>\n<td>Create new <code>TransactionState</code> object with empty operation list. Store transaction ID from log entry.</td>\n</tr>\n<tr>\n<td><strong>NO_TRANSACTION</strong></td>\n<td>Receive <code>RawLogEntry</code> without <code>BEGIN</code> (implicit tx)</td>\n<td><code>TRANSACTION_OPEN</code></td>\n<td>Create new <code>TransactionState</code> for implicit transaction. Generate synthetic transaction ID. Add operation to transaction&#39;s operation list.</td>\n</tr>\n<tr>\n<td><strong>TRANSACTION_OPEN</strong></td>\n<td>Receive <code>RawLogEntry</code> with database operation (INSERT/UPDATE/DELETE)</td>\n<td><code>TRANSACTION_OPEN</code></td>\n<td>Add operation to transaction&#39;s operation list. For UPDATE operations, attempt to pair with previous operation on same PK to build before/after image.</td>\n</tr>\n<tr>\n<td><strong>TRANSACTION_OPEN</strong></td>\n<td>Receive <code>RawLogEntry</code> with <code>COMMIT</code> marker</td>\n<td><code>NO_TRANSACTION</code></td>\n<td>1. Finalize all operations in transaction (complete before/after images)<br>2. Generate <code>ChangeEvent</code> for each operation<br>3. Emit events in transaction order<br>4. Discard <code>TransactionState</code></td>\n</tr>\n<tr>\n<td><strong>TRANSACTION_OPEN</strong></td>\n<td>Receive <code>RawLogEntry</code> with <code>ROLLBACK</code> marker</td>\n<td><code>NO_TRANSACTION</code></td>\n<td>Discard <code>TransactionState</code> without emitting any events. Log warning about rolled back transaction.</td>\n</tr>\n<tr>\n<td><strong>TRANSACTION_OPEN</strong></td>\n<td>Timeout (transaction open &gt; threshold)</td>\n<td><code>NO_TRANSACTION</code></td>\n<td>Emit events with &quot;incomplete transaction&quot; warning flag. Used for long-running transactions that might indicate issues.</td>\n</tr>\n<tr>\n<td><strong>ANY</strong></td>\n<td>Pipeline reset request</td>\n<td><code>NO_TRANSACTION</code></td>\n<td>Discard all <code>TransactionState</code> objects. Log count of abandoned transactions.</td>\n</tr>\n</tbody></table>\n<h4 id=\"623-transactionstate-data-structure\">6.2.3 TransactionState Data Structure</h4>\n<p>Internally, the builder maintains a map of transaction ID to <code>TransactionState</code> objects:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>transactionId</code></td>\n<td><code>String</code></td>\n<td>Unique identifier for this transaction (from database log or generated)</td>\n</tr>\n<tr>\n<td><code>startLsn</code></td>\n<td><code>String</code></td>\n<td>Log Sequence Number where transaction began</td>\n</tr>\n<tr>\n<td><code>operations</code></td>\n<td><code>List&lt;TransactionOperation&gt;</code></td>\n<td>Ordered list of operations within this transaction</td>\n</tr>\n<tr>\n<td><code>startTimestamp</code></td>\n<td><code>Long</code></td>\n<td>When transaction began (monotonic clock)</td>\n</tr>\n<tr>\n<td><code>lastActivityTimestamp</code></td>\n<td><code>Long</code></td>\n<td>When last operation was added (for timeout detection)</td>\n</tr>\n<tr>\n<td><code>sourceDatabase</code></td>\n<td><code>String</code></td>\n<td>Which database this transaction came from (important in multi-source CDC)</td>\n</tr>\n</tbody></table>\n<p>Each <code>TransactionOperation</code> contains:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>sequenceNumber</code></td>\n<td><code>int</code></td>\n<td>Order within transaction (1, 2, 3...)</td>\n</tr>\n<tr>\n<td><code>tableName</code></td>\n<td><code>String</code></td>\n<td>Fully qualified table name</td>\n</tr>\n<tr>\n<td><code>operationType</code></td>\n<td><code>String</code></td>\n<td><code>OPERATION_INSERT</code>, <code>OPERATION_UPDATE</code>, or <code>OPERATION_DELETE</code></td>\n</tr>\n<tr>\n<td><code>primaryKey</code></td>\n<td><code>Map&lt;String, Object&gt;</code></td>\n<td>Primary key values (for deduplication and ordering)</td>\n</tr>\n<tr>\n<td><code>rowData</code></td>\n<td><code>Map&lt;String, Object&gt;</code></td>\n<td>Complete row data at time of operation</td>\n</tr>\n<tr>\n<td><code>previousOperation</code></td>\n<td><code>TransactionOperation</code></td>\n<td>Reference to previous operation on same PK in same transaction (for building before-image)</td>\n</tr>\n</tbody></table>\n<h4 id=\"624-beforeafter-image-construction-algorithm\">6.2.4 Before/After Image Construction Algorithm</h4>\n<p>The most complex logic in the builder is constructing complete before and after images for UPDATE operations:</p>\n<ol>\n<li><p><strong>When receiving an INSERT operation</strong>: Store the row data as the &quot;after&quot; image. The &quot;before&quot; image is <code>null</code> (since row didn&#39;t exist before).</p>\n</li>\n<li><p><strong>When receiving a DELETE operation</strong>: Store the row data as the &quot;before&quot; image. The &quot;after&quot; image is <code>null</code> (since row doesn&#39;t exist after).</p>\n</li>\n<li><p><strong>When receiving an UPDATE operation</strong>: The builder must find the previous state of the same row. This requires:</p>\n<ul>\n<li>Identifying the row by primary key (from the UPDATE&#39;s row data)</li>\n<li>Looking backward in the current transaction&#39;s operation list</li>\n<li>Finding the most recent operation on that same primary key</li>\n</ul>\n<p>The algorithm works as follows:</p>\n<ol>\n<li>Extract primary key values from the UPDATE&#39;s <code>rowData</code></li>\n<li>Search backward through the current transaction&#39;s <code>operations</code> list</li>\n<li>For each previous operation:<ul>\n<li>If it operates on the same table AND has matching primary key values:<ul>\n<li>If previous operation is INSERT or UPDATE: Use its <code>rowData</code> as the &quot;before&quot; image</li>\n<li>If previous operation is DELETE: This is an error (can&#39;t UPDATE a deleted row) - log warning, use empty map as &quot;before&quot;</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>If no matching operation found in current transaction:<ul>\n<li>The &quot;before&quot; image is unknown within this transaction scope</li>\n<li>Store the UPDATE&#39;s <code>rowData</code> as the &quot;after&quot; image only</li>\n<li>Mark the event as having &quot;partial before image&quot;</li>\n</ul>\n</li>\n</ol>\n</li>\n</ol>\n<p>This algorithm ensures that within a single transaction, we can reconstruct the complete change history of each row. However, it cannot reconstruct changes that span transactions (that requires log parsing across transaction boundaries, which is more complex).</p>\n<h3 id=\"63-adr-ordering-and-deduplication-strategy\">6.3 ADR: Ordering and Deduplication Strategy</h3>\n<blockquote>\n<p><strong>Decision: Primary Key-Based Partitioned Ordering with Transaction-Aware Deduplication</strong></p>\n<ul>\n<li><strong>Context</strong>: Change events must be delivered to consumers in the exact order they occurred in the database, but different consumers may process events at different speeds. We need to guarantee that (1) events for the same row are processed in order, (2) events across different rows can be processed in parallel, and (3) no duplicate events are delivered even after failures and restarts.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Global Sequence Numbering</strong>: Assign a monotonically increasing sequence number to every event across all tables. Consumers process in strict global order.</li>\n<li><strong>Table-Level Ordering</strong>: Maintain order per table, allowing parallel processing across tables but serial within each table.</li>\n<li><strong>Primary Key Partitioned Ordering</strong>: Partition by (table, primary key hash), guaranteeing order per primary key while allowing maximal parallelism.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: <strong>Primary Key Partitioned Ordering</strong> using a combination of LSN-based sequencing and idempotent producer logic.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Correctness</strong>: For most use cases (materialized views, cache invalidation, audit trails), only the order of changes to the <em>same row</em> matters. Changes to different rows are independent and can be safely reordered.</li>\n<li><strong>Performance</strong>: Partitioning by primary key allows horizontal scaling - each partition can be processed independently by different consumer instances.</li>\n<li><strong>Real-world alignment</strong>: This matches how databases work internally - they lock at row level, not table level, during concurrent modifications.</li>\n<li><strong>Idempotency guarantee</strong>: By including the source LSN in each event and making producers idempotent, we can safely retry events after failures without creating duplicates.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li>✅ Enables high parallelism while maintaining correctness for row-level ordering</li>\n<li>✅ Aligns naturally with Kafka partitioning strategy (partition key = table + primary key hash)</li>\n<li>⚠️ Requires consumers to handle out-of-order events <em>across different primary keys</em> (which they should be able to do anyway)</li>\n<li>⚠️ Slightly more complex deduplication logic needed compared to global ordering</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p>The following table compares the ordering strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Global Sequence Numbering</strong></td>\n<td>Simplest for consumers (strict total order)</td>\n<td>Terrible throughput (single processing thread)</td>\n<td>Severely limits scalability - becomes bottleneck at scale</td>\n</tr>\n<tr>\n<td><strong>Table-Level Ordering</strong></td>\n<td>Good parallelism across tables</td>\n<td>Poor parallelism for large tables with many concurrent updates</td>\n<td>Still serializes updates to different rows in same table unnecessarily</td>\n</tr>\n<tr>\n<td><strong>Primary Key Partitioned Ordering</strong></td>\n<td>Maximum parallelism while maintaining row-level correctness</td>\n<td>Consumers must handle cross-key ordering relaxations</td>\n<td><strong>CHOSEN</strong>: Best trade-off - matches real database concurrency model</td>\n</tr>\n</tbody></table>\n<h4 id=\"631-deduplication-implementation-strategy\">6.3.1 Deduplication Implementation Strategy</h4>\n<p>To achieve idempotent production (avoiding duplicate events during retries), the builder implements a two-layer deduplication strategy:</p>\n<p><strong>Layer 1: Producer-Initiated Deduplication</strong></p>\n<ul>\n<li>Each <code>ChangeEvent</code> includes a unique <code>eventId</code> computed as: <code>{sourceDatabase}_{transactionId}_{tableName}_{primaryKeyHash}_{operationSequence}</code></li>\n<li>The Event Streamer (next component) maintains a cache of recently sent event IDs</li>\n<li>Before sending an event, it checks the cache - if present, skip sending</li>\n<li>Cache is sized based on maximum flight window (e.g., 5 minutes of events at peak throughput)</li>\n</ul>\n<p><strong>Layer 2: Consumer-Initiated Deduplication</strong></p>\n<ul>\n<li>Consumers store the <code>eventId</code> of the last successfully processed event per partition</li>\n<li>When reprocessing after failure, consumers skip events with <code>eventId</code> ≤ last processed</li>\n<li>This provides end-to-end idempotency even if producer deduplication fails</li>\n</ul>\n<p>The builder&#39;s role in this strategy is to generate deterministic, unique <code>eventId</code> values that capture the identity of the change operation unambiguously.</p>\n<h4 id=\"632-ordering-guarantees-by-operation-type\">6.3.2 Ordering Guarantees by Operation Type</h4>\n<p>The builder enforces different ordering semantics based on operation type:</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Ordering Guarantee</th>\n<th>Implementation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>INSERT</strong></td>\n<td>Must be delivered before any UPDATE/DELETE on same PK</td>\n<td>Sequence number starts at 1 for each PK</td>\n</tr>\n<tr>\n<td><strong>UPDATE</strong></td>\n<td>Must be delivered in chronological order for same PK</td>\n<td>Increment sequence number for each operation on same PK</td>\n</tr>\n<tr>\n<td><strong>DELETE</strong></td>\n<td>Must be delivered after all previous operations, before any subsequent INSERT with same PK</td>\n<td>Sequence number increments, but PK is considered &quot;deleted&quot; after</td>\n</tr>\n<tr>\n<td><strong>TRUNCATE</strong></td>\n<td>Special event that invalidates all previous ordering for table</td>\n<td>Resets all sequence numbers for table</td>\n</tr>\n</tbody></table>\n<h3 id=\"64-implementation-guidance\">6.4 Implementation Guidance</h3>\n<p>The Change Event Builder is where the core business logic of CDC lives. While the Log Connector handles database-specific details, this component implements the universal logic of transaction assembly and event construction.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Transaction State Store</strong></td>\n<td>In-memory <code>ConcurrentHashMap</code> with periodic flush to disk</td>\n<td>Embedded key-value store (RocksDB) with transaction isolation</td>\n</tr>\n<tr>\n<td><strong>Before/After Image Cache</strong></td>\n<td>LRU cache of recent row states per transaction</td>\n<td>Persistent snapshot store with point-in-time recovery</td>\n</tr>\n<tr>\n<td><strong>Event ID Generation</strong></td>\n<td>Deterministic hash of (LSN + table + PK)</td>\n<td>UUID v5 (name-based) with namespace for collision avoidance</td>\n</tr>\n<tr>\n<td><strong>Timeout Detection</strong></td>\n<td>Scheduled thread pool checking timestamps</td>\n<td>Reactive streams with backpressure-aware timeouts</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-system/\n├── src/main/java/com/example/cdc/\n│   ├── builder/\n│   │   ├── ChangeEventBuilder.java          # Main builder class (implements interface)\n│   │   ├── TransactionState.java            # Internal transaction state representation\n│   │   ├── TransactionOperation.java        # Individual operation within transaction\n│   │   ├── BeforeAfterImageBuilder.java     # Dedicated class for image construction\n│   │   ├── EventIdGenerator.java            # Deterministic ID generation\n│   │   └── DeduplicationCache.java          # In-memory cache for producer deduplication\n│   ├── model/                               # (From previous section)\n│   │   ├── RawLogEntry.java\n│   │   ├── ChangeEvent.java\n│   │   └── SchemaVersion.java\n│   └── Main.java\n└── config/\n    └── application.yml</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p>Here&#39;s a complete, ready-to-use implementation of the <code>TransactionState</code> and <code>TransactionOperation</code> classes that form the foundation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.example.cdc.builder;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.ConcurrentHashMap;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Represents an ongoing database transaction being tracked.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * This is an internal class not exposed outside the builder.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> TransactionState</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String transactionId;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String startLsn;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> startTimestamp;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">TransactionOperation</span><span style=\"color:#E1E4E8\">> operations;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> lastActivityTimestamp;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">TransactionOperation</span><span style=\"color:#E1E4E8\">> lastOperationByKey;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> TransactionState</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">transactionId</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">startLsn</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.transactionId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> transactionId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.startLsn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> startLsn;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.startTimestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> System.</span><span style=\"color:#B392F0\">currentTimeMillis</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.lastActivityTimestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> startTimestamp;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.operations </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.lastOperationByKey </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> addOperation</span><span style=\"color:#E1E4E8\">(TransactionOperation </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        operations.</span><span style=\"color:#B392F0\">add</span><span style=\"color:#E1E4E8\">(operation);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastActivityTimestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> System.</span><span style=\"color:#B392F0\">currentTimeMillis</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Track last operation per primary key for quick before-image lookup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        String key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operation.</span><span style=\"color:#B392F0\">getTableName</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \":\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#B392F0\"> hashPrimaryKey</span><span style=\"color:#E1E4E8\">(operation.</span><span style=\"color:#B392F0\">getPrimaryKey</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastOperationByKey.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(key, operation);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> TransactionOperation </span><span style=\"color:#B392F0\">findPreviousOperation</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                                     Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">primaryKey</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        String key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tableName </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \":\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#B392F0\"> hashPrimaryKey</span><span style=\"color:#E1E4E8\">(primaryKey);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> lastOperationByKey.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(key);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#B392F0\"> isTimedOut</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">long</span><span style=\"color:#FFAB70\"> timeoutMs</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (System.</span><span style=\"color:#B392F0\">currentTimeMillis</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> lastActivityTimestamp) </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> timeoutMs;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">TransactionOperation</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getOperationsInOrder</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Collections.</span><span style=\"color:#B392F0\">unmodifiableList</span><span style=\"color:#E1E4E8\">(operations);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Helper method to create consistent key for maps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">hashPrimaryKey</span><span style=\"color:#E1E4E8\">(Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">primaryKey</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Simple hash - in production, use proper hashing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Integer.</span><span style=\"color:#B392F0\">toHexString</span><span style=\"color:#E1E4E8\">(primaryKey.</span><span style=\"color:#B392F0\">hashCode</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getTransactionId</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> transactionId; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getStartLsn</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> startLsn; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> long</span><span style=\"color:#B392F0\"> getStartTimestamp</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> startTimestamp; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> long</span><span style=\"color:#B392F0\"> getLastActivityTimestamp</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> lastActivityTimestamp; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Represents a single database operation (INSERT/UPDATE/DELETE) within a transaction.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> TransactionOperation</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> sequenceNumber;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String tableName;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String operationType;  </span><span style=\"color:#6A737D\">// INSERT, UPDATE, DELETE</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> primaryKey;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> rowData;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> timestamp;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> TransactionOperation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> sequenceNumber</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               String </span><span style=\"color:#FFAB70\">operationType</span><span style=\"color:#E1E4E8\">, Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">primaryKey</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">rowData</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.sequenceNumber </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sequenceNumber;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.tableName </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tableName;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.operationType </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operationType;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.primaryKey </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Collections.</span><span style=\"color:#B392F0\">unmodifiableMap</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>(primaryKey));</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.rowData </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Collections.</span><span style=\"color:#B392F0\">unmodifiableMap</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>(rowData));</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> System.</span><span style=\"color:#B392F0\">currentTimeMillis</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> int</span><span style=\"color:#B392F0\"> getSequenceNumber</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> sequenceNumber; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getTableName</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> tableName; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getOperationType</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> operationType; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getPrimaryKey</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> primaryKey; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getRowData</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> rowData; }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> long</span><span style=\"color:#B392F0\"> getTimestamp</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> timestamp; }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<p>Here&#39;s the main <code>ChangeEventBuilder</code> class with detailed TODO comments mapping to the algorithms described earlier:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.example.cdc.builder;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.model.RawLogEntry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.model.ChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.example.cdc.model.SchemaVersion;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ChangeEventBuilder</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">TransactionState</span><span style=\"color:#E1E4E8\">> activeTransactions;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">SchemaVersion</span><span style=\"color:#E1E4E8\">> schemaCache;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> DeduplicationCache deduplicationCache;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> long</span><span style=\"color:#E1E4E8\"> transactionTimeoutMs;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> nextOperationSequence;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> ChangeEventBuilder</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">long</span><span style=\"color:#FFAB70\"> transactionTimeoutMs</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.activeTransactions </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ConcurrentHashMap&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.schemaCache </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.deduplicationCache </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> DeduplicationCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">); </span><span style=\"color:#6A737D\">// 10k entry cache</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.transactionTimeoutMs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> transactionTimeoutMs;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.nextOperationSequence </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Processes a single raw log entry through the transaction state machine.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * May return 0-n events (0 when accumulating, 1+ when transaction commits).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">processEntry</span><span style=\"color:#E1E4E8\">(RawLogEntry </span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Check if entry marks a transaction boundary (BEGIN/COMMIT/ROLLBACK)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - If BEGIN: create new TransactionState, store in activeTransactions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - If COMMIT: </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        a. Retrieve TransactionState for this transaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        b. Call buildEventsForTransaction() to convert all operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        c. Remove from activeTransactions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        d. Add all built events to result list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - If ROLLBACK: remove TransactionState without building events</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: If entry is a regular database operation (INSERT/UPDATE/DELETE):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Determine which transaction it belongs to (use transactionId from entry or implicit)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Retrieve or create TransactionState for that transaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Create TransactionOperation with sequence number (increment nextOperationSequence)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Add operation to TransactionState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Check for timed-out transactions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Iterate through activeTransactions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - For each transaction older than transactionTimeoutMs:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        a. Log warning about long-running transaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        b. Build events anyway (with \"incomplete\" flag)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        c. Remove from activeTransactions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        d. Add events to result list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: For each event to be emitted:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Generate deterministic eventId using EventIdGenerator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Check deduplicationCache - if already sent recently, skip</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Attach appropriate SchemaVersion from schemaCache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Set commitTimestamp (use entry timestamp or current time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Processes a batch of entries more efficiently than individual processing.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">processBatch</span><span style=\"color:#E1E4E8\">(List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">entries</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> allEvents </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Group entries by transaction for batch processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Create Map&#x3C;transactionId, List&#x3C;RawLogEntry>></span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - For each entry, add to appropriate transaction group</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Process each transaction group independently (enables parallelism)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - For each transaction group:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        a. Sort entries by LSN or sequence within transaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        b. Process through same logic as processEntry() but batched</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        c. Collect all resulting events</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Sort events across transactions by commit timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - This maintains global ordering across transactions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use stable sort to preserve within-transaction ordering</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Apply batch deduplication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Check all event IDs in one batch operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Filter out any duplicates within the batch itself</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> allEvents;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Converts all operations in a completed transaction to ChangeEvent objects.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">buildEventsForTransaction</span><span style=\"color:#E1E4E8\">(TransactionState </span><span style=\"color:#FFAB70\">transaction</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> events </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Get operations in sequence order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">TransactionOperation</span><span style=\"color:#E1E4E8\">> operations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> transaction.</span><span style=\"color:#B392F0\">getOperationsInOrder</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: For each operation, build corresponding ChangeEvent</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (TransactionOperation op </span><span style=\"color:#F97583\">:</span><span style=\"color:#E1E4E8\"> operations) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ChangeEvent event </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ChangeEvent</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Set basic fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            event.</span><span style=\"color:#B392F0\">setSourceTable</span><span style=\"color:#E1E4E8\">(op.</span><span style=\"color:#B392F0\">getTableName</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            event.</span><span style=\"color:#B392F0\">setOperationType</span><span style=\"color:#E1E4E8\">(op.</span><span style=\"color:#B392F0\">getOperationType</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            event.</span><span style=\"color:#B392F0\">setTransactionId</span><span style=\"color:#E1E4E8\">(transaction.</span><span style=\"color:#B392F0\">getTransactionId</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            event.</span><span style=\"color:#B392F0\">setCommitTimestamp</span><span style=\"color:#E1E4E8\">(transaction.</span><span style=\"color:#B392F0\">getLastActivityTimestamp</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Handle operation-specific logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            switch</span><span style=\"color:#E1E4E8\"> (op.</span><span style=\"color:#B392F0\">getOperationType</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                case</span><span style=\"color:#9ECBFF\"> \"INSERT\"</span><span style=\"color:#F97583\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Before image is null, after image is rowData</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    event.</span><span style=\"color:#B392F0\">setBeforeImage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">null</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    event.</span><span style=\"color:#B392F0\">setAfterImage</span><span style=\"color:#E1E4E8\">(op.</span><span style=\"color:#B392F0\">getRowData</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                case</span><span style=\"color:#9ECBFF\"> \"UPDATE\"</span><span style=\"color:#F97583\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Find previous operation on same PK for before image</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    TransactionOperation previous </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> transaction.</span><span style=\"color:#B392F0\">findPreviousOperation</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        op.</span><span style=\"color:#B392F0\">getTableName</span><span style=\"color:#E1E4E8\">(), op.</span><span style=\"color:#B392F0\">getPrimaryKey</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> (previous </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> previous </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> op) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        // Found previous state in same transaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        event.</span><span style=\"color:#B392F0\">setBeforeImage</span><span style=\"color:#E1E4E8\">(previous.</span><span style=\"color:#B392F0\">getRowData</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        // No previous state in this transaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        // Mark as partial before image (null or empty)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        event.</span><span style=\"color:#B392F0\">setBeforeImage</span><span style=\"color:#E1E4E8\">(Collections.</span><span style=\"color:#B392F0\">emptyMap</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    event.</span><span style=\"color:#B392F0\">setAfterImage</span><span style=\"color:#E1E4E8\">(op.</span><span style=\"color:#B392F0\">getRowData</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                case</span><span style=\"color:#9ECBFF\"> \"DELETE\"</span><span style=\"color:#F97583\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Before image is rowData, after image is null</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    event.</span><span style=\"color:#B392F0\">setBeforeImage</span><span style=\"color:#E1E4E8\">(op.</span><span style=\"color:#B392F0\">getRowData</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    event.</span><span style=\"color:#B392F0\">setAfterImage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">null</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Generate deterministic event ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            String eventId </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> generateEventId</span><span style=\"color:#E1E4E8\">(transaction, op);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            event.</span><span style=\"color:#B392F0\">setEventId</span><span style=\"color:#E1E4E8\">(eventId);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 6: Attach schema version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            SchemaVersion schema </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schemaCache.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(op.</span><span style=\"color:#B392F0\">getTableName</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (schema </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                event.</span><span style=\"color:#B392F0\">setSchemaVersionId</span><span style=\"color:#E1E4E8\">(schema.</span><span style=\"color:#B392F0\">getSchemaId</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            events.</span><span style=\"color:#B392F0\">add</span><span style=\"color:#E1E4E8\">(event);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> events;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Generates a deterministic event ID that uniquely identifies this change.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">generateEventId</span><span style=\"color:#E1E4E8\">(TransactionState </span><span style=\"color:#FFAB70\">transaction</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   TransactionOperation </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Create string representation combining:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Source database identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Transaction ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Table name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Primary key hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Operation sequence number within transaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Hash the combined string (SHA-256 or similar)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use first 16 chars of hex representation for readability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Format as: db_{dbId}_tx_{txId}_{table}_{pkHash}_{seq}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"event_\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> transaction.</span><span style=\"color:#B392F0\">getTransactionId</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"_\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               operation.</span><span style=\"color:#B392F0\">getSequenceNumber</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Forces all pending transactions to be emitted (for shutdown).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">flushPendingTransactions</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> allEvents </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Iterate through all activeTransactions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (TransactionState transaction </span><span style=\"color:#F97583\">:</span><span style=\"color:#E1E4E8\"> activeTransactions.</span><span style=\"color:#B392F0\">values</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Build events for each transaction (mark as \"forced flush\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> events </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> buildEventsForTransaction</span><span style=\"color:#E1E4E8\">(transaction);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Add special marker to each event indicating incomplete transaction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> (ChangeEvent event </span><span style=\"color:#F97583\">:</span><span style=\"color:#E1E4E8\"> events) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // Add metadata flag</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            allEvents.</span><span style=\"color:#B392F0\">addAll</span><span style=\"color:#E1E4E8\">(events);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Clear activeTransactions map</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        activeTransactions.</span><span style=\"color:#B392F0\">clear</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> allEvents;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Common Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Assuming All Updates Have Before Images</strong></p>\n<ul>\n<li><strong>Description</strong>: Treating every UPDATE as having a complete before image from the same transaction. In reality, the first UPDATE in a transaction has no before image within that transaction.</li>\n<li><strong>Why it&#39;s wrong</strong>: Creates incorrect change events showing &quot;null to value&quot; instead of &quot;unknown to value.&quot; Consumers might misinterpret this as row creation.</li>\n<li><strong>Fix</strong>: Implement proper before-image search logic that handles the &quot;no previous state in transaction&quot; case by marking the event appropriately.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring Transaction Rollbacks</strong></p>\n<ul>\n<li><strong>Description</strong>: Building and emitting events for operations that are later rolled back.</li>\n<li><strong>Why it&#39;s wrong</strong>: Delivers phantom changes that never actually happened in the database, corrupting downstream state.</li>\n<li><strong>Fix</strong>: Never emit events until seeing COMMIT. Buffer all operations in memory until transaction commits successfully.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Overly Aggressive Deduplication</strong></p>\n<ul>\n<li><strong>Description</strong>: Caching event IDs for too long and incorrectly filtering legitimate events that happen to have similar characteristics.</li>\n<li><strong>Why it&#39;s wrong</strong>: Drops real changes, creating data loss in downstream systems.</li>\n<li><strong>Fix</strong>: Use deterministic event ID generation that&#39;s truly unique per change, and limit cache size to reasonable time window (e.g., 5 minutes).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Handling Long-Running Transactions</strong></p>\n<ul>\n<li><strong>Description</strong>: Holding transaction state indefinitely, consuming increasing memory, potentially causing OOM errors.</li>\n<li><strong>Why it&#39;s wrong</strong>: System becomes unstable with large transactions or transaction leaks.</li>\n<li><strong>Fix</strong>: Implement transaction timeout with configurable threshold. Force-flush or abort transactions exceeding threshold with appropriate warnings.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the Change Event Builder, verify correctness with this test:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run the comprehensive test suite</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">mvn</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -Dtest=ChangeEventBuilderTest</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Tests for single transaction with multiple operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Tests for UPDATE before-image construction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Tests for transaction rollback (no events emitted)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Tests for event ID uniqueness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - All tests passing</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 1. Start the CDC pipeline with a test database</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 2. Execute this SQL sequence:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    BEGIN;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    INSERT INTO users(id, name) VALUES (1, 'Alice');</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    UPDATE users SET name = 'Bob' WHERE id = 1;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    COMMIT;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 3. Check the emitted events:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    - Should see exactly 2 events (INSERT then UPDATE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    - INSERT event: before=null, after={'id':1, 'name':'Alice'}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    - UPDATE event: before={'id':1, 'name':'Alice'}, after={'id':1, 'name':'Bob'}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    - Event IDs should be different but deterministic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#    - Both events should have same transactionId</span></span></code></pre></div>\n\n<p><strong>G. Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Missing before-images for UPDATEs</strong></td>\n<td>Not searching previous operations in same transaction</td>\n<td>Log the transaction operation list before building events</td>\n<td>Implement <code>findPreviousOperation()</code> method that searches backward</td>\n</tr>\n<tr>\n<td><strong>Duplicate events after restart</strong></td>\n<td>Event ID generation not deterministic across restarts</td>\n<td>Compare event IDs generated from same log position</td>\n<td>Include LSN or other persistent identifier in event ID calculation</td>\n</tr>\n<tr>\n<td><strong>Memory leak over time</strong></td>\n<td>Transaction state not cleared after timeout</td>\n<td>Monitor <code>getTransactionCount()</code> over time</td>\n<td>Implement background thread to check for timed-out transactions</td>\n</tr>\n<tr>\n<td><strong>Events emitted for rolled back transactions</strong></td>\n<td>Emitting events before seeing COMMIT</td>\n<td>Check if COMMIT log entry is being processed</td>\n<td>Buffer operations until COMMIT, discard on ROLLBACK</td>\n</tr>\n<tr>\n<td><strong>Slow processing with large transactions</strong></td>\n<td>Building all events only at COMMIT time</td>\n<td>Profile memory usage during large transaction</td>\n<td>Implement incremental event building or streaming within transaction</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (Event Streaming &amp; Delivery)</p>\n</blockquote>\n<h2 id=\"7-component-event-streamer-amp-delivery\">7. Component: Event Streamer &amp; Delivery</h2>\n<p>The <strong>Event Streamer &amp; Delivery</strong> component is the system&#39;s distribution engine. It takes validated <code>ChangeEvent</code> objects from the <code>ChangeEventBuilder</code> and reliably transmits them to downstream consumers via a message broker (like Apache Kafka). Its core responsibility is guaranteeing that every database change is delivered to consumers <strong>at least once</strong>, while preserving the <strong>order of changes</strong> for any given database row, even in the face of network failures, broker restarts, or consumer crashes. This component embodies the system&#39;s promise of real-time, reliable data propagation.</p>\n<h3 id=\"71-mental-model-the-reliable-postal-service\">7.1 Mental Model: The Reliable Postal Service</h3>\n<p>Think of this component as a highly reliable, trackable postal service for data packages (<code>ChangeEvent</code>s). The postal service (<code>EventStreamer</code>) must deliver every package (<code>event</code>) to a set of post office boxes (<code>Kafka topic partitions</code>). The challenge is ensuring no package gets lost, even if a mail truck breaks down (<code>producer failure</code>), and that packages destined for the same household (<code>database row with the same primary key</code>) arrive in the exact order they were mailed.</p>\n<ul>\n<li><strong>Postmaster (Producer):</strong> The <code>EventStreamer</code> acts as the postmaster, accepting packages, assigning them a destination box based on the recipient&#39;s address (<code>partition key</code>), and dispatching them. It keeps a meticulous log (<code>producer state</code>) of which packages have been successfully handed off to the postal trucks (<code>Kafka brokers</code>).</li>\n<li><strong>Post Office Boxes (Partitions):</strong> Each Kafka topic is split into partitions, akin to rows of post office boxes. Packages for the same household must go into the same box to preserve order. Different households can use different boxes, allowing for parallel delivery.</li>\n<li><strong>Delivery Receipts (Acknowledgments):</strong> Before considering a package &quot;delivered,&quot; the postmaster waits for a signed receipt (<code>acknowledgment</code> or <code>ack</code>) from the postal system confirming the package is safely stored. If a receipt is missing, the package is re-sent.</li>\n<li><strong>Backpressure (Mailroom Congestion):</strong> If the post office boxes are full (consumers are slow), the postal service must slow down or temporarily stop accepting new packages from the mail sorting facility (<code>ChangeEventBuilder</code>) to avoid overwhelming the system. This is <strong>backpressure</strong>.</li>\n</ul>\n<p>This mental model clarifies the dual focus: <strong>reliability</strong> (no lost packages) and <strong>ordered delivery per key</strong> (same household, same box).</p>\n<h3 id=\"72-interface-and-delivery-semantics\">7.2 Interface and Delivery Semantics</h3>\n<p>The <code>EventStreamer</code> component exposes a simple interface for receiving events and manages complex internal state to fulfill its delivery guarantees. Its primary contract is defined by its interaction with the <code>ChangeEventBuilder</code> (upstream) and the Kafka cluster (downstream).</p>\n<p><strong>Core Interface Methods:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description &amp; Side Effects</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>initialize</code></td>\n<td><code>config: AppConfig</code></td>\n<td><code>void</code></td>\n<td>Prepares the streamer: creates Kafka producer instance, configures serializers, connects to the schema registry, and initializes internal metrics and state.</td>\n</tr>\n<tr>\n<td><code>start</code></td>\n<td><code>-</code></td>\n<td><code>void</code></td>\n<td>Starts the internal event processing loop. Begins accepting events from the upstream builder via a <code>publish</code> call.</td>\n</tr>\n<tr>\n<td><code>stop</code></td>\n<td><code>-</code></td>\n<td><code>void</code></td>\n<td>Initiates a graceful shutdown: stops accepting new events, flushes any in-flight messages to Kafka, waits for all pending acknowledgments, and closes the Kafka producer connection.</td>\n</tr>\n<tr>\n<td><code>publish</code></td>\n<td><code>events: List&lt;ChangeEvent&gt;</code></td>\n<td><code>void</code></td>\n<td><strong>Primary ingestion method.</strong> Accepts a batch of events from the <code>ChangeEventBuilder</code>. The method is <strong>blocking</strong> under backpressure conditions. It assigns each event a target Kafka partition, serializes it, and hands it to the Kafka producer client. It internally tracks the events until successful acknowledgment.</td>\n</tr>\n<tr>\n<td><code>getDeliveryStatus</code></td>\n<td><code>-</code></td>\n<td><code>Map&lt;String, Long&gt;</code></td>\n<td>Returns a map of the last successfully published Log Sequence Number (LSN) per database replication slot. Used for health checks and offset persistence.</td>\n</tr>\n<tr>\n<td><code>isHealthy</code></td>\n<td><code>-</code></td>\n<td><code>boolean</code></td>\n<td>Checks the health of the underlying Kafka producer connection and whether any unrecoverable publishing errors have occurred.</td>\n</tr>\n</tbody></table>\n<p><strong>Internal State Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>State Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>kafkaProducer</code></td>\n<td><code>KafkaProducer&lt;String, byte[]&gt;</code></td>\n<td>The configured Apache Kafka producer client. It handles network communication, batching, retries, and acknowledgments.</td>\n</tr>\n<tr>\n<td><code>pendingEvents</code></td>\n<td><code>ConcurrentMap&lt;String, ChangeEvent&gt;</code></td>\n<td>A thread-safe map tracking events that have been sent but not yet acknowledged by Kafka. Keyed by a <strong>deterministic event ID</strong> (e.g., <code>transactionId:sequence</code>).</td>\n</tr>\n<tr>\n<td><code>lastAckedLsnBySlot</code></td>\n<td><code>ConcurrentMap&lt;String, String&gt;</code></td>\n<td>The most recent Log Sequence Number (LSN) for which <em>all</em> preceding events have been acknowledged per replication slot. This is the safe point to which the system can recover.</td>\n</tr>\n<tr>\n<td><code>backpressureSignal</code></td>\n<td><code>AtomicBoolean</code></td>\n<td>A flag set when consumer lag exceeds a threshold. When <code>true</code>, the <code>publish</code> method will block, applying backpressure to the upstream <code>ChangeEventBuilder</code>.</td>\n</tr>\n<tr>\n<td><code>inFlightEventCount</code></td>\n<td><code>AtomicInteger</code></td>\n<td>A counter of events sent to Kafka but not yet acknowledged. Used to limit the total number of unacknowledged events (in-flight) to prevent memory exhaustion.</td>\n</tr>\n</tbody></table>\n<p><strong>Delivery Semantics: At-Least-Once</strong>\nThe component guarantees <strong>at-least-once delivery</strong>. This means for every <code>ChangeEvent</code> given to the <code>publish</code> method, the following will hold:</p>\n<ol>\n<li>The event will be successfully written to the Kafka topic <strong>at least one time</strong>.</li>\n<li>In case of failures (e.g., producer crash after send but before acknowledgment), the event <strong>may be duplicated</strong>, appearing in the topic more than once.</li>\n<li>For any given primary key, the order of events in the Kafka topic will match the order of the original database transactions.</li>\n</ol>\n<p>This is achieved through a combination of Kafka&#39;s idempotent producer (to avoid duplicates on retries <em>within a single producer session</em>) and the component&#39;s internal tracking of the last <em>safely persisted</em> LSN. The algorithm is detailed in the next section.</p>\n<h3 id=\"73-internal-behavior-and-algorithm\">7.3 Internal Behavior and Algorithm</h3>\n<p>The <code>EventStreamer</code> operates as a stateful publisher. The following numbered steps describe the lifecycle of a <code>ChangeEvent</code> from receipt to confirmed delivery.</p>\n<ol>\n<li><p><strong>Event Reception &amp; Partition Assignment:</strong> The <code>publish(List&lt;ChangeEvent&gt;)</code> method receives a batch. For each <code>ChangeEvent</code>:\na.  Extract the <strong>partition key</strong>. This is a string derived from the event&#39;s <code>sourceTable</code> and its primary key values (from either <code>beforeImage</code> or <code>afterImage</code>). For example: <code>&quot;users:pk_value_123&quot;</code>.\nb.  Use a <strong>consistent hash function</strong> on the partition key to select a specific partition within the Kafka topic for that table. This ensures all events for the same database row go to the same partition.\nc.  Retrieve the corresponding <code>SchemaVersion</code> from the cache (or Schema Registry) using the <code>schemaVersionId</code>.\nd.  Serialize the <code>ChangeEvent</code> and its schema into a byte array using the <code>EventSerializer.serialize(event, schemaVersion)</code>.</p>\n</li>\n<li><p><strong>Producer Send with Callback:</strong>\na.  Construct a Kafka <code>ProducerRecord</code> with the target topic, partition key, and serialized value.\nb.  Before sending, add the event&#39;s deterministic ID to the <code>pendingEvents</code> map.\nc.  Invoke <code>kafkaProducer.send(record, callback)</code>. This is an asynchronous operation.\nd.  The registered <strong>callback</strong> is executed by a Kafka producer thread when the broker responds. The callback is critical for reliability.</p>\n</li>\n<li><p><strong>Callback Logic (Acknowledgment Handling):</strong>\na.  <strong>On Success:</strong> The Kafka broker has durably stored the message. The callback:\ni.  Removes the event&#39;s ID from the <code>pendingEvents</code> map.\nii.  Updates the <code>lastAckedLsnBySlot</code> for the event&#39;s source replication slot. The LSN to record is the event&#39;s own LSN, but only if <em>all events with a lower LSN for that slot are also acknowledged</em>. This requires tracking LSNs in order.\nb.  <strong>On Failure:</strong> The send failed after exhausting retries (e.g., topic not found, authorization error). The callback:\ni.  Logs the error and marks the component&#39;s health as <code>DEGRADED</code> or <code>STOPPED</code>.\nii.  The event remains in <code>pendingEvents</code>. A recovery procedure (e.g., restart) will be needed.</p>\n</li>\n<li><p><strong>Backpressure Mechanism:</strong> A separate monitoring thread periodically checks consumer lag for the target topics (via Kafka admin APIs or metrics).\na.  If lag for any partition exceeds <code>config.backpressureThresholdMs</code>, it sets the <code>backpressureSignal</code> to <code>true</code>.\nb.  The <code>publish</code> method checks this signal. If active, it blocks (using <code>Thread.sleep</code> or a lock) before processing the next batch, slowing the ingestion rate.\nc.  When lag falls below the threshold, the signal is cleared and publishing resumes at full speed.</p>\n</li>\n<li><p><strong>Graceful Shutdown &amp; Recovery:</strong> When <code>stop()</code> is called:\na.  The <code>publish</code> method stops accepting new events.\nb.  <code>kafkaProducer.flush()</code> is called, blocking until all sent messages receive a callback.\nc.  After flush, the <code>lastAckedLsnBySlot</code> map contains the most recent <em>consistent</em> LSNs. These LSNs are persisted to the <code>OffsetStore</code> (e.g., a file), marking the recovery point.\nd.  On the next startup, <code>initialize()</code> loads these LSNs. The <code>LogConnector</code> is instructed to start reading the transaction log from just <em>after</em> these LSNs, ensuring no change event is missed, though some may be replayed (at-least-once).</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Key Design Insight:</strong> The &quot;safely persisted LSN&quot; is only advanced when <em>all events up to that LSN are acknowledged</em>. This ensures that if the process crashes, upon restart it will replay from an LSN where some events might have been sent but not acknowledged, potentially causing duplicates, but guaranteeing no data loss.</p>\n</blockquote>\n<h3 id=\"74-adr-at-least-once-vs-exactly-once-delivery\">7.4 ADR: At-Least-Once vs. Exactly-Once Delivery</h3>\n<blockquote>\n<p><strong>Decision: Implement At-Least-Once Delivery with Idempotent Kafka Producer</strong></p>\n<ul>\n<li><strong>Context</strong>: The CDC system must guarantee no data loss. Downstream consumers can tolerate duplicate events (idempotent consumption) but cannot tolerate missing events. The complexity and performance cost of exactly-once semantics is high.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>At-Least-Once with Idempotent Producer</strong>: Use Kafka&#39;s <code>enable.idempotence=true</code> producer setting to prevent duplicates caused by internal producer retries, but accept duplicates from producer restarts/replays.</li>\n<li><strong>Transactional/Exactly-Once Semantics</strong>: Use Kafka transactions to coordinate the publishing of events and the storing of the source LSN in a single atomic operation, eliminating duplicates even across producer sessions.</li>\n<li><strong>At-Most-Once</strong>: Send events without waiting for acknowledgment. Lower latency but risk of data loss on failure.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement <strong>At-Least-Once with Idempotent Producer</strong> (Option 1).</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Consumer Idempotency is Feasible</strong>: Downstream consumers of change data (e.g., updating a cache, populating a data warehouse) can often be designed to handle duplicate events idempotently (e.g., using an upsert operation based on primary key).</li>\n<li><strong>Lower Complexity</strong>: Exactly-once semantics require managing Kafka transactions and a distributed commit protocol, significantly increasing the complexity of the <code>EventStreamer</code> and its recovery procedures.</li>\n<li><strong>Performance</strong>: At-least-once has lower latency and higher throughput compared to the two-phase commit of transactional producers.</li>\n<li><strong>Practical Robustness</strong>: The combination of idempotent producer (prevents intra-session duplicates) and deterministic event ID (allows consumers to deduplicate) reduces the duplicate rate to realistically manageable levels (only on producer restart).</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Simpler implementation, better performance, and a well-understood reliability model.</li>\n<li><strong>Negative</strong>: Downstream consumers <strong>must</strong> be designed to be idempotent or include a deduplication layer based on the event&#39;s deterministic ID.</li>\n<li><strong>Mitigation</strong>: The design provides the <code>transactionId</code> and a per-transaction sequence number within <code>ChangeEvent</code>, giving consumers the necessary information for robust deduplication.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>At-Least-Once with Idempotent Producer</td>\n<td>Simpler, performant, prevents <em>some</em> duplicates, aligns with common consumer patterns.</td>\n<td>Consumers must handle duplicates from producer restarts.</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Transactional/Exactly-Once</td>\n<td>Strongest guarantee, no duplicates.</td>\n<td>High complexity, performance overhead, harder to debug and recover.</td>\n<td>No</td>\n</tr>\n<tr>\n<td>At-Most-Once</td>\n<td>Lowest latency, simplest.</td>\n<td>Risk of data loss, unacceptable for CDC.</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"75-adr-topic-partitioning-strategy\">7.5 ADR: Topic Partitioning Strategy</h3>\n<blockquote>\n<p><strong>Decision: Partition by Table and Primary Key Hash</strong></p>\n<ul>\n<li><strong>Context</strong>: Events must be published to Kafka topics in a way that preserves order for changes to the same database row while allowing parallel consumption across different rows. The topic structure must also be manageable and align with consumer needs.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Single Topic, Partition by Table+PK Hash</strong>: All events go to one topic named <code>cdc.events</code>. Partitions are assigned by hashing a key composed of <code>tableName:primaryKey</code>.</li>\n<li><strong>Topic-per-Table, Partition by PK Hash</strong>: Each database table has a dedicated Kafka topic (e.g., <code>cdc.users</code>, <code>cdc.orders</code>). Partitions within that topic are assigned by hashing the primary key.</li>\n<li><strong>Single Partition per Table</strong>: Each table maps to a single Kafka partition (or a single topic with one partition). Guarantees order for all events in a table but offers zero parallelism.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement <strong>Topic-per-Table, Partition by PK Hash</strong> (Option 2).</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Clean Separation of Concerns</strong>: Consumers can subscribe only to the tables they care about, simplifying their filter logic and reducing unnecessary data transfer.</li>\n<li><strong>Independent Scaling</strong>: Consumer groups can scale independently per table. A backlog in the <code>orders</code> topic doesn&#39;t affect consumers of the <code>users</code> topic.</li>\n<li><strong>Administrative Flexibility</strong>: Topic-level settings (retention policy, compaction, partitions) can be tailored to the data characteristics of each table.</li>\n<li><strong>Preserves Ordering</strong>: Hashing the primary key within a table-specific topic guarantees order per row, which is the core requirement.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Positive</strong>: Clean architecture, better scalability, and operational flexibility.</li>\n<li><strong>Negative</strong>: Requires dynamic topic creation/validation (or pre-creation) as new tables are added to the source database. Slightly more complex management.</li>\n<li><strong>Mitigation</strong>: The <code>EventStreamer</code> can include logic to check if a topic exists on first use and create it with predefined settings (e.g., partition count from <code>KafkaConfig.partitionsPerTable</code>).</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Topic-per-Table, Partition by PK Hash</td>\n<td>Clean separation, independent scaling, preserves order.</td>\n<td>Requires topic management.</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Single Topic, Partition by Table+PK Hash</td>\n<td>Single topic to manage.</td>\n<td>All consumers read all data, harder to manage ACLs and quotas.</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Single Partition per Table</td>\n<td>Trivial to implement, strong order.</td>\n<td>No parallelism, severe throughput bottleneck.</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h3 id=\"76-common-pitfalls-in-event-delivery\">7.6 Common Pitfalls in Event Delivery</h3>\n<p>⚠️ <strong>Pitfall: Committing the Offset Before Successful Delivery</strong></p>\n<ul>\n<li><strong>Description</strong>: The <code>LogConnector</code> advances its persisted LSN (offset) immediately after passing a <code>ChangeEvent</code> to the <code>EventStreamer.publish()</code> method, <em>before</em> receiving the Kafka acknowledgment.</li>\n<li><strong>Why it&#39;s Wrong</strong>: If the CDC process crashes after the offset is saved but before Kafka acknowledges the event, the event is lost forever. On restart, the <code>LogConnector</code> will resume from the later LSN, skipping the lost event.</li>\n<li><strong>Fix</strong>: <strong>Always advance the persisted source offset only after the corresponding event is acknowledged by Kafka.</strong> This is implemented in the producer callback, updating <code>lastAckedLsnBySlot</code>, which is then periodically persisted.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring Consumer Lag (No Backpressure)</strong></p>\n<ul>\n<li><strong>Description</strong>: The <code>EventStreamer</code> publishes events as fast as the database produces them, regardless of how quickly consumers are processing them.</li>\n<li><strong>Why it&#39;s Wrong</strong>: If consumers fall behind (e.g., due to a bug or downstream system slowdown), the Kafka topic partitions will fill up. This can cause the producer to block or fail, and eventually lead to the <code>LogConnector</code> being unable to read new transaction log segments because the database&#39;s retention policy forces them to be deleted—<strong>resulting in permanent data loss</strong>.</li>\n<li><strong>Fix</strong>: <strong>Implement a backpressure feedback loop.</strong> Monitor consumer lag (using Kafka&#39;s end-offset and consumer group offset APIs). When lag exceeds a configured threshold, pause the <code>EventStreamer.publish()</code> method, which will eventually cause the <code>LogConnector</code> to stop reading and applying backpressure all the way to the database log reader.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Partitioning by Table Only</strong></p>\n<ul>\n<li><strong>Description</strong>: Using only <code>tableName</code> as the Kafka partition key (e.g., all <code>users</code> events go to partition 1, all <code>orders</code> to partition 2).</li>\n<li><strong>Why it&#39;s Wrong</strong>: This destroys ordering guarantees for individual rows. Two updates to <code>user_id=123</code> could be processed by different consumer instances working on different partitions, leading to race conditions and stale data.</li>\n<li><strong>Fix</strong>: <strong>Always include the primary key value in the partition key.</strong> The key should be <code>tableName:primaryKeyHash</code>. This ensures all events for a specific row are ordered within a single partition.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not Handling Producer Buffer Full Exceptions</strong></p>\n<ul>\n<li><strong>Description</strong>: Relying on the default behavior when the Kafka producer&#39;s internal memory buffer (<code>buffer.memory</code>) is full.</li>\n<li><strong>Why it&#39;s Wrong</strong>: The default behavior for the <code>KafkaProducer.send()</code> method when the buffer is full is to block indefinitely. This can cause a complete stall of the CDC pipeline without clear logging.</li>\n<li><strong>Fix</strong>: <strong>Configure a reasonable <code>max.block.ms</code></strong> (e.g., 60 seconds) on the Kafka producer. This will cause the <code>send()</code> method to throw a <code>TimeoutException</code> after this period, which the <code>EventStreamer</code> can catch, log as a critical error, and trigger a graceful shutdown or alert, rather than hanging silently.</li>\n</ul>\n<h3 id=\"77-implementation-guidance\">7.7 Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Message Broker</td>\n<td>Apache Kafka (single broker for dev)</td>\n<td>Apache Kafka Cluster (with replication)</td>\n</tr>\n<tr>\n<td>Client Library</td>\n<td><code>kafka-clients</code> (Java)</td>\n<td>Confluent Kafka (includes Schema Registry client)</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Log-based lag detection</td>\n<td>Kafka Metrics Reporter + Prometheus/Grafana</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON (with schema ID)</td>\n<td>Apache Avro (with Schema Registry integration)</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended Project File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── core/\n│   │   ├── AppConfig.java\n│   │   ├── ChangeEvent.java\n│   │   └── ...\n│   ├── streaming/\n│   │   ├── EventStreamer.java          # This component's main class\n│   │   ├── KafkaEventPublisher.java    # Kafka-specific implementation\n│   │   ├── StreamerMetrics.java        # Lag monitoring &amp; backpressure logic\n│   │   └── DeliveryCallback.java       # Kafka producer callback handler\n│   ├── serialization/\n│   │   └── EventSerializer.java\n│   └── Main.java\n└── config/\n    └── application.yaml</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code (Kafka Producer Setup):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.streaming;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.apache.kafka.clients.producer.KafkaProducer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.apache.kafka.clients.producer.ProducerConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.apache.kafka.common.serialization.StringSerializer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Properties;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> KafkaProducerFactory</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> KafkaProducer&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">[]> </span><span style=\"color:#B392F0\">createProducer</span><span style=\"color:#E1E4E8\">(AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Properties props </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> Properties</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, config.</span><span style=\"color:#B392F0\">getKafka</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">getBootstrapServers</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.</span><span style=\"color:#B392F0\">getName</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.</span><span style=\"color:#B392F0\">getName</span><span style=\"color:#E1E4E8\">()); </span><span style=\"color:#6A737D\">// We serialize to bytes ourselves</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Critical reliability settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.ACKS_CONFIG, config.</span><span style=\"color:#B392F0\">getKafka</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">getAcks</span><span style=\"color:#E1E4E8\">()); </span><span style=\"color:#6A737D\">// \"all\" for strongest guarantee</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, config.</span><span style=\"color:#B392F0\">getKafka</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">isEnableIdempotence</span><span style=\"color:#E1E4E8\">()); </span><span style=\"color:#6A737D\">// true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE); </span><span style=\"color:#6A737D\">// Retry forever</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">); </span><span style=\"color:#6A737D\">// Required for idempotence when retries > 0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Backpressure &#x26; memory settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.BUFFER_MEMORY_CONFIG, </span><span style=\"color:#79B8FF\">33554432</span><span style=\"color:#E1E4E8\">); </span><span style=\"color:#6A737D\">// 32 MB default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.MAX_BLOCK_MS_CONFIG, </span><span style=\"color:#79B8FF\">60000</span><span style=\"color:#E1E4E8\">); </span><span style=\"color:#6A737D\">// Throw exception after 1 min if buffer full</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Batching for throughput (can be tuned)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.LINGER_MS_CONFIG, </span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(ProducerConfig.BATCH_SIZE_CONFIG, </span><span style=\"color:#79B8FF\">16384</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> KafkaProducer&#x3C;>(props);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.streaming;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.core.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.apache.kafka.clients.producer.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.List;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.ConcurrentHashMap;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.atomic.AtomicBoolean;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> KafkaEventPublisher</span><span style=\"color:#F97583\"> implements</span><span style=\"color:#B392F0\"> EventStreamer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> KafkaProducer&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">[]> producer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> ConcurrentHashMap&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> pendingEvents;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> ConcurrentHashMap&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">> lastAckedLsnBySlot;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> AtomicBoolean backpressureSignal;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> StreamerMetrics metrics;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">(AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Create Kafka producer using KafkaProducerFactory.createProducer(config)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Initialize concurrent maps for pendingEvents and lastAckedLsnBySlot</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Initialize backpressureSignal (set to false)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Initialize StreamerMetrics (start a scheduled thread to check consumer lag)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Load last persisted LSNs from OffsetStore and populate lastAckedLsnBySlot</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> publish</span><span style=\"color:#E1E4E8\">(List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">events</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (ChangeEvent event </span><span style=\"color:#F97583\">:</span><span style=\"color:#E1E4E8\"> events) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Check backpressureSignal. If true, loop with Thread.sleep(100) until false.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Generate partition key: sourceTable + \":\" + hash(primaryKeyValues)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Retrieve SchemaVersion for event.getSchemaVersionId() from cache/registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Serialize event: byte[] payload = EventSerializer.serialize(event, schemaVersion)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Create ProducerRecord with topic (e.g., \"cdc.\" + event.getSourceTable()), partition key, payload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 6: Generate a deterministic event ID (e.g., event.getTransactionId() + \":\" + sequence) and add to pendingEvents map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 7: Call producer.send(record, new DeliveryCallback(eventId, event, this)) with custom callback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 8: Increment in-flight event count; if it exceeds a limit (e.g., 10,000), apply backpressure.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Inner class for handling Kafka acknowledgment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> DeliveryCallback</span><span style=\"color:#F97583\"> implements</span><span style=\"color:#B392F0\"> Callback</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String eventId;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ChangeEvent event;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> KafkaEventPublisher publisher;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        DeliveryCallback</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">eventId</span><span style=\"color:#E1E4E8\">, ChangeEvent </span><span style=\"color:#FFAB70\">event</span><span style=\"color:#E1E4E8\">, KafkaEventPublisher </span><span style=\"color:#FFAB70\">publisher</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            this</span><span style=\"color:#E1E4E8\">.eventId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> eventId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            this</span><span style=\"color:#E1E4E8\">.event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            this</span><span style=\"color:#E1E4E8\">.publisher </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> publisher;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> onCompletion</span><span style=\"color:#E1E4E8\">(RecordMetadata </span><span style=\"color:#FFAB70\">metadata</span><span style=\"color:#E1E4E8\">, Exception </span><span style=\"color:#FFAB70\">exception</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (exception </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 1: Remove eventId from publisher.pendingEvents</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 2: Update publisher.lastAckedLsnBySlot for event's slot.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                //         Important: Only advance the LSN if this event's LSN is the next in sequence for its slot.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                //         This may require maintaining an ordered list or heap of pending LSNs per slot.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 3: Decrement publisher.inFlightEventCount</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 4: Periodically (e.g., every 1000 events), persist the map lastAckedLsnBySlot to OffsetStore.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 1: Log severe error: \"Failed to publish event after retries: \" + exception</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 2: Mark publisher health as DEGRADED or STOPPED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // TODO 3: Trigger an alert (e.g., via logging or metrics)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // The event remains in pendingEvents. A system restart will replay it.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Set a flag to stop accepting new publish() calls</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Call producer.flush() to wait for all pending sends</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: After flush, persist final lastAckedLsnBySlot to OffsetStore</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Call producer.close()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Shutdown the metrics monitoring thread</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Java-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>java.util.concurrent.ConcurrentHashMap</code> for thread-safe maps like <code>pendingEvents</code>.</li>\n<li>The Kafka <code>Producer.send()</code> callback executes on a <strong>Kafka producer I/O thread</strong>. Keep callback logic lightweight to avoid blocking network I/O. Offload complex logic (like updating complex ordered LSN structures) to a separate single-threaded executor if needed.</li>\n<li>Use <code>AtomicBoolean</code> for the <code>backpressureSignal</code> to ensure safe visibility across threads.</li>\n<li>For periodic tasks (like lag monitoring), use a <code>ScheduledExecutorService</code> rather than a plain <code>Thread.sleep</code> loop.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint (Verifying Event Delivery):</strong></p>\n<ol>\n<li><strong>Run Integration Test:</strong> Start a local Kafka broker (e.g., using Docker). Configure the CDC pipeline to connect to it. Perform INSERT/UPDATE/DELETE operations on the source database.</li>\n<li><strong>Expected Output:</strong> Use the Kafka console consumer to inspect the topic: <code>kafka-console-consumer --bootstrap-server localhost:9092 --topic cdc.users --from-beginning</code>. You should see JSON (or Avro) formatted change events.</li>\n<li><strong>Verify Ordering:</strong> Update the same database row multiple times in quick succession. The events in the Kafka topic should appear in the exact same order.</li>\n<li><strong>Verify At-Least-Once:</strong> Kill the CDC process (<code>Ctrl+C</code>) while it&#39;s processing events, then restart it. Check the Kafka topic for duplicate events (should be present, confirming at-least-once). The pipeline should resume from the correct point with no events missing.</li>\n<li><strong>Signs of Trouble:</strong> If no events appear, check Kafka connection logs, producer configuration (especially <code>bootstrap.servers</code> and <code>topic</code> naming). If events are out of order, verify the partition key logic includes the primary key.</li>\n</ol>\n<p><strong>G. Debugging Tips Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No events in Kafka topic.</td>\n<td>1. Producer not connected. 2. Topic doesn&#39;t exist. 3. All sends are failing.</td>\n<td>Check logs for connection errors. Use <code>kafka-topics --list</code> to verify topic exists. Enable producer DEBUG logging.</td>\n<td>Verify <code>bootstrap.servers</code>. Enable auto-topic creation or pre-create the topic.</td>\n</tr>\n<tr>\n<td>Events are lost after CDC restart.</td>\n<td>Source LSN offset persisted <strong>before</strong> Kafka acknowledgment.</td>\n<td>Check the persisted offset file. It should be <strong>behind</strong> the LSN of the last event seen in Kafka.</td>\n<td>Ensure <code>lastAckedLsnBySlot</code> is only updated in the Kafka producer callback.</td>\n</tr>\n<tr>\n<td>CDC pipeline slows to a crawl or stops.</td>\n<td>Backpressure is constantly active due to high consumer lag.</td>\n<td>Check metrics/ logs for consumer lag alerts. Use <code>kafka-consumer-groups</code> command to check lag.</td>\n<td>Investigate and fix the slow consumer. Temporarily increase <code>backpressureThresholdMs</code> for relief.</td>\n</tr>\n<tr>\n<td>Duplicate events for every change.</td>\n<td>Idempotent producer disabled, or producer is restarted frequently without proper offset tracking.</td>\n<td>Check config <code>enable.idempotence</code>. Look for producer restart logs.</td>\n<td>Enable idempotence. Ensure the producer&#39;s <code>transactional.id</code> is stable across restarts if using transactions.</td>\n</tr>\n<tr>\n<td><code>TimeoutException</code> from <code>producer.send()</code>.</td>\n<td>Producer buffer is full, likely due to no backpressure and very slow consumers.</td>\n<td>Check <code>inFlightEventCount</code> and consumer lag. The broker might also be slow.</td>\n<td>Implement or tune backpressure. Increase <code>buffer.memory</code> or <code>max.block.ms</code> as a temporary measure.</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (Schema Evolution &amp; Compatibility)</p>\n</blockquote>\n<h2 id=\"8-component-schema-registry-amp-evolution\">8. Component: Schema Registry &amp; Evolution</h2>\n<p>In a CDC system, the raw data flowing from transaction logs is just bytes without context. The <strong>Schema Registry &amp; Evolution</strong> component provides the essential <em>context</em> by managing the structure of data — the table schemas — and ensuring that changes to that structure (like adding a column) don&#39;t silently break downstream consumers. It acts as a central authority for data shape, versioning schemas, validating changes, and notifying the system when the contract between producers and consumers evolves.</p>\n<h3 id=\"81-mental-model-the-contract-librarian\">8.1 Mental Model: The Contract Librarian</h3>\n<p>Think of the Schema Registry as a <strong>Librarian for Legal Contracts</strong>. Every table in your source database has a &quot;contract&quot; (its schema) that defines the exact terms of data exchange: column names, data types, and optionality. Each time a developer runs an <code>ALTER TABLE</code>, they are proposing an amendment to this contract.</p>\n<p>The Librarian&#39;s (Registry&#39;s) duties are:</p>\n<ol>\n<li><strong>Archival</strong>: Store every ratified version of every contract, meticulously labeled and indexed.</li>\n<li><strong>Validation (Legal Review)</strong>: When a new amendment (schema change) is proposed, the librarian checks it against compatibility rules (e.g., &quot;Can existing readers still understand data written under the new contract?&quot;). They reject breaking changes.</li>\n<li><strong>Distribution of Updates</strong>: Once a new version is accepted and stored, the librarian stamps a new version number on it and sends out a formal notification (a <code>SchemaChangeEvent</code>) to all interested parties (consumers) so they can update their local copies of the contract.</li>\n<li><strong>Reference Service</strong>: When a data packet (<code>ChangeEvent</code>) arrives at a consumer, it carries a reference to a specific contract version (e.g., &quot;schema v2&quot;). The consumer can ask the librarian, &quot;Please give me the full text of contract v2 for table <code>users</code>&quot; to correctly interpret the data.</li>\n</ol>\n<p>This model highlights the component&#39;s core responsibilities: stateful storage, rule-based validation, and change notification, which are critical for a system where data producers (the database) and consumers (applications) evolve independently.</p>\n<h3 id=\"82-interface-and-versioning\">8.2 Interface and Versioning</h3>\n<p>The Schema Registry exposes a well-defined API for schema lifecycle management. Its primary data structure is the <code>SchemaVersion</code>, which encapsulates a snapshot of a table&#39;s structure at a point in time.</p>\n<p><strong>Core Data Structure: <code>SchemaVersion</code></strong></p>\n<p>This object represents a single, immutable version of a table&#39;s schema. Once registered, it is never modified.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>schemaId</code></td>\n<td align=\"left\"><code>String</code></td>\n<td align=\"left\">A globally unique identifier for this specific schema version. Typically formatted as <code>&lt;tableName&gt;-v&lt;versionNumber&gt;</code> (e.g., <code>public.users-v3</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>tableName</code></td>\n<td align=\"left\"><code>String</code></td>\n<td align=\"left\">The fully-qualified name of the source database table (e.g., <code>public.users</code>, <code>inventory.products</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>version</code></td>\n<td align=\"left\"><code>Integer</code></td>\n<td align=\"left\">A monotonically increasing integer version number, scoped to the <code>tableName</code>. Starts at 1.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>columnDefinitions</code></td>\n<td align=\"left\"><code>Map&lt;String, ColumnType&gt;</code></td>\n<td align=\"left\">The core schema definition. Maps column names to their type metadata.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>compatibilityMode</code></td>\n<td align=\"left\"><code>String</code></td>\n<td align=\"left\">The compatibility rule applied when this version was registered (e.g., <code>BACKWARD</code>, <code>FORWARD</code>, <code>FULL</code>). This influences which future changes are allowed.</td>\n</tr>\n</tbody></table>\n<p><strong>Supporting Data Structure: <code>ColumnType</code></strong></p>\n<p>This nested object provides detailed metadata for a single column, essential for accurate serialization and compatibility checking.</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Field</th>\n<th align=\"left\">Type</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>sqlType</code></td>\n<td align=\"left\"><code>String</code></td>\n<td align=\"left\">The database-native type name (e.g., <code>VARCHAR(255)</code>, <code>INT</code>, <code>TIMESTAMP WITH TIME ZONE</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>javaClass</code></td>\n<td align=\"left\"><code>Class&lt;?&gt;</code></td>\n<td align=\"left\">The corresponding Java class for runtime serialization/deserialization (e.g., <code>String.class</code>, <code>Long.class</code>).</td>\n</tr>\n<tr>\n<td align=\"left\"><code>nullable</code></td>\n<td align=\"left\"><code>boolean</code></td>\n<td align=\"left\">Indicates if the column can contain <code>NULL</code> values.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>defaultValue</code></td>\n<td align=\"left\"><code>Object</code></td>\n<td align=\"left\">The default value for the column, used during deserialization if a field is missing in old data. Can be <code>null</code>.</td>\n</tr>\n</tbody></table>\n<p><strong>Registry Interface (<code>SchemaRegistry</code>)</strong></p>\n<p>The component&#39;s functionality is accessed via the following interface. This defines the contract for both the registry implementation and its clients (like the <code>ChangeEventBuilder</code>).</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Method</th>\n<th align=\"left\">Parameters</th>\n<th align=\"left\">Returns</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><code>registerSchema</code></td>\n<td align=\"left\"><code>tableName: String</code>, <code>newColumnDefs: Map&lt;String, ColumnType&gt;</code>, <code>compatibilityMode: String</code></td>\n<td align=\"left\"><code>SchemaVersion</code></td>\n<td align=\"left\">The core registration workflow. Takes a proposed new schema for a table, validates it against the latest registered version using the specified <code>compatibilityMode</code>, and if valid, stores it as a new version, incrementing the version number. Returns the newly registered <code>SchemaVersion</code>.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>getSchemaById</code></td>\n<td align=\"left\"><code>schemaId: String</code></td>\n<td align=\"left\"><code>SchemaVersion</code></td>\n<td align=\"left\">Retrieves a specific schema version by its unique <code>schemaId</code>. Used by consumers to deserialize a <code>ChangeEvent</code> that references a particular version.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>getLatestSchema</code></td>\n<td align=\"left\"><code>tableName: String</code></td>\n<td align=\"left\"><code>SchemaVersion</code></td>\n<td align=\"left\">Returns the most recent schema version for a given table. Used by the <code>ChangeEventBuilder</code> to attach schema info to new events and by the registry&#39;s own validation logic.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>checkCompatibility</code></td>\n<td align=\"left\"><code>newColumnDefs: Map&lt;String, ColumnType&gt;</code>, <code>existingSchema: SchemaVersion</code>, <code>mode: String</code></td>\n<td align=\"left\"><code>boolean</code></td>\n<td align=\"left\">Performs a dry-run compatibility check. Returns <code>true</code> if the proposed <code>newColumnDefs</code> are compatible with the <code>existingSchema</code> according to the rules of the specified <code>mode</code>. Does not register a new version.</td>\n</tr>\n<tr>\n<td align=\"left\"><code>getSchemaHistory</code></td>\n<td align=\"left\"><code>tableName: String</code></td>\n<td align=\"left\"><code>List&lt;SchemaVersion&gt;</code></td>\n<td align=\"left\">Returns all schema versions for a table, in chronological order. Useful for audit and migration planning.</td>\n</tr>\n</tbody></table>\n<p><strong>Versioning and Lifecycle Flow</strong></p>\n<ol>\n<li><strong>Initial Registration</strong>: When the CDC pipeline starts for the first time, it must fetch the current schema of each captured table from the source database and call <code>registerSchema</code> with a special flag (e.g., <code>INITIAL</code> mode) to create version 1.</li>\n<li><strong>Event Tagging</strong>: Every <code>ChangeEvent</code> produced by the <code>ChangeEventBuilder</code> includes a <code>schemaVersionId</code> field (e.g., <code>public.users-v2</code>), creating a hard link between the data and its schema definition.</li>\n<li><strong>Schema Change Detection</strong>: The <code>LogConnector</code> and <code>Parser</code> detect Data Definition Language (DDL) events (e.g., <code>ALTER TABLE...</code>). These are passed to the registry as a proposal for a new schema.</li>\n<li><strong>Validation &amp; Registration</strong>: The registry performs <code>checkCompatibility</code>. If it passes, <code>registerSchema</code> is called, creating a new <code>SchemaVersion</code>. If it fails, the change is rejected, and an alert is raised (the DDL may need to be re-evaluated).</li>\n<li><strong>Change Notification</strong>: Upon successful registration, the registry (or the <code>ChangeEventBuilder</code>) emits a special <code>SchemaChangeEvent</code> to a dedicated topic. This event contains the old and new <code>schemaId</code>, allowing consumers to lazily fetch the new schema when they encounter data with the new version ID.</li>\n</ol>\n<h3 id=\"83-adr-choosing-a-schema-compatibility-mode\">8.3 ADR: Choosing a Schema Compatibility Mode</h3>\n<blockquote>\n<p><strong>Decision: Enforce Backward Compatibility by Default</strong></p>\n<ul>\n<li><strong>Context</strong>: The CDC system serves multiple, independently deployed consumer applications. Their code deserializes <code>ChangeEvent</code> data based on a known schema. When the source database schema evolves, we must decide which types of changes are safe to allow without breaking existing consumers.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Backward Compatibility (Recommended Default)</strong>: New schema can read data written with the old schema. This means consumers using the <em>new</em> schema version can still process events written with an <em>old</em> schema. In practice, you can only add optional fields (nullable or with defaults) and cannot remove/rename fields or make required fields optional.</li>\n<li><strong>Forward Compatibility</strong>: Old schema can read data written with the new schema. Consumers using an <em>old</em> schema version can process events written with a <em>new</em> schema. This allows removing fields and making optional fields required, but prevents adding new required fields.</li>\n<li><strong>Full Compatibility</strong>: A combination of both backward and forward compatibility. The strictest mode, allowing only safe changes like adding/removing optional fields.</li>\n<li><strong>No Compatibility Checking</strong>: Allow any schema change, accepting that consumers may break. This is simple but operationally dangerous.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: The primary compatibility mode for the Schema Registry will be <strong>Backward Compatibility</strong>.</li>\n<li><strong>Rationale</strong>:<ul>\n<li><strong>Consumer Deployment Lag</strong>: In a real-world scenario, it is far more common for consumers to be updated <em>after</em> the producer (database) changes. Backward compatibility ensures that an updated consumer (with new logic for a new column) can still process old data in the pipeline during a rolling deployment, preventing downtime.</li>\n<li><strong>Practical Schema Evolution</strong>: The most common, safe schema change is &quot;adding a new nullable column with a default value.&quot; This is backward-compatible and matches typical application evolution (e.g., adding a <code>last_login_at</code> timestamp column).</li>\n<li><strong>Industry Practice</strong>: Systems like Apache Avro and Confluent Schema Registry default to backward compatibility for similar reasons, making it a well-understood pattern.</li>\n</ul>\n</li>\n<li><strong>Consequences</strong>:<ul>\n<li><strong>Enables Safe Rolling Updates</strong>: Producers (database) can evolve first. New consumers can be deployed later, reading both old and new events seamlessly.</li>\n<li><strong>Restricts Certain Changes</strong>: Renaming or deleting a column, or changing a column&#39;s data type in an incompatible way (e.g., <code>INT</code> to <code>VARCHAR</code>) will be rejected by the registry. These changes require a coordinated &quot;big bang&quot; update or a more complex migration strategy (like writing to a new table/topic).</li>\n<li><strong>Schema Proliferation</strong>: Over time, many schema versions will accumulate as columns are added. This is managed by the registry and is a normal part of operation.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<p><strong>Compatibility Mode Comparison</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Mode</th>\n<th align=\"left\">Allows (Example)</th>\n<th align=\"left\">Disallows (Example)</th>\n<th align=\"left\">Best For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Backward</strong> (OUR CHOICE)</td>\n<td align=\"left\">Adding a nullable column (<code>ALTER TABLE users ADD COLUMN middle_name VARCHAR(100) NULL;</code>).</td>\n<td align=\"left\">Removing a column, changing <code>VARCHAR(100)</code> to <code>VARCHAR(50)</code>.</td>\n<td align=\"left\">Evolving producers first; the most common, safe evolution pattern.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Forward</strong></td>\n<td align=\"left\">Removing a column, making a nullable column <code>NOT NULL</code> (if default exists).</td>\n<td align=\"left\">Adding a new required (<code>NOT NULL</code>) column without a default.</td>\n<td align=\"left\">Evolving consumers first; less common in database-driven CDC.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Full</strong></td>\n<td align=\"left\">Adding an optional column, removing an optional column.</td>\n<td align=\"left\">Any change that breaks either backward or forward compatibility.</td>\n<td align=\"left\">Highly controlled environments where all changes must be non-breaking.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>None</strong></td>\n<td align=\"left\">Any DDL statement.</td>\n<td align=\"left\">(No validation).</td>\n<td align=\"left\">Prototyping or environments where breaking changes are acceptable.</td>\n</tr>\n</tbody></table>\n<h3 id=\"84-common-pitfalls-in-schema-evolution\">8.4 Common Pitfalls in Schema Evolution</h3>\n<p>⚠️ <strong>Pitfall: Silent Data Loss from Type Narrowing</strong></p>\n<ul>\n<li><strong>The Mistake</strong>: Allowing an <code>ALTER TABLE</code> that changes a column&#39;s type in a lossy way, such as <code>BIGINT</code> to <code>INT</code> or <code>VARCHAR(255)</code> to <code>VARCHAR(10)</code>, without proper validation.</li>\n<li><strong>Why It&#39;s Wrong</strong>: The CDC system may successfully serialize the change event, but when a consumer with the new schema deserializes an old value (e.g., the number 5,000,000,000 into an <code>INT</code> field), it will overflow or truncate, corrupting data silently. This violates the CDC system&#39;s core promise of faithful replication.</li>\n<li><strong>How to Fix</strong>: The <code>checkCompatibility</code> logic must include <strong>type coercion safety checks</strong>. It should understand type families (e.g., integer types, string types) and only allow changes where every possible value in the old type can be represented in the new type without loss. For complex changes, the check should fail, forcing the use of a multi-step migration (e.g., add new column, backfill, switch consumers, drop old column).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Treating the Registry as Stateless</strong></p>\n<ul>\n<li><strong>The Mistake</strong>: Implementing the <code>SchemaRegistry</code> as an in-memory component within the CDC pipeline process, losing all registered schemas on restart.</li>\n<li><strong>Why It&#39;s Wrong</strong>: On restart, the pipeline cannot correctly tag events with <code>schemaVersionId</code>, and consumers cannot resolve historical event schemas. This breaks the entire versioning system.</li>\n<li><strong>How to Fix</strong>: The registry&#39;s state (the <code>SchemaVersion</code> objects) must be <strong>durably persisted</strong>. The implementation guidance provides a simple file-based starter. For production, this would be backed by a database (like PostgreSQL) or a dedicated service (like Confluent Schema Registry).</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring DDL Events in Transaction Logs</strong></p>\n<ul>\n<li><strong>The Mistake</strong>: The <code>LogParser</code> is configured to only parse <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> (DML) events, silently skipping <code>CREATE TABLE</code>, <code>ALTER TABLE</code> (DDL) events.</li>\n<li><strong>Why It&#39;s Wrong</strong>: The pipeline&#39;s view of table schema becomes stale. Newly added columns will not appear in <code>ChangeEvent</code>s, and the registry will not know about the change, causing a mismatch between the actual database state and the CDC stream.</li>\n<li><strong>How to Fix</strong>: Ensure the <code>LogParser</code> implementation for your database (PostgreSQL logical decoding, MySQL binlog) is configured to capture DDL events. These events must be routed to the <code>SchemaRegistry</code> for processing, triggering the registration flow shown in the diagram.</li>\n</ul>\n<p><img src=\"/api/project/cdc-system/architecture-doc/asset?path=diagrams%2Fschema-evolution-flowchart.svg\" alt=\"Flowchart: Handling a Schema Change (ALTER TABLE)\"></p>\n<p>⚠️ <strong>Pitfall: Forgetting to Notify Consumers of Schema Changes</strong></p>\n<ul>\n<li><strong>The Mistake</strong>: The registry registers a new schema version but does not emit any notification. Consumers only discover the new schema when they receive a <code>ChangeEvent</code> with an unknown <code>schemaVersionId</code> and have to query the registry reactively.</li>\n<li><strong>Why It&#39;s Wrong</strong>: While the system still works, it introduces latency and uncertainty. Consumers may fail temporarily if they cannot fetch the schema. Proactive notification allows consumers to pre-fetch and warm up their deserializers, leading to smoother operations.</li>\n<li><strong>How to Fix</strong>: The <code>registerSchema</code> method, upon successful registration, should trigger the publication of a <code>SchemaChangeEvent</code> to a dedicated Kafka topic (e.g., <code>_schemas</code>). Consumers subscribe to this topic to be alerted of relevant changes.</li>\n</ul>\n<h3 id=\"85-implementation-guidance\">8.5 Implementation Guidance</h3>\n<p>This section provides concrete Java code to build a functional, file-based <code>SchemaRegistry</code>. It&#39;s designed for learning and can be replaced with a more robust backend (like HTTP-based Confluent Schema Registry) for production.</p>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Component</th>\n<th align=\"left\">Simple Option (Learning)</th>\n<th align=\"left\">Advanced Option (Production)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\"><strong>Schema Storage</strong></td>\n<td align=\"left\">Local JSON files (one per <code>SchemaVersion</code>). Easy to inspect and debug.</td>\n<td align=\"left\">Dedicated service (Confluent Schema Registry) or a database table (PostgreSQL). Provides high availability, REST API, and advanced features.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Compatibility Logic</strong></td>\n<td align=\"left\">Custom Java rules implementing backward compatibility checks.</td>\n<td align=\"left\">Leverage the Avro or Protobuf schema compatibility libraries, which are battle-tested.</td>\n</tr>\n<tr>\n<td align=\"left\"><strong>Change Notification</strong></td>\n<td align=\"left\">Writing a special <code>SchemaChangeEvent</code> to the main CDC Kafka topic.</td>\n<td align=\"left\">Using a dedicated <code>_schemas</code> topic and the Confluent Schema Registry&#39;s REST hooks.</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── Main.java\n│   ├── pipeline/\n│   │   ├── CdcPipeline.java\n│   │   ├── LogConnector.java\n│   │   ├── ChangeEventBuilder.java\n│   │   └── EventStreamer.java\n│   ├── schema/\n│   │   ├── SchemaRegistry.java           # Interface\n│   │   ├── FileBasedSchemaRegistry.java  # Simple implementation\n│   │   ├── SchemaVersion.java            # Data class\n│   │   ├── ColumnType.java               # Data class\n│   │   ├── CompatibilityChecker.java     # Logic for validation\n│   │   └── SchemaChangeEvent.java        # Notification POJO\n│   ├── serialization/\n│   │   └── EventSerializer.java\n│   └── config/\n│       └── AppConfig.java\n├── src/main/resources/\n│   └── application.yaml\n└── schemas/                              # Directory for storing schema JSON files\n    ├── public.users-v1.json\n    ├── public.users-v2.json\n    └── inventory.products-v1.json</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>First, here is the complete, ready-to-use <code>ColumnType</code> and <code>SchemaVersion</code> data classes.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/main/java/com/cdc/schema/ColumnType.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.schema;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ColumnType</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String sqlType;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Class&#x3C;</span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\">> javaClass;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#E1E4E8\"> nullable;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Object defaultValue;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> ColumnType</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">sqlType</span><span style=\"color:#E1E4E8\">, Class&#x3C;</span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">javaClass</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">boolean</span><span style=\"color:#FFAB70\"> nullable</span><span style=\"color:#E1E4E8\">, Object </span><span style=\"color:#FFAB70\">defaultValue</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.sqlType </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sqlType;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.javaClass </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> javaClass;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.nullable </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nullable;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.defaultValue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultValue;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters, equals, hashCode, toString omitted for brevity.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/main/java/com/cdc/schema/SchemaVersion.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.schema;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Map;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> SchemaVersion</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String schemaId;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String tableName;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> version;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ColumnType</span><span style=\"color:#E1E4E8\">> columnDefinitions;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String compatibilityMode;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> SchemaVersion</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">schemaId</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> version</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ColumnType</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">columnDefinitions</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">compatibilityMode</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.schemaId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schemaId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.tableName </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tableName;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.version </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> version;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.columnDefinitions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Map.</span><span style=\"color:#B392F0\">copyOf</span><span style=\"color:#E1E4E8\">(columnDefinitions); </span><span style=\"color:#6A737D\">// Defensive copy</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.compatibilityMode </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> compatibilityMode;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Getters, equals, hashCode, toString omitted for brevity.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>Below is the <code>FileBasedSchemaRegistry</code> implementation skeleton. The TODOs guide you through the key algorithms for registration and compatibility checking.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/main/java/com/cdc/schema/FileBasedSchemaRegistry.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.schema;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.config.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.fasterxml.jackson.databind.ObjectMapper;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.File;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.IOException;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.nio.file.Files;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.nio.file.Path;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.nio.file.Paths;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> FileBasedSchemaRegistry</span><span style=\"color:#F97583\"> implements</span><span style=\"color:#B392F0\"> SchemaRegistry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Path schemaStorageDir;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ObjectMapper objectMapper </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ObjectMapper</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">SchemaVersion</span><span style=\"color:#E1E4E8\">> cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>();</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> FileBasedSchemaRegistry</span><span style=\"color:#E1E4E8\">(AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.schemaStorageDir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Paths.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(config.</span><span style=\"color:#B392F0\">getSchemaConfig</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">getRegistryUrl</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Files.</span><span style=\"color:#B392F0\">createDirectories</span><span style=\"color:#E1E4E8\">(schemaStorageDir);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (IOException </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> RuntimeException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Failed to create schema storage directory\"</span><span style=\"color:#E1E4E8\">, e);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        loadExistingSchemasIntoCache</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> loadExistingSchemasIntoCache</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: List all JSON files in schemaStorageDir</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: For each file, read its contents and deserialize into a SchemaVersion object using objectMapper.readValue(file, SchemaVersion.class)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Put each deserialized SchemaVersion into the 'cache' map, keyed by its schemaId</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> SchemaVersion </span><span style=\"color:#B392F0\">registerSchema</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                        Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ColumnType</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">newColumnDefs</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                        String </span><span style=\"color:#FFAB70\">compatibilityMode</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Get the latest existing schema for this table (call getLatestSchema(tableName))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SchemaVersion latest </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> getLatestSchema</span><span style=\"color:#E1E4E8\">(tableName);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: If a latest schema exists, perform compatibility check:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //    if (!checkCompatibility(newColumnDefs, latest, compatibilityMode)) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //        throw new SchemaCompatibilityException(\"Proposed schema is not compatible\");</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Determine the new version number.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //    int newVersion = (latest == null) ? 1 : latest.getVersion() + 1;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Create a new SchemaVersion object with a generated schemaId (e.g., tableName + \"-v\" + newVersion)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Persist this new SchemaVersion to a file (e.g., schemaId + \".json\") using objectMapper.writeValue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: Update the in-memory cache with the new SchemaVersion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: (Optional) Emit a SchemaChangeEvent notification (e.g., via a callback or event bus)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 8: Return the newly created SchemaVersion object</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> SchemaVersion </span><span style=\"color:#B392F0\">getSchemaById</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">schemaId</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Check the in-memory cache first. Return if found.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: If not in cache, attempt to load from the corresponding JSON file in schemaStorageDir.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: If the file doesn't exist, throw a SchemaNotFoundException.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Cache and return the loaded schema.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> SchemaVersion </span><span style=\"color:#B392F0\">getLatestSchema</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Filter the cache values (or files) to find all schemas for the given tableName.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Among those, find the one with the highest version number.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Return it (or null if none exist).</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#B392F0\"> checkCompatibility</span><span style=\"color:#E1E4E8\">(Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ColumnType</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">newColumnDefs</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                      SchemaVersion </span><span style=\"color:#FFAB70\">existingSchema</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                      String </span><span style=\"color:#FFAB70\">mode</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (existingSchema </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// First version is always compatible</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"NONE\"</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">equals</span><span style=\"color:#E1E4E8\">(mode)) </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">ColumnType</span><span style=\"color:#E1E4E8\">> oldDefs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> existingSchema.</span><span style=\"color:#B392F0\">getColumnDefinitions</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Implement backward compatibility rules (the default choice from ADR 8.3)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Rule 1: New columns can be added only if they are nullable (nullable==true) OR have a non-null defaultValue.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //         Iterate through newColumnDefs keys not present in oldDefs and check this condition.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Rule 2: Existing columns cannot be removed. All keys in oldDefs must be present in newColumnDefs.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Rule 3: The data type of an existing column cannot change in an incompatible way.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //         For columns present in both, compare ColumnType. For learning, a simple equals check on sqlType is a start.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //         For advanced implementation, you would need a type coercion matrix (e.g., INT -> BIGINT is safe, but not vice-versa).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: If all rules pass, return true. If any rule fails, return false.</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">SchemaVersion</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getSchemaHistory</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Filter cache values for the tableName, sort by version ascending, and return as a List.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> List.</span><span style=\"color:#B392F0\">of</span><span style=\"color:#E1E4E8\">(); </span><span style=\"color:#6A737D\">// Placeholder</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li><strong>Immutability</strong>: Make <code>SchemaVersion</code> and <code>ColumnType</code> immutable (final fields, no setters). This guarantees that a registered schema can never be altered after the fact, which is crucial for consistency.</li>\n<li><strong>Jackson for JSON</strong>: Use the Jackson library (<code>ObjectMapper</code>) for simple JSON serialization/deserialization of <code>SchemaVersion</code> objects to files. Remember to register the <code>JavaTimeModule</code> if you add timestamp fields.</li>\n<li><strong>Concurrent Access</strong>: The <code>FileBasedSchemaRegistry</code> may be accessed by multiple threads (e.g., the main pipeline thread and a consumer&#39;s fetch request). Use <code>ConcurrentHashMap</code> for the cache and consider synchronization for the <code>registerSchema</code> method to prevent race conditions when creating a new version.</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong>\nTo verify your Schema Registry implementation for Milestone 3:</p>\n<ol>\n<li><strong>Run Unit Tests</strong>: Create a JUnit test class <code>FileBasedSchemaRegistryTest</code>.</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">    mvn</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -Dtest=FileBasedSchemaRegistryTest</span></span></code></pre></div>\n<ol start=\"2\">\n<li><strong>Expected Behavior</strong>:<ul>\n<li>Test <code>registerSchema</code> for a new table creates <code>my_table-v1.json</code>.</li>\n<li>Test adding a nullable column passes compatibility and creates <code>my_table-v2.json</code>.</li>\n<li>Test removing a column fails compatibility with a clear exception.</li>\n<li>Test <code>getSchemaById</code> correctly retrieves a previously saved version.</li>\n</ul>\n</li>\n<li><strong>Integration Verification</strong>: Start your CDC pipeline with a test PostgreSQL database. Run a simple <code>ALTER TABLE ADD COLUMN ...</code>. Check that:<ul>\n<li>A new schema file is created in the <code>schemas/</code> directory.</li>\n<li>Subsequent <code>ChangeEvent</code>s from that table include the new <code>schemaVersionId</code>.</li>\n<li>(Optional) A <code>SchemaChangeEvent</code> is published to your Kafka topic.</li>\n</ul>\n</li>\n</ol>\n<p><strong>G. Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">Symptom</th>\n<th align=\"left\">Likely Cause</th>\n<th align=\"left\">How to Diagnose</th>\n<th align=\"left\">Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">Consumer fails with &quot;Unknown schemaId&quot;</td>\n<td align=\"left\">The registry did not persist the schema, or the <code>schemaVersionId</code> in the event is malformed.</td>\n<td align=\"left\">Check the <code>schemas/</code> directory for the expected JSON file. Inspect the <code>schemaVersionId</code> field in a raw <code>ChangeEvent</code> using <code>kafka-console-consumer</code>.</td>\n<td align=\"left\">Ensure <code>registerSchema</code> successfully writes the file and that the <code>ChangeEventBuilder</code> uses the <code>schemaId</code> returned by the registry.</td>\n</tr>\n<tr>\n<td align=\"left\">New column data is missing from events after <code>ALTER TABLE</code>.</td>\n<td align=\"left\">DDL event not being captured or processed.</td>\n<td align=\"left\">Check the logs of your <code>LogConnector</code>/<code>Parser</code> for signs of DDL parsing. Verify the database replication slot/output plugin is configured for DDL.</td>\n<td align=\"left\">Ensure your database-specific parser is set up to decode DDL statements and route them to <code>SchemaRegistry.registerSchema</code>.</td>\n</tr>\n<tr>\n<td align=\"left\">Compatibility check passes but consumers break.</td>\n<td align=\"left\">The compatibility rules are too lenient (e.g., missing type coercion check).</td>\n<td align=\"left\">Manually compare the <code>sqlType</code> of a changed column between old and new schema files. Test if a sample extreme value (e.g., a large number, long string) would be lost.</td>\n<td align=\"left\">Strengthen the <code>checkCompatibility</code> logic, particularly the type comparison, to be more conservative. Implement a type safety matrix.</td>\n</tr>\n<tr>\n<td align=\"left\">Registry loses all schemas on restart.</td>\n<td align=\"left\">Schemas are only stored in memory (<code>cache</code>) not persisted to disk.</td>\n<td align=\"left\">The <code>schemas/</code> directory is empty or missing. The <code>loadExistingSchemasIntoCache</code> method might be failing silently.</td>\n<td align=\"left\">Check for IO exceptions during <code>loadExistingSchemasIntoCache</code>. Add logging. Ensure the <code>schemaStorageDir</code> path in config is correct and writable.</td>\n</tr>\n</tbody></table>\n<h2 id=\"9-interactions-and-data-flow\">9. Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (log parsing and event construction), Milestone 2 (event streaming and delivery), Milestone 3 (schema evolution handling)</p>\n</blockquote>\n<p>This section traces the complete journey of a database change as it transforms from a binary log entry to a consumable event in a downstream system. Understanding these interaction flows is critical for debugging, performance optimization, and ensuring the system&#39;s reliability guarantees hold end-to-end. We&#39;ll examine two primary scenarios: normal data manipulation operations (INSERT, UPDATE, DELETE) and schema evolution events (ALTER TABLE).</p>\n<h3 id=\"91-normal-operation-flow\">9.1 Normal Operation Flow</h3>\n<p><strong>Mental Model: The Assembly Line Tour</strong>\nImagine our CDC pipeline as a specialized factory assembly line. Raw materials (transaction log entries) arrive from the database warehouse. The first station (Log Connector) unpacks and inspects these materials. The second station (Change Event Builder) assembles the parts into finished products (ChangeEvents), ensuring all pieces from the same transaction box are grouped together. The third station (Event Streamer) packages and ships these products via a reliable courier service (Kafka), which guarantees delivery even if trucks break down. Finally, consumers receive these packages at their loading docks. This tour follows a single product from raw material to delivery.</p>\n<p>The normal operation flow encompasses the most common path through the system: a data manipulation language (DML) operation like <code>INSERT INTO users (id, name) VALUES (123, &#39;Alice&#39;)</code> that successfully commits in the source database and must be delivered to downstream consumers.</p>\n<p><strong>End-to-End Sequence</strong>\nThe following sequence diagram illustrates the complete flow. Each numbered step corresponds to a critical interaction described in detail below.</p>\n<p><img src=\"/api/project/cdc-system/architecture-doc/asset?path=diagrams%2Fevent-delivery-sequence.svg\" alt=\"Sequence Diagram: Normal Event Delivery\"></p>\n<p><strong>Detailed Step-by-Step Walkthrough</strong></p>\n<ol>\n<li><p><strong>Database Transaction Commit</strong></p>\n<ul>\n<li><strong>Actor</strong>: Application using the source database</li>\n<li><strong>Action</strong>: The application issues a <code>COMMIT</code> statement for a transaction that includes one or more DML operations.</li>\n<li><strong>Database Internal</strong>: The database&#39;s transaction manager ensures atomicity by writing all changes to the Write-Ahead Log (WAL) or binlog first, then marking the transaction as committed in the transaction log. This creates a durable record of the change with a unique <strong>log sequence number (LSN)</strong> that serves as a precise timestamp in the log stream.</li>\n</ul>\n</li>\n<li><p><strong>Log Connector Polling and Capture</strong></p>\n<ul>\n<li><strong>Trigger</strong>: The <code>LogConnector.workerThread</code> continuously polls the database&#39;s logical decoding interface (PostgreSQL&#39;s <code>pg_logical_slot_get_changes</code> or MySQL&#39;s binary log stream).</li>\n<li><strong>Mechanism</strong>: Using the <code>lastProcessedLSN</code> from the <code>OffsetStore</code>, the connector requests all changes after that position. The database returns a batch of raw log entries in their internal binary format.</li>\n<li><strong>Data Transformation</strong>: Each binary entry passes through the database-specific <code>LogParser</code>, which decodes it into a structured <code>RawLogEntry</code> containing the table name, operation type, and the actual row data (both old and new values for updates).</li>\n<li><strong>State Update</strong>: The connector immediately persists the newest LSN to the <code>OffsetStore.save()</code> to record progress, ensuring no data loss if the connector crashes.</li>\n</ul>\n</li>\n<li><p><strong>Change Event Construction</strong></p>\n<ul>\n<li><strong>Input</strong>: The <code>LogConnector.getNextBatch()</code> returns a list of <code>RawLogEntry</code> objects to the main pipeline thread.</li>\n<li><strong>Transaction Grouping</strong>: The pipeline calls <code>ChangeEventBuilder.processBatch()</code> with these entries. The builder maintains an <code>activeTransactions</code> map keyed by <code>transactionId</code>. As entries arrive, they&#39;re added to the appropriate <code>TransactionState.operations</code> list.</li>\n<li><strong>Deduplication</strong>: For each operation, the builder checks the <code>lastOperationByKey</code> map within the transaction state. If a newer operation for the same primary key already exists in the same transaction (a &quot;hot row&quot; updated multiple times), it replaces the previous operation, ensuring only the final state is emitted.</li>\n<li><strong>Commit Detection</strong>: When a <code>RawLogEntry</code> with a <code>COMMIT</code> marker is processed, the builder calls <code>buildEventsForTransaction()</code> for that transaction. This method:<ol>\n<li>Converts each <code>TransactionOperation</code> to a <code>ChangeEvent</code> with a <code>deterministic event ID</code> generated via <code>generateEventId()</code>.</li>\n<li>Attaches the latest <code>SchemaVersion</code> for the table from the schema cache.</li>\n<li>Sets the <code>commitTimestamp</code> from the transaction&#39;s commit record.</li>\n</ol>\n</li>\n<li><strong>Output</strong>: A list of complete <code>ChangeEvent</code> objects, ordered by their operation sequence within the transaction, is returned.</li>\n</ul>\n</li>\n<li><p><strong>Event Serialization and Publishing</strong></p>\n<ul>\n<li><strong>Serialization</strong>: Each <code>ChangeEvent</code> is serialized to bytes using <code>EventSerializer.serialize()</code>, which encodes the event data according to the schema (e.g., Avro, JSON with schema ID).</li>\n<li><strong>Partition Assignment</strong>: The <code>KafkaEventPublisher</code> computes a partition key: typically <code>tableName + &quot;:&quot; + primaryKeyHash</code>. This ensures all events for the same database row go to the same Kafka partition, preserving order.</li>\n<li><strong>Reliable Send</strong>: The publisher calls <code>KafkaProducer.send()</code> with the serialized bytes, registering a <code>DeliveryCallback</code> for each event. The producer is configured with <code>idempotence=true</code> and <code>acks=all</code> to prevent duplicates and ensure writes are replicated.</li>\n<li><strong>In-flight Tracking</strong>: The event is added to <code>pendingEvents</code> map (keyed by <code>eventId</code>) until the callback fires.</li>\n</ul>\n</li>\n<li><p><strong>Kafka Acknowledgment and Offset Management</strong></p>\n<ul>\n<li><strong>Broker Replication</strong>: Kafka leaders replicate the message to follower replicas based on the <code>replicationFactor</code>. Once all in-sync replicas acknowledge, the broker sends a success response.</li>\n<li><strong>Callback Execution</strong>: The <code>DeliveryCallback.onCompletion()</code> is invoked by the producer thread. On success:<ol>\n<li>The event is removed from <code>pendingEvents</code>.</li>\n<li>The <code>lastAckedLsnBySlot</code> for the replication slot is updated to the event&#39;s source LSN.</li>\n<li>The <code>inFlightEventCount</code> is decremented.</li>\n</ol>\n</li>\n<li><strong>Backpressure Signal</strong>: If <code>inFlightEventCount</code> exceeds a threshold (e.g., 10,000), the <code>backpressureSignal</code> is set to <code>true</code>, causing the <code>LogConnector</code> to pause reading.</li>\n</ul>\n</li>\n<li><p><strong>Consumer Processing</strong></p>\n<ul>\n<li><strong>Poll Loop</strong>: Downstream consumers run a continuous poll loop, fetching events from their assigned Kafka partitions.</li>\n<li><strong>Deserialization</strong>: Each consumer uses the <code>schemaVersionId</code> in the event (or the Confluent Schema Registry&#39;s wire format) to fetch the appropriate <code>SchemaVersion</code> and deserialize the bytes using <code>EventSerializer.deserialize()</code>.</li>\n<li><strong>Order Guarantee</strong>: Because each partition is consumed by only one consumer in a group, and events for the same row are in the same partition, consumers process events per-row in exact commit order.</li>\n<li><strong>Offset Commit</strong>: After successfully processing an event, the consumer commits its offset to Kafka (either synchronously or asynchronously). This commit acts as a checkpoint; if the consumer restarts, it resumes from this offset.</li>\n</ul>\n</li>\n<li><p><strong>Monitoring and Health Feedback</strong></p>\n<ul>\n<li><strong>Lag Monitoring</strong>: The <code>StreamerMetrics</code> component periodically checks consumer group lag via Kafka&#39;s AdminClient. If lag exceeds <code>lagThresholdMs</code>, alerts are triggered.</li>\n<li><strong>Health Status</strong>: The <code>CdcPipeline.getHealthStatus()</code> aggregates status from all components: <code>LogConnector</code> connectivity, <code>EventStreamer</code> connection to Kafka, and schema registry accessibility. This status is exposed for dashboards and readiness probes.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Concrete Example Walkthrough: User Registration</strong>\nLet&#39;s trace a concrete scenario where a user signs up on a website:</p>\n<ol>\n<li><strong>Application</strong>: <code>INSERT INTO users (id, email, created_at) VALUES (&#39;u123&#39;, &#39;alice@example.com&#39;, NOW()); COMMIT;</code></li>\n<li><strong>PostgreSQL</strong>: Writes the INSERT to WAL at position <code>0/18AB1234</code>, assigns transaction ID <code>tx789</code>.</li>\n<li><strong>Log Connector</strong>: Polls replication slot <code>cdc_slot</code>, receives binary WAL entry for the INSERT. <code>PostgreSQLParser</code> decodes it to <code>RawLogEntry</code> with <code>operationType=OPERATION_INSERT</code>, <code>rowData={id:u123, email:alice@example.com, created_at:2023-10-01...}</code>.</li>\n<li><strong>Change Event Builder</strong>: Creates <code>TransactionState</code> for <code>tx789</code>, adds the operation. Upon commit entry, builds <code>ChangeEvent</code> with <code>eventId=&quot;tx789-1&quot;</code>, <code>afterImage</code> containing the row, <code>schemaVersionId=&quot;users_v3&quot;</code>.</li>\n<li><strong>Event Streamer</strong>: Serializes to Avro using schema <code>users_v3</code>. Partition key = hash of <code>users:u123</code> → partition 5 of topic <code>cdc.users</code>. Sends to Kafka.</li>\n<li><strong>Kafka</strong>: Replicates to 3 brokers, acknowledges after all replicas persist.</li>\n<li><strong>Consumer</strong>: Email service consumes from partition 5, deserializes event, triggers welcome email to <code>alice@example.com</code>, commits offset.</li>\n<li><strong>Monitoring</strong>: Dashboard shows 0ms lag for email service consumer group.</li>\n</ol>\n<p><strong>Failure Recovery Scenario</strong>\nIf the <code>KafkaEventPublisher</code> crashes after sending but before receiving acknowledgment, the event remains in <code>pendingEvents</code>. On restart, the publisher scans <code>pendingEvents</code> and compares each event&#39;s LSN with <code>lastAckedLsnBySlot</code>. For events with LSN greater than the last acknowledged, it re-sends them (idempotent producer prevents duplicates at the broker). This ensures at-least-once delivery despite crashes.</p>\n<h3 id=\"92-schema-change-flow\">9.2 Schema Change Flow</h3>\n<p><strong>Mental Model: The Building Code Update</strong>\nImagine a city&#39;s building department (Schema Registry) that maintains official blueprints for all structures. When a homeowner wants to add a new room (new column), they submit plans. The department checks if the addition violates any safety codes (compatibility rules) and, if approved, issues a new version of the blueprint. All future construction (new ChangeEvents) must use the new blueprint. Crucially, existing houses don&#39;t need immediate renovation—old blueprints still accurately describe them. The department also notifies all contractors (consumers) about the blueprint update so they can prepare to work with the new format.</p>\n<p>Schema changes (DDL operations like <code>ALTER TABLE</code>) present a unique challenge because they modify the very structure of the data being captured. The system must: 1) detect the schema change, 2) validate its compatibility, 3) register the new schema version, 4) notify consumers, and 5) ensure future events use the correct schema.</p>\n<p><strong>Schema Change Handling Flowchart</strong>\nThe following flowchart depicts the decision process when a DDL event is encountered:</p>\n<p><img src=\"/api/project/cdc-system/architecture-doc/asset?path=diagrams%2Fschema-evolution-flowchart.svg\" alt=\"Flowchart: Handling a Schema Change (ALTER TABLE)\"></p>\n<p><strong>Step-by-Step DDL Handling Algorithm</strong></p>\n<ol>\n<li><p><strong>DDL Detection and Parsing</strong></p>\n<ul>\n<li><strong>Source</strong>: The DDL statement appears in the transaction log just like DML operations. The <code>LogParser</code> identifies it by operation type (often <code>DDL</code> or via statement prefix like <code>ALTER</code>).</li>\n<li><strong>Parsing</strong>: The parser extracts the table name, change type (ADD COLUMN, DROP COLUMN, ALTER COLUMN TYPE), and exact SQL statement.</li>\n<li><strong>Output</strong>: A special <code>RawLogEntry</code> with <code>operationType=&quot;DDL&quot;</code> and the SQL statement in <code>rowData</code>.</li>\n</ul>\n</li>\n<li><p><strong>Schema Extraction and Comparison</strong></p>\n<ul>\n<li><strong>Current Schema Fetch</strong>: The <code>ChangeEventBuilder</code> upon receiving the DDL entry calls <code>SchemaRegistry.getLatestSchema(tableName)</code> to retrieve the current <code>SchemaVersion</code>.</li>\n<li><strong>New Schema Inference</strong>: The system parses the DDL statement to infer the new column structure. For <code>ADD COLUMN email VARCHAR(255) NULL</code>, it adds the column to a copy of the current <code>columnDefinitions</code> with <code>nullable=true</code>.</li>\n<li><strong>Compatibility Check</strong>: <code>SchemaRegistry.checkCompatibility()</code> is invoked with the new column definitions, current schema, and the configured <code>compatibilityMode</code> (typically <code>COMPATIBILITY_BACKWARD</code>).<ul>\n<li><strong>Backward Compatibility Check Rules</strong>:<ol>\n<li>New columns may be added only if they are nullable or have a default value.</li>\n<li>Existing columns cannot be removed.</li>\n<li>Column data types may only change to a wider or compatible type (e.g., <code>VARCHAR(100)</code> → <code>VARCHAR(255)</code> is allowed; reverse is not).</li>\n<li>Column constraints may only be relaxed (e.g., <code>NOT NULL</code> → <code>NULL</code> allowed; reverse is not).</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Compliance Decision and Action</strong></p>\n<ul>\n<li><strong>If Compatible</strong>: The new schema is registered via <code>SchemaRegistry.registerSchema()</code>, which assigns a new version number (e.g., increments from v3 to v4) and persists it to storage. The schema cache is updated.</li>\n<li><strong>If Incompatible</strong>: The change is rejected. The system logs an error and can either:<ol>\n<li><strong>Stop the pipeline</strong> to prevent data loss (safe default).</li>\n<li><strong>Continue with old schema</strong> but flag the mismatch (requires manual intervention).</li>\n<li><strong>Apply emergency schema override</strong> via administrative API.</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p><strong>Schema Change Event Emission</strong></p>\n<ul>\n<li><strong>Notification Event</strong>: Regardless of compatibility outcome, a special <code>SchemaChangeEvent</code> is constructed containing the table name, old and new schema IDs, DDL statement, and timestamp.</li>\n<li><strong>Delivery</strong>: This event is published to a dedicated Kafka topic (<code>_schema_changes</code>) with its own schema (or to the regular table topic with a special header). Consumers subscribe to this topic to learn about schema changes.</li>\n<li><strong>Consumer Action</strong>: Upon receiving a <code>SchemaChangeEvent</code>, consumers should:<ol>\n<li>Fetch the new schema from the registry.</li>\n<li>Update their deserializers or database schemas accordingly.</li>\n<li>Possibly replay events from the new schema version if needed.</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Continuation</strong></p>\n<ul>\n<li><strong>Future Events</strong>: All subsequent <code>ChangeEvent</code> objects for the modified table include the new <code>schemaVersionId</code>.</li>\n<li><strong>Backfill Consideration</strong>: Existing events with the old schema remain valid; their schema IDs still resolve. No backfilling is required because old events are self-described with their schema version.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Concrete Example: Adding a Profile Photo Column</strong>\nLet&#39;s walk through adding an optional profile photo URL to the users table:</p>\n<ol>\n<li><strong>DDL Execution</strong>: <code>ALTER TABLE users ADD COLUMN profile_photo_url VARCHAR(512) NULL;</code></li>\n<li><strong>Log Capture</strong>: <code>PostgreSQLParser</code> creates <code>RawLogEntry</code> with <code>operationType=&quot;DDL&quot;</code>, <code>rowData={&quot;sql&quot;: &quot;ALTER TABLE users ADD COLUMN...&quot;}</code>.</li>\n<li><strong>Schema Inference</strong>: Current schema <code>users_v3</code> has columns <code>id, email, created_at</code>. New schema adds <code>profile_photo_url</code> (VARCHAR, nullable).</li>\n<li><strong>Compatibility Check</strong>: New column is nullable → passes backward compatibility.</li>\n<li><strong>Registration</strong>: <code>SchemaRegistry.registerSchema()</code> creates <code>users_v4</code>, stores it, updates cache.</li>\n<li><strong>Notification</strong>: <code>SchemaChangeEvent</code> published: <code>tableName=&quot;users&quot;</code>, <code>oldSchemaId=&quot;users_v3&quot;</code>, <code>newSchemaId=&quot;users_v4&quot;</code>.</li>\n<li><strong>Consumer Notification</strong>: Analytics service receives the schema change event, fetches <code>users_v4</code>, updates its Avro deserializer to accept the new optional field.</li>\n<li><strong>Subsequent Data</strong>: A new <code>UPDATE users SET profile_photo_url=&#39;...&#39;</code> produces a <code>ChangeEvent</code> with <code>schemaVersionId=&quot;users_v4&quot;</code> containing the new column value.</li>\n</ol>\n<p><strong>ADR: Immediate vs. Deferred Schema Application</strong></p>\n<blockquote>\n<p><strong>Decision: Immediate Schema Application with Backward-Only Compatibility</strong></p>\n<ul>\n<li><strong>Context</strong>: When a DDL operation is detected, the system must decide whether to immediately apply the new schema to future events or buffer events until consumers acknowledge readiness.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Immediate Application</strong>: Apply new schema immediately after registration; emit events with new schema version.</li>\n<li><strong>Consumer-ACK Application</strong>: Wait for all registered consumers to acknowledge they can handle the new schema before applying it.</li>\n<li><strong>Dual-Writing</strong>: Continue writing with old schema for a grace period while also writing with new schema.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Immediate schema application with backward-only compatibility.</li>\n<li><strong>Rationale</strong>: Backward-compatible changes (add optional columns) don&#39;t break existing consumers—they simply ignore the new fields. This approach minimizes complexity and avoids the distributed coordination problem of tracking consumer readiness. The schema change event notifies consumers, but they can upgrade at their own pace.</li>\n<li><strong>Consequences</strong>: Consumers must be resilient to unknown fields (skip them or store generically). Breaking changes (non-backward-compatible) require pipeline stoppage and coordinated upgrade—a deliberate operational process.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Immediate Application</td>\n<td>Simple, no coordination overhead, low latency</td>\n<td>Consumers may temporarily see fields they can&#39;t parse if not upgraded</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Consumer-ACK Application</td>\n<td>Safe, ensures no consumer breaks</td>\n<td>Complex coordination, requires consumer registration, can block pipeline</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Dual-Writing</td>\n<td>Maximum compatibility, no blocking</td>\n<td>Double write overhead, consumers must handle duplicate events</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p><strong>Common Pitfalls in Schema Change Flow</strong></p>\n<p>⚠️ <strong>Pitfall: Silent Data Truncation on Type Narrowing</strong></p>\n<ul>\n<li><strong>Description</strong>: Changing a column from <code>VARCHAR(255)</code> to <code>VARCHAR(100)</code> passes some compatibility checks (still same SQL type) but can truncate data if existing rows have longer values.</li>\n<li><strong>Why Wrong</strong>: The CDC system might not validate actual data lengths, leading to silent data loss when events are serialized.</li>\n<li><strong>Fix</strong>: Implement <strong>data compatibility checks</strong> in addition to schema compatibility. Before accepting a narrowing change, sample existing rows to ensure no data would be truncated, or reject such changes outright.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Missing Default Values for New NOT NULL Columns</strong></p>\n<ul>\n<li><strong>Description</strong>: Adding a <code>NOT NULL</code> column without a default value is backward incompatible, but some databases allow it (with a one-time rewrite of the table).</li>\n<li><strong>Why Wrong</strong>: The CDC system might allow this change, but subsequent INSERT events for old schema consumers will fail validation (missing required field).</li>\n<li><strong>Fix</strong>: Treat <code>NOT NULL</code> column additions as backward incompatible unless a default value is provided. Reject such DDL or require an explicit default.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Schema Registry Single Point of Failure</strong></p>\n<ul>\n<li><strong>Description</strong>: If the schema registry becomes unavailable, the entire pipeline may stop because every event needs a schema for serialization.</li>\n<li><strong>Why Wrong</strong>: Downtime of the registry causes data pipeline stoppage, violating availability goals.</li>\n<li><strong>Fix</strong>: Implement robust caching in <code>ChangeEventBuilder</code> (schema cache with TTL) and allow operation with stale schemas during registry outages. Use a replicated registry backend (e.g., Kafka-based schema registry with multiple nodes).</li>\n</ul>\n<h3 id=\"93-implementation-guidance\">9.3 Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequence Diagram Generation</td>\n<td>PlantUML (text-based, integrates with docs)</td>\n<td>Mermaid.js (JavaScript, renders in browsers)</td>\n</tr>\n<tr>\n<td>Schema Change Detection</td>\n<td>Parse SQL strings with regex/string matching</td>\n<td>Use SQL parser library (ANTLR, JSqlParser)</td>\n</tr>\n<tr>\n<td>Compatibility Checking</td>\n<td>Manual rule implementation per change type</td>\n<td>Integrate Avro Schema Registry compatibility API</td>\n</tr>\n<tr>\n<td>Schema Storage</td>\n<td>File-based (JSON files per schema version)</td>\n<td>Dedicated schema registry service (Confluent, Apicurio)</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── pipeline/\n│   │   ├── CdcPipeline.java              # Main orchestration, implements flow\n│   │   └── PipelineHealthChecker.java    # Health status aggregation\n│   ├── interactions/\n│   │   ├── FlowOrchestrator.java         # Coordinates steps 1-7 in normal flow\n│   │   ├── DdlEventHandler.java          # Handles schema change flow\n│   │   └── RecoveryCoordinator.java      # Manages failure recovery scenarios\n│   ├── diagram/\n│   │   ├── SequenceDiagramGenerator.java # (Optional) generates diagrams from actual traces\n│   │   └── FlowVisualizer.java           # (Optional) renders flows for debugging\n│   └── monitoring/\n│       └── FlowMetricsCollector.java     # Tracks latencies between steps</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code: Flow Orchestrator</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.interactions;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.config.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.LogConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.builder.ChangeEventBuilder;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.streamer.EventStreamer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.schema.SchemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.events.ChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.events.RawLogEntry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.health.HealthStatus;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.Logger;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.LoggerFactory;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.List;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.atomic.AtomicBoolean;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Orchestrates the end-to-end flow from log reading to event delivery.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Implements the normal operation sequence with error handling and backpressure.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> FlowOrchestrator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Logger LOG </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoggerFactory.</span><span style=\"color:#B392F0\">getLogger</span><span style=\"color:#E1E4E8\">(FlowOrchestrator.class);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> LogConnector logConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> ChangeEventBuilder eventBuilder;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> EventStreamer eventStreamer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> SchemaRegistry schemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> batchSize;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> AtomicBoolean running </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> AtomicBoolean</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> Thread processingThread;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> FlowOrchestrator</span><span style=\"color:#E1E4E8\">(LogConnector </span><span style=\"color:#FFAB70\">connector</span><span style=\"color:#E1E4E8\">, ChangeEventBuilder </span><span style=\"color:#FFAB70\">builder</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           EventStreamer </span><span style=\"color:#FFAB70\">streamer</span><span style=\"color:#E1E4E8\">, SchemaRegistry </span><span style=\"color:#FFAB70\">registry</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.logConnector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connector;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.eventBuilder </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> builder;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.eventStreamer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> streamer;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.schemaRegistry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> registry;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.batchSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">getMaxBatchSize</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Starts the continuous processing loop.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (running.</span><span style=\"color:#B392F0\">compareAndSet</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">)) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            processingThread </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> Thread</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">this</span><span style=\"color:#F97583\">::</span><span style=\"color:#E1E4E8\">processingLoop, </span><span style=\"color:#9ECBFF\">\"flow-orchestrator\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            processingThread.</span><span style=\"color:#B392F0\">start</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Flow orchestrator started\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Stops the processing loop gracefully.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        running.</span><span style=\"color:#B392F0\">set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">false</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (processingThread </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            processingThread.</span><span style=\"color:#B392F0\">interrupt</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                processingThread.</span><span style=\"color:#B392F0\">join</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5000</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (InterruptedException </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                Thread.</span><span style=\"color:#B392F0\">currentThread</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">interrupt</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Flow orchestrator stopped\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Main processing loop implementing the normal flow sequence.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> processingLoop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Starting processing loop with batch size {}\"</span><span style=\"color:#E1E4E8\">, batchSize);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#E1E4E8\"> (running.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">Thread.</span><span style=\"color:#B392F0\">currentThread</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">isInterrupted</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // Step 2: Poll for raw log entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> rawEntries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logConnector.</span><span style=\"color:#B392F0\">getNextBatch</span><span style=\"color:#E1E4E8\">(batchSize);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> (rawEntries.</span><span style=\"color:#B392F0\">isEmpty</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // No data available, sleep briefly to avoid tight loop</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    Thread.</span><span style=\"color:#B392F0\">sleep</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    continue</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                LOG.</span><span style=\"color:#B392F0\">debug</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Processing batch of {} raw log entries\"</span><span style=\"color:#E1E4E8\">, rawEntries.</span><span style=\"color:#B392F0\">size</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // Step 3: Build change events (with transaction grouping)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> changeEvents </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> eventBuilder.</span><span style=\"color:#B392F0\">processBatch</span><span style=\"color:#E1E4E8\">(rawEntries);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">changeEvents.</span><span style=\"color:#B392F0\">isEmpty</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    LOG.</span><span style=\"color:#B392F0\">debug</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Built {} change events from batch\"</span><span style=\"color:#E1E4E8\">, changeEvents.</span><span style=\"color:#B392F0\">size</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Step 4: Publish events to stream</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    eventStreamer.</span><span style=\"color:#B392F0\">publish</span><span style=\"color:#E1E4E8\">(changeEvents);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Monitor backpressure signal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> (eventStreamer </span><span style=\"color:#F97583\">instanceof</span><span style=\"color:#E1E4E8\"> KafkaEventPublisher) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        KafkaEventPublisher publisher </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (KafkaEventPublisher) eventStreamer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        if</span><span style=\"color:#E1E4E8\"> (publisher.</span><span style=\"color:#B392F0\">isBackpressureRequired</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            LOG.</span><span style=\"color:#B392F0\">warn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Backpressure activated, pausing log connector\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            logConnector.</span><span style=\"color:#B392F0\">pause</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                            // Wait until backpressure clears</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                            while</span><span style=\"color:#E1E4E8\"> (publisher.</span><span style=\"color:#B392F0\">isBackpressureRequired</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> running.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                Thread.</span><span style=\"color:#B392F0\">sleep</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            logConnector.</span><span style=\"color:#B392F0\">resume</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (InterruptedException </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                LOG.</span><span style=\"color:#B392F0\">info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Processing loop interrupted\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                Thread.</span><span style=\"color:#B392F0\">currentThread</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">interrupt</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (Exception </span><span style=\"color:#FFAB70\">e</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                LOG.</span><span style=\"color:#B392F0\">error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Unexpected error in processing loop\"</span><span style=\"color:#E1E4E8\">, e);</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // Implement circuit breaker pattern here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                // After N consecutive errors, transition to DEGRADED health state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    Thread.</span><span style=\"color:#B392F0\">sleep</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">); </span><span style=\"color:#6A737D\">// Avoid tight error loop</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                } </span><span style=\"color:#F97583\">catch</span><span style=\"color:#E1E4E8\"> (InterruptedException </span><span style=\"color:#FFAB70\">ie</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    Thread.</span><span style=\"color:#B392F0\">currentThread</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">interrupt</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Returns the health status based on component health and processing state.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> HealthStatus </span><span style=\"color:#B392F0\">getHealthStatus</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">running.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.STOPPED;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Check if all components are healthy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Implementation depends on component health APIs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> HealthStatus.HEALTHY;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton: DDL Event Handler</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.interactions;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.events.RawLogEntry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.events.SchemaChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.schema.SchemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.schema.SchemaVersion;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.config.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.Logger;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.slf4j.LoggerFactory;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Map;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Handles the schema change flow when DDL events are detected.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> DdlEventHandler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Logger LOG </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoggerFactory.</span><span style=\"color:#B392F0\">getLogger</span><span style=\"color:#E1E4E8\">(DdlEventHandler.class);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> SchemaRegistry schemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String compatibilityMode;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> EventStreamer eventStreamer; </span><span style=\"color:#6A737D\">// For publishing schema change events</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> DdlEventHandler</span><span style=\"color:#E1E4E8\">(SchemaRegistry </span><span style=\"color:#FFAB70\">registry</span><span style=\"color:#E1E4E8\">, AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">, EventStreamer </span><span style=\"color:#FFAB70\">streamer</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.schemaRegistry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> registry;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.compatibilityMode </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">getSchema</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">getCompatibilityMode</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.eventStreamer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> streamer;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Process a DDL log entry and handle the schema change flow.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@param</span><span style=\"color:#FFAB70\"> ddlEntry</span><span style=\"color:#6A737D\"> The raw log entry containing DDL statement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * </span><span style=\"color:#F97583\">@return</span><span style=\"color:#6A737D\"> true if schema change was successfully processed, false otherwise</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#B392F0\"> handleDdlEvent</span><span style=\"color:#E1E4E8\">(RawLogEntry </span><span style=\"color:#FFAB70\">ddlEntry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Extract table name and DDL statement from ddlEntry.rowData</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Parse the SQL statement to determine change type (ADD, DROP, ALTER)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use regex or simple string matching for basic parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Advanced: Use a SQL parser library for robust parsing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Retrieve current schema for the table using schemaRegistry.getLatestSchema()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Handle case where table has no registered schema (first time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Infer new schema by applying DDL change to current schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Create a copy of current columnDefinitions map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Based on DDL type, modify the map:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //     * ADD COLUMN: Add new entry with appropriate ColumnType</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //     * DROP COLUMN: Remove entry (check compatibility first!)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //     * ALTER COLUMN: Modify existing ColumnType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Perform compatibility check using schemaRegistry.checkCompatibility()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Pass new column definitions, current schema, and compatibilityMode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - If incompatible:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //       1. Log error with specific incompatibility reason</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //       2. Optionally stop pipeline or trigger alert</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //       3. Return false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Register new schema version using schemaRegistry.registerSchema()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - This will assign a new version ID and persist the schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Update any local schema caches</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: Create and publish SchemaChangeEvent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Construct SchemaChangeEvent with tableName, old/new schema IDs, timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Serialize and publish to dedicated schema change topic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use eventStreamer.publish() with appropriate topic routing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: Log the successful schema change for audit purposes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Include old and new schema versions, DDL statement, timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// Return true if all steps succeeded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Simple SQL parser for extracting ALTER TABLE components.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * This is a basic implementation; consider using a proper SQL parser for production.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">Object</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">parseAlterTable</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">ddlSql</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Implement basic parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Extract table name after \"ALTER TABLE\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Determine operation type (ADD, DROP, ALTER)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - For ADD COLUMN: extract column name, data type, nullability, default</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Return as structured map</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Map.</span><span style=\"color:#B392F0\">of</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints (Java)</strong></p>\n<ol>\n<li><strong>Sequence Tracking</strong>: Use <code>AtomicLong</code> for sequence numbers within transactions and <code>ThreadLocal</code> random for generating unique IDs when needed.</li>\n<li><strong>Backpressure Implementation</strong>: Implement <code>Flow.Control</code> (Reactive Streams) interface for more sophisticated backpressure, or use simple semaphores (<code>Semaphore</code> with permits equal to max in-flight events).</li>\n<li><strong>Schema Caching</strong>: Use Guava&#39;s <code>LoadingCache</code> with TTL for schema caching to avoid registry overload.</li>\n<li><strong>DDL Parsing</strong>: Use <code>com.github.jsqlparser:jsqlparser</code> for robust SQL parsing instead of regex.</li>\n<li><strong>Flow Visualization</strong>: Use Micrometer <code>@Timed</code> annotations on each step method to generate latency histograms for each stage of the pipeline.</li>\n</ol>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the interaction flows, verify the system with this end-to-end test:</p>\n<ol>\n<li><strong>Start the Pipeline</strong>: Run <code>CdcPipeline.start()</code> and confirm all components initialize.</li>\n<li><strong>Create Test Table</strong>: In PostgreSQL: <code>CREATE TABLE test_users (id TEXT PRIMARY KEY, name TEXT);</code></li>\n<li><strong>Insert a Row</strong>: <code>INSERT INTO test_users VALUES (&#39;test1&#39;, &#39;Alice&#39;); COMMIT;</code></li>\n<li><strong>Verify Event Flow</strong>:<ul>\n<li>Check logs for <code>RawLogEntry</code> parsed from WAL.</li>\n<li>Confirm <code>ChangeEvent</code> built with correct <code>schemaVersionId</code>.</li>\n<li>Monitor Kafka topic <code>cdc.test_users</code> for the event arrival.</li>\n<li>Use a test consumer to receive and deserialize the event.</li>\n</ul>\n</li>\n<li><strong>Test Schema Change</strong>:<ul>\n<li>Execute: <code>ALTER TABLE test_users ADD COLUMN email TEXT NULL;</code></li>\n<li>Verify <code>SchemaChangeEvent</code> appears in <code>_schema_changes</code> topic.</li>\n<li>Insert another row with email, confirm new schema version is used.</li>\n</ul>\n</li>\n<li><strong>Metrics Verification</strong>: Check exposed metrics (via JMX or HTTP endpoint) for:<ul>\n<li><code>cdc_flow_latency_ms</code> histogram showing time from commit to consumer.</li>\n<li><code>cdc_schema_changes_total</code> counter incremented.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Expected Output</strong>: The pipeline should process the INSERT within milliseconds, and the schema change should be handled without stopping the pipeline. Consumer lag should remain near zero.</p>\n<p><strong>Debugging Tip</strong>: If events aren&#39;t flowing, use this diagnostic checklist:</p>\n<ul>\n<li>Check <code>LogConnector.getLastProcessedLSN()</code> to see if it&#39;s advancing.</li>\n<li>Verify <code>OffsetStore</code> file has current LSN persisted.</li>\n<li>Check Kafka producer metrics for send errors.</li>\n<li>Inspect schema registry logs for compatibility check failures.</li>\n</ul>\n<hr>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (log position recovery), Milestone 2 (delivery guarantees during failures), Milestone 3 (schema registry failure handling)</p>\n</blockquote>\n<h2 id=\"10-error-handling-and-edge-cases\">10. Error Handling and Edge Cases</h2>\n<p>A CDC system operates in the critical path between the source database and downstream systems, making robust error handling non-negotiable. Unlike batch systems that can be restarted from the beginning, a CDC pipeline must handle transient failures without data loss while maintaining strict ordering guarantees. This section details the <strong>failure modes</strong> each component can encounter, the <strong>detection mechanisms</strong> that identify these failures, and the <strong>recovery strategies</strong> that restore normal operation.</p>\n<h3 id=\"101-failure-modes-and-detection\">10.1 Failure Modes and Detection</h3>\n<p>Think of the CDC pipeline as a <strong>convoy of armored trucks</strong> transporting valuable goods (data changes). Failure modes represent different attack vectors on this convoy: roadblocks (network partitions), mechanical failures (database restarts), hijacking attempts (corrupt data), and communication breakdowns (consumer lag). Detection mechanisms are the convoy&#39;s security detail—constantly monitoring for threats through checkpoints, heartbeat signals, and surveillance.</p>\n<p>Each component faces distinct failure modes that require specific detection strategies. The system employs a multi-layered monitoring approach combining <strong>active health checks</strong>, <strong>passive metric monitoring</strong>, and <strong>transaction boundary verification</strong>.</p>\n<h4 id=\"1011-log-connector-amp-parser-failures\">10.1.1 Log Connector &amp; Parser Failures</h4>\n<p>The Log Connector operates closest to the source database and must handle low-level database connectivity and log parsing failures.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Indicators &amp; Symptoms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Database connection loss</strong></td>\n<td>Active health checks with exponential backoff</td>\n<td>TCP connection timeout, JDBC/Go driver connection errors, repeated authentication failures</td>\n</tr>\n<tr>\n<td><strong>WAL/binlog slot deletion</strong></td>\n<td>Slot existence verification on startup and periodic checks</td>\n<td>PostgreSQL: <code>pg_replication_slot</code> query returns no rows; MySQL: <code>SHOW SLAVE STATUS</code> shows missing connection</td>\n</tr>\n<tr>\n<td><strong>Log position corruption</strong></td>\n<td>Checksum validation of stored offsets</td>\n<td>Stored LSN doesn&#39;t exist in current log, LSN comparison shows position moving backward</td>\n</tr>\n<tr>\n<td><strong>Database version mismatch</strong></td>\n<td>Parser compatibility check on initialization</td>\n<td>Binary log format version differs from expected, unsupported column types in parsed entries</td>\n</tr>\n<tr>\n<td><strong>WAL segment rotation before consumption</strong></td>\n<td>Monitoring WAL retention settings vs consumption rate</td>\n<td>PostgreSQL: <code>pg_stat_replication</code> shows <code>sent_lsn</code> far behind <code>write_lsn</code>; gaps in LSN sequence</td>\n</tr>\n<tr>\n<td><strong>Parser memory exhaustion</strong></td>\n<td>JVM heap monitoring / Go memory profiling</td>\n<td><code>OutOfMemoryError</code> in Java, parser thread crash, progressive slowdown in parsing throughput</td>\n</tr>\n<tr>\n<td><strong>Database restart with slot persistence</strong></td>\n<td>Reconnection logic with slot state verification</td>\n<td>Connection drops, followed by successful reconnection but slot shows <code>active_pid = NULL</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Insight</strong>: The Log Connector must distinguish between <strong>transient network glitches</strong> (retry with backoff) and <strong>permanent configuration issues</strong> (halt with alert). A 3-strike rule is effective: three consecutive connection failures within 60 seconds triggers a degraded health status and alert.</p>\n</blockquote>\n<h4 id=\"1012-change-event-builder-failures\">10.1.2 Change Event Builder Failures</h4>\n<p>The Change Event Builder manages transaction state in memory, making it vulnerable to state corruption and memory-related issues.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Indicators &amp; Symptoms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Transaction timeout</strong></td>\n<td>Last activity timestamp monitoring</td>\n<td>Transaction remains in <code>activeTransactions</code> map beyond <code>transactionTimeoutMs</code></td>\n</tr>\n<tr>\n<td><strong>Memory leak in transaction state</strong></td>\n<td>Active transaction count monitoring</td>\n<td><code>getTransactionCount()</code> grows unbounded over time without corresponding commit/rollback</td>\n</tr>\n<tr>\n<td><strong>Deduplication cache overflow</strong></td>\n<td>Cache size monitoring vs configured limits</td>\n<td><code>deduplicationCache.size()</code> exceeds maximum threshold, older entries being evicted prematurely</td>\n</tr>\n<tr>\n<td><strong>Schema cache staleness</strong></td>\n<td>Schema version ID mismatch during event building</td>\n<td><code>SchemaVersion</code> ID in cache doesn&#39;t match <code>schemaVersionId</code> in incoming <code>RawLogEntry</code></td>\n</tr>\n<tr>\n<td><strong>Transaction state corruption</strong></td>\n<td>Consistency checks on transaction boundaries</td>\n<td>Transaction has <code>startLsn</code> but no operations, or operations out of sequence order</td>\n</tr>\n<tr>\n<td><strong>Clock skew affecting timestamps</strong></td>\n<td>NTP synchronization monitoring</td>\n<td><code>commitTimestamp</code> from database vs system clock shows &gt;5 second discrepancy</td>\n</tr>\n</tbody></table>\n<h4 id=\"1013-event-streamer-amp-delivery-failures\">10.1.3 Event Streamer &amp; Delivery Failures</h4>\n<p>The Event Streamer interfaces with Kafka, facing network partitions, broker failures, and consumer backpressure scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Indicators &amp; Symptoms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Kafka broker unreachable</strong></td>\n<td>Producer/AdminClient heartbeat failures</td>\n<td><code>KafkaException</code> with &quot;Failed to update metadata&quot;, producer <code>send()</code> timeouts</td>\n</tr>\n<tr>\n<td><strong>Producer idempotence sequence gap</strong></td>\n<td>Out-of-order sequence number detection</td>\n<td>Producer internal metrics show <code>sequence_number_gap</code> &gt; 0 for any topic partition</td>\n</tr>\n<tr>\n<td><strong>Consumer lag exceeding threshold</strong></td>\n<td>Consumer group lag monitoring via AdminClient</td>\n<td><code>consumer_lag</code> &gt; configured <code>lagThresholdMs</code> for any partition</td>\n</tr>\n<tr>\n<td><strong>Backpressure cascade</strong></td>\n<td>Pipeline stage queue monitoring</td>\n<td><code>LogConnector.buffer</code> near capacity, <code>KafkaEventPublisher.inFlightEventCount</code> &gt; maximum in-flight</td>\n</tr>\n<tr>\n<td><strong>Delivery callback timeout</strong></td>\n<td>Async send operation timeout monitoring</td>\n<td><code>DeliveryCallback</code> not invoked within <code>delivery.timeout.ms</code> configuration</td>\n</tr>\n<tr>\n<td><strong>Topic auto-creation failure</strong></td>\n<td>Topic existence verification on startup</td>\n<td><code>UnknownTopicOrPartitionException</code> when producing to topic, despite <code>auto.create.topics.enable=true</code></td>\n</tr>\n<tr>\n<td><strong>Producer buffer exhaustion</strong></td>\n<td>Buffer pool memory monitoring</td>\n<td><code>buffer.memory</code> usage &gt; 90%, producer blocking on <code>send()</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"1014-schema-registry-failures\">10.1.4 Schema Registry Failures</h4>\n<p>The Schema Registry is a critical metadata service whose failure can halt schema evolution.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Indicators &amp; Symptoms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Registry service unavailable</strong></td>\n<td>Health endpoint polling</td>\n<td>HTTP 503/connect timeout from registry URL, cache miss with no fallback</td>\n</tr>\n<tr>\n<td><strong>Disk corruption in file-based registry</strong></td>\n<td>Checksum validation on schema files</td>\n<td><code>FileBasedSchemaRegistry.loadExistingSchemasIntoCache()</code> throws JSON parsing exception</td>\n</tr>\n<tr>\n<td><strong>Schema compatibility violation</strong></td>\n<td>Pre-commit compatibility checking</td>\n<td><code>SchemaRegistry.checkCompatibility()</code> returns <code>false</code> for DDL that should pass</td>\n</tr>\n<tr>\n<td><strong>Schema ID collision</strong></td>\n<td>Duplicate ID detection during registration</td>\n<td>Two different schema definitions with same <code>schemaId</code> in cache</td>\n</tr>\n<tr>\n<td><strong>Backward compatibility break</strong></td>\n<td>Consumer compatibility check</td>\n<td>Consumer fails to deserialize events with <code>UnsupportedSchemaException</code></td>\n</tr>\n<tr>\n<td><strong>Schema cache incoherency</strong></td>\n<td>Cross-component schema version verification</td>\n<td><code>ChangeEventBuilder.schemaCache</code> version differs from <code>SchemaRegistry</code> persisted version</td>\n</tr>\n</tbody></table>\n<h4 id=\"1015-systemic-failure-modes\">10.1.5 Systemic Failure Modes</h4>\n<p>These failures affect multiple components or the entire pipeline.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Mechanism</th>\n<th>Indicators &amp; Symptoms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Network partition isolating components</strong></td>\n<td>Inter-component heartbeat and ping</td>\n<td>gRPC/HTTP health checks fail between components, but each component&#39;s self-check passes</td>\n</tr>\n<tr>\n<td><strong>Clock skew across deployment nodes</strong></td>\n<td>Cross-node timestamp comparison</td>\n<td>Events show out-of-order <code>commitTimestamp</code> vs <code>processingTimestamp</code> &gt; allowable skew</td>\n</tr>\n<tr>\n<td><strong>Coordinated pipeline restart</strong></td>\n<td>Graceful shutdown sequence tracking</td>\n<td>Multiple components restart simultaneously, losing in-memory transaction state</td>\n</tr>\n<tr>\n<td><strong>Database failover to replica</strong></td>\n<td>Database host/port change detection</td>\n<td>Connection succeeds to new host, but WAL position doesn&#39;t continue from previous LSN</td>\n</tr>\n<tr>\n<td><strong>ZooKeeper/Kafka controller election</strong></td>\n<td>Broker metadata staleness</td>\n<td>Producer receives &quot;Not Leader for Partition&quot; errors despite recent successful sends</td>\n</tr>\n<tr>\n<td><strong>Disk full on state persistence volume</strong></td>\n<td>Disk space monitoring for offset and schema files</td>\n<td><code>FileOffsetStore.save()</code> throws <code>IOException</code> due to &quot;No space left on device&quot;</td>\n</tr>\n</tbody></table>\n<h3 id=\"102-recovery-strategies\">10.2 Recovery Strategies</h3>\n<p>Once a failure is detected, the system must execute a recovery strategy that <strong>minimizes data loss</strong>, <strong>preserves ordering guarantees</strong>, and <strong>restores throughput</strong> without manual intervention. Think of recovery as the convoy&#39;s emergency protocols: when a truck breaks down, goods are transferred to backup vehicles; when a road is blocked, an alternate route is calculated; when communication is lost, pre-established rendezvous points are used.</p>\n<h4 id=\"1021-log-connector-recovery-strategies\">10.2.1 Log Connector Recovery Strategies</h4>\n<p>The Log Connector&#39;s primary recovery challenge is resuming from the correct log position without missing or duplicating changes.</p>\n<p><strong>Database Connection Loss Recovery:</strong></p>\n<ol>\n<li><strong>Immediate Actions</strong>: Close existing connection, increment failure counter</li>\n<li><strong>Backoff Strategy</strong>: Wait using exponential backoff (1s, 2s, 4s, 8s, 16s, 30s max)</li>\n<li><strong>Reconnection Attempt</strong>: <ul>\n<li>Re-establish database connection with same parameters</li>\n<li>Verify replication slot still exists and is active</li>\n<li>Query current WAL position to ensure it&#39;s ahead of <code>lastProcessedLSN</code></li>\n</ul>\n</li>\n<li><strong>Resumption Logic</strong>:<ul>\n<li>If reconnection succeeds and slot exists: resume reading from <code>lastProcessedLSN</code></li>\n<li>If slot was deleted: create new slot, trigger <strong>full table resync</strong> (non-goal but needed)</li>\n<li>If <code>lastProcessedLSN</code> no longer in retained logs: restart from earliest available LSN, log warning</li>\n</ul>\n</li>\n</ol>\n<p><strong>WAL Position Corruption Recovery:</strong></p>\n<ol>\n<li><strong>Corruption Detection</strong>: Compare stored LSN with minimum available LSN in <code>pg_ls_waldir()</code> or <code>SHOW BINARY LOGS</code></li>\n<li><strong>Recovery Options</strong>:<ul>\n<li>If corruption is minor (few segments behind): restart from last known valid LSN in retained logs</li>\n<li>If corruption is major (beyond retention): reset to current LSN, emit <strong>gap detection event</strong></li>\n</ul>\n</li>\n<li><strong>Gap Handling</strong>: Generate a synthetic <code>ChangeEvent</code> with <code>operationType=GAP_DETECTED</code> to alert consumers of potential missing data</li>\n</ol>\n<p><strong>Transaction Log Overflow Prevention:</strong></p>\n<blockquote>\n<p><strong>Decision: Conservative WAL Retention Policy</strong></p>\n<ul>\n<li><strong>Context</strong>: The database may recycle WAL segments before the CDC connector reads them if consumption is too slow</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Increase WAL retention settings on database (requires DB admin rights)</li>\n<li>Implement backpressure to slow source database writes (impacts primary workload)</li>\n<li>Add intermediate disk buffer in connector (adds complexity and failure mode)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Configure database WAL retention to hold at least 24 hours of changes, monitor consumption lag, and alert before reaching retention limits</li>\n<li><strong>Rationale</strong>: Simplicity and reliability—letting the database manage retention with safe margins avoids complex buffering logic</li>\n<li><strong>Consequences</strong>: Requires database configuration access, uses more disk on source DB</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Increase DB retention</td>\n<td>Simple, reliable</td>\n<td>DB disk usage, requires privileges</td>\n<td><strong>Yes</strong> (primary)</td>\n</tr>\n<tr>\n<td>Backpressure to source</td>\n<td>Minimizes disk usage</td>\n<td>Impacts primary workload, complex</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Connector disk buffer</td>\n<td>Decouples from DB config</td>\n<td>Another state to manage, disk failure risk</td>\n<td>No (fallback)</td>\n</tr>\n</tbody></table>\n<h4 id=\"1022-change-event-builder-recovery-strategies\">10.2.2 Change Event Builder Recovery Strategies</h4>\n<p>The Change Event Builder&#39;s in-memory state is volatile and must be reconstructed or flushed during failures.</p>\n<p><strong>Pipeline Restart with Pending Transactions:</strong></p>\n<ol>\n<li><strong>Problem</strong>: Pipeline stops while transactions are open in <code>activeTransactions</code> map</li>\n<li><strong>Recovery Strategy</strong>: <ul>\n<li>On graceful shutdown: <code>flushPendingTransactions()</code> emits all pending events with <code>transactionId=ABORTED</code> marker</li>\n<li>On crash recovery: Transaction timeout detection will eventually clean up stale transactions</li>\n</ul>\n</li>\n<li><strong>Data Guarantee</strong>: Consumers must handle <code>ABORTED</code> markers by discarding or compensating for partial transactions</li>\n</ol>\n<p><strong>Transaction Timeout Recovery:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>1. Background cleaner thread runs every transactionTimeoutMs/2\n2. For each TransactionState in activeTransactions:\n3.   If (currentTime - lastActivityTimestamp) &gt; transactionTimeoutMs:\n4.     Log warning: &quot;Transaction {transactionId} timed out after {timeoutMs}ms&quot;\n5.     Call buildEventsForTransaction() with ABORTED marker\n6.     Remove from activeTransactions map\n7.     Emit abort events to streamer</code></pre></div>\n\n<p><strong>Memory Exhaustion Prevention:</strong></p>\n<ol>\n<li><strong>Monitoring</strong>: Track <code>activeTransactions.size()</code> and <code>deduplicationCache</code> memory usage</li>\n<li><strong>Circuit Breaker</strong>: If transaction count exceeds <code>maxConcurrentTransactions</code>:<ul>\n<li>Pause Log Connector (backpressure)</li>\n<li>Force-flush oldest transactions (by <code>lastActivityTimestamp</code>)</li>\n<li>Resume when below 80% threshold</li>\n</ul>\n</li>\n<li><strong>Deduplication Cache Management</strong>: Use LRU eviction when cache reaches size limit</li>\n</ol>\n<h4 id=\"1023-event-streamer-recovery-strategies\">10.2.3 Event Streamer Recovery Strategies</h4>\n<p>The Event Streamer must guarantee at-least-once delivery despite Kafka broker failures and network issues.</p>\n<p><strong>Kafka Producer Failure Recovery:</strong></p>\n<blockquote>\n<p><strong>Decision: Producer Retry with Idempotence</strong></p>\n<ul>\n<li><strong>Context</strong>: Network partitions or broker failures can cause individual produce requests to fail</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Retry indefinitely with backoff (risk of indefinite hang)</li>\n<li>Retry N times then fail (risk of data loss)</li>\n<li>Retry with dead letter queue (adds complexity)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Configure producer with <code>retries=MAX_INT</code>, <code>delivery.timeout.ms=120000</code>, <code>enable.idempotence=true</code></li>\n<li><strong>Rationale</strong>: Kafka&#39;s idempotent producer ensures exactly-once semantics in the producer-broker interaction, making infinite retries safe</li>\n<li><strong>Consequences</strong>: Producer may block for up to 2 minutes during complete broker outage; requires <code>max.in.flight.requests.per.connection=1</code> or 5</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Recovery Step</th>\n<th>Action</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1. Send failure detection</td>\n<td><code>KafkaException</code> caught in <code>DeliveryCallback</code></td>\n<td>Network or broker issue</td>\n</tr>\n<tr>\n<td>2. Automatic retry</td>\n<td>Kafka producer internal retry logic</td>\n<td>Built-in exponential backoff</td>\n</tr>\n<tr>\n<td>3. Retry exhaustion</td>\n<td><code>TimeoutException</code> after <code>delivery.timeout.ms</code></td>\n<td>Brokers unavailable beyond tolerance</td>\n</tr>\n<tr>\n<td>4. Circuit breaker</td>\n<td>Pause pipeline, buffer in memory</td>\n<td>Prevent unbounded memory growth</td>\n</tr>\n<tr>\n<td>5. AdminClient health check</td>\n<td>Verify broker availability</td>\n<td>Distinguish network vs broker failure</td>\n</tr>\n<tr>\n<td>6. Resume production</td>\n<td>When brokers return</td>\n<td>Continue from buffered events</td>\n</tr>\n</tbody></table>\n<p><strong>Consumer Lag Recovery:</strong></p>\n<ol>\n<li><strong>Detection</strong>: <code>StreamerMetrics</code> monitors consumer group lag via AdminClient <code>listConsumerGroupOffsets()</code></li>\n<li><strong>Threshold Breach</strong>: When lag &gt; <code>lagThresholdMs</code> for &gt;5 consecutive checks:<ul>\n<li>Scale alert to operations team</li>\n<li>Optionally scale out consumer applications</li>\n</ul>\n</li>\n<li><strong>Backpressure Application</strong>: If lag exceeds critical threshold (2x <code>lagThresholdMs</code>):<ul>\n<li>Reduce <code>LogConnector</code> batch size</li>\n<li>Increase polling interval</li>\n<li>Pause connector if lag continues growing</li>\n</ul>\n</li>\n</ol>\n<p><strong>Delivery Semantic Preservation During Failures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Scenario</th>\n<th>At-Least-Once Guarantee Preservation</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Producer crash before callback</td>\n<td>Events may be lost</td>\n<td>Reload from <code>lastProcessedLSN</code> and reproduce</td>\n</tr>\n<tr>\n<td>Producer crash after callback</td>\n<td>Events delivered (Kafka has them)</td>\n<td>Continue from last acknowledged offset</td>\n</tr>\n<tr>\n<td>Kafka broker loss of acknowledged writes</td>\n<td>Events lost (unlikely with <code>acks=all</code>)</td>\n<td>Reprocess from last confirmed LSN</td>\n</tr>\n<tr>\n<td>Network partition during send</td>\n<td>Unknown delivery state</td>\n<td>Retry with idempotent producer (no duplicates)</td>\n</tr>\n</tbody></table>\n<h4 id=\"1024-schema-registry-recovery-strategies\">10.2.4 Schema Registry Recovery Strategies</h4>\n<p>Schema Registry failures can halt the entire pipeline since events cannot be serialized without schema information.</p>\n<p><strong>Registry Service Unavailable Recovery:</strong></p>\n<ol>\n<li><strong>Cached Schema Fallback</strong>: Use <code>ChangeEventBuilder.schemaCache</code> as read-through cache</li>\n<li><strong>Degraded Mode Operation</strong>:<ul>\n<li>Continue processing events with cached schemas</li>\n<li>Buffer schema registration requests in memory queue</li>\n<li>Log warning about schema registry unavailability</li>\n</ul>\n</li>\n<li><strong>Recovery on Service Restoration</strong>:<ul>\n<li>Flush buffered schema registrations</li>\n<li>Validate cache coherence with registry</li>\n<li>Continue normal operation</li>\n</ul>\n</li>\n</ol>\n<p><strong>Schema Compatibility Violation Recovery:</strong></p>\n<ol>\n<li><strong>Prevention</strong>: Always run <code>checkCompatibility()</code> before applying DDL to database</li>\n<li><strong>Detection</strong>: DDL parser detects incompatible change (column removal, type narrowing)</li>\n<li><strong>Recovery Options</strong>:<ul>\n<li><strong>Option A</strong>: Reject DDL (requires DBA intervention)</li>\n<li><strong>Option B</strong>: Register schema with <code>COMPATIBILITY_NONE</code> and alert consumers</li>\n<li><strong>Option C</strong>: Create new table version and dual-write during migration</li>\n</ul>\n</li>\n</ol>\n<p><strong>File-Based Registry Corruption Recovery:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>1. On startup, FileBasedSchemaRegistry.loadExistingSchemasIntoCache():\n2.   Try: Read and parse all *.schema.json files\n3.   Catch JsonProcessingException:\n4.     Attempt repair: Skip corrupt file, log error\n5.     If critical schemas missing: \n6.       Fallback to reconstructing from database INFORMATION_SCHEMA\n7.       Log alert for manual schema reconciliation\n8.   Finally: Initialize cache with whatever schemas loaded successfully</code></pre></div>\n\n<h4 id=\"1025-systemic-recovery-strategies\">10.2.5 Systemic Recovery Strategies</h4>\n<p>These strategies coordinate recovery across multiple components.</p>\n<p><strong>Pipeline-Wide Restart Recovery:</strong></p>\n<ol>\n<li><strong>Ordered Shutdown Protocol</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   1. Stop Log Connector (stops reading new changes)\n   2. Call ChangeEventBuilder.flushPendingTransactions()\n   3. Wait for EventStreamer to drain pendingEvents queue\n   4. Persist final offset via OffsetStore.save()\n   5. Close all connections (DB, Kafka, Registry)</code></pre></div>\n<ol start=\"2\">\n<li><strong>Ordered Startup Protocol</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>   1. Initialize SchemaRegistry and load cache\n   2. Initialize EventStreamer and verify Kafka connectivity\n   3. Initialize ChangeEventBuilder with empty state\n   4. Initialize LogConnector with offset from OffsetStore.load()\n   5. Start LogConnector reading from stored LSN</code></pre></div>\n<ol start=\"3\">\n<li><strong>Crash Recovery</strong>: On startup, verify component state consistency and reset if inconsistent</li>\n</ol>\n<p><strong>Database Failover Recovery:</strong></p>\n<ol>\n<li><strong>Detection</strong>: Connection drops, then reconnects to different host (detected via DNS/hostname change)</li>\n<li><strong>Position Verification</strong>: Query <code>pg_current_wal_lsn()</code> or <code>SHOW MASTER STATUS</code> on new primary</li>\n<li><strong>Recovery Decision Table</strong>:</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>New primary has LSN ≥ lastProcessedLSN</td>\n<td>Resume reading from lastProcessedLSN</td>\n</tr>\n<tr>\n<td>New primary has LSN &lt; lastProcessedLSN (data loss)</td>\n<td>Reset to new primary&#39;s LSN, emit GAP_DETECTED event</td>\n</tr>\n<tr>\n<td>Cannot determine LSN continuity</td>\n<td>Pause pipeline, alert for manual intervention</td>\n</tr>\n</tbody></table>\n<p><strong>Clock Skew Mitigation:</strong></p>\n<ol>\n<li><strong>Detection</strong>: Monitor <code>commitTimestamp</code> vs system clock on <code>ChangeEvent</code> creation</li>\n<li><strong>Mitigation</strong>: Use database-provided timestamps exclusively, never system clock</li>\n<li><strong>Correction</strong>: If skew detected, log warning but don&#39;t adjust timestamps (preserve source truth)</li>\n</ol>\n<h4 id=\"1026-recovery-verification-and-testing\">10.2.6 Recovery Verification and Testing</h4>\n<blockquote>\n<p><strong>Critical Principle</strong>: Recovery logic must be tested with the same rigor as normal operation. Use chaos engineering principles to inject failures and verify recovery.</p>\n</blockquote>\n<p><strong>Recovery Verification Checklist:</strong></p>\n<ul>\n<li><input disabled=\"\" type=\"checkbox\"> <strong>Position Recovery</strong>: Restart pipeline, verify events continue from exact LSN without gap or repeat</li>\n<li><input disabled=\"\" type=\"checkbox\"> <strong>Transaction Integrity</strong>: Crash during multi-statement transaction, verify consumers see all-or-nothing</li>\n<li><input disabled=\"\" type=\"checkbox\"> <strong>Schema Evolution</strong>: Registry failure during DDL, verify cached schemas allow continued operation</li>\n<li><input disabled=\"\" type=\"checkbox\"> <strong>Backpressure Resilience</strong>: Slow consumer to stall, verify pipeline pauses without data loss</li>\n<li><input disabled=\"\" type=\"checkbox\"> <strong>Network Partition</strong>: Simulate Kafka broker isolation, verify producer buffers and resumes</li>\n</ul>\n<p><strong>Common Recovery Pitfalls and Solutions:</strong></p>\n<p>⚠️ <strong>Pitfall: Off-by-One in LSN Recovery</strong></p>\n<ul>\n<li><strong>Description</strong>: Resuming from <code>lastProcessedLSN</code> instead of <code>lastProcessedLSN + 1</code> causes duplicate processing of the last event</li>\n<li><strong>Why Wrong</strong>: Database logs include the LSN of the next byte to read, not the last byte read</li>\n<li><strong>Fix</strong>: Store and resume using exclusive upper bound semantics. When reconnecting, request replication from <code>lastProcessedLSN + 1</code> if database API supports it.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Transaction State Resurrection After Timeout</strong></p>\n<ul>\n<li><strong>Description</strong>: Transaction timeout emits abort events, but later the actual commit arrives from database</li>\n<li><strong>Why Wrong</strong>: Consumers see contradictory events (abort then commit) for same transaction</li>\n<li><strong>Fix</strong>: Maintain timed-out transaction IDs in short-term tombstone cache (5x timeout period) and discard late commits</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Kafka Producer Duplicate on Retry</strong></p>\n<ul>\n<li><strong>Description</strong>: Without idempotence, producer retries after timeout can create duplicate messages</li>\n<li><strong>Why Wrong</strong>: Breaks exactly-once semantics, consumers process same change twice</li>\n<li><strong>Fix</strong>: Enable <code>enable.idempotence=true</code> and configure <code>max.in.flight.requests.per.connection=5</code></li>\n</ul>\n<p>⚠️ <strong>Pitfall: Schema Cache Incoherency Across Restarts</strong></p>\n<ul>\n<li><strong>Description</strong>: Different pipeline instances have different schema versions in cache after partial registry failure</li>\n<li><strong>Why Wrong</strong>: Events serialized with wrong schema version, causing consumer deserialization failures</li>\n<li><strong>Fix</strong>: Implement registry cache invalidation protocol or use consistent distributed cache (Redis)</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations-table\">Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Health Checks</td>\n<td>ScheduledExecutorService + custom health endpoints</td>\n<td>Micrometer + Prometheus + Grafana alerts</td>\n</tr>\n<tr>\n<td>Retry Logic</td>\n<td>Exponential backoff with jitter (Resilience4j)</td>\n<td>Circuit breaker pattern with fallback (Resilience4j)</td>\n</tr>\n<tr>\n<td>State Recovery</td>\n<td>File-based offset store + JSON schemas</td>\n<td>Distributed state store (Apache BookKeeper)</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>SLF4J logs + JMX metrics</td>\n<td>OpenTelemetry tracing + metrics export</td>\n</tr>\n<tr>\n<td>Failure Injection</td>\n<td>Manual test scenarios</td>\n<td>Chaos Mesh for automated chaos testing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── error/\n│   │   ├── RecoveryCoordinator.java      # Orchestrates cross-component recovery\n│   │   ├── CircuitBreakerManager.java    # Manages circuit breakers per component\n│   │   └── FailureDetector.java          # Aggregates health checks\n│   ├── health/\n│   │   ├── HealthChecker.java            # Interface for component health\n│   │   ├── CompositeHealthCheck.java     # Aggregates all component health\n│   │   └── HealthEndpoint.java           # REST endpoint for health status\n│   ├── recovery/\n│   │   ├── OffsetRecoveryService.java    # Handles LSN recovery scenarios\n│   │   ├── TransactionRecoveryService.java # Recovers transaction state\n│   │   └── SchemaRecoveryService.java    # Handles schema registry failures\n│   └── monitor/\n│       ├── LagMonitor.java               # Consumer lag monitoring\n│       ├── ThroughputMonitor.java        # Pipeline throughput tracking\n│       └── AlertManager.java             # Sends alerts on threshold breaches\n└── src/test/java/com/cdc/\n    └── recovery/\n        ├── FailureInjectionTest.java     # Injects failures and verifies recovery\n        ├── ChaosTest.java                # Chaos engineering style tests\n        └── RecoveryIntegrationTest.java  # End-to-end recovery scenarios</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code-circuit-breaker-manager\">Infrastructure Starter Code: Circuit Breaker Manager</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Complete ready-to-use circuit breaker infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.error;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> io.github.resilience4j.circuitbreaker.CircuitBreaker;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> io.github.resilience4j.circuitbreaker.CircuitBreakerRegistry;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.time.Duration;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Map;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.ConcurrentHashMap;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> CircuitBreakerManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> CircuitBreakerRegistry registry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">CircuitBreaker</span><span style=\"color:#E1E4E8\">> breakers;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> CircuitBreakerManager</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Configure default circuit breaker settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CircuitBreakerConfig config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreakerConfig.</span><span style=\"color:#B392F0\">custom</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">failureRateThreshold</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\">// Open after 50% failure rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">slowCallRateThreshold</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// All calls considered for slowness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">slowCallDurationThreshold</span><span style=\"color:#E1E4E8\">(Duration.</span><span style=\"color:#B392F0\">ofSeconds</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">permittedNumberOfCallsInHalfOpenState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">slidingWindowSize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">minimumNumberOfCalls</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">waitDurationInOpenState</span><span style=\"color:#E1E4E8\">(Duration.</span><span style=\"color:#B392F0\">ofSeconds</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">automaticTransitionFromOpenToHalfOpenEnabled</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">recordExceptions</span><span style=\"color:#E1E4E8\">(IOException.class, TimeoutException.class)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">build</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreakerRegistry.</span><span style=\"color:#B392F0\">of</span><span style=\"color:#E1E4E8\">(config);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.breakers </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ConcurrentHashMap&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> CircuitBreaker </span><span style=\"color:#B392F0\">getOrCreate</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> breakers.</span><span style=\"color:#B392F0\">computeIfAbsent</span><span style=\"color:#E1E4E8\">(name, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            k </span><span style=\"color:#F97583\">-></span><span style=\"color:#E1E4E8\"> registry.</span><span style=\"color:#B392F0\">circuitBreaker</span><span style=\"color:#E1E4E8\">(k));</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> recordSuccess</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CircuitBreaker breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> breakers.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(name);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (breaker </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            breaker.</span><span style=\"color:#B392F0\">onSuccess</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> recordFailure</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#E1E4E8\">, Throwable </span><span style=\"color:#FFAB70\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CircuitBreaker breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> breakers.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(name);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (breaker </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            breaker.</span><span style=\"color:#B392F0\">onError</span><span style=\"color:#E1E4E8\">(error);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> boolean</span><span style=\"color:#B392F0\"> isCallPermitted</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CircuitBreaker breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> breakers.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(name);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> breaker </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> breaker.</span><span style=\"color:#B392F0\">tryAcquirePermission</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getState</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CircuitBreaker breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> breakers.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(name);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> breaker </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> ?</span><span style=\"color:#E1E4E8\"> breaker.</span><span style=\"color:#B392F0\">getState</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">:</span><span style=\"color:#9ECBFF\"> \"NO_BREAKER\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-recovery-coordinator\">Core Logic Skeleton: Recovery Coordinator</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.recovery;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.CdcPipeline;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.error.CircuitBreakerManager;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.health.HealthStatus;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> RecoveryCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> CircuitBreakerManager circuitBreakerManager;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> OffsetRecoveryService offsetRecoveryService;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> AppConfig config;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> volatile</span><span style=\"color:#E1E4E8\"> RecoveryState currentState;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> RecoveryCoordinator</span><span style=\"color:#E1E4E8\">(AppConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.circuitBreakerManager </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> CircuitBreakerManager</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.offsetRecoveryService </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> OffsetRecoveryService</span><span style=\"color:#E1E4E8\">(config);</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.currentState </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RecoveryState.NORMAL;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> handleComponentFailure</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">componentName</span><span style=\"color:#E1E4E8\">, Throwable </span><span style=\"color:#FFAB70\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Record failure in circuit breaker for this component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Check if component is in OPEN circuit state (too many failures)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: If OPEN, transition pipeline to DEGRADED health status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Determine recovery strategy based on component and error type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Execute appropriate recovery: retry, reset, or wait for manual intervention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: If recovery successful, record success in circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: Transition pipeline back to HEALTHY status when all components recovered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> onPipelineStart</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Load last known offsets from all offset stores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Verify offset consistency across components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: If inconsistencies found, run offset recovery protocol</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Initialize all circuit breakers to CLOSED state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Start background health monitoring threads</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> onPipelineStop</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Execute graceful shutdown sequence for all components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Flush all pending transactions and events</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Persist final offsets with checksums</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Close all circuit breakers and cleanup resources</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> RecoveryStrategy </span><span style=\"color:#B392F0\">determineRecoveryStrategy</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, Throwable </span><span style=\"color:#FFAB70\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Classify error type: transient, persistent, or unknown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Check component-specific recovery policies from config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Consider current system state and other component health</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Return appropriate strategy: RETRY, RESET, DEGRADE, or HALT</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    enum</span><span style=\"color:#B392F0\"> RecoveryState</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        NORMAL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">DEGRADED</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">RECOVERING</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">HALTED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    enum</span><span style=\"color:#B392F0\"> RecoveryStrategy</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        RETRY_WITH_BACKOFF</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">RESET_AND_RECOVER</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">DEGRADE_FUNCTIONALITY</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">HALT_FOR_INTERVENTION</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints-java-error-handling\">Language-Specific Hints: Java Error Handling</h4>\n<ul>\n<li><strong>Use Resilience4j</strong> for circuit breakers, retries, and bulkheads instead of rolling your own</li>\n<li><strong>For database reconnection</strong>, use HikariCP connection pool with proper validation queries</li>\n<li><strong>Monitor JVM metrics</strong> using Micrometer and expose via <code>/actuator/metrics</code> endpoint</li>\n<li><strong>Use try-with-resources</strong> for all Closeable database and network connections</li>\n<li><strong>Implement <code>Thread.UncaughtExceptionHandler</code></strong> for worker threads to capture and log unexpected crashes</li>\n<li><strong>For memory-sensitive components</strong>, use <code>SoftReference</code> for caches that can be GC&#39;d under memory pressure</li>\n</ul>\n<h4 id=\"milestone-checkpoint-recovery-verification\">Milestone Checkpoint: Recovery Verification</h4>\n<p><strong>After implementing error handling</strong>, run the recovery verification test:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># 1. Start the pipeline with a test database</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">./gradlew</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#79B8FF\"> --args=</span><span style=\"color:#9ECBFF\">'config/test-config.yaml'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 2. Insert some test data to establish baseline</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">INSERT</span><span style=\"color:#9ECBFF\"> INTO</span><span style=\"color:#9ECBFF\"> test_table</span><span style=\"color:#9ECBFF\"> VALUES</span><span style=\"color:#E1E4E8\"> (1, </span><span style=\"color:#9ECBFF\">'initial'</span><span style=\"color:#E1E4E8\">), (</span><span style=\"color:#B392F0\">2,</span><span style=\"color:#9ECBFF\"> 'data'</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 3. Simulate a failure (in another terminal)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">kill</span><span style=\"color:#79B8FF\"> -SIGSTOP</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#9ECBFF\">connector_pi</span><span style=\"color:#E1E4E8\">d</span><span style=\"color:#F97583\">></span><span style=\"color:#6A737D\">  # Pause the log connector thread</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 4. Make more changes while connector is paused</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">INSERT</span><span style=\"color:#9ECBFF\"> INTO</span><span style=\"color:#9ECBFF\"> test_table</span><span style=\"color:#9ECBFF\"> VALUES</span><span style=\"color:#E1E4E8\"> (3, </span><span style=\"color:#9ECBFF\">'during_failure'</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 5. Resume the connector</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">kill</span><span style=\"color:#79B8FF\"> -SIGCONT</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#9ECBFF\">connector_pi</span><span style=\"color:#E1E4E8\">d</span><span style=\"color:#F97583\">></span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># 6. Verify recovery:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Connector should reconnect and resume from last LSN</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - All events (including #3) should be delivered to Kafka</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - No duplicates of events #1 and #2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Health endpoint should show brief DEGRADED then back to HEALTHY</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/health</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: {\"status\":\"HEALTHY\",\"components\":{\"logConnector\":\"HEALTHY\",...}}</span></span></code></pre></div>\n\n<p><strong>Signs of correct recovery</strong>:</p>\n<ul>\n<li>Consumer receives exactly 3 events (no duplicates)</li>\n<li>Pipeline throughput returns to normal within 30 seconds</li>\n<li>Logs show &quot;Reconnected to database&quot; and &quot;Resumed from LSN: X/Y&quot;</li>\n</ul>\n<p><strong>Signs of incorrect recovery</strong>:</p>\n<ul>\n<li>Consumer receives 4+ events (duplicates present)</li>\n<li>Pipeline remains in DEGRADED state</li>\n<li>Logs show &quot;Gap detected&quot; or &quot;Could not resume from LSN&quot;</li>\n</ul>\n<h4 id=\"debugging-tips-for-error-recovery\">Debugging Tips for Error Recovery</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Duplicate events after restart</strong></td>\n<td>Off-by-one in LSN recovery</td>\n<td>Compare <code>lastProcessedLSN</code> in offset store vs database&#39;s current LSN</td>\n<td>Resume from <code>lastProcessedLSN + 1</code></td>\n</tr>\n<tr>\n<td><strong>Pipeline hangs indefinitely after failure</strong></td>\n<td>Circuit breaker stuck OPEN</td>\n<td>Check circuit breaker state via <code>/actuator/circuitbreakers</code></td>\n<td>Manually reset breaker or adjust failure threshold</td>\n</tr>\n<tr>\n<td><strong>Transactions split across restart</strong></td>\n<td>Incomplete flush on shutdown</td>\n<td>Check shutdown logs for &quot;flushPendingTransactions&quot;</td>\n<td>Implement graceful shutdown hook in <code>CdcPipeline.stop()</code></td>\n</tr>\n<tr>\n<td><strong>Schema mismatch after registry restart</strong></td>\n<td>Cache incoherency</td>\n<td>Compare schema version in event vs registry <code>getLatestSchema()</code></td>\n<td>Implement cache invalidation on registry update</td>\n</tr>\n<tr>\n<td><strong>Consumer lag spikes but no backpressure</strong></td>\n<td>Backpressure signal not propagating</td>\n<td>Check <code>LogConnector.buffer</code> size and <code>backpressureSignal</code> state</td>\n<td>Ensure backpressure chain: Consumer → Streamer → EventBuilder → Connector</td>\n</tr>\n<tr>\n<td><strong>Memory grows unbounded during network partition</strong></td>\n<td>Producer buffer not limited</td>\n<td>Monitor <code>buffer.memory</code> usage in Kafka producer metrics</td>\n<td>Set <code>buffer.memory</code> limit and implement circuit breaker</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Log Parsing &amp; Change Events), Milestone 2 (Event Streaming &amp; Delivery), Milestone 3 (Schema Evolution &amp; Compatibility)</p>\n</blockquote>\n<h2 id=\"11-testing-strategy\">11. Testing Strategy</h2>\n<p>Testing a Change Data Capture system is akin to verifying a complex pipeline that transforms raw, low-level signals into meaningful, ordered messages. The core challenge lies in validating three critical properties: <strong>correctness of transformation</strong> (parsing raw logs into structured events), <strong>reliability of delivery</strong> (ensuring no data loss and proper ordering), and <strong>robustness to evolution</strong> (handling schema changes without breaking consumers). This section outlines a comprehensive testing approach that builds confidence incrementally, from isolated unit tests to full-system integration tests, ensuring each milestone&#39;s acceptance criteria are met.</p>\n<h3 id=\"111-test-pyramid-for-cdc\">11.1 Test Pyramid for CDC</h3>\n<p>The CDC testing strategy follows the classic <strong>test pyramid</strong> model, but with specialized layers tailored to the unique characteristics of data pipeline systems. Think of it as building a quality verification pipeline that mirrors the data pipeline itself: we test each component in isolation, then verify their integration, and finally validate the entire system&#39;s behavior under real-world conditions.</p>\n<blockquote>\n<p><strong>Key Insight:</strong> In CDC systems, the most critical and difficult-to-test behaviors occur at the boundaries between components and during failure scenarios. Therefore, the testing pyramid should be bottom-heavy for logic correctness but invest significantly in integration and end-to-end tests for reliability guarantees.</p>\n</blockquote>\n<h4 id=\"layer-1-unit-tests-the-foundation\">Layer 1: Unit Tests (The Foundation)</h4>\n<p>Unit tests verify individual components in complete isolation, mocking all dependencies. Their primary goal is to ensure each component&#39;s internal logic behaves correctly according to its specification.</p>\n<p><strong>Mental Model:</strong> Think of unit tests as quality control stations on an assembly line. Each station inspects a single component (e.g., a gear, a circuit board) in isolation, ensuring it meets specifications before it&#39;s installed in the larger machine. If a gear has the wrong number of teeth, we catch it here before the entire assembly fails.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Key Test Scenarios</th>\n<th>Mock Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>LogParser</code> implementations</td>\n<td>- Parse valid binary log entry → correct <code>RawLogEntry</code><br>- Parse malformed binary data → appropriate exception<br>- Handle different operation types (INSERT, UPDATE, DELETE)<br>- Extract LSN from various log formats</td>\n<td>Raw byte source (mocked)</td>\n</tr>\n<tr>\n<td><code>ChangeEventBuilder</code></td>\n<td>- Process single operation → emit single <code>ChangeEvent</code><br>- Process multiple operations in transaction → batch emission on commit<br>- Handle UPDATE with before/after images<br>- Deduplicate operations on same primary key within transaction<br>- Timeout stale transactions</td>\n<td><code>SchemaRegistry</code> (mocked)</td>\n</tr>\n<tr>\n<td><code>EventSerializer</code></td>\n<td>- Serialize <code>ChangeEvent</code> with schema → bytes<br>- Deserialize bytes with schema → identical <code>ChangeEvent</code><br>- Handle missing fields during deserialization<br>- Version mismatch handling</td>\n<td>None (pure function)</td>\n</tr>\n<tr>\n<td><code>FileOffsetStore</code></td>\n<td>- Save LSN → file persisted<br>- Load LSN from file → correct value returned<br>- Handle missing offset file → return null/default</td>\n<td>File system (could use temp files)</td>\n</tr>\n<tr>\n<td><code>SchemaRegistry</code></td>\n<td>- Register new schema → version incremented<br>- Check compatibility → pass/fail correctly<br>- Retrieve schema by ID/version</td>\n<td>Underlying storage (in-memory)</td>\n</tr>\n</tbody></table>\n<p><strong>Common Pitfalls in Unit Testing CDC Components:</strong></p>\n<p>⚠️ <strong>Pitfall: Testing with overly simplistic mock data that doesn&#39;t reflect real database log formats.</strong></p>\n<ul>\n<li><strong>Why it&#39;s wrong:</strong> Database logs contain complex binary structures, flags, headers, and metadata. Using simple JSON-like structures in tests gives false confidence—the parser might pass tests but fail on real data.</li>\n<li><strong>How to avoid:</strong> Use <strong>actual binary log snippets</strong> captured from the target database (PostgreSQL/MySQL) as test fixtures. These can be obtained through database utilities and stored as binary files in test resources.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Not testing transaction boundary behavior thoroughly.</strong></p>\n<ul>\n<li><strong>Why it&#39;s wrong:</strong> CDC&#39;s core guarantee is transaction atomicity—all operations in a transaction should be emitted together. Missing edge cases (like partial transaction reads after crash) can break this guarantee.</li>\n<li><strong>How to avoid:</strong> Create comprehensive test cases for transaction state machine transitions: begin → multiple operations → commit, begin → operation → rollback, nested transactions (if supported), and interrupted transactions.</li>\n</ul>\n<p>⚠️ <strong>Pitfall: Ignoring schema evolution scenarios in serializer tests.</strong></p>\n<ul>\n<li><strong>Why it&#39;s wrong:</strong> In production, schemas evolve constantly. Serializers must handle backward/forward compatibility correctly, which is complex logic that needs explicit testing.</li>\n<li><strong>How to avoid:</strong> Test serialization/deserialization across multiple schema versions, verifying that added columns appear as null/default in old schemas and removed columns are ignored gracefully.</li>\n</ul>\n<h4 id=\"layer-2-integration-tests-the-connective-tissue\">Layer 2: Integration Tests (The Connective Tissue)</h4>\n<p>Integration tests verify that components work correctly together and with external systems. For CDC, this primarily means testing the interaction with actual database instances and message brokers.</p>\n<p><strong>Mental Model:</strong> Imagine testing the plumbing in a house after installing all pipes and fixtures. We turn on the water and check for leaks at connection points, verify water pressure through the system, and ensure drainage works. Similarly, integration tests verify data flows correctly between components and that connections to external systems (database, Kafka) are robust.</p>\n<table>\n<thead>\n<tr>\n<th>Integration Point</th>\n<th>Test Scenario</th>\n<th>Setup Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>LogConnector</code> ↔ Database</td>\n<td>- Connect to database and read WAL/binlog<br>- Resume from saved LSN after restart<br>- Handle database restart/connection drop<br>- Process large transactions without OOM</td>\n<td>Real/embedded PostgreSQL or MySQL instance with logical replication enabled</td>\n</tr>\n<tr>\n<td><code>EventStreamer</code> ↔ Kafka</td>\n<td>- Publish events to Kafka topic<br>- Verify partitioning by primary key<br>- Validate at-least-once delivery with producer retries<br>- Consumer can read and process events</td>\n<td>Testcontainers with Kafka container, or embedded Kafka</td>\n</tr>\n<tr>\n<td><code>SchemaRegistry</code> ↔ Database DDL</td>\n<td>- Detect <code>ALTER TABLE</code> and register new schema<br>- Emit <code>SchemaChangeEvent</code> to dedicated topic<br>- Verify compatibility checks block breaking changes</td>\n<td>Database instance with DDL execution capability</td>\n</tr>\n<tr>\n<td>CDC Pipeline End-to-End</td>\n<td>- Database INSERT → <code>ChangeEvent</code> appears in Kafka<br>- Transaction atomicity preserved<br>- Ordering guarantee per primary key</td>\n<td>Full pipeline with source database and Kafka</td>\n</tr>\n</tbody></table>\n<p><strong>Integration Testing Architecture Decision Record:</strong></p>\n<blockquote>\n<p><strong>Decision: Use Testcontainers for Database and Kafka Integration Tests</strong></p>\n<ul>\n<li><strong>Context:</strong> We need realistic integration tests that verify component interactions with actual PostgreSQL/MySQL and Kafka, but cannot rely on pre-existing external services that may not be available in CI environments.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Embedded databases (H2, Derby) with simulation:</strong> Use in-memory databases that partially emulate PostgreSQL/MySQL WAL or binlog behavior.</li>\n<li><strong>Docker containers managed manually:</strong> Scripts to start/stop Docker containers before test runs.</li>\n<li><strong>Testcontainers library:</strong> Programmatic management of Docker containers as part of test lifecycle.</li>\n<li><strong>Mock servers (WireMock for HTTP, MockKafka):</strong> Simulate external systems with mocked behavior.</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Use Testcontainers for database and Kafka integration tests.</li>\n<li><strong>Rationale:</strong> <ul>\n<li><strong>Realistic behavior:</strong> Testcontainers provide actual PostgreSQL/MySQL and Kafka instances, ensuring we test against real protocol implementations and binary formats.</li>\n<li><strong>CI compatibility:</strong> Containers run seamlessly in CI environments without special configuration.</li>\n<li><strong>Lifecycle management:</strong> Testcontainers handles container startup, cleanup, and port assignment automatically.</li>\n<li><strong>Development experience:</strong> Developers can run tests locally without installing database/Kafka binaries.</li>\n</ul>\n</li>\n<li><strong>Consequences:</strong><ul>\n<li><strong>Slower tests:</strong> Container startup adds 10-30 seconds per test class.</li>\n<li><strong>Docker dependency:</strong> Requires Docker installed on development and CI machines.</li>\n<li><strong>Resource usage:</strong> Multiple containers running concurrently during test suites.</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Why Not Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Embedded databases</td>\n<td>Fast startup, no external dependencies</td>\n<td>Doesn&#39;t test real WAL/binlog formats, behavioral differences</td>\n<td>Critical to test real log parsing</td>\n</tr>\n<tr>\n<td>Manual Docker scripts</td>\n<td>Real systems, full control</td>\n<td>Complex lifecycle management, port conflicts, CI unfriendly</td>\n<td>Too much maintenance overhead</td>\n</tr>\n<tr>\n<td><strong>Testcontainers</strong></td>\n<td><strong>Real systems, automatic management, CI-friendly</strong></td>\n<td><strong>Slower tests, requires Docker</strong></td>\n<td><strong>Best balance of realism and practicality</strong></td>\n</tr>\n<tr>\n<td>Mock servers</td>\n<td>Very fast, deterministic</td>\n<td>Unrealistic behavior misses edge cases</td>\n<td>Insufficient for testing protocol-level interactions</td>\n</tr>\n</tbody></table>\n<h4 id=\"layer-3-end-to-end-tests-the-system-verification\">Layer 3: End-to-End Tests (The System Verification)</h4>\n<p>End-to-end (E2E) tests validate the complete system against real-world scenarios and acceptance criteria. They simulate production-like workloads and verify the system&#39;s guarantees holistically.</p>\n<p><strong>Mental Model:</strong> Consider E2E tests as a dress rehearsal for a theatrical production. All actors (components), props (databases, brokers), and stage directions (orchestration) come together for a full performance. We verify the audience (consumers) receives the intended story (data) correctly, even when unexpected events occur (failures).</p>\n<p><strong>Core E2E Test Scenarios:</strong></p>\n<ol>\n<li><p><strong>At-Least-Once Delivery Verification:</strong></p>\n<ul>\n<li><strong>Setup:</strong> Pipeline running, consumer reading from Kafka</li>\n<li><strong>Action:</strong> Kill pipeline process abruptly mid-processing</li>\n<li><strong>Verification:</strong> Restart pipeline, verify all events eventually delivered (duplicates allowed but documented)</li>\n<li><strong>Measurement:</strong> Count events at source vs. consumer, allow for small discrepancy window</li>\n</ul>\n</li>\n<li><p><strong>Ordering Guarantee Per Primary Key:</strong></p>\n<ul>\n<li><strong>Setup:</strong> Pipeline running with multiple concurrent writers to same table</li>\n<li><strong>Action:</strong> Execute sequence of updates to same row: UPDATE row A (v1→v2), UPDATE row A (v2→v3), DELETE row A</li>\n<li><strong>Verification:</strong> Consumer receives events in exact sequence; no v3 before v2, no DELETE before final UPDATE</li>\n</ul>\n</li>\n<li><p><strong>Schema Evolution Handling:</strong></p>\n<ul>\n<li><strong>Setup:</strong> Pipeline with active consumer using schema v1</li>\n<li><strong>Action:</strong> Execute <code>ALTER TABLE ADD COLUMN new_field VARCHAR(50) DEFAULT &#39;default&#39;</code></li>\n<li><strong>Verification:</strong> <ul>\n<li>Schema registry creates v2 with backward compatibility</li>\n<li>New events include <code>new_field</code> with &#39;default&#39; or actual values</li>\n<li>Consumer using v1 schema can still deserialize events (new field ignored)</li>\n<li><code>SchemaChangeEvent</code> published to notify consumers</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Backpressure Propagation:</strong></p>\n<ul>\n<li><strong>Setup:</strong> Pipeline with slow consumer (artificial delay)</li>\n<li><strong>Action:</strong> High-volume writes to source database</li>\n<li><strong>Verification:</strong> <ul>\n<li>Pipeline slows down ingestion (observe <code>LogConnector</code> pause state)</li>\n<li>No OOM errors from unbounded buffering</li>\n<li>System recovers when consumer catches up</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Recovery from Corruption/Gap Detection:</strong></p>\n<ul>\n<li><strong>Setup:</strong> Pipeline with offset tracking</li>\n<li><strong>Action:</strong> Manually corrupt offset store or delete WAL segments</li>\n<li><strong>Verification:</strong> <ul>\n<li>Pipeline detects gap/invalid LSN</li>\n<li>Emits <code>gap detection event</code> or enters recovery mode</li>\n<li>Can be configured to halt or continue from latest</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>E2E Test Environment Requirements:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Recommended Implementation</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Source Database</td>\n<td>PostgreSQL container (logical replication enabled) or MySQL container (binlog enabled)</td>\n<td>Real transaction log generation</td>\n</tr>\n<tr>\n<td>Message Broker</td>\n<td>Kafka container with 3 brokers (test replication)</td>\n<td>Real event streaming with persistence</td>\n</tr>\n<tr>\n<td>Test Harness</td>\n<td>JUnit test with <code>@Testcontainers</code>, setup/teardown methods</td>\n<td>Orchestrate test scenario and verification</td>\n</tr>\n<tr>\n<td>Verification Consumer</td>\n<td>Custom Kafka consumer with state tracking</td>\n<td>Validate delivery guarantees</td>\n</tr>\n<tr>\n<td>Load Generator</td>\n<td>Separate thread executing SQL statements</td>\n<td>Simulate database workload</td>\n</tr>\n</tbody></table>\n<h3 id=\"112-milestone-verification-checkpoints\">11.2 Milestone Verification Checkpoints</h3>\n<p>Each project milestone has specific acceptance criteria that must be demonstrably verified. These checkpoints provide concrete, actionable steps to validate that the implementation meets requirements.</p>\n<h4 id=\"milestone-1-log-parsing-amp-change-events\">Milestone 1: Log Parsing &amp; Change Events</h4>\n<p><strong>Checkpoint Goal:</strong> Validate that the CDC pipeline can correctly read database transaction logs, parse them into structured change events with before/after images, and track position for resumption.</p>\n<table>\n<thead>\n<tr>\n<th>Acceptance Criteria</th>\n<th>Verification Method</th>\n<th>Expected Result</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parser connects to PostgreSQL WAL or MySQL binlog</td>\n<td>Integration test with real database</td>\n<td>Connection established, replication slot created (PostgreSQL) or binlog client connected (MySQL)</td>\n<td>Connection errors, permission denied, replication not enabled</td>\n</tr>\n<tr>\n<td>INSERT, UPDATE, DELETE operations correctly extracted</td>\n<td>Unit tests with real binary log fixtures</td>\n<td><code>RawLogEntry</code> contains correct <code>operationType</code>, <code>tableName</code>, <code>rowData</code></td>\n<td>Missing operations, wrong table, incorrect row values</td>\n</tr>\n<tr>\n<td>Change events include before/after images for UPDATE</td>\n<td>Test scenario: UPDATE row with known values</td>\n<td><code>ChangeEvent.beforeImage</code> has old values, <code>ChangeEvent.afterImage</code> has new values</td>\n<td>Missing before image, swapped images, partial data</td>\n</tr>\n<tr>\n<td>Position tracking enables resuming after restart</td>\n<td>Integration test: Stop pipeline, note LSN, restart, verify continuation</td>\n<td>Pipeline resumes from same LSN, no missed events, no duplicates from before LSN</td>\n<td>Starts from beginning, skips events, infinite duplicate loop</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Procedure for Milestone 1:</strong></p>\n<ol>\n<li><p><strong>Setup test environment:</strong></p>\n<ul>\n<li>Start PostgreSQL container with <code>wal_level=logical</code></li>\n<li>Create test table with primary key</li>\n<li>Initialize CDC pipeline with <code>LogConnector</code> and <code>ChangeEventBuilder</code></li>\n</ul>\n</li>\n<li><p><strong>Execute test sequence:</strong></p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">   BEGIN</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   INSERT INTO</span><span style=\"color:#E1E4E8\"> test_table (id, </span><span style=\"color:#F97583\">name</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">VALUES</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'Alice'</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   UPDATE</span><span style=\"color:#E1E4E8\"> test_table </span><span style=\"color:#F97583\">SET</span><span style=\"color:#F97583\"> name</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'Alicia'</span><span style=\"color:#F97583\"> WHERE</span><span style=\"color:#E1E4E8\"> id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   DELETE</span><span style=\"color:#F97583\"> FROM</span><span style=\"color:#E1E4E8\"> test_table </span><span style=\"color:#F97583\">WHERE</span><span style=\"color:#E1E4E8\"> id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">   COMMIT</span><span style=\"color:#E1E4E8\">;</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><p><strong>Verify pipeline output:</strong></p>\n<ul>\n<li>Capture events from <code>ChangeEventBuilder</code></li>\n<li>Validate: 3 events (INSERT, UPDATE, DELETE)</li>\n<li>For UPDATE: <code>beforeImage.name=&#39;Alice&#39;</code>, <code>afterImage.name=&#39;Alicia&#39;</code></li>\n<li>Check <code>eventId</code> is deterministic and unique per operation</li>\n</ul>\n</li>\n<li><p><strong>Test restart recovery:</strong></p>\n<ul>\n<li>Note <code>lastProcessedLSN</code> from <code>LogConnector</code></li>\n<li>Stop pipeline</li>\n<li>Insert new row in database</li>\n<li>Restart pipeline</li>\n<li>Verify: Pipeline reads from saved LSN, captures new row event, does NOT re-emit old events</li>\n</ul>\n</li>\n</ol>\n<p><strong>Success Signal:</strong> All verification steps pass; pipeline correctly handles the complete lifecycle of a row with proper images and position tracking.</p>\n<h4 id=\"milestone-2-event-streaming-amp-delivery\">Milestone 2: Event Streaming &amp; Delivery</h4>\n<p><strong>Checkpoint Goal:</strong> Validate that change events are reliably delivered to consumers with at-least-once semantics and ordering guarantees per primary key, while handling backpressure.</p>\n<table>\n<thead>\n<tr>\n<th>Acceptance Criteria</th>\n<th>Verification Method</th>\n<th>Expected Result</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Events published to Kafka partitioned by table and primary key</td>\n<td>Integration test with Kafka, inspect message metadata</td>\n<td>Events for same table+pk go to same partition; different pkeys may go to different partitions</td>\n<td>All events to single partition, random partitioning, partition imbalance</td>\n</tr>\n<tr>\n<td>At-least-once delivery after transient failures</td>\n<td>Simulate Kafka broker restart during production</td>\n<td>Consumer eventually receives all events (may have duplicates); producer logs show retries</td>\n<td>Events lost permanently, pipeline hangs, consumer starves</td>\n</tr>\n<tr>\n<td>Events for same primary key delivered in order</td>\n<td>Concurrent updates to same row, verify sequence</td>\n<td>Consumer sees events in commit order; no timestamp inversion for same pk</td>\n<td>Out-of-order delivery, UPDATE before INSERT for same key</td>\n</tr>\n<tr>\n<td>Consumer lag monitoring and alerts</td>\n<td>Test with slow consumer, fast producer</td>\n<td>Lag metric increases, alert triggers when threshold exceeded</td>\n<td>No lag reporting, false alerts, missing alert on actual lag</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Procedure for Milestone 2:</strong></p>\n<ol>\n<li><p><strong>Setup partitioned streaming test:</strong></p>\n<ul>\n<li>Start Kafka container with 3 partitions for test topic</li>\n<li>Create pipeline with <code>KafkaEventPublisher</code> configured for partitioning by <code>tableName</code> and primary key</li>\n<li>Start verification consumer tracking partition assignments</li>\n</ul>\n</li>\n<li><p><strong>Test at-least-once delivery:</strong></p>\n<ul>\n<li>Begin producing events</li>\n<li>Midway, kill Kafka broker container</li>\n<li>Wait 10 seconds, restart broker</li>\n<li>Continue producing</li>\n<li>Verify consumer receives all events (compare source vs. consumed count)</li>\n<li>Allow for small number of duplicates (retry behavior)</li>\n</ul>\n</li>\n<li><p><strong>Test ordering guarantee:</strong></p>\n<ul>\n<li>Create 3 threads concurrently updating same row with sequence numbers</li>\n<li>Each thread: <code>UPDATE table SET seq = X WHERE id = 1</code></li>\n<li>Verify consumer sees monotonic increasing seq values only</li>\n<li>No skipped sequence numbers (unless transaction rolled back)</li>\n</ul>\n</li>\n<li><p><strong>Test backpressure propagation:</strong></p>\n<ul>\n<li>Configure consumer with 1-second artificial processing delay</li>\n<li>Produce events at high rate (1000/second)</li>\n<li>Monitor <code>KafkaEventPublisher.inFlightEventCount</code></li>\n<li>Verify it plateaus (doesn&#39;t grow unbounded)</li>\n<li>Observe <code>LogConnector</code> enters PAUSED state or slows ingestion</li>\n</ul>\n</li>\n</ol>\n<p><strong>Success Signal:</strong> Events are reliably delivered despite failures, ordering per primary key is preserved, and the system gracefully handles consumer slowdowns without resource exhaustion.</p>\n<h4 id=\"milestone-3-schema-evolution-amp-compatibility\">Milestone 3: Schema Evolution &amp; Compatibility</h4>\n<p><strong>Checkpoint Goal:</strong> Validate that schema changes are handled gracefully with versioning, compatibility checking, and consumer notification.</p>\n<table>\n<thead>\n<tr>\n<th>Acceptance Criteria</th>\n<th>Verification Method</th>\n<th>Expected Result</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Schema registry stores versioned schemas</td>\n<td>Unit/integration tests with registry</td>\n<td>Schema stored with version ID, retrievable by ID/version, history maintained</td>\n<td>Missing versions, incorrect retrieval, no version increment</td>\n</tr>\n<tr>\n<td>Column additions pass backward compatibility</td>\n<td>Test: <code>ALTER TABLE ADD COLUMN new_col INT DEFAULT 0</code></td>\n<td>New schema registered successfully, events include new column, old consumers can still read</td>\n<td>Compatibility check fails incorrectly, events unreadable by old consumers</td>\n</tr>\n<tr>\n<td>Column removal fails forward compatibility</td>\n<td>Test: <code>ALTER TABLE DROP COLUMN existing_col</code> (if nullable)</td>\n<td>Schema registration rejected (if forward compatibility required) or warning issued</td>\n<td>Allowed when shouldn&#39;t be, data loss occurs</td>\n</tr>\n<tr>\n<td>Schema change events notify consumers</td>\n<td>Test: ALTER TABLE, monitor schema change topic</td>\n<td><code>SchemaChangeEvent</code> published with old/new schema IDs, consumers can update</td>\n<td>No event published, event missing critical information</td>\n</tr>\n</tbody></table>\n<p><strong>Verification Procedure for Milestone 3:</strong></p>\n<ol>\n<li><p><strong>Setup schema evolution test:</strong></p>\n<ul>\n<li>Initialize <code>SchemaRegistry</code> (file-based or real)</li>\n<li>Start pipeline with schema integration</li>\n<li>Register initial schema for test table</li>\n</ul>\n</li>\n<li><p><strong>Test backward-compatible addition:</strong></p>\n<ul>\n<li>Execute: <code>ALTER TABLE test ADD COLUMN email VARCHAR(100) NULL</code></li>\n<li>Verify: New schema version registered (v2)</li>\n<li>Insert row with email value</li>\n<li>Verify event contains email field</li>\n<li>Test old consumer (using v1 schema) can deserialize event (email field ignored)</li>\n</ul>\n</li>\n<li><p><strong>Test forward compatibility violation:</strong></p>\n<ul>\n<li>Set compatibility mode to <code>FORWARD</code> (or <code>FULL</code>)</li>\n<li>Attempt: <code>ALTER TABLE test DROP COLUMN name</code></li>\n<li>Verify: Schema registration rejected with clear error</li>\n<li>Verify: No <code>SchemaChangeEvent</code> published</li>\n</ul>\n</li>\n<li><p><strong>Test schema change notification:</strong></p>\n<ul>\n<li>Subscribe consumer to schema change topic</li>\n<li>Execute compatible ALTER TABLE</li>\n<li>Verify consumer receives <code>SchemaChangeEvent</code> within 5 seconds</li>\n<li>Event contains: <code>tableName</code>, <code>oldSchemaId</code>, <code>newSchemaId</code>, <code>changeTimestamp</code></li>\n</ul>\n</li>\n<li><p><strong>Test type coercion rules:</strong></p>\n<ul>\n<li>Initial schema: <code>column1 INT</code></li>\n<li>New schema: <code>column1 BIGINT</code> (widening)</li>\n<li>Verify: Allowed (backward compatible)</li>\n<li>New schema: <code>column1 SMALLINT</code> (narrowing)</li>\n<li>Verify: Rejected or warned about potential data loss</li>\n</ul>\n</li>\n</ol>\n<p><strong>Success Signal:</strong> Schema registry properly manages versions, compatibility checks prevent breaking changes, and consumers are notified of evolution to adapt their processing logic.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option (Getting Started)</th>\n<th>Advanced Option (Production-like)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>JUnit 5 + AssertJ + Mockito</td>\n<td>JUnit 5 + AssertJ + Mockito + JaCoCo (code coverage)</td>\n</tr>\n<tr>\n<td>Database Integration</td>\n<td>Testcontainers PostgreSQL/MySQL</td>\n<td>Testcontainers with custom initialization scripts</td>\n</tr>\n<tr>\n<td>Kafka Integration</td>\n<td>Testcontainers Kafka</td>\n<td>Testcontainers Kafka with Schema Registry container</td>\n</tr>\n<tr>\n<td>Schema Testing</td>\n<td>In-memory <code>FileBasedSchemaRegistry</code></td>\n<td>Integration with actual Schema Registry (e.g., Confluent)</td>\n</tr>\n<tr>\n<td>Load/Stress Testing</td>\n<td>Simple multithreaded producers</td>\n<td>Gatling/Taurus for systematic load testing</td>\n</tr>\n<tr>\n<td>Chaos Testing</td>\n<td>Manual failure injection</td>\n<td>Chaos Mesh/Litmus for automated chaos experiments</td>\n</tr>\n<tr>\n<td>E2E Test Orchestration</td>\n<td>JUnit <code>@Testcontainers</code> with <code>@BeforeAll</code></td>\n<td>Dedicated test runner with separate monitoring</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-system/\n├── src/main/java/com/cdc/\n│   ├── core/                    # Core domain types and interfaces\n│   │   ├── ChangeEvent.java\n│   │   ├── RawLogEntry.java\n│   │   └── SchemaVersion.java\n│   ├── connector/               # Milestone 1 components\n│   │   ├── LogConnector.java\n│   │   ├── parser/\n│   │   │   ├── LogParser.java\n│   │   │   ├── PostgresWalParser.java\n│   │   │   └── MySqlBinlogParser.java\n│   │   └── offset/\n│   │       ├── OffsetStore.java\n│   │       └── FileOffsetStore.java\n│   ├── builder/                 # ChangeEventBuilder\n│   │   ├── ChangeEventBuilder.java\n│   │   ├── TransactionState.java\n│   │   └── TransactionOperation.java\n│   ├── streaming/               # Milestone 2 components\n│   │   ├── EventStreamer.java\n│   │   ├── KafkaEventPublisher.java\n│   │   ├── DeliveryCallback.java\n│   │   └── metrics/\n│   │       └── StreamerMetrics.java\n│   ├── schema/                  # Milestone 3 components\n│   │   ├── SchemaRegistry.java\n│   │   ├── FileBasedSchemaRegistry.java\n│   │   ├── SchemaChangeEvent.java\n│   │   └── compatibility/\n│   │       └── CompatibilityChecker.java\n│   └── config/                  # Configuration\n│       ├── AppConfig.java\n│       ├── DatabaseConfig.java\n│       └── ConfigLoader.java\n└── src/test/java/com/cdc/       # Test structure mirrors main\n    ├── unit/\n    │   ├── connector/\n    │   │   ├── LogConnectorTest.java\n    │   │   └── parser/\n    │   │       ├── PostgresWalParserTest.java\n    │   │       └── fixtures/    # Binary log fixture files\n    │   ├── builder/\n    │   │   └── ChangeEventBuilderTest.java\n    │   ├── streaming/\n    │   │   └── KafkaEventPublisherTest.java\n    │   └── schema/\n    │       └── SchemaRegistryTest.java\n    ├── integration/\n    │   ├── DatabaseIntegrationTest.java\n    │   ├── KafkaIntegrationTest.java\n    │   └── resources/\n    │       └── testcontainers.properties\n    └── e2e/\n        ├── DeliveryGuaranteeTest.java\n        ├── SchemaEvolutionTest.java\n        └── RecoveryTest.java</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Testcontainers Configuration Helper:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/test/java/com/cdc/testutils/TestContainersSetup.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.testutils;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.testcontainers.containers.GenericContainer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.testcontainers.containers.KafkaContainer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.testcontainers.containers.PostgreSQLContainer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.testcontainers.utility.DockerImageName;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.HashMap;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.Map;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> TestContainersSetup</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String POSTGRES_IMAGE </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"postgres:15-alpine\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> String KAFKA_IMAGE </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"confluentinc/cp-kafka:7.4.0\"</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> PostgreSQLContainer&#x3C;</span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\">> postgresContainer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> KafkaContainer kafkaContainer;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> synchronized</span><span style=\"color:#E1E4E8\"> PostgreSQLContainer&#x3C;</span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getPostgresContainer</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (postgresContainer </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            postgresContainer </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> PostgreSQLContainer&#x3C;>(DockerImageName.</span><span style=\"color:#B392F0\">parse</span><span style=\"color:#E1E4E8\">(POSTGRES_IMAGE))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                .</span><span style=\"color:#B392F0\">withDatabaseName</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"cdc_test\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                .</span><span style=\"color:#B392F0\">withUsername</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"testuser\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                .</span><span style=\"color:#B392F0\">withPassword</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"testpass\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                .</span><span style=\"color:#B392F0\">withCommand</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"postgres\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"-c\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"wal_level=logical\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"-c\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"max_replication_slots=5\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            postgresContainer.</span><span style=\"color:#B392F0\">start</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> postgresContainer;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> synchronized</span><span style=\"color:#E1E4E8\"> KafkaContainer </span><span style=\"color:#B392F0\">getKafkaContainer</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (kafkaContainer </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            kafkaContainer </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> KafkaContainer</span><span style=\"color:#E1E4E8\">(DockerImageName.</span><span style=\"color:#B392F0\">parse</span><span style=\"color:#E1E4E8\">(KAFKA_IMAGE))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                .</span><span style=\"color:#B392F0\">withEnv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                .</span><span style=\"color:#B392F0\">withEnv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"KAFKA_TRANSACTION_STATE_LOG_MIN_ISR\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                .</span><span style=\"color:#B392F0\">withEnv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"1\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            kafkaContainer.</span><span style=\"color:#B392F0\">start</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> kafkaContainer;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getPostgresConnectionProperties</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        PostgreSQLContainer&#x3C;</span><span style=\"color:#F97583\">?</span><span style=\"color:#E1E4E8\">> pg </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> getPostgresContainer</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Map&#x3C;</span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">String</span><span style=\"color:#E1E4E8\">> props </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> HashMap&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"database.host\"</span><span style=\"color:#E1E4E8\">, pg.</span><span style=\"color:#B392F0\">getHost</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"database.port\"</span><span style=\"color:#E1E4E8\">, String.</span><span style=\"color:#B392F0\">valueOf</span><span style=\"color:#E1E4E8\">(pg.</span><span style=\"color:#B392F0\">getFirstMappedPort</span><span style=\"color:#E1E4E8\">()));</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"database.name\"</span><span style=\"color:#E1E4E8\">, pg.</span><span style=\"color:#B392F0\">getDatabaseName</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"database.username\"</span><span style=\"color:#E1E4E8\">, pg.</span><span style=\"color:#B392F0\">getUsername</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        props.</span><span style=\"color:#B392F0\">put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"database.password\"</span><span style=\"color:#E1E4E8\">, pg.</span><span style=\"color:#B392F0\">getPassword</span><span style=\"color:#E1E4E8\">());</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> props;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getKafkaBootstrapServers</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> getKafkaContainer</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">getBootstrapServers</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> stopAll</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (kafkaContainer </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            kafkaContainer.</span><span style=\"color:#B392F0\">stop</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            kafkaContainer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (postgresContainer </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            postgresContainer.</span><span style=\"color:#B392F0\">stop</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            postgresContainer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Binary Log Fixture Loader:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/test/java/com/cdc/testutils/BinaryFixtureLoader.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.testutils;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.IOException;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.io.InputStream;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.nio.file.Files;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.nio.file.Path;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.nio.file.Paths;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> BinaryFixtureLoader</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> byte</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#B392F0\">loadFixture</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">fixtureName</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> IOException {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Load from classpath first (for unit tests)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        InputStream is </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BinaryFixtureLoader.class.</span><span style=\"color:#B392F0\">getClassLoader</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            .</span><span style=\"color:#B392F0\">getResourceAsStream</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"fixtures/\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> fixtureName);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (is </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> is.</span><span style=\"color:#B392F0\">readAllBytes</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Fallback to file system (for development)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Path fixturePath </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Paths.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"src/test/resources/fixtures\"</span><span style=\"color:#E1E4E8\">, fixtureName);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (Files.</span><span style=\"color:#B392F0\">exists</span><span style=\"color:#E1E4E8\">(fixturePath)) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> Files.</span><span style=\"color:#B392F0\">readAllBytes</span><span style=\"color:#E1E4E8\">(fixturePath);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> IOException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Fixture not found: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> fixtureName);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> saveFixture</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">fixtureName</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">[] </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> IOException {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Path fixtureDir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Paths.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"src/test/resources/fixtures\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">!</span><span style=\"color:#E1E4E8\">Files.</span><span style=\"color:#B392F0\">exists</span><span style=\"color:#E1E4E8\">(fixtureDir)) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Files.</span><span style=\"color:#B392F0\">createDirectories</span><span style=\"color:#E1E4E8\">(fixtureDir);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Path fixturePath </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fixtureDir.</span><span style=\"color:#B392F0\">resolve</span><span style=\"color:#E1E4E8\">(fixtureName);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Files.</span><span style=\"color:#B392F0\">write</span><span style=\"color:#E1E4E8\">(fixturePath, data);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Helper to capture real WAL/binlog snippets for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> capturePostgresWalSnippet</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">outputFixtureName</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                                 String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                                 String... </span><span style=\"color:#FFAB70\">sqlStatements</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // This would be implemented to connect to PostgreSQL,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // enable logical decoding, execute SQL, and save WAL segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // For now, it's a placeholder showing the intent</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        System.out.</span><span style=\"color:#B392F0\">println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Capture WAL snippet: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> outputFixtureName);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        System.out.</span><span style=\"color:#B392F0\">println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Execute on table \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> tableName </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \":\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (String sql </span><span style=\"color:#F97583\">:</span><span style=\"color:#E1E4E8\"> sqlStatements) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            System.out.</span><span style=\"color:#B392F0\">println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"  \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> sql);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>E2E Delivery Guarantee Test Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/test/java/com/cdc/e2e/DeliveryGuaranteeTest.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.e2e;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.core.ChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.config.AppConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.connector.LogConnector;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.builder.ChangeEventBuilder;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.streaming.KafkaEventPublisher;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.junit.jupiter.api.Test;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.testcontainers.junit.jupiter.Testcontainers;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.List;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.CountDownLatch;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.TimeUnit;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.atomic.AtomicInteger;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">@</span><span style=\"color:#F97583\">Testcontainers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> DeliveryGuaranteeTest</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Test</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> testAtLeastOnceDelivery_WithKafkaBrokerRestart</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Setup Testcontainers for PostgreSQL and Kafka</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use TestContainersSetup.getPostgresContainer()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use TestContainersSetup.getKafkaContainer()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Create and configure AppConfig for test environment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Set database connection properties from test container</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Set Kafka bootstrap servers from test container</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Configure producer with idempotence=true, acks=all</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Initialize pipeline components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - LogConnector with test configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - ChangeEventBuilder</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - KafkaEventPublisher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Start pipeline and verification consumer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Start pipeline components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Start separate thread running verification consumer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Verification consumer should count received events</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Generate test data in database</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Create test table with primary key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Insert 100 rows in a transaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: Simulate failure during processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - After 50 events published, stop Kafka container abruptly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Wait 2 seconds, then restart Kafka container</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: Continue data generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Insert another 100 rows</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 8: Wait for stabilization and verify</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Wait up to 30 seconds for pipeline to recover</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Stop pipeline and consumer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Verify total events received >= 200 (allowing duplicates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Log duplicate count for analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 9: Cleanup test resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Drop test table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Close all connections</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Test</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> testOrderingPerPrimaryKey_WithConcurrentUpdates</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Setup test environment with single row to update</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Create multiple producer threads</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Each thread updates same row with sequential value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Use synchronized counter to generate unique sequence numbers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Start consumer that tracks sequence per primary key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Consumer should reject out-of-order sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Fail test if sequence gap detected</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Run concurrent updates for 10 seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Verify ordering guarantee</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - All sequences processed in order for each primary key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - No gaps in sequence (unless transaction rolled back)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Log any anomalies for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Schema Evolution Test Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: src/test/java/com/cdc/e2e/SchemaEvolutionTest.java</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.e2e;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.schema.SchemaRegistry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.schema.SchemaChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.junit.jupiter.api.Test;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> org.testcontainers.junit.jupiter.Testcontainers;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.BlockingQueue;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.LinkedBlockingQueue;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.TimeUnit;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">@</span><span style=\"color:#F97583\">Testcontainers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> SchemaEvolutionTest</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Test</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> testBackwardCompatibleColumnAddition</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Setup test table with initial schema (id INT, name VARCHAR)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Register initial schema v1 in SchemaRegistry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Start schema change event listener</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Subscribe to schema change topic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Collect SchemaChangeEvents in BlockingQueue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Execute backward-compatible ALTER TABLE</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - ALTER TABLE test ADD COLUMN email VARCHAR(100) NULL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Verify schema evolution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Wait for SchemaChangeEvent in queue (timeout 10s)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Verify event contains oldSchemaId=v1, newSchemaId=v2</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Retrieve v2 schema, verify it has email column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Test data compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Insert row with email value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Consume event with v1 schema deserializer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Verify event deserializes without error (email field ignored)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Consume same event with v2 schema deserializer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Verify email field present with correct value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Test</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> testForwardCompatibilityViolationDetection</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> Exception {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Setup SchemaRegistry with FORWARD compatibility mode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Attempt to remove a nullable column</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - ALTER TABLE test DROP COLUMN name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Verify schema registration rejected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - SchemaRegistry.registerSchema should throw CompatibilityException</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - No SchemaChangeEvent published</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Verify pipeline continues with old schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Insert new row (should work with old schema)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        //   - Events should still use v1 schema ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints-java\">E. Language-Specific Hints (Java)</h4>\n<ol>\n<li><p><strong>Use JUnit 5&#39;s <code>@Testcontainers</code> and <code>@Container</code>:</strong> These annotations simplify container lifecycle management. Static containers are shared across tests in a class; non-static containers are restarted per test.</p>\n</li>\n<li><p><strong>Leverage AssertJ for fluent assertions:</strong> Its rich assertion library makes test code more readable:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   assertThat</span><span style=\"color:#E1E4E8\">(changeEvent.</span><span style=\"color:#B392F0\">getAfterImage</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       .</span><span style=\"color:#B392F0\">containsEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       .</span><span style=\"color:#B392F0\">containsEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Alice\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">       .</span><span style=\"color:#B392F0\">hasSize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">);</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Use <code>Awaitility</code> for asynchronous testing:</strong> When testing eventual consistency (like at-least-once delivery), Awaitility provides clean polling syntax:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   await</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">atMost</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">, SECONDS)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">          .</span><span style=\"color:#B392F0\">until</span><span style=\"color:#E1E4E8\">(() </span><span style=\"color:#F97583\">-></span><span style=\"color:#E1E4E8\"> consumer.</span><span style=\"color:#B392F0\">getReceivedCount</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> expectedCount);</span></span></code></pre></div>\n\n<ol start=\"4\">\n<li><p><strong>Mock external dependencies with Mockito:</strong> For unit tests, mock interfaces like <code>SchemaRegistry</code> and <code>OffsetStore</code> to isolate component behavior.</p>\n</li>\n<li><p><strong>Use <code>TemporaryFolder</code> JUnit extension for file-based tests:</strong> When testing <code>FileOffsetStore</code>, create temporary directories that auto-clean after tests.</p>\n</li>\n<li><p><strong>Configure Kafka test producers/consumers with unique group IDs:</strong> In integration tests, use <code>UUID.randomUUID().toString()</code> for consumer group IDs to avoid conflicts between test runs.</p>\n</li>\n<li><p><strong>Capture logs for debugging:</strong> Use SLF4J with Logback and configure a memory appender to capture logs during tests for post-mortem analysis.</p>\n</li>\n</ol>\n<h4 id=\"f-milestone-checkpoint-verification\">F. Milestone Checkpoint Verification</h4>\n<p><strong>Milestone 1 Checkpoint Command:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run all unit and integration tests for Milestone 1 components</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">./gradlew</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> --tests</span><span style=\"color:#9ECBFF\"> \"*LogConnector*\"</span><span style=\"color:#79B8FF\"> --tests</span><span style=\"color:#9ECBFF\"> \"*ChangeEventBuilder*\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Or for Maven</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">mvn</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -Dtest=</span><span style=\"color:#9ECBFF\">\"*LogConnector*,*ChangeEventBuilder*\"</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<ul>\n<li>All tests pass (green)</li>\n<li>Log output shows successful connection to test database</li>\n<li>Test report indicates parsing of INSERT/UPDATE/DELETE operations</li>\n<li>No exceptions or warnings about missing LSN tracking</li>\n</ul>\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li>Start a local PostgreSQL instance with logical replication enabled</li>\n<li>Run the CDC pipeline against a test table</li>\n<li>Execute: <code>INSERT → UPDATE → DELETE</code> sequence</li>\n<li>Check console/logs for three <code>ChangeEvent</code> emissions with correct before/after images</li>\n<li>Stop and restart pipeline, verify it resumes from correct position</li>\n</ol>\n<p><strong>Failure Indicators &amp; Diagnosis:</strong></p>\n<ul>\n<li><strong>No events emitted:</strong> Check database connection and replication slot creation</li>\n<li><strong>Missing before/after images:</strong> Verify UPDATE operation parsing logic</li>\n<li><strong>Cannot resume after restart:</strong> Check <code>FileOffsetStore</code> persistence logic</li>\n</ul>\n<p><strong>Milestone 2 Checkpoint Command:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run streaming delivery tests (requires Docker for Testcontainers)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">./gradlew</span><span style=\"color:#9ECBFF\"> integrationTest</span><span style=\"color:#79B8FF\"> --tests</span><span style=\"color:#9ECBFF\"> \"*DeliveryGuaranteeTest*\"</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<ul>\n<li>Tests pass with occasional warnings about duplicates (expected for at-least-once)</li>\n<li>Kafka producer metrics show retries during broker restart test</li>\n<li>Ordering test shows monotonic sequences for each primary key</li>\n</ul>\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li>Start pipeline with Kafka connection</li>\n<li>Produce concurrent updates to same row from multiple threads</li>\n<li>Consume events and verify sequence order</li>\n<li>Kill Kafka broker mid-stream, restart, verify no permanent data loss</li>\n</ol>\n<p><strong>Failure Indicators &amp; Diagnosis:</strong></p>\n<ul>\n<li><strong>Events lost permanently:</strong> Check producer <code>acks=all</code> and idempotence configuration</li>\n<li><strong>Out-of-order delivery:</strong> Verify partition key includes primary key, not just table name</li>\n<li><strong>Pipeline hangs on broker failure:</strong> Check producer timeout and retry configurations</li>\n</ul>\n<p><strong>Milestone 3 Checkpoint Command:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run schema evolution tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">./gradlew</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> --tests</span><span style=\"color:#9ECBFF\"> \"*SchemaEvolutionTest*\"</span></span></code></pre></div>\n\n<p><strong>Expected Output:</strong></p>\n<ul>\n<li>Tests pass showing backward compatibility working</li>\n<li>Forward compatibility violation correctly rejected</li>\n<li>Schema change events published and consumable</li>\n</ul>\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li>Register initial schema for a table</li>\n<li>Add nullable column, verify new events include it</li>\n<li>Try to remove column with forward compatibility enabled, verify rejection</li>\n<li>Check schema change topic for notification events</li>\n</ol>\n<p><strong>Failure Indicators &amp; Diagnosis:</strong></p>\n<ul>\n<li><strong>Schema registration succeeds when it should fail:</strong> Check compatibility mode configuration</li>\n<li><strong>Old consumers cannot read new events:</strong> Verify backward compatibility logic</li>\n<li><strong>No schema change events:</strong> Check schema registry integration with event publisher</li>\n</ul>\n<h4 id=\"g-debugging-tips-for-test-failures\">G. Debugging Tips for Test Failures</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit tests pass but integration tests fail</td>\n<td>Differences between mock data and real database behavior</td>\n<td>Compare binary log fixture with actual WAL/binlog output using hex dump</td>\n<td>Update fixtures with real database captures using <code>BinaryFixtureLoader.capturePostgresWalSnippet</code></td>\n</tr>\n<tr>\n<td>Pipeline hangs in integration test</td>\n<td>Deadlock between producer and consumer threads</td>\n<td>Use thread dump (<code>jstack</code>) or debug logging in <code>LogConnector</code> and <code>KafkaEventPublisher</code></td>\n<td>Implement proper shutdown hooks, use timeouts in blocking operations</td>\n</tr>\n<tr>\n<td>Events missing after restart test</td>\n<td>Offset not persisted before shutdown</td>\n<td>Check <code>FileOffsetStore.save()</code> is called during graceful shutdown</td>\n<td>Call <code>save()</code> in <code>@PreDestroy</code> or shutdown hook, add flush to disk</td>\n</tr>\n<tr>\n<td>Duplicate events exceed expected count</td>\n<td>Producer retries without deduplication</td>\n<td>Check producer idempotence configuration, examine event IDs for duplicates</td>\n<td>Enable idempotent producer, implement deduplication in consumer</td>\n</tr>\n<tr>\n<td>Schema evolution test times out</td>\n<td><code>SchemaChangeEvent</code> not published</td>\n<td>Check schema registry integration, verify topic subscription</td>\n<td>Ensure schema registry calls <code>publishSchemaChange()</code> after registration</td>\n</tr>\n<tr>\n<td>Consumer lag alert not firing</td>\n<td>Metrics not collected or threshold too high</td>\n<td>Check <code>StreamerMetrics</code> polling interval, verify consumer group exists</td>\n<td>Reduce polling interval, verify consumer group naming in test</td>\n</tr>\n<tr>\n<td>Memory leak in long-running tests</td>\n<td>Unbounded growth of <code>TransactionState</code> cache</td>\n<td>Monitor <code>ChangeEventBuilder.getTransactionCount()</code> over time</td>\n<td>Implement transaction timeout and cleanup of stale transactions</td>\n</tr>\n<tr>\n<td>Test passes locally but fails in CI</td>\n<td>Resource constraints or timing differences</td>\n<td>Compare container logs, check for slower startup in CI environment</td>\n<td>Increase timeouts in CI configuration, add wait strategies for containers</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (log parsing issues), Milestone 2 (delivery and ordering problems), Milestone 3 (schema compatibility bugs)</p>\n</blockquote>\n<h2 id=\"12-debugging-guide\">12. Debugging Guide</h2>\n<p>Debugging a Change Data Capture system is fundamentally different from debugging a typical application. You are debugging a <strong>continuous, stateful pipeline</strong> that processes a never-ending stream of binary log data while maintaining complex invariants about ordering, delivery, and schema compatibility. The system operates in real-time, and failures often manifest as subtle data inconsistencies downstream—missing records, duplicate events, or unreadable schema versions—hours or days after the initial problem occurred.</p>\n<p>Think of CDC debugging as being an <strong>air traffic controller for data streams</strong>. You cannot stop the flow (database transactions keep happening), but you must monitor multiple moving parts simultaneously—log positions, event queues, consumer offsets, and schema versions—to detect anomalies and reroute traffic when something goes wrong. This section provides the radar screens and emergency checklists for that controller.</p>\n<h3 id=\"121-common-bugs-and-fixes\">12.1 Common Bugs and Fixes</h3>\n<p>This table categorizes the most frequent issues learners encounter when building a CDC system, organized by the component where they typically manifest. Each entry follows a <strong>Symptom → Root Cause → Diagnostic Steps → Fix</strong> pattern.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Log Connector &amp; Parser (Milestone 1)</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Pipeline processes zero events; no errors in logs.</td>\n<td><strong>Log Position Reset</strong>: The <code>LogConnector</code> started reading from an incorrect <code>logSequenceNumber</code>, typically because the <code>OffsetStore</code> was corrupted or the replication slot was recreated.</td>\n<td>1. Check <code>LogConnector.lastProcessedLSN</code> in health endpoint.<br>2. Compare with <code>pg_replication_slots</code> (PostgreSQL) or <code>SHOW MASTER STATUS</code> (MySQL).<br>3. Verify <code>FileOffsetStore</code> file contents are valid.</td>\n<td>Reset offset: manually set <code>lastProcessedLSN</code> to current database LSN (losing missed events) or perform a full table resync.</td>\n</tr>\n<tr>\n<td>Events for the same row appear out-of-order in the stream.</td>\n<td><strong>Transaction Boundary Violation</strong>: The <code>ChangeEventBuilder</code> emitted events before the transaction committed, or the <code>LogParser</code> processed WAL entries in non-commit order due to parallel parsing.</td>\n<td>1. Inspect <code>ChangeEvent.transactionId</code> and <code>commitTimestamp</code>—same transaction events should share ID.<br>2. Check <code>LogParser</code> implementation for concurrent processing without proper ordering.</td>\n<td>Ensure <code>ChangeEventBuilder</code> buffers operations per transaction and only emits on commit. Use single-threaded log parsing or sequence numbers.</td>\n</tr>\n<tr>\n<td>UPDATE events show identical <code>beforeImage</code> and <code>afterImage</code>.</td>\n<td><strong>Partial Row Capture</strong>: The <code>LogParser</code> only captures changed columns (PostgreSQL toast, MySQL partial updates) but the event builder fails to merge with previous row state.</td>\n<td>1. Examine raw <code>RawLogEntry.rowData</code>—does it contain all columns or just changed ones?<br>2. Check if <code>ChangeEventBuilder</code> maintains row state across updates within a transaction.</td>\n<td>Implement row state caching in <code>TransactionState.lastOperationByKey</code> to merge partial updates with previous full image.</td>\n</tr>\n<tr>\n<td><strong>Event Streamer &amp; Delivery (Milestone 2)</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Consumer receives duplicate events for the same change.</td>\n<td><strong>At-Least-Once Duplication</strong>: The <code>KafkaEventPublisher</code> crashed after sending but before persisting the <code>lastProcessedLSN</code>, causing reprocessing of the same batch on restart.</td>\n<td>1. Check for <code>KafkaEventPublisher</code> restart logs near duplicate events.<br>2. Verify <code>pendingEvents</code> map is cleared only after offset persistence.<br>3. Look for &quot;producer idempotence&quot; configuration errors.</td>\n<td>Implement <strong>idempotent publishing</strong>: store offset <strong>after</strong> Kafka ack (in <code>DeliveryCallback</code>). Use Kafka&#39;s transactional producer with <code>enable.idempotence=true</code>.</td>\n</tr>\n<tr>\n<td>Consumer lag increases indefinitely; <code>KafkaEventPublisher</code> logs &quot;backpressure enabled&quot;.</td>\n<td><strong>Consumer Processing Stalled</strong>: Downstream consumers stopped reading, causing Kafka partitions to fill, triggering <code>KafkaEventPublisher.backpressureSignal</code>.</td>\n<td>1. Check consumer health and logs.<br>2. Use <code>kafka-consumer-groups</code> to verify consumer offsets are advancing.<br>3. Verify <code>max.poll.records</code> and <code>max.partition.fetch.bytes</code> aren&#39;t too small.</td>\n<td>1. Restart stuck consumers.<br>2. Increase partition count for hot tables.<br>3. Implement dead-letter queue for poison-pill messages.</td>\n</tr>\n<tr>\n<td>Events for same primary key appear in different partitions, breaking ordering.</td>\n<td><strong>Incorrect Partition Key</strong>: <code>KafkaEventPublisher</code> uses <code>ChangeEvent.sourceTable</code> as partition key instead of composite <code>table+primaryKey</code>.</td>\n<td>1. Inspect event metadata in Kafka using <code>kafka-console-consumer</code>.<br>2. Check <code>KafkaEventPublisher</code>&#39;s key generation logic.</td>\n<td>Partition key must be deterministic: `String.format(&quot;%s</td>\n</tr>\n<tr>\n<td><strong>Schema Registry &amp; Evolution (Milestone 3)</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Consumers fail to deserialize events with &quot;Unknown field&quot; or &quot;Missing required field&quot;.</td>\n<td><strong>Schema Cache Incoherency</strong>: Consumer uses outdated schema version because <code>SchemaRegistry</code> cache wasn&#39;t invalidated after schema change.</td>\n<td>1. Compare <code>ChangeEvent.schemaVersionId</code> with <code>SchemaRegistry.getLatestSchema()</code>.<br>2. Check if <code>SchemaChangeEvent</code> was emitted and processed.<br>3. Verify consumer schema cache TTL settings.</td>\n<td>1. Implement cache invalidation on schema change (broadcast event).<br>2. Embed minimal schema in <code>ChangeEvent</code> envelope for fallback.<br>3. Use <code>SchemaVersion.version</code> as Kafka message header.</td>\n</tr>\n<tr>\n<td>ALTER TABLE ADD COLUMN causes consumer crashes despite backward compatibility.</td>\n<td><strong>Default Value Missing</strong>: New nullable column added without <code>DEFAULT</code>, causing serialization to omit field, breaking consumer expecting <code>null</code>.</td>\n<td>1. Check <code>SchemaVersion.columnDefinitions</code> for the new column&#39;s <code>nullable</code> and <code>defaultValue</code>.<br>2. Verify <code>EventSerializer</code> handling of missing fields.</td>\n<td>Schema registry must reject column additions without <code>nullable=true</code> OR <code>defaultValue</code>. <code>EventSerializer</code> must fill missing fields with schema-defined default.</td>\n</tr>\n<tr>\n<td>DROP COLUMN breaks all existing consumers immediately.</td>\n<td><strong>Forward Compatibility Violation</strong>: Schema registry allowed breaking change because compatibility mode was <code>COMPATIBILITY_BACKWARD</code> instead of <code>COMPATIBILITY_FULL</code>.</td>\n<td>1. Check <code>SchemaVersion.compatibilityMode</code> at registration time.<br>2. Review <code>SchemaRegistry.checkCompatibility</code> logic for forward checks.</td>\n<td>Set default compatibility to <code>COMPATIBILITY_FULL</code>. Implement <strong>soft deletes</strong>: mark column deprecated but keep in schema for forward compatibility.</td>\n</tr>\n<tr>\n<td><strong>Cross-Component Issues</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Pipeline stops after database restart with &quot;replication slot not found&quot;.</td>\n<td><strong>Slot Cleanup</strong>: PostgreSQL automatically drops inactive replication slots, or MySQL binlog expired due to <code>expire_logs_days</code>.</td>\n<td>1. Check PostgreSQL <code>pg_replication_slots</code> slot status.<br>2. Verify MySQL <code>binlog_expire_logs_seconds</code> setting.<br>3. Look for &quot;slot&quot;, &quot;missing&quot;, or &quot;expired&quot; in error logs.</td>\n<td>1. Increase <code>wal_keep_size</code> (PostgreSQL) or <code>binlog_expire_logs_seconds</code> (MySQL).<br>2. Implement <strong>slot heartbeat</strong>: periodically update slot LSN even during idle periods.</td>\n</tr>\n<tr>\n<td>Memory usage grows unbounded; eventual OutOfMemoryError.</td>\n<td><strong>Transaction Leak</strong>: <code>ChangeEventBuilder.activeTransactions</code> never removes completed transactions, or <code>TransactionState</code> grows too large.</td>\n<td>1. Monitor <code>ChangeEventBuilder.getTransactionCount()</code> over time—should stabilize.<br>2. Check for missing <code>COMMIT</code>/<code>ROLLBACK</code> log entries.<br>3. Look for large transactions (&gt;10k operations).</td>\n<td>1. Implement <strong>transaction timeout</strong>: flush transactions older than <code>transactionTimeoutMs</code>.<br>2. Add memory-bound to <code>activeTransactions</code> with LRU eviction.</td>\n</tr>\n<tr>\n<td>Events arrive with significant delay (seconds to minutes).</td>\n<td><strong>Pipeline Congestion</strong>: One component (parser, builder, streamer) becomes bottleneck, causing queue buildup.</td>\n<td>1. Measure queue sizes: <code>LogConnector.buffer</code>, <code>KafkaEventPublisher.pendingEvents</code>.<br>2. Profile CPU usage per component thread.<br>3. Check for synchronous network calls in critical path.</td>\n<td>1. Increase batch sizes.<br>2. Parallelize independent tables.<br>3. Use async I/O for Kafka publishing.</td>\n</tr>\n</tbody></table>\n<p><strong>Mental Model: The Pipeline Pressure Gauge</strong>\nThink of the CDC pipeline as a series of connected pipes with pressure sensors. Normal operation maintains steady pressure. A blockage upstream causes pressure drop downstream; a blockage downstream causes pressure buildup upstream. Monitoring queue sizes (<code>buffer</code>, <code>pendingEvents</code>) and processing latency (<code>commitTimestamp</code> vs current time) gives you the pressure readings. Debugging is about locating the blockage and clearing it without bursting the pipes.</p>\n<h4 id=\"architecture-decision-debuggability-vs-performance\">Architecture Decision: Debuggability vs Performance</h4>\n<blockquote>\n<p><strong>Decision: Structured Logging Over Binary Performance</strong></p>\n<ul>\n<li><strong>Context</strong>: The CDC pipeline processes high-volume binary data where every microsecond counts. Learners often optimize by removing &quot;expensive&quot; logging.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li><strong>Minimal logging</strong>: Only log errors and fatal events for maximum performance.</li>\n<li><strong>Structured debug logging</strong>: Log key pipeline state (LSN, transaction ID, event counts) with sampling or debug flags.</li>\n<li><strong>Comprehensive audit logging</strong>: Log every event transformation for complete debuggability.</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Option 2—structured debug logging with runtime configuration.</li>\n<li><strong>Rationale</strong>: Performance impact is minimal when logging is guarded by <code>if (logger.isDebugEnabled())</code> and uses parameterized messages. The ability to enable debug logs in production during incidents is invaluable for diagnosing data flow issues without restarting or deploying new code.</li>\n<li><strong>Consequences</strong>: Adds slight overhead even when disabled (condition checks). Requires careful log message design to avoid exposing sensitive data. Provides crucial visibility into pipeline health and data transformation correctness.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Logging Strategy</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Suitable For</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Minimal</td>\n<td>Maximum performance, smallest log volume</td>\n<td>Impossible to debug pipeline state without code changes</td>\n<td>Production-only deployments with perfect monitoring</td>\n</tr>\n<tr>\n<td>Structured Debug</td>\n<td>Balance of performance and debuggability, runtime toggle</td>\n<td>Slight overhead, requires careful message design</td>\n<td>Development, testing, and production troubleshooting</td>\n</tr>\n<tr>\n<td>Comprehensive Audit</td>\n<td>Complete event lineage, perfect for forensic analysis</td>\n<td>Significant performance overhead, massive log volume</td>\n<td>Regulatory environments requiring full audit trails</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Silent Data Loss</strong>\n<strong>Description</strong>: The pipeline appears healthy (no errors), but downstream consumers are missing events for specific time ranges or tables.\n<strong>Why it&#39;s wrong</strong>: Data loss violates the core guarantee of CDC. The issue often stems from gaps in log sequence numbers that the parser silently skips.\n<strong>How to fix</strong>: Implement <strong>gap detection</strong>: when <code>LogConnector</code> reads LSNs, compare consecutive numbers. If a gap exceeds threshold (e.g., more than 1), emit a <code>GapDetectionEvent</code> to alert operators and trigger investigation. Store last processed LSN atomically with event publishing.</p>\n<h3 id=\"122-inspection-and-tracing-techniques\">12.2 Inspection and Tracing Techniques</h3>\n<p>Effective CDC debugging requires inspecting the <strong>four pillars of pipeline state</strong>: Log Position, Event Content, Consumer Offset, and Schema Version. This section provides concrete techniques for each.</p>\n<h4 id=\"1221-log-position-inspection\">12.2.1 Log Position Inspection</h4>\n<p>The log position (<code>logSequenceNumber</code> or LSN) is the pipeline&#39;s <strong>checkpoint</strong>. If it&#39;s wrong, you&#39;ll have data loss or duplication.</p>\n<p><strong>Diagnostic Commands</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Database</th>\n<th>Command</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PostgreSQL</td>\n<td><code>SELECT pg_current_wal_lsn();</code><br><code>SELECT * FROM pg_replication_slots;</code></td>\n<td>Get current WAL LSN and replication slot status</td>\n</tr>\n<tr>\n<td>MySQL</td>\n<td><code>SHOW MASTER STATUS;</code><br><code>SHOW BINARY LOGS;</code></td>\n<td>Get current binlog position and file list</td>\n</tr>\n<tr>\n<td>Both</td>\n<td>Check <code>FileOffsetStore</code> file: <code>cat /path/to/offsets.properties</code></td>\n<td>Verify persisted LSN matches database</td>\n</tr>\n</tbody></table>\n<p><strong>Health Endpoint Implementation</strong>:\nAdd a health endpoint to <code>CdcPipeline</code> that exposes:</p>\n<ul>\n<li><code>LogConnector.lastProcessedLSN</code></li>\n<li>Current database LSN (queried live)</li>\n<li>Lag = current LSN - last processed LSN (in bytes or number of transactions)</li>\n<li><code>LogConnector.running</code> state</li>\n</ul>\n<p><strong>Interpreting Results</strong>:</p>\n<ul>\n<li><strong>Zero lag</strong>: Pipeline is caught up.</li>\n<li><strong>Growing lag</strong>: Pipeline cannot keep up with database write rate.</li>\n<li><strong>Negative or impossible lag</strong>: LSN comparison logic broken (different formats).</li>\n<li><strong>Last processed LSN stale</strong>: Pipeline stopped processing.</li>\n</ul>\n<h4 id=\"1222-event-content-inspection\">12.2.2 Event Content Inspection</h4>\n<p>You need to see what events look like at different pipeline stages without affecting production flow.</p>\n<p><strong>Technique 1: Diagnostic Topic</strong>\nCreate a separate Kafka topic <code>cdc_debug</code> where you publish a copy of every event. Use a <code>KafkaEventPublisher</code> with sampling (e.g., 1% of events) or conditional publishing (only for specific tables). Consume this topic with a simple console consumer to see raw events.</p>\n<p><strong>Technique 2: In-Memory Event Snapshot</strong>\nImplement a circular buffer in <code>ChangeEventBuilder</code> that stores the last N events processed. Expose via JMX or HTTP endpoint for immediate inspection. Include full event details: <code>eventId</code>, <code>beforeImage</code>, <code>afterImage</code>, <code>schemaVersionId</code>.</p>\n<p><strong>Event Content Checklist</strong>:\nWhen inspecting an event, verify:</p>\n<ol>\n<li><strong>Operation Type</strong>: Matches database operation (INSERT/UPDATE/DELETE).</li>\n<li><strong>Primary Key Present</strong>: In <code>beforeImage</code> (for UPDATE/DELETE) and <code>afterImage</code> (for INSERT/UPDATE).</li>\n<li><strong>Schema Version Valid</strong>: <code>schemaVersionId</code> exists in registry.</li>\n<li><strong>Timestamps Monotonic</strong>: <code>commitTimestamp</code> increases within same transaction.</li>\n<li><strong>Transaction Boundaries</strong>: Events with same <code>transactionId</code> have contiguous <code>commitTimestamp</code>.</li>\n</ol>\n<h4 id=\"1223-consumer-offset-monitoring\">12.2.3 Consumer Offset Monitoring</h4>\n<p>Consumer lag indicates if downstream systems are keeping up. Monitor both <strong>partition lag</strong> (offset difference) and <strong>processing latency</strong> (time from commit to consumer ack).</p>\n<p><strong>Kafka Consumer Groups</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check consumer group status</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">kafka-consumer-groups</span><span style=\"color:#79B8FF\"> --bootstrap-server</span><span style=\"color:#9ECBFF\"> localhost:9092</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  --describe</span><span style=\"color:#79B8FF\"> --group</span><span style=\"color:#9ECBFF\"> cdc-consumers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Output columns:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># GROUP, TOPIC, PARTITION, CURRENT-OFFSET, LOG-END-OFFSET, LAG, CONSUMER-ID</span></span></code></pre></div>\n<p><strong>Interpretation</strong>:</p>\n<ul>\n<li><strong>LAG = 0</strong>: Consumer caught up.</li>\n<li><strong>LAG growing</strong>: Consumer falling behind.</li>\n<li><strong>CURRENT-OFFSET not moving</strong>: Consumer stuck or dead.</li>\n<li><strong>Multiple consumers for same partition</strong>: Rebalance in progress.</li>\n</ul>\n<p><strong>Implement Lag Alerting</strong>:\nIn <code>StreamerMetrics</code>, periodically check consumer group lag and trigger alert if:</p>\n<ol>\n<li>Lag exceeds threshold (e.g., 10,000 messages).</li>\n<li>Lag growth rate indicates impending backlog.</li>\n<li>Consumer has been stuck at same offset for &gt; timeout period.</li>\n</ol>\n<h4 id=\"1224-schema-version-tracing\">12.2.4 Schema Version Tracing</h4>\n<p>Schema issues manifest as deserialization failures. You need to trace which schema version each consumer uses.</p>\n<p><strong>Technique: Embedded Schema Lineage</strong>:\nAdd schema metadata to <code>ChangeEvent</code> as headers (Kafka) or envelope fields:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"event\"</span><span style=\"color:#E1E4E8\">: { </span><span style=\"color:#FDAEB7;font-style:italic\">...</span><span style=\"color:#E1E4E8\"> },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"schema_meta\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"version\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"fingerprint\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"sha256:abc123\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"compatibility\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"BACKWARD\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Schema Debug Endpoints</strong>:\nExtend <code>SchemaRegistry</code> with diagnostic endpoints:</p>\n<ul>\n<li><code>GET /schemas/{table}/versions</code> - List all versions</li>\n<li><code>GET /schemas/{table}/diff/{v1}/{v2}</code> - Show changes between versions</li>\n<li><code>GET /events/by-schema/{schemaId}</code> - Find events using specific schema (requires indexing)</li>\n</ul>\n<p><strong>Common Schema Debugging Scenarios</strong>:</p>\n<ol>\n<li><strong>Consumer reports &quot;Unknown field &#39;new_column&#39;&quot;</strong>: Check if consumer has old schema cached. Force cache refresh or restart consumer.</li>\n<li><strong>Serialization fails with &quot;Invalid type for field &#39;amount&#39;&quot;</strong>: Verify <code>ColumnType.javaClass</code> matches actual data type in database. Check for database type changes (INT → BIGINT).</li>\n<li><strong>Events appear with wrong schema version</strong>: Check <code>ChangeEventBuilder.schemaCache</code> invalidation logic. Verify DDL event parsing correctly triggers schema registration.</li>\n</ol>\n<h4 id=\"1225-distributed-tracing-for-cdc\">12.2.5 Distributed Tracing for CDC</h4>\n<p>For production systems, implement distributed tracing to follow an event through the entire pipeline:</p>\n<p><strong>Trace Points</strong>:</p>\n<ol>\n<li><strong>Database commit</strong>: Capture transaction ID and LSN (requires database instrumentation).</li>\n<li><strong>Log parsing</strong>: Trace entry → <code>RawLogEntry</code> transformation.</li>\n<li><strong>Event building</strong>: Trace transaction assembly and deduplication.</li>\n<li><strong>Publishing</strong>: Trace Kafka produce and ack.</li>\n<li><strong>Consumption</strong>: Trace consumer processing.</li>\n</ol>\n<p><strong>Correlation IDs</strong>:\nUse a consistent correlation ID across all stages:</p>\n<ul>\n<li>Start with <code>transactionId</code> from database.</li>\n<li>Propagate through <code>ChangeEvent.eventId</code>.</li>\n<li>Include in Kafka message headers.</li>\n<li>Log at every component with <code>MDC</code> (Mapped Diagnostic Context).</li>\n</ul>\n<p><strong>Visualization</strong>:\nUse Jaeger or Zipkin to visualize pipeline latency bottlenecks:</p>\n<ul>\n<li>Parse latency: time from LSN read to <code>RawLogEntry</code>.</li>\n<li>Build latency: time from <code>RawLogEntry</code> to <code>ChangeEvent</code>.</li>\n<li>Publish latency: time from <code>ChangeEvent</code> to Kafka ack.</li>\n<li>End-to-end latency: time from database commit to consumer ack.</li>\n</ul>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"a-technology-recommendations-table\">A. Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log Position Inspection</td>\n<td>Custom HTTP health endpoint with JSON response</td>\n<td>Prometheus metrics exporter with Grafana dashboard</td>\n</tr>\n<tr>\n<td>Event Content Inspection</td>\n<td>In-memory circular buffer exposed via REST endpoint</td>\n<td>Elasticsearch indexing of events with Kibana visualization</td>\n</tr>\n<tr>\n<td>Consumer Offset Monitoring</td>\n<td>Periodic <code>kafka-consumer-groups</code> script with email alerts</td>\n<td>Kafka Streams lag analysis with real-time alerting to PagerDuty</td>\n</tr>\n<tr>\n<td>Schema Version Tracing</td>\n<td>Schema registry with audit log table</td>\n<td>Schema lineage tracking with Apache Atlas or DataHub</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Log-based correlation with unique IDs per event</td>\n<td>OpenTelemetry instrumentation with Jaeger visualization</td>\n</tr>\n</tbody></table>\n<h4 id=\"b-recommended-file-structure\">B. Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-system/\n├── src/main/java/com/cdc/\n│   ├── pipeline/\n│   │   ├── CdcPipeline.java              # Main pipeline with health endpoint\n│   │   └── HealthStatus.java\n│   ├── debug/\n│   │   ├── EventSnapshotBuffer.java      # Circular buffer for event inspection\n│   │   ├── DiagnosticPublisher.java      # Publishes sample events to debug topic\n│   │   └── GapDetector.java              # Detects LSN gaps\n│   ├── metrics/\n│   │   ├── PipelineMetrics.java          # Tracks latency, throughput\n│   │   ├── LagMonitor.java               # Monitors consumer lag\n│   │   └── AlertManager.java             # Sends alerts on anomalies\n│   └── tracing/\n│       ├── CorrelationId.java            # Propagates correlation IDs\n│       ├── OpenTelemetryConfig.java      # Tracing configuration\n│       └── TracingInterceptor.java       # Adds tracing to components\n└── scripts/\n    ├── check_lag.sh                      # Kafka consumer lag checker\n    ├── validate_schema.py                # Schema compatibility validator\n    └── replay_events.py                  # Replays events from specific LSN</code></pre></div>\n\n<h4 id=\"c-infrastructure-starter-code-event-snapshot-buffer\">C. Infrastructure Starter Code: Event Snapshot Buffer</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.debug;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.events.ChangeEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.concurrent.atomic.AtomicInteger;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">/**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Circular buffer that stores the last N ChangeEvents for debugging.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> * Thread-safe for concurrent reads and writes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"> */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> EventSnapshotBuffer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> ChangeEvent</span><span style=\"color:#E1E4E8\">[] buffer;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> capacity;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> AtomicInteger writeIndex </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> AtomicInteger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> volatile</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\"> size </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> EventSnapshotBuffer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> capacity</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> capacity;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.buffer </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#F97583\"> ChangeEvent</span><span style=\"color:#E1E4E8\">[capacity];</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Add an event to the buffer, overwriting oldest if full.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> add</span><span style=\"color:#E1E4E8\">(ChangeEvent </span><span style=\"color:#FFAB70\">event</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        int</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> writeIndex.</span><span style=\"color:#B392F0\">getAndUpdate</span><span style=\"color:#E1E4E8\">(i </span><span style=\"color:#F97583\">-></span><span style=\"color:#E1E4E8\"> (i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> capacity);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buffer[index] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        synchronized</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Math.</span><span style=\"color:#B392F0\">min</span><span style=\"color:#E1E4E8\">(size </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, capacity);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Get the most recent events (newest first).</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getRecentEvents</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> count</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        int</span><span style=\"color:#E1E4E8\"> currentSize;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        int</span><span style=\"color:#E1E4E8\"> currentWriteIndex;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        synchronized</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            currentSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> size;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            currentWriteIndex </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> writeIndex.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Math.</span><span style=\"color:#B392F0\">min</span><span style=\"color:#E1E4E8\">(count, currentSize);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> count; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Calculate index in circular buffer (going backwards from write pointer)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            int</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (currentWriteIndex </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> capacity) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> capacity;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ChangeEvent event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> buffer[index];</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (event </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result.</span><span style=\"color:#B392F0\">add</span><span style=\"color:#E1E4E8\">(event);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Find events by table name.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getEventsByTable</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">tableName</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> maxResults</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">ChangeEvent</span><span style=\"color:#E1E4E8\">> result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        int</span><span style=\"color:#E1E4E8\"> currentSize;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        int</span><span style=\"color:#E1E4E8\"> currentWriteIndex;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        synchronized</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            currentSize </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> size;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            currentWriteIndex </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> writeIndex.</span><span style=\"color:#B392F0\">get</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> currentSize </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> result.</span><span style=\"color:#B392F0\">size</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> maxResults; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            int</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (currentWriteIndex </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> capacity) </span><span style=\"color:#F97583\">%</span><span style=\"color:#E1E4E8\"> capacity;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ChangeEvent event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> buffer[index];</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (event </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> tableName.</span><span style=\"color:#B392F0\">equals</span><span style=\"color:#E1E4E8\">(event.</span><span style=\"color:#B392F0\">getSourceTable</span><span style=\"color:#E1E4E8\">())) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result.</span><span style=\"color:#B392F0\">add</span><span style=\"color:#E1E4E8\">(event);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> int</span><span style=\"color:#B392F0\"> getSize</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        synchronized</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> size;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> clear</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        synchronized</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">this</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Arrays.</span><span style=\"color:#B392F0\">fill</span><span style=\"color:#E1E4E8\">(buffer, </span><span style=\"color:#79B8FF\">null</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            writeIndex.</span><span style=\"color:#B392F0\">set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            size </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-gap-detection-in-logconnector\">D. Core Logic Skeleton: Gap Detection in LogConnector</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.connector;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.debug.GapDetectionEvent;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.math.BigInteger;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> LogConnector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... existing fields ...</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String previousLSN;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> GapDetectionEvent.Builder gapEventBuilder;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Process a batch of log entries, detecting gaps in LSN sequence.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    protected</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">processBatchWithGapDetection</span><span style=\"color:#E1E4E8\">(List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#FFAB70\">entries</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> processed </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> (RawLogEntry entry </span><span style=\"color:#F97583\">:</span><span style=\"color:#E1E4E8\"> entries) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Convert current entry LSN and previousLSN to comparable format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // (PostgreSQL LSN is hex like \"0/16EFE78\", MySQL binlog position is numeric)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Use a database-specific LSN comparator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: If previousLSN is not null, check if there's a gap</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // A gap means the difference > 1 (or > expected increment based on entry size)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: If gap detected and larger than threshold (configurable):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // - Build GapDetectionEvent with previousLSN, currentLSN, gap size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // - Publish to special \"cdc_gap\" topic or alert channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // - Log warning with gap details</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Update previousLSN to current entry's LSN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            processed.</span><span style=\"color:#B392F0\">add</span><span style=\"color:#E1E4E8\">(entry);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> processed;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    /**</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * Compare two LSNs and return the gap size in \"units\" (bytes for PostgreSQL, </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     * positions for MySQL). Returns 0 if consecutive, >0 if gap, &#x3C;0 if overlap/error.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">     */</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> long</span><span style=\"color:#B392F0\"> calculateLsnGap</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">previousLSN</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">currentLSN</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO: Implement database-specific LSN comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // PostgreSQL example:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Parse \"0/16EFE78\" into two hex parts: segment (0) and offset (16EFE78 hex)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Convert to bytes: (segment &#x3C;&#x3C; 32) + offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Return difference: currentBytes - previousBytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // MySQL example:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Parse position as long: currentPosition - previousPosition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Return 0 for same LSN, >0 for gap, &#x3C;0 if current &#x3C; previous (should not happen)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0L</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-java-specific-debugging-hints\">E. Java-Specific Debugging Hints</h4>\n<ol>\n<li><strong>JMX Monitoring</strong>: Expose <code>LogConnector.lastProcessedLSN</code> and <code>ChangeEventBuilder.getTransactionCount()</code> as JMX attributes for JConsole/VisualVM monitoring.</li>\n<li><strong>Logback MDC</strong>: Use <code>MDC.put(&quot;txId&quot;, transactionId)</code> in your logging configuration to automatically include transaction ID in all log messages during event processing.</li>\n<li><strong>Heap Dumps</strong>: Enable <code>-XX:+HeapDumpOnOutOfMemoryError</code> to capture heap dumps when memory leaks occur. Analyze with Eclipse MAT to find retained <code>TransactionState</code> objects.</li>\n<li><strong>JFR (Java Flight Recorder)</strong>: Use JFR to profile pipeline latency without significant overhead: <code>-XX:+FlightRecorder -XX:StartFlightRecording=filename=cdc.jfr</code>.</li>\n</ol>\n<h4 id=\"f-debugging-checklist-milestone-verification\">F. Debugging Checklist Milestone Verification</h4>\n<p>After implementing debugging tools, verify by simulating failures:</p>\n<p><strong>For Milestone 1 (Log Parsing)</strong>:</p>\n<ol>\n<li>Stop pipeline, delete offset file, restart. Verify:<ul>\n<li>Pipeline detects missing offset and logs warning</li>\n<li>Can resume from current database position (configurable)</li>\n<li>Does not reprocess old events</li>\n</ul>\n</li>\n<li>Force a gap by manually advancing database LSN (e.g., <code>SELECT pg_switch_wal();</code> in PostgreSQL). Verify:<ul>\n<li><code>GapDetector</code> emits gap event</li>\n<li>Pipeline continues processing after gap</li>\n<li>Lag metrics show the gap size</li>\n</ul>\n</li>\n</ol>\n<p><strong>For Milestone 2 (Event Delivery)</strong>:</p>\n<ol>\n<li>Kill Kafka, send database events. Verify:<ul>\n<li><code>KafkaEventPublisher</code> enters backpressure mode</li>\n<li>Events are buffered in memory (monitor <code>pendingEvents</code> size)</li>\n<li>When Kafka restarts, buffered events are sent</li>\n<li>No events lost during outage</li>\n</ul>\n</li>\n<li>Create a slow consumer. Verify:<ul>\n<li>Lag metrics increase</li>\n<li>Alerts trigger when threshold exceeded</li>\n<li><code>EventStreamer</code> can throttle production</li>\n</ul>\n</li>\n</ol>\n<p><strong>For Milestone 3 (Schema Evolution)</strong>:</p>\n<ol>\n<li>Add a nullable column to a table. Verify:<ul>\n<li>Schema registry registers new version</li>\n<li><code>SchemaChangeEvent</code> is emitted</li>\n<li>Consumers can deserialize old and new events</li>\n</ul>\n</li>\n<li>Attempt to drop a column. Verify:<ul>\n<li>Schema registry rejects change (if compatibility mode is FULL)</li>\n<li>Error message explains violation</li>\n<li>Existing events still deserialize correctly</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"g-advanced-debugging-chaos-engineering\">G. Advanced Debugging: Chaos Engineering</h4>\n<p>For resilience testing, intentionally inject failures:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.test.chaos;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> ChaosMonkey</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#F97583\"> final</span><span style=\"color:#E1E4E8\"> Random random </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> Random</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> maybeFail</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">double</span><span style=\"color:#FFAB70\"> failureProbability</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (random.</span><span style=\"color:#B392F0\">nextDouble</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> failureProbability) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            switch</span><span style=\"color:#E1E4E8\"> (component) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                case</span><span style=\"color:#9ECBFF\"> \"LogParser\"</span><span style=\"color:#F97583\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Simulate corrupt log entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> LogParseException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Chaos: simulated corrupt WAL entry\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                case</span><span style=\"color:#9ECBFF\"> \"KafkaPublisher\"</span><span style=\"color:#F97583\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Simulate producer timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> TimeoutException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Chaos: Kafka broker timeout\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                case</span><span style=\"color:#9ECBFF\"> \"SchemaRegistry\"</span><span style=\"color:#F97583\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    // Simulate registry unavailable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> ConnectException</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Chaos: schema registry connection refused\"</span><span style=\"color:#E1E4E8\">);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n<p>Use this in test environments to verify your recovery strategies handle real-world failures.</p>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3 (forward-looking extensions)</p>\n</blockquote>\n<h2 id=\"13-future-extensions\">13. Future Extensions</h2>\n<p>The current CDC system design provides a solid foundation for reliable, real-time change data capture from PostgreSQL and MySQL databases. However, as organizations grow and their data infrastructure evolves, several natural extension points emerge. This section outlines a strategic roadmap for enhancing the system beyond its initial scope, organized by increasing levels of complexity and business value.</p>\n<p>Think of the current system as a <strong>modular data highway with standardized entry ramps</strong>. Each extension adds either new entry ramps (supporting different databases), improved safety features (stronger delivery guarantees), or new service lanes (managed operations). The roadmap prioritizes extensions that maximize architectural leverage while minimizing disruption to existing components.</p>\n<h3 id=\"131-possible-extension-roadmap\">13.1 Possible Extension Roadmap</h3>\n<h4 id=\"extension-1-multi-database-support\">Extension 1: Multi-Database Support</h4>\n<p><strong>Mental Model: The Universal Language Translator</strong>\nImagine the current <code>LogParser</code> interface as a translator who only speaks two languages (PostgreSQL WAL and MySQL binlog). This extension hires additional translators fluent in Oracle&#39;s Redo Log, SQL Server&#39;s Change Data Capture tables, and MongoDB&#39;s Oplog. Each translator understands the unique grammar and idioms of their database but produces the same standardized <code>RawLogEntry</code> &quot;phrasebook&quot; for downstream components.</p>\n<p><strong>Architectural Approach:</strong>\nThe existing <code>LogParser</code> interface provides the perfect abstraction point for this extension. Each new database implementation would:</p>\n<ol>\n<li>Implement the <code>LogParser</code> interface with database-specific connection and parsing logic</li>\n<li>Translate native log concepts (Oracle&#39;s SCN, SQL Server&#39;s LSN) to the system&#39;s <code>logSequenceNumber</code></li>\n<li>Map database-specific data types to the internal <code>ColumnType</code> representation</li>\n<li>Handle database-specific replication features (Oracle&#39;s LogMiner, SQL Server&#39;s CDC tables)</li>\n</ol>\n<p><strong>Implementation Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Database</th>\n<th>Native Log Format</th>\n<th>Key Challenge</th>\n<th>Recommended Parser Class Name</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Oracle</td>\n<td>Redo Log + Archived Redo Log</td>\n<td>Requires LogMiner API; complex data type mapping</td>\n<td><code>OracleRedoLogParser</code></td>\n</tr>\n<tr>\n<td>SQL Server</td>\n<td>Transaction Log + CDC Tables</td>\n<td>Dual-source (log + CDC tables); temporal tables</td>\n<td><code>SqlServerCdcParser</code></td>\n</tr>\n<tr>\n<td>MongoDB</td>\n<td>Oplog (operation log)</td>\n<td>Document model; schema-less nature</td>\n<td><code>MongoOplogParser</code></td>\n</tr>\n<tr>\n<td>Cassandra</td>\n<td>Commit Log</td>\n<td>Distributed log; different consistency model</td>\n<td><code>CassandraCommitLogParser</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight:</strong> The key to successful multi-database support is not just parsing different log formats, but abstracting away database-specific concepts like transaction boundaries (some databases support nested transactions) and data type systems (Oracle&#39;s NUMBER vs PostgreSQL&#39;s NUMERIC).</p>\n</blockquote>\n<p><strong>Integration Requirements:</strong></p>\n<ul>\n<li>Extend <code>DatabaseConfig.type</code> to support new enum values: <code>ORACLE</code>, <code>SQL_SERVER</code>, <code>MONGODB</code>, <code>CASSANDRA</code></li>\n<li>Add database-specific configuration sections to <code>DatabaseConfig</code></li>\n<li>Create a parser factory that instantiates the appropriate parser based on configuration</li>\n<li>Update schema discovery to query each database&#39;s metadata tables appropriately</li>\n</ul>\n<p><strong>Priority Rationale:</strong> High business value with moderate technical complexity. Organizations often have heterogeneous database environments, and supporting multiple sources eliminates the need for separate CDC solutions.</p>\n<h4 id=\"extension-2-exactly-once-delivery-semantics\">Extension 2: Exactly-Once Delivery Semantics</h4>\n<p><strong>Mental Model: The Guaranteed Delivery Receipt</strong>\nThe current at-least-once delivery is like sending a package with tracking but no guarantee against duplicates. Exactly-once semantics adds a <strong>unique package ID registry</strong> that prevents the same package from being delivered twice, even if the delivery truck breaks down mid-route and a replacement truck retries the delivery.</p>\n<p><strong>Technical Implementation:</strong>\nAchieving exactly-once semantics requires coordinated idempotency across three layers:</p>\n<ol>\n<li><strong>Producer Idempotency:</strong> Already partially implemented via Kafka&#39;s <code>enableIdempotence</code> configuration</li>\n<li><strong>Transactional Publishing:</strong> Grouping multiple events into atomic transactions</li>\n<li><strong>Consumer Deduplication:</strong> Tracking delivered events to skip duplicates during retries</li>\n</ol>\n<p><strong>Architecture Decision Record for Exactly-Once Implementation:</strong></p>\n<blockquote>\n<p><strong>Decision: Hybrid Idempotent Producer with Consumer-State Deduplication</strong></p>\n<ul>\n<li><strong>Context:</strong> The system currently implements at-least-once delivery. Some downstream consumers (e.g., financial systems) cannot tolerate duplicate events, even temporarily.</li>\n<li><strong>Options Considered:</strong><ol>\n<li><strong>Kafka Transactions Only:</strong> Use Kafka&#39;s transactional producer with consumer isolation level <code>read_committed</code></li>\n<li><strong>Deterministic Event IDs with Global Deduplication:</strong> Generate deterministic event IDs and maintain a global deduplication store (Redis/DynamoDB)</li>\n<li><strong>Hybrid Approach:</strong> Kafka transactions for producer idempotency + consumer-side bloom filter for fast deduplication</li>\n</ol>\n</li>\n<li><strong>Decision:</strong> Option 3 (Hybrid Approach) for balanced reliability and performance</li>\n<li><strong>Rationale:</strong> Kafka transactions handle producer-side duplicates well but add coordination overhead. Adding consumer-side bloom filters with occasional persistent checks provides near-perfect deduplication without requiring synchronous writes to a global store for every event.</li>\n<li><strong>Consequences:</strong> Adds memory overhead for bloom filters, requires occasional checkpointing to persistent storage, and introduces a small false-positive rate for duplicate detection (tunable).</li>\n</ul>\n</blockquote>\n<p><strong>Implementation Components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Extension Required</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>KafkaEventPublisher</code></td>\n<td>Transactional producer</td>\n<td>Configure <code>transactional.id</code> and use <code>beginTransaction()</code>, <code>commitTransaction()</code></td>\n</tr>\n<tr>\n<td><code>DeliveryCallback</code></td>\n<td>Transaction awareness</td>\n<td>Track events per transaction for rollback on failure</td>\n</tr>\n<tr>\n<td>New: <code>EventDeduplicator</code></td>\n<td>Bloom filter + persistent store</td>\n<td>Fast in-memory duplicate check with periodic persistence</td>\n</tr>\n<tr>\n<td><code>ChangeEventBuilder</code></td>\n<td>Deterministic ID enhancement</td>\n<td>Ensure event IDs are truly deterministic across restarts</td>\n</tr>\n</tbody></table>\n<p><strong>Sequence for Exactly-Once Delivery:</strong></p>\n<ol>\n<li>Producer starts Kafka transaction with unique <code>transactional.id</code></li>\n<li>Events published with deterministic <code>eventId</code> in headers</li>\n<li>Consumer checks <code>EventDeduplicator</code> before processing</li>\n<li>If not duplicate, process and record in deduplication store</li>\n<li>Consumer commits offsets only after successful processing AND deduplication recording</li>\n<li>Producer commits transaction after all events acknowledged</li>\n</ol>\n<p><strong>Priority Rationale:</strong> High technical value with high implementation complexity. Critical for financial, auditing, and billing use cases where duplicate processing has real monetary consequences.</p>\n<h4 id=\"extension-3-fully-managed-cloud-service\">Extension 3: Fully Managed Cloud Service</h4>\n<p><strong>Mental Model: The CDC-as-a-Service Utility</strong>\nTransform the current self-hosted system into a <strong>managed data utility service</strong>, similar to moving from a personal electricity generator to connecting to the power grid. Users provide database credentials and configuration, and the service handles scaling, monitoring, failover, and maintenance automatically.</p>\n<p><strong>Service Architecture Components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Service Component</th>\n<th>Responsibility</th>\n<th>Implementation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Control Plane</td>\n<td>User onboarding, configuration management</td>\n<td>REST API + Web UI; stores config in managed database</td>\n</tr>\n<tr>\n<td>Data Plane</td>\n<td>Actual CDC pipeline execution</td>\n<td>Containerized pipeline per customer/database</td>\n</tr>\n<tr>\n<td>Scaling Controller</td>\n<td>Auto-scaling based on throughput</td>\n<td>Kubernetes HPA + custom metrics (events/sec, lag)</td>\n</tr>\n<tr>\n<td>Multi-Tenancy Layer</td>\n<td>Isolation between customers</td>\n<td>Separate Kafka clusters/namespaces; network isolation</td>\n</tr>\n<tr>\n<td>Billing Engine</td>\n<td>Usage tracking and invoicing</td>\n<td>Metering pipeline + integration with billing providers</td>\n</tr>\n</tbody></table>\n<p><strong>Deployment Models:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Isolation Level</th>\n<th>Cost Efficiency</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shared Everything</td>\n<td>Single pipeline for all customers</td>\n<td>Highest</td>\n<td>Low (but poor isolation)</td>\n</tr>\n<tr>\n<td>Shared Kafka, Isolated Processors</td>\n<td>Dedicated containers per customer</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Fully Isolated</td>\n<td>Dedicated resources per customer</td>\n<td>Lowest</td>\n<td>High (but best isolation)</td>\n</tr>\n</tbody></table>\n<p><strong>Key Technical Challenges:</strong></p>\n<ol>\n<li><strong>Secure Credential Management:</strong> Use cloud KMS (Key Management Service) or HashiCorp Vault for rotating database credentials</li>\n<li><strong>Noisy Neighbor Problem:</strong> Implement fair scheduling and resource quotas</li>\n<li><strong>Customer Self-Service:</strong> Build UI for schema browsing, replay, and monitoring</li>\n<li><strong>Compliance:</strong> SOC2, GDPR, HIPAA compliance for sensitive data</li>\n</ol>\n<p><strong>Priority Rationale:</strong> Transformational business model shift from software product to service. High implementation complexity but potentially highest long-term value through recurring revenue.</p>\n<h4 id=\"extension-4-real-time-transformation-and-enrichment\">Extension 4: Real-Time Transformation and Enrichment</h4>\n<p><strong>Mental Model: The Data Assembly Line with Quality Control</strong>\nExtend the simple pipeline into a <strong>real-time data processing assembly line</strong> where raw change events pass through quality check stations, get assembled with additional parts (enrichment), and are packaged for specific destinations. Think of raw logs as raw materials that get transformed into finished goods customized for each consumer.</p>\n<p><strong>Transformation Types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Transformation Type</th>\n<th>Example</th>\n<th>Implementation Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Field Masking/Redaction</td>\n<td>Hide PII columns (SSN, email)</td>\n<td>Streaming processor with configurable rules</td>\n</tr>\n<tr>\n<td>Data Enrichment</td>\n<td>Add geolocation from IP address</td>\n<td>External API call with caching</td>\n</tr>\n<tr>\n<td>Format Conversion</td>\n<td>JSON to Avro, Avro to Parquet</td>\n<td>Schema-aware serialization pipeline</td>\n</tr>\n<tr>\n<td>Filtering</td>\n<td>Only specific tables/operations</td>\n<td>Configurable predicate evaluation</td>\n</tr>\n<tr>\n<td>Aggregation</td>\n<td>Count events per minute</td>\n<td>Windowed streaming aggregation</td>\n</tr>\n</tbody></table>\n<p><strong>Architectural Integration Points:</strong></p>\n<ul>\n<li><strong>Post-Parser, Pre-Builder:</strong> For schema-aware transformations</li>\n<li><strong>Post-Builder, Pre-Streamer:</strong> For event-level transformations</li>\n<li><strong>Post-Streamer (Sink Connectors):</strong> For destination-specific formatting</li>\n</ul>\n<p><strong>Implementation Strategy Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Flexibility</th>\n<th>Performance</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Embedded Processor Chain</td>\n<td>High</td>\n<td>High (in-process)</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Sidecar Transform Service</td>\n<td>Medium</td>\n<td>Medium (IPC overhead)</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Separate Stream Processing</td>\n<td>Highest</td>\n<td>Variable</td>\n<td>Highest</td>\n</tr>\n</tbody></table>\n<p><strong>Example Transformation Pipeline Configuration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#85E89D\">transformations</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">FIELD_MASK</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    table</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">users</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    columns</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">ssn</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">password_hash</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    method</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">HASH_SHA256</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">ENRICH</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    table</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">orders</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    join</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      source_field</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">customer_id</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      lookup_table</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">customers</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      join_fields</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">tier</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">lifetime_value</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  - </span><span style=\"color:#85E89D\">type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">FILTER</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    condition</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"operation = 'DELETE' and table = 'audit_log'\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    action</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">DROP</span></span></code></pre></div>\n\n<p><strong>Priority Rationale:</strong> Medium-high business value with medium technical complexity. Enables use cases like real-time analytics, data lake population, and compliance-aware data sharing.</p>\n<h4 id=\"extension-5-advanced-monitoring-and-observability\">Extension 5: Advanced Monitoring and Observability</h4>\n<p><strong>Mental Model: The CDC Health Dashboard with Predictive Analytics</strong>\nTransform basic monitoring into a <strong>predictive health intelligence system</strong> that doesn&#39;t just alert when something breaks, but warns when something is <em>about</em> to break. Like a car dashboard that shows current speed but also predicts engine failure based on vibration patterns.</p>\n<p><strong>Enhanced Monitoring Components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Current Capability</th>\n<th>Extended Capability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lag Monitoring</td>\n<td>Current consumer lag</td>\n<td><strong>Predictive lag forecasting</strong> using ML models</td>\n</tr>\n<tr>\n<td>Error Tracking</td>\n<td>Error count and types</td>\n<td><strong>Error correlation engine</strong> linking database errors to pipeline issues</td>\n</tr>\n<tr>\n<td>Performance Metrics</td>\n<td>Throughput and latency</td>\n<td><strong>Bottleneck identification</strong> with flame graphs</td>\n</tr>\n<tr>\n<td>Schema Compliance</td>\n<td>Basic compatibility checks</td>\n<td><strong>Drift detection</strong> between source and consumer schemas</td>\n</tr>\n<tr>\n<td>Data Quality</td>\n<td>Basic validation</td>\n<td><strong>Statistical anomaly detection</strong> in data patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Observability Stack Enhancement:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Layer</th>\n<th>Tools/Technologies</th>\n<th>Implementation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metrics Collection</td>\n<td>Prometheus + custom exporters</td>\n<td>Extend <code>StreamerMetrics</code> with histogram buckets</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Jaeger/OpenTelemetry</td>\n<td>Add trace context propagation through pipeline</td>\n</tr>\n<tr>\n<td>Log Aggregation</td>\n<td>ELK Stack (Elasticsearch, Logstash, Kibana)</td>\n<td>Structured logging with correlation IDs</td>\n</tr>\n<tr>\n<td>Alerting</td>\n<td>AlertManager + custom rules</td>\n<td>Dynamic threshold adjustment based on patterns</td>\n</tr>\n<tr>\n<td>Visualization</td>\n<td>Grafana dashboards</td>\n<td>Pre-built dashboards for common CDC scenarios</td>\n</tr>\n</tbody></table>\n<p><strong>Predictive Analytics Implementation:</strong></p>\n<ol>\n<li><strong>Feature Collection:</strong> Gather time-series data (lag, throughput, error rates, database load)</li>\n<li><strong>Model Training:</strong> Offline training of anomaly detection models</li>\n<li><strong>Real-time Scoring:</strong> Apply models to streaming metrics</li>\n<li><strong>Alert Fusion:</strong> Combine multiple weak signals into strong alerts</li>\n</ol>\n<p><strong>Priority Rationale:</strong> Medium business value with medium-high technical complexity. Critical for production operations at scale, reduces mean time to resolution (MTTR), and enables proactive maintenance.</p>\n<h4 id=\"extension-6-change-event-querying-and-playback\">Extension 6: Change Event Querying and Playback</h4>\n<p><strong>Mental Model: The Database Time Machine</strong>\nAdd the ability to <strong>query the stream of changes as if it were a time-series database</strong> and <strong>replay events from any point in time</strong>. This transforms the CDC system from a simple pipe into an <strong>auditable, queryable historical record</strong> of all database changes.</p>\n<p><strong>Core Capabilities:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Capability</th>\n<th>Description</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Point-in-Time Query</td>\n<td>&quot;Show me the state of user 123 at 2023-06-15 14:30:00&quot;</td>\n<td>Audit investigations, dispute resolution</td>\n</tr>\n<tr>\n<td>Change History Browse</td>\n<td>&quot;Show all changes to the orders table last week&quot;</td>\n<td>Compliance reporting, debugging</td>\n</tr>\n<tr>\n<td>Selective Replay</td>\n<td>&quot;Replay only customer table changes to test environment&quot;</td>\n<td>Test data provisioning, environment synchronization</td>\n</tr>\n<tr>\n<td>Event Search</td>\n<td>&quot;Find all events where email changed from domain X to Y&quot;</td>\n<td>Security incident investigation</td>\n</tr>\n</tbody></table>\n<p><strong>Architectural Components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Change Event Store</td>\n<td>Persistent storage of all change events</td>\n<td>Apache Druid/ClickHouse for time-series queries</td>\n</tr>\n<tr>\n<td>Indexing Engine</td>\n<td>Index events by table, primary key, timestamp</td>\n<td>Elasticsearch for full-text search on JSON payloads</td>\n</tr>\n<tr>\n<td>Query API</td>\n<td>REST/GraphQL interface for querying changes</td>\n<td>Separate service with query planning/optimization</td>\n</tr>\n<tr>\n<td>Replay Engine</td>\n<td>Controlled event replay at various speeds</td>\n<td>Kafka consumer groups with offset manipulation</td>\n</tr>\n</tbody></table>\n<p><strong>Query Language Example:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Find all changes to user email addresses in June 2023</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#F97583\"> *</span><span style=\"color:#F97583\"> FROM</span><span style=\"color:#E1E4E8\"> change_events</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">WHERE</span><span style=\"color:#E1E4E8\"> table_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'users'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  AND</span><span style=\"color:#E1E4E8\"> operation_type </span><span style=\"color:#F97583\">IN</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">'INSERT'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'UPDATE'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  AND</span><span style=\"color:#E1E4E8\"> commit_timestamp </span><span style=\"color:#F97583\">BETWEEN</span><span style=\"color:#9ECBFF\"> '2023-06-01'</span><span style=\"color:#F97583\"> AND</span><span style=\"color:#9ECBFF\"> '2023-06-30'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  AND</span><span style=\"color:#E1E4E8\"> JSON_EXTRACT(after_image, </span><span style=\"color:#9ECBFF\">'$.email'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> JSON_EXTRACT(before_image, </span><span style=\"color:#9ECBFF\">'$.email'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">ORDER BY</span><span style=\"color:#E1E4E8\"> commit_timestamp </span><span style=\"color:#F97583\">DESC</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">LIMIT</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">;</span></span></code></pre></div>\n\n<p><strong>Storage Considerations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Storage Strategy</th>\n<th>Retention</th>\n<th>Query Performance</th>\n<th>Storage Cost</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Kafka + Compaction</td>\n<td>Limited by topic retention</td>\n<td>Good for recent data</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Data Lake (S3 + Parquet)</td>\n<td>Unlimited (cost-based)</td>\n<td>Good for analytical queries</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Time-Series Database</td>\n<td>Configurable retention</td>\n<td>Excellent for time-range queries</td>\n<td>Medium-High</td>\n</tr>\n<tr>\n<td>Hybrid (Hot/Warm/Cold)</td>\n<td>Intelligent tiering</td>\n<td>Optimized for access patterns</td>\n<td>Optimized</td>\n</tr>\n</tbody></table>\n<p><strong>Priority Rationale:</strong> High business value with high implementation complexity. Enables powerful audit, compliance, and debugging capabilities that go beyond simple change streaming.</p>\n<h4 id=\"extension-roadmap-prioritization-matrix\">Extension Roadmap Prioritization Matrix</h4>\n<p><strong>Evaluation Criteria:</strong></p>\n<ul>\n<li><strong>Business Value (BV):</strong> Revenue impact, cost savings, competitive advantage</li>\n<li><strong>Technical Complexity (TC):</strong> Implementation effort, architectural changes required</li>\n<li><strong>Strategic Alignment (SA):</strong> Alignment with long-term platform vision</li>\n<li><strong>Customer Demand (CD):</strong> Direct requests from users/stakeholders</li>\n</ul>\n<p><strong>Prioritization Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>BV (1-10)</th>\n<th>TC (1-10)</th>\n<th>SA (1-10)</th>\n<th>CD (1-10)</th>\n<th>Weighted Score</th>\n<th>Recommended Phase</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-Database Support</td>\n<td>9</td>\n<td>6</td>\n<td>8</td>\n<td>7</td>\n<td>7.5</td>\n<td>Phase 1 (Next 3 months)</td>\n</tr>\n<tr>\n<td>Exactly-Once Delivery</td>\n<td>8</td>\n<td>8</td>\n<td>7</td>\n<td>6</td>\n<td>7.3</td>\n<td>Phase 1</td>\n</tr>\n<tr>\n<td>Real-Time Transformation</td>\n<td>7</td>\n<td>5</td>\n<td>6</td>\n<td>8</td>\n<td>6.5</td>\n<td>Phase 2 (3-6 months)</td>\n</tr>\n<tr>\n<td>Advanced Monitoring</td>\n<td>6</td>\n<td>7</td>\n<td>7</td>\n<td>5</td>\n<td>6.3</td>\n<td>Phase 2</td>\n</tr>\n<tr>\n<td>Change Event Querying</td>\n<td>8</td>\n<td>9</td>\n<td>6</td>\n<td>4</td>\n<td>6.8</td>\n<td>Phase 3 (6-12 months)</td>\n</tr>\n<tr>\n<td>Managed Cloud Service</td>\n<td>10</td>\n<td>10</td>\n<td>9</td>\n<td>3</td>\n<td>8.0</td>\n<td>Phase 3 (strategic)</td>\n</tr>\n</tbody></table>\n<p><strong>Scoring Formula:</strong> <code>(BV×0.3) + (TC×0.2) + (SA×0.3) + (CD×0.2)</code> where lower TC is better (inverted: <code>10 - TC</code>)</p>\n<p><strong>Phase Implementation Strategy:</strong></p>\n<p><strong>Phase 1 (Foundation Extensions):</strong></p>\n<ul>\n<li>Focus on extensions that build directly on existing architecture</li>\n<li>Multi-database support extends the <code>LogParser</code> abstraction</li>\n<li>Exactly-once delivery enhances existing <code>EventStreamer</code> components</li>\n<li><strong>Key Deliverable:</strong> Enterprise-ready CDC with broader database coverage and stronger guarantees</li>\n</ul>\n<p><strong>Phase 2 (Operational Excellence):</strong></p>\n<ul>\n<li>Enhance operational capabilities for production deployments</li>\n<li>Real-time transformation enables new use cases</li>\n<li>Advanced monitoring reduces operational burden</li>\n<li><strong>Key Deliverable:</strong> Self-service transformation and predictive operations</li>\n</ul>\n<p><strong>Phase 3 (Platform Transformation):</strong></p>\n<ul>\n<li>Transform from tool to platform</li>\n<li>Query capabilities create new data product opportunities</li>\n<li>Managed service enables new business model</li>\n<li><strong>Key Deliverable:</strong> Full CDC-as-a-Platform with time-travel query capabilities</li>\n</ul>\n<blockquote>\n<p><strong>Strategic Insight:</strong> The most valuable long-term extension is the managed cloud service, but it requires all other extensions to be mature. Start with Phase 1 extensions to build a robust multi-database foundation, then enhance operations in Phase 2, finally transforming to a platform in Phase 3. This creates value at each stage while building toward the strategic vision.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations for Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Extension</th>\n<th>Core Technology</th>\n<th>Complementary Technologies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-Database</td>\n<td>Database-specific JDBC drivers + native APIs</td>\n<td>Connection pooling (HikariCP), connection health checks</td>\n</tr>\n<tr>\n<td>Exactly-Once</td>\n<td>Kafka Transactions API</td>\n<td>Redis for deduplication store, Bloom filter libraries (Guava)</td>\n</tr>\n<tr>\n<td>Managed Service</td>\n<td>Kubernetes Operator SDK</td>\n<td>HashiCorp Vault, Prometheus Operator, Service Mesh (Istio/Linkerd)</td>\n</tr>\n<tr>\n<td>Real-time Transformation</td>\n<td>Apache Flink/KSQL</td>\n<td>External cache (Redis), rule engine (Drools)</td>\n</tr>\n<tr>\n<td>Advanced Monitoring</td>\n<td>OpenTelemetry SDK</td>\n<td>ML frameworks (TensorFlow Lite), anomaly detection libraries</td>\n</tr>\n<tr>\n<td>Query &amp; Playback</td>\n<td>Apache Druid/ClickHouse</td>\n<td>Query engine (Presto/Trino), S3/MinIO for storage</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure for Multi-Database Extension:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>cdc-system/\n├── src/main/java/com/cdc/\n│   ├── parser/\n│   │   ├── LogParser.java                    # Interface\n│   │   ├── PostgresWalParser.java            # Existing implementation\n│   │   ├── MySqlBinlogParser.java            # Existing implementation\n│   │   ├── oracle/\n│   │   │   ├── OracleRedoLogParser.java      # New: Oracle implementation\n│   │   │   ├── LogMinerHelper.java           # New: Oracle LogMiner utilities\n│   │   │   └── OracleDataTypeMapper.java     # New: Oracle type mapping\n│   │   ├── sqlserver/\n│   │   │   ├── SqlServerCdcParser.java       # New: SQL Server implementation\n│   │   │   └── ChangeTableReader.java        # New: CDC table reader\n│   │   └── ParserFactory.java                # Updated to support new parsers\n│   ├── config/\n│   │   ├── DatabaseConfig.java               # Extended with new database types\n│   │   └── OracleConfig.java                 # New: Oracle-specific config\n│   └── util/\n│       └── DatabaseDetector.java             # New: Auto-detects database type</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code for Database Detector:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// DatabaseDetector.java - Auto-detects database type from JDBC URL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.util;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.sql.Connection;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.sql.DatabaseMetaData;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.sql.SQLException;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> javax.sql.DataSource;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> DatabaseDetector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> static</span><span style=\"color:#E1E4E8\"> DatabaseType </span><span style=\"color:#B392F0\">detect</span><span style=\"color:#E1E4E8\">(DataSource </span><span style=\"color:#FFAB70\">dataSource</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">throws</span><span style=\"color:#E1E4E8\"> SQLException {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\"> (Connection conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dataSource.</span><span style=\"color:#B392F0\">getConnection</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            DatabaseMetaData meta </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conn.</span><span style=\"color:#B392F0\">getMetaData</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            String databaseName </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> meta.</span><span style=\"color:#B392F0\">getDatabaseProductName</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">toLowerCase</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            String url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> meta.</span><span style=\"color:#B392F0\">getURL</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">toLowerCase</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (databaseName.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"postgresql\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> url.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"postgres\"</span><span style=\"color:#E1E4E8\">)) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> DatabaseType.POSTGRESQL;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> (databaseName.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"mysql\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> url.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"mysql\"</span><span style=\"color:#E1E4E8\">)) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> DatabaseType.MYSQL;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> (databaseName.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"oracle\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> url.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"oracle\"</span><span style=\"color:#E1E4E8\">)) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> DatabaseType.ORACLE;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> (databaseName.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"sql server\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> url.</span><span style=\"color:#B392F0\">contains</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"sqlserver\"</span><span style=\"color:#E1E4E8\">)) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> DatabaseType.SQL_SERVER;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                throw</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> UnsupportedDatabaseException</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"Unsupported database: \"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> databaseName);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> enum</span><span style=\"color:#B392F0\"> DatabaseType</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        POSTGRESQL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">MYSQL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">ORACLE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">SQL_SERVER</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton for Oracle Parser:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">java</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// OracleRedoLogParser.java - Skeleton for Oracle implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#E1E4E8\"> com.cdc.parser.oracle;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.parser.LogParser;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.config.OracleConfig;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> com.cdc.model.RawLogEntry;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> java.util.List;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">public</span><span style=\"color:#F97583\"> class</span><span style=\"color:#B392F0\"> OracleRedoLogParser</span><span style=\"color:#F97583\"> implements</span><span style=\"color:#B392F0\"> LogParser</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> OracleConfig config;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> LogMinerHelper logMiner;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> OracleDataTypeMapper typeMapper;</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> String currentScn; </span><span style=\"color:#6A737D\">// Oracle's equivalent of LSN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#B392F0\"> OracleRedoLogParser</span><span style=\"color:#E1E4E8\">(OracleConfig </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.typeMapper </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> OracleDataTypeMapper</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Initialize LogMiner session with Oracle JDBC connection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Configure LogMiner to read from archived redo logs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Set up dictionary (source: online catalog or dictionary file)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> </span><span style=\"color:#B392F0\">getNextBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">int</span><span style=\"color:#FFAB70\"> batchSize</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        List&#x3C;</span><span style=\"color:#F97583\">RawLogEntry</span><span style=\"color:#E1E4E8\">> entries </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#E1E4E8\"> ArrayList&#x3C;>();</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Use LogMiner API to fetch next batch of redo entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Filter for DML operations (INSERT, UPDATE, DELETE)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 6: Parse redo entry into table name, operation type, row data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 7: Map Oracle data types (NUMBER, DATE, CLOB) to ColumnType</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 8: Convert Oracle SCN to logSequenceNumber string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 9: Handle Oracle-specific features: nested tables, LOBs, etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 10: Update currentScn to track position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> entries;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#E1E4E8\"> String </span><span style=\"color:#B392F0\">getCurrentPosition</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> currentScn;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    @</span><span style=\"color:#F97583\">Override</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    public</span><span style=\"color:#F97583\"> void</span><span style=\"color:#B392F0\"> seekToPosition</span><span style=\"color:#E1E4E8\">(String </span><span style=\"color:#FFAB70\">position</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 11: Implement SCN-based seeking for Oracle</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Oracle SCN is numeric, can convert from string</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.currentScn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position;</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 12: Restart LogMiner session from specified SCN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Helper method for Oracle-specific type conversion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    private</span><span style=\"color:#E1E4E8\"> Object </span><span style=\"color:#B392F0\">convertOracleValue</span><span style=\"color:#E1E4E8\">(Object </span><span style=\"color:#FFAB70\">oracleValue</span><span style=\"color:#E1E4E8\">, String </span><span style=\"color:#FFAB70\">oracleType</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 13: Implement Oracle-to-Java type mapping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Handle NUMBER (precision/scale), DATE, TIMESTAMP, CLOB, BLOB</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> typeMapper.</span><span style=\"color:#B392F0\">convert</span><span style=\"color:#E1E4E8\">(oracleValue, oracleType);</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints for Java Extensions:</strong></p>\n<ol>\n<li><strong>Oracle JDBC:</strong> Use <code>ojdbc11.jar</code> for Java 11+, enable JDBC thin driver for better performance</li>\n<li><strong>SQL Server:</strong> Use Microsoft&#39;s <code>mssql-jdbc</code> driver, enable snapshot isolation for CDC tables</li>\n<li><strong>Connection Pooling:</strong> Configure different pool settings per database type (Oracle prefers smaller pools)</li>\n<li><strong>Error Handling:</strong> Database-specific error codes mapping (Oracle ORA-<em>, SQL Server MSSQL</em>)</li>\n<li><strong>Performance:</strong> Oracle LogMiner benefits from <code>DBMS_LOGMNR.NO_ROWID_IN_STMT</code> option for better performance</li>\n</ol>\n<p><strong>F. Milestone Checkpoint for Multi-Database Extension:</strong></p>\n<p><strong>Command to Test:</strong> <code>mvn test -Dtest=MultiDatabaseIntegrationTest</code></p>\n<p><strong>Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] MultiDatabaseIntegrationTest\n[INFO] ✓ testOracleParserBasicOperations\n[INFO] ✓ testSqlServerCdcTableReading\n[INFO] ✓ testDatabaseTypeAutoDetection\n[INFO] ✓ testCrossDatabaseEventConsistency\n[INFO] ✓ testParserFactoryCreatesCorrectType</code></pre></div>\n\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li>Start test Oracle database with sample data: <code>docker run -d --name test-oracle -p 1521:1521 container-registry.oracle.com/database/express:21.3.0-xe</code></li>\n<li>Run pipeline with Oracle configuration</li>\n<li>Execute DML on Oracle: <code>INSERT INTO test_table VALUES (1, &#39;test&#39;); COMMIT;</code></li>\n<li>Verify events appear in Kafka topic with correct Oracle SCN as <code>logSequenceNumber</code></li>\n<li>Stop and restart pipeline, verify it resumes from last SCN</li>\n</ol>\n<p><strong>Signs of Issues:</strong></p>\n<ul>\n<li>❌ &quot;ORA-01291: missing logfile&quot; → LogMiner not configured with correct log files</li>\n<li>❌ &quot;Invalid column type: ORA-00932&quot; → Data type mapping incorrect</li>\n<li>❌ Events out of order → SCN tracking not monotonic</li>\n<li>❌ High memory usage → LogMiner session not properly sized/closed</li>\n</ul>\n<p><strong>G. Debugging Tips for Multi-Database Implementation:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parser fails with &quot;unsupported database&quot;</td>\n<td>Database type detection failure</td>\n<td>Check JDBC URL format and DatabaseMetaData output</td>\n<td>Update <code>DatabaseDetector</code> with correct pattern matching</td>\n</tr>\n<tr>\n<td>Oracle events missing LOB data</td>\n<td>LogMiner not configured for LOB tracking</td>\n<td>Check LogMiner options: <code>DBMS_LOGMNR.NO_LOB_TRACKING</code></td>\n<td>Add <code>+LOBS</code> to LogMiner options</td>\n</tr>\n<tr>\n<td>SQL Server CDC events delayed</td>\n<td>Change table cleanup job too aggressive</td>\n<td>Query <code>sys.dm_cdc_errors</code>, check retention period</td>\n<td>Adjust <code>cdc.retention</code> or polling interval</td>\n</tr>\n<tr>\n<td>Mixed database events out of order</td>\n<td>Different LSN/SCN formats causing sort issues</td>\n<td>Compare LSN strings across databases</td>\n<td>Normalize position format (pad numeric SCNs)</td>\n</tr>\n<tr>\n<td>Memory leak with multiple parsers</td>\n<td>Connection pools not closed properly</td>\n<td>Monitor heap with VisualVM, check connection counts</td>\n<td>Implement proper <code>close()</code> method in parsers</td>\n</tr>\n<tr>\n<td>Type conversion errors</td>\n<td>Database-specific types not mapped</td>\n<td>Log raw type string before conversion</td>\n<td>Extend <code>OracleDataTypeMapper</code> with missing types</td>\n</tr>\n</tbody></table>\n<h2 id=\"14-glossary\">14. Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1, Milestone 2, Milestone 3 (foundational vocabulary for all milestones)</p>\n</blockquote>\n<p>This glossary serves as a <strong>centralized reference</strong> for all technical terms, acronyms, and domain-specific vocabulary used throughout this design document. Think of it as a <strong>dictionary for your CDC journey</strong> — whenever you encounter an unfamiliar term, return here for a clear, concise definition with cross-references to related concepts.</p>\n<h3 id=\"141-terms-and-definitions\">14.1 Terms and Definitions</h3>\n<p>The following table provides alphabetized definitions of key terms, structured for quick lookup:</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Related Concepts</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>After Image</strong></td>\n<td>The complete state of a database row <em>after</em> a change operation (INSERT, UPDATE) has been applied. In a CDC context, this represents the new values that downstream consumers should see.</td>\n<td><code>ChangeEvent.afterImage</code>, <code>OPERATION_UPDATE</code>, <code>OPERATION_INSERT</code>, Before Image</td>\n</tr>\n<tr>\n<td><strong>At-Least-Once Delivery</strong></td>\n<td>A message delivery guarantee where <strong>each event is delivered at least once</strong> to consumers, but duplicates may occur during failure recovery. This is the standard guarantee for our CDC pipeline, balancing reliability with implementation complexity.</td>\n<td>Delivery Semantics, Exactly-Once Delivery, Idempotency, <code>KafkaConfig.enableIdempotence</code></td>\n</tr>\n<tr>\n<td><strong>Backpressure</strong></td>\n<td>A <strong>flow control mechanism</strong> that slows down data production when downstream components (consumers) cannot keep up. In our pipeline, when Kafka consumer lag exceeds thresholds, the Event Streamer signals the Log Connector to pause reading.</td>\n<td><code>AppConfig.backpressureThresholdMs</code>, Consumer Lag, Flow Control, <code>KafkaEventPublisher.backpressureSignal</code></td>\n</tr>\n<tr>\n<td><strong>Backward Compatibility</strong></td>\n<td>A schema compatibility rule where <strong>newer schema versions can read data written with older schema versions</strong>. This is typically the most important compatibility mode for CDC, ensuring consumers with updated schemas can process historical events.</td>\n<td><code>COMPATIBILITY_BACKWARD</code>, Schema Evolution, <code>SchemaRegistry.checkCompatibility()</code>, Forward Compatibility</td>\n</tr>\n<tr>\n<td><strong>Before Image</strong></td>\n<td>The complete state of a database row <em>before</em> a change operation (UPDATE, DELETE) has been applied. This provides consumers with the previous values, enabling delta calculations or undo operations.</td>\n<td><code>ChangeEvent.beforeImage</code>, <code>OPERATION_UPDATE</code>, <code>OPERATION_DELETE</code>, After Image</td>\n</tr>\n<tr>\n<td><strong>Binlog (Binary Log)</strong></td>\n<td>MySQL&#39;s <strong>transaction log mechanism</strong> that records all data modifications (INSERT, UPDATE, DELETE) and schema changes (DDL) in a binary format. It&#39;s used for replication and forms the source for MySQL-based CDC.</td>\n<td>Write-Ahead Log (WAL), Logical Decoding, <code>DatabaseType.MYSQL</code>, Redo Log</td>\n</tr>\n<tr>\n<td><strong>CDC (Change Data Capture)</strong></td>\n<td>The <strong>process of identifying and capturing changes</strong> made to a database, then delivering those changes to downstream systems in real-time. Our system implements log-based CDC, which reads transaction logs rather than polling tables.</td>\n<td>Transaction Log, Event Streaming, Data Synchronization</td>\n</tr>\n<tr>\n<td><strong>CDC Tables</strong></td>\n<td>SQL Server&#39;s <strong>native change data capture feature</strong> that uses special system tables to track changes. Unlike log-based CDC, this approach adds triggers or uses SQL Server&#39;s change tracking tables.</td>\n<td><code>DatabaseType.SQL_SERVER</code>, Trigger-Based CDC, System Tables</td>\n</tr>\n<tr>\n<td><strong>Change Event</strong></td>\n<td>The <strong>core data structure</strong> representing a single logical change to a database row. It contains the operation type, before/after images, schema version, and transaction context, and is the primary payload delivered to consumers.</td>\n<td><code>ChangeEvent</code> type, <code>RawLogEntry</code>, Schema Version, Transaction Boundary</td>\n</tr>\n<tr>\n<td><strong>Chaos Engineering</strong></td>\n<td>The <strong>practice of intentionally injecting failures</strong> into a system to test its resilience and recovery mechanisms. In our testing strategy, the <code>ChaosMonkey</code> component simulates various failure scenarios.</td>\n<td><code>ChaosMonkey</code>, Resilience Testing, Failure Injection, Circuit Breaker</td>\n</tr>\n<tr>\n<td><strong>Circuit Breaker</strong></td>\n<td>A <strong>design pattern</strong> that prevents cascading failures by failing fast when downstream services are unavailable. After consecutive failures, the circuit &quot;opens&quot; and stops making calls, periodically testing if the service has recovered.</td>\n<td><code>CircuitBreakerManager</code>, Resilience, Exponential Backoff, <code>RecoveryState.HALTED</code></td>\n</tr>\n<tr>\n<td><strong>Column Type</strong></td>\n<td>A <strong>metadata structure</strong> defining the characteristics of a database column, including its SQL data type, corresponding Java class, nullability, and default value. These definitions are stored in <code>SchemaVersion.columnDefinitions</code>.</td>\n<td><code>ColumnType</code> type, Schema Definition, <code>SchemaVersion</code>, Type Coercion</td>\n</tr>\n<tr>\n<td><strong>Compatibility Mode</strong></td>\n<td>The <strong>set of rules</strong> governing whether a schema change is allowed based on its impact on existing consumers. Our system supports four modes: <code>BACKWARD</code>, <code>FORWARD</code>, <code>FULL</code>, and <code>NONE</code>.</td>\n<td><code>COMPATIBILITY_BACKWARD</code>, <code>COMPATIBILITY_FORWARD</code>, <code>COMPATIBILITY_FULL</code>, <code>COMPATIBILITY_NONE</code></td>\n</tr>\n<tr>\n<td><strong>Consumer Lag</strong></td>\n<td>The <strong>difference between the latest produced offset</strong> in a Kafka partition and the <strong>last consumed offset</strong> by a consumer. High lag indicates consumers are falling behind, triggering backpressure mechanisms.</td>\n<td><code>StreamerMetrics.lagThresholdMs</code>, Backpressure, Offset Management, Kafka Consumer</td>\n</tr>\n<tr>\n<td><strong>Correlation ID</strong></td>\n<td>A <strong>unique identifier</strong> propagated across system components to trace a request or operation through the entire pipeline. Used for debugging and observability to connect related logs.</td>\n<td>Tracing, Observability, MDC, Debugging</td>\n</tr>\n<tr>\n<td><strong>DDL (Data Definition Language)</strong></td>\n<td><strong>SQL statements that define database structure</strong>, such as CREATE, ALTER, DROP, and TRUNCATE. DDL events require special handling in CDC as they change schema definitions and may invalidate in-flight transactions.</td>\n<td>Schema Evolution, <code>SchemaChangeEvent</code>, ALTER TABLE, DDL Parsing</td>\n</tr>\n<tr>\n<td><strong>Deduplication Cache</strong></td>\n<td>A <strong>temporary in-memory store</strong> used by the <code>ChangeEventBuilder</code> to identify and eliminate duplicate change events that might arise from log replay or retry mechanisms.</td>\n<td><code>ChangeEventBuilder.deduplicationCache</code>, Deterministic Event ID, Idempotency</td>\n</tr>\n<tr>\n<td><strong>Delivery Semantics</strong></td>\n<td>The <strong>guarantees provided about message delivery</strong> between producers and consumers. The three main levels are at-most-once, at-least-once, and exactly-once, each with different trade-offs in complexity and reliability.</td>\n<td>At-Least-Once Delivery, Exactly-Once Delivery, Message Reliability</td>\n</tr>\n<tr>\n<td><strong>Deterministic Event ID</strong></td>\n<td>An <strong>event identifier that can be regenerated</strong> from the same inputs (transaction ID, table, primary key, sequence number). This enables idempotent processing and duplicate detection without external state.</td>\n<td><code>ChangeEventBuilder.generateEventId()</code>, Deduplication, Idempotency</td>\n</tr>\n<tr>\n<td><strong>Event Snapshot Buffer</strong></td>\n<td>A <strong>circular buffer</strong> that stores recent <code>ChangeEvent</code> instances for debugging and recovery purposes. When a gap or corruption is detected, recent events can be replayed or analyzed.</td>\n<td><code>EventSnapshotBuffer</code>, Circular Buffer, Gap Detection, Recovery</td>\n</tr>\n<tr>\n<td><strong>Exactly-Once Delivery</strong></td>\n<td>A <strong>message delivery guarantee</strong> where each event is delivered <strong>exactly once</strong> to consumers, with no duplicates and no missed messages. This is complex to implement and typically requires transactional coordination across systems.</td>\n<td>At-Least-Once Delivery, Idempotent Producer, Transactional Messaging</td>\n</tr>\n<tr>\n<td><strong>Exponential Backoff</strong></td>\n<td>A <strong>retry strategy</strong> where wait time between retry attempts doubles after each failure. This prevents overwhelming a failing service while gradually attempting recovery. Used in our <code>RecoveryCoordinator</code>.</td>\n<td><code>RecoveryStrategy.RETRY_WITH_BACKOFF</code>, Circuit Breaker, Resilience</td>\n</tr>\n<tr>\n<td><strong>Forward Compatibility</strong></td>\n<td>A schema compatibility rule where <strong>older schema versions can read data written with newer schema versions</strong>. This is less common in CDC but useful when consumers upgrade slower than producers.</td>\n<td><code>COMPATIBILITY_FORWARD</code>, Schema Evolution, Backward Compatibility</td>\n</tr>\n<tr>\n<td><strong>Full Compatibility</strong></td>\n<td>A schema compatibility rule that <strong>combines backward and forward compatibility</strong> — any version can read data written by any other version. This provides maximum flexibility but imposes strict constraints on schema changes.</td>\n<td><code>COMPATIBILITY_FULL</code>, Schema Evolution, Compatibility Mode</td>\n</tr>\n<tr>\n<td><strong>Gap Detection</strong></td>\n<td>The <strong>process of identifying missing log sequence numbers</strong> in the CDC pipeline. When the connector detects non-sequential LSNs, it emits a <code>GapDetectionEvent</code> for monitoring and potential manual intervention.</td>\n<td><code>GapDetectionEvent</code>, <code>LogConnector.calculateLsnGap()</code>, LSN Recovery, Data Loss</td>\n</tr>\n<tr>\n<td><strong>Gap Detection Event</strong></td>\n<td>A <strong>special notification event</strong> emitted when the CDC pipeline detects potentially missing data between consecutive log sequence numbers. Contains metadata about the gap size and suspected cause.</td>\n<td><code>GapDetectionEvent</code> type, Gap Detection, Data Integrity</td>\n</tr>\n<tr>\n<td><strong>Graceful Shutdown</strong></td>\n<td>A <strong>shutdown procedure</strong> that completes in-flight work before terminating. Our pipeline&#39;s <code>stop()</code> methods persist current state (LSN offsets, transaction boundaries) to enable clean restarts.</td>\n<td><code>CdcPipeline.stop()</code>, State Persistence, <code>LogConnector.stop()</code></td>\n</tr>\n<tr>\n<td><strong>Health Status</strong></td>\n<td>An <strong>enum representing the operational state</strong> of a component or the entire pipeline. Values include <code>HEALTHY</code>, <code>DEGRADED</code>, and <code>STOPPED</code>, used for monitoring and automatic recovery.</td>\n<td><code>HealthStatus</code> enum, Monitoring, <code>CdcPipeline.getHealthStatus()</code></td>\n</tr>\n<tr>\n<td><strong>Idempotency</strong></td>\n<td>The <strong>property where repeating an operation</strong> yields the same result as performing it once. In CDC, idempotent consumers can safely reprocess duplicate events without causing data corruption.</td>\n<td>At-Least-Once Delivery, Deterministic Event ID, Deduplication</td>\n</tr>\n<tr>\n<td><strong>Idempotent Producer</strong></td>\n<td>A <strong>Kafka producer configuration</strong> that prevents duplicate message submission during retries by using sequence numbers and broker deduplication. Enabled via <code>KafkaConfig.enableIdempotence</code>.</td>\n<td><code>KafkaConfig.enableIdempotence</code>, Exactly-Once Delivery, Producer Idempotence</td>\n</tr>\n<tr>\n<td><strong>LSN (Log Sequence Number)</strong></td>\n<td>A <strong>pointer to a specific position</strong> in a transaction log (PostgreSQL WAL). It&#39;s a critical concept for resuming log reading after restarts and ensuring no changes are missed.</td>\n<td><code>RawLogEntry.logSequenceNumber</code>, Offset Tracking, <code>FileOffsetStore</code>, WAL</td>\n</tr>\n<tr>\n<td><strong>LSN Recovery</strong></td>\n<td>The <strong>process of resuming log reading</strong> from the correct log sequence number after a pipeline failure or restart. Our <code>OffsetStore</code> persists the last processed LSN to enable this recovery.</td>\n<td><code>FileOffsetStore</code>, <code>LogConnector.lastProcessedLSN</code>, Crash Recovery</td>\n</tr>\n<tr>\n<td><strong>Logical Decoding</strong></td>\n<td>The <strong>process of interpreting transaction log entries</strong> (which are physical page changes) as logical row-level changes (INSERT/UPDATE/DELETE of specific rows). This is what transforms <code>RawLogEntry</code> into <code>ChangeEvent</code>.</td>\n<td><code>LogParser</code>, Change Event Builder, WAL Parsing</td>\n</tr>\n<tr>\n<td><strong>LogMiner</strong></td>\n<td><strong>Oracle&#39;s utility for reading redo log files</strong> and extracting logical change information. Used by our <code>OracleRedoLogParser</code> to implement CDC for Oracle databases.</td>\n<td><code>OracleRedoLogParser</code>, <code>LogMinerHelper</code>, Redo Log, SCN</td>\n</tr>\n<tr>\n<td><strong>Log Sequence Number (LSN)</strong></td>\n<td>(Alternative phrasing) A <strong>database-specific identifier</strong> that uniquely identifies a position in the transaction log. PostgreSQL uses LSNs, MySQL uses binlog positions, Oracle uses SCNs.</td>\n<td>Log Sequence Number, Binlog Position, SCN</td>\n</tr>\n<tr>\n<td><strong>MDC (Mapped Diagnostic Context)</strong></td>\n<td><strong>Thread-local storage for contextual logging information</strong> in Java. Used to attach correlation IDs and other metadata to log messages for better traceability across components.</td>\n<td>Correlation ID, Observability, Structured Logging</td>\n</tr>\n<tr>\n<td><strong>Oplog</strong></td>\n<td><strong>MongoDB&#39;s operation log</strong> for replication, recording all write operations. While not currently implemented, it represents another transaction log type that could be supported in future extensions.</td>\n<td>Transaction Log, NoSQL CDC, Replication Log</td>\n</tr>\n<tr>\n<td><strong>Partition Key</strong></td>\n<td>The <strong>value used to determine which Kafka partition</strong> a message is sent to. In our CDC system, we use a composite of table name and primary key hash to ensure ordering per-row.</td>\n<td><code>KafkaConfig.partitionsPerTable</code>, Event Ordering, Kafka Partitioning</td>\n</tr>\n<tr>\n<td><strong>Pipeline Pattern</strong></td>\n<td>An <strong>architectural pattern</strong> where data flows through a series of processing stages (components), each with a single responsibility. Our CDC system uses this pattern: Log Connector → Event Builder → Event Streamer.</td>\n<td>Component Architecture, Data Flow, Processing Stages</td>\n</tr>\n<tr>\n<td><strong>Raw Log Entry</strong></td>\n<td>The <strong>parsed but unprocessed representation</strong> of a transaction log entry. Contains low-level details like LSN, operation type, and raw row data, but hasn&#39;t been assembled into complete change events with transaction context.</td>\n<td><code>RawLogEntry</code> type, <code>LogParser</code>, <code>LogConnector.getNextBatch()</code></td>\n</tr>\n<tr>\n<td><strong>Recovery State</strong></td>\n<td>An <strong>enum representing the current recovery status</strong> of the pipeline. Values include <code>NORMAL</code>, <code>DEGRADED</code>, <code>RECOVERING</code>, and <code>HALTED</code>, guiding the <code>RecoveryCoordinator</code>&#39;s actions.</td>\n<td><code>RecoveryState</code> enum, <code>RecoveryCoordinator</code>, Failure Recovery</td>\n</tr>\n<tr>\n<td><strong>Recovery Strategy</strong></td>\n<td>The <strong>specific approach taken</strong> to recover from a failure. Our <code>RecoveryCoordinator</code> selects from strategies like <code>RETRY_WITH_BACKOFF</code>, <code>RESET_AND_RECOVER</code>, <code>DEGRADE_FUNCTIONALITY</code>, or <code>HALT_FOR_INTERVENTION</code>.</td>\n<td><code>RecoveryStrategy</code> enum, <code>RecoveryCoordinator.determineRecoveryStrategy()</code></td>\n</tr>\n<tr>\n<td><strong>Redo Log</strong></td>\n<td><strong>Oracle&#39;s transaction log</strong> for crash recovery, recording all changes made to the database. Similar to PostgreSQL&#39;s WAL but with Oracle-specific format and access methods.</td>\n<td><code>DatabaseType.ORACLE</code>, <code>OracleRedoLogParser</code>, LogMiner, SCN</td>\n</tr>\n<tr>\n<td><strong>Replication Slot</strong></td>\n<td>(PostgreSQL) A <strong>feature that ensures WAL segments are retained</strong> until consumed by a logical replication consumer. Our CDC system uses replication slots to prevent premature cleanup of unprocessed logs.</td>\n<td><code>DatabaseConfig.slotName</code>, WAL Retention, Logical Replication</td>\n</tr>\n<tr>\n<td><strong>Schema Cache Incoherency</strong></td>\n<td>A <strong>state where different components</strong> have different schema versions cached, leading to serialization/deserialization mismatches. Mitigated by TTL-based cache invalidation and schema version IDs in events.</td>\n<td><code>SchemaRegistry</code>, Cache Coherency, <code>ChangeEvent.schemaVersionId</code></td>\n</tr>\n<tr>\n<td><strong>Schema Change Event</strong></td>\n<td>A <strong>special notification event</strong> emitted when a new schema version is registered. Notifies consumers that they should update their deserialization logic and potentially migrate data.</td>\n<td><code>SchemaChangeEvent</code> type, Schema Evolution, DDL</td>\n</tr>\n<tr>\n<td><strong>Schema Compatibility</strong></td>\n<td><strong>Rules governing whether a schema change</strong> breaks existing consumers. Determines if old consumers can read new data (forward compatibility) and new consumers can read old data (backward compatibility).</td>\n<td>Compatibility Mode, Schema Evolution, <code>SchemaRegistry.checkCompatibility()</code></td>\n</tr>\n<tr>\n<td><strong>Schema Evolution</strong></td>\n<td>The <strong>process of managing changes</strong> to a data schema over time while maintaining compatibility with existing consumers. A core challenge in CDC systems that must handle live database schema changes.</td>\n<td>Schema Registry, Compatibility Mode, DDL</td>\n</tr>\n<tr>\n<td><strong>Schema Registry</strong></td>\n<td>A <strong>centralized service</strong> that stores and manages schema versions for CDC. Provides versioning, compatibility checking, and schema retrieval for serialization/deserialization.</td>\n<td><code>FileBasedSchemaRegistry</code>, Schema Evolution, Schema Versioning</td>\n</tr>\n<tr>\n<td><strong>Schema Version</strong></td>\n<td>A <strong>specific iteration of a table&#39;s schema</strong> with a unique identifier. Contains column definitions and compatibility mode, and is referenced by <code>ChangeEvent</code> instances for proper deserialization.</td>\n<td><code>SchemaVersion</code> type, Versioning, <code>SchemaRegistry.registerSchema()</code></td>\n</tr>\n<tr>\n<td><strong>SCN (System Change Number)</strong></td>\n<td><strong>Oracle&#39;s equivalent of LSN</strong> — a numeric identifier that increments with every database change. Used as the position marker for Oracle CDC via the <code>OracleRedoLogParser</code>.</td>\n<td><code>OracleRedoLogParser.currentScn</code>, Redo Log, LogMiner</td>\n</tr>\n<tr>\n<td><strong>Transaction Boundary</strong></td>\n<td>The <strong>grouping of all changes</strong> that commit together atomically. In CDC, we must respect these boundaries, emitting change events only after seeing commit records in the transaction log.</td>\n<td><code>TransactionState</code>, <code>ChangeEventBuilder</code>, Atomicity</td>\n</tr>\n<tr>\n<td><strong>Transaction Timeout</strong></td>\n<td><strong>Automatic abortion of transactions</strong> that exceed a maximum allowed duration. Our <code>ChangeEventBuilder</code> flushes or discards transactions that haven&#39;t been committed within <code>transactionTimeoutMs</code>.</td>\n<td><code>ChangeEventBuilder.transactionTimeoutMs</code>, Stuck Transactions, <code>TransactionState.lastActivityTimestamp</code></td>\n</tr>\n<tr>\n<td><strong>Type Coercion</strong></td>\n<td>The <strong>automatic or implicit conversion</strong> of values from one data type to another during schema evolution. For example, converting <code>VARCHAR</code> to <code>TEXT</code> is generally safe, while <code>INT</code> to <code>VARCHAR</code> requires careful handling.</td>\n<td>Schema Evolution, <code>ColumnType</code>, Data Type Mapping</td>\n</tr>\n<tr>\n<td><strong>Write-Ahead Log (WAL)</strong></td>\n<td><strong>PostgreSQL&#39;s transaction log</strong> for crash recovery. Records all intended changes before they&#39;re applied to data files, enabling recovery and forming the basis for PostgreSQL CDC via logical decoding.</td>\n<td><code>DatabaseType.POSTGRESQL</code>, Logical Decoding, LSN, Replication Slot</td>\n</tr>\n</tbody></table>\n<hr>\n","toc":[{"level":1,"text":"Project CDC: Real-Time Change Data Capture System Design","id":"project-cdc-real-time-change-data-capture-system-design"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"1. Context and Problem Statement","id":"1-context-and-problem-statement"},{"level":3,"text":"1.1 The Newspaper Analogy for CDC","id":"11-the-newspaper-analogy-for-cdc"},{"level":3,"text":"1.2 The Technical Challenge of Log-Based CDC","id":"12-the-technical-challenge-of-log-based-cdc"},{"level":3,"text":"1.3 Existing Approaches and Trade-offs","id":"13-existing-approaches-and-trade-offs"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"2. Goals and Non-Goals","id":"2-goals-and-non-goals"},{"level":3,"text":"2.1 Goals (Must Have)","id":"21-goals-must-have"},{"level":4,"text":"Functional Goals","id":"functional-goals"},{"level":4,"text":"Non-Functional Goals (Quality Attributes)","id":"non-functional-goals-quality-attributes"},{"level":3,"text":"2.2 Non-Goals (Explicitly Out of Scope)","id":"22-non-goals-explicitly-out-of-scope"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended Project File Structure","id":"b-recommended-project-file-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints (Java)","id":"e-language-specific-hints-java"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"3. High-Level Architecture","id":"3-high-level-architecture"},{"level":3,"text":"3.1 Component Overview and Responsibilities","id":"31-component-overview-and-responsibilities"},{"level":3,"text":"3.2 Recommended Project File Structure","id":"32-recommended-project-file-structure"},{"level":3,"text":"3.3 Implementation Guidance","id":"33-implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Complete Starter Code for Configuration Loading","id":"b-complete-starter-code-for-configuration-loading"},{"level":4,"text":"C. Core Component Skeleton Code","id":"c-core-component-skeleton-code"},{"level":4,"text":"D. Language-Specific Hints for Java","id":"d-language-specific-hints-for-java"},{"level":4,"text":"E. Milestone Checkpoint for Architecture Implementation","id":"e-milestone-checkpoint-for-architecture-implementation"},{"level":2,"text":"4. Data Model","id":"4-data-model"},{"level":3,"text":"4.1 Core Type Definitions","id":"41-core-type-definitions"},{"level":4,"text":"The Foundation: RawLogEntry","id":"the-foundation-rawlogentry"},{"level":4,"text":"The Deliverable: ChangeEvent","id":"the-deliverable-changeevent"},{"level":4,"text":"The Contract: SchemaVersion","id":"the-contract-schemaversion"},{"level":4,"text":"The Configuration: AppConfig and Sub-Configs","id":"the-configuration-appconfig-and-sub-configs"},{"level":4,"text":"The Health Indicator: HealthStatus Enum","id":"the-health-indicator-healthstatus-enum"},{"level":3,"text":"4.2 Serialization Format for Events","id":"42-serialization-format-for-events"},{"level":4,"text":"ADR: Choosing a Serialization Format for Change Events","id":"adr-choosing-a-serialization-format-for-change-events"},{"level":4,"text":"Serialized Message Structure","id":"serialized-message-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File Structure","id":"b-recommended-file-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints (Java)","id":"e-language-specific-hints-java"},{"level":4,"text":"F. Milestone Checkpoint","id":"f-milestone-checkpoint"},{"level":4,"text":"G. Debugging Tips","id":"g-debugging-tips"},{"level":2,"text":"5. Component: Log Connector &amp; Parser","id":"5-component-log-connector-amp-parser"},{"level":3,"text":"5.1 Mental Model: The Database Translator","id":"51-mental-model-the-database-translator"},{"level":3,"text":"5.2 Interface and State","id":"52-interface-and-state"},{"level":4,"text":"Interface","id":"interface"},{"level":4,"text":"Core Data Structure: RawLogEntry","id":"core-data-structure-rawlogentry"},{"level":4,"text":"Internal State","id":"internal-state"},{"level":3,"text":"5.3 Internal Behavior and Algorithm","id":"53-internal-behavior-and-algorithm"},{"level":3,"text":"5.4 ADR: Database-Specific vs. Generic Parser","id":"54-adr-database-specific-vs-generic-parser"},{"level":3,"text":"5.5 Common Pitfalls in Log Parsing","id":"55-common-pitfalls-in-log-parsing"},{"level":3,"text":"5.6 Implementation Guidance","id":"56-implementation-guidance"},{"level":2,"text":"6. Component: Change Event Builder","id":"6-component-change-event-builder"},{"level":3,"text":"6.1 Mental Model: The Event Assembler","id":"61-mental-model-the-event-assembler"},{"level":3,"text":"6.2 Interface and Transaction State","id":"62-interface-and-transaction-state"},{"level":4,"text":"6.2.1 Public Interface","id":"621-public-interface"},{"level":4,"text":"6.2.2 Internal Transaction State Machine","id":"622-internal-transaction-state-machine"},{"level":4,"text":"6.2.3 TransactionState Data Structure","id":"623-transactionstate-data-structure"},{"level":4,"text":"6.2.4 Before/After Image Construction Algorithm","id":"624-beforeafter-image-construction-algorithm"},{"level":3,"text":"6.3 ADR: Ordering and Deduplication Strategy","id":"63-adr-ordering-and-deduplication-strategy"},{"level":4,"text":"6.3.1 Deduplication Implementation Strategy","id":"631-deduplication-implementation-strategy"},{"level":4,"text":"6.3.2 Ordering Guarantees by Operation Type","id":"632-ordering-guarantees-by-operation-type"},{"level":3,"text":"6.4 Implementation Guidance","id":"64-implementation-guidance"},{"level":2,"text":"7. Component: Event Streamer &amp; Delivery","id":"7-component-event-streamer-amp-delivery"},{"level":3,"text":"7.1 Mental Model: The Reliable Postal Service","id":"71-mental-model-the-reliable-postal-service"},{"level":3,"text":"7.2 Interface and Delivery Semantics","id":"72-interface-and-delivery-semantics"},{"level":3,"text":"7.3 Internal Behavior and Algorithm","id":"73-internal-behavior-and-algorithm"},{"level":3,"text":"7.4 ADR: At-Least-Once vs. Exactly-Once Delivery","id":"74-adr-at-least-once-vs-exactly-once-delivery"},{"level":3,"text":"7.5 ADR: Topic Partitioning Strategy","id":"75-adr-topic-partitioning-strategy"},{"level":3,"text":"7.6 Common Pitfalls in Event Delivery","id":"76-common-pitfalls-in-event-delivery"},{"level":3,"text":"7.7 Implementation Guidance","id":"77-implementation-guidance"},{"level":2,"text":"8. Component: Schema Registry &amp; Evolution","id":"8-component-schema-registry-amp-evolution"},{"level":3,"text":"8.1 Mental Model: The Contract Librarian","id":"81-mental-model-the-contract-librarian"},{"level":3,"text":"8.2 Interface and Versioning","id":"82-interface-and-versioning"},{"level":3,"text":"8.3 ADR: Choosing a Schema Compatibility Mode","id":"83-adr-choosing-a-schema-compatibility-mode"},{"level":3,"text":"8.4 Common Pitfalls in Schema Evolution","id":"84-common-pitfalls-in-schema-evolution"},{"level":3,"text":"8.5 Implementation Guidance","id":"85-implementation-guidance"},{"level":2,"text":"9. Interactions and Data Flow","id":"9-interactions-and-data-flow"},{"level":3,"text":"9.1 Normal Operation Flow","id":"91-normal-operation-flow"},{"level":3,"text":"9.2 Schema Change Flow","id":"92-schema-change-flow"},{"level":3,"text":"9.3 Implementation Guidance","id":"93-implementation-guidance"},{"level":2,"text":"10. Error Handling and Edge Cases","id":"10-error-handling-and-edge-cases"},{"level":3,"text":"10.1 Failure Modes and Detection","id":"101-failure-modes-and-detection"},{"level":4,"text":"10.1.1 Log Connector &amp; Parser Failures","id":"1011-log-connector-amp-parser-failures"},{"level":4,"text":"10.1.2 Change Event Builder Failures","id":"1012-change-event-builder-failures"},{"level":4,"text":"10.1.3 Event Streamer &amp; Delivery Failures","id":"1013-event-streamer-amp-delivery-failures"},{"level":4,"text":"10.1.4 Schema Registry Failures","id":"1014-schema-registry-failures"},{"level":4,"text":"10.1.5 Systemic Failure Modes","id":"1015-systemic-failure-modes"},{"level":3,"text":"10.2 Recovery Strategies","id":"102-recovery-strategies"},{"level":4,"text":"10.2.1 Log Connector Recovery Strategies","id":"1021-log-connector-recovery-strategies"},{"level":4,"text":"10.2.2 Change Event Builder Recovery Strategies","id":"1022-change-event-builder-recovery-strategies"},{"level":4,"text":"10.2.3 Event Streamer Recovery Strategies","id":"1023-event-streamer-recovery-strategies"},{"level":4,"text":"10.2.4 Schema Registry Recovery Strategies","id":"1024-schema-registry-recovery-strategies"},{"level":4,"text":"10.2.5 Systemic Recovery Strategies","id":"1025-systemic-recovery-strategies"},{"level":4,"text":"10.2.6 Recovery Verification and Testing","id":"1026-recovery-verification-and-testing"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations Table","id":"technology-recommendations-table"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code: Circuit Breaker Manager","id":"infrastructure-starter-code-circuit-breaker-manager"},{"level":4,"text":"Core Logic Skeleton: Recovery Coordinator","id":"core-logic-skeleton-recovery-coordinator"},{"level":4,"text":"Language-Specific Hints: Java Error Handling","id":"language-specific-hints-java-error-handling"},{"level":4,"text":"Milestone Checkpoint: Recovery Verification","id":"milestone-checkpoint-recovery-verification"},{"level":4,"text":"Debugging Tips for Error Recovery","id":"debugging-tips-for-error-recovery"},{"level":2,"text":"11. Testing Strategy","id":"11-testing-strategy"},{"level":3,"text":"11.1 Test Pyramid for CDC","id":"111-test-pyramid-for-cdc"},{"level":4,"text":"Layer 1: Unit Tests (The Foundation)","id":"layer-1-unit-tests-the-foundation"},{"level":4,"text":"Layer 2: Integration Tests (The Connective Tissue)","id":"layer-2-integration-tests-the-connective-tissue"},{"level":4,"text":"Layer 3: End-to-End Tests (The System Verification)","id":"layer-3-end-to-end-tests-the-system-verification"},{"level":3,"text":"11.2 Milestone Verification Checkpoints","id":"112-milestone-verification-checkpoints"},{"level":4,"text":"Milestone 1: Log Parsing &amp; Change Events","id":"milestone-1-log-parsing-amp-change-events"},{"level":4,"text":"Milestone 2: Event Streaming &amp; Delivery","id":"milestone-2-event-streaming-amp-delivery"},{"level":4,"text":"Milestone 3: Schema Evolution &amp; Compatibility","id":"milestone-3-schema-evolution-amp-compatibility"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints (Java)","id":"e-language-specific-hints-java"},{"level":4,"text":"F. Milestone Checkpoint Verification","id":"f-milestone-checkpoint-verification"},{"level":4,"text":"G. Debugging Tips for Test Failures","id":"g-debugging-tips-for-test-failures"},{"level":2,"text":"12. Debugging Guide","id":"12-debugging-guide"},{"level":3,"text":"12.1 Common Bugs and Fixes","id":"121-common-bugs-and-fixes"},{"level":4,"text":"Architecture Decision: Debuggability vs Performance","id":"architecture-decision-debuggability-vs-performance"},{"level":3,"text":"12.2 Inspection and Tracing Techniques","id":"122-inspection-and-tracing-techniques"},{"level":4,"text":"12.2.1 Log Position Inspection","id":"1221-log-position-inspection"},{"level":4,"text":"12.2.2 Event Content Inspection","id":"1222-event-content-inspection"},{"level":4,"text":"12.2.3 Consumer Offset Monitoring","id":"1223-consumer-offset-monitoring"},{"level":4,"text":"12.2.4 Schema Version Tracing","id":"1224-schema-version-tracing"},{"level":4,"text":"12.2.5 Distributed Tracing for CDC","id":"1225-distributed-tracing-for-cdc"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations Table","id":"a-technology-recommendations-table"},{"level":4,"text":"B. Recommended File Structure","id":"b-recommended-file-structure"},{"level":4,"text":"C. Infrastructure Starter Code: Event Snapshot Buffer","id":"c-infrastructure-starter-code-event-snapshot-buffer"},{"level":4,"text":"D. Core Logic Skeleton: Gap Detection in LogConnector","id":"d-core-logic-skeleton-gap-detection-in-logconnector"},{"level":4,"text":"E. Java-Specific Debugging Hints","id":"e-java-specific-debugging-hints"},{"level":4,"text":"F. Debugging Checklist Milestone Verification","id":"f-debugging-checklist-milestone-verification"},{"level":4,"text":"G. Advanced Debugging: Chaos Engineering","id":"g-advanced-debugging-chaos-engineering"},{"level":2,"text":"13. Future Extensions","id":"13-future-extensions"},{"level":3,"text":"13.1 Possible Extension Roadmap","id":"131-possible-extension-roadmap"},{"level":4,"text":"Extension 1: Multi-Database Support","id":"extension-1-multi-database-support"},{"level":4,"text":"Extension 2: Exactly-Once Delivery Semantics","id":"extension-2-exactly-once-delivery-semantics"},{"level":4,"text":"Extension 3: Fully Managed Cloud Service","id":"extension-3-fully-managed-cloud-service"},{"level":4,"text":"Extension 4: Real-Time Transformation and Enrichment","id":"extension-4-real-time-transformation-and-enrichment"},{"level":4,"text":"Extension 5: Advanced Monitoring and Observability","id":"extension-5-advanced-monitoring-and-observability"},{"level":4,"text":"Extension 6: Change Event Querying and Playback","id":"extension-6-change-event-querying-and-playback"},{"level":4,"text":"Extension Roadmap Prioritization Matrix","id":"extension-roadmap-prioritization-matrix"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"14. Glossary","id":"14-glossary"},{"level":3,"text":"14.1 Terms and Definitions","id":"141-terms-and-definitions"}],"title":"Project CDC: Real-Time Change Data Capture System Design","markdown":"# Project CDC: Real-Time Change Data Capture System Design\n\n\n## Overview\n\nThis document outlines the design for a Change Data Capture (CDC) system that streams database changes (INSERT, UPDATE, DELETE) to downstream consumers in real-time. The core challenge is reliably reading low-level database transaction logs (like PostgreSQL WAL or MySQL binlog), transforming them into structured change events, and delivering them with strong ordering guarantees, all while handling schema changes and database-specific complexities without impacting the source database's performance.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n> **Milestone(s):** Milestone 1 (foundational concepts), Milestone 2 (delivery concepts), Milestone 3 (schema concepts)\n\n## 1. Context and Problem Statement\n\nModern applications increasingly rely on real-time data flows—when a user updates their profile, the search index should reflect it instantly; when an inventory item sells out, the e-commerce listing should update immediately; when a financial transaction occurs, fraud detection must analyze it in seconds. This creates a fundamental challenge: **how do you reliably propagate database changes to downstream systems the moment they happen?**\n\nTraditional approaches like periodic polling or batch ETL jobs are fundamentally mismatched for this real-time requirement. They introduce significant latency (minutes to hours), waste resources by repeatedly querying unchanged data, and can't guarantee they capture every change. The core problem this design document addresses is building a system that captures database changes **as they occur** with **minimal performance impact** on the source database, **strong ordering guarantees**, and **robust handling of schema evolution**—all while being maintainable across different database technologies.\n\n### 1.1 The Newspaper Analogy for CDC\n\nThink of your application database as a town hall where official records are kept. Different departments (downstream systems) need the latest information: the tax office needs new business registrations, the postal service needs address changes, and the library needs updates to public records.\n\n**Polling-Based Approach (The Town Crier):** Every hour, each department sends a clerk to the town hall to manually copy all records changed since their last visit. This is wasteful (clerks often find nothing changed), slow (changes aren't known until the next hourly visit), and error-prone (what if the clerk misses a page?).\n\n**Trigger-Based Approach (The Overzealous Clerk):** The town hall hires a clerk who sits beside every record book. Whenever a record is modified, the clerk immediately makes a copy and rushes it to all departments. This ensures real-time delivery but causes chaos—the clerk's constant interruptions slow down the main record-keeping work, and if the clerk gets sick, all updates stop.\n\n**Log-Based CDC (The Newspaper Printer):** The town hall maintains a **printing press** that automatically produces a copy of every change made to the records, in the exact order they occurred. This press operates in a separate room, not interfering with the main clerks. At 5 AM each morning, the press rolls, producing a **newspaper** (the transaction log) containing yesterday's changes. Downstream systems simply subscribe to the newspaper delivery. This approach is **non-intrusive** (doesn't slow record-keeping), **complete** (every change is printed), and **ordered** (changes appear in the sequence they happened).\n\n> **Key Insight:** Transaction logs (Write-Ahead Logs in PostgreSQL, binlogs in MySQL) are the database's internal \"newspaper printer.\" They already exist for crash recovery purposes. Log-based CDC reads this existing byproduct, making it the most efficient and reliable method for change capture.\n\n### 1.2 The Technical Challenge of Log-Based CDC\n\nWhile the newspaper analogy simplifies the concept, the engineering reality involves navigating a labyrinth of low-level complexities. Building a production-grade CDC system is challenging because you're interfacing with the database's most critical internal mechanism—its transaction log—which was designed for recovery, not for external consumption.\n\n**1. Database-Specific Binary Formats:** Each database implements its transaction log differently. PostgreSQL's WAL is a low-level physical log recording page-level changes, while MySQL's binlog can be configured for statement-based (SQL commands) or row-based (actual row changes) logging. Parsing these requires deep understanding of each database's internal binary format, which can change between minor versions.\n\n**2. Transaction Boundary Reconstruction:** Databases write to their logs during transactions, not just at commit. A single logical update might involve multiple log entries across different tables. The CDC system must **reconstruct transaction boundaries**—grouping all changes belonging to a single transaction and only emitting them when the commit record appears. This requires maintaining in-memory state for pending transactions, which introduces complexity around memory management and crash recovery.\n\n**3. Ordering and Duplication Guarantees:** Downstream consumers often require that changes to the same database row are processed in the exact order they occurred. If a row is updated from value A→B→C, consumers must see A→B→C, never A→C→B or B→A→C. The CDC system must guarantee **per-primary-key ordering** even when reading from a log that might interleave changes from multiple concurrent transactions. Additionally, after a system restart or network failure, the CDC system must resume from exactly where it left off without **skipping changes** or **reprocessing duplicates**.\n\n**4. Schema Evolution Without Breaking Consumers:** Databases evolve—columns are added, removed, or renamed; data types change. A CDC system emitting change events must handle these **schema changes gracefully**. If a consumer built to expect three columns suddenly receives events with four columns, it will fail. The system needs a **versioned schema registry** and **compatibility rules** to ensure old consumers can still read new events (backward compatibility) and new consumers can read old events (forward compatibility).\n\n**5. Performance Isolation and Backpressure:** The CDC system must operate without degrading the source database's performance. This means:\n   - **Minimal read load:** Efficiently tailing the log without expensive queries.\n   - **Backpressure handling:** If downstream consumers slow down, the CDC system must **pause log reading** rather than buffer events infinitely in memory, which could lead to out-of-memory crashes.\n   - **Network resilience:** Surviving temporary disconnections from the database or message broker without data loss.\n\nThe following table summarizes these core challenges and their implications:\n\n| Challenge | Technical Implication | Consequence If Mishandled |\n|-----------|----------------------|---------------------------|\n| **Binary Log Formats** | Need separate parser for each database (PostgreSQL, MySQL, etc.) | System only works with one database; breaks on version upgrades |\n| **Transaction Reconstruction** | Must buffer uncommitted changes in memory | Memory exhaustion on large transactions; lost changes on crash |\n| **Ordering Guarantees** | Must track position per partition/key | Consumers process changes out-of-order, causing data corruption |\n| **Schema Evolution** | Need versioned schemas + compatibility checks | Consumer crashes on schema changes; requires manual intervention |\n| **Backpressure** | Must coordinate between log reader and event publisher | Memory overflow; stalled log reading causing lag |\n\n### 1.3 Existing Approaches and Trade-offs\n\nBefore committing to log-based CDC, we evaluated three primary architectural approaches, each with distinct trade-offs. The decision matrix below compares them across critical dimensions for our use case:\n\n| Approach | How It Works | Latency | Performance Impact on Source DB | Reliability/Completeness | Implementation Complexity |\n|----------|--------------|---------|--------------------------------|--------------------------|---------------------------|\n| **Application-Level Dual Writes** | Application code writes to DB and simultaneously publishes to message queue | Very Low (ms) | High (doubles write latency, adds failure modes) | Low (race conditions, partial failures cause inconsistency) | Medium |\n| **Database Triggers** | Database triggers fire on DML, calling stored procedure to publish changes | Low (ms) | Very High (serializes writes, adds trigger overhead) | Medium (trigger failures silently lose changes) | High (database-specific, hard to debug) |\n| **Polling with Timestamps/Version Columns** | Periodic SELECT queries for rows where `updated_at > last_check` | High (seconds-minutes) | Medium (indexed queries still cause load) | Low (misses deletions, concurrent updates) | Low |\n| **Log-Based CDC (Our Choice)** | Read database transaction log (WAL/binlog) asynchronously | Low (ms-seconds) | **Very Low** (reads existing log, non-intrusive) | **High** (captures every change, ordered) | **Very High** (complex parsing, state management) |\n\n> **Decision: Choosing Log-Based CDC Over Alternatives**\n> - **Context**: We need a production-grade CDC system that guarantees complete change capture with minimal performance impact on source databases, suitable for high-volume transactional systems.\n> - **Options Considered**:\n>   1. **Application-Level Dual Writes**: Modify every application to write to both DB and message queue.\n>   2. **Database Triggers**: Use database triggers to publish changes via stored procedures.\n>   3. **Polling with Change Tracking**: Periodic queries using timestamps or SQL Server Change Tracking.\n>   4. **Log-Based CDC**: Asynchronous reading of transaction logs.\n> - **Decision**: Implement log-based CDC using database transaction logs (PostgreSQL WAL, MySQL binlog).\n> - **Rationale**:\n>   - **Performance Isolation**: Transaction logs are already written for crash recovery; reading them adds negligible load compared to triggers (which serialize writes) or dual writes (which double latency).\n>   - **Completeness Guarantee**: Logs capture EVERY committed change including deletions—polling approaches inherently miss deletions unless using tombstone flags.\n>   - **Ordering Preservation**: Logs preserve the exact commit sequence, enabling per-key ordering guarantees crucial for downstream consistency.\n>   - **No Application Changes**: Works with existing applications without code modifications—critical for legacy systems.\n> - **Consequences**:\n>   - **Increased Implementation Complexity**: Must build/maintain database-specific log parsers that understand binary formats.\n>   - **Database Version Coupling**: Parser may need updates for new database versions.\n>   - **Initial Snapshot Requirement**: Need a mechanism to capture initial table state before beginning log streaming.\n\n**Deep Dive: Why Not Application-Level Dual Writes?**\n\nWhile dual writes seem simple—just add a `kafkaProducer.send()` after your database `INSERT`—they suffer from **distributed consistency problems**. Consider this scenario:\n\n1. Application writes to Database → SUCCESS\n2. Application writes to Kafka → NETWORK TIMEOUT\n3. What now? The database has the change but Kafka doesn't.\n\nThe application must now decide: roll back the database transaction (impacting user experience) or retry the Kafka write (potentially causing duplicates). Even with retries, there's a window where the systems are inconsistent. This problem is formally known as the **Two Generals' Problem**—there's no guaranteed way to coordinate two distributed systems to both commit or both rollback. Log-based CDC avoids this by making the database the **single source of truth** and treating change capture as an asynchronous follower.\n\n**Deep Dive: The Pitfalls of Trigger-Based CDC**\n\nTriggers seem appealing—they're database-native and guarantee capture. However, they introduce **severe performance penalties**:\n\n- **Serial Execution**: Triggers execute within the same transaction as the DML operation. If the trigger code (which publishes to Kafka) takes 100ms, every database write now takes 100ms longer.\n- **Single Point of Failure**: If the network to Kafka is down, the trigger fails, causing the entire database transaction to fail—blocking all writes.\n- **Debugging Complexity**: Trigger errors manifest as mysterious database errors, requiring deep database-specific expertise to troubleshoot.\n\nA real-world example: An e-commerce platform implemented trigger-based CDC. During Black Friday, write volume increased 50x. The trigger overhead caused transaction locks to be held longer, leading to **deadlock storms** that brought the entire checkout system to a halt.\n\n**Deep Dive: Limitations of Polling Approaches**\n\nPolling with `updated_at` columns or Change Tracking has fundamental gaps:\n\n1. **Misses Deletions**: Unless you implement soft deletes with a `deleted_at` column, `DELETE` operations vanish without a trace.\n2. **Race Conditions**: If two updates occur between polling intervals, you might only capture the second one.\n3. **Performance Scaling**: As the table grows, `SELECT ... WHERE updated_at > ?` queries become expensive even with indexes, competing with operational queries.\n4. **No Transaction Boundaries**: You see individual row changes but lose the information that changes to Table A and Table B were part of the same atomic transaction.\n\nThese limitations make polling unsuitable for financial systems, inventory management, or any use case requiring complete, ordered change capture.\n\n> **Design Principle:** A CDC system should be **invisible to the source database**—it shouldn't add latency to writes, consume significant CPU, or become a single point of failure. Log-based CDC, while complex to implement, is the only approach that satisfies this principle while providing the completeness and ordering guarantees required for mission-critical data pipelines.\n\nBy embracing log-based CDC, we accept the upfront complexity of parsing binary logs in exchange for a robust, performant foundation that can scale with the database without impacting it. The remainder of this design document details how we navigate these complexities through careful component design, state management, and fault-tolerant streaming.\n\n### Implementation Guidance\n\nSince this section is purely conceptual (Context and Problem Statement), there is no implementation code required. However, we provide foundational technology recommendations that will inform the implementation in subsequent sections.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option (For Learning/Prototyping) | Advanced Option (Production-Grade) |\n|-----------|------------------------------------------|-----------------------------------|\n| **Database Connection** | JDBC for initial snapshot + direct TCP socket for log streaming | Database-native libraries (PostgreSQL: `pgjdbc` with `PGReplicationStream`; MySQL: `mysql-connector-j` with binlog listener) |\n| **Log Parsing** | Custom parser for a single database version using documented format | Debezium Connector framework (pluggable parsers, handles version differences) |\n| **Event Streaming** | Apache Kafka with Java Producer API | Kafka with exactly-once semantics (Kafka Transactions) or Apache Pulsar |\n| **Schema Management** | Simple file-based schema registry (JSON files) | Confluent Schema Registry (REST API, Avro schemas, compatibility checks) |\n| **Monitoring** | Log files + manual offset tracking | Prometheus metrics + Grafana dashboards for consumer lag, parse errors |\n\n**B. Recommended Project File Structure:**\n\nWhile the full architecture will be detailed in Section 3, we recommend starting with this structure to organize the codebase:\n\n```\ncdc-system/\n├── docs/                           # Design documents, diagrams\n│   └── diagrams/                   # SVG diagram files\n├── lib/                            # Database-specific parser libraries (if not using Debezium)\n│   ├── postgres-wal-parser/\n│   └── mysql-binlog-parser/\n├── src/main/java/com/cdc/\n│   ├── core/                       # Core data models and interfaces\n│   │   ├── ChangeEvent.java\n│   │   ├── RawLogEntry.java\n│   │   └── SchemaVersion.java\n│   ├── connector/                  # Log Connector & Parser (Milestone 1)\n│   │   ├── LogConnector.java\n│   │   ├── postgres/              # PostgreSQL-specific implementation\n│   │   └── mysql/                 # MySQL-specific implementation\n│   ├── builder/                    # Change Event Builder (Milestone 1)\n│   │   └── ChangeEventBuilder.java\n│   ├── streamer/                   # Event Streamer & Delivery (Milestone 2)\n│   │   └── EventStreamer.java\n│   ├── schema/                     # Schema Registry & Evolution (Milestone 3)\n│   │   └── SchemaRegistry.java\n│   └── util/                       # Utilities (serialization, configuration, metrics)\n├── config/                         # Configuration files\n│   ├── cdc-config.yaml\n│   └── log4j2.xml\n├── tests/                          # Test suites\n│   ├── unit/                       # Unit tests\n│   ├── integration/                # Integration tests with embedded DB\n│   └── e2e/                        # End-to-end tests\n└── README.md\n```\n\n**C. Infrastructure Starter Code:**\n\nBefore implementing log parsing, you'll need a configuration management utility. Here's a complete, ready-to-use configuration loader:\n\n```java\npackage com.cdc.util;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.dataformat.yaml.YAMLFactory;\nimport java.io.File;\nimport java.io.IOException;\n\n/**\n * Configuration loader for CDC system. Loads YAML configuration files.\n * This is a prerequisite utility - copy and use as-is.\n */\npublic class ConfigLoader {\n    private static final ObjectMapper mapper = new ObjectMapper(new YAMLFactory());\n    \n    /**\n     * Load configuration from YAML file.\n     * @param configPath Path to YAML configuration file\n     * @param configClass Configuration class type\n     * @return Populated configuration object\n     * @throws IOException If file cannot be read or parsed\n     */\n    public static <T> T loadConfig(String configPath, Class<T> configClass) throws IOException {\n        File configFile = new File(configPath);\n        if (!configFile.exists()) {\n            throw new IOException(\"Configuration file not found: \" + configPath);\n        }\n        return mapper.readValue(configFile, configClass);\n    }\n}\n\n/**\n * Main CDC configuration class. Extend this with database-specific sections.\n */\npublic class CdcConfig {\n    private DatabaseConfig database;\n    private StreamingConfig streaming;\n    private SchemaConfig schema;\n    \n    // Getters and setters\n    public DatabaseConfig getDatabase() { return database; }\n    public void setDatabase(DatabaseConfig database) { this.database = database; }\n    \n    public static class DatabaseConfig {\n        private String type; // \"postgresql\" or \"mysql\"\n        private String host;\n        private int port;\n        private String database;\n        private String username;\n        private String password;\n        private String slotName; // For PostgreSQL replication slot\n        // Getters and setters\n    }\n    \n    public static class StreamingConfig {\n        private String bootstrapServers; // Kafka bootstrap servers\n        private String topicPrefix = \"cdc.\";\n        private int maxInFlightRequests = 5;\n        // Getters and setters\n    }\n    \n    public static class SchemaConfig {\n        private String registryUrl; // Schema Registry URL\n        private boolean autoRegisterSchemas = true;\n        // Getters and setters\n    }\n}\n```\n\n**D. Language-Specific Hints (Java):**\n\n- **Use try-with-resources**: Always use `try-with-resources` for database connections, network sockets, and file streams to ensure proper cleanup.\n- **Logging framework**: Use SLF4J with Logback or Log4j2 for structured logging. Avoid `System.out.println()` in production code.\n- **Concurrency**: Use `java.util.concurrent` classes (`ExecutorService`, `ConcurrentHashMap`) rather than manual `synchronized` blocks where possible.\n- **Error handling**: Use specific exception types (`IOException`, `SQLException`) rather than generic `Exception` catches to handle different failure modes appropriately.\n- **Configuration**: Use environment variables for secrets (passwords, API keys) rather than hardcoding in configuration files.\n\n**E. Milestone Checkpoint (Conceptual Foundation):**\n\nBefore proceeding to implementation, verify you understand these core concepts:\n- ✅ **Understand the newspaper analogy** and why log-based CDC is non-intrusive.\n- ✅ **Identify the three main challenges**: binary log parsing, transaction reconstruction, and schema evolution.\n- ✅ **Articulate why dual writes and triggers are problematic** for production systems.\n- ✅ **Set up your development environment** with Java 11+, Maven/Gradle, and access to a PostgreSQL or MySQL instance.\n\n**F. Debugging Tips (Conceptual Level):**\n\n| Symptom | Likely Conceptual Misunderstanding | How to Diagnose | Fix |\n|---------|------------------------------------|-----------------|-----|\n| \"Why not just query the table every minute?\" | Doesn't understand latency and completeness requirements | Ask: \"What happens if a row is updated twice between queries? What about DELETE operations?\" | Review polling limitations in section 1.3 |\n| \"Can't we just add a Kafka send() in our DAO layer?\" | Doesn't understand distributed consistency problem | Diagram the dual write failure scenario with network timeout | Review \"Why Not Application-Level Dual Writes?\" |\n| \"Triggers are built into the database, so they must be reliable\" | Doesn't understand performance impact of serial execution | Test trigger overhead with a simple stored procedure that sleeps for 100ms | Review trigger pitfalls in section 1.3 |\n\n---\n\n\n## 2. Goals and Non-Goals\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3 (foundational requirements for all milestones)\n\nThis section establishes a clear contract between the system designers and implementers by defining **what the system must accomplish** (Goals) and **what it explicitly will not handle** (Non-Goals). In a complex system like CDC where requirements could expand infinitely, these boundaries prevent scope creep and ensure focus on delivering core value. Think of this as the architectural constitution—it defines the principles that all subsequent design decisions must uphold.\n\n### 2.1 Goals (Must Have)\n\nThe system must deliver **real-time, reliable, and ordered** change data capture from transactional databases to downstream consumers. These goals are categorized into functional requirements (what the system does) and non-functional requirements (how well it does it).\n\n#### Functional Goals\n\n| Goal Category | Specific Requirement | Description & Rationale | Associated Milestone |\n|---------------|---------------------|------------------------|---------------------|\n| **Log Reading & Parsing** | Connect to database transaction logs | Establish a direct connection to PostgreSQL's Write-Ahead Log (WAL) or MySQL's binlog using their respective native protocols (logical decoding for PostgreSQL, binlog replication for MySQL). This is foundational—without reading the log, there is no CDC. | 1 |\n| | Parse INSERT, UPDATE, DELETE operations | Extract these three DML (Data Manipulation Language) operations from log entries. These represent the core data changes that downstream systems care about. DDL (Data Definition Language) operations like `ALTER TABLE` are handled separately under schema evolution. | 1 |\n| | Capture before and after images for UPDATE | For UPDATE operations, extract both the previous state (`beforeImage`) and the new state (`afterImage`) of the row. This enables consumers to understand what changed, not just the new value, which is critical for audit trails, incremental materialized views, and conflict resolution. | 1 |\n| | Track and persist log position | Record the **log sequence number (LSN)** for PostgreSQL or **binlog position** for MySQL after successfully processing each transaction. This state must be durably stored (e.g., in a separate state table or file) to enable resumption from the exact point after a restart, preventing data loss or duplication. | 1 |\n| **Event Construction & Delivery** | Publish events to a message broker | Stream `ChangeEvent` objects to Apache Kafka (or a similar message broker like Pulsar). The broker provides buffering, persistence, and fan-out capabilities that decouple the CDC system from consumers. | 2 |\n| | Guarantee at-least-once delivery | Ensure every `ChangeEvent` reaches the message broker at least once, even in the face of network failures or producer restarts. This is achieved through producer retries with idempotence enabled and acknowledgment (ack) from all in-sync replicas before considering an event \"sent.\" | 2 |\n| | Maintain ordering per primary key | Guarantee that all changes to the same database row (identified by its primary key) are delivered to consumers in the exact order they were committed in the source database. This preserves causality and prevents race conditions in consumer processing. | 2 |\n| | Handle backpressure | Monitor consumer lag (the difference between the latest event produced and the last event consumed). If lag exceeds a configurable threshold, the system should slow down or pause log reading to prevent overwhelming downstream systems and the broker. | 2 |\n| **Schema Management** | Store and version table schemas | Maintain a history of `SchemaVersion` objects for each table in a **schema registry**. Each version includes the complete column definition (`columnDefinitions`) at that point in time. | 3 |\n| | Enforce backward compatibility | Validate that new schema versions are **backward compatible** with the previous version. This means consumers using the old schema can read data written with the new schema. In practice, this allows adding nullable columns or columns with default values but prohibits removing columns or narrowing column types. | 3 |\n| | Propagate schema change events | Emit a special `ChangeEvent` (with `operationType = \"SCHEMA_CHANGE\"`) to a dedicated topic when a compatible schema change occurs. This event includes the new `schemaVersionId` and metadata about the change, allowing consumers to refresh their local schema cache or trigger reprocessing. | 3 |\n| **Operational & Observability** | Provide monitoring metrics | Expose key metrics: number of events processed per second, consumer lag per partition, parse errors, schema validation failures, and database connection health. These should be available via a Prometheus endpoint or similar. | 2, 3 |\n| | Support graceful shutdown and restart | On shutdown signal, complete processing of the current log transaction, flush all pending events to the broker, commit the final log position, and then exit. On restart, reload the last committed position and resume reading from that exact point. | 1, 2 |\n| | Configuration externalization | All operational parameters (database connection details, broker addresses, thresholds) must be loadable from external configuration files or environment variables, not hardcoded. The `ConfigLoader.loadConfig` utility is provided for this purpose. | All |\n\n> **Architectural Insight:** The functional goals are intentionally scoped to **logical decoding** of committed transactions only. We avoid **trigger-based** CDC (which modifies application tables) and **query-based** CDC (which polls tables), as these impose unacceptable performance overhead on the source database and cannot capture deletes. Reading the transaction log is the only non-intrusive method for real-time change capture.\n\n#### Non-Functional Goals (Quality Attributes)\n\n| Quality Attribute | Target | Justification |\n|------------------|--------|--------------|\n| **Latency** | End-to-end latency (database commit to event available to consumer) ≤ 1 second under normal load. | Real-time use cases (e.g., fraud detection, live dashboards) require sub-second freshness. The primary latency contributors are log flush frequency on the database (controllable via configuration) and network round-trip to the broker. |\n| **Throughput** | Sustain processing of at least 10,000 change events per second on a single CDC instance. | Must handle peak write loads of typical online transaction processing (OLTP) systems. Throughput is primarily bounded by log parsing CPU, network bandwidth to the broker, and serialization overhead. |\n| **Availability** | 99.9% uptime (≈8.76 hours of downtime per year). The system should be restartable without data loss. | While not mission-critical like the source database, the CDC system is a core data pipeline component. High availability is achieved through fast recovery (resuming from persisted position) and potentially multiple instances (though only one can read the log at a time). |\n| **Durability** | Zero data loss under single-node failure of the CDC system. | Achieved by **only advancing the persisted log position after events are confirmed as durably stored in the message broker** (acknowledged by all in-sync replicas). This creates a chain of durability from database log → broker → consumer. |\n| **Operational Simplicity** | Single binary deployment with minimal runtime dependencies (besides the database and broker). | Reduces deployment complexity and operational overhead. The CDC system should not require a separate cluster manager, distributed coordination service (like ZooKeeper), or complex orchestration to function in its basic form. |\n| **Extensibility** | Clear interfaces for adding support for new database types (e.g., Oracle, SQL Server). | The core CDC logic should be database-agnostic. Database-specific parsing is isolated behind a `LogConnector` interface, allowing new adapters to be plugged in with minimal changes to the event pipeline. |\n\n### 2.2 Non-Goals (Explicitly Out of Scope)\n\nExplicitly stating what the system **will not do** is equally important to prevent misaligned expectations and endless feature creep. These decisions allow us to build a focused, robust core system.\n\n> **Decision: Focus on Core CDC Pipeline, Not Auxiliary Services**  \n> **Context:** A complete enterprise data platform might include many related services: data transformation, enrichment, dead-letter queues, complex event processing, and managed deployments.  \n> **Options Considered:**  \n> 1. **Build a comprehensive data pipeline platform** with built-in transformation, enrichment, and complex routing.  \n> 2. **Build only the core CDC extraction and delivery pipeline**, delegating other concerns to specialized downstream systems.  \n> **Decision:** Option 2—focus on being a reliable source of raw change events.  \n> **Rationale:** The Unix philosophy: \"Do one thing well.\" By outputting standardized `ChangeEvent` objects to a message broker, we enable any number of downstream systems (Stream processors like Flink, ETL tools, custom applications) to consume and transform the data according to their specific needs. This keeps the CDC system simple, maintainable, and focused on its hardest problem: reliable log reading.  \n> **Consequences:** Users must deploy additional systems for transformations, but they gain flexibility in choosing the right tool for each job. The CDC system remains stable and easier to reason about.\n\n| Non-Goal Area | Specific Exclusions | Reasoning & Recommended Alternative |\n|---------------|-------------------|-----------------------------------|\n| **Data Transformation & Enrichment** | No in-flight modification of event payloads (e.g., joining with reference data, masking sensitive fields, computing derived columns). | Transformation is a separate concern with different scalability and fault-tolerance requirements. **Alternative:** Stream processing frameworks (Apache Flink, Kafka Streams) or dedicated ETL tools should consume raw events and perform transformations. |\n| **Exactly-Once Delivery Semantics** | The system will not guarantee exactly-once delivery to end consumers in the face of all possible failures. | Achieving exactly-once semantics across the entire pipeline (database → CDC → broker → consumer) requires distributed transactions and heavy coordination, which adds immense complexity and performance overhead. **Alternative:** We guarantee **at-least-once** with idempotent producers to the broker, and consumers must implement **idempotent processing** (e.g., deduplication by `eventId` or by primary key + timestamp) to achieve effectively-once semantics. |\n| **Global Ordering Across All Events** | Events across different tables or different primary keys within the same table are **not** guaranteed to be delivered in commit order relative to each other. | Maintaining strict global ordering would require single-threaded processing or complex distributed sequencing, destroying throughput. **Alternative:** We guarantee **partition-level ordering** (per primary key). Consumers needing cross-key causality must infer it via timestamps or use more advanced techniques like distributed snapshots, which are out of scope. |\n| **DDL Change Automation** | The system will not automatically apply DDL changes (like `ALTER TABLE`) to downstream data stores or data warehouses. | Schema changes often require careful migration planning, data backfilling, and consumer coordination. Automating this is risky. **Alternative:** The system **will detect and notify** of schema changes via `SchemaChangeEvent`. A separate orchestration process or human operator should then manage the downstream migration. |\n| **Change Data Capture for Non-Supported Databases** | Out-of-the-box support for databases other than PostgreSQL and MySQL (e.g., Oracle, SQL Server, MongoDB). | Each database has unique log formats and replication protocols. Supporting all would make the codebase unwieldy. **Alternative:** The system is designed with a pluggable `LogConnector` interface. Support for additional databases can be added as extensions in future versions without modifying the core pipeline. |\n| **Historical Data Snapshotting** | The system will not perform initial full-table snapshots (bulk load of existing data) before beginning incremental change capture. | Snapshotting requires different mechanisms (e.g., table locks, consistent reads) and has different recovery characteristics than log reading. **Alternative:** A separate, one-time snapshot tool should be used to bootstrap consumers. The CDC system then streams changes from the snapshot point forward. |\n| **Long-Term Event Storage & Archive** | Events are not stored indefinitely within the CDC system. Retention is delegated to the message broker. | Storage management is a separate concern. **Alternative:** Configure Kafka retention policies based on storage capacity and compliance requirements. For archival needs, use Kafka Connect to sink events to long-term storage (S3, HDFS). |\n| **Graphical User Interface (GUI)** | No administrative web UI for monitoring, configuration, or control. | Building a production-grade UI is a major undertaking that distracts from core data pipeline reliability. **Alternative:** Expose metrics via Prometheus (for dashboards with Grafana) and provide command-line tools for operational tasks. |\n| **Multi-Tenancy & Access Control** | No built-in user authentication, authorization, or tenant isolation within the CDC system itself. | Security boundaries should be enforced at the infrastructure level (database permissions, network isolation, broker ACLs). **Alternative:** Run separate CDC instances per tenant, or rely on database and broker-level security features. |\n\n> **Key Design Principle:** By explicitly excluding these areas, we commit to building a **simple, composable component** rather than a monolithic platform. This aligns with the modern data stack philosophy, where best-of-breed tools are connected via streaming platforms. The CDC system's job is to be the most reliable and performant bridge between the database and that ecosystem.\n\nThe following table summarizes the architectural trade-offs implied by these non-goals:\n\n| Trade-off | Benefit | Cost |\n|-----------|---------|------|\n| **At-least-once vs Exactly-once** | Simpler implementation, higher throughput, no need for distributed transaction coordinator. | Consumers must implement idempotent processing; possible duplicate events during failure recovery. |\n| **Partition ordering vs Global ordering** | Enables massive parallelism (multiple partitions processed concurrently), high throughput. | Applications needing cross-key causality must implement more complex logic or accept eventual consistency. |\n| **No built-in transformation** | Focused codebase, easier debugging, stability in core function. | Requires additional stream processing component in the architecture, increasing system complexity overall. |\n| **Notification-only for DDL** | Safe, avoids automatic actions that could break downstream systems. | Requires operational processes to handle schema migrations, potentially slower response to changes. |\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option (For Learning/Dev) | Advanced Option (For Production) |\n|-----------|----------------------------------|----------------------------------|\n| **Log Parser Library** | Direct JDBC for initial testing (simpler but higher latency) | **Debezium Embedded Engine** (mature, handles version-specific log formats, includes state management) |\n| **Message Broker** | Apache Kafka (single broker in Docker) | **Apache Kafka Cluster** (multiple brokers with replication factor ≥ 2 for high availability) |\n| **Schema Registry** | In-memory `ConcurrentHashMap`-based registry (for testing) | **Confluent Schema Registry** (RESTful, supports Avro, Protobuf, JSON Schema, compatibility checking) |\n| **Metrics & Monitoring** | Logging to stdout with structured JSON | **Micrometer + Prometheus** (exposes metrics endpoint) + **Grafana** (dashboards) |\n| **Configuration** | YAML file loaded via `ConfigLoader` | **HashiCorp Consul** or **Spring Cloud Config** (dynamic configuration, secrets management) |\n| **State Persistence** | File-based storage (JSON in local file) | **Kafka `__consumer_offsets`** topic or a dedicated database table (more durable, supports multi-instance) |\n\n#### B. Recommended Project File Structure\n\n```\ncdc-system/\n├── config/\n│   └── application.yml              # Main configuration file\n├── src/main/java/com/cdc/\n│   ├── core/\n│   │   ├── ConfigLoader.java        # Provided utility\n│   │   ├── RawLogEntry.java         # Core data type\n│   │   ├── ChangeEvent.java         # Core data type\n│   │   └── SchemaVersion.java       # Core data type\n│   ├── connector/\n│   │   ├── LogConnector.java        # Interface for database-specific connectors\n│   │   ├── postgres/\n│   │   │   ├── PostgresWalConnector.java\n│   │   │   └── PostgresWalParser.java\n│   │   └── mysql/\n│   │       ├── MySqlBinlogConnector.java\n│   │       └── MySqlBinlogParser.java\n│   ├── builder/\n│   │   ├── ChangeEventBuilder.java\n│   │   └── TransactionState.java    # Tracks in-progress transactions\n│   ├── streamer/\n│   │   ├── EventStreamer.java\n│   │   ├── KafkaEventPublisher.java\n│   │   └── BackpressureController.java\n│   ├── schema/\n│   │   ├── SchemaRegistry.java\n│   │   ├── CompatibilityChecker.java\n│   │   └── InMemorySchemaRegistry.java # Simple implementation\n│   ├── state/\n│   │   └── PositionManager.java     # Persists and loads LSN/binlog position\n│   └── Main.java                    # Application entry point\n└── README.md\n```\n\n#### C. Infrastructure Starter Code\n\n**Configuration Loader (Provided Utility):**\n```java\npackage com.cdc.core;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.dataformat.yaml.YAMLFactory;\nimport java.io.File;\nimport java.io.IOException;\n\npublic class ConfigLoader {\n    private static final ObjectMapper mapper = new ObjectMapper(new YAMLFactory());\n    \n    /**\n     * Loads configuration from a YAML file and maps it to the specified class.\n     * @param configPath Path to the YAML configuration file\n     * @param configClass Class of the configuration object (e.g., AppConfig.class)\n     * @return Populated configuration object\n     * @throws IOException If file cannot be read or parsed\n     */\n    public static <T> T loadConfig(String configPath, Class<T> configClass) throws IOException {\n        return mapper.readValue(new File(configPath), configClass);\n    }\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**Application Configuration Skeleton:**\n```java\npackage com.cdc.core;\n\npublic class AppConfig {\n    private DatabaseConfig database;\n    private KafkaConfig kafka;\n    private SchemaConfig schema;\n    private int maxBatchSize;\n    private long backpressureThresholdMs;\n    \n    // TODO: Add getters and setters for all fields\n    \n    public static class DatabaseConfig {\n        private String type; // \"postgresql\" or \"mysql\"\n        private String host;\n        private int port;\n        private String database;\n        private String username;\n        private String password;\n        private String slotName; // For PostgreSQL logical decoding slot\n        // TODO: Add getters and setters\n    }\n    \n    public static class KafkaConfig {\n        private String bootstrapServers;\n        private String topicPrefix;\n        private int replicationFactor;\n        private int partitionsPerTable;\n        private String acks; // \"all\" for at-least-once\n        private boolean enableIdempotence;\n        // TODO: Add getters and setters\n    }\n    \n    public static class SchemaConfig {\n        private String registryType; // \"inmemory\" or \"confluent\"\n        private String registryUrl; // For Confluent Schema Registry\n        // TODO: Add getters and setters\n    }\n}\n```\n\n**Main Application Entry Point:**\n```java\npackage com.cdc;\n\nimport com.cdc.core.AppConfig;\nimport com.cdc.core.ConfigLoader;\nimport com.cdc.connector.LogConnector;\nimport com.cdc.connector.postgres.PostgresWalConnector;\nimport com.cdc.connector.mysql.MySqlBinlogConnector;\nimport com.cdc.builder.ChangeEventBuilder;\nimport com.cdc.streamer.EventStreamer;\nimport com.cdc.schema.SchemaRegistry;\nimport com.cdc.state.PositionManager;\n\npublic class Main {\n    public static void main(String[] args) {\n        try {\n            // TODO 1: Load configuration from \"config/application.yml\"\n            // TODO 2: Initialize PositionManager to load last saved log position\n            // TODO 3: Initialize SchemaRegistry (in-memory or remote)\n            // TODO 4: Create appropriate LogConnector based on database type\n            // TODO 5: Initialize ChangeEventBuilder with SchemaRegistry and PositionManager\n            // TODO 6: Initialize EventStreamer with Kafka configuration\n            // TODO 7: Connect LogConnector to database and start reading from last position\n            // TODO 8: For each RawLogEntry parsed, pass to ChangeEventBuilder\n            // TODO 9: For each ChangeEvent built, pass to EventStreamer for publishing\n            // TODO 10: Handle shutdown signal to gracefully stop all components\n        } catch (Exception e) {\n            System.err.println(\"CDC System failed to start: \" + e.getMessage());\n            e.printStackTrace();\n            System.exit(1);\n        }\n    }\n}\n```\n\n#### E. Language-Specific Hints (Java)\n\n1. **Use try-with-resources** for all database and network connections to ensure proper cleanup.\n2. **For JSON serialization** of configuration and state, use Jackson (`ObjectMapper`) as shown in `ConfigLoader`.\n3. **For Kafka integration**, use the official `kafka-clients` library. Configure `ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG = true` and `ProducerConfig.ACKS_CONFIG = \"all\"` for at-least-once semantics.\n4. **For concurrent state management** in `ChangeEventBuilder`, use `ConcurrentHashMap` for tracking transaction state, keyed by `transactionId`.\n5. **Use SLF4J with Logback** for structured logging, not `System.out.println`.\n\n#### F. Milestone Checkpoint\n\nAfter implementing the foundational structure from this section (configuration loading, basic type definitions), verify:\n\n1. **Configuration Loading Works:**\n   ```\n   $ java -cp target/cdc-system.jar com.cdc.Main\n   Should print: \"CDC System starting with configuration from config/application.yml\"\n   ```\n   Create a minimal `config/application.yml`:\n   ```yaml\n   database:\n     type: \"postgresql\"\n     host: \"localhost\"\n     port: 5432\n     database: \"mydb\"\n   kafka:\n     bootstrapServers: \"localhost:9092\"\n   ```\n\n2. **Type Definitions Compile:** Ensure the three core types (`RawLogEntry`, `ChangeEvent`, `SchemaVersion`) compile without errors. You should be able to create instances of each in a simple test.\n\n3. **Project Structure Validated:** Confirm your project follows the recommended directory layout. This foundation will make subsequent milestone implementation much smoother.\n\n#### G. Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|----------------|------|\n| \"Configuration file not found\" error on startup | Wrong path or working directory | Print `new File(\".\").getAbsolutePath()` to see current directory | Use absolute path in `ConfigLoader` or run from project root |\n| `RawLogEntry`/`ChangeEvent` fields missing or wrong type | Type definitions don't match naming conventions | Compare field names exactly with NAMING CONVENTIONS section | Ensure fields match EXACTLY: `logSequenceNumber String`, `operationType String`, etc. |\n| Unable to import Jackson classes in `ConfigLoader` | Missing dependency in build file (pom.xml or build.gradle) | Check if `jackson-dataformat-yaml` is in dependencies | Add: `<groupId>com.fasterxml.jackson.dataformat</groupId><artifactId>jackson-dataformat-yaml</artifactId>` |\n\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3\n\n## 3. High-Level Architecture\n\nThis section provides a comprehensive architectural blueprint of the Change Data Capture (CDC) system. Before diving into component-level details, we establish the \"big picture\" view of how the system transforms low-level database transaction logs into reliable, schema-aware event streams for downstream consumers. Think of the architecture as an **industrial assembly line**: raw database log entries enter at one end and, through a series of specialized processing stations, emerge as polished, packaged change events ready for delivery. Each station has a specific responsibility, maintains its own state, and communicates with others through well-defined interfaces.\n\nThe architecture follows a **pipeline pattern** with clear separation of concerns, enabling independent development, testing, and scaling of each component. The system is designed to be **database-agnostic** at the pipeline level, with database-specific details encapsulated in pluggable adapters. This modular approach balances the need for consistent processing semantics across different database systems with the reality that each database implements its transaction logs differently.\n\n![High-Level System Component Diagram](./diagrams/system-component.svg)\n\n### 3.1 Component Overview and Responsibilities\n\nThe system comprises four core internal components that work together, plus three external systems they interact with. The following table provides a complete reference of each component's role, ownership, and key responsibilities:\n\n| Component | Role (Mental Model) | What It Owns (State) | Key Responsibilities | Input → Output Transformation |\n|-----------|---------------------|----------------------|----------------------|-------------------------------|\n| **Log Connector & Parser** (Milestone 1) | **The Database Translator** – reads the database's internal \"flight recorder\" and converts its binary format into structured data. | - Database connection handle<br>- Last processed LSN/offset<br>- Parser state (connected, reading, paused)<br>- Database-specific parser implementation | 1. Establishes and maintains connection to database transaction log (WAL/binlog)<br>2. Reads binary log entries in streaming fashion<br>3. Parses binary format into structured `RawLogEntry` objects<br>4. Tracks position (LSN) for resumption after restart<br>5. Handles database-specific quirks and version differences | Binary log entry → `RawLogEntry` |\n| **Change Event Builder** (Milestone 1) | **The Event Assembler** – groups related log entries into complete change events, ensuring transaction atomicity and deduplication. | - In-progress transaction buffer<br>- Last seen event IDs for deduplication<br>- Mapping between database types and schema IDs | 1. Groups `RawLogEntry` objects by transaction boundaries<br>2. Constructs `ChangeEvent` with before/after images<br>3. Attaches schema version ID to each event<br>4. Performs deduplication based on primary keys<br>5. Orders events within same transaction by operation sequence | `RawLogEntry` (multiple) → `ChangeEvent` |\n| **Event Streamer & Delivery** (Milestone 2) | **The Reliable Postal Service** – ensures change events reach their destination with delivery guarantees and proper ordering. | - Kafka producer instances<br>- Producer configuration and state<br>- Delivery callbacks and error handlers<br>- Backpressure monitoring state | 1. Publishes `ChangeEvent` objects to Kafka topics<br>2. Implements at-least-once delivery semantics<br>3. Enforces ordering per primary key via partitioning<br>4. Monitors consumer lag and applies backpressure<br>5. Handles broker failures and network issues | `ChangeEvent` → Kafka message (persisted) |\n| **Schema Registry** (Milestone 3) | **The Contract Librarian** – maintains the evolving \"data contracts\" between database tables and their consumers. | - Versioned schema definitions<br>- Compatibility rules configuration<br>- Schema change history<br>- Client connection cache | 1. Stores and versions table schema definitions<br>2. Validates schema changes against compatibility rules<br>3. Provides schema lookup by version ID<br>4. Emits schema change notifications<br>5. Handles schema evolution migrations | DDL statement → `SchemaVersion` + compatibility check result |\n| **Source Database** (External) | **The Event Source** – the operational database whose changes we capture. | Transaction logs (WAL/binlog), table schemas, and actual data. | 1. Generates transaction log entries for all data changes<br>2. Provides logical decoding/replication interfaces<br>3. Maintains log retention and rotation policies | Data modifications → Transaction log entries |\n| **Message Broker (Kafka)** (External) | **The Event Highway** – provides durable, ordered, partitioned event storage and distribution. | Topics, partitions, offsets, and consumer groups. | 1. Persists change events with configurable retention<br>2. Distributes events to multiple consumer groups<br>3. Maintains ordering within partitions<br>4. Tracks consumer progress via offsets | Producer messages → Consumer-accessible streams |\n| **Downstream Consumers** (External) | **The Event Destinations** – applications that react to or analyze database changes. | Their own processing state and business logic. | 1. Subscribe to Kafka topics<br>2. Process change events according to their business needs<br>3. Commit offsets to track progress<br>4. Handle schema evolution notifications | Change events → Business actions/analytics |\n\n**Data Flow Through the Pipeline:**\n\nThe end-to-end flow follows this sequence:\n\n1. **Capture**: When a database transaction commits, the database writes corresponding entries to its transaction log (PostgreSQL WAL or MySQL binlog). The Log Connector & Parser continuously reads these entries via the database's logical decoding interface (e.g., PostgreSQL's `pgoutput` or MySQL's binlog replication protocol).\n\n2. **Parse & Translate**: Each binary log entry is parsed into a `RawLogEntry` containing the operation type, table name, affected row data (both old and new values for updates), and the log sequence number (LSN). This parsing is database-specific but produces a standardized intermediate format.\n\n3. **Assemble & Enrich**: The Change Event Builder collects all `RawLogEntry` objects belonging to the same transaction. Once the transaction commit is detected, it assembles complete `ChangeEvent` objects, attaches the appropriate schema version ID by querying the Schema Registry, and ensures deduplication and ordering.\n\n4. **Deliver & Guarantee**: The Event Streamer publishes each `ChangeEvent` to Kafka, using the table name and primary key to determine the target partition. It waits for acknowledgment from Kafka (configured for at-least-once semantics) before marking the event as delivered. If consumers fall behind, backpressure mechanisms slow down or pause the parser.\n\n5. **Evolve & Notify**: When a DDL statement (like `ALTER TABLE`) is detected in the transaction log, the Schema Registry is consulted. The new schema is validated for compatibility, registered with a new version ID, and a special schema change event is emitted to notify consumers.\n\n> **Key Design Insight**: The pipeline is designed with **idempotency** in mind. Each component can be restarted and resume from its last known good state without creating duplicates or losing data. This is achieved through persistent position tracking (LSN offsets), idempotent Kafka producers, and idempotent schema registration.\n\n### 3.2 Recommended Project File Structure\n\nA well-organized codebase is critical for managing the complexity of a CDC system. The following structure separates concerns logically, isolates database-specific code, and creates clear boundaries for testing and deployment. This structure follows Maven/Gradle conventions for Java projects but can be adapted for other languages.\n\n```\ncdc-system/\n├── README.md                           # Project overview and setup instructions\n├── pom.xml (or build.gradle)           # Build configuration and dependencies\n├── config/\n│   ├── application.yaml                # Main configuration file (loaded by ConfigLoader)\n│   ├── logback.xml                     # Logging configuration\n│   └── database-specific/\n│       ├── postgresql-parser.yaml      # PostgreSQL-specific parser settings\n│       └── mysql-parser.yaml           # MySQL-specific parser settings\n├── src/main/java/com/example/cdc/\n│   ├── Main.java                       # Entry point: Main.main(String[])\n│   ├── config/\n│   │   ├── AppConfig.java              # Root configuration class\n│   │   ├── DatabaseConfig.java         # Database connection settings\n│   │   ├── KafkaConfig.java            # Kafka producer settings\n│   │   ├── SchemaConfig.java           # Schema registry settings\n│   │   └── ConfigLoader.java           # Utility: ConfigLoader.loadConfig()\n│   ├── model/                          # Core data types (immutable value objects)\n│   │   ├── RawLogEntry.java            # Parsed log entry\n│   │   ├── ChangeEvent.java            # Final change event\n│   │   ├── SchemaVersion.java          # Schema version metadata\n│   │   └── OperationType.java          # Enum: INSERT, UPDATE, DELETE\n│   ├── connector/                      # Log Connector & Parser component\n│   │   ├── LogConnector.java           # Main interface for log reading\n│   │   ├── state/\n│   │   │   ├── ConnectorState.java     # Enum: IDLE, CONNECTING, READING, etc.\n│   │   │   └── PositionTracker.java    # Manages LSN/offset persistence\n│   │   ├── parser/\n│   │   │   ├── LogParser.java          # Generic parser interface\n│   │   │   ├── postgresql/\n│   │   │   │   ├── PostgresParser.java # PostgreSQL WAL parser implementation\n│   │   │   │   └── PgOutputDecoder.java# Handles pgoutput logical decoding\n│   │   │   └── mysql/\n│   │   │       ├── MySqlParser.java    # MySQL binlog parser implementation\n│   │   │       └── BinlogDecoder.java  # Handles MySQL binlog format\n│   │   └── exception/\n│   │       └── ParseException.java     # Custom exception for parse failures\n│   ├── builder/                        # Change Event Builder component\n│   │   ├── ChangeEventBuilder.java     # Main builder interface\n│   │   ├── TransactionBuffer.java      # Buffers entries per transaction\n│   │   ├── Deduplicator.java           # Removes duplicate events\n│   │   └── EventOrderer.java           # Ensures causal ordering\n│   ├── streamer/                       # Event Streamer & Delivery component\n│   │   ├── EventStreamer.java          # Main streaming interface\n│   │   ├── KafkaEventPublisher.java    # Kafka implementation\n│   │   ├── DeliveryGuarantee.java      # Implements at-least-once logic\n│   │   ├── BackpressureController.java # Monitors lag and applies backpressure\n│   │   └── partitioner/\n│   │       ├── EventPartitioner.java   # Interface for partitioning strategy\n│   │       └── TablePkPartitioner.java # Partitions by table + primary key hash\n│   ├── schema/                         # Schema Registry component\n│   │   ├── SchemaRegistry.java         # Main registry interface\n│   │   ├── InMemorySchemaRegistry.java # Simple in-memory implementation\n│   │   ├── SchemaValidator.java        # Validates compatibility\n│   │   ├── evolution/\n│   │   │   ├── CompatibilityMode.java  # Enum: BACKWARD, FORWARD, FULL\n│   │   │   └── SchemaMigrator.java     # Applies schema migrations\n│   │   └── event/\n│   │       └── SchemaChangeEvent.java  # Special event for schema changes\n│   ├── pipeline/                       # Orchestrates the entire pipeline\n│   │   ├── CdcPipeline.java            # Main pipeline coordinator\n│   │   └── PipelineStateManager.java   # Manages pipeline lifecycle and state\n│   └── util/                           # Shared utilities\n│       ├── JsonSerializer.java         # JSON serialization/deserialization\n│       ├── MetricsCollector.java       # Collects and exposes metrics\n│       └── RetryUtil.java              # Retry logic with exponential backoff\n└── src/test/java/com/example/cdc/      # Comprehensive test suite\n    ├── unit/                           # Unit tests for each component\n    ├── integration/                    # Integration tests with embedded DB/Kafka\n    └── e2e/                            # End-to-end pipeline tests\n```\n\n**Rationale for Key Organizational Decisions:**\n\n1. **Database-Specific Code Isolation**: Placing PostgreSQL and MySQL parsers in separate subdirectories (`connector/parser/postgresql/`, `connector/parser/mysql/`) allows clean separation. New database adapters can be added without modifying core parsing logic.\n\n2. **Configuration Centralization**: The `config/` directory at the root holds all configuration files, with database-specific configurations separated. The `ConfigLoader` utility provides a single way to load any configuration class.\n\n3. **Immutable Model Objects**: All classes in the `model/` package are designed as immutable value objects with clear validation in their constructors. This prevents accidental mutation as events flow through the pipeline.\n\n4. **Component Cohesion**: Each major component (`connector/`, `builder/`, `streamer/`, `schema/`) contains all related classes, including their interfaces, implementations, and supporting utilities. This makes the codebase navigable and enforces architectural boundaries.\n\n5. **Pipeline Orchestration**: The `pipeline/` package contains the high-level coordinator that wires components together and manages the overall lifecycle. This separates coordination logic from component implementation details.\n\n6. **Test Structure Mirroring**: The test directory structure mirrors the main source structure, making it easy to locate tests for any component.\n\n> **Important**: This file structure assumes a single monolithic deployment. For production-scale systems, components like the Schema Registry might be deployed as separate services. The code organization still works well in such cases—each component package would become its own microservice project.\n\n### 3.3 Implementation Guidance\n\nThis section provides concrete starting points for implementing the architectural components described above. Since the core learning value is in implementing the CDC logic itself, we provide complete starter code for infrastructure (configuration, serialization) and skeletons for the core components that you'll fill in.\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option (Getting Started) | Advanced Option (Production) |\n|-----------|---------------------------------|------------------------------|\n| **Log Parsing** | Database-native JDBC + replication protocols (PostgreSQL JDBC with `PGReplicationStream`, MySQL Connector/J with binlog) | Debezium Engine (embedded) for robust, battle-tested parsing |\n| **Event Streaming** | Apache Kafka with plain Java producer | Confluent Platform with KStreams for stream processing, Schema Registry integration |\n| **Schema Registry** | In-memory registry (for development) | Confluent Schema Registry or Apicurio Registry for production |\n| **Serialization** | JSON (simple, human-readable) | Avro with Schema Registry (compact, schema-enforced) |\n| **Metrics** | SLF4J logging | Micrometer/Prometheus metrics with Grafana dashboards |\n| **Configuration** | YAML files with SnakeYAML | Spring Boot Configuration or TypeSafe Config |\n\n#### B. Complete Starter Code for Configuration Loading\n\nSince configuration management is a prerequisite but not the core learning goal, here's complete, working code for the configuration system:\n\n**File: `src/main/java/com/example/cdc/config/ConfigLoader.java`**\n```java\npackage com.example.cdc.config;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.dataformat.yaml.YAMLFactory;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\n\n/**\n * Utility class to load configuration from YAML files.\n * Uses Jackson's YAML parser for deserialization.\n */\npublic class ConfigLoader {\n    private static final Logger LOG = LoggerFactory.getLogger(ConfigLoader.class);\n    private static final ObjectMapper YAML_MAPPER = new ObjectMapper(new YAMLFactory());\n    \n    static {\n        YAML_MAPPER.findAndRegisterModules(); // Handles Java 8 date/time types\n    }\n    \n    /**\n     * Loads configuration from a YAML file on the classpath or filesystem.\n     * \n     * @param configPath Path to the configuration file (e.g., \"config/application.yaml\")\n     * @param configClass The class to deserialize into (e.g., AppConfig.class)\n     * @return Fully populated configuration object\n     * @throws IllegalArgumentException if configuration cannot be loaded\n     */\n    public static <T> T loadConfig(String configPath, Class<T> configClass) {\n        try {\n            // First try classpath (for packaged JARs)\n            InputStream classpathStream = ConfigLoader.class.getClassLoader()\n                    .getResourceAsStream(configPath);\n            if (classpathStream != null) {\n                LOG.info(\"Loading configuration from classpath: {}\", configPath);\n                return YAML_MAPPER.readValue(classpathStream, configClass);\n            }\n            \n            // Fall back to filesystem (for development)\n            File configFile = new File(configPath);\n            if (configFile.exists()) {\n                LOG.info(\"Loading configuration from filesystem: {}\", configPath);\n                return YAML_MAPPER.readValue(configFile, configClass);\n            }\n            \n            throw new IOException(\"Configuration file not found: \" + configPath);\n        } catch (IOException e) {\n            throw new IllegalArgumentException(\"Failed to load configuration from \" + configPath, e);\n        }\n    }\n}\n```\n\n**File: `src/main/java/com/example/cdc/config/AppConfig.java`**\n```java\npackage com.example.cdc.config;\n\n/**\n * Root configuration class containing all subsystem configurations.\n * Maps directly to the top-level YAML structure.\n */\npublic class AppConfig {\n    private DatabaseConfig database;\n    private KafkaConfig kafka;\n    private SchemaConfig schema;\n    private int maxBatchSize = 1000;\n    private long backpressureThresholdMs = 5000;\n    \n    // Getters and setters (required for Jackson deserialization)\n    public DatabaseConfig getDatabase() { return database; }\n    public void setDatabase(DatabaseConfig database) { this.database = database; }\n    \n    public KafkaConfig getKafka() { return kafka; }\n    public void setKafka(KafkaConfig kafka) { this.kafka = kafka; }\n    \n    public SchemaConfig getSchema() { return schema; }\n    public void setSchema(SchemaConfig schema) { this.schema = schema; }\n    \n    public int getMaxBatchSize() { return maxBatchSize; }\n    public void setMaxBatchSize(int maxBatchSize) { this.maxBatchSize = maxBatchSize; }\n    \n    public long getBackpressureThresholdMs() { return backpressureThresholdMs; }\n    public void setBackpressureThresholdMs(long backpressureThresholdMs) { \n        this.backpressureThresholdMs = backpressureThresholdMs; \n    }\n}\n```\n\n**File: `src/main/java/com/example/cdc/config/DatabaseConfig.java`**\n```java\npackage com.example.cdc.config;\n\n/**\n * Database connection and replication configuration.\n */\npublic class DatabaseConfig {\n    private String type; // \"postgresql\" or \"mysql\"\n    private String host;\n    private int port;\n    private String database;\n    private String username;\n    private String password;\n    private String slotName; // PostgreSQL replication slot name\n    \n    // Getters and setters\n    public String getType() { return type; }\n    public void setType(String type) { this.type = type; }\n    \n    public String getHost() { return host; }\n    public void setHost(String host) { this.host = host; }\n    \n    public int getPort() { return port; }\n    public void setPort(int port) { this.port = port; }\n    \n    public String getDatabase() { return database; }\n    public void setDatabase(String database) { this.database = database; }\n    \n    public String getUsername() { return username; }\n    public void setUsername(String username) { this.username = username; }\n    \n    public String getPassword() { return password; }\n    public void setPassword(String password) { this.password = password; }\n    \n    public String getSlotName() { return slotName; }\n    public void setSlotName(String slotName) { this.slotName = slotName; }\n}\n```\n\n**Sample Configuration File: `config/application.yaml`**\n```yaml\ndatabase:\n  type: \"postgresql\"\n  host: \"localhost\"\n  port: 5432\n  database: \"mydb\"\n  username: \"replicator\"\n  password: \"secretpassword\"\n  slotName: \"cdc_slot\"\n\nkafka:\n  bootstrapServers: \"localhost:9092\"\n  topicPrefix: \"cdc.\"\n  replicationFactor: 1\n  partitionsPerTable: 3\n  acks: \"all\"\n  enableIdempotence: true\n\nschema:\n  registryType: \"inmemory\"\n  registryUrl: \"http://localhost:8081\"\n\nmaxBatchSize: 500\nbackpressureThresholdMs: 3000\n```\n\n#### C. Core Component Skeleton Code\n\nHere's the skeleton for the main pipeline coordinator that you'll implement:\n\n**File: `src/main/java/com/example/cdc/pipeline/CdcPipeline.java`**\n```java\npackage com.example.cdc.pipeline;\n\nimport com.example.cdc.config.AppConfig;\nimport com.example.cdc.connector.LogConnector;\nimport com.example.cdc.builder.ChangeEventBuilder;\nimport com.example.cdc.streamer.EventStreamer;\nimport com.example.cdc.schema.SchemaRegistry;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Main pipeline coordinator that wires all components together and manages the\n * end-to-end flow from database logs to Kafka.\n */\npublic class CdcPipeline {\n    private static final Logger LOG = LoggerFactory.getLogger(CdcPipeline.class);\n    \n    private final AppConfig config;\n    private LogConnector logConnector;\n    private ChangeEventBuilder eventBuilder;\n    private EventStreamer eventStreamer;\n    private SchemaRegistry schemaRegistry;\n    private volatile boolean running = false;\n    \n    public CdcPipeline(AppConfig config) {\n        this.config = config;\n    }\n    \n    /**\n     * Initializes all components in the correct order.\n     * 1. Schema Registry (needed for event building)\n     * 2. Event Streamer (needed for backpressure signaling)\n     * 3. Change Event Builder (needs both registry and streamer)\n     * 4. Log Connector (needs builder for callbacks)\n     */\n    public void initialize() {\n        LOG.info(\"Initializing CDC pipeline...\");\n        \n        // TODO 1: Create and initialize SchemaRegistry based on config.getSchema()\n        // TODO 2: Create and initialize EventStreamer based on config.getKafka()\n        // TODO 3: Create ChangeEventBuilder, passing the schema registry\n        // TODO 4: Create LogConnector, passing config.getDatabase() and the event builder\n        // TODO 5: Set up backpressure linkage: streamer -> connector\n        // TODO 6: Register shutdown hook for graceful termination\n        \n        LOG.info(\"CDC pipeline initialized successfully\");\n    }\n    \n    /**\n     * Starts the pipeline and begins processing database changes.\n     * This method should not block; it starts processing in background threads.\n     */\n    public void start() {\n        if (running) {\n            LOG.warn(\"Pipeline already running\");\n            return;\n        }\n        \n        LOG.info(\"Starting CDC pipeline...\");\n        running = true;\n        \n        // TODO 1: Start the event streamer\n        // TODO 2: Start the log connector (begins reading and parsing)\n        // TODO 3: Log startup completion with initial LSN/offset\n        \n        LOG.info(\"CDC pipeline started\");\n    }\n    \n    /**\n     * Stops the pipeline gracefully, ensuring all in-flight events are processed.\n     */\n    public void stop() {\n        if (!running) {\n            return;\n        }\n        \n        LOG.info(\"Stopping CDC pipeline gracefully...\");\n        running = false;\n        \n        // TODO 1: Stop the log connector (flush remaining entries)\n        // TODO 2: Stop the event builder (complete pending transactions)\n        // TODO 3: Stop the event streamer (flush and close Kafka producer)\n        // TODO 4: Stop the schema registry\n        \n        LOG.info(\"CDC pipeline stopped\");\n    }\n    \n    /**\n     * Returns the current health status of the pipeline.\n     */\n    public HealthStatus getHealthStatus() {\n        // TODO: Check each component's health and return aggregated status\n        return running ? HealthStatus.HEALTHY : HealthStatus.STOPPED;\n    }\n    \n    public enum HealthStatus {\n        HEALTHY, DEGRADED, STOPPED\n    }\n}\n```\n\n#### D. Language-Specific Hints for Java\n\n1. **Use try-with-resources** for all database and network connections to ensure proper cleanup.\n2. **Prefer CompletableFuture** for asynchronous operations in the pipeline to avoid blocking threads.\n3. **Use SLF4J with parameterized logging** (`LOG.debug(\"Processing event {}\", eventId)`) instead of string concatenation for performance.\n4. **Make model classes immutable** using `final` fields and builders (Lombok `@Builder` or manual).\n5. **Use Jackson's `ObjectMapper`** for YAML parsing—it's more robust than SnakeYAML for complex object graphs.\n6. **Configure Kafka producer with `enable.idempotence=true`** and `acks=all` for at-least-once semantics.\n7. **Store LSN/offset frequently** (every few seconds or per batch) to minimize replay on restart.\n\n#### E. Milestone Checkpoint for Architecture Implementation\n\nAfter setting up the project structure and configuration:\n\n1. **Run the configuration test** to verify everything loads:\n   ```\n   mvn test -Dtest=ConfigLoaderTest\n   ```\n   Expected: All tests pass, showing configuration loads from YAML correctly.\n\n2. **Verify the pipeline skeleton compiles**:\n   ```\n   mvn clean compile\n   ```\n   Expected: Build succeeds with no compilation errors.\n\n3. **Check the component wiring** by creating a simple main class:\n   ```java\n   public class Main {\n       public static void main(String[] args) {\n           AppConfig config = ConfigLoader.loadConfig(\"config/application.yaml\", AppConfig.class);\n           CdcPipeline pipeline = new CdcPipeline(config);\n           pipeline.initialize();\n           System.out.println(\"Pipeline initialized successfully!\");\n       }\n   }\n   ```\n   Expected: Program runs and prints success message (components will be stubs initially).\n\n**Signs of Trouble:**\n- `ClassNotFoundException` for Jackson YAML: Add `jackson-dataformat-yaml` dependency.\n- Configuration file not found: Ensure `config/application.yaml` is in your classpath or current directory.\n- Null pointer in pipeline: You're missing component initialization—implement the TODOs in `CdcPipeline.initialize()`.\n\n---\n\n\n> **Milestone(s):** Milestone 1 (foundational), Milestone 2 (delivery), Milestone 3 (schema)\n\n## 4. Data Model\n\nThis section defines the fundamental data structures that constitute the **vocabulary** of our CDC system. Just as a postal system needs standardized envelopes, addresses, and tracking numbers to function reliably, our CDC pipeline requires precisely defined data types to ensure consistent communication between components. These types represent the contract that governs how change information is captured, transformed, and delivered throughout the system.\n\n![Data Model Class Diagram](./diagrams/data-model-class.svg)\n\nThe diagram above illustrates the relationships between our core data types. Think of `RawLogEntry` as the **raw material** extracted from the database's transaction log, `ChangeEvent` as the **finished product** packaged for delivery to consumers, and `SchemaVersion` as the **blueprint** that defines how to interpret the data inside each event. The configuration types (`AppConfig`, `DatabaseConfig`, etc.) serve as the **operating instructions** that tell each component how to behave in a specific deployment environment.\n\n### 4.1 Core Type Definitions\n\nThis subsection details the exact structure of each data type that flows through the CDC pipeline. Each type serves a distinct purpose in the transformation journey from low-level database log entries to consumable change notifications.\n\n#### The Foundation: RawLogEntry\n\nThink of a `RawLogEntry` as a **verbatim transcript of a database's internal diary**. When the database records a change in its Write-Ahead Log (WAL) or binlog, it writes in a compact, database-specific binary format optimized for crash recovery, not for external consumption. The `RawLogEntry` is our first translation of that raw binary into a structured, but still database-specific, representation. It contains all the raw data we need, but hasn't yet been normalized into our system's universal format.\n\nThe `RawLogEntry` is the output of the Log Connector & Parser (Milestone 1) and serves as the input to the Change Event Builder. Its fields capture the essential facts from a single log entry:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `logSequenceNumber` | String | The **unique pointer** to this exact position in the transaction log. For PostgreSQL, this is a Log Sequence Number (LSN) like `0/18A3C58`. For MySQL, it's a binlog position and file name. This is the critical bookmark that allows the system to resume processing after a restart without missing or duplicating events. |\n| `databaseType` | String | The **source database flavor** (e.g., `\"POSTGRESQL\"`, `\"MYSQL\"`). This is necessary because the interpretation of the `rowData` map and the exact semantics of the `logSequenceNumber` can vary between databases. The Event Builder uses this to apply any database-specific normalization logic. |\n| `tableName` | String | The fully-qualified name of the database table that was modified (e.g., `\"public.users\"` or `\"inventory.products\"`). This identifies the target of the change. |\n| `operationType` | String | The **type of database operation** that occurred. Must be one of the constants: `OPERATION_INSERT` (\"INSERT\"), `OPERATION_UPDATE` (\"UPDATE\"), or `OPERATION_DELETE` (\"DELETE\"). This tells the Event Builder how to populate the `beforeImage` and `afterImage` in the final `ChangeEvent`. |\n| `rowData` | Map<String, Object> | The **raw column values** as read directly from the log. This is a map where keys are column names and values are the database-native Java objects (e.g., `String`, `Long`, `java.sql.Timestamp`, `byte[]`). For an INSERT, this map contains the new row values. For a DELETE, it contains the old row values. For an UPDATE, the contents depend on the database's log format—it might contain only the new values, only the changed values, or both. The Event Builder's job is to reconcile this into consistent before/after images. |\n| `timestamp` | Long | The **database server's timestamp** (in milliseconds since epoch) when this log entry was written. This is the closest proxy we have to when the change *actually occurred* at the source, which is crucial for maintaining causal ordering and for consumer applications that care about event time. |\n\n> **Design Insight:** The `RawLogEntry` is intentionally *database-aware*. It doesn't try to abstract away database-specific details immediately. This separation of concerns allows the Log Parser to focus solely on the complex task of interpreting native log formats, while the Event Builder handles the normalization. If we later add support for Oracle or SQL Server, we would create new parser implementations that all produce this same `RawLogEntry` type, keeping the rest of the pipeline unchanged.\n\n#### The Deliverable: ChangeEvent\n\nA `ChangeEvent` is the **canonical, packaged notification of a single row change** that our system guarantees to deliver to consumers. If `RawLogEntry` is a raw ingredient, `ChangeEvent` is the plated meal ready for service. It is the output of the Change Event Builder and the input to the Event Streamer (Milestone 2).\n\nThis type embodies the core value proposition of CDC: a reliable, ordered, self-contained record of \"what changed.\" Every field serves a specific purpose for downstream consumers:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `eventId` | String | A **globally unique identifier** for this specific change event, typically a UUID (e.g., `\"123e4567-e89b-12d3-a456-426614174000\"`). This is critical for idempotent processing by consumers. If a consumer receives the same event twice (due to at-least-once delivery), it can use this ID to deduplicate. |\n| `sourceTable` | String | The fully-qualified table name (e.g., `\"public.orders\"`). Carried over from `RawLogEntry` but now in a normalized form (e.g., lowercased). |\n| `operationType` | String | The type of operation (`OPERATION_INSERT`, `OPERATION_UPDATE`, `OPERATION_DELETE`). |\n| `beforeImage` | Map<String, Object> | A **snapshot of the entire row *before* the change**. For an INSERT, this is `null` (or an empty map). For a DELETE, this contains all column values of the deleted row. For an UPDATE, this contains the complete row state prior to the update. This \"full before image\" is essential for many use cases like audit trails, undo operations, or synchronizing to systems that need to know what was removed. |\n| `afterImage` | Map<String, Object> | A **snapshot of the entire row *after* the change**. For an INSERT, this is the new row. For a DELETE, this is `null` (or an empty map). For an UPDATE, this contains the complete row state after the update. Together with `beforeImage`, this gives consumers a complete picture of the change. |\n| `schemaVersionId` | String | A **reference to the exact schema version** that defines the structure of `beforeImage` and `afterImage`. This is typically a compound key like `\"public.orders:v2\"`. Consumers use this ID to fetch the correct `SchemaVersion` from the Schema Registry (Milestone 3) to properly deserialize and interpret the column values. This field is what enables safe schema evolution. |\n| `transactionId` | String | The **database transaction identifier** that grouped this change with others. In PostgreSQL, this might be the XID; in MySQL, it might be the GTID or a logical grouping ID. This allows consumers to reason about atomicity—they know which changes were committed together, which is important for maintaining data consistency in the target system. |\n| `commitTimestamp` | Long | The **exact moment the transaction was committed** on the source database (milliseconds since epoch). This is more authoritative than the log write timestamp in `RawLogEntry` because it reflects the point at which the change became visible to other database sessions. It's the definitive \"event time\" for the change. |\n\n> **Key Behavior:** For UPDATE operations, our system guarantees to provide **both full before and after images**. This is a deliberate design choice that simplifies consumer logic at the cost of slightly larger message sizes. Some CDC systems only send the changed columns (deltas), but that forces consumers to maintain state to reconstruct full rows. Our approach favors completeness and stateless consumption.\n\n#### The Contract: SchemaVersion\n\nThe `SchemaVersion` acts as the **evolving dictionary** that defines how to read the data within a `ChangeEvent`. Imagine you're receiving a document written in a language that occasionally adds new words or changes grammar rules. The `SchemaVersion` is the versioned dictionary you consult to understand each document correctly. It is stored and managed by the Schema Registry component (Milestone 3).\n\nEach version captures the exact column structure of a table at a point in time:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `schemaId` | String | A **unique, stable identifier** for this specific schema version. Usually follows a pattern like `\"{tableName}:v{version}\"` (e.g., `\"public.users:v3\"`). This is the value that appears in the `ChangeEvent.schemaVersionId` field. |\n| `tableName` | String | The fully-qualified table name this schema defines. |\n| `version` | Integer | A monotonically increasing integer (starting at 1) representing the version of the schema for this table. Increments each time a backward-compatible change is made. |\n| `columnDefinitions` | Map<String, ColumnType> | The **complete column specification**. The key is the column name; the value is a `ColumnType` object (a custom type we define) that includes the Java/SQL type (e.g., `VARCHAR`, `BIGINT`), nullability, default value, and any type-specific parameters (like length for strings, precision for decimals). This map defines the contract for the keys and value types in the `ChangeEvent`'s `beforeImage` and `afterImage` maps. |\n| `compatibilityMode` | String | The **compatibility rule** that was in effect when this schema version was registered. Common values: `\"BACKWARD\"` (default—new schema can read data written by old schema), `\"FORWARD\"` (old schema can read data written by new schema), `\"FULL\"` (both). This is stored for audit and to understand the evolution constraints applied. |\n\n**About ColumnType:** While not in the explicit naming conventions, `ColumnType` is a necessary auxiliary type we must define. It's a simple value object with fields like `sqlType` (String), `javaClass` (Class<?>), `nullable` (boolean), and `defaultValue` (Object). It allows the Schema Registry to perform precise compatibility checks (e.g., \"can data of type `BIGINT` be safely read as `INTEGER`?\").\n\n#### The Configuration: AppConfig and Sub-Configs\n\nConfiguration types are the **deployment manifests** that parameterize the CDC pipeline for a specific environment. They tell each component *where* to connect, *how* to behave, and *what* limits to respect. We use a hierarchical structure where `AppConfig` is the root, containing nested configurations for each major subsystem.\n\n**DatabaseConfig** — Directs the Log Connector:\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `type` | String | Database type: `\"POSTGRESQL\"` or `\"MYSQL\"`. Determines which parser implementation to use. |\n| `host`, `port` | String, int | Network location of the source database. |\n| `database`, `username`, `password` | String | Credentials and target database name. |\n| `slotName` | String | (PostgreSQL-specific) The logical replication slot name. Essential for persistent WAL tracking. |\n\n**KafkaConfig** — Controls the Event Streamer:\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `bootstrapServers` | String | Comma-separated list of Kafka broker addresses (e.g., `\"broker1:9092,broker2:9092\"`). |\n| `topicPrefix` | String | Prefix for auto-created topics (e.g., `\"cdc.\"` results in topics like `cdc.public.users`). |\n| `replicationFactor`, `partitionsPerTable` | int | Topic creation parameters controlling durability and parallelism. |\n| `acks` | String | Producer acknowledgment setting: `\"all\"` for strongest durability (required for at-least-once). |\n| `enableIdempotence` | boolean | Must be `true` to prevent duplicate message production on retries. |\n\n**SchemaConfig** — Guides Schema Registry integration:\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `registryType` | String | Registry implementation: `\"MEMORY\"` for simple testing, `\"KAFKA\"` for Confluent Schema Registry, or `\"CUSTOM\"`. |\n| `registryUrl` | String | The HTTP endpoint of the schema registry service. |\n\n**AppConfig** — The root container:\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `database` | DatabaseConfig | Database connection parameters. |\n| `kafka` | KafkaConfig | Message broker parameters. |\n| `schema` | SchemaConfig | Schema registry parameters. |\n| `maxBatchSize` | int | Maximum number of `ChangeEvent`s to batch before publishing to Kafka. Balances latency and throughput. |\n| `backpressureThresholdMs` | long | If consumer lag exceeds this many milliseconds, the pipeline will pause reading from the database log to apply backpressure. |\n\n#### The Health Indicator: HealthStatus Enum\n\nThis simple enum provides a **standardized status signal** for the entire pipeline, used by monitoring systems and orchestration tools (like Kubernetes).\n\n| Constant | Description |\n|----------|-------------|\n| `HEALTHY` | All components are operating normally, connected to their respective sources (database, Kafka), and processing events. |\n| `DEGRADED` | The pipeline is running but experiencing issues—for example, the database connection is intermittent, consumer lag is above threshold, or the schema registry is unreachable. Events may still be flowing, but the system requires attention. |\n| `STOPPED` | The pipeline is not processing events, either due to a fatal error, a manual stop, or a failure to initialize. |\n\n### 4.2 Serialization Format for Events\n\nSerialization is the process of converting our in-memory `ChangeEvent` objects into a byte stream for transmission over the network (to Kafka) and for persistent storage. The choice of serialization format is a critical architectural decision with profound implications for performance, compatibility, and system evolution.\n\n> **Mental Model:** Think of serialization as the **packaging material** for your shipped product. You could use bulky, human-readable cardboard (JSON), compact and efficient but rigid plastic (Java Serialization), or a smart, self-describing vacuum-sealed bag (Avro/Protobuf). The right packaging protects the contents, minimizes space, and includes a label that explains what's inside even if the product design changes slightly.\n\n#### ADR: Choosing a Serialization Format for Change Events\n\n**Decision: Use Apache Avro as the primary serialization format for `ChangeEvent`s, with JSON as a fallback for debugging and simple deployments.**\n\n- **Context:** `ChangeEvent`s must be serialized for publishing to Kafka and for consumers to read. The format must support high throughput (low CPU/memory overhead), compact representation (to reduce network and storage costs), and most importantly, **robust schema evolution** to handle database schema changes without breaking existing consumers.\n\n- **Options Considered:**\n  1. **JSON (Plain Text):** Human-readable, language-agnostic, and simple to implement. No schema required for deserialization.\n  2. **Java Serialization:** Built into Java, but produces bulky output, is Java-specific, and handles schema evolution poorly.\n  3. **Protocol Buffers (Protobuf):** Binary, efficient, and supports backward/forward compatibility through field numbers. Requires `.proto` schema definitions.\n  4. **Apache Avro:** Binary, highly efficient, uses JSON for schema definition, and has excellent schema evolution capabilities with a centralized **Schema Registry** pattern. Supports \"schema-less\" reading if the reader's schema is provided.\n\n- **Decision Rationale:**\n  Avro is chosen because its design aligns perfectly with the requirements of a CDC system. Its **schema evolution rules** are well-defined and match the compatibility modes (BACKWARD, FORWARD, FULL) we need for Milestone 3. The integration with a Schema Registry (where schemas are stored by ID) means the serialized data is incredibly compact—it only includes the data bytes, not field names or metadata. Consumers fetch the schema from the registry using the `schemaVersionId` included in the message (or in message headers). This enables:\n  - **Backward Compatibility:** Adding a nullable column to a table generates a new Avro schema. Old consumers (with the old schema) can still read new events because Avro ignores the unknown new field.\n  - **Forward Compatibility:** Removing a column is trickier, but if done with a default value in the schema, new consumers can read old data.\n  - **Efficiency:** Avro binary encoding is significantly smaller and faster to parse than JSON.\n  - **Ecosystem Integration:** Kafka's Confluent Platform has first-class Avro and Schema Registry support, which simplifies operational tooling.\n\n- **Consequences:**\n  - We must integrate and operate a Schema Registry service (an additional component).\n  - Consumers must use Avro deserializers and have network access to the registry (or cache schemas locally).\n  - Debugging requires tooling to decode Avro binaries (though we can also produce JSON for debugging topics).\n\n**Comparison of Serialization Options:**\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| **JSON** | Human-readable, universal support, no schema needed, easy debugging. | Verbose (high overhead), no built-in schema evolution support, slower parsing. | The overhead and lack of robust schema evolution are disqualifying for a high-volume CDC system. |\n| **Java Serialization** | Built-in, requires no configuration. | Java-only, extremely verbose, fragile to class changes, terrible performance. | Not cross-platform and performs poorly. |\n| **Protocol Buffers** | Efficient, robust cross-language support, good backward compatibility. | Forward compatibility less straightforward than Avro, requires code generation, `.proto` files must be distributed. | Slightly less ideal for the \"schema registry\" pattern where the schema is fetched at runtime. Avro's dynamic deserialization fits better. |\n| **Apache Avro** | Excellent schema evolution, very efficient, designed for Schema Registry pattern, dynamic deserialization possible. | Requires a Schema Registry, slightly steeper learning curve. | **CHOSEN:** Best aligns with our need for schema evolution and operational integration with Kafka. |\n\n#### Serialized Message Structure\n\nWhen a `ChangeEvent` is published to Kafka, it is wrapped in a **envelope structure** that includes metadata alongside the actual Avro-encoded payload. This envelope is what the Kafka producer sends and the consumer receives.\n\nThe following table describes the structure of the Kafka message value (the main payload):\n\n| Field in Serialized Form | Avro Type | Description |\n|--------------------------|-----------|-------------|\n| `schemaId` | string | The `schemaVersionId` (e.g., `\"public.orders:v2\"`). This may alternatively be placed in the Kafka message *headers* (as `\"schema.id\"`) for even more efficient consumer deserialization, where the deserializer fetches the schema by this ID. |\n| `eventId` | string | UUID of the event, for deduplication. |\n| `sourceTable` | string | Source table name. |\n| `operationType` | string | \"INSERT\", \"UPDATE\", \"DELETE\". |\n| `beforeImage` | map<string, bytes> | **Avro-encoded map.** The keys are column names. Each value is the **Avro-encoded bytes** of that column's value, according to the column's type in the schema. This nested encoding allows each field to be typed precisely. Alternatively, the entire `beforeImage` could be a single Avro record; the map approach is more flexible for sparse updates (though we always send full images). |\n| `afterImage` | map<string, bytes> | Same as `beforeImage`, for the after state. |\n| `transactionId` | string | Database transaction identifier. |\n| `commitTimestamp` | long | Commit timestamp in milliseconds. |\n\n**Alternative: Embedded Schema Approach:** For simplicity in early milestones or testing, we can use a **JSON serialization with an embedded schema version**. The message would be a JSON object containing all `ChangeEvent` fields, with `beforeImage` and `afterImage` as JSON objects. The `schemaVersionId` would be a field within the JSON. This is less efficient but removes the Schema Registry dependency for initial development.\n\nWe will support both modes via configuration: an `AvroWithRegistry` mode and a `JsonEmbedded` mode. The `JsonEmbedded` mode is useful for Milestone 1 and 2 completion before implementing the full Schema Registry in Milestone 3.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option (for Milestone 1/2) | Advanced Option (for Milestone 3 & Production) |\n|-----------|-----------------------------------|-----------------------------------------------|\n| **Serialization** | JSON using Jackson library. Embed `schemaVersionId` in the JSON structure. | Apache Avro with Confluent Schema Registry. Use `KafkaAvroSerializer` and `KafkaAvroDeserializer`. |\n| **Schema Registry** | In-memory map storing `SchemaVersion` objects. No persistence. | Confluent Schema Registry (HTTP service) or Apicurio Registry. Provides persistence, REST API, and compatibility enforcement. |\n| **Configuration** | YAML file parsed with Jackson `ObjectMapper`. | YAML file with environment variable overrides (using `${ENV_VAR}` substitution) and validation with Java Bean Validation. |\n\n#### B. Recommended File Structure\n\nPlace data model types in a dedicated package to keep them organized and separate from component logic.\n\n```\ncdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── config/\n│   │   ├── AppConfig.java          ← Root configuration\n│   │   ├── DatabaseConfig.java\n│   │   ├── KafkaConfig.java\n│   │   ├── SchemaConfig.java\n│   │   └── ConfigLoader.java       ← loadConfig method\n│   ├── model/                      ← Core data types\n│   │   ├── RawLogEntry.java\n│   │   ├── ChangeEvent.java\n│   │   ├── SchemaVersion.java\n│   │   ├── ColumnType.java         ← Auxiliary type\n│   │   └── HealthStatus.java       ← Enum\n│   ├── pipeline/\n│   │   └── CdcPipeline.java        ← Main orchestrator class\n│   └── Main.java                   ← main method\n├── config/\n│   └── cdc-config.yaml             ← YAML configuration file\n└── pom.xml or build.gradle\n```\n\n#### C. Infrastructure Starter Code\n\nHere is a complete, ready-to-use implementation of the data model types and configuration loader. This is foundational code that learners can use directly.\n\n**ColumnType.java (auxiliary type for SchemaVersion):**\n```java\npackage com.cdc.model;\n\nimport java.util.Objects;\n\n/**\n * Represents the type and constraints of a single database column.\n * Used within SchemaVersion.columnDefinitions.\n */\npublic class ColumnType {\n    private final String sqlType;        // e.g., \"VARCHAR\", \"BIGINT\", \"TIMESTAMP\"\n    private final Class<?> javaClass;    // e.g., String.class, Long.class\n    private final boolean nullable;\n    private final Object defaultValue;   // Can be null\n\n    public ColumnType(String sqlType, Class<?> javaClass, boolean nullable, Object defaultValue) {\n        this.sqlType = sqlType;\n        this.javaClass = javaClass;\n        this.nullable = nullable;\n        this.defaultValue = defaultValue;\n    }\n\n    // Getters, equals, hashCode, toString omitted for brevity but should be implemented.\n}\n```\n\n**RawLogEntry.java:**\n```java\npackage com.cdc.model;\n\nimport java.util.Map;\nimport java.util.HashMap;\n\n/**\n * Represents a single parsed entry from a database transaction log.\n * This is a raw, database-specific representation before normalization.\n */\npublic class RawLogEntry {\n    private final String logSequenceNumber;\n    private final String databaseType;\n    private final String tableName;\n    private final String operationType;  // Use constants OPERATION_INSERT, etc.\n    private final Map<String, Object> rowData;\n    private final long timestamp;\n\n    public RawLogEntry(String logSequenceNumber, String databaseType, String tableName,\n                       String operationType, Map<String, Object> rowData, long timestamp) {\n        this.logSequenceNumber = logSequenceNumber;\n        this.databaseType = databaseType;\n        this.tableName = tableName;\n        this.operationType = operationType;\n        this.rowData = new HashMap<>(rowData); // Defensive copy\n        this.timestamp = timestamp;\n    }\n\n    // Getters, equals, hashCode, toString omitted for brevity.\n}\n```\n\n**ChangeEvent.java:**\n```java\npackage com.cdc.model;\n\nimport java.util.Map;\nimport java.util.HashMap;\n\n/**\n * The canonical representation of a single row change, ready for delivery to consumers.\n */\npublic class ChangeEvent {\n    private final String eventId;\n    private final String sourceTable;\n    private final String operationType;\n    private final Map<String, Object> beforeImage;\n    private final Map<String, Object> afterImage;\n    private final String schemaVersionId;\n    private final String transactionId;\n    private final long commitTimestamp;\n\n    public ChangeEvent(String eventId, String sourceTable, String operationType,\n                       Map<String, Object> beforeImage, Map<String, Object> afterImage,\n                       String schemaVersionId, String transactionId, long commitTimestamp) {\n        this.eventId = eventId;\n        this.sourceTable = sourceTable;\n        this.operationType = operationType;\n        this.beforeImage = beforeImage == null ? null : new HashMap<>(beforeImage);\n        this.afterImage = afterImage == null ? null : new HashMap<>(afterImage);\n        this.schemaVersionId = schemaVersionId;\n        this.transactionId = transactionId;\n        this.commitTimestamp = commitTimestamp;\n    }\n\n    // Getters, equals, hashCode, toString omitted.\n}\n```\n\n**SchemaVersion.java:**\n```java\npackage com.cdc.model;\n\nimport java.util.Map;\nimport java.util.HashMap;\n\n/**\n * Represents a versioned schema for a database table.\n */\npublic class SchemaVersion {\n    private final String schemaId;\n    private final String tableName;\n    private final int version;\n    private final Map<String, ColumnType> columnDefinitions;\n    private final String compatibilityMode;\n\n    public SchemaVersion(String schemaId, String tableName, int version,\n                         Map<String, ColumnType> columnDefinitions, String compatibilityMode) {\n        this.schemaId = schemaId;\n        this.tableName = tableName;\n        this.version = version;\n        this.columnDefinitions = new HashMap<>(columnDefinitions);\n        this.compatibilityMode = compatibilityMode;\n    }\n\n    // Getters, equals, hashCode, toString omitted.\n}\n```\n\n**Configuration Classes (AppConfig.java as example):**\n```java\npackage com.cdc.config;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\n\npublic class AppConfig {\n    private DatabaseConfig database;\n    private KafkaConfig kafka;\n    private SchemaConfig schema;\n    private int maxBatchSize = 1000;\n    private long backpressureThresholdMs = 5000;\n\n    // No-arg constructor for Jackson\n    public AppConfig() {}\n\n    // Getters and setters for each field (required for Jackson binding).\n    public DatabaseConfig getDatabase() { return database; }\n    public void setDatabase(DatabaseConfig database) { this.database = database; }\n    // ... same for other fields\n}\n```\n\n**ConfigLoader.java (complete utility):**\n```java\npackage com.cdc.config;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.dataformat.yaml.YAMLFactory;\nimport java.io.File;\nimport java.io.IOException;\n\npublic class ConfigLoader {\n    private static final ObjectMapper MAPPER = new ObjectMapper(new YAMLFactory());\n\n    /**\n     * Loads configuration from a YAML file.\n     * @param configPath Path to the YAML file.\n     * @param clazz The configuration class (e.g., AppConfig.class).\n     * @return The populated configuration object.\n     * @throws IOException If the file cannot be read or parsed.\n     */\n    public static <T> T loadConfig(String configPath, Class<T> clazz) throws IOException {\n        File file = new File(configPath);\n        if (!file.exists()) {\n            throw new IOException(\"Configuration file not found: \" + configPath);\n        }\n        return MAPPER.readValue(file, clazz);\n    }\n}\n```\n\n#### D. Core Logic Skeleton Code\n\nFor the **serialization logic** that learners will implement in later milestones, here is a skeleton interface. The actual Avro/JSON implementation will be filled in during the Event Streamer and Schema Registry components.\n\n**EventSerializer.java (interface to be implemented):**\n```java\npackage com.cdc.serialization;\n\nimport com.cdc.model.ChangeEvent;\nimport com.cdc.model.SchemaVersion;\n\n/**\n * Defines how to serialize and deserialize ChangeEvent objects.\n */\npublic interface EventSerializer {\n    /**\n     * Serializes a ChangeEvent into bytes for transmission.\n     * @param event The event to serialize.\n     * @param schema The SchemaVersion that describes the event's structure.\n     * @return Byte array representing the serialized event.\n     */\n    byte[] serialize(ChangeEvent event, SchemaVersion schema);\n\n    /**\n     * Deserializes bytes back into a ChangeEvent.\n     * @param data The serialized byte array.\n     * @param schema The SchemaVersion to use for deserialization.\n     * @return The reconstructed ChangeEvent.\n     */\n    ChangeEvent deserialize(byte[] data, SchemaVersion schema);\n}\n```\n\n**JsonEventSerializer.java (simple implementation for early milestones):**\n```java\npackage com.cdc.serialization;\n\nimport com.cdc.model.ChangeEvent;\nimport com.cdc.model.SchemaVersion;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport java.io.IOException;\n\n/**\n * A simple JSON serializer for testing and early development.\n * WARNING: This does not handle schema evolution properly.\n */\npublic class JsonEventSerializer implements EventSerializer {\n    private final ObjectMapper objectMapper = new ObjectMapper();\n\n    @Override\n    public byte[] serialize(ChangeEvent event, SchemaVersion schema) {\n        try {\n            // TODO 1: Convert the ChangeEvent to a JSON byte array using objectMapper.writeValueAsBytes\n            // TODO 2: Consider including the schemaVersionId as a field in the JSON if not already present\n            // TODO 3: Return the byte array\n            return new byte[0]; // Replace with actual implementation\n        } catch (Exception e) {\n            throw new RuntimeException(\"Failed to serialize ChangeEvent\", e);\n        }\n    }\n\n    @Override\n    public ChangeEvent deserialize(byte[] data, SchemaVersion schema) {\n        try {\n            // TODO 1: Use objectMapper.readValue to parse the byte array into a ChangeEvent object\n            // TODO 2: Validate that the event's schemaVersionId matches the provided schema's ID\n            // TODO 3: Return the ChangeEvent\n            return null; // Replace with actual implementation\n        } catch (IOException e) {\n            throw new RuntimeException(\"Failed to deserialize ChangeEvent\", e);\n        }\n    }\n}\n```\n\n#### E. Language-Specific Hints (Java)\n- Use **Jackson** (`com.fasterxml.jackson`) for YAML and JSON processing. Include `jackson-databind` and `jackson-dataformat-yaml` dependencies.\n- For **Avro**, use `org.apache.avro` and `io.confluent:kafka-avro-serializer` if using Confluent's Schema Registry.\n- Always make data model classes **immutable** (final fields, no setters) where possible. This prevents accidental mutation as events flow through the pipeline.\n- Implement `equals()`, `hashCode()`, and `toString()` for all model classes—this greatly aids debugging and testing.\n- For the `rowData` and image maps, use **defensive copying** in constructors and getters to prevent external modification of internal state.\n\n#### F. Milestone Checkpoint\nAfter implementing the data model classes and configuration loader:\n1. **Compilation:** Run `mvn compile` or `gradle compileJava`. It should succeed without errors.\n2. **Unit Test:** Write a simple test in `src/test/java` that:\n   - Creates a `RawLogEntry` with sample data.\n   - Creates a `ChangeEvent` from that raw entry (manually, for now).\n   - Serializes the `ChangeEvent` to JSON using the `JsonEventSerializer`.\n   - Deserializes it back and asserts equality.\n3. **Configuration Loading:** Write a test that loads a sample `cdc-config.yaml` file using `ConfigLoader.loadConfig` and asserts the values are correctly populated into `AppConfig`.\n4. **Expected Behavior:** You should be able to run these tests and see them pass. This confirms your data structures are correctly defined and can be serialized, which is foundational for all subsequent milestones.\n\n#### G. Debugging Tips\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| `NullPointerException` when accessing map fields in `ChangeEvent`. | The `beforeImage` or `afterImage` map is null for INSERT/DELETE operations, and code doesn't handle null. | Check the operation type and ensure your logic handles null images appropriately. | Use `Optional.ofNullable(event.getBeforeImage())` or explicit null checks before iterating. |\n| Configuration values are not loaded (remain default). | YAML file field names don't match Java bean property names, or file is not in the expected path. | Print the loaded `AppConfig` object. Check that YAML keys match the Java field names (case-sensitive). | Ensure YAML uses `camelCase` keys matching the Java getter/setter names (e.g., `maxBatchSize`). |\n| Serialized JSON is extremely large. | The `rowData` or image maps contain large binary objects (e.g., `byte[]` of a BLOB column). | Inspect the JSON output; look for base64-encoded strings (Jackson's default for `byte[]`). | Consider excluding large binary columns from CDC or using a binary serialization format (Avro) which is more efficient. |\n| `ClassCastException` when putting values into `rowData` map. | Database driver returns a database-specific type (e.g., `PGobject`) that cannot be cast to expected Java type. | Log the actual class of the value from the database log parser. | Convert database-specific types to standard Java types in the Log Parser (e.g., convert PostgreSQL `PGobject` to `String` for JSONB). |\n\n\n> **Milestone(s):** Milestone 1 (Log Parsing & Change Events)\n\n## 5. Component: Log Connector & Parser\n\nThis component is the **foundational data ingestion layer** of the CDC pipeline. Its sole responsibility is to establish a persistent, stateful connection to the source database's transaction log and translate raw, binary log entries into structured `RawLogEntry` objects. It is the \"eyes\" of the CDC system, continuously watching the database's internal journal of changes. Think of this component as a specialized, low-level translator that understands the database's private, binary language (the Write-Ahead Log or binlog) and converts it into a standardized internal dialect that the rest of the system can understand.\n\n### 5.1 Mental Model: The Database Translator\n\nImagine you're tasked with translating a live, never-ending speech given in a complex, technical dialect (PostgreSQL's WAL or MySQL's binlog) into plain English for a group of reporters (downstream components). The speech is delivered at high speed, sometimes with interruptions and corrections, and you must keep your place meticulously. You have two main tools: a **dictionary** (the parser's logic) to translate each word and sentence structure, and a **bookmark** (the log sequence number) to note exactly where you stopped reading, so you can resume seamlessly after a coffee break (a system restart). This translator must be fluent in multiple dialects (different database engines) but maintain the same output format.\n\nMore technically, the component acts as a **stateful, pull-based iterator** over the database's logical replication stream. It does not interpret transaction boundaries or deduplicate entries—it simply provides a reliable stream of \"raw facts\" about individual row-level changes as they are durably recorded by the database engine itself. Its core challenge is dealing with the **opaque, vendor-specific, and version-dependent** binary formats while maintaining a persistent read position that survives crashes.\n\n### 5.2 Interface and State\n\nThe Log Connector & Parser exposes a simple, iterator-like interface for the downstream Change Event Builder to consume raw log entries. It manages significant internal state to track its position and connection health.\n\n#### Interface\n\nThe primary interaction is through a `LogConnector` interface that provides methods for lifecycle management and data retrieval.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `initialize` | `AppConfig` | `void` | Prepares the connector using application configuration. Establishes the initial connection or replication slot and sets the starting LSN from persisted state. |\n| `start` | — | `void` | Begins the continuous reading and parsing loop. This is typically a blocking or long-running operation that publishes parsed `RawLogEntry` objects to an internal buffer or queue. |\n| `stop` | — | `void` | Gracefully stops the reading loop, closes the database connection, and ensures the last processed LSN is safely persisted. |\n| `getNextBatch` | `int maxBatchSize` | `List<RawLogEntry>` | **Primary consumption method.** Fetches the next batch of parsed log entries from an internal buffer. Blocks if the buffer is empty and the connector is still running. Used by the Event Builder. |\n| `getLastProcessedLSN` | — | `String` | Returns the most recent Log Sequence Number that has been *successfully parsed and made available* via `getNextBatch`. This is the position that would be used for recovery. |\n| `getHealthStatus` | — | `HealthStatus` | Reports the current health: `HEALTHY` (reading), `DEGRADED` (connected but lagging, or buffer full), `STOPPED` (disconnected or error). |\n\n#### Core Data Structure: `RawLogEntry`\n\nThis is the standardized output of the parser. It represents a single, atomic change to a row, extracted directly from the transaction log.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `logSequenceNumber` | `String` | **The critical bookmark.** The database-specific, monotonically increasing identifier for this exact position in the transaction log (e.g., PostgreSQL LSN like `0/15E8E10`, MySQL binlog position `471`). This must be persisted for recovery. |\n| `databaseType` | `String` | Identifies the source database engine (e.g., `\"POSTGRESQL\"`, `\"MYSQL\"`). Needed because downstream logic may handle certain edge cases differently per database. |\n| `tableName` | `String` | The fully-qualified name of the table that was modified (e.g., `\"public.users\"`). |\n| `operationType` | `String` | The type of operation. Must be one of the constants: `OPERATION_INSERT`, `OPERATION_UPDATE`, or `OPERATION_DELETE`. |\n| `rowData` | `Map<String, Object>` | A map representing the **full row** involved in the change. For `INSERT`, this is the new row values. For `DELETE`, this is the old row values. For `UPDATE`, this map contains the **new values (after-image)**. The parser extracts this directly from the log, with column names as keys and Java-object representations of the column values. |\n| `timestamp` | `Long` | The database server's timestamp (epoch milliseconds) when this specific change was written to the transaction log. This is often the commit time, but may be the statement execution time depending on the database. |\n\n#### Internal State\n\nThe component maintains several key pieces of state, which can be visualized in the state machine diagram:\n![Log Parser State Machine](./diagrams/parser-state-machine.svg)\n\n| State Variable | Type | Purpose | Persistence |\n|----------------|------|---------|-------------|\n| `currentLSN` | `String` | The LSN of the last log entry that was *read from the source*. This advances as we read, even before parsing is complete. | Volatile (in-memory). |\n| `lastProcessedLSN` | `String` | The LSN of the last entry *successfully parsed and published* to the internal buffer (via `getNextBatch`). This is the recovery point. | **Must be durably stored** (e.g., in a Kafka topic `__cdc_offsets`, or a local file). |\n| `replicationSlotName` | `String` | (PostgreSQL-specific) The name of the logical replication slot. This ensures the database retains WAL segments until we have consumed them. | Stored in configuration and in the database system catalog. |\n| `connectionHandle` | `Object` (DB-specific) | The active connection or session to the database's log streaming interface (e.g., PostgreSQL's `PGReplicationConnection`, MySQL's `BinlogClient`). | Volatile. |\n| `internalBuffer` | `BlockingQueue<RawLogEntry>` | A bounded queue that holds parsed entries waiting to be consumed by `getNextBatch`. Its size is a key backpressure control. | Volatile. |\n| `state` | `Enum` | The current state of the connector (e.g., `IDLE`, `CONNECTING`, `READING_LOG`, `PAUSED`, `ERROR`). | Volatile. |\n\n### 5.3 Internal Behavior and Algorithm\n\nThe core of the parser is a continuous loop that runs after `start()` is called. Here is the step-by-step algorithm, which is implemented in the `start` method or a dedicated background thread it launches.\n\n1.  **Initialization & Recovery**: Load the `lastProcessedLSN` from the persistent offset store. This is the LSN from which to start (or resume) reading. If no offset exists (first run), start from the current database's LSN or from a specific timestamp defined in the configuration.\n2.  **Establish Connection**: Open a low-level, streaming connection to the database's logical replication interface. For PostgreSQL, this involves creating a replication connection and starting logical decoding for the chosen slot. For MySQL, it involves connecting as a replica and requesting binlog events starting from the recovered position.\n3.  **Enter Main Loop**: While the connector state is `READING_LOG`, continuously perform the following sub-steps:\n    1.  **Read Raw Log Chunk**: Blockingly read the next chunk of bytes from the database stream. The database will send data as it's written to the log. Handle network timeouts and keep-alives.\n    2.  **Parse Chunk into Logical Updates**: Pass the binary chunk to a database-specific parser. This parser understands the log's format (e.g., PostgreSQL's XLog, MySQL's Binlog Event) and breaks it into a sequence of logical row change events. This step is complex and vendor-specific.\n    3.  **Iterate Over Parsed Events**: For each logical row change event within the chunk:\n        a.  **Extract Metadata**: Identify the table, operation type (`INSERT`/`UPDATE`/`DELETE`), and the LSN for this specific event.\n        b.  **Extract Row Data**: Deserialize the column values from the log's binary format into Java objects (e.g., `Integer`, `String`, `java.sql.Timestamp`). Build the `rowData` map.\n        c.  **Construct RawLogEntry**: Create a `RawLogEntry` object with the extracted data, the current `databaseType`, and the server `timestamp`.\n        d.  **Update currentLSN**: Set `currentLSN` to the event's LSN.\n        e.  **Apply Backpressure Check**: Check if the `internalBuffer` has available capacity. If it's full, transition to the `PAUSED` state, which should pause reading from the database stream (often by stopping the read loop or not consuming the stream), applying backpressure at the source.\n        f.  **Buffer Entry**: Place the constructed `RawLogEntry` into the `internalBuffer`.\n    4.  **Advance Persistent Offset**: After a configured batch size or time interval, or when the buffer is flushed to consumers, update the `lastProcessedLSN` in the durable store to the `currentLSN`. This is a critical durability operation—it should happen *after* entries are in the buffer but *before* acknowledging to the database that they are consumed (if the database protocol requires it, like PostgreSQL's `confirmReceive`).\n4.  **Handle Stop Signal**: When `stop()` is called, set state to `STOPPING`, flush any remaining parsed entries, perform a final update of `lastProcessedLSN`, close the database connection gracefully, and update state to `STOPPED`.\n5.  **Error Handling**: If a parse error, network failure, or database disconnection occurs, transition to the `ERROR` state. Log the error comprehensively (including the problematic LSN and raw bytes if possible). The recovery strategy (see Section 10) should attempt to reconnect from the last successfully processed LSN.\n\n> **Key Design Insight:** The parser's main loop is deliberately simple—it transforms binary data to structured data. It does **not** handle transaction grouping, deduplication, or schema matching. Those responsibilities belong to the downstream Change Event Builder. This separation of concerns keeps the parsing component focused, fast, and resilient to changes in business logic.\n\n### 5.4 ADR: Database-Specific vs. Generic Parser\n\n> **Decision: Use a Pluggable Parser Architecture with Database-Specific Implementations**\n\n*   **Context**: Our CDC system must support multiple database engines (PostgreSQL, MySQL), each with a completely different, complex, and evolving binary log format. We need to decide how to structure the parsing logic to manage this complexity while maintaining a clean codebase.\n*   **Options Considered**:\n    1.  **Monolithic Generic Parser**: A single parser module that contains conditional logic (`if databaseType == POSTGRESQL`) to handle all supported formats.\n    2.  **Pluggable Parser Interface**: Define a common `LogParser` interface (e.g., `parseChunk(byte[]): List<RawLogEntry>`, `getRequiredConfig()`). Implement this interface in separate, isolated modules for each database (e.g., `PostgresWalParser`, `MysqlBinlogParser`). The main `LogConnector` selects and loads the appropriate implementation at runtime based on configuration.\n*   **Decision**: We chose **Option 2: Pluggable Parser Interface**.\n*   **Rationale**:\n    *   **Separation of Concerns**: Each database's parsing logic is isolated, making the code easier to understand, test, and maintain. Changes to MySQL's binlog format only affect the `MysqlBinlogParser` class.\n    *   **Independent Evolution**: New database support (e.g., Oracle, SQL Server) can be added by implementing a new plugin without touching any core connector code. This aligns with the Open/Closed Principle.\n    *   **Runtime Flexibility**: The correct parser is loaded based on `DatabaseConfig.type`, allowing a single binary to support multiple source databases.\n    *   **Mitigates Complexity**: The binary formats are so divergent that a generic parser would become a tangled, unmaintainable \"big ball of mud.\" A pluggable architecture forces clean abstraction boundaries.\n*   **Consequences**:\n    *   **Enables**: Clean code structure, easier testing, and straightforward addition of new database vendors.\n    *   **Introduces**: Additional overhead of defining and maintaining an interface. Requires a mechanism to discover/load plugins (can be simple classpath scanning or Spring-style dependency injection).\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Monolithic Generic Parser | Slightly simpler initial structure; no plugin loading logic. | Code becomes highly complex and entangled; a change for one database risks breaking another; difficult to test in isolation. | ❌ |\n| **Pluggable Parser Interface** | **Clean separation; easier testing and maintenance; scalable to new databases.** | Requires careful interface design and a plugin loading mechanism. | ✅ |\n\n### 5.5 Common Pitfalls in Log Parsing\n\n⚠️ **Pitfall 1: Forgetting to Persist the LSN Before Acknowledging to the Database**\n*   **Description**: In databases like PostgreSQL, after consuming WAL data, you must send an acknowledgement (via `confirmReceive(LSN)`). If you acknowledge an LSN but then crash before persisting your `lastProcessedLSN`, upon restart you will have lost track of those events. The database may discard the acknowledged WAL segments, making the data unrecoverable.\n*   **Why it's wrong**: This causes **permanent data loss**. The CDC pipeline will resume from an older LSN, missing all changes between the old LSN and the acknowledged one.\n*   **How to fix**: **Always persist the `lastProcessedLSN` to durable storage *before* sending the acknowledgement to the database**. Treat this as a transactional operation: write the offset, then acknowledge. Better yet, use a transactional offset store if the database supports it.\n\n⚠️ **Pitfall 2: Ignoring DDL Events in the Log**\n*   **Description**: Transaction logs contain not only `INSERT`/`UPDATE`/`DELETE` (DML) but also `ALTER TABLE`, `DROP TABLE` (DDL) events. A naive parser might skip these because they don't produce a `RawLogEntry` with `rowData`.\n*   **Why it's wrong**: Schema changes are critical. If a column is added and the parser continues to parse old-format log entries for new changes, it will fail or produce incorrect data because its internal understanding of the table layout is stale.\n*   **How to fix**: Detect DDL events in the log stream. When one is encountered, **pause the parsing stream**, notify the Schema Registry component (see Section 8) to update its schema definition, and potentially invalidate the current parser's cached table metadata. Only resume parsing after the schema is updated.\n\n⚠️ **Pitfall 3: Not Handling Large Transactions (Huge Binlog Events)**\n*   **Description**: A single transaction can modify millions of rows (e.g., a bulk `UPDATE`). The database may write this as a single, massive log entry or event. Loading this entire entry into memory for parsing can cause an OutOfMemoryError.\n*   **Why it's wrong**: It crashes the CDC process, requiring manual intervention and potentially causing significant lag.\n*   **How to fix**: Implement **streaming parsing within a single log event**. Process rows incrementally as they are parsed, flushing `RawLogEntry` objects to the buffer in batches. Alternatively, configure the source database to limit the size of WAL/binglog events (not always possible). Ensure the parser can handle partial reads and resume.\n\n⚠️ **Pitfall 4: Assuming Log Format Stability**\n*   **Description**: Writing parsing logic tightly coupled to a specific minor version of a database's log format (e.g., MySQL 8.0.25's binlog event flags).\n*   **Why it's wrong**: Database upgrades can change the log format, breaking the parser silently (producing garbled data) or with a clear error. This creates operational fragility during maintenance.\n*   **How to fix**: **Use a well-maintained, community-vetted library for the heavy lifting of binary log parsing** where possible (e.g., Debezium's embedded parsers, `mysql-binlog-connector-java`). If writing your own, clearly document the supported database versions and include version checks at connector startup. Write robust parsing logic that can fail fast with a clear error message on format mismatches.\n\n### 5.6 Implementation Guidance\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n| :--- | :--- | :--- |\n| **PostgreSQL Connector** | Use the official PostgreSQL JDBC driver's `PGReplicationConnection` API for logical decoding. | Use the **Debezium PostgreSQL connector** library (`io.debezium:debezium-connector-postgres`) which handles slot management, parsing, and schema history. |\n| **MySQL Connector** | Use the `mysql-binlog-connector-java` library for raw binlog streaming. | Use the **Debezium MySQL connector** library (`io.debezium:debezium-connector-mysql`). |\n| **Offset/Persistence Store** | A simple file on local disk (e.g., `offset.properties`). | A dedicated Kafka topic (`__cdc_offsets`) for centralized, fault-tolerant offset storage. |\n| **Internal Buffering** | `java.util.concurrent.ArrayBlockingQueue<RawLogEntry>` | A reactive stream (Project Reactor, RxJava) with configurable backpressure strategies. |\n\n**B. Recommended File/Module Structure**\n```\ncdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── config/\n│   │   ├── AppConfig.java          # Your provided config classes\n│   │   └── ConfigLoader.java\n│   ├── connector/\n│   │   ├── LogConnector.java       # Main interface and abstract class\n│   │   ├── parser/\n│   │   │   ├── LogParser.java      # Common parser interface\n│   │   │   ├── postgres/\n│   │   │   │   ├── PostgresWalParser.java\n│   │   │   │   └── PostgresConnector.java # Implements LogConnector for PG\n│   │   │   └── mysql/\n│   │   │       ├── MysqlBinlogParser.java\n│   │   │       └── MysqlConnector.java    # Implements LogConnector for MySQL\n│   │   ├── offset/\n│   │   │   ├── OffsetStore.java\n│   │   │   ├── FileOffsetStore.java\n│   │   │   └── KafkaOffsetStore.java\n│   │   └── model/\n│   │       └── RawLogEntry.java    # Your provided data model\n│   └── Main.java\n└── config/\n    └── application.yaml\n```\n\n**C. Infrastructure Starter Code**\n\nHere is a complete, ready-to-use file-based `OffsetStore`. The Log Connector will depend on this to save and load its position.\n\n```java\n// File: src/main/java/com/cdc/connector/offset/FileOffsetStore.java\npackage com.cdc.connector.offset;\n\nimport java.io.*;\nimport java.util.Properties;\n\n/**\n * Simple durable store for the last processed Log Sequence Number.\n * Persists to a file in key=value format.\n */\npublic class FileOffsetStore implements OffsetStore {\n    private final File offsetFile;\n    private final Properties properties;\n\n    public FileOffsetStore(String filePath) {\n        this.offsetFile = new File(filePath);\n        this.properties = new Properties();\n        load();\n    }\n\n    private void load() {\n        if (!offsetFile.exists()) {\n            return;\n        }\n        try (InputStream input = new FileInputStream(offsetFile)) {\n            properties.load(input);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Failed to load offset file: \" + offsetFile, e);\n        }\n    }\n\n    @Override\n    public void save(String slotName, String lastProcessedLSN) {\n        properties.setProperty(slotName, lastProcessedLSN);\n        try (OutputStream output = new FileOutputStream(offsetFile)) {\n            properties.store(output, \"CDC Offset Store\");\n        } catch (IOException e) {\n            throw new RuntimeException(\"Failed to save offset to file: \" + offsetFile, e);\n        }\n    }\n\n    @Override\n    public String load(String slotName) {\n        return properties.getProperty(slotName);\n    }\n}\n\n// File: src/main/java/com/cdc/connector/offset/OffsetStore.java\npackage com.cdc.connector.offset;\n\npublic interface OffsetStore {\n    void save(String slotName, String lastProcessedLSN);\n    String load(String slotName);\n}\n```\n\n**D. Core Logic Skeleton Code**\n\nHere is the skeleton for the primary loop of a generic `LogConnector`. Database-specific implementations (like `PostgresConnector`) will extend this.\n\n```java\n// File: src/main/java/com/cdc/connector/LogConnector.java\npackage com.cdc.connector;\n\nimport com.cdc.config.AppConfig;\nimport com.cdc.connector.model.RawLogEntry;\nimport com.cdc.connector.offset.OffsetStore;\nimport com.cdc.connector.parser.LogParser;\n\nimport java.util.List;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\n\n/**\n * Abstract base class for database-specific log connectors.\n * Manages the lifecycle, offset persistence, and buffering.\n * Subclasses implement the database-specific connection and raw byte reading.\n */\npublic abstract class LogConnector {\n    protected final AppConfig config;\n    protected final OffsetStore offsetStore;\n    protected final LogParser parser;\n    protected final BlockingQueue<RawLogEntry> buffer;\n    protected volatile String lastProcessedLSN;\n    protected volatile boolean running = false;\n    protected Thread workerThread;\n\n    public LogConnector(AppConfig config, OffsetStore offsetStore, LogParser parser) {\n        this.config = config;\n        this.offsetStore = offsetStore;\n        this.parser = parser;\n        this.buffer = new ArrayBlockingQueue<>(config.getMaxBatchSize() * 10); // Buffer 10x batch size\n    }\n\n    public void initialize() {\n        // TODO 1: Load the last processed LSN from the OffsetStore using the configured slot name.\n        // TODO 2: If no offset exists, determine a safe starting LSN (e.g., current database LSN or based on config).\n        // TODO 3: Set the internal `lastProcessedLSN` variable to the loaded/calculated value.\n        // TODO 4: Perform any database-specific initialization (e.g., create replication slot if needed).\n    }\n\n    public void start() {\n        this.running = true;\n        this.workerThread = new Thread(this::runLoop, \"log-connector-worker\");\n        this.workerThread.start();\n    }\n\n    private void runLoop() {\n        // TODO 1: Establish a low-level, streaming connection to the database log.\n        // TODO 2: While `running` is true, perform the main read-parse-buffer loop:\n        //   a. Read a chunk of raw bytes from the database stream.\n        //   b. If the buffer is full, pause reading (maybe sleep briefly) to apply backpressure.\n        //   c. Parse the raw bytes using the injected `parser` to get a List<RawLogEntry>.\n        //   d. For each RawLogEntry in the list:\n        //        i. Update the in-memory `lastProcessedLSN` to this entry's LSN.\n        //        ii. Put the entry into the `buffer` (this may block if buffer is full).\n        //   e. Periodically (e.g., after each batch or time interval), call `persistOffset()`.\n        // TODO 3: If an unrecoverable error occurs, set `running` to false and log the error.\n        // TODO 4: Upon exit (when `running` becomes false), close the database connection gracefully.\n    }\n\n    public List<RawLogEntry> getNextBatch(int maxBatchSize) {\n        // TODO 1: Create an empty list to hold the batch.\n        // TODO 2: Poll the `buffer` for up to `maxBatchSize` entries, adding them to the list.\n        // TODO 3: If the buffer is empty and the connector is still running, optionally block for a short time to wait for data.\n        // TODO 4: Return the list (may be empty if no data is available).\n        return null;\n    }\n\n    protected void persistOffset() {\n        // TODO 1: Save the current `lastProcessedLSN` to the OffsetStore.\n        // TODO 2: (Database-specific) If required by the database protocol (e.g., PostgreSQL), send an acknowledgement for this LSN.\n    }\n\n    public void stop() {\n        // TODO 1: Set `running` to false to signal the worker thread to stop.\n        // TODO 2: Interrupt the worker thread if it's blocked on reading or buffering.\n        // TODO 3: Wait for the worker thread to terminate (with a timeout).\n        // TODO 4: Call `persistOffset()` one final time to flush the most recent LSN.\n        // TODO 5: Perform any database-specific cleanup (e.g., close replication connection).\n    }\n\n    public String getLastProcessedLSN() {\n        return lastProcessedLSN;\n    }\n\n    // Abstract methods to be implemented by database-specific subclasses\n    protected abstract void connectToLogStream() throws Exception;\n    protected abstract byte[] readLogChunk() throws Exception;\n    protected abstract void acknowledgeLSN(String lsn) throws Exception;\n    protected abstract void closeConnection() throws Exception;\n}\n```\n\n**E. Language-Specific Hints (Java)**\n*   **Use `Thread.interrupt()` for graceful shutdown**: In the `stop()` method, call `workerThread.interrupt()`. Ensure your `readLogChunk` and `buffer.put()` operations are interruptible or check the `running` flag periodically.\n*   **Mind the blocking queue capacity**: The `ArrayBlockingQueue` size is a critical tuning parameter. Too small, and you throttle throughput unnecessarily. Too large, and you risk excessive memory usage during consumer lag.\n*   **Prefer `java.time` types**: When parsing database timestamps, convert them to `java.time.Instant` or `Long` (epoch millis) immediately for consistency within your `RawLogEntry`.\n*   **Resource Management**: Use try-with-resources for any database connections or streams opened in the `connectToLogStream` and `readLogChunk` implementations.\n\n**F. Milestone Checkpoint**\nAfter implementing the Log Connector & Parser for one database (e.g., PostgreSQL), you should be able to verify Milestone 1 acceptance criteria:\n\n1.  **Run a Simple Test**:\n    ```bash\n    # Start your CDC application with a config pointing to a test PostgreSQL database.\n    java -jar cdc-pipeline.jar config/application.yaml\n    ```\n2.  **Expected Behavior**:\n    *   The application should start without errors, create a replication slot in PostgreSQL (check with `SELECT * FROM pg_replication_slots;`).\n    *   Make an `INSERT` into a monitored table in your test database.\n    *   In your application logs, you should see a debug message indicating a `RawLogEntry` was parsed and buffered.\n    *   You can write a simple test consumer that calls `connector.getNextBatch(10)` and prints the received `RawLogEntry` objects, verifying the `tableName`, `operationType`, and `rowData` are correct.\n3.  **Check Offset Persistence**: Stop the application. Check the offset file (e.g., `offset.properties`). It should contain an LSN. Restart the application; it should resume from that LSN without re-reading the old change.\n4.  **Signs of Trouble**:\n    *   **\"No pgoutput plugin installed\"**: Ensure `wal_level=logical` in `postgresql.conf` and the `pgoutput` plugin is available.\n    *   **\"Replication slot already exists\"**: Your connector should handle this gracefully (re-use it). Ensure your slot management logic is correct.\n    *   **No events are parsed**: Verify your table has `REPLICA IDENTITY FULL` (for full before/after images in updates/deletes) and that you are reading from the correct slot.\n\n\n> **Milestone(s):** Milestone 1 (Change Events construction, transaction boundaries), Milestone 2 (ordering guarantees), Milestone 3 (schema attachment)\n\n## 6. Component: Change Event Builder\n\nThe **Change Event Builder** is the central transformation engine of the CDC pipeline, responsible for converting low-level, database-specific log entries into standardized, semantically rich change events ready for downstream consumption. While the Log Connector extracts raw data from transaction logs, this component assembles complete business-level change narratives with proper transaction context, deduplication, and schema attachment.\n\n### 6.1 Mental Model: The Event Assembler\n\n**Think of the Change Event Builder as a film editor assembling raw footage into a coherent movie.** \n\nThe Log Connector provides individual film frames (`RawLogEntry` objects) – isolated snapshots of database operations. However, these frames lack crucial context:\n- Which frames belong to the same \"scene\" (transaction)?\n- When did the scene start and end (commit boundaries)?\n- What objects appear before and after changes (before/after images)?\n- What's the storyline for each main character (primary key ordering)?\n\nThe Change Event Builder performs the editorial work:\n1. **Grouping**: It collects all log entries belonging to the same database transaction (using transaction IDs or commit boundaries)\n2. **Storytelling**: It arranges changes in the order they occurred within each transaction\n3. **Character Development**: For UPDATE operations, it pairs \"before\" and \"after\" images to show exactly what changed\n4. **Quality Control**: It removes duplicate frames (idempotent processing) and ensures continuity (no missing frames)\n5. **Metadata Addition**: It attaches schema version information so consumers understand the data format\n6. **Final Cut**: It outputs complete, self-contained `ChangeEvent` objects – the finished scenes ready for distribution\n\nThis mental model emphasizes that raw log parsing is just the beginning – the real value comes from assembling isolated database operations into coherent, transactionally consistent change stories that downstream systems can reliably consume.\n\n### 6.2 Interface and Transaction State\n\nThe Change Event Builder exposes a simple streaming interface while maintaining complex internal state about ongoing transactions. Its primary responsibility is transforming sequences of `RawLogEntry` objects into sequences of `ChangeEvent` objects, handling all transaction-level logic internally.\n\n#### 6.2.1 Public Interface\n\nThe component provides two operational modes: batch processing for throughput and streaming for low latency. Both share the same internal transaction state management.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `processEntry(RawLogEntry)` | `entry`: A single parsed log entry | `List<ChangeEvent>` | Processes a single log entry through the transaction state machine. May return 0-n events (0 when accumulating transaction state, 1+ when transaction commits). This is the streaming API. |\n| `processBatch(List<RawLogEntry>)` | `entries`: Batch of parsed entries | `List<ChangeEvent>` | Processes multiple log entries efficiently. Maintains same guarantees as streaming API but with better throughput due to batched state transitions. |\n| `flushPendingTransactions()` | None | `List<ChangeEvent>` | Forces all pending (uncommitted) transactions to be emitted as events with a special \"transaction rolled back\" marker. Used during shutdown or when switching database connections to prevent state leaks. |\n| `getTransactionCount()` | None | `int` | Returns the number of currently active (uncommitted) transactions being tracked. Used for monitoring and health checks. |\n| `reset()` | None | `void` | Clears all internal transaction state. Used after connection loss or position reset to start fresh. |\n\n#### 6.2.2 Internal Transaction State Machine\n\nThe builder maintains a state machine for each database transaction it encounters. This is the core complexity of the component – tracking which operations belong to which transaction and when to emit them.\n\n| Current State | Event | Next State | Action Taken |\n|---------------|-------|------------|--------------|\n| **NO_TRANSACTION** | Receive `RawLogEntry` with `BEGIN` marker | `TRANSACTION_OPEN` | Create new `TransactionState` object with empty operation list. Store transaction ID from log entry. |\n| **NO_TRANSACTION** | Receive `RawLogEntry` without `BEGIN` (implicit tx) | `TRANSACTION_OPEN` | Create new `TransactionState` for implicit transaction. Generate synthetic transaction ID. Add operation to transaction's operation list. |\n| **TRANSACTION_OPEN** | Receive `RawLogEntry` with database operation (INSERT/UPDATE/DELETE) | `TRANSACTION_OPEN` | Add operation to transaction's operation list. For UPDATE operations, attempt to pair with previous operation on same PK to build before/after image. |\n| **TRANSACTION_OPEN** | Receive `RawLogEntry` with `COMMIT` marker | `NO_TRANSACTION` | 1. Finalize all operations in transaction (complete before/after images)<br>2. Generate `ChangeEvent` for each operation<br>3. Emit events in transaction order<br>4. Discard `TransactionState` |\n| **TRANSACTION_OPEN** | Receive `RawLogEntry` with `ROLLBACK` marker | `NO_TRANSACTION` | Discard `TransactionState` without emitting any events. Log warning about rolled back transaction. |\n| **TRANSACTION_OPEN** | Timeout (transaction open > threshold) | `NO_TRANSACTION` | Emit events with \"incomplete transaction\" warning flag. Used for long-running transactions that might indicate issues. |\n| **ANY** | Pipeline reset request | `NO_TRANSACTION` | Discard all `TransactionState` objects. Log count of abandoned transactions. |\n\n#### 6.2.3 TransactionState Data Structure\n\nInternally, the builder maintains a map of transaction ID to `TransactionState` objects:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `transactionId` | `String` | Unique identifier for this transaction (from database log or generated) |\n| `startLsn` | `String` | Log Sequence Number where transaction began |\n| `operations` | `List<TransactionOperation>` | Ordered list of operations within this transaction |\n| `startTimestamp` | `Long` | When transaction began (monotonic clock) |\n| `lastActivityTimestamp` | `Long` | When last operation was added (for timeout detection) |\n| `sourceDatabase` | `String` | Which database this transaction came from (important in multi-source CDC) |\n\nEach `TransactionOperation` contains:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `sequenceNumber` | `int` | Order within transaction (1, 2, 3...) |\n| `tableName` | `String` | Fully qualified table name |\n| `operationType` | `String` | `OPERATION_INSERT`, `OPERATION_UPDATE`, or `OPERATION_DELETE` |\n| `primaryKey` | `Map<String, Object>` | Primary key values (for deduplication and ordering) |\n| `rowData` | `Map<String, Object>` | Complete row data at time of operation |\n| `previousOperation` | `TransactionOperation` | Reference to previous operation on same PK in same transaction (for building before-image) |\n\n#### 6.2.4 Before/After Image Construction Algorithm\n\nThe most complex logic in the builder is constructing complete before and after images for UPDATE operations:\n\n1. **When receiving an INSERT operation**: Store the row data as the \"after\" image. The \"before\" image is `null` (since row didn't exist before).\n\n2. **When receiving a DELETE operation**: Store the row data as the \"before\" image. The \"after\" image is `null` (since row doesn't exist after).\n\n3. **When receiving an UPDATE operation**: The builder must find the previous state of the same row. This requires:\n   - Identifying the row by primary key (from the UPDATE's row data)\n   - Looking backward in the current transaction's operation list\n   - Finding the most recent operation on that same primary key\n   \n   The algorithm works as follows:\n   \n   1. Extract primary key values from the UPDATE's `rowData`\n   2. Search backward through the current transaction's `operations` list\n   3. For each previous operation:\n      - If it operates on the same table AND has matching primary key values:\n        - If previous operation is INSERT or UPDATE: Use its `rowData` as the \"before\" image\n        - If previous operation is DELETE: This is an error (can't UPDATE a deleted row) - log warning, use empty map as \"before\"\n   4. If no matching operation found in current transaction:\n      - The \"before\" image is unknown within this transaction scope\n      - Store the UPDATE's `rowData` as the \"after\" image only\n      - Mark the event as having \"partial before image\"\n\nThis algorithm ensures that within a single transaction, we can reconstruct the complete change history of each row. However, it cannot reconstruct changes that span transactions (that requires log parsing across transaction boundaries, which is more complex).\n\n### 6.3 ADR: Ordering and Deduplication Strategy\n\n> **Decision: Primary Key-Based Partitioned Ordering with Transaction-Aware Deduplication**\n> - **Context**: Change events must be delivered to consumers in the exact order they occurred in the database, but different consumers may process events at different speeds. We need to guarantee that (1) events for the same row are processed in order, (2) events across different rows can be processed in parallel, and (3) no duplicate events are delivered even after failures and restarts.\n> - **Options Considered**:\n>   1. **Global Sequence Numbering**: Assign a monotonically increasing sequence number to every event across all tables. Consumers process in strict global order.\n>   2. **Table-Level Ordering**: Maintain order per table, allowing parallel processing across tables but serial within each table.\n>   3. **Primary Key Partitioned Ordering**: Partition by (table, primary key hash), guaranteeing order per primary key while allowing maximal parallelism.\n> - **Decision**: **Primary Key Partitioned Ordering** using a combination of LSN-based sequencing and idempotent producer logic.\n> - **Rationale**:\n>   - **Correctness**: For most use cases (materialized views, cache invalidation, audit trails), only the order of changes to the *same row* matters. Changes to different rows are independent and can be safely reordered.\n>   - **Performance**: Partitioning by primary key allows horizontal scaling - each partition can be processed independently by different consumer instances.\n>   - **Real-world alignment**: This matches how databases work internally - they lock at row level, not table level, during concurrent modifications.\n>   - **Idempotency guarantee**: By including the source LSN in each event and making producers idempotent, we can safely retry events after failures without creating duplicates.\n> - **Consequences**:\n>   - ✅ Enables high parallelism while maintaining correctness for row-level ordering\n>   - ✅ Aligns naturally with Kafka partitioning strategy (partition key = table + primary key hash)\n>   - ⚠️ Requires consumers to handle out-of-order events *across different primary keys* (which they should be able to do anyway)\n>   - ⚠️ Slightly more complex deduplication logic needed compared to global ordering\n\nThe following table compares the ordering strategies:\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| **Global Sequence Numbering** | Simplest for consumers (strict total order) | Terrible throughput (single processing thread) | Severely limits scalability - becomes bottleneck at scale |\n| **Table-Level Ordering** | Good parallelism across tables | Poor parallelism for large tables with many concurrent updates | Still serializes updates to different rows in same table unnecessarily |\n| **Primary Key Partitioned Ordering** | Maximum parallelism while maintaining row-level correctness | Consumers must handle cross-key ordering relaxations | **CHOSEN**: Best trade-off - matches real database concurrency model |\n\n#### 6.3.1 Deduplication Implementation Strategy\n\nTo achieve idempotent production (avoiding duplicate events during retries), the builder implements a two-layer deduplication strategy:\n\n**Layer 1: Producer-Initiated Deduplication**\n- Each `ChangeEvent` includes a unique `eventId` computed as: `{sourceDatabase}_{transactionId}_{tableName}_{primaryKeyHash}_{operationSequence}`\n- The Event Streamer (next component) maintains a cache of recently sent event IDs\n- Before sending an event, it checks the cache - if present, skip sending\n- Cache is sized based on maximum flight window (e.g., 5 minutes of events at peak throughput)\n\n**Layer 2: Consumer-Initiated Deduplication**\n- Consumers store the `eventId` of the last successfully processed event per partition\n- When reprocessing after failure, consumers skip events with `eventId` ≤ last processed\n- This provides end-to-end idempotency even if producer deduplication fails\n\nThe builder's role in this strategy is to generate deterministic, unique `eventId` values that capture the identity of the change operation unambiguously.\n\n#### 6.3.2 Ordering Guarantees by Operation Type\n\nThe builder enforces different ordering semantics based on operation type:\n\n| Operation Type | Ordering Guarantee | Implementation Approach |\n|----------------|-------------------|-------------------------|\n| **INSERT** | Must be delivered before any UPDATE/DELETE on same PK | Sequence number starts at 1 for each PK |\n| **UPDATE** | Must be delivered in chronological order for same PK | Increment sequence number for each operation on same PK |\n| **DELETE** | Must be delivered after all previous operations, before any subsequent INSERT with same PK | Sequence number increments, but PK is considered \"deleted\" after |\n| **TRUNCATE** | Special event that invalidates all previous ordering for table | Resets all sequence numbers for table |\n\n### 6.4 Implementation Guidance\n\nThe Change Event Builder is where the core business logic of CDC lives. While the Log Connector handles database-specific details, this component implements the universal logic of transaction assembly and event construction.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Transaction State Store** | In-memory `ConcurrentHashMap` with periodic flush to disk | Embedded key-value store (RocksDB) with transaction isolation |\n| **Before/After Image Cache** | LRU cache of recent row states per transaction | Persistent snapshot store with point-in-time recovery |\n| **Event ID Generation** | Deterministic hash of (LSN + table + PK) | UUID v5 (name-based) with namespace for collision avoidance |\n| **Timeout Detection** | Scheduled thread pool checking timestamps | Reactive streams with backpressure-aware timeouts |\n\n**B. Recommended File/Module Structure:**\n\n```\ncdc-system/\n├── src/main/java/com/example/cdc/\n│   ├── builder/\n│   │   ├── ChangeEventBuilder.java          # Main builder class (implements interface)\n│   │   ├── TransactionState.java            # Internal transaction state representation\n│   │   ├── TransactionOperation.java        # Individual operation within transaction\n│   │   ├── BeforeAfterImageBuilder.java     # Dedicated class for image construction\n│   │   ├── EventIdGenerator.java            # Deterministic ID generation\n│   │   └── DeduplicationCache.java          # In-memory cache for producer deduplication\n│   ├── model/                               # (From previous section)\n│   │   ├── RawLogEntry.java\n│   │   ├── ChangeEvent.java\n│   │   └── SchemaVersion.java\n│   └── Main.java\n└── config/\n    └── application.yml\n```\n\n**C. Infrastructure Starter Code:**\n\nHere's a complete, ready-to-use implementation of the `TransactionState` and `TransactionOperation` classes that form the foundation:\n\n```java\npackage com.example.cdc.builder;\n\nimport java.util.*;\nimport java.util.concurrent.ConcurrentHashMap;\n\n/**\n * Represents an ongoing database transaction being tracked.\n * This is an internal class not exposed outside the builder.\n */\npublic class TransactionState {\n    private final String transactionId;\n    private final String startLsn;\n    private final long startTimestamp;\n    private final List<TransactionOperation> operations;\n    private long lastActivityTimestamp;\n    private final Map<String, TransactionOperation> lastOperationByKey;\n    \n    public TransactionState(String transactionId, String startLsn) {\n        this.transactionId = transactionId;\n        this.startLsn = startLsn;\n        this.startTimestamp = System.currentTimeMillis();\n        this.lastActivityTimestamp = startTimestamp;\n        this.operations = new ArrayList<>();\n        this.lastOperationByKey = new HashMap<>();\n    }\n    \n    public void addOperation(TransactionOperation operation) {\n        operations.add(operation);\n        lastActivityTimestamp = System.currentTimeMillis();\n        \n        // Track last operation per primary key for quick before-image lookup\n        String key = operation.getTableName() + \":\" + hashPrimaryKey(operation.getPrimaryKey());\n        lastOperationByKey.put(key, operation);\n    }\n    \n    public TransactionOperation findPreviousOperation(String tableName, \n                                                     Map<String, Object> primaryKey) {\n        String key = tableName + \":\" + hashPrimaryKey(primaryKey);\n        return lastOperationByKey.get(key);\n    }\n    \n    public boolean isTimedOut(long timeoutMs) {\n        return (System.currentTimeMillis() - lastActivityTimestamp) > timeoutMs;\n    }\n    \n    public List<TransactionOperation> getOperationsInOrder() {\n        return Collections.unmodifiableList(operations);\n    }\n    \n    // Helper method to create consistent key for maps\n    private String hashPrimaryKey(Map<String, Object> primaryKey) {\n        // Simple hash - in production, use proper hashing\n        return Integer.toHexString(primaryKey.hashCode());\n    }\n    \n    // Getters\n    public String getTransactionId() { return transactionId; }\n    public String getStartLsn() { return startLsn; }\n    public long getStartTimestamp() { return startTimestamp; }\n    public long getLastActivityTimestamp() { return lastActivityTimestamp; }\n}\n\n/**\n * Represents a single database operation (INSERT/UPDATE/DELETE) within a transaction.\n */\npublic class TransactionOperation {\n    private final int sequenceNumber;\n    private final String tableName;\n    private final String operationType;  // INSERT, UPDATE, DELETE\n    private final Map<String, Object> primaryKey;\n    private final Map<String, Object> rowData;\n    private final long timestamp;\n    \n    public TransactionOperation(int sequenceNumber, String tableName, \n                               String operationType, Map<String, Object> primaryKey,\n                               Map<String, Object> rowData) {\n        this.sequenceNumber = sequenceNumber;\n        this.tableName = tableName;\n        this.operationType = operationType;\n        this.primaryKey = Collections.unmodifiableMap(new HashMap<>(primaryKey));\n        this.rowData = Collections.unmodifiableMap(new HashMap<>(rowData));\n        this.timestamp = System.currentTimeMillis();\n    }\n    \n    // Getters\n    public int getSequenceNumber() { return sequenceNumber; }\n    public String getTableName() { return tableName; }\n    public String getOperationType() { return operationType; }\n    public Map<String, Object> getPrimaryKey() { return primaryKey; }\n    public Map<String, Object> getRowData() { return rowData; }\n    public long getTimestamp() { return timestamp; }\n}\n```\n\n**D. Core Logic Skeleton Code:**\n\nHere's the main `ChangeEventBuilder` class with detailed TODO comments mapping to the algorithms described earlier:\n\n```java\npackage com.example.cdc.builder;\n\nimport com.example.cdc.model.RawLogEntry;\nimport com.example.cdc.model.ChangeEvent;\nimport com.example.cdc.model.SchemaVersion;\nimport java.util.*;\n\npublic class ChangeEventBuilder {\n    private final Map<String, TransactionState> activeTransactions;\n    private final Map<String, SchemaVersion> schemaCache;\n    private final DeduplicationCache deduplicationCache;\n    private final long transactionTimeoutMs;\n    private int nextOperationSequence;\n    \n    public ChangeEventBuilder(long transactionTimeoutMs) {\n        this.activeTransactions = new ConcurrentHashMap<>();\n        this.schemaCache = new HashMap<>();\n        this.deduplicationCache = new DeduplicationCache(10000); // 10k entry cache\n        this.transactionTimeoutMs = transactionTimeoutMs;\n        this.nextOperationSequence = 1;\n    }\n    \n    /**\n     * Processes a single raw log entry through the transaction state machine.\n     * May return 0-n events (0 when accumulating, 1+ when transaction commits).\n     */\n    public List<ChangeEvent> processEntry(RawLogEntry entry) {\n        List<ChangeEvent> result = new ArrayList<>();\n        \n        // TODO 1: Check if entry marks a transaction boundary (BEGIN/COMMIT/ROLLBACK)\n        //   - If BEGIN: create new TransactionState, store in activeTransactions\n        //   - If COMMIT: \n        //        a. Retrieve TransactionState for this transaction\n        //        b. Call buildEventsForTransaction() to convert all operations\n        //        c. Remove from activeTransactions\n        //        d. Add all built events to result list\n        //   - If ROLLBACK: remove TransactionState without building events\n        \n        // TODO 2: If entry is a regular database operation (INSERT/UPDATE/DELETE):\n        //   - Determine which transaction it belongs to (use transactionId from entry or implicit)\n        //   - Retrieve or create TransactionState for that transaction\n        //   - Create TransactionOperation with sequence number (increment nextOperationSequence)\n        //   - Add operation to TransactionState\n        \n        // TODO 3: Check for timed-out transactions\n        //   - Iterate through activeTransactions\n        //   - For each transaction older than transactionTimeoutMs:\n        //        a. Log warning about long-running transaction\n        //        b. Build events anyway (with \"incomplete\" flag)\n        //        c. Remove from activeTransactions\n        //        d. Add events to result list\n        \n        // TODO 4: For each event to be emitted:\n        //   - Generate deterministic eventId using EventIdGenerator\n        //   - Check deduplicationCache - if already sent recently, skip\n        //   - Attach appropriate SchemaVersion from schemaCache\n        //   - Set commitTimestamp (use entry timestamp or current time)\n        \n        return result;\n    }\n    \n    /**\n     * Processes a batch of entries more efficiently than individual processing.\n     */\n    public List<ChangeEvent> processBatch(List<RawLogEntry> entries) {\n        List<ChangeEvent> allEvents = new ArrayList<>();\n        \n        // TODO 1: Group entries by transaction for batch processing\n        //   - Create Map<transactionId, List<RawLogEntry>>\n        //   - For each entry, add to appropriate transaction group\n        \n        // TODO 2: Process each transaction group independently (enables parallelism)\n        //   - For each transaction group:\n        //        a. Sort entries by LSN or sequence within transaction\n        //        b. Process through same logic as processEntry() but batched\n        //        c. Collect all resulting events\n        \n        // TODO 3: Sort events across transactions by commit timestamp\n        //   - This maintains global ordering across transactions\n        //   - Use stable sort to preserve within-transaction ordering\n        \n        // TODO 4: Apply batch deduplication\n        //   - Check all event IDs in one batch operation\n        //   - Filter out any duplicates within the batch itself\n        \n        return allEvents;\n    }\n    \n    /**\n     * Converts all operations in a completed transaction to ChangeEvent objects.\n     */\n    private List<ChangeEvent> buildEventsForTransaction(TransactionState transaction) {\n        List<ChangeEvent> events = new ArrayList<>();\n        \n        // TODO 1: Get operations in sequence order\n        List<TransactionOperation> operations = transaction.getOperationsInOrder();\n        \n        // TODO 2: For each operation, build corresponding ChangeEvent\n        for (TransactionOperation op : operations) {\n            ChangeEvent event = new ChangeEvent();\n            \n            // TODO 3: Set basic fields\n            event.setSourceTable(op.getTableName());\n            event.setOperationType(op.getOperationType());\n            event.setTransactionId(transaction.getTransactionId());\n            event.setCommitTimestamp(transaction.getLastActivityTimestamp());\n            \n            // TODO 4: Handle operation-specific logic\n            switch (op.getOperationType()) {\n                case \"INSERT\":\n                    // Before image is null, after image is rowData\n                    event.setBeforeImage(null);\n                    event.setAfterImage(op.getRowData());\n                    break;\n                    \n                case \"UPDATE\":\n                    // Find previous operation on same PK for before image\n                    TransactionOperation previous = transaction.findPreviousOperation(\n                        op.getTableName(), op.getPrimaryKey());\n                    \n                    if (previous != null && previous != op) {\n                        // Found previous state in same transaction\n                        event.setBeforeImage(previous.getRowData());\n                    } else {\n                        // No previous state in this transaction\n                        // Mark as partial before image (null or empty)\n                        event.setBeforeImage(Collections.emptyMap());\n                    }\n                    event.setAfterImage(op.getRowData());\n                    break;\n                    \n                case \"DELETE\":\n                    // Before image is rowData, after image is null\n                    event.setBeforeImage(op.getRowData());\n                    event.setAfterImage(null);\n                    break;\n            }\n            \n            // TODO 5: Generate deterministic event ID\n            String eventId = generateEventId(transaction, op);\n            event.setEventId(eventId);\n            \n            // TODO 6: Attach schema version\n            SchemaVersion schema = schemaCache.get(op.getTableName());\n            if (schema != null) {\n                event.setSchemaVersionId(schema.getSchemaId());\n            }\n            \n            events.add(event);\n        }\n        \n        return events;\n    }\n    \n    /**\n     * Generates a deterministic event ID that uniquely identifies this change.\n     */\n    private String generateEventId(TransactionState transaction, \n                                   TransactionOperation operation) {\n        // TODO 1: Create string representation combining:\n        //   - Source database identifier\n        //   - Transaction ID\n        //   - Table name\n        //   - Primary key hash\n        //   - Operation sequence number within transaction\n        \n        // TODO 2: Hash the combined string (SHA-256 or similar)\n        //   - Use first 16 chars of hex representation for readability\n        \n        // TODO 3: Format as: db_{dbId}_tx_{txId}_{table}_{pkHash}_{seq}\n        \n        return \"event_\" + transaction.getTransactionId() + \"_\" + \n               operation.getSequenceNumber();\n    }\n    \n    /**\n     * Forces all pending transactions to be emitted (for shutdown).\n     */\n    public List<ChangeEvent> flushPendingTransactions() {\n        List<ChangeEvent> allEvents = new ArrayList<>();\n        \n        // TODO 1: Iterate through all activeTransactions\n        for (TransactionState transaction : activeTransactions.values()) {\n            // TODO 2: Build events for each transaction (mark as \"forced flush\")\n            List<ChangeEvent> events = buildEventsForTransaction(transaction);\n            \n            // TODO 3: Add special marker to each event indicating incomplete transaction\n            for (ChangeEvent event : events) {\n                // Add metadata flag\n            }\n            \n            allEvents.addAll(events);\n        }\n        \n        // TODO 4: Clear activeTransactions map\n        activeTransactions.clear();\n        \n        return allEvents;\n    }\n}\n```\n\n**E. Common Pitfalls:**\n\n⚠️ **Pitfall: Assuming All Updates Have Before Images**\n- **Description**: Treating every UPDATE as having a complete before image from the same transaction. In reality, the first UPDATE in a transaction has no before image within that transaction.\n- **Why it's wrong**: Creates incorrect change events showing \"null to value\" instead of \"unknown to value.\" Consumers might misinterpret this as row creation.\n- **Fix**: Implement proper before-image search logic that handles the \"no previous state in transaction\" case by marking the event appropriately.\n\n⚠️ **Pitfall: Ignoring Transaction Rollbacks**\n- **Description**: Building and emitting events for operations that are later rolled back.\n- **Why it's wrong**: Delivers phantom changes that never actually happened in the database, corrupting downstream state.\n- **Fix**: Never emit events until seeing COMMIT. Buffer all operations in memory until transaction commits successfully.\n\n⚠️ **Pitfall: Overly Aggressive Deduplication**\n- **Description**: Caching event IDs for too long and incorrectly filtering legitimate events that happen to have similar characteristics.\n- **Why it's wrong**: Drops real changes, creating data loss in downstream systems.\n- **Fix**: Use deterministic event ID generation that's truly unique per change, and limit cache size to reasonable time window (e.g., 5 minutes).\n\n⚠️ **Pitfall: Not Handling Long-Running Transactions**\n- **Description**: Holding transaction state indefinitely, consuming increasing memory, potentially causing OOM errors.\n- **Why it's wrong**: System becomes unstable with large transactions or transaction leaks.\n- **Fix**: Implement transaction timeout with configurable threshold. Force-flush or abort transactions exceeding threshold with appropriate warnings.\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the Change Event Builder, verify correctness with this test:\n\n```bash\n# Run the comprehensive test suite\nmvn test -Dtest=ChangeEventBuilderTest\n\n# Expected output should show:\n# - Tests for single transaction with multiple operations\n# - Tests for UPDATE before-image construction\n# - Tests for transaction rollback (no events emitted)\n# - Tests for event ID uniqueness\n# - All tests passing\n\n# Manual verification:\n# 1. Start the CDC pipeline with a test database\n# 2. Execute this SQL sequence:\n#    BEGIN;\n#    INSERT INTO users(id, name) VALUES (1, 'Alice');\n#    UPDATE users SET name = 'Bob' WHERE id = 1;\n#    COMMIT;\n# 3. Check the emitted events:\n#    - Should see exactly 2 events (INSERT then UPDATE)\n#    - INSERT event: before=null, after={'id':1, 'name':'Alice'}\n#    - UPDATE event: before={'id':1, 'name':'Alice'}, after={'id':1, 'name':'Bob'}\n#    - Event IDs should be different but deterministic\n#    - Both events should have same transactionId\n```\n\n**G. Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Missing before-images for UPDATEs** | Not searching previous operations in same transaction | Log the transaction operation list before building events | Implement `findPreviousOperation()` method that searches backward |\n| **Duplicate events after restart** | Event ID generation not deterministic across restarts | Compare event IDs generated from same log position | Include LSN or other persistent identifier in event ID calculation |\n| **Memory leak over time** | Transaction state not cleared after timeout | Monitor `getTransactionCount()` over time | Implement background thread to check for timed-out transactions |\n| **Events emitted for rolled back transactions** | Emitting events before seeing COMMIT | Check if COMMIT log entry is being processed | Buffer operations until COMMIT, discard on ROLLBACK |\n| **Slow processing with large transactions** | Building all events only at COMMIT time | Profile memory usage during large transaction | Implement incremental event building or streaming within transaction |\n\n\n> **Milestone(s):** Milestone 2 (Event Streaming & Delivery)\n\n## 7. Component: Event Streamer & Delivery\n\nThe **Event Streamer & Delivery** component is the system's distribution engine. It takes validated `ChangeEvent` objects from the `ChangeEventBuilder` and reliably transmits them to downstream consumers via a message broker (like Apache Kafka). Its core responsibility is guaranteeing that every database change is delivered to consumers **at least once**, while preserving the **order of changes** for any given database row, even in the face of network failures, broker restarts, or consumer crashes. This component embodies the system's promise of real-time, reliable data propagation.\n\n### 7.1 Mental Model: The Reliable Postal Service\n\nThink of this component as a highly reliable, trackable postal service for data packages (`ChangeEvent`s). The postal service (`EventStreamer`) must deliver every package (`event`) to a set of post office boxes (`Kafka topic partitions`). The challenge is ensuring no package gets lost, even if a mail truck breaks down (`producer failure`), and that packages destined for the same household (`database row with the same primary key`) arrive in the exact order they were mailed.\n\n- **Postmaster (Producer):** The `EventStreamer` acts as the postmaster, accepting packages, assigning them a destination box based on the recipient's address (`partition key`), and dispatching them. It keeps a meticulous log (`producer state`) of which packages have been successfully handed off to the postal trucks (`Kafka brokers`).\n- **Post Office Boxes (Partitions):** Each Kafka topic is split into partitions, akin to rows of post office boxes. Packages for the same household must go into the same box to preserve order. Different households can use different boxes, allowing for parallel delivery.\n- **Delivery Receipts (Acknowledgments):** Before considering a package \"delivered,\" the postmaster waits for a signed receipt (`acknowledgment` or `ack`) from the postal system confirming the package is safely stored. If a receipt is missing, the package is re-sent.\n- **Backpressure (Mailroom Congestion):** If the post office boxes are full (consumers are slow), the postal service must slow down or temporarily stop accepting new packages from the mail sorting facility (`ChangeEventBuilder`) to avoid overwhelming the system. This is **backpressure**.\n\nThis mental model clarifies the dual focus: **reliability** (no lost packages) and **ordered delivery per key** (same household, same box).\n\n### 7.2 Interface and Delivery Semantics\n\nThe `EventStreamer` component exposes a simple interface for receiving events and manages complex internal state to fulfill its delivery guarantees. Its primary contract is defined by its interaction with the `ChangeEventBuilder` (upstream) and the Kafka cluster (downstream).\n\n**Core Interface Methods:**\n\n| Method Name | Parameters | Returns | Description & Side Effects |\n|-------------|------------|---------|----------------------------|\n| `initialize` | `config: AppConfig` | `void` | Prepares the streamer: creates Kafka producer instance, configures serializers, connects to the schema registry, and initializes internal metrics and state. |\n| `start` | `-` | `void` | Starts the internal event processing loop. Begins accepting events from the upstream builder via a `publish` call. |\n| `stop` | `-` | `void` | Initiates a graceful shutdown: stops accepting new events, flushes any in-flight messages to Kafka, waits for all pending acknowledgments, and closes the Kafka producer connection. |\n| `publish` | `events: List<ChangeEvent>` | `void` | **Primary ingestion method.** Accepts a batch of events from the `ChangeEventBuilder`. The method is **blocking** under backpressure conditions. It assigns each event a target Kafka partition, serializes it, and hands it to the Kafka producer client. It internally tracks the events until successful acknowledgment. |\n| `getDeliveryStatus` | `-` | `Map<String, Long>` | Returns a map of the last successfully published Log Sequence Number (LSN) per database replication slot. Used for health checks and offset persistence. |\n| `isHealthy` | `-` | `boolean` | Checks the health of the underlying Kafka producer connection and whether any unrecoverable publishing errors have occurred. |\n\n**Internal State Table:**\n\n| State Field | Type | Description |\n|-------------|------|-------------|\n| `kafkaProducer` | `KafkaProducer<String, byte[]>` | The configured Apache Kafka producer client. It handles network communication, batching, retries, and acknowledgments. |\n| `pendingEvents` | `ConcurrentMap<String, ChangeEvent>` | A thread-safe map tracking events that have been sent but not yet acknowledged by Kafka. Keyed by a **deterministic event ID** (e.g., `transactionId:sequence`). |\n| `lastAckedLsnBySlot` | `ConcurrentMap<String, String>` | The most recent Log Sequence Number (LSN) for which *all* preceding events have been acknowledged per replication slot. This is the safe point to which the system can recover. |\n| `backpressureSignal` | `AtomicBoolean` | A flag set when consumer lag exceeds a threshold. When `true`, the `publish` method will block, applying backpressure to the upstream `ChangeEventBuilder`. |\n| `inFlightEventCount` | `AtomicInteger` | A counter of events sent to Kafka but not yet acknowledged. Used to limit the total number of unacknowledged events (in-flight) to prevent memory exhaustion. |\n\n**Delivery Semantics: At-Least-Once**\nThe component guarantees **at-least-once delivery**. This means for every `ChangeEvent` given to the `publish` method, the following will hold:\n1.  The event will be successfully written to the Kafka topic **at least one time**.\n2.  In case of failures (e.g., producer crash after send but before acknowledgment), the event **may be duplicated**, appearing in the topic more than once.\n3.  For any given primary key, the order of events in the Kafka topic will match the order of the original database transactions.\n\nThis is achieved through a combination of Kafka's idempotent producer (to avoid duplicates on retries *within a single producer session*) and the component's internal tracking of the last *safely persisted* LSN. The algorithm is detailed in the next section.\n\n### 7.3 Internal Behavior and Algorithm\n\nThe `EventStreamer` operates as a stateful publisher. The following numbered steps describe the lifecycle of a `ChangeEvent` from receipt to confirmed delivery.\n\n1.  **Event Reception & Partition Assignment:** The `publish(List<ChangeEvent>)` method receives a batch. For each `ChangeEvent`:\n    a.  Extract the **partition key**. This is a string derived from the event's `sourceTable` and its primary key values (from either `beforeImage` or `afterImage`). For example: `\"users:pk_value_123\"`.\n    b.  Use a **consistent hash function** on the partition key to select a specific partition within the Kafka topic for that table. This ensures all events for the same database row go to the same partition.\n    c.  Retrieve the corresponding `SchemaVersion` from the cache (or Schema Registry) using the `schemaVersionId`.\n    d.  Serialize the `ChangeEvent` and its schema into a byte array using the `EventSerializer.serialize(event, schemaVersion)`.\n\n2.  **Producer Send with Callback:**\n    a.  Construct a Kafka `ProducerRecord` with the target topic, partition key, and serialized value.\n    b.  Before sending, add the event's deterministic ID to the `pendingEvents` map.\n    c.  Invoke `kafkaProducer.send(record, callback)`. This is an asynchronous operation.\n    d.  The registered **callback** is executed by a Kafka producer thread when the broker responds. The callback is critical for reliability.\n\n3.  **Callback Logic (Acknowledgment Handling):**\n    a.  **On Success:** The Kafka broker has durably stored the message. The callback:\n        i.  Removes the event's ID from the `pendingEvents` map.\n        ii.  Updates the `lastAckedLsnBySlot` for the event's source replication slot. The LSN to record is the event's own LSN, but only if *all events with a lower LSN for that slot are also acknowledged*. This requires tracking LSNs in order.\n    b.  **On Failure:** The send failed after exhausting retries (e.g., topic not found, authorization error). The callback:\n        i.  Logs the error and marks the component's health as `DEGRADED` or `STOPPED`.\n        ii.  The event remains in `pendingEvents`. A recovery procedure (e.g., restart) will be needed.\n\n4.  **Backpressure Mechanism:** A separate monitoring thread periodically checks consumer lag for the target topics (via Kafka admin APIs or metrics).\n    a.  If lag for any partition exceeds `config.backpressureThresholdMs`, it sets the `backpressureSignal` to `true`.\n    b.  The `publish` method checks this signal. If active, it blocks (using `Thread.sleep` or a lock) before processing the next batch, slowing the ingestion rate.\n    c.  When lag falls below the threshold, the signal is cleared and publishing resumes at full speed.\n\n5.  **Graceful Shutdown & Recovery:** When `stop()` is called:\n    a.  The `publish` method stops accepting new events.\n    b.  `kafkaProducer.flush()` is called, blocking until all sent messages receive a callback.\n    c.  After flush, the `lastAckedLsnBySlot` map contains the most recent *consistent* LSNs. These LSNs are persisted to the `OffsetStore` (e.g., a file), marking the recovery point.\n    d.  On the next startup, `initialize()` loads these LSNs. The `LogConnector` is instructed to start reading the transaction log from just *after* these LSNs, ensuring no change event is missed, though some may be replayed (at-least-once).\n\n> **Key Design Insight:** The \"safely persisted LSN\" is only advanced when *all events up to that LSN are acknowledged*. This ensures that if the process crashes, upon restart it will replay from an LSN where some events might have been sent but not acknowledged, potentially causing duplicates, but guaranteeing no data loss.\n\n### 7.4 ADR: At-Least-Once vs. Exactly-Once Delivery\n\n> **Decision: Implement At-Least-Once Delivery with Idempotent Kafka Producer**\n> - **Context**: The CDC system must guarantee no data loss. Downstream consumers can tolerate duplicate events (idempotent consumption) but cannot tolerate missing events. The complexity and performance cost of exactly-once semantics is high.\n> - **Options Considered**:\n>     1.  **At-Least-Once with Idempotent Producer**: Use Kafka's `enable.idempotence=true` producer setting to prevent duplicates caused by internal producer retries, but accept duplicates from producer restarts/replays.\n>     2.  **Transactional/Exactly-Once Semantics**: Use Kafka transactions to coordinate the publishing of events and the storing of the source LSN in a single atomic operation, eliminating duplicates even across producer sessions.\n>     3.  **At-Most-Once**: Send events without waiting for acknowledgment. Lower latency but risk of data loss on failure.\n> - **Decision**: Implement **At-Least-Once with Idempotent Producer** (Option 1).\n> - **Rationale**:\n>     - **Consumer Idempotency is Feasible**: Downstream consumers of change data (e.g., updating a cache, populating a data warehouse) can often be designed to handle duplicate events idempotently (e.g., using an upsert operation based on primary key).\n>     - **Lower Complexity**: Exactly-once semantics require managing Kafka transactions and a distributed commit protocol, significantly increasing the complexity of the `EventStreamer` and its recovery procedures.\n>     - **Performance**: At-least-once has lower latency and higher throughput compared to the two-phase commit of transactional producers.\n>     - **Practical Robustness**: The combination of idempotent producer (prevents intra-session duplicates) and deterministic event ID (allows consumers to deduplicate) reduces the duplicate rate to realistically manageable levels (only on producer restart).\n> - **Consequences**:\n>     - **Positive**: Simpler implementation, better performance, and a well-understood reliability model.\n>     - **Negative**: Downstream consumers **must** be designed to be idempotent or include a deduplication layer based on the event's deterministic ID.\n>     - **Mitigation**: The design provides the `transactionId` and a per-transaction sequence number within `ChangeEvent`, giving consumers the necessary information for robust deduplication.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| At-Least-Once with Idempotent Producer | Simpler, performant, prevents *some* duplicates, aligns with common consumer patterns. | Consumers must handle duplicates from producer restarts. | **Yes** |\n| Transactional/Exactly-Once | Strongest guarantee, no duplicates. | High complexity, performance overhead, harder to debug and recover. | No |\n| At-Most-Once | Lowest latency, simplest. | Risk of data loss, unacceptable for CDC. | No |\n\n### 7.5 ADR: Topic Partitioning Strategy\n\n> **Decision: Partition by Table and Primary Key Hash**\n> - **Context**: Events must be published to Kafka topics in a way that preserves order for changes to the same database row while allowing parallel consumption across different rows. The topic structure must also be manageable and align with consumer needs.\n> - **Options Considered**:\n>     1.  **Single Topic, Partition by Table+PK Hash**: All events go to one topic named `cdc.events`. Partitions are assigned by hashing a key composed of `tableName:primaryKey`.\n>     2.  **Topic-per-Table, Partition by PK Hash**: Each database table has a dedicated Kafka topic (e.g., `cdc.users`, `cdc.orders`). Partitions within that topic are assigned by hashing the primary key.\n>     3.  **Single Partition per Table**: Each table maps to a single Kafka partition (or a single topic with one partition). Guarantees order for all events in a table but offers zero parallelism.\n> - **Decision**: Implement **Topic-per-Table, Partition by PK Hash** (Option 2).\n> - **Rationale**:\n>     - **Clean Separation of Concerns**: Consumers can subscribe only to the tables they care about, simplifying their filter logic and reducing unnecessary data transfer.\n>     - **Independent Scaling**: Consumer groups can scale independently per table. A backlog in the `orders` topic doesn't affect consumers of the `users` topic.\n>     - **Administrative Flexibility**: Topic-level settings (retention policy, compaction, partitions) can be tailored to the data characteristics of each table.\n>     - **Preserves Ordering**: Hashing the primary key within a table-specific topic guarantees order per row, which is the core requirement.\n> - **Consequences**:\n>     - **Positive**: Clean architecture, better scalability, and operational flexibility.\n>     - **Negative**: Requires dynamic topic creation/validation (or pre-creation) as new tables are added to the source database. Slightly more complex management.\n>     - **Mitigation**: The `EventStreamer` can include logic to check if a topic exists on first use and create it with predefined settings (e.g., partition count from `KafkaConfig.partitionsPerTable`).\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Topic-per-Table, Partition by PK Hash | Clean separation, independent scaling, preserves order. | Requires topic management. | **Yes** |\n| Single Topic, Partition by Table+PK Hash | Single topic to manage. | All consumers read all data, harder to manage ACLs and quotas. | No |\n| Single Partition per Table | Trivial to implement, strong order. | No parallelism, severe throughput bottleneck. | No |\n\n### 7.6 Common Pitfalls in Event Delivery\n\n⚠️ **Pitfall: Committing the Offset Before Successful Delivery**\n- **Description**: The `LogConnector` advances its persisted LSN (offset) immediately after passing a `ChangeEvent` to the `EventStreamer.publish()` method, *before* receiving the Kafka acknowledgment.\n- **Why it's Wrong**: If the CDC process crashes after the offset is saved but before Kafka acknowledges the event, the event is lost forever. On restart, the `LogConnector` will resume from the later LSN, skipping the lost event.\n- **Fix**: **Always advance the persisted source offset only after the corresponding event is acknowledged by Kafka.** This is implemented in the producer callback, updating `lastAckedLsnBySlot`, which is then periodically persisted.\n\n⚠️ **Pitfall: Ignoring Consumer Lag (No Backpressure)**\n- **Description**: The `EventStreamer` publishes events as fast as the database produces them, regardless of how quickly consumers are processing them.\n- **Why it's Wrong**: If consumers fall behind (e.g., due to a bug or downstream system slowdown), the Kafka topic partitions will fill up. This can cause the producer to block or fail, and eventually lead to the `LogConnector` being unable to read new transaction log segments because the database's retention policy forces them to be deleted—**resulting in permanent data loss**.\n- **Fix**: **Implement a backpressure feedback loop.** Monitor consumer lag (using Kafka's end-offset and consumer group offset APIs). When lag exceeds a configured threshold, pause the `EventStreamer.publish()` method, which will eventually cause the `LogConnector` to stop reading and applying backpressure all the way to the database log reader.\n\n⚠️ **Pitfall: Partitioning by Table Only**\n- **Description**: Using only `tableName` as the Kafka partition key (e.g., all `users` events go to partition 1, all `orders` to partition 2).\n- **Why it's Wrong**: This destroys ordering guarantees for individual rows. Two updates to `user_id=123` could be processed by different consumer instances working on different partitions, leading to race conditions and stale data.\n- **Fix**: **Always include the primary key value in the partition key.** The key should be `tableName:primaryKeyHash`. This ensures all events for a specific row are ordered within a single partition.\n\n⚠️ **Pitfall: Not Handling Producer Buffer Full Exceptions**\n- **Description**: Relying on the default behavior when the Kafka producer's internal memory buffer (`buffer.memory`) is full.\n- **Why it's Wrong**: The default behavior for the `KafkaProducer.send()` method when the buffer is full is to block indefinitely. This can cause a complete stall of the CDC pipeline without clear logging.\n- **Fix**: **Configure a reasonable `max.block.ms`** (e.g., 60 seconds) on the Kafka producer. This will cause the `send()` method to throw a `TimeoutException` after this period, which the `EventStreamer` can catch, log as a critical error, and trigger a graceful shutdown or alert, rather than hanging silently.\n\n### 7.7 Implementation Guidance\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Message Broker | Apache Kafka (single broker for dev) | Apache Kafka Cluster (with replication) |\n| Client Library | `kafka-clients` (Java) | Confluent Kafka (includes Schema Registry client) |\n| Monitoring | Log-based lag detection | Kafka Metrics Reporter + Prometheus/Grafana |\n| Serialization | JSON (with schema ID) | Apache Avro (with Schema Registry integration) |\n\n**B. Recommended Project File Structure:**\n\n```\ncdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── core/\n│   │   ├── AppConfig.java\n│   │   ├── ChangeEvent.java\n│   │   └── ...\n│   ├── streaming/\n│   │   ├── EventStreamer.java          # This component's main class\n│   │   ├── KafkaEventPublisher.java    # Kafka-specific implementation\n│   │   ├── StreamerMetrics.java        # Lag monitoring & backpressure logic\n│   │   └── DeliveryCallback.java       # Kafka producer callback handler\n│   ├── serialization/\n│   │   └── EventSerializer.java\n│   └── Main.java\n└── config/\n    └── application.yaml\n```\n\n**C. Infrastructure Starter Code (Kafka Producer Setup):**\n\n```java\npackage com.cdc.streaming;\n\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.common.serialization.StringSerializer;\nimport java.util.Properties;\n\npublic class KafkaProducerFactory {\n\n    public static KafkaProducer<String, byte[]> createProducer(AppConfig config) {\n        Properties props = new Properties();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, config.getKafka().getBootstrapServers());\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName()); // We serialize to bytes ourselves\n\n        // Critical reliability settings\n        props.put(ProducerConfig.ACKS_CONFIG, config.getKafka().getAcks()); // \"all\" for strongest guarantee\n        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, config.getKafka().isEnableIdempotence()); // true\n        props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE); // Retry forever\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5); // Required for idempotence when retries > 0\n\n        // Backpressure & memory settings\n        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); // 32 MB default\n        props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 60000); // Throw exception after 1 min if buffer full\n\n        // Batching for throughput (can be tuned)\n        props.put(ProducerConfig.LINGER_MS_CONFIG, 20);\n        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);\n\n        return new KafkaProducer<>(props);\n    }\n}\n```\n\n**D. Core Logic Skeleton Code:**\n\n```java\npackage com.cdc.streaming;\n\nimport com.cdc.core.*;\nimport org.apache.kafka.clients.producer.*;\nimport java.util.List;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\npublic class KafkaEventPublisher implements EventStreamer {\n    private KafkaProducer<String, byte[]> producer;\n    private ConcurrentHashMap<String, ChangeEvent> pendingEvents;\n    private ConcurrentHashMap<String, String> lastAckedLsnBySlot;\n    private AtomicBoolean backpressureSignal;\n    private StreamerMetrics metrics;\n\n    @Override\n    public void initialize(AppConfig config) {\n        // TODO 1: Create Kafka producer using KafkaProducerFactory.createProducer(config)\n        // TODO 2: Initialize concurrent maps for pendingEvents and lastAckedLsnBySlot\n        // TODO 3: Initialize backpressureSignal (set to false)\n        // TODO 4: Initialize StreamerMetrics (start a scheduled thread to check consumer lag)\n        // TODO 5: Load last persisted LSNs from OffsetStore and populate lastAckedLsnBySlot\n    }\n\n    @Override\n    public void publish(List<ChangeEvent> events) {\n        for (ChangeEvent event : events) {\n            // TODO 1: Check backpressureSignal. If true, loop with Thread.sleep(100) until false.\n            // TODO 2: Generate partition key: sourceTable + \":\" + hash(primaryKeyValues)\n            // TODO 3: Retrieve SchemaVersion for event.getSchemaVersionId() from cache/registry\n            // TODO 4: Serialize event: byte[] payload = EventSerializer.serialize(event, schemaVersion)\n            // TODO 5: Create ProducerRecord with topic (e.g., \"cdc.\" + event.getSourceTable()), partition key, payload\n            // TODO 6: Generate a deterministic event ID (e.g., event.getTransactionId() + \":\" + sequence) and add to pendingEvents map\n            // TODO 7: Call producer.send(record, new DeliveryCallback(eventId, event, this)) with custom callback\n            // TODO 8: Increment in-flight event count; if it exceeds a limit (e.g., 10,000), apply backpressure.\n        }\n    }\n\n    // Inner class for handling Kafka acknowledgment\n    private static class DeliveryCallback implements Callback {\n        private final String eventId;\n        private final ChangeEvent event;\n        private final KafkaEventPublisher publisher;\n\n        DeliveryCallback(String eventId, ChangeEvent event, KafkaEventPublisher publisher) {\n            this.eventId = eventId;\n            this.event = event;\n            this.publisher = publisher;\n        }\n\n        @Override\n        public void onCompletion(RecordMetadata metadata, Exception exception) {\n            if (exception == null) {\n                // TODO 1: Remove eventId from publisher.pendingEvents\n                // TODO 2: Update publisher.lastAckedLsnBySlot for event's slot.\n                //         Important: Only advance the LSN if this event's LSN is the next in sequence for its slot.\n                //         This may require maintaining an ordered list or heap of pending LSNs per slot.\n                // TODO 3: Decrement publisher.inFlightEventCount\n                // TODO 4: Periodically (e.g., every 1000 events), persist the map lastAckedLsnBySlot to OffsetStore.\n            } else {\n                // TODO 1: Log severe error: \"Failed to publish event after retries: \" + exception\n                // TODO 2: Mark publisher health as DEGRADED or STOPPED\n                // TODO 3: Trigger an alert (e.g., via logging or metrics)\n                // The event remains in pendingEvents. A system restart will replay it.\n            }\n        }\n    }\n\n    @Override\n    public void stop() {\n        // TODO 1: Set a flag to stop accepting new publish() calls\n        // TODO 2: Call producer.flush() to wait for all pending sends\n        // TODO 3: After flush, persist final lastAckedLsnBySlot to OffsetStore\n        // TODO 4: Call producer.close()\n        // TODO 5: Shutdown the metrics monitoring thread\n    }\n}\n```\n\n**E. Java-Specific Hints:**\n- Use `java.util.concurrent.ConcurrentHashMap` for thread-safe maps like `pendingEvents`.\n- The Kafka `Producer.send()` callback executes on a **Kafka producer I/O thread**. Keep callback logic lightweight to avoid blocking network I/O. Offload complex logic (like updating complex ordered LSN structures) to a separate single-threaded executor if needed.\n- Use `AtomicBoolean` for the `backpressureSignal` to ensure safe visibility across threads.\n- For periodic tasks (like lag monitoring), use a `ScheduledExecutorService` rather than a plain `Thread.sleep` loop.\n\n**F. Milestone Checkpoint (Verifying Event Delivery):**\n1.  **Run Integration Test:** Start a local Kafka broker (e.g., using Docker). Configure the CDC pipeline to connect to it. Perform INSERT/UPDATE/DELETE operations on the source database.\n2.  **Expected Output:** Use the Kafka console consumer to inspect the topic: `kafka-console-consumer --bootstrap-server localhost:9092 --topic cdc.users --from-beginning`. You should see JSON (or Avro) formatted change events.\n3.  **Verify Ordering:** Update the same database row multiple times in quick succession. The events in the Kafka topic should appear in the exact same order.\n4.  **Verify At-Least-Once:** Kill the CDC process (`Ctrl+C`) while it's processing events, then restart it. Check the Kafka topic for duplicate events (should be present, confirming at-least-once). The pipeline should resume from the correct point with no events missing.\n5.  **Signs of Trouble:** If no events appear, check Kafka connection logs, producer configuration (especially `bootstrap.servers` and `topic` naming). If events are out of order, verify the partition key logic includes the primary key.\n\n**G. Debugging Tips Table:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| No events in Kafka topic. | 1. Producer not connected. 2. Topic doesn't exist. 3. All sends are failing. | Check logs for connection errors. Use `kafka-topics --list` to verify topic exists. Enable producer DEBUG logging. | Verify `bootstrap.servers`. Enable auto-topic creation or pre-create the topic. |\n| Events are lost after CDC restart. | Source LSN offset persisted **before** Kafka acknowledgment. | Check the persisted offset file. It should be **behind** the LSN of the last event seen in Kafka. | Ensure `lastAckedLsnBySlot` is only updated in the Kafka producer callback. |\n| CDC pipeline slows to a crawl or stops. | Backpressure is constantly active due to high consumer lag. | Check metrics/ logs for consumer lag alerts. Use `kafka-consumer-groups` command to check lag. | Investigate and fix the slow consumer. Temporarily increase `backpressureThresholdMs` for relief. |\n| Duplicate events for every change. | Idempotent producer disabled, or producer is restarted frequently without proper offset tracking. | Check config `enable.idempotence`. Look for producer restart logs. | Enable idempotence. Ensure the producer's `transactional.id` is stable across restarts if using transactions. |\n| `TimeoutException` from `producer.send()`. | Producer buffer is full, likely due to no backpressure and very slow consumers. | Check `inFlightEventCount` and consumer lag. The broker might also be slow. | Implement or tune backpressure. Increase `buffer.memory` or `max.block.ms` as a temporary measure. |\n\n\n> **Milestone(s):** Milestone 3 (Schema Evolution & Compatibility)\n\n## 8. Component: Schema Registry & Evolution\n\nIn a CDC system, the raw data flowing from transaction logs is just bytes without context. The **Schema Registry & Evolution** component provides the essential *context* by managing the structure of data — the table schemas — and ensuring that changes to that structure (like adding a column) don't silently break downstream consumers. It acts as a central authority for data shape, versioning schemas, validating changes, and notifying the system when the contract between producers and consumers evolves.\n\n### 8.1 Mental Model: The Contract Librarian\n\nThink of the Schema Registry as a **Librarian for Legal Contracts**. Every table in your source database has a \"contract\" (its schema) that defines the exact terms of data exchange: column names, data types, and optionality. Each time a developer runs an `ALTER TABLE`, they are proposing an amendment to this contract.\n\nThe Librarian's (Registry's) duties are:\n1.  **Archival**: Store every ratified version of every contract, meticulously labeled and indexed.\n2.  **Validation (Legal Review)**: When a new amendment (schema change) is proposed, the librarian checks it against compatibility rules (e.g., \"Can existing readers still understand data written under the new contract?\"). They reject breaking changes.\n3.  **Distribution of Updates**: Once a new version is accepted and stored, the librarian stamps a new version number on it and sends out a formal notification (a `SchemaChangeEvent`) to all interested parties (consumers) so they can update their local copies of the contract.\n4.  **Reference Service**: When a data packet (`ChangeEvent`) arrives at a consumer, it carries a reference to a specific contract version (e.g., \"schema v2\"). The consumer can ask the librarian, \"Please give me the full text of contract v2 for table `users`\" to correctly interpret the data.\n\nThis model highlights the component's core responsibilities: stateful storage, rule-based validation, and change notification, which are critical for a system where data producers (the database) and consumers (applications) evolve independently.\n\n### 8.2 Interface and Versioning\n\nThe Schema Registry exposes a well-defined API for schema lifecycle management. Its primary data structure is the `SchemaVersion`, which encapsulates a snapshot of a table's structure at a point in time.\n\n**Core Data Structure: `SchemaVersion`**\n\nThis object represents a single, immutable version of a table's schema. Once registered, it is never modified.\n\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `schemaId` | `String` | A globally unique identifier for this specific schema version. Typically formatted as `<tableName>-v<versionNumber>` (e.g., `public.users-v3`). |\n| `tableName` | `String` | The fully-qualified name of the source database table (e.g., `public.users`, `inventory.products`). |\n| `version` | `Integer` | A monotonically increasing integer version number, scoped to the `tableName`. Starts at 1. |\n| `columnDefinitions` | `Map<String, ColumnType>` | The core schema definition. Maps column names to their type metadata. |\n| `compatibilityMode` | `String` | The compatibility rule applied when this version was registered (e.g., `BACKWARD`, `FORWARD`, `FULL`). This influences which future changes are allowed. |\n\n**Supporting Data Structure: `ColumnType`**\n\nThis nested object provides detailed metadata for a single column, essential for accurate serialization and compatibility checking.\n\n| Field | Type | Description |\n| :--- | :--- | :--- |\n| `sqlType` | `String` | The database-native type name (e.g., `VARCHAR(255)`, `INT`, `TIMESTAMP WITH TIME ZONE`). |\n| `javaClass` | `Class<?>` | The corresponding Java class for runtime serialization/deserialization (e.g., `String.class`, `Long.class`). |\n| `nullable` | `boolean` | Indicates if the column can contain `NULL` values. |\n| `defaultValue` | `Object` | The default value for the column, used during deserialization if a field is missing in old data. Can be `null`. |\n\n**Registry Interface (`SchemaRegistry`)**\n\nThe component's functionality is accessed via the following interface. This defines the contract for both the registry implementation and its clients (like the `ChangeEventBuilder`).\n\n| Method | Parameters | Returns | Description |\n| :--- | :--- | :--- | :--- |\n| `registerSchema` | `tableName: String`, `newColumnDefs: Map<String, ColumnType>`, `compatibilityMode: String` | `SchemaVersion` | The core registration workflow. Takes a proposed new schema for a table, validates it against the latest registered version using the specified `compatibilityMode`, and if valid, stores it as a new version, incrementing the version number. Returns the newly registered `SchemaVersion`. |\n| `getSchemaById` | `schemaId: String` | `SchemaVersion` | Retrieves a specific schema version by its unique `schemaId`. Used by consumers to deserialize a `ChangeEvent` that references a particular version. |\n| `getLatestSchema` | `tableName: String` | `SchemaVersion` | Returns the most recent schema version for a given table. Used by the `ChangeEventBuilder` to attach schema info to new events and by the registry's own validation logic. |\n| `checkCompatibility` | `newColumnDefs: Map<String, ColumnType>`, `existingSchema: SchemaVersion`, `mode: String` | `boolean` | Performs a dry-run compatibility check. Returns `true` if the proposed `newColumnDefs` are compatible with the `existingSchema` according to the rules of the specified `mode`. Does not register a new version. |\n| `getSchemaHistory` | `tableName: String` | `List<SchemaVersion>` | Returns all schema versions for a table, in chronological order. Useful for audit and migration planning. |\n\n**Versioning and Lifecycle Flow**\n1.  **Initial Registration**: When the CDC pipeline starts for the first time, it must fetch the current schema of each captured table from the source database and call `registerSchema` with a special flag (e.g., `INITIAL` mode) to create version 1.\n2.  **Event Tagging**: Every `ChangeEvent` produced by the `ChangeEventBuilder` includes a `schemaVersionId` field (e.g., `public.users-v2`), creating a hard link between the data and its schema definition.\n3.  **Schema Change Detection**: The `LogConnector` and `Parser` detect Data Definition Language (DDL) events (e.g., `ALTER TABLE...`). These are passed to the registry as a proposal for a new schema.\n4.  **Validation & Registration**: The registry performs `checkCompatibility`. If it passes, `registerSchema` is called, creating a new `SchemaVersion`. If it fails, the change is rejected, and an alert is raised (the DDL may need to be re-evaluated).\n5.  **Change Notification**: Upon successful registration, the registry (or the `ChangeEventBuilder`) emits a special `SchemaChangeEvent` to a dedicated topic. This event contains the old and new `schemaId`, allowing consumers to lazily fetch the new schema when they encounter data with the new version ID.\n\n### 8.3 ADR: Choosing a Schema Compatibility Mode\n\n> **Decision: Enforce Backward Compatibility by Default**\n> - **Context**: The CDC system serves multiple, independently deployed consumer applications. Their code deserializes `ChangeEvent` data based on a known schema. When the source database schema evolves, we must decide which types of changes are safe to allow without breaking existing consumers.\n> - **Options Considered**:\n>     1.  **Backward Compatibility (Recommended Default)**: New schema can read data written with the old schema. This means consumers using the *new* schema version can still process events written with an *old* schema. In practice, you can only add optional fields (nullable or with defaults) and cannot remove/rename fields or make required fields optional.\n>     2.  **Forward Compatibility**: Old schema can read data written with the new schema. Consumers using an *old* schema version can process events written with a *new* schema. This allows removing fields and making optional fields required, but prevents adding new required fields.\n>     3.  **Full Compatibility**: A combination of both backward and forward compatibility. The strictest mode, allowing only safe changes like adding/removing optional fields.\n>     4.  **No Compatibility Checking**: Allow any schema change, accepting that consumers may break. This is simple but operationally dangerous.\n> - **Decision**: The primary compatibility mode for the Schema Registry will be **Backward Compatibility**.\n> - **Rationale**:\n>     - **Consumer Deployment Lag**: In a real-world scenario, it is far more common for consumers to be updated *after* the producer (database) changes. Backward compatibility ensures that an updated consumer (with new logic for a new column) can still process old data in the pipeline during a rolling deployment, preventing downtime.\n>     - **Practical Schema Evolution**: The most common, safe schema change is \"adding a new nullable column with a default value.\" This is backward-compatible and matches typical application evolution (e.g., adding a `last_login_at` timestamp column).\n>     - **Industry Practice**: Systems like Apache Avro and Confluent Schema Registry default to backward compatibility for similar reasons, making it a well-understood pattern.\n> - **Consequences**:\n>     - **Enables Safe Rolling Updates**: Producers (database) can evolve first. New consumers can be deployed later, reading both old and new events seamlessly.\n>     - **Restricts Certain Changes**: Renaming or deleting a column, or changing a column's data type in an incompatible way (e.g., `INT` to `VARCHAR`) will be rejected by the registry. These changes require a coordinated \"big bang\" update or a more complex migration strategy (like writing to a new table/topic).\n>     - **Schema Proliferation**: Over time, many schema versions will accumulate as columns are added. This is managed by the registry and is a normal part of operation.\n\n**Compatibility Mode Comparison**\n\n| Mode | Allows (Example) | Disallows (Example) | Best For |\n| :--- | :--- | :--- | :--- |\n| **Backward** (OUR CHOICE) | Adding a nullable column (`ALTER TABLE users ADD COLUMN middle_name VARCHAR(100) NULL;`). | Removing a column, changing `VARCHAR(100)` to `VARCHAR(50)`. | Evolving producers first; the most common, safe evolution pattern. |\n| **Forward** | Removing a column, making a nullable column `NOT NULL` (if default exists). | Adding a new required (`NOT NULL`) column without a default. | Evolving consumers first; less common in database-driven CDC. |\n| **Full** | Adding an optional column, removing an optional column. | Any change that breaks either backward or forward compatibility. | Highly controlled environments where all changes must be non-breaking. |\n| **None** | Any DDL statement. | (No validation). | Prototyping or environments where breaking changes are acceptable. |\n\n### 8.4 Common Pitfalls in Schema Evolution\n\n⚠️ **Pitfall: Silent Data Loss from Type Narrowing**\n*   **The Mistake**: Allowing an `ALTER TABLE` that changes a column's type in a lossy way, such as `BIGINT` to `INT` or `VARCHAR(255)` to `VARCHAR(10)`, without proper validation.\n*   **Why It's Wrong**: The CDC system may successfully serialize the change event, but when a consumer with the new schema deserializes an old value (e.g., the number 5,000,000,000 into an `INT` field), it will overflow or truncate, corrupting data silently. This violates the CDC system's core promise of faithful replication.\n*   **How to Fix**: The `checkCompatibility` logic must include **type coercion safety checks**. It should understand type families (e.g., integer types, string types) and only allow changes where every possible value in the old type can be represented in the new type without loss. For complex changes, the check should fail, forcing the use of a multi-step migration (e.g., add new column, backfill, switch consumers, drop old column).\n\n⚠️ **Pitfall: Treating the Registry as Stateless**\n*   **The Mistake**: Implementing the `SchemaRegistry` as an in-memory component within the CDC pipeline process, losing all registered schemas on restart.\n*   **Why It's Wrong**: On restart, the pipeline cannot correctly tag events with `schemaVersionId`, and consumers cannot resolve historical event schemas. This breaks the entire versioning system.\n*   **How to Fix**: The registry's state (the `SchemaVersion` objects) must be **durably persisted**. The implementation guidance provides a simple file-based starter. For production, this would be backed by a database (like PostgreSQL) or a dedicated service (like Confluent Schema Registry).\n\n⚠️ **Pitfall: Ignoring DDL Events in Transaction Logs**\n*   **The Mistake**: The `LogParser` is configured to only parse `INSERT`, `UPDATE`, `DELETE` (DML) events, silently skipping `CREATE TABLE`, `ALTER TABLE` (DDL) events.\n*   **Why It's Wrong**: The pipeline's view of table schema becomes stale. Newly added columns will not appear in `ChangeEvent`s, and the registry will not know about the change, causing a mismatch between the actual database state and the CDC stream.\n*   **How to Fix**: Ensure the `LogParser` implementation for your database (PostgreSQL logical decoding, MySQL binlog) is configured to capture DDL events. These events must be routed to the `SchemaRegistry` for processing, triggering the registration flow shown in the diagram.\n\n![Flowchart: Handling a Schema Change (ALTER TABLE)](./diagrams/schema-evolution-flowchart.svg)\n\n⚠️ **Pitfall: Forgetting to Notify Consumers of Schema Changes**\n*   **The Mistake**: The registry registers a new schema version but does not emit any notification. Consumers only discover the new schema when they receive a `ChangeEvent` with an unknown `schemaVersionId` and have to query the registry reactively.\n*   **Why It's Wrong**: While the system still works, it introduces latency and uncertainty. Consumers may fail temporarily if they cannot fetch the schema. Proactive notification allows consumers to pre-fetch and warm up their deserializers, leading to smoother operations.\n*   **How to Fix**: The `registerSchema` method, upon successful registration, should trigger the publication of a `SchemaChangeEvent` to a dedicated Kafka topic (e.g., `_schemas`). Consumers subscribe to this topic to be alerted of relevant changes.\n\n### 8.5 Implementation Guidance\n\nThis section provides concrete Java code to build a functional, file-based `SchemaRegistry`. It's designed for learning and can be replaced with a more robust backend (like HTTP-based Confluent Schema Registry) for production.\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option (Learning) | Advanced Option (Production) |\n| :--- | :--- | :--- |\n| **Schema Storage** | Local JSON files (one per `SchemaVersion`). Easy to inspect and debug. | Dedicated service (Confluent Schema Registry) or a database table (PostgreSQL). Provides high availability, REST API, and advanced features. |\n| **Compatibility Logic** | Custom Java rules implementing backward compatibility checks. | Leverage the Avro or Protobuf schema compatibility libraries, which are battle-tested. |\n| **Change Notification** | Writing a special `SchemaChangeEvent` to the main CDC Kafka topic. | Using a dedicated `_schemas` topic and the Confluent Schema Registry's REST hooks. |\n\n**B. Recommended File/Module Structure**\n\n```\ncdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── Main.java\n│   ├── pipeline/\n│   │   ├── CdcPipeline.java\n│   │   ├── LogConnector.java\n│   │   ├── ChangeEventBuilder.java\n│   │   └── EventStreamer.java\n│   ├── schema/\n│   │   ├── SchemaRegistry.java           # Interface\n│   │   ├── FileBasedSchemaRegistry.java  # Simple implementation\n│   │   ├── SchemaVersion.java            # Data class\n│   │   ├── ColumnType.java               # Data class\n│   │   ├── CompatibilityChecker.java     # Logic for validation\n│   │   └── SchemaChangeEvent.java        # Notification POJO\n│   ├── serialization/\n│   │   └── EventSerializer.java\n│   └── config/\n│       └── AppConfig.java\n├── src/main/resources/\n│   └── application.yaml\n└── schemas/                              # Directory for storing schema JSON files\n    ├── public.users-v1.json\n    ├── public.users-v2.json\n    └── inventory.products-v1.json\n```\n\n**C. Infrastructure Starter Code**\n\nFirst, here is the complete, ready-to-use `ColumnType` and `SchemaVersion` data classes.\n\n```java\n// File: src/main/java/com/cdc/schema/ColumnType.java\npackage com.cdc.schema;\n\npublic class ColumnType {\n    private final String sqlType;\n    private final Class<?> javaClass;\n    private final boolean nullable;\n    private final Object defaultValue;\n\n    public ColumnType(String sqlType, Class<?> javaClass, boolean nullable, Object defaultValue) {\n        this.sqlType = sqlType;\n        this.javaClass = javaClass;\n        this.nullable = nullable;\n        this.defaultValue = defaultValue;\n    }\n\n    // Getters, equals, hashCode, toString omitted for brevity.\n}\n```\n\n```java\n// File: src/main/java/com/cdc/schema/SchemaVersion.java\npackage com.cdc.schema;\n\nimport java.util.Map;\n\npublic class SchemaVersion {\n    private final String schemaId;\n    private final String tableName;\n    private final int version;\n    private final Map<String, ColumnType> columnDefinitions;\n    private final String compatibilityMode;\n\n    public SchemaVersion(String schemaId, String tableName, int version,\n                         Map<String, ColumnType> columnDefinitions, String compatibilityMode) {\n        this.schemaId = schemaId;\n        this.tableName = tableName;\n        this.version = version;\n        this.columnDefinitions = Map.copyOf(columnDefinitions); // Defensive copy\n        this.compatibilityMode = compatibilityMode;\n    }\n\n    // Getters, equals, hashCode, toString omitted for brevity.\n}\n```\n\n**D. Core Logic Skeleton Code**\n\nBelow is the `FileBasedSchemaRegistry` implementation skeleton. The TODOs guide you through the key algorithms for registration and compatibility checking.\n\n```java\n// File: src/main/java/com/cdc/schema/FileBasedSchemaRegistry.java\npackage com.cdc.schema;\n\nimport com.cdc.config.AppConfig;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.*;\n\npublic class FileBasedSchemaRegistry implements SchemaRegistry {\n    private final Path schemaStorageDir;\n    private final ObjectMapper objectMapper = new ObjectMapper();\n    private final Map<String, SchemaVersion> cache = new HashMap<>();\n\n    public FileBasedSchemaRegistry(AppConfig config) {\n        this.schemaStorageDir = Paths.get(config.getSchemaConfig().getRegistryUrl());\n        try {\n            Files.createDirectories(schemaStorageDir);\n        } catch (IOException e) {\n            throw new RuntimeException(\"Failed to create schema storage directory\", e);\n        }\n        loadExistingSchemasIntoCache();\n    }\n\n    private void loadExistingSchemasIntoCache() {\n        // TODO 1: List all JSON files in schemaStorageDir\n        // TODO 2: For each file, read its contents and deserialize into a SchemaVersion object using objectMapper.readValue(file, SchemaVersion.class)\n        // TODO 3: Put each deserialized SchemaVersion into the 'cache' map, keyed by its schemaId\n    }\n\n    @Override\n    public SchemaVersion registerSchema(String tableName,\n                                        Map<String, ColumnType> newColumnDefs,\n                                        String compatibilityMode) {\n        // TODO 1: Get the latest existing schema for this table (call getLatestSchema(tableName))\n        SchemaVersion latest = getLatestSchema(tableName);\n\n        // TODO 2: If a latest schema exists, perform compatibility check:\n        //    if (!checkCompatibility(newColumnDefs, latest, compatibilityMode)) {\n        //        throw new SchemaCompatibilityException(\"Proposed schema is not compatible\");\n        //    }\n        // TODO 3: Determine the new version number.\n        //    int newVersion = (latest == null) ? 1 : latest.getVersion() + 1;\n        // TODO 4: Create a new SchemaVersion object with a generated schemaId (e.g., tableName + \"-v\" + newVersion)\n        // TODO 5: Persist this new SchemaVersion to a file (e.g., schemaId + \".json\") using objectMapper.writeValue\n        // TODO 6: Update the in-memory cache with the new SchemaVersion\n        // TODO 7: (Optional) Emit a SchemaChangeEvent notification (e.g., via a callback or event bus)\n        // TODO 8: Return the newly created SchemaVersion object\n\n        return null; // Placeholder\n    }\n\n    @Override\n    public SchemaVersion getSchemaById(String schemaId) {\n        // TODO 1: Check the in-memory cache first. Return if found.\n        // TODO 2: If not in cache, attempt to load from the corresponding JSON file in schemaStorageDir.\n        // TODO 3: If the file doesn't exist, throw a SchemaNotFoundException.\n        // TODO 4: Cache and return the loaded schema.\n        return null; // Placeholder\n    }\n\n    @Override\n    public SchemaVersion getLatestSchema(String tableName) {\n        // TODO 1: Filter the cache values (or files) to find all schemas for the given tableName.\n        // TODO 2: Among those, find the one with the highest version number.\n        // TODO 3: Return it (or null if none exist).\n        return null; // Placeholder\n    }\n\n    @Override\n    public boolean checkCompatibility(Map<String, ColumnType> newColumnDefs,\n                                      SchemaVersion existingSchema,\n                                      String mode) {\n        if (existingSchema == null) return true; // First version is always compatible\n        if (\"NONE\".equals(mode)) return true;\n\n        Map<String, ColumnType> oldDefs = existingSchema.getColumnDefinitions();\n\n        // TODO: Implement backward compatibility rules (the default choice from ADR 8.3)\n        // Rule 1: New columns can be added only if they are nullable (nullable==true) OR have a non-null defaultValue.\n        //         Iterate through newColumnDefs keys not present in oldDefs and check this condition.\n        // Rule 2: Existing columns cannot be removed. All keys in oldDefs must be present in newColumnDefs.\n        // Rule 3: The data type of an existing column cannot change in an incompatible way.\n        //         For columns present in both, compare ColumnType. For learning, a simple equals check on sqlType is a start.\n        //         For advanced implementation, you would need a type coercion matrix (e.g., INT -> BIGINT is safe, but not vice-versa).\n        // TODO: If all rules pass, return true. If any rule fails, return false.\n\n        return false; // Placeholder\n    }\n\n    @Override\n    public List<SchemaVersion> getSchemaHistory(String tableName) {\n        // TODO: Filter cache values for the tableName, sort by version ascending, and return as a List.\n        return List.of(); // Placeholder\n    }\n}\n```\n\n**E. Language-Specific Hints**\n*   **Immutability**: Make `SchemaVersion` and `ColumnType` immutable (final fields, no setters). This guarantees that a registered schema can never be altered after the fact, which is crucial for consistency.\n*   **Jackson for JSON**: Use the Jackson library (`ObjectMapper`) for simple JSON serialization/deserialization of `SchemaVersion` objects to files. Remember to register the `JavaTimeModule` if you add timestamp fields.\n*   **Concurrent Access**: The `FileBasedSchemaRegistry` may be accessed by multiple threads (e.g., the main pipeline thread and a consumer's fetch request). Use `ConcurrentHashMap` for the cache and consider synchronization for the `registerSchema` method to prevent race conditions when creating a new version.\n\n**F. Milestone Checkpoint**\nTo verify your Schema Registry implementation for Milestone 3:\n1.  **Run Unit Tests**: Create a JUnit test class `FileBasedSchemaRegistryTest`.\n    ```bash\n    mvn test -Dtest=FileBasedSchemaRegistryTest\n    ```\n2.  **Expected Behavior**:\n    *   Test `registerSchema` for a new table creates `my_table-v1.json`.\n    *   Test adding a nullable column passes compatibility and creates `my_table-v2.json`.\n    *   Test removing a column fails compatibility with a clear exception.\n    *   Test `getSchemaById` correctly retrieves a previously saved version.\n3.  **Integration Verification**: Start your CDC pipeline with a test PostgreSQL database. Run a simple `ALTER TABLE ADD COLUMN ...`. Check that:\n    *   A new schema file is created in the `schemas/` directory.\n    *   Subsequent `ChangeEvent`s from that table include the new `schemaVersionId`.\n    *   (Optional) A `SchemaChangeEvent` is published to your Kafka topic.\n\n**G. Debugging Tips**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n| :--- | :--- | :--- | :--- |\n| Consumer fails with \"Unknown schemaId\" | The registry did not persist the schema, or the `schemaVersionId` in the event is malformed. | Check the `schemas/` directory for the expected JSON file. Inspect the `schemaVersionId` field in a raw `ChangeEvent` using `kafka-console-consumer`. | Ensure `registerSchema` successfully writes the file and that the `ChangeEventBuilder` uses the `schemaId` returned by the registry. |\n| New column data is missing from events after `ALTER TABLE`. | DDL event not being captured or processed. | Check the logs of your `LogConnector`/`Parser` for signs of DDL parsing. Verify the database replication slot/output plugin is configured for DDL. | Ensure your database-specific parser is set up to decode DDL statements and route them to `SchemaRegistry.registerSchema`. |\n| Compatibility check passes but consumers break. | The compatibility rules are too lenient (e.g., missing type coercion check). | Manually compare the `sqlType` of a changed column between old and new schema files. Test if a sample extreme value (e.g., a large number, long string) would be lost. | Strengthen the `checkCompatibility` logic, particularly the type comparison, to be more conservative. Implement a type safety matrix. |\n| Registry loses all schemas on restart. | Schemas are only stored in memory (`cache`) not persisted to disk. | The `schemas/` directory is empty or missing. The `loadExistingSchemasIntoCache` method might be failing silently. | Check for IO exceptions during `loadExistingSchemasIntoCache`. Add logging. Ensure the `schemaStorageDir` path in config is correct and writable. |\n\n\n## 9. Interactions and Data Flow\n\n> **Milestone(s):** Milestone 1 (log parsing and event construction), Milestone 2 (event streaming and delivery), Milestone 3 (schema evolution handling)\n\nThis section traces the complete journey of a database change as it transforms from a binary log entry to a consumable event in a downstream system. Understanding these interaction flows is critical for debugging, performance optimization, and ensuring the system's reliability guarantees hold end-to-end. We'll examine two primary scenarios: normal data manipulation operations (INSERT, UPDATE, DELETE) and schema evolution events (ALTER TABLE).\n\n### 9.1 Normal Operation Flow\n\n**Mental Model: The Assembly Line Tour**\nImagine our CDC pipeline as a specialized factory assembly line. Raw materials (transaction log entries) arrive from the database warehouse. The first station (Log Connector) unpacks and inspects these materials. The second station (Change Event Builder) assembles the parts into finished products (ChangeEvents), ensuring all pieces from the same transaction box are grouped together. The third station (Event Streamer) packages and ships these products via a reliable courier service (Kafka), which guarantees delivery even if trucks break down. Finally, consumers receive these packages at their loading docks. This tour follows a single product from raw material to delivery.\n\nThe normal operation flow encompasses the most common path through the system: a data manipulation language (DML) operation like `INSERT INTO users (id, name) VALUES (123, 'Alice')` that successfully commits in the source database and must be delivered to downstream consumers.\n\n**End-to-End Sequence**\nThe following sequence diagram illustrates the complete flow. Each numbered step corresponds to a critical interaction described in detail below.\n\n![Sequence Diagram: Normal Event Delivery](./diagrams/event-delivery-sequence.svg)\n\n**Detailed Step-by-Step Walkthrough**\n\n1. **Database Transaction Commit**\n   - **Actor**: Application using the source database\n   - **Action**: The application issues a `COMMIT` statement for a transaction that includes one or more DML operations.\n   - **Database Internal**: The database's transaction manager ensures atomicity by writing all changes to the Write-Ahead Log (WAL) or binlog first, then marking the transaction as committed in the transaction log. This creates a durable record of the change with a unique **log sequence number (LSN)** that serves as a precise timestamp in the log stream.\n\n2. **Log Connector Polling and Capture**\n   - **Trigger**: The `LogConnector.workerThread` continuously polls the database's logical decoding interface (PostgreSQL's `pg_logical_slot_get_changes` or MySQL's binary log stream).\n   - **Mechanism**: Using the `lastProcessedLSN` from the `OffsetStore`, the connector requests all changes after that position. The database returns a batch of raw log entries in their internal binary format.\n   - **Data Transformation**: Each binary entry passes through the database-specific `LogParser`, which decodes it into a structured `RawLogEntry` containing the table name, operation type, and the actual row data (both old and new values for updates).\n   - **State Update**: The connector immediately persists the newest LSN to the `OffsetStore.save()` to record progress, ensuring no data loss if the connector crashes.\n\n3. **Change Event Construction**\n   - **Input**: The `LogConnector.getNextBatch()` returns a list of `RawLogEntry` objects to the main pipeline thread.\n   - **Transaction Grouping**: The pipeline calls `ChangeEventBuilder.processBatch()` with these entries. The builder maintains an `activeTransactions` map keyed by `transactionId`. As entries arrive, they're added to the appropriate `TransactionState.operations` list.\n   - **Deduplication**: For each operation, the builder checks the `lastOperationByKey` map within the transaction state. If a newer operation for the same primary key already exists in the same transaction (a \"hot row\" updated multiple times), it replaces the previous operation, ensuring only the final state is emitted.\n   - **Commit Detection**: When a `RawLogEntry` with a `COMMIT` marker is processed, the builder calls `buildEventsForTransaction()` for that transaction. This method:\n     1. Converts each `TransactionOperation` to a `ChangeEvent` with a `deterministic event ID` generated via `generateEventId()`.\n     2. Attaches the latest `SchemaVersion` for the table from the schema cache.\n     3. Sets the `commitTimestamp` from the transaction's commit record.\n   - **Output**: A list of complete `ChangeEvent` objects, ordered by their operation sequence within the transaction, is returned.\n\n4. **Event Serialization and Publishing**\n   - **Serialization**: Each `ChangeEvent` is serialized to bytes using `EventSerializer.serialize()`, which encodes the event data according to the schema (e.g., Avro, JSON with schema ID).\n   - **Partition Assignment**: The `KafkaEventPublisher` computes a partition key: typically `tableName + \":\" + primaryKeyHash`. This ensures all events for the same database row go to the same Kafka partition, preserving order.\n   - **Reliable Send**: The publisher calls `KafkaProducer.send()` with the serialized bytes, registering a `DeliveryCallback` for each event. The producer is configured with `idempotence=true` and `acks=all` to prevent duplicates and ensure writes are replicated.\n   - **In-flight Tracking**: The event is added to `pendingEvents` map (keyed by `eventId`) until the callback fires.\n\n5. **Kafka Acknowledgment and Offset Management**\n   - **Broker Replication**: Kafka leaders replicate the message to follower replicas based on the `replicationFactor`. Once all in-sync replicas acknowledge, the broker sends a success response.\n   - **Callback Execution**: The `DeliveryCallback.onCompletion()` is invoked by the producer thread. On success:\n     1. The event is removed from `pendingEvents`.\n     2. The `lastAckedLsnBySlot` for the replication slot is updated to the event's source LSN.\n     3. The `inFlightEventCount` is decremented.\n   - **Backpressure Signal**: If `inFlightEventCount` exceeds a threshold (e.g., 10,000), the `backpressureSignal` is set to `true`, causing the `LogConnector` to pause reading.\n\n6. **Consumer Processing**\n   - **Poll Loop**: Downstream consumers run a continuous poll loop, fetching events from their assigned Kafka partitions.\n   - **Deserialization**: Each consumer uses the `schemaVersionId` in the event (or the Confluent Schema Registry's wire format) to fetch the appropriate `SchemaVersion` and deserialize the bytes using `EventSerializer.deserialize()`.\n   - **Order Guarantee**: Because each partition is consumed by only one consumer in a group, and events for the same row are in the same partition, consumers process events per-row in exact commit order.\n   - **Offset Commit**: After successfully processing an event, the consumer commits its offset to Kafka (either synchronously or asynchronously). This commit acts as a checkpoint; if the consumer restarts, it resumes from this offset.\n\n7. **Monitoring and Health Feedback**\n   - **Lag Monitoring**: The `StreamerMetrics` component periodically checks consumer group lag via Kafka's AdminClient. If lag exceeds `lagThresholdMs`, alerts are triggered.\n   - **Health Status**: The `CdcPipeline.getHealthStatus()` aggregates status from all components: `LogConnector` connectivity, `EventStreamer` connection to Kafka, and schema registry accessibility. This status is exposed for dashboards and readiness probes.\n\n**Concrete Example Walkthrough: User Registration**\nLet's trace a concrete scenario where a user signs up on a website:\n\n1. **Application**: `INSERT INTO users (id, email, created_at) VALUES ('u123', 'alice@example.com', NOW()); COMMIT;`\n2. **PostgreSQL**: Writes the INSERT to WAL at position `0/18AB1234`, assigns transaction ID `tx789`.\n3. **Log Connector**: Polls replication slot `cdc_slot`, receives binary WAL entry for the INSERT. `PostgreSQLParser` decodes it to `RawLogEntry` with `operationType=OPERATION_INSERT`, `rowData={id:u123, email:alice@example.com, created_at:2023-10-01...}`.\n4. **Change Event Builder**: Creates `TransactionState` for `tx789`, adds the operation. Upon commit entry, builds `ChangeEvent` with `eventId=\"tx789-1\"`, `afterImage` containing the row, `schemaVersionId=\"users_v3\"`.\n5. **Event Streamer**: Serializes to Avro using schema `users_v3`. Partition key = hash of `users:u123` → partition 5 of topic `cdc.users`. Sends to Kafka.\n6. **Kafka**: Replicates to 3 brokers, acknowledges after all replicas persist.\n7. **Consumer**: Email service consumes from partition 5, deserializes event, triggers welcome email to `alice@example.com`, commits offset.\n8. **Monitoring**: Dashboard shows 0ms lag for email service consumer group.\n\n**Failure Recovery Scenario**\nIf the `KafkaEventPublisher` crashes after sending but before receiving acknowledgment, the event remains in `pendingEvents`. On restart, the publisher scans `pendingEvents` and compares each event's LSN with `lastAckedLsnBySlot`. For events with LSN greater than the last acknowledged, it re-sends them (idempotent producer prevents duplicates at the broker). This ensures at-least-once delivery despite crashes.\n\n### 9.2 Schema Change Flow\n\n**Mental Model: The Building Code Update**\nImagine a city's building department (Schema Registry) that maintains official blueprints for all structures. When a homeowner wants to add a new room (new column), they submit plans. The department checks if the addition violates any safety codes (compatibility rules) and, if approved, issues a new version of the blueprint. All future construction (new ChangeEvents) must use the new blueprint. Crucially, existing houses don't need immediate renovation—old blueprints still accurately describe them. The department also notifies all contractors (consumers) about the blueprint update so they can prepare to work with the new format.\n\nSchema changes (DDL operations like `ALTER TABLE`) present a unique challenge because they modify the very structure of the data being captured. The system must: 1) detect the schema change, 2) validate its compatibility, 3) register the new schema version, 4) notify consumers, and 5) ensure future events use the correct schema.\n\n**Schema Change Handling Flowchart**\nThe following flowchart depicts the decision process when a DDL event is encountered:\n\n![Flowchart: Handling a Schema Change (ALTER TABLE)](./diagrams/schema-evolution-flowchart.svg)\n\n**Step-by-Step DDL Handling Algorithm**\n\n1. **DDL Detection and Parsing**\n   - **Source**: The DDL statement appears in the transaction log just like DML operations. The `LogParser` identifies it by operation type (often `DDL` or via statement prefix like `ALTER`).\n   - **Parsing**: The parser extracts the table name, change type (ADD COLUMN, DROP COLUMN, ALTER COLUMN TYPE), and exact SQL statement.\n   - **Output**: A special `RawLogEntry` with `operationType=\"DDL\"` and the SQL statement in `rowData`.\n\n2. **Schema Extraction and Comparison**\n   - **Current Schema Fetch**: The `ChangeEventBuilder` upon receiving the DDL entry calls `SchemaRegistry.getLatestSchema(tableName)` to retrieve the current `SchemaVersion`.\n   - **New Schema Inference**: The system parses the DDL statement to infer the new column structure. For `ADD COLUMN email VARCHAR(255) NULL`, it adds the column to a copy of the current `columnDefinitions` with `nullable=true`.\n   - **Compatibility Check**: `SchemaRegistry.checkCompatibility()` is invoked with the new column definitions, current schema, and the configured `compatibilityMode` (typically `COMPATIBILITY_BACKWARD`).\n     - **Backward Compatibility Check Rules**:\n       1. New columns may be added only if they are nullable or have a default value.\n       2. Existing columns cannot be removed.\n       3. Column data types may only change to a wider or compatible type (e.g., `VARCHAR(100)` → `VARCHAR(255)` is allowed; reverse is not).\n       4. Column constraints may only be relaxed (e.g., `NOT NULL` → `NULL` allowed; reverse is not).\n\n3. **Compliance Decision and Action**\n   - **If Compatible**: The new schema is registered via `SchemaRegistry.registerSchema()`, which assigns a new version number (e.g., increments from v3 to v4) and persists it to storage. The schema cache is updated.\n   - **If Incompatible**: The change is rejected. The system logs an error and can either:\n     1. **Stop the pipeline** to prevent data loss (safe default).\n     2. **Continue with old schema** but flag the mismatch (requires manual intervention).\n     3. **Apply emergency schema override** via administrative API.\n\n4. **Schema Change Event Emission**\n   - **Notification Event**: Regardless of compatibility outcome, a special `SchemaChangeEvent` is constructed containing the table name, old and new schema IDs, DDL statement, and timestamp.\n   - **Delivery**: This event is published to a dedicated Kafka topic (`_schema_changes`) with its own schema (or to the regular table topic with a special header). Consumers subscribe to this topic to learn about schema changes.\n   - **Consumer Action**: Upon receiving a `SchemaChangeEvent`, consumers should:\n     1. Fetch the new schema from the registry.\n     2. Update their deserializers or database schemas accordingly.\n     3. Possibly replay events from the new schema version if needed.\n\n5. **Pipeline Continuation**\n   - **Future Events**: All subsequent `ChangeEvent` objects for the modified table include the new `schemaVersionId`.\n   - **Backfill Consideration**: Existing events with the old schema remain valid; their schema IDs still resolve. No backfilling is required because old events are self-described with their schema version.\n\n**Concrete Example: Adding a Profile Photo Column**\nLet's walk through adding an optional profile photo URL to the users table:\n\n1. **DDL Execution**: `ALTER TABLE users ADD COLUMN profile_photo_url VARCHAR(512) NULL;`\n2. **Log Capture**: `PostgreSQLParser` creates `RawLogEntry` with `operationType=\"DDL\"`, `rowData={\"sql\": \"ALTER TABLE users ADD COLUMN...\"}`.\n3. **Schema Inference**: Current schema `users_v3` has columns `id, email, created_at`. New schema adds `profile_photo_url` (VARCHAR, nullable).\n4. **Compatibility Check**: New column is nullable → passes backward compatibility.\n5. **Registration**: `SchemaRegistry.registerSchema()` creates `users_v4`, stores it, updates cache.\n6. **Notification**: `SchemaChangeEvent` published: `tableName=\"users\"`, `oldSchemaId=\"users_v3\"`, `newSchemaId=\"users_v4\"`.\n7. **Consumer Notification**: Analytics service receives the schema change event, fetches `users_v4`, updates its Avro deserializer to accept the new optional field.\n8. **Subsequent Data**: A new `UPDATE users SET profile_photo_url='...'` produces a `ChangeEvent` with `schemaVersionId=\"users_v4\"` containing the new column value.\n\n**ADR: Immediate vs. Deferred Schema Application**\n\n> **Decision: Immediate Schema Application with Backward-Only Compatibility**\n> - **Context**: When a DDL operation is detected, the system must decide whether to immediately apply the new schema to future events or buffer events until consumers acknowledge readiness.\n> - **Options Considered**:\n>   1. **Immediate Application**: Apply new schema immediately after registration; emit events with new schema version.\n>   2. **Consumer-ACK Application**: Wait for all registered consumers to acknowledge they can handle the new schema before applying it.\n>   3. **Dual-Writing**: Continue writing with old schema for a grace period while also writing with new schema.\n> - **Decision**: Immediate schema application with backward-only compatibility.\n> - **Rationale**: Backward-compatible changes (add optional columns) don't break existing consumers—they simply ignore the new fields. This approach minimizes complexity and avoids the distributed coordination problem of tracking consumer readiness. The schema change event notifies consumers, but they can upgrade at their own pace.\n> - **Consequences**: Consumers must be resilient to unknown fields (skip them or store generically). Breaking changes (non-backward-compatible) require pipeline stoppage and coordinated upgrade—a deliberate operational process.\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Immediate Application | Simple, no coordination overhead, low latency | Consumers may temporarily see fields they can't parse if not upgraded | **Yes** |\n| Consumer-ACK Application | Safe, ensures no consumer breaks | Complex coordination, requires consumer registration, can block pipeline | No |\n| Dual-Writing | Maximum compatibility, no blocking | Double write overhead, consumers must handle duplicate events | No |\n\n**Common Pitfalls in Schema Change Flow**\n\n⚠️ **Pitfall: Silent Data Truncation on Type Narrowing**\n- **Description**: Changing a column from `VARCHAR(255)` to `VARCHAR(100)` passes some compatibility checks (still same SQL type) but can truncate data if existing rows have longer values.\n- **Why Wrong**: The CDC system might not validate actual data lengths, leading to silent data loss when events are serialized.\n- **Fix**: Implement **data compatibility checks** in addition to schema compatibility. Before accepting a narrowing change, sample existing rows to ensure no data would be truncated, or reject such changes outright.\n\n⚠️ **Pitfall: Missing Default Values for New NOT NULL Columns**\n- **Description**: Adding a `NOT NULL` column without a default value is backward incompatible, but some databases allow it (with a one-time rewrite of the table).\n- **Why Wrong**: The CDC system might allow this change, but subsequent INSERT events for old schema consumers will fail validation (missing required field).\n- **Fix**: Treat `NOT NULL` column additions as backward incompatible unless a default value is provided. Reject such DDL or require an explicit default.\n\n⚠️ **Pitfall: Schema Registry Single Point of Failure**\n- **Description**: If the schema registry becomes unavailable, the entire pipeline may stop because every event needs a schema for serialization.\n- **Why Wrong**: Downtime of the registry causes data pipeline stoppage, violating availability goals.\n- **Fix**: Implement robust caching in `ChangeEventBuilder` (schema cache with TTL) and allow operation with stale schemas during registry outages. Use a replicated registry backend (e.g., Kafka-based schema registry with multiple nodes).\n\n### 9.3 Implementation Guidance\n\n**A. Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Sequence Diagram Generation | PlantUML (text-based, integrates with docs) | Mermaid.js (JavaScript, renders in browsers) |\n| Schema Change Detection | Parse SQL strings with regex/string matching | Use SQL parser library (ANTLR, JSqlParser) |\n| Compatibility Checking | Manual rule implementation per change type | Integrate Avro Schema Registry compatibility API |\n| Schema Storage | File-based (JSON files per schema version) | Dedicated schema registry service (Confluent, Apicurio) |\n\n**B. Recommended File/Module Structure**\n```\ncdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── pipeline/\n│   │   ├── CdcPipeline.java              # Main orchestration, implements flow\n│   │   └── PipelineHealthChecker.java    # Health status aggregation\n│   ├── interactions/\n│   │   ├── FlowOrchestrator.java         # Coordinates steps 1-7 in normal flow\n│   │   ├── DdlEventHandler.java          # Handles schema change flow\n│   │   └── RecoveryCoordinator.java      # Manages failure recovery scenarios\n│   ├── diagram/\n│   │   ├── SequenceDiagramGenerator.java # (Optional) generates diagrams from actual traces\n│   │   └── FlowVisualizer.java           # (Optional) renders flows for debugging\n│   └── monitoring/\n│       └── FlowMetricsCollector.java     # Tracks latencies between steps\n```\n\n**C. Infrastructure Starter Code: Flow Orchestrator**\n\n```java\npackage com.cdc.interactions;\n\nimport com.cdc.config.AppConfig;\nimport com.cdc.connector.LogConnector;\nimport com.cdc.builder.ChangeEventBuilder;\nimport com.cdc.streamer.EventStreamer;\nimport com.cdc.schema.SchemaRegistry;\nimport com.cdc.events.ChangeEvent;\nimport com.cdc.events.RawLogEntry;\nimport com.cdc.health.HealthStatus;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.List;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\n/**\n * Orchestrates the end-to-end flow from log reading to event delivery.\n * Implements the normal operation sequence with error handling and backpressure.\n */\npublic class FlowOrchestrator {\n    private static final Logger LOG = LoggerFactory.getLogger(FlowOrchestrator.class);\n    \n    private final LogConnector logConnector;\n    private final ChangeEventBuilder eventBuilder;\n    private final EventStreamer eventStreamer;\n    private final SchemaRegistry schemaRegistry;\n    private final int batchSize;\n    private final AtomicBoolean running = new AtomicBoolean(false);\n    private Thread processingThread;\n    \n    public FlowOrchestrator(LogConnector connector, ChangeEventBuilder builder,\n                           EventStreamer streamer, SchemaRegistry registry,\n                           AppConfig config) {\n        this.logConnector = connector;\n        this.eventBuilder = builder;\n        this.eventStreamer = streamer;\n        this.schemaRegistry = registry;\n        this.batchSize = config.getMaxBatchSize();\n    }\n    \n    /**\n     * Starts the continuous processing loop.\n     */\n    public void start() {\n        if (running.compareAndSet(false, true)) {\n            processingThread = new Thread(this::processingLoop, \"flow-orchestrator\");\n            processingThread.start();\n            LOG.info(\"Flow orchestrator started\");\n        }\n    }\n    \n    /**\n     * Stops the processing loop gracefully.\n     */\n    public void stop() {\n        running.set(false);\n        if (processingThread != null) {\n            processingThread.interrupt();\n            try {\n                processingThread.join(5000);\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            }\n        }\n        LOG.info(\"Flow orchestrator stopped\");\n    }\n    \n    /**\n     * Main processing loop implementing the normal flow sequence.\n     */\n    private void processingLoop() {\n        LOG.info(\"Starting processing loop with batch size {}\", batchSize);\n        \n        while (running.get() && !Thread.currentThread().isInterrupted()) {\n            try {\n                // Step 2: Poll for raw log entries\n                List<RawLogEntry> rawEntries = logConnector.getNextBatch(batchSize);\n                \n                if (rawEntries.isEmpty()) {\n                    // No data available, sleep briefly to avoid tight loop\n                    Thread.sleep(100);\n                    continue;\n                }\n                \n                LOG.debug(\"Processing batch of {} raw log entries\", rawEntries.size());\n                \n                // Step 3: Build change events (with transaction grouping)\n                List<ChangeEvent> changeEvents = eventBuilder.processBatch(rawEntries);\n                \n                if (!changeEvents.isEmpty()) {\n                    LOG.debug(\"Built {} change events from batch\", changeEvents.size());\n                    \n                    // Step 4: Publish events to stream\n                    eventStreamer.publish(changeEvents);\n                    \n                    // Monitor backpressure signal\n                    if (eventStreamer instanceof KafkaEventPublisher) {\n                        KafkaEventPublisher publisher = (KafkaEventPublisher) eventStreamer;\n                        if (publisher.isBackpressureRequired()) {\n                            LOG.warn(\"Backpressure activated, pausing log connector\");\n                            logConnector.pause();\n                            // Wait until backpressure clears\n                            while (publisher.isBackpressureRequired() && running.get()) {\n                                Thread.sleep(100);\n                            }\n                            logConnector.resume();\n                        }\n                    }\n                }\n                \n            } catch (InterruptedException e) {\n                LOG.info(\"Processing loop interrupted\");\n                Thread.currentThread().interrupt();\n                break;\n            } catch (Exception e) {\n                LOG.error(\"Unexpected error in processing loop\", e);\n                // Implement circuit breaker pattern here\n                // After N consecutive errors, transition to DEGRADED health state\n                try {\n                    Thread.sleep(1000); // Avoid tight error loop\n                } catch (InterruptedException ie) {\n                    Thread.currentThread().interrupt();\n                    break;\n                }\n            }\n        }\n    }\n    \n    /**\n     * Returns the health status based on component health and processing state.\n     */\n    public HealthStatus getHealthStatus() {\n        if (!running.get()) {\n            return HealthStatus.STOPPED;\n        }\n        // Check if all components are healthy\n        // Implementation depends on component health APIs\n        return HealthStatus.HEALTHY;\n    }\n}\n```\n\n**D. Core Logic Skeleton: DDL Event Handler**\n\n```java\npackage com.cdc.interactions;\n\nimport com.cdc.events.RawLogEntry;\nimport com.cdc.events.SchemaChangeEvent;\nimport com.cdc.schema.SchemaRegistry;\nimport com.cdc.schema.SchemaVersion;\nimport com.cdc.config.AppConfig;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.Map;\n\n/**\n * Handles the schema change flow when DDL events are detected.\n */\npublic class DdlEventHandler {\n    private static final Logger LOG = LoggerFactory.getLogger(DdlEventHandler.class);\n    \n    private final SchemaRegistry schemaRegistry;\n    private final String compatibilityMode;\n    private final EventStreamer eventStreamer; // For publishing schema change events\n    \n    public DdlEventHandler(SchemaRegistry registry, AppConfig config, EventStreamer streamer) {\n        this.schemaRegistry = registry;\n        this.compatibilityMode = config.getSchema().getCompatibilityMode();\n        this.eventStreamer = streamer;\n    }\n    \n    /**\n     * Process a DDL log entry and handle the schema change flow.\n     * \n     * @param ddlEntry The raw log entry containing DDL statement\n     * @return true if schema change was successfully processed, false otherwise\n     */\n    public boolean handleDdlEvent(RawLogEntry ddlEntry) {\n        // TODO 1: Extract table name and DDL statement from ddlEntry.rowData\n        //   - Parse the SQL statement to determine change type (ADD, DROP, ALTER)\n        //   - Use regex or simple string matching for basic parsing\n        //   - Advanced: Use a SQL parser library for robust parsing\n        \n        // TODO 2: Retrieve current schema for the table using schemaRegistry.getLatestSchema()\n        //   - Handle case where table has no registered schema (first time)\n        \n        // TODO 3: Infer new schema by applying DDL change to current schema\n        //   - Create a copy of current columnDefinitions map\n        //   - Based on DDL type, modify the map:\n        //     * ADD COLUMN: Add new entry with appropriate ColumnType\n        //     * DROP COLUMN: Remove entry (check compatibility first!)\n        //     * ALTER COLUMN: Modify existing ColumnType\n        \n        // TODO 4: Perform compatibility check using schemaRegistry.checkCompatibility()\n        //   - Pass new column definitions, current schema, and compatibilityMode\n        //   - If incompatible:\n        //       1. Log error with specific incompatibility reason\n        //       2. Optionally stop pipeline or trigger alert\n        //       3. Return false\n        \n        // TODO 5: Register new schema version using schemaRegistry.registerSchema()\n        //   - This will assign a new version ID and persist the schema\n        //   - Update any local schema caches\n        \n        // TODO 6: Create and publish SchemaChangeEvent\n        //   - Construct SchemaChangeEvent with tableName, old/new schema IDs, timestamp\n        //   - Serialize and publish to dedicated schema change topic\n        //   - Use eventStreamer.publish() with appropriate topic routing\n        \n        // TODO 7: Log the successful schema change for audit purposes\n        //   - Include old and new schema versions, DDL statement, timestamp\n        \n        return true; // Return true if all steps succeeded\n    }\n    \n    /**\n     * Simple SQL parser for extracting ALTER TABLE components.\n     * This is a basic implementation; consider using a proper SQL parser for production.\n     */\n    private Map<String, Object> parseAlterTable(String ddlSql) {\n        // TODO: Implement basic parsing\n        //   - Extract table name after \"ALTER TABLE\"\n        //   - Determine operation type (ADD, DROP, ALTER)\n        //   - For ADD COLUMN: extract column name, data type, nullability, default\n        //   - Return as structured map\n        return Map.of();\n    }\n}\n```\n\n**E. Language-Specific Hints (Java)**\n\n1. **Sequence Tracking**: Use `AtomicLong` for sequence numbers within transactions and `ThreadLocal` random for generating unique IDs when needed.\n2. **Backpressure Implementation**: Implement `Flow.Control` (Reactive Streams) interface for more sophisticated backpressure, or use simple semaphores (`Semaphore` with permits equal to max in-flight events).\n3. **Schema Caching**: Use Guava's `LoadingCache` with TTL for schema caching to avoid registry overload.\n4. **DDL Parsing**: Use `com.github.jsqlparser:jsqlparser` for robust SQL parsing instead of regex.\n5. **Flow Visualization**: Use Micrometer `@Timed` annotations on each step method to generate latency histograms for each stage of the pipeline.\n\n**F. Milestone Checkpoint**\n\nAfter implementing the interaction flows, verify the system with this end-to-end test:\n\n1. **Start the Pipeline**: Run `CdcPipeline.start()` and confirm all components initialize.\n2. **Create Test Table**: In PostgreSQL: `CREATE TABLE test_users (id TEXT PRIMARY KEY, name TEXT);`\n3. **Insert a Row**: `INSERT INTO test_users VALUES ('test1', 'Alice'); COMMIT;`\n4. **Verify Event Flow**:\n   - Check logs for `RawLogEntry` parsed from WAL.\n   - Confirm `ChangeEvent` built with correct `schemaVersionId`.\n   - Monitor Kafka topic `cdc.test_users` for the event arrival.\n   - Use a test consumer to receive and deserialize the event.\n5. **Test Schema Change**:\n   - Execute: `ALTER TABLE test_users ADD COLUMN email TEXT NULL;`\n   - Verify `SchemaChangeEvent` appears in `_schema_changes` topic.\n   - Insert another row with email, confirm new schema version is used.\n6. **Metrics Verification**: Check exposed metrics (via JMX or HTTP endpoint) for:\n   - `cdc_flow_latency_ms` histogram showing time from commit to consumer.\n   - `cdc_schema_changes_total` counter incremented.\n\n**Expected Output**: The pipeline should process the INSERT within milliseconds, and the schema change should be handled without stopping the pipeline. Consumer lag should remain near zero.\n\n**Debugging Tip**: If events aren't flowing, use this diagnostic checklist:\n- Check `LogConnector.getLastProcessedLSN()` to see if it's advancing.\n- Verify `OffsetStore` file has current LSN persisted.\n- Check Kafka producer metrics for send errors.\n- Inspect schema registry logs for compatibility check failures.\n\n---\n\n\n> **Milestone(s):** Milestone 1 (log position recovery), Milestone 2 (delivery guarantees during failures), Milestone 3 (schema registry failure handling)\n\n## 10. Error Handling and Edge Cases\n\nA CDC system operates in the critical path between the source database and downstream systems, making robust error handling non-negotiable. Unlike batch systems that can be restarted from the beginning, a CDC pipeline must handle transient failures without data loss while maintaining strict ordering guarantees. This section details the **failure modes** each component can encounter, the **detection mechanisms** that identify these failures, and the **recovery strategies** that restore normal operation.\n\n### 10.1 Failure Modes and Detection\n\nThink of the CDC pipeline as a **convoy of armored trucks** transporting valuable goods (data changes). Failure modes represent different attack vectors on this convoy: roadblocks (network partitions), mechanical failures (database restarts), hijacking attempts (corrupt data), and communication breakdowns (consumer lag). Detection mechanisms are the convoy's security detail—constantly monitoring for threats through checkpoints, heartbeat signals, and surveillance.\n\nEach component faces distinct failure modes that require specific detection strategies. The system employs a multi-layered monitoring approach combining **active health checks**, **passive metric monitoring**, and **transaction boundary verification**.\n\n#### 10.1.1 Log Connector & Parser Failures\n\nThe Log Connector operates closest to the source database and must handle low-level database connectivity and log parsing failures.\n\n| Failure Mode | Detection Mechanism | Indicators & Symptoms |\n|--------------|---------------------|----------------------|\n| **Database connection loss** | Active health checks with exponential backoff | TCP connection timeout, JDBC/Go driver connection errors, repeated authentication failures |\n| **WAL/binlog slot deletion** | Slot existence verification on startup and periodic checks | PostgreSQL: `pg_replication_slot` query returns no rows; MySQL: `SHOW SLAVE STATUS` shows missing connection |\n| **Log position corruption** | Checksum validation of stored offsets | Stored LSN doesn't exist in current log, LSN comparison shows position moving backward |\n| **Database version mismatch** | Parser compatibility check on initialization | Binary log format version differs from expected, unsupported column types in parsed entries |\n| **WAL segment rotation before consumption** | Monitoring WAL retention settings vs consumption rate | PostgreSQL: `pg_stat_replication` shows `sent_lsn` far behind `write_lsn`; gaps in LSN sequence |\n| **Parser memory exhaustion** | JVM heap monitoring / Go memory profiling | `OutOfMemoryError` in Java, parser thread crash, progressive slowdown in parsing throughput |\n| **Database restart with slot persistence** | Reconnection logic with slot state verification | Connection drops, followed by successful reconnection but slot shows `active_pid = NULL` |\n\n> **Critical Insight**: The Log Connector must distinguish between **transient network glitches** (retry with backoff) and **permanent configuration issues** (halt with alert). A 3-strike rule is effective: three consecutive connection failures within 60 seconds triggers a degraded health status and alert.\n\n#### 10.1.2 Change Event Builder Failures\n\nThe Change Event Builder manages transaction state in memory, making it vulnerable to state corruption and memory-related issues.\n\n| Failure Mode | Detection Mechanism | Indicators & Symptoms |\n|--------------|---------------------|----------------------|\n| **Transaction timeout** | Last activity timestamp monitoring | Transaction remains in `activeTransactions` map beyond `transactionTimeoutMs` |\n| **Memory leak in transaction state** | Active transaction count monitoring | `getTransactionCount()` grows unbounded over time without corresponding commit/rollback |\n| **Deduplication cache overflow** | Cache size monitoring vs configured limits | `deduplicationCache.size()` exceeds maximum threshold, older entries being evicted prematurely |\n| **Schema cache staleness** | Schema version ID mismatch during event building | `SchemaVersion` ID in cache doesn't match `schemaVersionId` in incoming `RawLogEntry` |\n| **Transaction state corruption** | Consistency checks on transaction boundaries | Transaction has `startLsn` but no operations, or operations out of sequence order |\n| **Clock skew affecting timestamps** | NTP synchronization monitoring | `commitTimestamp` from database vs system clock shows >5 second discrepancy |\n\n#### 10.1.3 Event Streamer & Delivery Failures\n\nThe Event Streamer interfaces with Kafka, facing network partitions, broker failures, and consumer backpressure scenarios.\n\n| Failure Mode | Detection Mechanism | Indicators & Symptoms |\n|--------------|---------------------|----------------------|\n| **Kafka broker unreachable** | Producer/AdminClient heartbeat failures | `KafkaException` with \"Failed to update metadata\", producer `send()` timeouts |\n| **Producer idempotence sequence gap** | Out-of-order sequence number detection | Producer internal metrics show `sequence_number_gap` > 0 for any topic partition |\n| **Consumer lag exceeding threshold** | Consumer group lag monitoring via AdminClient | `consumer_lag` > configured `lagThresholdMs` for any partition |\n| **Backpressure cascade** | Pipeline stage queue monitoring | `LogConnector.buffer` near capacity, `KafkaEventPublisher.inFlightEventCount` > maximum in-flight |\n| **Delivery callback timeout** | Async send operation timeout monitoring | `DeliveryCallback` not invoked within `delivery.timeout.ms` configuration |\n| **Topic auto-creation failure** | Topic existence verification on startup | `UnknownTopicOrPartitionException` when producing to topic, despite `auto.create.topics.enable=true` |\n| **Producer buffer exhaustion** | Buffer pool memory monitoring | `buffer.memory` usage > 90%, producer blocking on `send()` |\n\n#### 10.1.4 Schema Registry Failures\n\nThe Schema Registry is a critical metadata service whose failure can halt schema evolution.\n\n| Failure Mode | Detection Mechanism | Indicators & Symptoms |\n|--------------|---------------------|----------------------|\n| **Registry service unavailable** | Health endpoint polling | HTTP 503/connect timeout from registry URL, cache miss with no fallback |\n| **Disk corruption in file-based registry** | Checksum validation on schema files | `FileBasedSchemaRegistry.loadExistingSchemasIntoCache()` throws JSON parsing exception |\n| **Schema compatibility violation** | Pre-commit compatibility checking | `SchemaRegistry.checkCompatibility()` returns `false` for DDL that should pass |\n| **Schema ID collision** | Duplicate ID detection during registration | Two different schema definitions with same `schemaId` in cache |\n| **Backward compatibility break** | Consumer compatibility check | Consumer fails to deserialize events with `UnsupportedSchemaException` |\n| **Schema cache incoherency** | Cross-component schema version verification | `ChangeEventBuilder.schemaCache` version differs from `SchemaRegistry` persisted version |\n\n#### 10.1.5 Systemic Failure Modes\n\nThese failures affect multiple components or the entire pipeline.\n\n| Failure Mode | Detection Mechanism | Indicators & Symptoms |\n|--------------|---------------------|----------------------|\n| **Network partition isolating components** | Inter-component heartbeat and ping | gRPC/HTTP health checks fail between components, but each component's self-check passes |\n| **Clock skew across deployment nodes** | Cross-node timestamp comparison | Events show out-of-order `commitTimestamp` vs `processingTimestamp` > allowable skew |\n| **Coordinated pipeline restart** | Graceful shutdown sequence tracking | Multiple components restart simultaneously, losing in-memory transaction state |\n| **Database failover to replica** | Database host/port change detection | Connection succeeds to new host, but WAL position doesn't continue from previous LSN |\n| **ZooKeeper/Kafka controller election** | Broker metadata staleness | Producer receives \"Not Leader for Partition\" errors despite recent successful sends |\n| **Disk full on state persistence volume** | Disk space monitoring for offset and schema files | `FileOffsetStore.save()` throws `IOException` due to \"No space left on device\" |\n\n### 10.2 Recovery Strategies\n\nOnce a failure is detected, the system must execute a recovery strategy that **minimizes data loss**, **preserves ordering guarantees**, and **restores throughput** without manual intervention. Think of recovery as the convoy's emergency protocols: when a truck breaks down, goods are transferred to backup vehicles; when a road is blocked, an alternate route is calculated; when communication is lost, pre-established rendezvous points are used.\n\n#### 10.2.1 Log Connector Recovery Strategies\n\nThe Log Connector's primary recovery challenge is resuming from the correct log position without missing or duplicating changes.\n\n**Database Connection Loss Recovery:**\n1. **Immediate Actions**: Close existing connection, increment failure counter\n2. **Backoff Strategy**: Wait using exponential backoff (1s, 2s, 4s, 8s, 16s, 30s max)\n3. **Reconnection Attempt**: \n   - Re-establish database connection with same parameters\n   - Verify replication slot still exists and is active\n   - Query current WAL position to ensure it's ahead of `lastProcessedLSN`\n4. **Resumption Logic**:\n   - If reconnection succeeds and slot exists: resume reading from `lastProcessedLSN`\n   - If slot was deleted: create new slot, trigger **full table resync** (non-goal but needed)\n   - If `lastProcessedLSN` no longer in retained logs: restart from earliest available LSN, log warning\n\n**WAL Position Corruption Recovery:**\n1. **Corruption Detection**: Compare stored LSN with minimum available LSN in `pg_ls_waldir()` or `SHOW BINARY LOGS`\n2. **Recovery Options**:\n   - If corruption is minor (few segments behind): restart from last known valid LSN in retained logs\n   - If corruption is major (beyond retention): reset to current LSN, emit **gap detection event**\n3. **Gap Handling**: Generate a synthetic `ChangeEvent` with `operationType=GAP_DETECTED` to alert consumers of potential missing data\n\n**Transaction Log Overflow Prevention:**\n> **Decision: Conservative WAL Retention Policy**\n> - **Context**: The database may recycle WAL segments before the CDC connector reads them if consumption is too slow\n> - **Options Considered**:\n>   1. Increase WAL retention settings on database (requires DB admin rights)\n>   2. Implement backpressure to slow source database writes (impacts primary workload)\n>   3. Add intermediate disk buffer in connector (adds complexity and failure mode)\n> - **Decision**: Configure database WAL retention to hold at least 24 hours of changes, monitor consumption lag, and alert before reaching retention limits\n> - **Rationale**: Simplicity and reliability—letting the database manage retention with safe margins avoids complex buffering logic\n> - **Consequences**: Requires database configuration access, uses more disk on source DB\n\n| Option | Pros | Cons | Chosen? |\n|--------|------|------|---------|\n| Increase DB retention | Simple, reliable | DB disk usage, requires privileges | **Yes** (primary) |\n| Backpressure to source | Minimizes disk usage | Impacts primary workload, complex | No |\n| Connector disk buffer | Decouples from DB config | Another state to manage, disk failure risk | No (fallback) |\n\n#### 10.2.2 Change Event Builder Recovery Strategies\n\nThe Change Event Builder's in-memory state is volatile and must be reconstructed or flushed during failures.\n\n**Pipeline Restart with Pending Transactions:**\n1. **Problem**: Pipeline stops while transactions are open in `activeTransactions` map\n2. **Recovery Strategy**: \n   - On graceful shutdown: `flushPendingTransactions()` emits all pending events with `transactionId=ABORTED` marker\n   - On crash recovery: Transaction timeout detection will eventually clean up stale transactions\n3. **Data Guarantee**: Consumers must handle `ABORTED` markers by discarding or compensating for partial transactions\n\n**Transaction Timeout Recovery:**\n```\n1. Background cleaner thread runs every transactionTimeoutMs/2\n2. For each TransactionState in activeTransactions:\n3.   If (currentTime - lastActivityTimestamp) > transactionTimeoutMs:\n4.     Log warning: \"Transaction {transactionId} timed out after {timeoutMs}ms\"\n5.     Call buildEventsForTransaction() with ABORTED marker\n6.     Remove from activeTransactions map\n7.     Emit abort events to streamer\n```\n\n**Memory Exhaustion Prevention:**\n1. **Monitoring**: Track `activeTransactions.size()` and `deduplicationCache` memory usage\n2. **Circuit Breaker**: If transaction count exceeds `maxConcurrentTransactions`:\n   - Pause Log Connector (backpressure)\n   - Force-flush oldest transactions (by `lastActivityTimestamp`)\n   - Resume when below 80% threshold\n3. **Deduplication Cache Management**: Use LRU eviction when cache reaches size limit\n\n#### 10.2.3 Event Streamer Recovery Strategies\n\nThe Event Streamer must guarantee at-least-once delivery despite Kafka broker failures and network issues.\n\n**Kafka Producer Failure Recovery:**\n\n> **Decision: Producer Retry with Idempotence**\n> - **Context**: Network partitions or broker failures can cause individual produce requests to fail\n> - **Options Considered**:\n>   1. Retry indefinitely with backoff (risk of indefinite hang)\n>   2. Retry N times then fail (risk of data loss)\n>   3. Retry with dead letter queue (adds complexity)\n> - **Decision**: Configure producer with `retries=MAX_INT`, `delivery.timeout.ms=120000`, `enable.idempotence=true`\n> - **Rationale**: Kafka's idempotent producer ensures exactly-once semantics in the producer-broker interaction, making infinite retries safe\n> - **Consequences**: Producer may block for up to 2 minutes during complete broker outage; requires `max.in.flight.requests.per.connection=1` or 5\n\n| Recovery Step | Action | Rationale |\n|---------------|--------|-----------|\n| 1. Send failure detection | `KafkaException` caught in `DeliveryCallback` | Network or broker issue |\n| 2. Automatic retry | Kafka producer internal retry logic | Built-in exponential backoff |\n| 3. Retry exhaustion | `TimeoutException` after `delivery.timeout.ms` | Brokers unavailable beyond tolerance |\n| 4. Circuit breaker | Pause pipeline, buffer in memory | Prevent unbounded memory growth |\n| 5. AdminClient health check | Verify broker availability | Distinguish network vs broker failure |\n| 6. Resume production | When brokers return | Continue from buffered events |\n\n**Consumer Lag Recovery:**\n1. **Detection**: `StreamerMetrics` monitors consumer group lag via AdminClient `listConsumerGroupOffsets()`\n2. **Threshold Breach**: When lag > `lagThresholdMs` for >5 consecutive checks:\n   - Scale alert to operations team\n   - Optionally scale out consumer applications\n3. **Backpressure Application**: If lag exceeds critical threshold (2x `lagThresholdMs`):\n   - Reduce `LogConnector` batch size\n   - Increase polling interval\n   - Pause connector if lag continues growing\n\n**Delivery Semantic Preservation During Failures:**\n\n| Failure Scenario | At-Least-Once Guarantee Preservation | Recovery Action |\n|------------------|--------------------------------------|-----------------|\n| Producer crash before callback | Events may be lost | Reload from `lastProcessedLSN` and reproduce |\n| Producer crash after callback | Events delivered (Kafka has them) | Continue from last acknowledged offset |\n| Kafka broker loss of acknowledged writes | Events lost (unlikely with `acks=all`) | Reprocess from last confirmed LSN |\n| Network partition during send | Unknown delivery state | Retry with idempotent producer (no duplicates) |\n\n#### 10.2.4 Schema Registry Recovery Strategies\n\nSchema Registry failures can halt the entire pipeline since events cannot be serialized without schema information.\n\n**Registry Service Unavailable Recovery:**\n1. **Cached Schema Fallback**: Use `ChangeEventBuilder.schemaCache` as read-through cache\n2. **Degraded Mode Operation**:\n   - Continue processing events with cached schemas\n   - Buffer schema registration requests in memory queue\n   - Log warning about schema registry unavailability\n3. **Recovery on Service Restoration**:\n   - Flush buffered schema registrations\n   - Validate cache coherence with registry\n   - Continue normal operation\n\n**Schema Compatibility Violation Recovery:**\n1. **Prevention**: Always run `checkCompatibility()` before applying DDL to database\n2. **Detection**: DDL parser detects incompatible change (column removal, type narrowing)\n3. **Recovery Options**:\n   - **Option A**: Reject DDL (requires DBA intervention)\n   - **Option B**: Register schema with `COMPATIBILITY_NONE` and alert consumers\n   - **Option C**: Create new table version and dual-write during migration\n\n**File-Based Registry Corruption Recovery:**\n```\n1. On startup, FileBasedSchemaRegistry.loadExistingSchemasIntoCache():\n2.   Try: Read and parse all *.schema.json files\n3.   Catch JsonProcessingException:\n4.     Attempt repair: Skip corrupt file, log error\n5.     If critical schemas missing: \n6.       Fallback to reconstructing from database INFORMATION_SCHEMA\n7.       Log alert for manual schema reconciliation\n8.   Finally: Initialize cache with whatever schemas loaded successfully\n```\n\n#### 10.2.5 Systemic Recovery Strategies\n\nThese strategies coordinate recovery across multiple components.\n\n**Pipeline-Wide Restart Recovery:**\n1. **Ordered Shutdown Protocol**:\n   ```\n   1. Stop Log Connector (stops reading new changes)\n   2. Call ChangeEventBuilder.flushPendingTransactions()\n   3. Wait for EventStreamer to drain pendingEvents queue\n   4. Persist final offset via OffsetStore.save()\n   5. Close all connections (DB, Kafka, Registry)\n   ```\n2. **Ordered Startup Protocol**:\n   ```\n   1. Initialize SchemaRegistry and load cache\n   2. Initialize EventStreamer and verify Kafka connectivity\n   3. Initialize ChangeEventBuilder with empty state\n   4. Initialize LogConnector with offset from OffsetStore.load()\n   5. Start LogConnector reading from stored LSN\n   ```\n3. **Crash Recovery**: On startup, verify component state consistency and reset if inconsistent\n\n**Database Failover Recovery:**\n1. **Detection**: Connection drops, then reconnects to different host (detected via DNS/hostname change)\n2. **Position Verification**: Query `pg_current_wal_lsn()` or `SHOW MASTER STATUS` on new primary\n3. **Recovery Decision Table**:\n\n| Scenario | Recovery Action |\n|----------|----------------|\n| New primary has LSN ≥ lastProcessedLSN | Resume reading from lastProcessedLSN |\n| New primary has LSN < lastProcessedLSN (data loss) | Reset to new primary's LSN, emit GAP_DETECTED event |\n| Cannot determine LSN continuity | Pause pipeline, alert for manual intervention |\n\n**Clock Skew Mitigation:**\n1. **Detection**: Monitor `commitTimestamp` vs system clock on `ChangeEvent` creation\n2. **Mitigation**: Use database-provided timestamps exclusively, never system clock\n3. **Correction**: If skew detected, log warning but don't adjust timestamps (preserve source truth)\n\n#### 10.2.6 Recovery Verification and Testing\n\n> **Critical Principle**: Recovery logic must be tested with the same rigor as normal operation. Use chaos engineering principles to inject failures and verify recovery.\n\n**Recovery Verification Checklist:**\n- [ ] **Position Recovery**: Restart pipeline, verify events continue from exact LSN without gap or repeat\n- [ ] **Transaction Integrity**: Crash during multi-statement transaction, verify consumers see all-or-nothing\n- [ ] **Schema Evolution**: Registry failure during DDL, verify cached schemas allow continued operation\n- [ ] **Backpressure Resilience**: Slow consumer to stall, verify pipeline pauses without data loss\n- [ ] **Network Partition**: Simulate Kafka broker isolation, verify producer buffers and resumes\n\n**Common Recovery Pitfalls and Solutions:**\n\n⚠️ **Pitfall: Off-by-One in LSN Recovery**\n- **Description**: Resuming from `lastProcessedLSN` instead of `lastProcessedLSN + 1` causes duplicate processing of the last event\n- **Why Wrong**: Database logs include the LSN of the next byte to read, not the last byte read\n- **Fix**: Store and resume using exclusive upper bound semantics. When reconnecting, request replication from `lastProcessedLSN + 1` if database API supports it.\n\n⚠️ **Pitfall: Transaction State Resurrection After Timeout**\n- **Description**: Transaction timeout emits abort events, but later the actual commit arrives from database\n- **Why Wrong**: Consumers see contradictory events (abort then commit) for same transaction\n- **Fix**: Maintain timed-out transaction IDs in short-term tombstone cache (5x timeout period) and discard late commits\n\n⚠️ **Pitfall: Kafka Producer Duplicate on Retry**\n- **Description**: Without idempotence, producer retries after timeout can create duplicate messages\n- **Why Wrong**: Breaks exactly-once semantics, consumers process same change twice\n- **Fix**: Enable `enable.idempotence=true` and configure `max.in.flight.requests.per.connection=5`\n\n⚠️ **Pitfall: Schema Cache Incoherency Across Restarts**\n- **Description**: Different pipeline instances have different schema versions in cache after partial registry failure\n- **Why Wrong**: Events serialized with wrong schema version, causing consumer deserialization failures\n- **Fix**: Implement registry cache invalidation protocol or use consistent distributed cache (Redis)\n\n### Implementation Guidance\n\n#### Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Health Checks | ScheduledExecutorService + custom health endpoints | Micrometer + Prometheus + Grafana alerts |\n| Retry Logic | Exponential backoff with jitter (Resilience4j) | Circuit breaker pattern with fallback (Resilience4j) |\n| State Recovery | File-based offset store + JSON schemas | Distributed state store (Apache BookKeeper) |\n| Monitoring | SLF4J logs + JMX metrics | OpenTelemetry tracing + metrics export |\n| Failure Injection | Manual test scenarios | Chaos Mesh for automated chaos testing |\n\n#### Recommended File Structure\n\n```\ncdc-pipeline/\n├── src/main/java/com/cdc/\n│   ├── error/\n│   │   ├── RecoveryCoordinator.java      # Orchestrates cross-component recovery\n│   │   ├── CircuitBreakerManager.java    # Manages circuit breakers per component\n│   │   └── FailureDetector.java          # Aggregates health checks\n│   ├── health/\n│   │   ├── HealthChecker.java            # Interface for component health\n│   │   ├── CompositeHealthCheck.java     # Aggregates all component health\n│   │   └── HealthEndpoint.java           # REST endpoint for health status\n│   ├── recovery/\n│   │   ├── OffsetRecoveryService.java    # Handles LSN recovery scenarios\n│   │   ├── TransactionRecoveryService.java # Recovers transaction state\n│   │   └── SchemaRecoveryService.java    # Handles schema registry failures\n│   └── monitor/\n│       ├── LagMonitor.java               # Consumer lag monitoring\n│       ├── ThroughputMonitor.java        # Pipeline throughput tracking\n│       └── AlertManager.java             # Sends alerts on threshold breaches\n└── src/test/java/com/cdc/\n    └── recovery/\n        ├── FailureInjectionTest.java     # Injects failures and verifies recovery\n        ├── ChaosTest.java                # Chaos engineering style tests\n        └── RecoveryIntegrationTest.java  # End-to-end recovery scenarios\n```\n\n#### Infrastructure Starter Code: Circuit Breaker Manager\n\n```java\n// Complete ready-to-use circuit breaker infrastructure\npackage com.cdc.error;\n\nimport io.github.resilience4j.circuitbreaker.CircuitBreaker;\nimport io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;\nimport io.github.resilience4j.circuitbreaker.CircuitBreakerRegistry;\n\nimport java.time.Duration;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\n\npublic class CircuitBreakerManager {\n    private final CircuitBreakerRegistry registry;\n    private final Map<String, CircuitBreaker> breakers;\n    \n    public CircuitBreakerManager() {\n        // Configure default circuit breaker settings\n        CircuitBreakerConfig config = CircuitBreakerConfig.custom()\n            .failureRateThreshold(50)  // Open after 50% failure rate\n            .slowCallRateThreshold(100) // All calls considered for slowness\n            .slowCallDurationThreshold(Duration.ofSeconds(5))\n            .permittedNumberOfCallsInHalfOpenState(3)\n            .slidingWindowSize(10)\n            .minimumNumberOfCalls(5)\n            .waitDurationInOpenState(Duration.ofSeconds(30))\n            .automaticTransitionFromOpenToHalfOpenEnabled(true)\n            .recordExceptions(IOException.class, TimeoutException.class)\n            .build();\n            \n        this.registry = CircuitBreakerRegistry.of(config);\n        this.breakers = new ConcurrentHashMap<>();\n    }\n    \n    public CircuitBreaker getOrCreate(String name) {\n        return breakers.computeIfAbsent(name, \n            k -> registry.circuitBreaker(k));\n    }\n    \n    public void recordSuccess(String name) {\n        CircuitBreaker breaker = breakers.get(name);\n        if (breaker != null) {\n            breaker.onSuccess();\n        }\n    }\n    \n    public void recordFailure(String name, Throwable error) {\n        CircuitBreaker breaker = breakers.get(name);\n        if (breaker != null) {\n            breaker.onError(error);\n        }\n    }\n    \n    public boolean isCallPermitted(String name) {\n        CircuitBreaker breaker = breakers.get(name);\n        return breaker == null || breaker.tryAcquirePermission();\n    }\n    \n    public String getState(String name) {\n        CircuitBreaker breaker = breakers.get(name);\n        return breaker != null ? breaker.getState().name() : \"NO_BREAKER\";\n    }\n}\n```\n\n#### Core Logic Skeleton: Recovery Coordinator\n\n```java\npackage com.cdc.recovery;\n\nimport com.cdc.AppConfig;\nimport com.cdc.CdcPipeline;\nimport com.cdc.error.CircuitBreakerManager;\nimport com.cdc.health.HealthStatus;\n\npublic class RecoveryCoordinator {\n    private final CircuitBreakerManager circuitBreakerManager;\n    private final OffsetRecoveryService offsetRecoveryService;\n    private final AppConfig config;\n    private volatile RecoveryState currentState;\n    \n    public RecoveryCoordinator(AppConfig config) {\n        this.config = config;\n        this.circuitBreakerManager = new CircuitBreakerManager();\n        this.offsetRecoveryService = new OffsetRecoveryService(config);\n        this.currentState = RecoveryState.NORMAL;\n    }\n    \n    public void handleComponentFailure(String componentName, Throwable error) {\n        // TODO 1: Record failure in circuit breaker for this component\n        // TODO 2: Check if component is in OPEN circuit state (too many failures)\n        // TODO 3: If OPEN, transition pipeline to DEGRADED health status\n        // TODO 4: Determine recovery strategy based on component and error type\n        // TODO 5: Execute appropriate recovery: retry, reset, or wait for manual intervention\n        // TODO 6: If recovery successful, record success in circuit breaker\n        // TODO 7: Transition pipeline back to HEALTHY status when all components recovered\n    }\n    \n    public void onPipelineStart() {\n        // TODO 1: Load last known offsets from all offset stores\n        // TODO 2: Verify offset consistency across components\n        // TODO 3: If inconsistencies found, run offset recovery protocol\n        // TODO 4: Initialize all circuit breakers to CLOSED state\n        // TODO 5: Start background health monitoring threads\n    }\n    \n    public void onPipelineStop() {\n        // TODO 1: Execute graceful shutdown sequence for all components\n        // TODO 2: Flush all pending transactions and events\n        // TODO 3: Persist final offsets with checksums\n        // TODO 4: Close all circuit breakers and cleanup resources\n    }\n    \n    public RecoveryStrategy determineRecoveryStrategy(String component, Throwable error) {\n        // TODO 1: Classify error type: transient, persistent, or unknown\n        // TODO 2: Check component-specific recovery policies from config\n        // TODO 3: Consider current system state and other component health\n        // TODO 4: Return appropriate strategy: RETRY, RESET, DEGRADE, or HALT\n        return null;\n    }\n    \n    enum RecoveryState {\n        NORMAL, DEGRADED, RECOVERING, HALTED\n    }\n    \n    enum RecoveryStrategy {\n        RETRY_WITH_BACKOFF, RESET_AND_RECOVER, DEGRADE_FUNCTIONALITY, HALT_FOR_INTERVENTION\n    }\n}\n```\n\n#### Language-Specific Hints: Java Error Handling\n\n- **Use Resilience4j** for circuit breakers, retries, and bulkheads instead of rolling your own\n- **For database reconnection**, use HikariCP connection pool with proper validation queries\n- **Monitor JVM metrics** using Micrometer and expose via `/actuator/metrics` endpoint\n- **Use try-with-resources** for all Closeable database and network connections\n- **Implement `Thread.UncaughtExceptionHandler`** for worker threads to capture and log unexpected crashes\n- **For memory-sensitive components**, use `SoftReference` for caches that can be GC'd under memory pressure\n\n#### Milestone Checkpoint: Recovery Verification\n\n**After implementing error handling**, run the recovery verification test:\n\n```bash\n# 1. Start the pipeline with a test database\n./gradlew run --args='config/test-config.yaml'\n\n# 2. Insert some test data to establish baseline\nINSERT INTO test_table VALUES (1, 'initial'), (2, 'data');\n\n# 3. Simulate a failure (in another terminal)\nkill -SIGSTOP <connector_pid>  # Pause the log connector thread\n\n# 4. Make more changes while connector is paused\nINSERT INTO test_table VALUES (3, 'during_failure');\n\n# 5. Resume the connector\nkill -SIGCONT <connector_pid>\n\n# 6. Verify recovery:\n# - Connector should reconnect and resume from last LSN\n# - All events (including #3) should be delivered to Kafka\n# - No duplicates of events #1 and #2\n# - Health endpoint should show brief DEGRADED then back to HEALTHY\n\ncurl http://localhost:8080/health\n# Expected: {\"status\":\"HEALTHY\",\"components\":{\"logConnector\":\"HEALTHY\",...}}\n```\n\n**Signs of correct recovery**:\n- Consumer receives exactly 3 events (no duplicates)\n- Pipeline throughput returns to normal within 30 seconds\n- Logs show \"Reconnected to database\" and \"Resumed from LSN: X/Y\"\n\n**Signs of incorrect recovery**:\n- Consumer receives 4+ events (duplicates present)\n- Pipeline remains in DEGRADED state\n- Logs show \"Gap detected\" or \"Could not resume from LSN\"\n\n#### Debugging Tips for Error Recovery\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Duplicate events after restart** | Off-by-one in LSN recovery | Compare `lastProcessedLSN` in offset store vs database's current LSN | Resume from `lastProcessedLSN + 1` |\n| **Pipeline hangs indefinitely after failure** | Circuit breaker stuck OPEN | Check circuit breaker state via `/actuator/circuitbreakers` | Manually reset breaker or adjust failure threshold |\n| **Transactions split across restart** | Incomplete flush on shutdown | Check shutdown logs for \"flushPendingTransactions\" | Implement graceful shutdown hook in `CdcPipeline.stop()` |\n| **Schema mismatch after registry restart** | Cache incoherency | Compare schema version in event vs registry `getLatestSchema()` | Implement cache invalidation on registry update |\n| **Consumer lag spikes but no backpressure** | Backpressure signal not propagating | Check `LogConnector.buffer` size and `backpressureSignal` state | Ensure backpressure chain: Consumer → Streamer → EventBuilder → Connector |\n| **Memory grows unbounded during network partition** | Producer buffer not limited | Monitor `buffer.memory` usage in Kafka producer metrics | Set `buffer.memory` limit and implement circuit breaker |\n\n\n> **Milestone(s):** Milestone 1 (Log Parsing & Change Events), Milestone 2 (Event Streaming & Delivery), Milestone 3 (Schema Evolution & Compatibility)\n\n## 11. Testing Strategy\n\nTesting a Change Data Capture system is akin to verifying a complex pipeline that transforms raw, low-level signals into meaningful, ordered messages. The core challenge lies in validating three critical properties: **correctness of transformation** (parsing raw logs into structured events), **reliability of delivery** (ensuring no data loss and proper ordering), and **robustness to evolution** (handling schema changes without breaking consumers). This section outlines a comprehensive testing approach that builds confidence incrementally, from isolated unit tests to full-system integration tests, ensuring each milestone's acceptance criteria are met.\n\n### 11.1 Test Pyramid for CDC\n\nThe CDC testing strategy follows the classic **test pyramid** model, but with specialized layers tailored to the unique characteristics of data pipeline systems. Think of it as building a quality verification pipeline that mirrors the data pipeline itself: we test each component in isolation, then verify their integration, and finally validate the entire system's behavior under real-world conditions.\n\n> **Key Insight:** In CDC systems, the most critical and difficult-to-test behaviors occur at the boundaries between components and during failure scenarios. Therefore, the testing pyramid should be bottom-heavy for logic correctness but invest significantly in integration and end-to-end tests for reliability guarantees.\n\n#### Layer 1: Unit Tests (The Foundation)\n\nUnit tests verify individual components in complete isolation, mocking all dependencies. Their primary goal is to ensure each component's internal logic behaves correctly according to its specification.\n\n**Mental Model:** Think of unit tests as quality control stations on an assembly line. Each station inspects a single component (e.g., a gear, a circuit board) in isolation, ensuring it meets specifications before it's installed in the larger machine. If a gear has the wrong number of teeth, we catch it here before the entire assembly fails.\n\n| Component | Key Test Scenarios | Mock Dependencies |\n|-----------|-------------------|-------------------|\n| `LogParser` implementations | - Parse valid binary log entry → correct `RawLogEntry`<br>- Parse malformed binary data → appropriate exception<br>- Handle different operation types (INSERT, UPDATE, DELETE)<br>- Extract LSN from various log formats | Raw byte source (mocked) |\n| `ChangeEventBuilder` | - Process single operation → emit single `ChangeEvent`<br>- Process multiple operations in transaction → batch emission on commit<br>- Handle UPDATE with before/after images<br>- Deduplicate operations on same primary key within transaction<br>- Timeout stale transactions | `SchemaRegistry` (mocked) |\n| `EventSerializer` | - Serialize `ChangeEvent` with schema → bytes<br>- Deserialize bytes with schema → identical `ChangeEvent`<br>- Handle missing fields during deserialization<br>- Version mismatch handling | None (pure function) |\n| `FileOffsetStore` | - Save LSN → file persisted<br>- Load LSN from file → correct value returned<br>- Handle missing offset file → return null/default | File system (could use temp files) |\n| `SchemaRegistry` | - Register new schema → version incremented<br>- Check compatibility → pass/fail correctly<br>- Retrieve schema by ID/version | Underlying storage (in-memory) |\n\n**Common Pitfalls in Unit Testing CDC Components:**\n\n⚠️ **Pitfall: Testing with overly simplistic mock data that doesn't reflect real database log formats.**\n- **Why it's wrong:** Database logs contain complex binary structures, flags, headers, and metadata. Using simple JSON-like structures in tests gives false confidence—the parser might pass tests but fail on real data.\n- **How to avoid:** Use **actual binary log snippets** captured from the target database (PostgreSQL/MySQL) as test fixtures. These can be obtained through database utilities and stored as binary files in test resources.\n\n⚠️ **Pitfall: Not testing transaction boundary behavior thoroughly.**\n- **Why it's wrong:** CDC's core guarantee is transaction atomicity—all operations in a transaction should be emitted together. Missing edge cases (like partial transaction reads after crash) can break this guarantee.\n- **How to avoid:** Create comprehensive test cases for transaction state machine transitions: begin → multiple operations → commit, begin → operation → rollback, nested transactions (if supported), and interrupted transactions.\n\n⚠️ **Pitfall: Ignoring schema evolution scenarios in serializer tests.**\n- **Why it's wrong:** In production, schemas evolve constantly. Serializers must handle backward/forward compatibility correctly, which is complex logic that needs explicit testing.\n- **How to avoid:** Test serialization/deserialization across multiple schema versions, verifying that added columns appear as null/default in old schemas and removed columns are ignored gracefully.\n\n#### Layer 2: Integration Tests (The Connective Tissue)\n\nIntegration tests verify that components work correctly together and with external systems. For CDC, this primarily means testing the interaction with actual database instances and message brokers.\n\n**Mental Model:** Imagine testing the plumbing in a house after installing all pipes and fixtures. We turn on the water and check for leaks at connection points, verify water pressure through the system, and ensure drainage works. Similarly, integration tests verify data flows correctly between components and that connections to external systems (database, Kafka) are robust.\n\n| Integration Point | Test Scenario | Setup Required |\n|------------------|---------------|----------------|\n| `LogConnector` ↔ Database | - Connect to database and read WAL/binlog<br>- Resume from saved LSN after restart<br>- Handle database restart/connection drop<br>- Process large transactions without OOM | Real/embedded PostgreSQL or MySQL instance with logical replication enabled |\n| `EventStreamer` ↔ Kafka | - Publish events to Kafka topic<br>- Verify partitioning by primary key<br>- Validate at-least-once delivery with producer retries<br>- Consumer can read and process events | Testcontainers with Kafka container, or embedded Kafka |\n| `SchemaRegistry` ↔ Database DDL | - Detect `ALTER TABLE` and register new schema<br>- Emit `SchemaChangeEvent` to dedicated topic<br>- Verify compatibility checks block breaking changes | Database instance with DDL execution capability |\n| CDC Pipeline End-to-End | - Database INSERT → `ChangeEvent` appears in Kafka<br>- Transaction atomicity preserved<br>- Ordering guarantee per primary key | Full pipeline with source database and Kafka |\n\n**Integration Testing Architecture Decision Record:**\n\n> **Decision: Use Testcontainers for Database and Kafka Integration Tests**\n> - **Context:** We need realistic integration tests that verify component interactions with actual PostgreSQL/MySQL and Kafka, but cannot rely on pre-existing external services that may not be available in CI environments.\n> - **Options Considered:**\n>   1. **Embedded databases (H2, Derby) with simulation:** Use in-memory databases that partially emulate PostgreSQL/MySQL WAL or binlog behavior.\n>   2. **Docker containers managed manually:** Scripts to start/stop Docker containers before test runs.\n>   3. **Testcontainers library:** Programmatic management of Docker containers as part of test lifecycle.\n>   4. **Mock servers (WireMock for HTTP, MockKafka):** Simulate external systems with mocked behavior.\n> - **Decision:** Use Testcontainers for database and Kafka integration tests.\n> - **Rationale:** \n>   - **Realistic behavior:** Testcontainers provide actual PostgreSQL/MySQL and Kafka instances, ensuring we test against real protocol implementations and binary formats.\n>   - **CI compatibility:** Containers run seamlessly in CI environments without special configuration.\n>   - **Lifecycle management:** Testcontainers handles container startup, cleanup, and port assignment automatically.\n>   - **Development experience:** Developers can run tests locally without installing database/Kafka binaries.\n> - **Consequences:**\n>   - **Slower tests:** Container startup adds 10-30 seconds per test class.\n>   - **Docker dependency:** Requires Docker installed on development and CI machines.\n>   - **Resource usage:** Multiple containers running concurrently during test suites.\n\n| Option | Pros | Cons | Why Not Chosen |\n|--------|------|------|----------------|\n| Embedded databases | Fast startup, no external dependencies | Doesn't test real WAL/binlog formats, behavioral differences | Critical to test real log parsing |\n| Manual Docker scripts | Real systems, full control | Complex lifecycle management, port conflicts, CI unfriendly | Too much maintenance overhead |\n| **Testcontainers** | **Real systems, automatic management, CI-friendly** | **Slower tests, requires Docker** | **Best balance of realism and practicality** |\n| Mock servers | Very fast, deterministic | Unrealistic behavior misses edge cases | Insufficient for testing protocol-level interactions |\n\n#### Layer 3: End-to-End Tests (The System Verification)\n\nEnd-to-end (E2E) tests validate the complete system against real-world scenarios and acceptance criteria. They simulate production-like workloads and verify the system's guarantees holistically.\n\n**Mental Model:** Consider E2E tests as a dress rehearsal for a theatrical production. All actors (components), props (databases, brokers), and stage directions (orchestration) come together for a full performance. We verify the audience (consumers) receives the intended story (data) correctly, even when unexpected events occur (failures).\n\n**Core E2E Test Scenarios:**\n\n1. **At-Least-Once Delivery Verification:**\n   - **Setup:** Pipeline running, consumer reading from Kafka\n   - **Action:** Kill pipeline process abruptly mid-processing\n   - **Verification:** Restart pipeline, verify all events eventually delivered (duplicates allowed but documented)\n   - **Measurement:** Count events at source vs. consumer, allow for small discrepancy window\n\n2. **Ordering Guarantee Per Primary Key:**\n   - **Setup:** Pipeline running with multiple concurrent writers to same table\n   - **Action:** Execute sequence of updates to same row: UPDATE row A (v1→v2), UPDATE row A (v2→v3), DELETE row A\n   - **Verification:** Consumer receives events in exact sequence; no v3 before v2, no DELETE before final UPDATE\n\n3. **Schema Evolution Handling:**\n   - **Setup:** Pipeline with active consumer using schema v1\n   - **Action:** Execute `ALTER TABLE ADD COLUMN new_field VARCHAR(50) DEFAULT 'default'`\n   - **Verification:** \n     - Schema registry creates v2 with backward compatibility\n     - New events include `new_field` with 'default' or actual values\n     - Consumer using v1 schema can still deserialize events (new field ignored)\n     - `SchemaChangeEvent` published to notify consumers\n\n4. **Backpressure Propagation:**\n   - **Setup:** Pipeline with slow consumer (artificial delay)\n   - **Action:** High-volume writes to source database\n   - **Verification:** \n     - Pipeline slows down ingestion (observe `LogConnector` pause state)\n     - No OOM errors from unbounded buffering\n     - System recovers when consumer catches up\n\n5. **Recovery from Corruption/Gap Detection:**\n   - **Setup:** Pipeline with offset tracking\n   - **Action:** Manually corrupt offset store or delete WAL segments\n   - **Verification:** \n     - Pipeline detects gap/invalid LSN\n     - Emits `gap detection event` or enters recovery mode\n     - Can be configured to halt or continue from latest\n\n**E2E Test Environment Requirements:**\n\n| Component | Recommended Implementation | Purpose |\n|-----------|----------------------------|---------|\n| Source Database | PostgreSQL container (logical replication enabled) or MySQL container (binlog enabled) | Real transaction log generation |\n| Message Broker | Kafka container with 3 brokers (test replication) | Real event streaming with persistence |\n| Test Harness | JUnit test with `@Testcontainers`, setup/teardown methods | Orchestrate test scenario and verification |\n| Verification Consumer | Custom Kafka consumer with state tracking | Validate delivery guarantees |\n| Load Generator | Separate thread executing SQL statements | Simulate database workload |\n\n### 11.2 Milestone Verification Checkpoints\n\nEach project milestone has specific acceptance criteria that must be demonstrably verified. These checkpoints provide concrete, actionable steps to validate that the implementation meets requirements.\n\n#### Milestone 1: Log Parsing & Change Events\n\n**Checkpoint Goal:** Validate that the CDC pipeline can correctly read database transaction logs, parse them into structured change events with before/after images, and track position for resumption.\n\n| Acceptance Criteria | Verification Method | Expected Result | Failure Indicators |\n|---------------------|---------------------|-----------------|-------------------|\n| Parser connects to PostgreSQL WAL or MySQL binlog | Integration test with real database | Connection established, replication slot created (PostgreSQL) or binlog client connected (MySQL) | Connection errors, permission denied, replication not enabled |\n| INSERT, UPDATE, DELETE operations correctly extracted | Unit tests with real binary log fixtures | `RawLogEntry` contains correct `operationType`, `tableName`, `rowData` | Missing operations, wrong table, incorrect row values |\n| Change events include before/after images for UPDATE | Test scenario: UPDATE row with known values | `ChangeEvent.beforeImage` has old values, `ChangeEvent.afterImage` has new values | Missing before image, swapped images, partial data |\n| Position tracking enables resuming after restart | Integration test: Stop pipeline, note LSN, restart, verify continuation | Pipeline resumes from same LSN, no missed events, no duplicates from before LSN | Starts from beginning, skips events, infinite duplicate loop |\n\n**Verification Procedure for Milestone 1:**\n\n1. **Setup test environment:**\n   - Start PostgreSQL container with `wal_level=logical`\n   - Create test table with primary key\n   - Initialize CDC pipeline with `LogConnector` and `ChangeEventBuilder`\n\n2. **Execute test sequence:**\n   ```sql\n   BEGIN;\n   INSERT INTO test_table (id, name) VALUES (1, 'Alice');\n   UPDATE test_table SET name = 'Alicia' WHERE id = 1;\n   DELETE FROM test_table WHERE id = 1;\n   COMMIT;\n   ```\n\n3. **Verify pipeline output:**\n   - Capture events from `ChangeEventBuilder`\n   - Validate: 3 events (INSERT, UPDATE, DELETE)\n   - For UPDATE: `beforeImage.name='Alice'`, `afterImage.name='Alicia'`\n   - Check `eventId` is deterministic and unique per operation\n\n4. **Test restart recovery:**\n   - Note `lastProcessedLSN` from `LogConnector`\n   - Stop pipeline\n   - Insert new row in database\n   - Restart pipeline\n   - Verify: Pipeline reads from saved LSN, captures new row event, does NOT re-emit old events\n\n**Success Signal:** All verification steps pass; pipeline correctly handles the complete lifecycle of a row with proper images and position tracking.\n\n#### Milestone 2: Event Streaming & Delivery\n\n**Checkpoint Goal:** Validate that change events are reliably delivered to consumers with at-least-once semantics and ordering guarantees per primary key, while handling backpressure.\n\n| Acceptance Criteria | Verification Method | Expected Result | Failure Indicators |\n|---------------------|---------------------|-----------------|-------------------|\n| Events published to Kafka partitioned by table and primary key | Integration test with Kafka, inspect message metadata | Events for same table+pk go to same partition; different pkeys may go to different partitions | All events to single partition, random partitioning, partition imbalance |\n| At-least-once delivery after transient failures | Simulate Kafka broker restart during production | Consumer eventually receives all events (may have duplicates); producer logs show retries | Events lost permanently, pipeline hangs, consumer starves |\n| Events for same primary key delivered in order | Concurrent updates to same row, verify sequence | Consumer sees events in commit order; no timestamp inversion for same pk | Out-of-order delivery, UPDATE before INSERT for same key |\n| Consumer lag monitoring and alerts | Test with slow consumer, fast producer | Lag metric increases, alert triggers when threshold exceeded | No lag reporting, false alerts, missing alert on actual lag |\n\n**Verification Procedure for Milestone 2:**\n\n1. **Setup partitioned streaming test:**\n   - Start Kafka container with 3 partitions for test topic\n   - Create pipeline with `KafkaEventPublisher` configured for partitioning by `tableName` and primary key\n   - Start verification consumer tracking partition assignments\n\n2. **Test at-least-once delivery:**\n   - Begin producing events\n   - Midway, kill Kafka broker container\n   - Wait 10 seconds, restart broker\n   - Continue producing\n   - Verify consumer receives all events (compare source vs. consumed count)\n   - Allow for small number of duplicates (retry behavior)\n\n3. **Test ordering guarantee:**\n   - Create 3 threads concurrently updating same row with sequence numbers\n   - Each thread: `UPDATE table SET seq = X WHERE id = 1`\n   - Verify consumer sees monotonic increasing seq values only\n   - No skipped sequence numbers (unless transaction rolled back)\n\n4. **Test backpressure propagation:**\n   - Configure consumer with 1-second artificial processing delay\n   - Produce events at high rate (1000/second)\n   - Monitor `KafkaEventPublisher.inFlightEventCount`\n   - Verify it plateaus (doesn't grow unbounded)\n   - Observe `LogConnector` enters PAUSED state or slows ingestion\n\n**Success Signal:** Events are reliably delivered despite failures, ordering per primary key is preserved, and the system gracefully handles consumer slowdowns without resource exhaustion.\n\n#### Milestone 3: Schema Evolution & Compatibility\n\n**Checkpoint Goal:** Validate that schema changes are handled gracefully with versioning, compatibility checking, and consumer notification.\n\n| Acceptance Criteria | Verification Method | Expected Result | Failure Indicators |\n|---------------------|---------------------|-----------------|-------------------|\n| Schema registry stores versioned schemas | Unit/integration tests with registry | Schema stored with version ID, retrievable by ID/version, history maintained | Missing versions, incorrect retrieval, no version increment |\n| Column additions pass backward compatibility | Test: `ALTER TABLE ADD COLUMN new_col INT DEFAULT 0` | New schema registered successfully, events include new column, old consumers can still read | Compatibility check fails incorrectly, events unreadable by old consumers |\n| Column removal fails forward compatibility | Test: `ALTER TABLE DROP COLUMN existing_col` (if nullable) | Schema registration rejected (if forward compatibility required) or warning issued | Allowed when shouldn't be, data loss occurs |\n| Schema change events notify consumers | Test: ALTER TABLE, monitor schema change topic | `SchemaChangeEvent` published with old/new schema IDs, consumers can update | No event published, event missing critical information |\n\n**Verification Procedure for Milestone 3:**\n\n1. **Setup schema evolution test:**\n   - Initialize `SchemaRegistry` (file-based or real)\n   - Start pipeline with schema integration\n   - Register initial schema for test table\n\n2. **Test backward-compatible addition:**\n   - Execute: `ALTER TABLE test ADD COLUMN email VARCHAR(100) NULL`\n   - Verify: New schema version registered (v2)\n   - Insert row with email value\n   - Verify event contains email field\n   - Test old consumer (using v1 schema) can deserialize event (email field ignored)\n\n3. **Test forward compatibility violation:**\n   - Set compatibility mode to `FORWARD` (or `FULL`)\n   - Attempt: `ALTER TABLE test DROP COLUMN name`\n   - Verify: Schema registration rejected with clear error\n   - Verify: No `SchemaChangeEvent` published\n\n4. **Test schema change notification:**\n   - Subscribe consumer to schema change topic\n   - Execute compatible ALTER TABLE\n   - Verify consumer receives `SchemaChangeEvent` within 5 seconds\n   - Event contains: `tableName`, `oldSchemaId`, `newSchemaId`, `changeTimestamp`\n\n5. **Test type coercion rules:**\n   - Initial schema: `column1 INT`\n   - New schema: `column1 BIGINT` (widening)\n   - Verify: Allowed (backward compatible)\n   - New schema: `column1 SMALLINT` (narrowing)\n   - Verify: Rejected or warned about potential data loss\n\n**Success Signal:** Schema registry properly manages versions, compatibility checks prevent breaking changes, and consumers are notified of evolution to adapt their processing logic.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n\n| Component | Simple Option (Getting Started) | Advanced Option (Production-like) |\n|-----------|---------------------------------|----------------------------------|\n| Unit Testing | JUnit 5 + AssertJ + Mockito | JUnit 5 + AssertJ + Mockito + JaCoCo (code coverage) |\n| Database Integration | Testcontainers PostgreSQL/MySQL | Testcontainers with custom initialization scripts |\n| Kafka Integration | Testcontainers Kafka | Testcontainers Kafka with Schema Registry container |\n| Schema Testing | In-memory `FileBasedSchemaRegistry` | Integration with actual Schema Registry (e.g., Confluent) |\n| Load/Stress Testing | Simple multithreaded producers | Gatling/Taurus for systematic load testing |\n| Chaos Testing | Manual failure injection | Chaos Mesh/Litmus for automated chaos experiments |\n| E2E Test Orchestration | JUnit `@Testcontainers` with `@BeforeAll` | Dedicated test runner with separate monitoring |\n\n#### B. Recommended File/Module Structure\n\n```\ncdc-system/\n├── src/main/java/com/cdc/\n│   ├── core/                    # Core domain types and interfaces\n│   │   ├── ChangeEvent.java\n│   │   ├── RawLogEntry.java\n│   │   └── SchemaVersion.java\n│   ├── connector/               # Milestone 1 components\n│   │   ├── LogConnector.java\n│   │   ├── parser/\n│   │   │   ├── LogParser.java\n│   │   │   ├── PostgresWalParser.java\n│   │   │   └── MySqlBinlogParser.java\n│   │   └── offset/\n│   │       ├── OffsetStore.java\n│   │       └── FileOffsetStore.java\n│   ├── builder/                 # ChangeEventBuilder\n│   │   ├── ChangeEventBuilder.java\n│   │   ├── TransactionState.java\n│   │   └── TransactionOperation.java\n│   ├── streaming/               # Milestone 2 components\n│   │   ├── EventStreamer.java\n│   │   ├── KafkaEventPublisher.java\n│   │   ├── DeliveryCallback.java\n│   │   └── metrics/\n│   │       └── StreamerMetrics.java\n│   ├── schema/                  # Milestone 3 components\n│   │   ├── SchemaRegistry.java\n│   │   ├── FileBasedSchemaRegistry.java\n│   │   ├── SchemaChangeEvent.java\n│   │   └── compatibility/\n│   │       └── CompatibilityChecker.java\n│   └── config/                  # Configuration\n│       ├── AppConfig.java\n│       ├── DatabaseConfig.java\n│       └── ConfigLoader.java\n└── src/test/java/com/cdc/       # Test structure mirrors main\n    ├── unit/\n    │   ├── connector/\n    │   │   ├── LogConnectorTest.java\n    │   │   └── parser/\n    │   │       ├── PostgresWalParserTest.java\n    │   │       └── fixtures/    # Binary log fixture files\n    │   ├── builder/\n    │   │   └── ChangeEventBuilderTest.java\n    │   ├── streaming/\n    │   │   └── KafkaEventPublisherTest.java\n    │   └── schema/\n    │       └── SchemaRegistryTest.java\n    ├── integration/\n    │   ├── DatabaseIntegrationTest.java\n    │   ├── KafkaIntegrationTest.java\n    │   └── resources/\n    │       └── testcontainers.properties\n    └── e2e/\n        ├── DeliveryGuaranteeTest.java\n        ├── SchemaEvolutionTest.java\n        └── RecoveryTest.java\n```\n\n#### C. Infrastructure Starter Code\n\n**Testcontainers Configuration Helper:**\n```java\n// File: src/test/java/com/cdc/testutils/TestContainersSetup.java\npackage com.cdc.testutils;\n\nimport org.testcontainers.containers.GenericContainer;\nimport org.testcontainers.containers.KafkaContainer;\nimport org.testcontainers.containers.PostgreSQLContainer;\nimport org.testcontainers.utility.DockerImageName;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class TestContainersSetup {\n    \n    private static final String POSTGRES_IMAGE = \"postgres:15-alpine\";\n    private static final String KAFKA_IMAGE = \"confluentinc/cp-kafka:7.4.0\";\n    \n    private static PostgreSQLContainer<?> postgresContainer;\n    private static KafkaContainer kafkaContainer;\n    \n    public static synchronized PostgreSQLContainer<?> getPostgresContainer() {\n        if (postgresContainer == null) {\n            postgresContainer = new PostgreSQLContainer<>(DockerImageName.parse(POSTGRES_IMAGE))\n                .withDatabaseName(\"cdc_test\")\n                .withUsername(\"testuser\")\n                .withPassword(\"testpass\")\n                .withCommand(\"postgres\", \"-c\", \"wal_level=logical\", \"-c\", \"max_replication_slots=5\");\n            postgresContainer.start();\n        }\n        return postgresContainer;\n    }\n    \n    public static synchronized KafkaContainer getKafkaContainer() {\n        if (kafkaContainer == null) {\n            kafkaContainer = new KafkaContainer(DockerImageName.parse(KAFKA_IMAGE))\n                .withEnv(\"KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR\", \"1\")\n                .withEnv(\"KAFKA_TRANSACTION_STATE_LOG_MIN_ISR\", \"1\")\n                .withEnv(\"KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS\", \"1\");\n            kafkaContainer.start();\n        }\n        return kafkaContainer;\n    }\n    \n    public static Map<String, String> getPostgresConnectionProperties() {\n        PostgreSQLContainer<?> pg = getPostgresContainer();\n        Map<String, String> props = new HashMap<>();\n        props.put(\"database.host\", pg.getHost());\n        props.put(\"database.port\", String.valueOf(pg.getFirstMappedPort()));\n        props.put(\"database.name\", pg.getDatabaseName());\n        props.put(\"database.username\", pg.getUsername());\n        props.put(\"database.password\", pg.getPassword());\n        return props;\n    }\n    \n    public static String getKafkaBootstrapServers() {\n        return getKafkaContainer().getBootstrapServers();\n    }\n    \n    public static void stopAll() {\n        if (kafkaContainer != null) {\n            kafkaContainer.stop();\n            kafkaContainer = null;\n        }\n        if (postgresContainer != null) {\n            postgresContainer.stop();\n            postgresContainer = null;\n        }\n    }\n}\n```\n\n**Binary Log Fixture Loader:**\n```java\n// File: src/test/java/com/cdc/testutils/BinaryFixtureLoader.java\npackage com.cdc.testutils;\n\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\npublic class BinaryFixtureLoader {\n    \n    public static byte[] loadFixture(String fixtureName) throws IOException {\n        // Load from classpath first (for unit tests)\n        InputStream is = BinaryFixtureLoader.class.getClassLoader()\n            .getResourceAsStream(\"fixtures/\" + fixtureName);\n        if (is != null) {\n            return is.readAllBytes();\n        }\n        \n        // Fallback to file system (for development)\n        Path fixturePath = Paths.get(\"src/test/resources/fixtures\", fixtureName);\n        if (Files.exists(fixturePath)) {\n            return Files.readAllBytes(fixturePath);\n        }\n        \n        throw new IOException(\"Fixture not found: \" + fixtureName);\n    }\n    \n    public static void saveFixture(String fixtureName, byte[] data) throws IOException {\n        Path fixtureDir = Paths.get(\"src/test/resources/fixtures\");\n        if (!Files.exists(fixtureDir)) {\n            Files.createDirectories(fixtureDir);\n        }\n        Path fixturePath = fixtureDir.resolve(fixtureName);\n        Files.write(fixturePath, data);\n    }\n    \n    // Helper to capture real WAL/binlog snippets for testing\n    public static void capturePostgresWalSnippet(String outputFixtureName, \n                                                 String tableName, \n                                                 String... sqlStatements) {\n        // This would be implemented to connect to PostgreSQL,\n        // enable logical decoding, execute SQL, and save WAL segment\n        // For now, it's a placeholder showing the intent\n        System.out.println(\"Capture WAL snippet: \" + outputFixtureName);\n        System.out.println(\"Execute on table \" + tableName + \":\");\n        for (String sql : sqlStatements) {\n            System.out.println(\"  \" + sql);\n        }\n    }\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**E2E Delivery Guarantee Test Skeleton:**\n```java\n// File: src/test/java/com/cdc/e2e/DeliveryGuaranteeTest.java\npackage com.cdc.e2e;\n\nimport com.cdc.core.ChangeEvent;\nimport com.cdc.config.AppConfig;\nimport com.cdc.connector.LogConnector;\nimport com.cdc.builder.ChangeEventBuilder;\nimport com.cdc.streaming.KafkaEventPublisher;\nimport org.junit.jupiter.api.Test;\nimport org.testcontainers.junit.jupiter.Testcontainers;\nimport java.util.List;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\n\n@Testcontainers\npublic class DeliveryGuaranteeTest {\n    \n    @Test\n    public void testAtLeastOnceDelivery_WithKafkaBrokerRestart() throws Exception {\n        // TODO 1: Setup Testcontainers for PostgreSQL and Kafka\n        //   - Use TestContainersSetup.getPostgresContainer()\n        //   - Use TestContainersSetup.getKafkaContainer()\n        \n        // TODO 2: Create and configure AppConfig for test environment\n        //   - Set database connection properties from test container\n        //   - Set Kafka bootstrap servers from test container\n        //   - Configure producer with idempotence=true, acks=all\n        \n        // TODO 3: Initialize pipeline components\n        //   - LogConnector with test configuration\n        //   - ChangeEventBuilder\n        //   - KafkaEventPublisher\n        \n        // TODO 4: Start pipeline and verification consumer\n        //   - Start pipeline components\n        //   - Start separate thread running verification consumer\n        //   - Verification consumer should count received events\n        \n        // TODO 5: Generate test data in database\n        //   - Create test table with primary key\n        //   - Insert 100 rows in a transaction\n        \n        // TODO 6: Simulate failure during processing\n        //   - After 50 events published, stop Kafka container abruptly\n        //   - Wait 2 seconds, then restart Kafka container\n        \n        // TODO 7: Continue data generation\n        //   - Insert another 100 rows\n        \n        // TODO 8: Wait for stabilization and verify\n        //   - Wait up to 30 seconds for pipeline to recover\n        //   - Stop pipeline and consumer\n        //   - Verify total events received >= 200 (allowing duplicates)\n        //   - Log duplicate count for analysis\n        \n        // TODO 9: Cleanup test resources\n        //   - Drop test table\n        //   - Close all connections\n    }\n    \n    @Test \n    public void testOrderingPerPrimaryKey_WithConcurrentUpdates() throws Exception {\n        // TODO 1: Setup test environment with single row to update\n        \n        // TODO 2: Create multiple producer threads\n        //   - Each thread updates same row with sequential value\n        //   - Use synchronized counter to generate unique sequence numbers\n        \n        // TODO 3: Start consumer that tracks sequence per primary key\n        //   - Consumer should reject out-of-order sequences\n        //   - Fail test if sequence gap detected\n        \n        // TODO 4: Run concurrent updates for 10 seconds\n        \n        // TODO 5: Verify ordering guarantee\n        //   - All sequences processed in order for each primary key\n        //   - No gaps in sequence (unless transaction rolled back)\n        //   - Log any anomalies for debugging\n    }\n}\n```\n\n**Schema Evolution Test Skeleton:**\n```java\n// File: src/test/java/com/cdc/e2e/SchemaEvolutionTest.java\npackage com.cdc.e2e;\n\nimport com.cdc.schema.SchemaRegistry;\nimport com.cdc.schema.SchemaChangeEvent;\nimport org.junit.jupiter.api.Test;\nimport org.testcontainers.junit.jupiter.Testcontainers;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.LinkedBlockingQueue;\nimport java.util.concurrent.TimeUnit;\n\n@Testcontainers\npublic class SchemaEvolutionTest {\n    \n    @Test\n    public void testBackwardCompatibleColumnAddition() throws Exception {\n        // TODO 1: Setup test table with initial schema (id INT, name VARCHAR)\n        //   - Register initial schema v1 in SchemaRegistry\n        \n        // TODO 2: Start schema change event listener\n        //   - Subscribe to schema change topic\n        //   - Collect SchemaChangeEvents in BlockingQueue\n        \n        // TODO 3: Execute backward-compatible ALTER TABLE\n        //   - ALTER TABLE test ADD COLUMN email VARCHAR(100) NULL\n        \n        // TODO 4: Verify schema evolution\n        //   - Wait for SchemaChangeEvent in queue (timeout 10s)\n        //   - Verify event contains oldSchemaId=v1, newSchemaId=v2\n        //   - Retrieve v2 schema, verify it has email column\n        \n        // TODO 5: Test data compatibility\n        //   - Insert row with email value\n        //   - Consume event with v1 schema deserializer\n        //   - Verify event deserializes without error (email field ignored)\n        //   - Consume same event with v2 schema deserializer\n        //   - Verify email field present with correct value\n    }\n    \n    @Test\n    public void testForwardCompatibilityViolationDetection() throws Exception {\n        // TODO 1: Setup SchemaRegistry with FORWARD compatibility mode\n        \n        // TODO 2: Attempt to remove a nullable column\n        //   - ALTER TABLE test DROP COLUMN name\n        \n        // TODO 3: Verify schema registration rejected\n        //   - SchemaRegistry.registerSchema should throw CompatibilityException\n        //   - No SchemaChangeEvent published\n        \n        // TODO 4: Verify pipeline continues with old schema\n        //   - Insert new row (should work with old schema)\n        //   - Events should still use v1 schema ID\n    }\n}\n```\n\n#### E. Language-Specific Hints (Java)\n\n1. **Use JUnit 5's `@Testcontainers` and `@Container`:** These annotations simplify container lifecycle management. Static containers are shared across tests in a class; non-static containers are restarted per test.\n\n2. **Leverage AssertJ for fluent assertions:** Its rich assertion library makes test code more readable:\n   ```java\n   assertThat(changeEvent.getAfterImage())\n       .containsEntry(\"id\", 1)\n       .containsEntry(\"name\", \"Alice\")\n       .hasSize(2);\n   ```\n\n3. **Use `Awaitility` for asynchronous testing:** When testing eventual consistency (like at-least-once delivery), Awaitility provides clean polling syntax:\n   ```java\n   await().atMost(30, SECONDS)\n          .until(() -> consumer.getReceivedCount() >= expectedCount);\n   ```\n\n4. **Mock external dependencies with Mockito:** For unit tests, mock interfaces like `SchemaRegistry` and `OffsetStore` to isolate component behavior.\n\n5. **Use `TemporaryFolder` JUnit extension for file-based tests:** When testing `FileOffsetStore`, create temporary directories that auto-clean after tests.\n\n6. **Configure Kafka test producers/consumers with unique group IDs:** In integration tests, use `UUID.randomUUID().toString()` for consumer group IDs to avoid conflicts between test runs.\n\n7. **Capture logs for debugging:** Use SLF4J with Logback and configure a memory appender to capture logs during tests for post-mortem analysis.\n\n#### F. Milestone Checkpoint Verification\n\n**Milestone 1 Checkpoint Command:**\n```bash\n# Run all unit and integration tests for Milestone 1 components\n./gradlew test --tests \"*LogConnector*\" --tests \"*ChangeEventBuilder*\"\n# Or for Maven\nmvn test -Dtest=\"*LogConnector*,*ChangeEventBuilder*\"\n```\n\n**Expected Output:**\n- All tests pass (green)\n- Log output shows successful connection to test database\n- Test report indicates parsing of INSERT/UPDATE/DELETE operations\n- No exceptions or warnings about missing LSN tracking\n\n**Manual Verification Steps:**\n1. Start a local PostgreSQL instance with logical replication enabled\n2. Run the CDC pipeline against a test table\n3. Execute: `INSERT → UPDATE → DELETE` sequence\n4. Check console/logs for three `ChangeEvent` emissions with correct before/after images\n5. Stop and restart pipeline, verify it resumes from correct position\n\n**Failure Indicators & Diagnosis:**\n- **No events emitted:** Check database connection and replication slot creation\n- **Missing before/after images:** Verify UPDATE operation parsing logic\n- **Cannot resume after restart:** Check `FileOffsetStore` persistence logic\n\n**Milestone 2 Checkpoint Command:**\n```bash\n# Run streaming delivery tests (requires Docker for Testcontainers)\n./gradlew integrationTest --tests \"*DeliveryGuaranteeTest*\"\n```\n\n**Expected Output:**\n- Tests pass with occasional warnings about duplicates (expected for at-least-once)\n- Kafka producer metrics show retries during broker restart test\n- Ordering test shows monotonic sequences for each primary key\n\n**Manual Verification Steps:**\n1. Start pipeline with Kafka connection\n2. Produce concurrent updates to same row from multiple threads\n3. Consume events and verify sequence order\n4. Kill Kafka broker mid-stream, restart, verify no permanent data loss\n\n**Failure Indicators & Diagnosis:**\n- **Events lost permanently:** Check producer `acks=all` and idempotence configuration\n- **Out-of-order delivery:** Verify partition key includes primary key, not just table name\n- **Pipeline hangs on broker failure:** Check producer timeout and retry configurations\n\n**Milestone 3 Checkpoint Command:**\n```bash\n# Run schema evolution tests\n./gradlew test --tests \"*SchemaEvolutionTest*\"\n```\n\n**Expected Output:**\n- Tests pass showing backward compatibility working\n- Forward compatibility violation correctly rejected\n- Schema change events published and consumable\n\n**Manual Verification Steps:**\n1. Register initial schema for a table\n2. Add nullable column, verify new events include it\n3. Try to remove column with forward compatibility enabled, verify rejection\n4. Check schema change topic for notification events\n\n**Failure Indicators & Diagnosis:**\n- **Schema registration succeeds when it should fail:** Check compatibility mode configuration\n- **Old consumers cannot read new events:** Verify backward compatibility logic\n- **No schema change events:** Check schema registry integration with event publisher\n\n#### G. Debugging Tips for Test Failures\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Unit tests pass but integration tests fail | Differences between mock data and real database behavior | Compare binary log fixture with actual WAL/binlog output using hex dump | Update fixtures with real database captures using `BinaryFixtureLoader.capturePostgresWalSnippet` |\n| Pipeline hangs in integration test | Deadlock between producer and consumer threads | Use thread dump (`jstack`) or debug logging in `LogConnector` and `KafkaEventPublisher` | Implement proper shutdown hooks, use timeouts in blocking operations |\n| Events missing after restart test | Offset not persisted before shutdown | Check `FileOffsetStore.save()` is called during graceful shutdown | Call `save()` in `@PreDestroy` or shutdown hook, add flush to disk |\n| Duplicate events exceed expected count | Producer retries without deduplication | Check producer idempotence configuration, examine event IDs for duplicates | Enable idempotent producer, implement deduplication in consumer |\n| Schema evolution test times out | `SchemaChangeEvent` not published | Check schema registry integration, verify topic subscription | Ensure schema registry calls `publishSchemaChange()` after registration |\n| Consumer lag alert not firing | Metrics not collected or threshold too high | Check `StreamerMetrics` polling interval, verify consumer group exists | Reduce polling interval, verify consumer group naming in test |\n| Memory leak in long-running tests | Unbounded growth of `TransactionState` cache | Monitor `ChangeEventBuilder.getTransactionCount()` over time | Implement transaction timeout and cleanup of stale transactions |\n| Test passes locally but fails in CI | Resource constraints or timing differences | Compare container logs, check for slower startup in CI environment | Increase timeouts in CI configuration, add wait strategies for containers |\n\n\n> **Milestone(s):** Milestone 1 (log parsing issues), Milestone 2 (delivery and ordering problems), Milestone 3 (schema compatibility bugs)\n\n## 12. Debugging Guide\n\nDebugging a Change Data Capture system is fundamentally different from debugging a typical application. You are debugging a **continuous, stateful pipeline** that processes a never-ending stream of binary log data while maintaining complex invariants about ordering, delivery, and schema compatibility. The system operates in real-time, and failures often manifest as subtle data inconsistencies downstream—missing records, duplicate events, or unreadable schema versions—hours or days after the initial problem occurred.\n\nThink of CDC debugging as being an **air traffic controller for data streams**. You cannot stop the flow (database transactions keep happening), but you must monitor multiple moving parts simultaneously—log positions, event queues, consumer offsets, and schema versions—to detect anomalies and reroute traffic when something goes wrong. This section provides the radar screens and emergency checklists for that controller.\n\n### 12.1 Common Bugs and Fixes\n\nThis table categorizes the most frequent issues learners encounter when building a CDC system, organized by the component where they typically manifest. Each entry follows a **Symptom → Root Cause → Diagnostic Steps → Fix** pattern.\n\n| Symptom | Root Cause | Diagnostic Steps | Fix |\n|---------|------------|------------------|-----|\n| **Log Connector & Parser (Milestone 1)** |\n| Pipeline processes zero events; no errors in logs. | **Log Position Reset**: The `LogConnector` started reading from an incorrect `logSequenceNumber`, typically because the `OffsetStore` was corrupted or the replication slot was recreated. | 1. Check `LogConnector.lastProcessedLSN` in health endpoint.<br>2. Compare with `pg_replication_slots` (PostgreSQL) or `SHOW MASTER STATUS` (MySQL).<br>3. Verify `FileOffsetStore` file contents are valid. | Reset offset: manually set `lastProcessedLSN` to current database LSN (losing missed events) or perform a full table resync. |\n| Events for the same row appear out-of-order in the stream. | **Transaction Boundary Violation**: The `ChangeEventBuilder` emitted events before the transaction committed, or the `LogParser` processed WAL entries in non-commit order due to parallel parsing. | 1. Inspect `ChangeEvent.transactionId` and `commitTimestamp`—same transaction events should share ID.<br>2. Check `LogParser` implementation for concurrent processing without proper ordering. | Ensure `ChangeEventBuilder` buffers operations per transaction and only emits on commit. Use single-threaded log parsing or sequence numbers. |\n| UPDATE events show identical `beforeImage` and `afterImage`. | **Partial Row Capture**: The `LogParser` only captures changed columns (PostgreSQL toast, MySQL partial updates) but the event builder fails to merge with previous row state. | 1. Examine raw `RawLogEntry.rowData`—does it contain all columns or just changed ones?<br>2. Check if `ChangeEventBuilder` maintains row state across updates within a transaction. | Implement row state caching in `TransactionState.lastOperationByKey` to merge partial updates with previous full image. |\n| **Event Streamer & Delivery (Milestone 2)** |\n| Consumer receives duplicate events for the same change. | **At-Least-Once Duplication**: The `KafkaEventPublisher` crashed after sending but before persisting the `lastProcessedLSN`, causing reprocessing of the same batch on restart. | 1. Check for `KafkaEventPublisher` restart logs near duplicate events.<br>2. Verify `pendingEvents` map is cleared only after offset persistence.<br>3. Look for \"producer idempotence\" configuration errors. | Implement **idempotent publishing**: store offset **after** Kafka ack (in `DeliveryCallback`). Use Kafka's transactional producer with `enable.idempotence=true`. |\n| Consumer lag increases indefinitely; `KafkaEventPublisher` logs \"backpressure enabled\". | **Consumer Processing Stalled**: Downstream consumers stopped reading, causing Kafka partitions to fill, triggering `KafkaEventPublisher.backpressureSignal`. | 1. Check consumer health and logs.<br>2. Use `kafka-consumer-groups` to verify consumer offsets are advancing.<br>3. Verify `max.poll.records` and `max.partition.fetch.bytes` aren't too small. | 1. Restart stuck consumers.<br>2. Increase partition count for hot tables.<br>3. Implement dead-letter queue for poison-pill messages. |\n| Events for same primary key appear in different partitions, breaking ordering. | **Incorrect Partition Key**: `KafkaEventPublisher` uses `ChangeEvent.sourceTable` as partition key instead of composite `table+primaryKey`. | 1. Inspect event metadata in Kafka using `kafka-console-consumer`.<br>2. Check `KafkaEventPublisher`'s key generation logic. | Partition key must be deterministic: `String.format(\"%s|%s\", tableName, primaryKeyHash)`. Use `MurmurHash` for even distribution. |\n| **Schema Registry & Evolution (Milestone 3)** |\n| Consumers fail to deserialize events with \"Unknown field\" or \"Missing required field\". | **Schema Cache Incoherency**: Consumer uses outdated schema version because `SchemaRegistry` cache wasn't invalidated after schema change. | 1. Compare `ChangeEvent.schemaVersionId` with `SchemaRegistry.getLatestSchema()`.<br>2. Check if `SchemaChangeEvent` was emitted and processed.<br>3. Verify consumer schema cache TTL settings. | 1. Implement cache invalidation on schema change (broadcast event).<br>2. Embed minimal schema in `ChangeEvent` envelope for fallback.<br>3. Use `SchemaVersion.version` as Kafka message header. |\n| ALTER TABLE ADD COLUMN causes consumer crashes despite backward compatibility. | **Default Value Missing**: New nullable column added without `DEFAULT`, causing serialization to omit field, breaking consumer expecting `null`. | 1. Check `SchemaVersion.columnDefinitions` for the new column's `nullable` and `defaultValue`.<br>2. Verify `EventSerializer` handling of missing fields. | Schema registry must reject column additions without `nullable=true` OR `defaultValue`. `EventSerializer` must fill missing fields with schema-defined default. |\n| DROP COLUMN breaks all existing consumers immediately. | **Forward Compatibility Violation**: Schema registry allowed breaking change because compatibility mode was `COMPATIBILITY_BACKWARD` instead of `COMPATIBILITY_FULL`. | 1. Check `SchemaVersion.compatibilityMode` at registration time.<br>2. Review `SchemaRegistry.checkCompatibility` logic for forward checks. | Set default compatibility to `COMPATIBILITY_FULL`. Implement **soft deletes**: mark column deprecated but keep in schema for forward compatibility. |\n| **Cross-Component Issues** |\n| Pipeline stops after database restart with \"replication slot not found\". | **Slot Cleanup**: PostgreSQL automatically drops inactive replication slots, or MySQL binlog expired due to `expire_logs_days`. | 1. Check PostgreSQL `pg_replication_slots` slot status.<br>2. Verify MySQL `binlog_expire_logs_seconds` setting.<br>3. Look for \"slot\", \"missing\", or \"expired\" in error logs. | 1. Increase `wal_keep_size` (PostgreSQL) or `binlog_expire_logs_seconds` (MySQL).<br>2. Implement **slot heartbeat**: periodically update slot LSN even during idle periods. |\n| Memory usage grows unbounded; eventual OutOfMemoryError. | **Transaction Leak**: `ChangeEventBuilder.activeTransactions` never removes completed transactions, or `TransactionState` grows too large. | 1. Monitor `ChangeEventBuilder.getTransactionCount()` over time—should stabilize.<br>2. Check for missing `COMMIT`/`ROLLBACK` log entries.<br>3. Look for large transactions (>10k operations). | 1. Implement **transaction timeout**: flush transactions older than `transactionTimeoutMs`.<br>2. Add memory-bound to `activeTransactions` with LRU eviction. |\n| Events arrive with significant delay (seconds to minutes). | **Pipeline Congestion**: One component (parser, builder, streamer) becomes bottleneck, causing queue buildup. | 1. Measure queue sizes: `LogConnector.buffer`, `KafkaEventPublisher.pendingEvents`.<br>2. Profile CPU usage per component thread.<br>3. Check for synchronous network calls in critical path. | 1. Increase batch sizes.<br>2. Parallelize independent tables.<br>3. Use async I/O for Kafka publishing. |\n\n**Mental Model: The Pipeline Pressure Gauge**\nThink of the CDC pipeline as a series of connected pipes with pressure sensors. Normal operation maintains steady pressure. A blockage upstream causes pressure drop downstream; a blockage downstream causes pressure buildup upstream. Monitoring queue sizes (`buffer`, `pendingEvents`) and processing latency (`commitTimestamp` vs current time) gives you the pressure readings. Debugging is about locating the blockage and clearing it without bursting the pipes.\n\n#### Architecture Decision: Debuggability vs Performance\n\n> **Decision: Structured Logging Over Binary Performance**\n> - **Context**: The CDC pipeline processes high-volume binary data where every microsecond counts. Learners often optimize by removing \"expensive\" logging.\n> - **Options Considered**:\n>   1. **Minimal logging**: Only log errors and fatal events for maximum performance.\n>   2. **Structured debug logging**: Log key pipeline state (LSN, transaction ID, event counts) with sampling or debug flags.\n>   3. **Comprehensive audit logging**: Log every event transformation for complete debuggability.\n> - **Decision**: Option 2—structured debug logging with runtime configuration.\n> - **Rationale**: Performance impact is minimal when logging is guarded by `if (logger.isDebugEnabled())` and uses parameterized messages. The ability to enable debug logs in production during incidents is invaluable for diagnosing data flow issues without restarting or deploying new code.\n> - **Consequences**: Adds slight overhead even when disabled (condition checks). Requires careful log message design to avoid exposing sensitive data. Provides crucial visibility into pipeline health and data transformation correctness.\n\n| Logging Strategy | Pros | Cons | Suitable For |\n|------------------|------|------|--------------|\n| Minimal | Maximum performance, smallest log volume | Impossible to debug pipeline state without code changes | Production-only deployments with perfect monitoring |\n| Structured Debug | Balance of performance and debuggability, runtime toggle | Slight overhead, requires careful message design | Development, testing, and production troubleshooting |\n| Comprehensive Audit | Complete event lineage, perfect for forensic analysis | Significant performance overhead, massive log volume | Regulatory environments requiring full audit trails |\n\n⚠️ **Pitfall: Silent Data Loss**\n**Description**: The pipeline appears healthy (no errors), but downstream consumers are missing events for specific time ranges or tables.\n**Why it's wrong**: Data loss violates the core guarantee of CDC. The issue often stems from gaps in log sequence numbers that the parser silently skips.\n**How to fix**: Implement **gap detection**: when `LogConnector` reads LSNs, compare consecutive numbers. If a gap exceeds threshold (e.g., more than 1), emit a `GapDetectionEvent` to alert operators and trigger investigation. Store last processed LSN atomically with event publishing.\n\n### 12.2 Inspection and Tracing Techniques\n\nEffective CDC debugging requires inspecting the **four pillars of pipeline state**: Log Position, Event Content, Consumer Offset, and Schema Version. This section provides concrete techniques for each.\n\n#### 12.2.1 Log Position Inspection\n\nThe log position (`logSequenceNumber` or LSN) is the pipeline's **checkpoint**. If it's wrong, you'll have data loss or duplication.\n\n**Diagnostic Commands**:\n| Database | Command | Purpose |\n|----------|---------|---------|\n| PostgreSQL | `SELECT pg_current_wal_lsn();`<br>`SELECT * FROM pg_replication_slots;` | Get current WAL LSN and replication slot status |\n| MySQL | `SHOW MASTER STATUS;`<br>`SHOW BINARY LOGS;` | Get current binlog position and file list |\n| Both | Check `FileOffsetStore` file: `cat /path/to/offsets.properties` | Verify persisted LSN matches database |\n\n**Health Endpoint Implementation**:\nAdd a health endpoint to `CdcPipeline` that exposes:\n- `LogConnector.lastProcessedLSN`\n- Current database LSN (queried live)\n- Lag = current LSN - last processed LSN (in bytes or number of transactions)\n- `LogConnector.running` state\n\n**Interpreting Results**:\n- **Zero lag**: Pipeline is caught up.\n- **Growing lag**: Pipeline cannot keep up with database write rate.\n- **Negative or impossible lag**: LSN comparison logic broken (different formats).\n- **Last processed LSN stale**: Pipeline stopped processing.\n\n#### 12.2.2 Event Content Inspection\n\nYou need to see what events look like at different pipeline stages without affecting production flow.\n\n**Technique 1: Diagnostic Topic**\nCreate a separate Kafka topic `cdc_debug` where you publish a copy of every event. Use a `KafkaEventPublisher` with sampling (e.g., 1% of events) or conditional publishing (only for specific tables). Consume this topic with a simple console consumer to see raw events.\n\n**Technique 2: In-Memory Event Snapshot**\nImplement a circular buffer in `ChangeEventBuilder` that stores the last N events processed. Expose via JMX or HTTP endpoint for immediate inspection. Include full event details: `eventId`, `beforeImage`, `afterImage`, `schemaVersionId`.\n\n**Event Content Checklist**:\nWhen inspecting an event, verify:\n1. **Operation Type**: Matches database operation (INSERT/UPDATE/DELETE).\n2. **Primary Key Present**: In `beforeImage` (for UPDATE/DELETE) and `afterImage` (for INSERT/UPDATE).\n3. **Schema Version Valid**: `schemaVersionId` exists in registry.\n4. **Timestamps Monotonic**: `commitTimestamp` increases within same transaction.\n5. **Transaction Boundaries**: Events with same `transactionId` have contiguous `commitTimestamp`.\n\n#### 12.2.3 Consumer Offset Monitoring\n\nConsumer lag indicates if downstream systems are keeping up. Monitor both **partition lag** (offset difference) and **processing latency** (time from commit to consumer ack).\n\n**Kafka Consumer Groups**:\n```bash\n# Check consumer group status\nkafka-consumer-groups --bootstrap-server localhost:9092 \\\n  --describe --group cdc-consumers\n\n# Output columns:\n# GROUP, TOPIC, PARTITION, CURRENT-OFFSET, LOG-END-OFFSET, LAG, CONSUMER-ID\n```\n**Interpretation**:\n- **LAG = 0**: Consumer caught up.\n- **LAG growing**: Consumer falling behind.\n- **CURRENT-OFFSET not moving**: Consumer stuck or dead.\n- **Multiple consumers for same partition**: Rebalance in progress.\n\n**Implement Lag Alerting**:\nIn `StreamerMetrics`, periodically check consumer group lag and trigger alert if:\n1. Lag exceeds threshold (e.g., 10,000 messages).\n2. Lag growth rate indicates impending backlog.\n3. Consumer has been stuck at same offset for > timeout period.\n\n#### 12.2.4 Schema Version Tracing\n\nSchema issues manifest as deserialization failures. You need to trace which schema version each consumer uses.\n\n**Technique: Embedded Schema Lineage**:\nAdd schema metadata to `ChangeEvent` as headers (Kafka) or envelope fields:\n```json\n{\n  \"event\": { ... },\n  \"schema_meta\": {\n    \"version\": 3,\n    \"fingerprint\": \"sha256:abc123\",\n    \"compatibility\": \"BACKWARD\"\n  }\n}\n```\n\n**Schema Debug Endpoints**:\nExtend `SchemaRegistry` with diagnostic endpoints:\n- `GET /schemas/{table}/versions` - List all versions\n- `GET /schemas/{table}/diff/{v1}/{v2}` - Show changes between versions\n- `GET /events/by-schema/{schemaId}` - Find events using specific schema (requires indexing)\n\n**Common Schema Debugging Scenarios**:\n1. **Consumer reports \"Unknown field 'new_column'\"**: Check if consumer has old schema cached. Force cache refresh or restart consumer.\n2. **Serialization fails with \"Invalid type for field 'amount'\"**: Verify `ColumnType.javaClass` matches actual data type in database. Check for database type changes (INT → BIGINT).\n3. **Events appear with wrong schema version**: Check `ChangeEventBuilder.schemaCache` invalidation logic. Verify DDL event parsing correctly triggers schema registration.\n\n#### 12.2.5 Distributed Tracing for CDC\n\nFor production systems, implement distributed tracing to follow an event through the entire pipeline:\n\n**Trace Points**:\n1. **Database commit**: Capture transaction ID and LSN (requires database instrumentation).\n2. **Log parsing**: Trace entry → `RawLogEntry` transformation.\n3. **Event building**: Trace transaction assembly and deduplication.\n4. **Publishing**: Trace Kafka produce and ack.\n5. **Consumption**: Trace consumer processing.\n\n**Correlation IDs**:\nUse a consistent correlation ID across all stages:\n- Start with `transactionId` from database.\n- Propagate through `ChangeEvent.eventId`.\n- Include in Kafka message headers.\n- Log at every component with `MDC` (Mapped Diagnostic Context).\n\n**Visualization**:\nUse Jaeger or Zipkin to visualize pipeline latency bottlenecks:\n- Parse latency: time from LSN read to `RawLogEntry`.\n- Build latency: time from `RawLogEntry` to `ChangeEvent`.\n- Publish latency: time from `ChangeEvent` to Kafka ack.\n- End-to-end latency: time from database commit to consumer ack.\n\n### Implementation Guidance\n\n#### A. Technology Recommendations Table\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Log Position Inspection | Custom HTTP health endpoint with JSON response | Prometheus metrics exporter with Grafana dashboard |\n| Event Content Inspection | In-memory circular buffer exposed via REST endpoint | Elasticsearch indexing of events with Kibana visualization |\n| Consumer Offset Monitoring | Periodic `kafka-consumer-groups` script with email alerts | Kafka Streams lag analysis with real-time alerting to PagerDuty |\n| Schema Version Tracing | Schema registry with audit log table | Schema lineage tracking with Apache Atlas or DataHub |\n| Distributed Tracing | Log-based correlation with unique IDs per event | OpenTelemetry instrumentation with Jaeger visualization |\n\n#### B. Recommended File Structure\n```\ncdc-system/\n├── src/main/java/com/cdc/\n│   ├── pipeline/\n│   │   ├── CdcPipeline.java              # Main pipeline with health endpoint\n│   │   └── HealthStatus.java\n│   ├── debug/\n│   │   ├── EventSnapshotBuffer.java      # Circular buffer for event inspection\n│   │   ├── DiagnosticPublisher.java      # Publishes sample events to debug topic\n│   │   └── GapDetector.java              # Detects LSN gaps\n│   ├── metrics/\n│   │   ├── PipelineMetrics.java          # Tracks latency, throughput\n│   │   ├── LagMonitor.java               # Monitors consumer lag\n│   │   └── AlertManager.java             # Sends alerts on anomalies\n│   └── tracing/\n│       ├── CorrelationId.java            # Propagates correlation IDs\n│       ├── OpenTelemetryConfig.java      # Tracing configuration\n│       └── TracingInterceptor.java       # Adds tracing to components\n└── scripts/\n    ├── check_lag.sh                      # Kafka consumer lag checker\n    ├── validate_schema.py                # Schema compatibility validator\n    └── replay_events.py                  # Replays events from specific LSN\n```\n\n#### C. Infrastructure Starter Code: Event Snapshot Buffer\n```java\npackage com.cdc.debug;\n\nimport com.cdc.events.ChangeEvent;\nimport java.util.*;\nimport java.util.concurrent.atomic.AtomicInteger;\n\n/**\n * Circular buffer that stores the last N ChangeEvents for debugging.\n * Thread-safe for concurrent reads and writes.\n */\npublic class EventSnapshotBuffer {\n    private final ChangeEvent[] buffer;\n    private final int capacity;\n    private final AtomicInteger writeIndex = new AtomicInteger(0);\n    private volatile int size = 0;\n    \n    public EventSnapshotBuffer(int capacity) {\n        this.capacity = capacity;\n        this.buffer = new ChangeEvent[capacity];\n    }\n    \n    /**\n     * Add an event to the buffer, overwriting oldest if full.\n     */\n    public void add(ChangeEvent event) {\n        int index = writeIndex.getAndUpdate(i -> (i + 1) % capacity);\n        buffer[index] = event;\n        synchronized (this) {\n            size = Math.min(size + 1, capacity);\n        }\n    }\n    \n    /**\n     * Get the most recent events (newest first).\n     */\n    public List<ChangeEvent> getRecentEvents(int count) {\n        List<ChangeEvent> result = new ArrayList<>();\n        int currentSize;\n        int currentWriteIndex;\n        \n        synchronized (this) {\n            currentSize = size;\n            currentWriteIndex = writeIndex.get();\n        }\n        \n        count = Math.min(count, currentSize);\n        for (int i = 0; i < count; i++) {\n            // Calculate index in circular buffer (going backwards from write pointer)\n            int index = (currentWriteIndex - 1 - i + capacity) % capacity;\n            ChangeEvent event = buffer[index];\n            if (event != null) {\n                result.add(event);\n            }\n        }\n        return result;\n    }\n    \n    /**\n     * Find events by table name.\n     */\n    public List<ChangeEvent> getEventsByTable(String tableName, int maxResults) {\n        List<ChangeEvent> result = new ArrayList<>();\n        int currentSize;\n        int currentWriteIndex;\n        \n        synchronized (this) {\n            currentSize = size;\n            currentWriteIndex = writeIndex.get();\n        }\n        \n        for (int i = 0; i < currentSize && result.size() < maxResults; i++) {\n            int index = (currentWriteIndex - 1 - i + capacity) % capacity;\n            ChangeEvent event = buffer[index];\n            if (event != null && tableName.equals(event.getSourceTable())) {\n                result.add(event);\n            }\n        }\n        return result;\n    }\n    \n    public int getSize() {\n        synchronized (this) {\n            return size;\n        }\n    }\n    \n    public void clear() {\n        synchronized (this) {\n            Arrays.fill(buffer, null);\n            writeIndex.set(0);\n            size = 0;\n        }\n    }\n}\n```\n\n#### D. Core Logic Skeleton: Gap Detection in LogConnector\n```java\npackage com.cdc.connector;\n\nimport com.cdc.debug.GapDetectionEvent;\nimport java.util.*;\nimport java.math.BigInteger;\n\npublic class LogConnector {\n    // ... existing fields ...\n    private String previousLSN;\n    private final GapDetectionEvent.Builder gapEventBuilder;\n    \n    /**\n     * Process a batch of log entries, detecting gaps in LSN sequence.\n     */\n    protected List<RawLogEntry> processBatchWithGapDetection(List<RawLogEntry> entries) {\n        List<RawLogEntry> processed = new ArrayList<>();\n        \n        for (RawLogEntry entry : entries) {\n            // TODO 1: Convert current entry LSN and previousLSN to comparable format\n            // (PostgreSQL LSN is hex like \"0/16EFE78\", MySQL binlog position is numeric)\n            // Use a database-specific LSN comparator\n            \n            // TODO 2: If previousLSN is not null, check if there's a gap\n            // A gap means the difference > 1 (or > expected increment based on entry size)\n            \n            // TODO 3: If gap detected and larger than threshold (configurable):\n            // - Build GapDetectionEvent with previousLSN, currentLSN, gap size\n            // - Publish to special \"cdc_gap\" topic or alert channel\n            // - Log warning with gap details\n            \n            // TODO 4: Update previousLSN to current entry's LSN\n            \n            processed.add(entry);\n        }\n        \n        return processed;\n    }\n    \n    /**\n     * Compare two LSNs and return the gap size in \"units\" (bytes for PostgreSQL, \n     * positions for MySQL). Returns 0 if consecutive, >0 if gap, <0 if overlap/error.\n     */\n    private long calculateLsnGap(String previousLSN, String currentLSN) {\n        // TODO: Implement database-specific LSN comparison\n        // PostgreSQL example:\n        // Parse \"0/16EFE78\" into two hex parts: segment (0) and offset (16EFE78 hex)\n        // Convert to bytes: (segment << 32) + offset\n        // Return difference: currentBytes - previousBytes\n        \n        // MySQL example:\n        // Parse position as long: currentPosition - previousPosition\n        \n        // Return 0 for same LSN, >0 for gap, <0 if current < previous (should not happen)\n        return 0L;\n    }\n}\n```\n\n#### E. Java-Specific Debugging Hints\n1. **JMX Monitoring**: Expose `LogConnector.lastProcessedLSN` and `ChangeEventBuilder.getTransactionCount()` as JMX attributes for JConsole/VisualVM monitoring.\n2. **Logback MDC**: Use `MDC.put(\"txId\", transactionId)` in your logging configuration to automatically include transaction ID in all log messages during event processing.\n3. **Heap Dumps**: Enable `-XX:+HeapDumpOnOutOfMemoryError` to capture heap dumps when memory leaks occur. Analyze with Eclipse MAT to find retained `TransactionState` objects.\n4. **JFR (Java Flight Recorder)**: Use JFR to profile pipeline latency without significant overhead: `-XX:+FlightRecorder -XX:StartFlightRecording=filename=cdc.jfr`.\n\n#### F. Debugging Checklist Milestone Verification\nAfter implementing debugging tools, verify by simulating failures:\n\n**For Milestone 1 (Log Parsing)**:\n1. Stop pipeline, delete offset file, restart. Verify:\n   - Pipeline detects missing offset and logs warning\n   - Can resume from current database position (configurable)\n   - Does not reprocess old events\n2. Force a gap by manually advancing database LSN (e.g., `SELECT pg_switch_wal();` in PostgreSQL). Verify:\n   - `GapDetector` emits gap event\n   - Pipeline continues processing after gap\n   - Lag metrics show the gap size\n\n**For Milestone 2 (Event Delivery)**:\n1. Kill Kafka, send database events. Verify:\n   - `KafkaEventPublisher` enters backpressure mode\n   - Events are buffered in memory (monitor `pendingEvents` size)\n   - When Kafka restarts, buffered events are sent\n   - No events lost during outage\n2. Create a slow consumer. Verify:\n   - Lag metrics increase\n   - Alerts trigger when threshold exceeded\n   - `EventStreamer` can throttle production\n\n**For Milestone 3 (Schema Evolution)**:\n1. Add a nullable column to a table. Verify:\n   - Schema registry registers new version\n   - `SchemaChangeEvent` is emitted\n   - Consumers can deserialize old and new events\n2. Attempt to drop a column. Verify:\n   - Schema registry rejects change (if compatibility mode is FULL)\n   - Error message explains violation\n   - Existing events still deserialize correctly\n\n#### G. Advanced Debugging: Chaos Engineering\nFor resilience testing, intentionally inject failures:\n```java\npackage com.cdc.test.chaos;\n\npublic class ChaosMonkey {\n    private final Random random = new Random();\n    \n    public void maybeFail(String component, double failureProbability) {\n        if (random.nextDouble() < failureProbability) {\n            switch (component) {\n                case \"LogParser\":\n                    // Simulate corrupt log entry\n                    throw new LogParseException(\"Chaos: simulated corrupt WAL entry\");\n                case \"KafkaPublisher\":\n                    // Simulate producer timeout\n                    throw new TimeoutException(\"Chaos: Kafka broker timeout\");\n                case \"SchemaRegistry\":\n                    // Simulate registry unavailable\n                    throw new ConnectException(\"Chaos: schema registry connection refused\");\n            }\n        }\n    }\n}\n```\nUse this in test environments to verify your recovery strategies handle real-world failures.\n\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3 (forward-looking extensions)\n\n## 13. Future Extensions\n\nThe current CDC system design provides a solid foundation for reliable, real-time change data capture from PostgreSQL and MySQL databases. However, as organizations grow and their data infrastructure evolves, several natural extension points emerge. This section outlines a strategic roadmap for enhancing the system beyond its initial scope, organized by increasing levels of complexity and business value.\n\nThink of the current system as a **modular data highway with standardized entry ramps**. Each extension adds either new entry ramps (supporting different databases), improved safety features (stronger delivery guarantees), or new service lanes (managed operations). The roadmap prioritizes extensions that maximize architectural leverage while minimizing disruption to existing components.\n\n### 13.1 Possible Extension Roadmap\n\n#### Extension 1: Multi-Database Support\n\n**Mental Model: The Universal Language Translator**\nImagine the current `LogParser` interface as a translator who only speaks two languages (PostgreSQL WAL and MySQL binlog). This extension hires additional translators fluent in Oracle's Redo Log, SQL Server's Change Data Capture tables, and MongoDB's Oplog. Each translator understands the unique grammar and idioms of their database but produces the same standardized `RawLogEntry` \"phrasebook\" for downstream components.\n\n**Architectural Approach:**\nThe existing `LogParser` interface provides the perfect abstraction point for this extension. Each new database implementation would:\n1. Implement the `LogParser` interface with database-specific connection and parsing logic\n2. Translate native log concepts (Oracle's SCN, SQL Server's LSN) to the system's `logSequenceNumber`\n3. Map database-specific data types to the internal `ColumnType` representation\n4. Handle database-specific replication features (Oracle's LogMiner, SQL Server's CDC tables)\n\n**Implementation Table:**\n| Database | Native Log Format | Key Challenge | Recommended Parser Class Name |\n|----------|-------------------|---------------|-------------------------------|\n| Oracle | Redo Log + Archived Redo Log | Requires LogMiner API; complex data type mapping | `OracleRedoLogParser` |\n| SQL Server | Transaction Log + CDC Tables | Dual-source (log + CDC tables); temporal tables | `SqlServerCdcParser` |\n| MongoDB | Oplog (operation log) | Document model; schema-less nature | `MongoOplogParser` |\n| Cassandra | Commit Log | Distributed log; different consistency model | `CassandraCommitLogParser` |\n\n> **Design Insight:** The key to successful multi-database support is not just parsing different log formats, but abstracting away database-specific concepts like transaction boundaries (some databases support nested transactions) and data type systems (Oracle's NUMBER vs PostgreSQL's NUMERIC).\n\n**Integration Requirements:**\n- Extend `DatabaseConfig.type` to support new enum values: `ORACLE`, `SQL_SERVER`, `MONGODB`, `CASSANDRA`\n- Add database-specific configuration sections to `DatabaseConfig`\n- Create a parser factory that instantiates the appropriate parser based on configuration\n- Update schema discovery to query each database's metadata tables appropriately\n\n**Priority Rationale:** High business value with moderate technical complexity. Organizations often have heterogeneous database environments, and supporting multiple sources eliminates the need for separate CDC solutions.\n\n#### Extension 2: Exactly-Once Delivery Semantics\n\n**Mental Model: The Guaranteed Delivery Receipt**\nThe current at-least-once delivery is like sending a package with tracking but no guarantee against duplicates. Exactly-once semantics adds a **unique package ID registry** that prevents the same package from being delivered twice, even if the delivery truck breaks down mid-route and a replacement truck retries the delivery.\n\n**Technical Implementation:**\nAchieving exactly-once semantics requires coordinated idempotency across three layers:\n1. **Producer Idempotency:** Already partially implemented via Kafka's `enableIdempotence` configuration\n2. **Transactional Publishing:** Grouping multiple events into atomic transactions\n3. **Consumer Deduplication:** Tracking delivered events to skip duplicates during retries\n\n**Architecture Decision Record for Exactly-Once Implementation:**\n\n> **Decision: Hybrid Idempotent Producer with Consumer-State Deduplication**\n> \n> - **Context:** The system currently implements at-least-once delivery. Some downstream consumers (e.g., financial systems) cannot tolerate duplicate events, even temporarily.\n> - **Options Considered:**\n>   1. **Kafka Transactions Only:** Use Kafka's transactional producer with consumer isolation level `read_committed`\n>   2. **Deterministic Event IDs with Global Deduplication:** Generate deterministic event IDs and maintain a global deduplication store (Redis/DynamoDB)\n>   3. **Hybrid Approach:** Kafka transactions for producer idempotency + consumer-side bloom filter for fast deduplication\n> - **Decision:** Option 3 (Hybrid Approach) for balanced reliability and performance\n> - **Rationale:** Kafka transactions handle producer-side duplicates well but add coordination overhead. Adding consumer-side bloom filters with occasional persistent checks provides near-perfect deduplication without requiring synchronous writes to a global store for every event.\n> - **Consequences:** Adds memory overhead for bloom filters, requires occasional checkpointing to persistent storage, and introduces a small false-positive rate for duplicate detection (tunable).\n\n**Implementation Components:**\n| Component | Extension Required | Description |\n|-----------|-------------------|-------------|\n| `KafkaEventPublisher` | Transactional producer | Configure `transactional.id` and use `beginTransaction()`, `commitTransaction()` |\n| `DeliveryCallback` | Transaction awareness | Track events per transaction for rollback on failure |\n| New: `EventDeduplicator` | Bloom filter + persistent store | Fast in-memory duplicate check with periodic persistence |\n| `ChangeEventBuilder` | Deterministic ID enhancement | Ensure event IDs are truly deterministic across restarts |\n\n**Sequence for Exactly-Once Delivery:**\n1. Producer starts Kafka transaction with unique `transactional.id`\n2. Events published with deterministic `eventId` in headers\n3. Consumer checks `EventDeduplicator` before processing\n4. If not duplicate, process and record in deduplication store\n5. Consumer commits offsets only after successful processing AND deduplication recording\n6. Producer commits transaction after all events acknowledged\n\n**Priority Rationale:** High technical value with high implementation complexity. Critical for financial, auditing, and billing use cases where duplicate processing has real monetary consequences.\n\n#### Extension 3: Fully Managed Cloud Service\n\n**Mental Model: The CDC-as-a-Service Utility**\nTransform the current self-hosted system into a **managed data utility service**, similar to moving from a personal electricity generator to connecting to the power grid. Users provide database credentials and configuration, and the service handles scaling, monitoring, failover, and maintenance automatically.\n\n**Service Architecture Components:**\n| Service Component | Responsibility | Implementation Approach |\n|-------------------|----------------|------------------------|\n| Control Plane | User onboarding, configuration management | REST API + Web UI; stores config in managed database |\n| Data Plane | Actual CDC pipeline execution | Containerized pipeline per customer/database |\n| Scaling Controller | Auto-scaling based on throughput | Kubernetes HPA + custom metrics (events/sec, lag) |\n| Multi-Tenancy Layer | Isolation between customers | Separate Kafka clusters/namespaces; network isolation |\n| Billing Engine | Usage tracking and invoicing | Metering pipeline + integration with billing providers |\n\n**Deployment Models:**\n| Model | Isolation Level | Cost Efficiency | Implementation Complexity |\n|-------|----------------|-----------------|---------------------------|\n| Shared Everything | Single pipeline for all customers | Highest | Low (but poor isolation) |\n| Shared Kafka, Isolated Processors | Dedicated containers per customer | High | Medium |\n| Fully Isolated | Dedicated resources per customer | Lowest | High (but best isolation) |\n\n**Key Technical Challenges:**\n1. **Secure Credential Management:** Use cloud KMS (Key Management Service) or HashiCorp Vault for rotating database credentials\n2. **Noisy Neighbor Problem:** Implement fair scheduling and resource quotas\n3. **Customer Self-Service:** Build UI for schema browsing, replay, and monitoring\n4. **Compliance:** SOC2, GDPR, HIPAA compliance for sensitive data\n\n**Priority Rationale:** Transformational business model shift from software product to service. High implementation complexity but potentially highest long-term value through recurring revenue.\n\n#### Extension 4: Real-Time Transformation and Enrichment\n\n**Mental Model: The Data Assembly Line with Quality Control**\nExtend the simple pipeline into a **real-time data processing assembly line** where raw change events pass through quality check stations, get assembled with additional parts (enrichment), and are packaged for specific destinations. Think of raw logs as raw materials that get transformed into finished goods customized for each consumer.\n\n**Transformation Types:**\n| Transformation Type | Example | Implementation Pattern |\n|---------------------|---------|------------------------|\n| Field Masking/Redaction | Hide PII columns (SSN, email) | Streaming processor with configurable rules |\n| Data Enrichment | Add geolocation from IP address | External API call with caching |\n| Format Conversion | JSON to Avro, Avro to Parquet | Schema-aware serialization pipeline |\n| Filtering | Only specific tables/operations | Configurable predicate evaluation |\n| Aggregation | Count events per minute | Windowed streaming aggregation |\n\n**Architectural Integration Points:**\n- **Post-Parser, Pre-Builder:** For schema-aware transformations\n- **Post-Builder, Pre-Streamer:** For event-level transformations\n- **Post-Streamer (Sink Connectors):** For destination-specific formatting\n\n**Implementation Strategy Table:**\n| Approach | Flexibility | Performance | Complexity |\n|----------|-------------|-------------|------------|\n| Embedded Processor Chain | High | High (in-process) | Medium |\n| Sidecar Transform Service | Medium | Medium (IPC overhead) | Low |\n| Separate Stream Processing | Highest | Variable | Highest |\n\n**Example Transformation Pipeline Configuration:**\n```yaml\ntransformations:\n  - type: FIELD_MASK\n    table: users\n    columns: [ssn, password_hash]\n    method: HASH_SHA256\n  - type: ENRICH\n    table: orders\n    join:\n      source_field: customer_id\n      lookup_table: customers\n      join_fields: [tier, lifetime_value]\n  - type: FILTER\n    condition: \"operation = 'DELETE' and table = 'audit_log'\"\n    action: DROP\n```\n\n**Priority Rationale:** Medium-high business value with medium technical complexity. Enables use cases like real-time analytics, data lake population, and compliance-aware data sharing.\n\n#### Extension 5: Advanced Monitoring and Observability\n\n**Mental Model: The CDC Health Dashboard with Predictive Analytics**\nTransform basic monitoring into a **predictive health intelligence system** that doesn't just alert when something breaks, but warns when something is *about* to break. Like a car dashboard that shows current speed but also predicts engine failure based on vibration patterns.\n\n**Enhanced Monitoring Components:**\n\n| Component | Current Capability | Extended Capability |\n|-----------|-------------------|---------------------|\n| Lag Monitoring | Current consumer lag | **Predictive lag forecasting** using ML models |\n| Error Tracking | Error count and types | **Error correlation engine** linking database errors to pipeline issues |\n| Performance Metrics | Throughput and latency | **Bottleneck identification** with flame graphs |\n| Schema Compliance | Basic compatibility checks | **Drift detection** between source and consumer schemas |\n| Data Quality | Basic validation | **Statistical anomaly detection** in data patterns |\n\n**Observability Stack Enhancement:**\n\n| Layer | Tools/Technologies | Implementation Approach |\n|-------|-------------------|------------------------|\n| Metrics Collection | Prometheus + custom exporters | Extend `StreamerMetrics` with histogram buckets |\n| Distributed Tracing | Jaeger/OpenTelemetry | Add trace context propagation through pipeline |\n| Log Aggregation | ELK Stack (Elasticsearch, Logstash, Kibana) | Structured logging with correlation IDs |\n| Alerting | AlertManager + custom rules | Dynamic threshold adjustment based on patterns |\n| Visualization | Grafana dashboards | Pre-built dashboards for common CDC scenarios |\n\n**Predictive Analytics Implementation:**\n1. **Feature Collection:** Gather time-series data (lag, throughput, error rates, database load)\n2. **Model Training:** Offline training of anomaly detection models\n3. **Real-time Scoring:** Apply models to streaming metrics\n4. **Alert Fusion:** Combine multiple weak signals into strong alerts\n\n**Priority Rationale:** Medium business value with medium-high technical complexity. Critical for production operations at scale, reduces mean time to resolution (MTTR), and enables proactive maintenance.\n\n#### Extension 6: Change Event Querying and Playback\n\n**Mental Model: The Database Time Machine**\nAdd the ability to **query the stream of changes as if it were a time-series database** and **replay events from any point in time**. This transforms the CDC system from a simple pipe into an **auditable, queryable historical record** of all database changes.\n\n**Core Capabilities:**\n\n| Capability | Description | Use Cases |\n|------------|-------------|-----------|\n| Point-in-Time Query | \"Show me the state of user 123 at 2023-06-15 14:30:00\" | Audit investigations, dispute resolution |\n| Change History Browse | \"Show all changes to the orders table last week\" | Compliance reporting, debugging |\n| Selective Replay | \"Replay only customer table changes to test environment\" | Test data provisioning, environment synchronization |\n| Event Search | \"Find all events where email changed from domain X to Y\" | Security incident investigation |\n\n**Architectural Components:**\n\n| Component | Responsibility | Implementation |\n|-----------|----------------|----------------|\n| Change Event Store | Persistent storage of all change events | Apache Druid/ClickHouse for time-series queries |\n| Indexing Engine | Index events by table, primary key, timestamp | Elasticsearch for full-text search on JSON payloads |\n| Query API | REST/GraphQL interface for querying changes | Separate service with query planning/optimization |\n| Replay Engine | Controlled event replay at various speeds | Kafka consumer groups with offset manipulation |\n\n**Query Language Example:**\n```sql\n-- Find all changes to user email addresses in June 2023\nSELECT * FROM change_events\nWHERE table_name = 'users'\n  AND operation_type IN ('INSERT', 'UPDATE')\n  AND commit_timestamp BETWEEN '2023-06-01' AND '2023-06-30'\n  AND JSON_EXTRACT(after_image, '$.email') != JSON_EXTRACT(before_image, '$.email')\nORDER BY commit_timestamp DESC\nLIMIT 100;\n```\n\n**Storage Considerations:**\n| Storage Strategy | Retention | Query Performance | Storage Cost |\n|------------------|-----------|-------------------|--------------|\n| Kafka + Compaction | Limited by topic retention | Good for recent data | Medium |\n| Data Lake (S3 + Parquet) | Unlimited (cost-based) | Good for analytical queries | Low |\n| Time-Series Database | Configurable retention | Excellent for time-range queries | Medium-High |\n| Hybrid (Hot/Warm/Cold) | Intelligent tiering | Optimized for access patterns | Optimized |\n\n**Priority Rationale:** High business value with high implementation complexity. Enables powerful audit, compliance, and debugging capabilities that go beyond simple change streaming.\n\n#### Extension Roadmap Prioritization Matrix\n\n**Evaluation Criteria:**\n- **Business Value (BV):** Revenue impact, cost savings, competitive advantage\n- **Technical Complexity (TC):** Implementation effort, architectural changes required\n- **Strategic Alignment (SA):** Alignment with long-term platform vision\n- **Customer Demand (CD):** Direct requests from users/stakeholders\n\n**Prioritization Table:**\n\n| Extension | BV (1-10) | TC (1-10) | SA (1-10) | CD (1-10) | Weighted Score | Recommended Phase |\n|-----------|-----------|-----------|-----------|-----------|----------------|-------------------|\n| Multi-Database Support | 9 | 6 | 8 | 7 | 7.5 | Phase 1 (Next 3 months) |\n| Exactly-Once Delivery | 8 | 8 | 7 | 6 | 7.3 | Phase 1 |\n| Real-Time Transformation | 7 | 5 | 6 | 8 | 6.5 | Phase 2 (3-6 months) |\n| Advanced Monitoring | 6 | 7 | 7 | 5 | 6.3 | Phase 2 |\n| Change Event Querying | 8 | 9 | 6 | 4 | 6.8 | Phase 3 (6-12 months) |\n| Managed Cloud Service | 10 | 10 | 9 | 3 | 8.0 | Phase 3 (strategic) |\n\n**Scoring Formula:** `(BV×0.3) + (TC×0.2) + (SA×0.3) + (CD×0.2)` where lower TC is better (inverted: `10 - TC`)\n\n**Phase Implementation Strategy:**\n\n**Phase 1 (Foundation Extensions):**\n- Focus on extensions that build directly on existing architecture\n- Multi-database support extends the `LogParser` abstraction\n- Exactly-once delivery enhances existing `EventStreamer` components\n- **Key Deliverable:** Enterprise-ready CDC with broader database coverage and stronger guarantees\n\n**Phase 2 (Operational Excellence):**\n- Enhance operational capabilities for production deployments\n- Real-time transformation enables new use cases\n- Advanced monitoring reduces operational burden\n- **Key Deliverable:** Self-service transformation and predictive operations\n\n**Phase 3 (Platform Transformation):**\n- Transform from tool to platform\n- Query capabilities create new data product opportunities\n- Managed service enables new business model\n- **Key Deliverable:** Full CDC-as-a-Platform with time-travel query capabilities\n\n> **Strategic Insight:** The most valuable long-term extension is the managed cloud service, but it requires all other extensions to be mature. Start with Phase 1 extensions to build a robust multi-database foundation, then enhance operations in Phase 2, finally transforming to a platform in Phase 3. This creates value at each stage while building toward the strategic vision.\n\n### Implementation Guidance\n\n**A. Technology Recommendations for Extensions:**\n\n| Extension | Core Technology | Complementary Technologies |\n|-----------|----------------|----------------------------|\n| Multi-Database | Database-specific JDBC drivers + native APIs | Connection pooling (HikariCP), connection health checks |\n| Exactly-Once | Kafka Transactions API | Redis for deduplication store, Bloom filter libraries (Guava) |\n| Managed Service | Kubernetes Operator SDK | HashiCorp Vault, Prometheus Operator, Service Mesh (Istio/Linkerd) |\n| Real-time Transformation | Apache Flink/KSQL | External cache (Redis), rule engine (Drools) |\n| Advanced Monitoring | OpenTelemetry SDK | ML frameworks (TensorFlow Lite), anomaly detection libraries |\n| Query & Playback | Apache Druid/ClickHouse | Query engine (Presto/Trino), S3/MinIO for storage |\n\n**B. Recommended File Structure for Multi-Database Extension:**\n\n```\ncdc-system/\n├── src/main/java/com/cdc/\n│   ├── parser/\n│   │   ├── LogParser.java                    # Interface\n│   │   ├── PostgresWalParser.java            # Existing implementation\n│   │   ├── MySqlBinlogParser.java            # Existing implementation\n│   │   ├── oracle/\n│   │   │   ├── OracleRedoLogParser.java      # New: Oracle implementation\n│   │   │   ├── LogMinerHelper.java           # New: Oracle LogMiner utilities\n│   │   │   └── OracleDataTypeMapper.java     # New: Oracle type mapping\n│   │   ├── sqlserver/\n│   │   │   ├── SqlServerCdcParser.java       # New: SQL Server implementation\n│   │   │   └── ChangeTableReader.java        # New: CDC table reader\n│   │   └── ParserFactory.java                # Updated to support new parsers\n│   ├── config/\n│   │   ├── DatabaseConfig.java               # Extended with new database types\n│   │   └── OracleConfig.java                 # New: Oracle-specific config\n│   └── util/\n│       └── DatabaseDetector.java             # New: Auto-detects database type\n```\n\n**C. Infrastructure Starter Code for Database Detector:**\n\n```java\n// DatabaseDetector.java - Auto-detects database type from JDBC URL\npackage com.cdc.util;\n\nimport java.sql.Connection;\nimport java.sql.DatabaseMetaData;\nimport java.sql.SQLException;\nimport javax.sql.DataSource;\n\npublic class DatabaseDetector {\n    \n    public static DatabaseType detect(DataSource dataSource) throws SQLException {\n        try (Connection conn = dataSource.getConnection()) {\n            DatabaseMetaData meta = conn.getMetaData();\n            String databaseName = meta.getDatabaseProductName().toLowerCase();\n            String url = meta.getURL().toLowerCase();\n            \n            if (databaseName.contains(\"postgresql\") || url.contains(\"postgres\")) {\n                return DatabaseType.POSTGRESQL;\n            } else if (databaseName.contains(\"mysql\") || url.contains(\"mysql\")) {\n                return DatabaseType.MYSQL;\n            } else if (databaseName.contains(\"oracle\") || url.contains(\"oracle\")) {\n                return DatabaseType.ORACLE;\n            } else if (databaseName.contains(\"sql server\") || url.contains(\"sqlserver\")) {\n                return DatabaseType.SQL_SERVER;\n            } else {\n                throw new UnsupportedDatabaseException(\n                    \"Unsupported database: \" + databaseName);\n            }\n        }\n    }\n    \n    public enum DatabaseType {\n        POSTGRESQL, MYSQL, ORACLE, SQL_SERVER\n    }\n}\n```\n\n**D. Core Logic Skeleton for Oracle Parser:**\n\n```java\n// OracleRedoLogParser.java - Skeleton for Oracle implementation\npackage com.cdc.parser.oracle;\n\nimport com.cdc.parser.LogParser;\nimport com.cdc.config.OracleConfig;\nimport com.cdc.model.RawLogEntry;\nimport java.util.List;\n\npublic class OracleRedoLogParser implements LogParser {\n    private OracleConfig config;\n    private LogMinerHelper logMiner;\n    private OracleDataTypeMapper typeMapper;\n    private String currentScn; // Oracle's equivalent of LSN\n    \n    public OracleRedoLogParser(OracleConfig config) {\n        this.config = config;\n        this.typeMapper = new OracleDataTypeMapper();\n        // TODO 1: Initialize LogMiner session with Oracle JDBC connection\n        // TODO 2: Configure LogMiner to read from archived redo logs\n        // TODO 3: Set up dictionary (source: online catalog or dictionary file)\n    }\n    \n    @Override\n    public List<RawLogEntry> getNextBatch(int batchSize) {\n        List<RawLogEntry> entries = new ArrayList<>();\n        \n        // TODO 4: Use LogMiner API to fetch next batch of redo entries\n        // TODO 5: Filter for DML operations (INSERT, UPDATE, DELETE)\n        // TODO 6: Parse redo entry into table name, operation type, row data\n        // TODO 7: Map Oracle data types (NUMBER, DATE, CLOB) to ColumnType\n        // TODO 8: Convert Oracle SCN to logSequenceNumber string\n        // TODO 9: Handle Oracle-specific features: nested tables, LOBs, etc.\n        // TODO 10: Update currentScn to track position\n        \n        return entries;\n    }\n    \n    @Override\n    public String getCurrentPosition() {\n        return currentScn;\n    }\n    \n    @Override\n    public void seekToPosition(String position) {\n        // TODO 11: Implement SCN-based seeking for Oracle\n        // Oracle SCN is numeric, can convert from string\n        this.currentScn = position;\n        // TODO 12: Restart LogMiner session from specified SCN\n    }\n    \n    // Helper method for Oracle-specific type conversion\n    private Object convertOracleValue(Object oracleValue, String oracleType) {\n        // TODO 13: Implement Oracle-to-Java type mapping\n        // Handle NUMBER (precision/scale), DATE, TIMESTAMP, CLOB, BLOB\n        return typeMapper.convert(oracleValue, oracleType);\n    }\n}\n```\n\n**E. Language-Specific Hints for Java Extensions:**\n\n1. **Oracle JDBC:** Use `ojdbc11.jar` for Java 11+, enable JDBC thin driver for better performance\n2. **SQL Server:** Use Microsoft's `mssql-jdbc` driver, enable snapshot isolation for CDC tables\n3. **Connection Pooling:** Configure different pool settings per database type (Oracle prefers smaller pools)\n4. **Error Handling:** Database-specific error codes mapping (Oracle ORA-*, SQL Server MSSQL*)\n5. **Performance:** Oracle LogMiner benefits from `DBMS_LOGMNR.NO_ROWID_IN_STMT` option for better performance\n\n**F. Milestone Checkpoint for Multi-Database Extension:**\n\n**Command to Test:** `mvn test -Dtest=MultiDatabaseIntegrationTest`\n\n**Expected Output:**\n```\n[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] MultiDatabaseIntegrationTest\n[INFO] ✓ testOracleParserBasicOperations\n[INFO] ✓ testSqlServerCdcTableReading\n[INFO] ✓ testDatabaseTypeAutoDetection\n[INFO] ✓ testCrossDatabaseEventConsistency\n[INFO] ✓ testParserFactoryCreatesCorrectType\n```\n\n**Manual Verification Steps:**\n1. Start test Oracle database with sample data: `docker run -d --name test-oracle -p 1521:1521 container-registry.oracle.com/database/express:21.3.0-xe`\n2. Run pipeline with Oracle configuration\n3. Execute DML on Oracle: `INSERT INTO test_table VALUES (1, 'test'); COMMIT;`\n4. Verify events appear in Kafka topic with correct Oracle SCN as `logSequenceNumber`\n5. Stop and restart pipeline, verify it resumes from last SCN\n\n**Signs of Issues:**\n- ❌ \"ORA-01291: missing logfile\" → LogMiner not configured with correct log files\n- ❌ \"Invalid column type: ORA-00932\" → Data type mapping incorrect\n- ❌ Events out of order → SCN tracking not monotonic\n- ❌ High memory usage → LogMiner session not properly sized/closed\n\n**G. Debugging Tips for Multi-Database Implementation:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Parser fails with \"unsupported database\" | Database type detection failure | Check JDBC URL format and DatabaseMetaData output | Update `DatabaseDetector` with correct pattern matching |\n| Oracle events missing LOB data | LogMiner not configured for LOB tracking | Check LogMiner options: `DBMS_LOGMNR.NO_LOB_TRACKING` | Add `+LOBS` to LogMiner options |\n| SQL Server CDC events delayed | Change table cleanup job too aggressive | Query `sys.dm_cdc_errors`, check retention period | Adjust `cdc.retention` or polling interval |\n| Mixed database events out of order | Different LSN/SCN formats causing sort issues | Compare LSN strings across databases | Normalize position format (pad numeric SCNs) |\n| Memory leak with multiple parsers | Connection pools not closed properly | Monitor heap with VisualVM, check connection counts | Implement proper `close()` method in parsers |\n| Type conversion errors | Database-specific types not mapped | Log raw type string before conversion | Extend `OracleDataTypeMapper` with missing types |\n\n\n## 14. Glossary\n\n> **Milestone(s):** Milestone 1, Milestone 2, Milestone 3 (foundational vocabulary for all milestones)\n\nThis glossary serves as a **centralized reference** for all technical terms, acronyms, and domain-specific vocabulary used throughout this design document. Think of it as a **dictionary for your CDC journey** — whenever you encounter an unfamiliar term, return here for a clear, concise definition with cross-references to related concepts.\n\n### 14.1 Terms and Definitions\n\nThe following table provides alphabetized definitions of key terms, structured for quick lookup:\n\n| Term | Definition | Related Concepts |\n|------|------------|------------------|\n| **After Image** | The complete state of a database row *after* a change operation (INSERT, UPDATE) has been applied. In a CDC context, this represents the new values that downstream consumers should see. | `ChangeEvent.afterImage`, `OPERATION_UPDATE`, `OPERATION_INSERT`, Before Image |\n| **At-Least-Once Delivery** | A message delivery guarantee where **each event is delivered at least once** to consumers, but duplicates may occur during failure recovery. This is the standard guarantee for our CDC pipeline, balancing reliability with implementation complexity. | Delivery Semantics, Exactly-Once Delivery, Idempotency, `KafkaConfig.enableIdempotence` |\n| **Backpressure** | A **flow control mechanism** that slows down data production when downstream components (consumers) cannot keep up. In our pipeline, when Kafka consumer lag exceeds thresholds, the Event Streamer signals the Log Connector to pause reading. | `AppConfig.backpressureThresholdMs`, Consumer Lag, Flow Control, `KafkaEventPublisher.backpressureSignal` |\n| **Backward Compatibility** | A schema compatibility rule where **newer schema versions can read data written with older schema versions**. This is typically the most important compatibility mode for CDC, ensuring consumers with updated schemas can process historical events. | `COMPATIBILITY_BACKWARD`, Schema Evolution, `SchemaRegistry.checkCompatibility()`, Forward Compatibility |\n| **Before Image** | The complete state of a database row *before* a change operation (UPDATE, DELETE) has been applied. This provides consumers with the previous values, enabling delta calculations or undo operations. | `ChangeEvent.beforeImage`, `OPERATION_UPDATE`, `OPERATION_DELETE`, After Image |\n| **Binlog (Binary Log)** | MySQL's **transaction log mechanism** that records all data modifications (INSERT, UPDATE, DELETE) and schema changes (DDL) in a binary format. It's used for replication and forms the source for MySQL-based CDC. | Write-Ahead Log (WAL), Logical Decoding, `DatabaseType.MYSQL`, Redo Log |\n| **CDC (Change Data Capture)** | The **process of identifying and capturing changes** made to a database, then delivering those changes to downstream systems in real-time. Our system implements log-based CDC, which reads transaction logs rather than polling tables. | Transaction Log, Event Streaming, Data Synchronization |\n| **CDC Tables** | SQL Server's **native change data capture feature** that uses special system tables to track changes. Unlike log-based CDC, this approach adds triggers or uses SQL Server's change tracking tables. | `DatabaseType.SQL_SERVER`, Trigger-Based CDC, System Tables |\n| **Change Event** | The **core data structure** representing a single logical change to a database row. It contains the operation type, before/after images, schema version, and transaction context, and is the primary payload delivered to consumers. | `ChangeEvent` type, `RawLogEntry`, Schema Version, Transaction Boundary |\n| **Chaos Engineering** | The **practice of intentionally injecting failures** into a system to test its resilience and recovery mechanisms. In our testing strategy, the `ChaosMonkey` component simulates various failure scenarios. | `ChaosMonkey`, Resilience Testing, Failure Injection, Circuit Breaker |\n| **Circuit Breaker** | A **design pattern** that prevents cascading failures by failing fast when downstream services are unavailable. After consecutive failures, the circuit \"opens\" and stops making calls, periodically testing if the service has recovered. | `CircuitBreakerManager`, Resilience, Exponential Backoff, `RecoveryState.HALTED` |\n| **Column Type** | A **metadata structure** defining the characteristics of a database column, including its SQL data type, corresponding Java class, nullability, and default value. These definitions are stored in `SchemaVersion.columnDefinitions`. | `ColumnType` type, Schema Definition, `SchemaVersion`, Type Coercion |\n| **Compatibility Mode** | The **set of rules** governing whether a schema change is allowed based on its impact on existing consumers. Our system supports four modes: `BACKWARD`, `FORWARD`, `FULL`, and `NONE`. | `COMPATIBILITY_BACKWARD`, `COMPATIBILITY_FORWARD`, `COMPATIBILITY_FULL`, `COMPATIBILITY_NONE` |\n| **Consumer Lag** | The **difference between the latest produced offset** in a Kafka partition and the **last consumed offset** by a consumer. High lag indicates consumers are falling behind, triggering backpressure mechanisms. | `StreamerMetrics.lagThresholdMs`, Backpressure, Offset Management, Kafka Consumer |\n| **Correlation ID** | A **unique identifier** propagated across system components to trace a request or operation through the entire pipeline. Used for debugging and observability to connect related logs. | Tracing, Observability, MDC, Debugging |\n| **DDL (Data Definition Language)** | **SQL statements that define database structure**, such as CREATE, ALTER, DROP, and TRUNCATE. DDL events require special handling in CDC as they change schema definitions and may invalidate in-flight transactions. | Schema Evolution, `SchemaChangeEvent`, ALTER TABLE, DDL Parsing |\n| **Deduplication Cache** | A **temporary in-memory store** used by the `ChangeEventBuilder` to identify and eliminate duplicate change events that might arise from log replay or retry mechanisms. | `ChangeEventBuilder.deduplicationCache`, Deterministic Event ID, Idempotency |\n| **Delivery Semantics** | The **guarantees provided about message delivery** between producers and consumers. The three main levels are at-most-once, at-least-once, and exactly-once, each with different trade-offs in complexity and reliability. | At-Least-Once Delivery, Exactly-Once Delivery, Message Reliability |\n| **Deterministic Event ID** | An **event identifier that can be regenerated** from the same inputs (transaction ID, table, primary key, sequence number). This enables idempotent processing and duplicate detection without external state. | `ChangeEventBuilder.generateEventId()`, Deduplication, Idempotency |\n| **Event Snapshot Buffer** | A **circular buffer** that stores recent `ChangeEvent` instances for debugging and recovery purposes. When a gap or corruption is detected, recent events can be replayed or analyzed. | `EventSnapshotBuffer`, Circular Buffer, Gap Detection, Recovery |\n| **Exactly-Once Delivery** | A **message delivery guarantee** where each event is delivered **exactly once** to consumers, with no duplicates and no missed messages. This is complex to implement and typically requires transactional coordination across systems. | At-Least-Once Delivery, Idempotent Producer, Transactional Messaging |\n| **Exponential Backoff** | A **retry strategy** where wait time between retry attempts doubles after each failure. This prevents overwhelming a failing service while gradually attempting recovery. Used in our `RecoveryCoordinator`. | `RecoveryStrategy.RETRY_WITH_BACKOFF`, Circuit Breaker, Resilience |\n| **Forward Compatibility** | A schema compatibility rule where **older schema versions can read data written with newer schema versions**. This is less common in CDC but useful when consumers upgrade slower than producers. | `COMPATIBILITY_FORWARD`, Schema Evolution, Backward Compatibility |\n| **Full Compatibility** | A schema compatibility rule that **combines backward and forward compatibility** — any version can read data written by any other version. This provides maximum flexibility but imposes strict constraints on schema changes. | `COMPATIBILITY_FULL`, Schema Evolution, Compatibility Mode |\n| **Gap Detection** | The **process of identifying missing log sequence numbers** in the CDC pipeline. When the connector detects non-sequential LSNs, it emits a `GapDetectionEvent` for monitoring and potential manual intervention. | `GapDetectionEvent`, `LogConnector.calculateLsnGap()`, LSN Recovery, Data Loss |\n| **Gap Detection Event** | A **special notification event** emitted when the CDC pipeline detects potentially missing data between consecutive log sequence numbers. Contains metadata about the gap size and suspected cause. | `GapDetectionEvent` type, Gap Detection, Data Integrity |\n| **Graceful Shutdown** | A **shutdown procedure** that completes in-flight work before terminating. Our pipeline's `stop()` methods persist current state (LSN offsets, transaction boundaries) to enable clean restarts. | `CdcPipeline.stop()`, State Persistence, `LogConnector.stop()` |\n| **Health Status** | An **enum representing the operational state** of a component or the entire pipeline. Values include `HEALTHY`, `DEGRADED`, and `STOPPED`, used for monitoring and automatic recovery. | `HealthStatus` enum, Monitoring, `CdcPipeline.getHealthStatus()` |\n| **Idempotency** | The **property where repeating an operation** yields the same result as performing it once. In CDC, idempotent consumers can safely reprocess duplicate events without causing data corruption. | At-Least-Once Delivery, Deterministic Event ID, Deduplication |\n| **Idempotent Producer** | A **Kafka producer configuration** that prevents duplicate message submission during retries by using sequence numbers and broker deduplication. Enabled via `KafkaConfig.enableIdempotence`. | `KafkaConfig.enableIdempotence`, Exactly-Once Delivery, Producer Idempotence |\n| **LSN (Log Sequence Number)** | A **pointer to a specific position** in a transaction log (PostgreSQL WAL). It's a critical concept for resuming log reading after restarts and ensuring no changes are missed. | `RawLogEntry.logSequenceNumber`, Offset Tracking, `FileOffsetStore`, WAL |\n| **LSN Recovery** | The **process of resuming log reading** from the correct log sequence number after a pipeline failure or restart. Our `OffsetStore` persists the last processed LSN to enable this recovery. | `FileOffsetStore`, `LogConnector.lastProcessedLSN`, Crash Recovery |\n| **Logical Decoding** | The **process of interpreting transaction log entries** (which are physical page changes) as logical row-level changes (INSERT/UPDATE/DELETE of specific rows). This is what transforms `RawLogEntry` into `ChangeEvent`. | `LogParser`, Change Event Builder, WAL Parsing |\n| **LogMiner** | **Oracle's utility for reading redo log files** and extracting logical change information. Used by our `OracleRedoLogParser` to implement CDC for Oracle databases. | `OracleRedoLogParser`, `LogMinerHelper`, Redo Log, SCN |\n| **Log Sequence Number (LSN)** | (Alternative phrasing) A **database-specific identifier** that uniquely identifies a position in the transaction log. PostgreSQL uses LSNs, MySQL uses binlog positions, Oracle uses SCNs. | Log Sequence Number, Binlog Position, SCN |\n| **MDC (Mapped Diagnostic Context)** | **Thread-local storage for contextual logging information** in Java. Used to attach correlation IDs and other metadata to log messages for better traceability across components. | Correlation ID, Observability, Structured Logging |\n| **Oplog** | **MongoDB's operation log** for replication, recording all write operations. While not currently implemented, it represents another transaction log type that could be supported in future extensions. | Transaction Log, NoSQL CDC, Replication Log |\n| **Partition Key** | The **value used to determine which Kafka partition** a message is sent to. In our CDC system, we use a composite of table name and primary key hash to ensure ordering per-row. | `KafkaConfig.partitionsPerTable`, Event Ordering, Kafka Partitioning |\n| **Pipeline Pattern** | An **architectural pattern** where data flows through a series of processing stages (components), each with a single responsibility. Our CDC system uses this pattern: Log Connector → Event Builder → Event Streamer. | Component Architecture, Data Flow, Processing Stages |\n| **Raw Log Entry** | The **parsed but unprocessed representation** of a transaction log entry. Contains low-level details like LSN, operation type, and raw row data, but hasn't been assembled into complete change events with transaction context. | `RawLogEntry` type, `LogParser`, `LogConnector.getNextBatch()` |\n| **Recovery State** | An **enum representing the current recovery status** of the pipeline. Values include `NORMAL`, `DEGRADED`, `RECOVERING`, and `HALTED`, guiding the `RecoveryCoordinator`'s actions. | `RecoveryState` enum, `RecoveryCoordinator`, Failure Recovery |\n| **Recovery Strategy** | The **specific approach taken** to recover from a failure. Our `RecoveryCoordinator` selects from strategies like `RETRY_WITH_BACKOFF`, `RESET_AND_RECOVER`, `DEGRADE_FUNCTIONALITY`, or `HALT_FOR_INTERVENTION`. | `RecoveryStrategy` enum, `RecoveryCoordinator.determineRecoveryStrategy()` |\n| **Redo Log** | **Oracle's transaction log** for crash recovery, recording all changes made to the database. Similar to PostgreSQL's WAL but with Oracle-specific format and access methods. | `DatabaseType.ORACLE`, `OracleRedoLogParser`, LogMiner, SCN |\n| **Replication Slot** | (PostgreSQL) A **feature that ensures WAL segments are retained** until consumed by a logical replication consumer. Our CDC system uses replication slots to prevent premature cleanup of unprocessed logs. | `DatabaseConfig.slotName`, WAL Retention, Logical Replication |\n| **Schema Cache Incoherency** | A **state where different components** have different schema versions cached, leading to serialization/deserialization mismatches. Mitigated by TTL-based cache invalidation and schema version IDs in events. | `SchemaRegistry`, Cache Coherency, `ChangeEvent.schemaVersionId` |\n| **Schema Change Event** | A **special notification event** emitted when a new schema version is registered. Notifies consumers that they should update their deserialization logic and potentially migrate data. | `SchemaChangeEvent` type, Schema Evolution, DDL |\n| **Schema Compatibility** | **Rules governing whether a schema change** breaks existing consumers. Determines if old consumers can read new data (forward compatibility) and new consumers can read old data (backward compatibility). | Compatibility Mode, Schema Evolution, `SchemaRegistry.checkCompatibility()` |\n| **Schema Evolution** | The **process of managing changes** to a data schema over time while maintaining compatibility with existing consumers. A core challenge in CDC systems that must handle live database schema changes. | Schema Registry, Compatibility Mode, DDL |\n| **Schema Registry** | A **centralized service** that stores and manages schema versions for CDC. Provides versioning, compatibility checking, and schema retrieval for serialization/deserialization. | `FileBasedSchemaRegistry`, Schema Evolution, Schema Versioning |\n| **Schema Version** | A **specific iteration of a table's schema** with a unique identifier. Contains column definitions and compatibility mode, and is referenced by `ChangeEvent` instances for proper deserialization. | `SchemaVersion` type, Versioning, `SchemaRegistry.registerSchema()` |\n| **SCN (System Change Number)** | **Oracle's equivalent of LSN** — a numeric identifier that increments with every database change. Used as the position marker for Oracle CDC via the `OracleRedoLogParser`. | `OracleRedoLogParser.currentScn`, Redo Log, LogMiner |\n| **Transaction Boundary** | The **grouping of all changes** that commit together atomically. In CDC, we must respect these boundaries, emitting change events only after seeing commit records in the transaction log. | `TransactionState`, `ChangeEventBuilder`, Atomicity |\n| **Transaction Timeout** | **Automatic abortion of transactions** that exceed a maximum allowed duration. Our `ChangeEventBuilder` flushes or discards transactions that haven't been committed within `transactionTimeoutMs`. | `ChangeEventBuilder.transactionTimeoutMs`, Stuck Transactions, `TransactionState.lastActivityTimestamp` |\n| **Type Coercion** | The **automatic or implicit conversion** of values from one data type to another during schema evolution. For example, converting `VARCHAR` to `TEXT` is generally safe, while `INT` to `VARCHAR` requires careful handling. | Schema Evolution, `ColumnType`, Data Type Mapping |\n| **Write-Ahead Log (WAL)** | **PostgreSQL's transaction log** for crash recovery. Records all intended changes before they're applied to data files, enabling recovery and forming the basis for PostgreSQL CDC via logical decoding. | `DatabaseType.POSTGRESQL`, Logical Decoding, LSN, Replication Slot |\n\n---\n"}