{"html":"<h1 id=\"config-file-parser-design-document\">Config File Parser: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A multi-format configuration file parser that supports INI, TOML, and YAML formats through unified parsing architecture. The key challenge is designing flexible tokenization and parsing components that can handle fundamentally different syntactic approaches while maintaining clean separation between lexical analysis and structural interpretation.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (foundational understanding for INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser)</p>\n</blockquote>\n<p>Configuration file parsing represents one of the most deceptively complex challenges in software engineering. What appears to be a straightforward task—reading structured text and converting it into usable data structures—quickly reveals layers of intricate problems that demand sophisticated solutions. This complexity stems not from any single difficult algorithm, but from the intersection of multiple challenging domains: lexical analysis, syntax parsing, type inference, error recovery, and the fundamental differences between how humans write configuration files and how computers process structured data.</p>\n<p>The core challenge lies in bridging the gap between human-friendly configuration syntax and machine-readable data structures while maintaining the flexibility and expressiveness that makes configuration files valuable in the first place. Configuration formats evolved to solve different problems for different communities, resulting in fundamentally incompatible approaches to representing the same underlying data. Building a unified parser that handles multiple formats requires understanding not just their syntactic differences, but the philosophical approaches that shaped their design.</p>\n<h3 id=\"why-configuration-parsing-is-hard\">Why Configuration Parsing is Hard</h3>\n<p>Think of configuration file parsing like being a universal translator at the United Nations, but instead of translating between human languages, you&#39;re translating between different ways of thinking about structured data. Each configuration format represents a different cultural approach to organizing information—INI files reflect the Windows registry mindset of hierarchical sections, TOML embodies the programmer&#39;s desire for explicit types and unambiguous syntax, while YAML embraces the document author&#39;s preference for visual structure and minimal punctuation.</p>\n<p>The fundamental challenge begins with <strong>lexical ambiguity</strong>—the same character sequence can mean entirely different things depending on context and format. Consider the innocent-looking string <code>key = &quot;value&quot;</code>. In INI format, this is a straightforward key-value assignment where the quotes are likely part of the value. In TOML, those quotes create a basic string with potential escape sequence processing. In YAML, this same line might be invalid because YAML prefers <code>key: value</code> syntax, or it could be interpreted as a mapping with a complex key containing an equals sign. A robust parser must not only recognize these differences but switch between entirely different parsing strategies based on format detection.</p>\n<p><strong>Context sensitivity</strong> presents another layer of complexity that novice parser implementers consistently underestimate. The meaning of characters and tokens changes dramatically based on their surrounding context. In TOML, the sequence <code>[[database]]</code> creates an array of tables entry, but <code>[database]</code> creates a simple table header, while <code>[ &quot;database&quot; ]</code> might be part of an inline array containing a single string. The parser must maintain sophisticated state tracking to distinguish between these contexts, often requiring lookahead parsing or backtracking when initial assumptions prove incorrect.</p>\n<p><strong>Whitespace semantics</strong> vary dramatically between formats in ways that create subtle but critical parsing challenges. INI files generally treat whitespace as optional padding around meaningful content, allowing liberal spacing that gets trimmed during processing. TOML follows similar principles but with stricter rules around string literals and multiline constructs. YAML, however, makes whitespace semantically meaningful—indentation levels determine nesting structure, and the difference between two spaces and four spaces can completely change the resulting data structure. A parser architecture must accommodate both whitespace-agnostic and whitespace-sensitive parsing modes, often within the same parsing session when handling mixed format scenarios.</p>\n<p>The <strong>type inference problem</strong> reveals another dimension of complexity that extends beyond simple syntax parsing. Configuration files exist primarily for human authorship, which means they optimize for writing convenience rather than parsing simplicity. Humans write <code>timeout = 30</code> expecting the parser to understand this represents a numeric value, not a string containing digits. They write <code>enabled = yes</code> expecting boolean interpretation, and <code>created = 2023-10-15T14:30:00Z</code> expecting datetime parsing. Each format handles type inference differently—TOML provides explicit type syntax but still requires inference for basic literals, YAML attempts aggressive type inference that can surprise users, while INI files traditionally treat everything as strings, leaving type interpretation to the application layer.</p>\n<p><strong>Error recovery and reporting</strong> becomes exponentially more complex in configuration parsing because configuration files are primarily authored by humans, not generated by tools. Human-authored content contains creative deviations from formal grammar, partial completions during editing, and errors that reflect misunderstanding of format rules rather than simple typos. A parser must not only detect errors but provide meaningful feedback that helps users understand both what went wrong and how to fix it. This requires maintaining enough parsing context to generate suggestions, tracking line and column positions through complex tokenization processes, and distinguishing between recoverable syntax errors and fundamental format violations.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Configuration parsing difficulty stems not from complex algorithms but from the impedance mismatch between human-friendly syntax and machine-processable structure. Each format makes different trade-offs in this space, requiring parsers to switch between fundamentally different processing paradigms.</p>\n</blockquote>\n<p>The <strong>nested structure mapping problem</strong> presents unique challenges that distinguish configuration parsing from simpler key-value processing. Modern configuration formats support deeply nested data structures that must be constructed incrementally as parsing proceeds. TOML&#39;s dotted key syntax like <code>database.connection.pool.size = 10</code> requires creating intermediate dictionary structures on demand while detecting conflicts with previously defined keys. YAML&#39;s indentation-based nesting requires stack-based tracking of scope levels with complex rules for when blocks end and new structures begin. INI files, despite their apparent simplicity, introduce nesting through section headers that create hierarchical organization.</p>\n<p><strong>Format detection and switching</strong> adds another layer of complexity that becomes critical in real-world applications. Users rarely want to specify the configuration format explicitly—they expect parsers to automatically detect whether a file is INI, TOML, or YAML based on content analysis. This detection must happen early enough to select the appropriate parsing strategy but late enough to have sufficient content for reliable identification. False positives in format detection can lead to confusing error messages when content gets parsed with the wrong grammar rules.</p>\n<p><strong>Unicode and encoding handling</strong> permeates every aspect of configuration parsing but often gets overlooked until problems emerge. Configuration files exist in international contexts with multi-byte character encodings, right-to-left text, combining characters, and normalization requirements. String literal processing must handle escape sequences that can introduce arbitrary Unicode code points, while maintaining proper character counting for error position reporting. Different formats have varying levels of Unicode sophistication—YAML requires full Unicode support for identifiers, while INI files traditionally assume ASCII-compatible encodings.</p>\n<h3 id=\"format-comparison-analysis\">Format Comparison Analysis</h3>\n<p>Understanding the specific characteristics and parsing requirements of each configuration format provides the foundation for designing a unified parsing architecture. Each format evolved to solve different problems, resulting in distinct approaches to syntax, semantics, and complexity management that directly influence parser design decisions.</p>\n<table>\n<thead>\n<tr>\n<th>Format Aspect</th>\n<th>INI</th>\n<th>TOML</th>\n<th>YAML Subset</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Primary Design Goal</strong></td>\n<td>Simple Windows registry-style configuration</td>\n<td>Unambiguous configuration with explicit types</td>\n<td>Human-readable documents with minimal syntax</td>\n</tr>\n<tr>\n<td><strong>Syntax Philosophy</strong></td>\n<td>Section-based key-value pairs</td>\n<td>Programming language inspired explicit syntax</td>\n<td>Indentation-based visual hierarchy</td>\n</tr>\n<tr>\n<td><strong>Parsing Complexity</strong></td>\n<td>Low - line-based processing</td>\n<td>High - recursive descent with lookahead</td>\n<td>Medium - indentation-sensitive with context</td>\n</tr>\n<tr>\n<td><strong>Type System</strong></td>\n<td>Implicit string-based</td>\n<td>Explicit with inference</td>\n<td>Aggressive implicit inference</td>\n</tr>\n<tr>\n<td><strong>Nesting Support</strong></td>\n<td>Section-based hierarchy only</td>\n<td>Full nested structures with dotted keys</td>\n<td>Arbitrary depth through indentation</td>\n</tr>\n<tr>\n<td><strong>Comment Handling</strong></td>\n<td>Semicolon and hash prefixes</td>\n<td>Hash prefix only</td>\n<td>Hash prefix with flow considerations</td>\n</tr>\n<tr>\n<td><strong>String Literals</strong></td>\n<td>Basic quoting with simple escapes</td>\n<td>Multiple string types with complex rules</td>\n<td>Quoted, unquoted, and multiline variants</td>\n</tr>\n<tr>\n<td><strong>Error Recovery</strong></td>\n<td>Forgiving - skip malformed lines</td>\n<td>Strict - fail on ambiguity</td>\n<td>Context-sensitive validation</td>\n</tr>\n</tbody></table>\n<p><strong>INI Format Parsing Characteristics</strong> reflect its origins as a simple configuration mechanism for early PC software. The format prioritizes ease of manual editing over sophisticated data representation, resulting in parsing requirements that favor line-by-line processing with minimal lookahead. INI parsers can process files streaming fashion, making decisions about each line independently based on simple pattern matching. This simplicity, however, creates ambiguities that require careful design decisions around edge cases.</p>\n<p>The section-based organization of INI files creates a natural parsing structure where the parser maintains minimal state—primarily the current section name and accumulated key-value pairs. Section headers use bracket syntax <code>[section_name]</code> that&#39;s visually distinct from key-value pairs, enabling reliable pattern-based identification. Key-value pairs support both equals and colon separators (<code>key = value</code> and <code>key: value</code>) with whitespace trimming, but this flexibility introduces edge cases around keys or values that contain these separator characters.</p>\n<p>INI comment handling appears straightforward but contains subtle complexities that impact parser design. Comments can begin with semicolons or hash symbols and extend to the end of the line, but the interaction between comments and quoted strings requires careful consideration. A line like <code>path = &quot;C:\\Program Files\\App&quot; ; comment</code> must distinguish between semicolons inside quoted strings and comment-starting semicolons, requiring some form of quoted string parsing even in this simple format.</p>\n<p><strong>TOML Format Parsing Characteristics</strong> represent the opposite extreme from INI simplicity, embracing explicit syntax that eliminates ambiguity at the cost of parsing complexity. TOML requires full tokenization with lookahead parsing, recursive descent for nested structures, and sophisticated type inference that respects explicit type annotations while handling implicit cases gracefully.</p>\n<p>The tokenization requirements for TOML include multiple string literal types that each follow different processing rules. Basic strings use double quotes with escape sequence processing, literal strings use single quotes with minimal processing, and multiline variants of both types have complex rules for handling leading and trailing whitespace. Integer literals can include underscores for readability (<code>1_000_000</code>), floating-point numbers support scientific notation, and the format includes native boolean and datetime types that require specialized parsing logic.</p>\n<p>TOML&#39;s table structure creates some of the most complex parsing challenges in configuration file processing. Simple tables use <code>[table_name]</code> syntax similar to INI sections, but nested tables use dotted notation <code>[table.subtable.deep]</code> that requires creating intermediate structures dynamically. Array-of-tables syntax <code>[[table_name]]</code> creates list structures containing dictionaries, with complex rules about when new entries are created versus when existing entries are extended. Dotted key assignments like <code>physical.color = &quot;orange&quot;</code> can create nested structures implicitly, but conflict with explicitly defined tables in ways that require careful validation.</p>\n<table>\n<thead>\n<tr>\n<th>TOML Parsing Challenge</th>\n<th>Complexity Level</th>\n<th>Key Difficulty</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Basic key-value pairs</strong></td>\n<td>Low</td>\n<td>Type inference and string literal handling</td>\n</tr>\n<tr>\n<td><strong>Inline tables and arrays</strong></td>\n<td>Medium</td>\n<td>Balanced bracket parsing with nested values</td>\n</tr>\n<tr>\n<td><strong>Table headers</strong></td>\n<td>Medium</td>\n<td>Nested structure creation and conflict detection</td>\n</tr>\n<tr>\n<td><strong>Array of tables</strong></td>\n<td>High</td>\n<td>List management with dictionary entry semantics</td>\n</tr>\n<tr>\n<td><strong>Dotted keys</strong></td>\n<td>High</td>\n<td>Implicit structure creation with conflict resolution</td>\n</tr>\n</tbody></table>\n<p><strong>YAML Subset Parsing Characteristics</strong> introduce indentation-sensitive parsing that requires fundamentally different algorithms from character-delimited formats. YAML parsers must track indentation levels using a stack-based approach, making parsing decisions based on the relationship between current and previous indentation rather than explicit delimiters.</p>\n<p>The indentation sensitivity of YAML creates parsing challenges that don&#39;t exist in other formats. The parser must distinguish between spaces and tabs (YAML forbids tabs for indentation), track indentation levels precisely, and determine when indentation changes indicate structure transitions versus continuation of existing structures. A mapping entry like <code>key: value</code> at indentation level 2 might start a new mapping, continue an existing sequence, or represent a nested value depending on the preceding context.</p>\n<p>YAML&#39;s type inference system attempts to automatically detect appropriate types for scalar values, but the rules can surprise users and complicate parsing. The string &quot;yes&quot; becomes boolean true, &quot;1.0&quot; becomes a floating-point number, and &quot;2023-10-15&quot; might become a date object depending on parser configuration. This aggressive inference requires parsers to implement pattern matching against multiple type signatures while providing mechanisms for users to override automatic detection through explicit quoting.</p>\n<p>Flow syntax in YAML provides inline alternatives to block structure using familiar bracket and brace notation (<code>[1, 2, 3]</code> for sequences and <code>{key: value}</code> for mappings). Supporting flow syntax requires YAML parsers to handle both indentation-sensitive block parsing and delimiter-based parsing within the same document, often switching between modes multiple times as parsing proceeds.</p>\n<blockquote>\n<p><strong>Decision: Unified Parsing Architecture vs Format-Specific Parsers</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support multiple configuration formats with different parsing requirements and complexity levels</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Completely separate parsers for each format with no shared code</li>\n<li>Unified parser that handles all formats through extensive configuration</li>\n<li>Shared tokenization layer with format-specific parsing logic</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Shared tokenization foundation with format-specific parsing components</li>\n<li><strong>Rationale</strong>: Balances code reuse for common functionality (string handling, position tracking, error reporting) while acknowledging that parsing strategies differ fundamentally between formats</li>\n<li><strong>Consequences</strong>: Enables consistent error reporting and position tracking across formats, allows incremental implementation starting with simpler formats, but requires careful interface design to accommodate different tokenization needs</li>\n</ul>\n</blockquote>\n<p>The architectural decision to use shared tokenization with format-specific parsers reflects the reality that while these formats differ significantly in syntax and semantics, they share common low-level requirements around string processing, position tracking, and error context management. This approach allows the implementation to start with the simpler INI format to establish core tokenization patterns, then extend the tokenizer capabilities as needed for TOML&#39;s more complex requirements, and finally adapt for YAML&#39;s unique indentation-sensitive needs.</p>\n<p><strong>Error Handling Strategy Comparison</strong> reveals how format characteristics influence error detection, recovery, and reporting approaches. Each format&#39;s syntax philosophy directly impacts what constitutes an error, how errors can be detected, and what recovery strategies are viable.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>INI Approach</th>\n<th>TOML Approach</th>\n<th>YAML Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Syntax Errors</strong></td>\n<td>Skip malformed lines, continue processing</td>\n<td>Fail fast with precise error location</td>\n<td>Context-sensitive validation with recovery hints</td>\n</tr>\n<tr>\n<td><strong>Type Errors</strong></td>\n<td>No type validation (strings only)</td>\n<td>Strict type checking with clear messages</td>\n<td>Type inference conflicts with override suggestions</td>\n</tr>\n<tr>\n<td><strong>Structure Errors</strong></td>\n<td>Section redefinition warnings</td>\n<td>Key redefinition failures with conflict location</td>\n<td>Indentation inconsistencies with structure visualization</td>\n</tr>\n<tr>\n<td><strong>Encoding Errors</strong></td>\n<td>Best-effort ASCII compatible processing</td>\n<td>UTF-8 validation with byte position reporting</td>\n<td>Full Unicode support with normalization guidance</td>\n</tr>\n</tbody></table>\n<p>This comprehensive comparison of format characteristics and parsing requirements establishes the foundation for designing a unified configuration parsing system that respects each format&#39;s unique properties while sharing common infrastructure where beneficial. The next sections will detail how these insights translate into specific architectural decisions and component designs.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation of a multi-format configuration parser requires careful technology selection and project organization that accommodates the varying complexity levels of different formats while maintaining clean separation between shared and format-specific functionality.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tokenizer Core</strong></td>\n<td>Regular expressions with manual state tracking</td>\n<td>Hand-written state machine with character-level control</td>\n</tr>\n<tr>\n<td><strong>String Processing</strong></td>\n<td>Python built-in string methods with manual escape handling</td>\n<td>Custom string literal parser with full Unicode support</td>\n</tr>\n<tr>\n<td><strong>Data Structures</strong></td>\n<td>Native dictionaries and lists with manual nesting</td>\n<td>Custom tree structures with metadata tracking</td>\n</tr>\n<tr>\n<td><strong>Error Reporting</strong></td>\n<td>Exception-based with basic line number tracking</td>\n<td>Structured error objects with position ranges and suggestions</td>\n</tr>\n<tr>\n<td><strong>File I/O</strong></td>\n<td>Direct file reading with encoding detection</td>\n<td>Streaming parser with configurable buffer sizes</td>\n</tr>\n<tr>\n<td><strong>Type Inference</strong></td>\n<td>String-based with manual conversion functions</td>\n<td>Plugin-based type detection with user customization</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config-parser/\n├── src/\n│   ├── __init__.py                    ← public API exports\n│   ├── core/\n│   │   ├── __init__.py               ← shared parsing infrastructure\n│   │   ├── tokenizer.py              ← base tokenization with position tracking\n│   │   ├── errors.py                 ← error types and reporting utilities\n│   │   ├── types.py                  ← token definitions and data structures\n│   │   └── detector.py               ← format detection logic\n│   ├── parsers/\n│   │   ├── __init__.py               ← parser registry and factory\n│   │   ├── ini_parser.py             ← INI format implementation (start here)\n│   │   ├── toml_parser.py            ← TOML format implementation\n│   │   └── yaml_parser.py            ← YAML subset implementation\n│   └── utils/\n│       ├── __init__.py               ← utility functions\n│       ├── string_utils.py           ← string literal processing helpers\n│       └── type_inference.py         ← automatic type detection\n├── tests/\n│   ├── test_data/                    ← sample configuration files\n│   ├── test_tokenizer.py            ← tokenizer unit tests\n│   ├── test_ini_parser.py           ← INI parser tests\n│   ├── test_toml_parser.py          ← TOML parser tests\n│   └── test_yaml_parser.py          ← YAML parser tests\n└── examples/\n    ├── basic_usage.py               ← simple parsing examples\n    └── advanced_features.py        ← error handling and customization</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/core/types.py - Complete token type definitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Common tokens across all formats</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NEWLINE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Format-specific tokens</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()      </span><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()        </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()        </span><span style=\"color:#6A737D\"># [ in TOML/YAML context</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()          </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()       </span><span style=\"color:#6A737D\"># {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()         </span><span style=\"color:#6A737D\"># }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # YAML-specific</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BLOCK_SEQUENCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()     </span><span style=\"color:#6A737D\"># -</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Special tokens</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVALID</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks position in source file for error reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a parsed token with position and value information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/core/errors.py - Complete error handling infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Position</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parsing errors with position information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggestion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._format_message())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_message</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"At </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">msg</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suggestion:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">msg</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">Suggestion: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.suggestion</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> msg</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors during tokenization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors in format-specific syntax.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors in logical structure (key conflicts, invalid nesting).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_error_context</span><span style=\"color:#E1E4E8\">(source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Position, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate visual error context showing problematic source location.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.split(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, position.line </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> context_lines </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(lines), position.line </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> context_lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(start_line, end_line):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line_num </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        marker </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \">>>\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"   \"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">marker</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#79B8FF\"> {</span><span style=\"color:#E1E4E8\">line_num</span><span style=\"color:#F97583\">:3</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">lines[i]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add column pointer for error line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pointer </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" \"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">7</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> position.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"^\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            context.append(pointer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">.join(context)</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/core/tokenizer.py - Base tokenizer with </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> implementation points</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Iterator, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType, Position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TokenError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base tokenizer providing position tracking and common functionality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: List[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_position</span><span style=\"color:#E1E4E8\">(self) -> Position:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current position for error reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Position(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.position)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look ahead at character without consuming it.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pos </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> offset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> pos </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#6A737D\">  # EOF marker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[pos]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume and return current character, updating position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.position]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main tokenization entry point - implement in subclasses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize tokenization state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Loop through characters calling appropriate token handlers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle end-of-file and return complete token list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure all tokens have proper position information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Subclasses must implement tokenize()\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Skip whitespace characters (except newlines in some formats).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Define which characters count as skippable whitespace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle format-specific whitespace rules (YAML indentation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update position tracking while skipping</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse quoted string with escape sequence handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Track starting position for error reporting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Consume opening quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Process characters until closing quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle escape sequences (\\n, \\t, \\\", \\\\, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle unterminated strings with helpful error messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return STRING token with processed value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse numeric literal (integer or float).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Collect digit characters and decimal points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle scientific notation (1e5, 1E-3)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle format-specific features (TOML underscores)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Distinguish between integers and floats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate number format and return NUMBER token</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>String Processing</strong>: Use Python&#39;s <code>str.translate()</code> for efficient character mapping during escape sequence processing</li>\n<li><strong>Position Tracking</strong>: Consider using <code>enumerate()</code> with <code>splitlines(keepends=True)</code> for line-aware processing</li>\n<li><strong>Regular Expressions</strong>: The <code>re</code> module&#39;s <code>finditer()</code> method provides position information useful for tokenization</li>\n<li><strong>Error Context</strong>: Use <code>textwrap.dedent()</code> and <code>textwrap.indent()</code> for clean error message formatting</li>\n<li><strong>Type Inference</strong>: Python&#39;s <code>ast.literal_eval()</code> can safely evaluate simple literals for type detection</li>\n<li><strong>File Encoding</strong>: Always specify encoding explicitly when opening files: <code>open(filename, &#39;r&#39;, encoding=&#39;utf-8&#39;)</code></li>\n<li><strong>Performance</strong>: For large configuration files, consider using <code>io.StringIO</code> for string manipulation instead of concatenation</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the basic tokenizer infrastructure:</p>\n<p><strong>Test Command</strong>: <code>python -m pytest tests/test_tokenizer.py -v</code></p>\n<p><strong>Expected Output</strong>: All tokenizer tests should pass, demonstrating proper position tracking, basic token recognition, and error handling.</p>\n<p><strong>Manual Verification</strong>: Create a simple test script:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> src.core.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseTokenizer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test with a simple input</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">source </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'key = \"value\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\"># comment</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BaseTokenizer(source)  </span><span style=\"color:#6A737D\"># Will fail until you implement a concrete subclass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer.tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' at </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">token.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Signs Something is Wrong</strong>:</p>\n<ul>\n<li>Position tracking jumps or becomes negative → Check advance() method character handling</li>\n<li>Tokens missing position information → Ensure current_position() called when creating tokens  </li>\n<li>String parsing fails on quotes → Verify escape sequence handling in read_string_literal()</li>\n<li>Performance issues on large files → Check for inefficient string concatenation in tokenizer loops</li>\n</ul>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (foundational scoping for INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser)</p>\n</blockquote>\n<p>Understanding the precise scope of our configuration file parser is essential for making informed architectural decisions and avoiding feature creep during implementation. Think of this section as drawing the boundaries of a map—we need to know not just what territories we&#39;ll explore, but also what lies beyond our borders so we don&#39;t accidentally wander into complexity quicksand. The goals and non-goals serve as our north star throughout the implementation journey, helping us make consistent decisions when faced with ambiguous requirements or tempting feature additions.</p>\n<p>The challenge in scoping a multi-format parser lies in the <strong>impedance mismatch</strong> between different configuration philosophies. INI files embrace simplicity with flat key-value pairs, TOML aims for human readability with explicit typing, and YAML prioritizes minimalist syntax with implicit typing. Each format makes different trade-offs between expressiveness, readability, and parsing complexity. Our scope must find the sweet spot that captures the essential utility of each format without drowning in edge cases that provide minimal real-world value.</p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p>The functional goals define the minimum viable product that delivers meaningful value to users while providing comprehensive learning experiences in parsing techniques. These goals were selected based on analyzing common configuration use cases and identifying the features that appear in 80% of real-world configuration files.</p>\n<h4 id=\"core-format-support-requirements\">Core Format Support Requirements</h4>\n<p>Our parser must support three distinct configuration formats, each representing a different parsing paradigm. This multi-format approach provides exposure to line-based parsing (INI), recursive descent parsing (TOML), and indentation-sensitive parsing (YAML). The format support is intentionally comprehensive within each format&#39;s core feature set rather than attempting to cover exotic edge cases.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Core Features</th>\n<th>Complexity Level</th>\n<th>Learning Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI</td>\n<td>Sections, key-value pairs, comments</td>\n<td>Low</td>\n<td>String processing, line parsing</td>\n</tr>\n<tr>\n<td>TOML</td>\n<td>Tables, arrays, type system, nested structures</td>\n<td>High</td>\n<td>Recursive descent, tokenization</td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Indentation-based nesting, mappings, sequences</td>\n<td>Medium</td>\n<td>Stack-based parsing, implicit typing</td>\n</tr>\n</tbody></table>\n<p><strong>INI Format Support Goals:</strong></p>\n<p>The INI parser serves as our foundational implementation, introducing core parsing concepts without overwhelming complexity. INI files follow a straightforward section-based organization that maps naturally to nested dictionaries. Our implementation must handle section headers using bracket notation <code>[section.subsection]</code>, creating appropriate nested dictionary structures. Key-value pairs support both equals and colon delimiters (<code>key=value</code> and <code>key: value</code>), with automatic whitespace trimming around keys and values.</p>\n<p>Comment handling requires supporting both semicolon and hash prefixes (<code>; comment</code> and <code># comment</code>), with comments allowed on their own lines and inline after values. String value parsing must handle both quoted and unquoted strings, with proper escape sequence processing for quoted strings including <code>\\&quot;</code>, <code>\\\\</code>, <code>\\n</code>, <code>\\r</code>, and <code>\\t</code>. The parser should support multi-line value continuation using backslash line endings, properly joining continued lines with space characters.</p>\n<p>Global key handling—keys that appear before any section header—requires creating an implicit root section or global namespace. This prevents data loss and provides consistent access patterns for simple configuration files that don&#39;t use sections.</p>\n<p><strong>TOML Format Support Goals:</strong></p>\n<p>TOML parsing represents the most complex component, introducing full tokenization and recursive descent parsing techniques. The tokenizer must recognize all TOML grammar elements including basic strings (quoted with double quotes), literal strings (quoted with single quotes), and their multiline variants (triple-quoted). Numeric parsing includes integers with optional underscores (<code>1_000_000</code>), floats with scientific notation, and hexadecimal/octal/binary integer formats.</p>\n<p>Table parsing handles both simple tables <code>[table.name]</code> and array-of-tables <code>[[array.name]]</code> syntax. Simple tables create nested dictionary structures where dotted paths like <code>[server.database.connection]</code> create three levels of nesting. Array-of-tables creates lists of dictionaries, where each occurrence of <code>[[servers]]</code> adds a new dictionary entry to the servers array.</p>\n<p>Inline syntax support includes inline tables <code>{ key1 = &quot;value1&quot;, key2 = 42 }</code> and inline arrays <code>[ 1, 2, &quot;three&quot;, true ]</code> with mixed type support. Dotted key notation like <code>physical.color = &quot;orange&quot;</code> automatically creates nested dictionary structures without requiring explicit table headers.</p>\n<p>The TOML type system requires proper handling of strings, integers, floats, booleans, datetime values, and nested combinations of arrays and tables. Type inference converts literal syntax to appropriate Python types automatically.</p>\n<p><strong>YAML Subset Support Goals:</strong></p>\n<p>Our YAML implementation focuses on the core block syntax that represents 90% of configuration use cases, deliberately avoiding the more exotic flow syntax variants that add complexity without proportional learning value. The subset includes indentation-based block structure parsing where nesting depth is determined by consistent indentation levels. Mappings use the <code>key: value</code> syntax with proper handling of nested mappings through indentation increases.</p>\n<p>Sequences use the dash prefix syntax (<code>- item</code>) for list creation, with support for both simple scalar sequences and complex nested sequences containing mappings or other sequences. Scalar type inference automatically converts unquoted values to appropriate types: numeric strings become integers or floats, <code>true</code>/<code>false</code>/<code>yes</code>/<code>no</code> become booleans, and everything else remains strings.</p>\n<p>Flow syntax support is limited to basic inline arrays <code>[item1, item2, item3]</code> and inline objects <code>{key1: value1, key2: value2}</code> to handle simple embedded structures without requiring full flow syntax parsing.</p>\n<h4 id=\"unified-interface-requirements\">Unified Interface Requirements</h4>\n<p>Beyond format-specific parsing, our system must provide a consistent interface that abstracts away format differences for consuming applications. This unified interface enables applications to work with configuration data without knowing the source format, promoting code reuse and simplifying configuration migration.</p>\n<p>The primary interface is a <code>parse_config(content, format=None)</code> function that accepts configuration content as a string and an optional format specifier. When format is not specified, the parser performs <strong>format detection</strong> by examining content characteristics like the presence of section brackets (INI), table headers (TOML), or significant indentation (YAML).</p>\n<p>All parsers return data in a <strong>unified output format</strong> consisting of nested dictionaries with string keys and mixed-type values (strings, numbers, booleans, lists, nested dictionaries). This standardized representation allows consuming code to navigate configuration hierarchies using consistent patterns regardless of source format.</p>\n<p>Error handling follows a structured approach using custom exception types: <code>TokenError</code> for lexical analysis failures, <code>SyntaxError</code> for grammar violations, and <code>StructureError</code> for logical inconsistencies like duplicate keys or circular references. All errors include position information (<code>Position</code> with line, column, and offset fields) and suggested fixes when possible.</p>\n<h4 id=\"development-and-learning-goals\">Development and Learning Goals</h4>\n<p>Since this project serves as a learning vehicle for parsing concepts, our functional goals include comprehensive educational outcomes alongside practical functionality. The implementation must demonstrate <strong>recursive descent parsing</strong> principles through the TOML parser, showing how complex nested structures can be parsed through function call recursion that mirrors grammar structure.</p>\n<p><strong>Tokenization</strong> concepts are thoroughly explored through the TOML lexer implementation, including state machine management for string literal parsing, lookahead techniques for disambiguating syntax, and position tracking for meaningful error reporting.</p>\n<p><strong>Type inference</strong> and automatic type conversion provide exposure to data type detection algorithms and the trade-offs between explicit and implicit typing systems. The different approaches across INI (mostly strings), TOML (explicit typing), and YAML (aggressive implicit typing) demonstrate the spectrum of type system design decisions.</p>\n<p>Error recovery and diagnostic message generation represent critical parsing skills that extend beyond configuration files to any language processing task. The implementation must demonstrate how to detect errors early, provide meaningful context, and suggest concrete fixes rather than cryptic failure messages.</p>\n<blockquote>\n<p><strong>Decision: Comprehensive Format Coverage vs. Focused Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Configuration parsers can either support many formats shallowly or fewer formats deeply</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Support 5+ formats (INI, TOML, YAML, JSON, XML) with basic functionality</li>\n<li>Support 3 formats (INI, TOML, YAML subset) with comprehensive feature coverage</li>\n<li>Support 1 format (TOML only) with production-ready completeness</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Option 2 - Three formats with comprehensive coverage</li>\n<li><strong>Rationale</strong>: Three formats provide exposure to fundamentally different parsing approaches (line-based, recursive descent, indentation-sensitive) while maintaining manageable complexity. Each format teaches distinct concepts without excessive overlap.</li>\n<li><strong>Consequences</strong>: Deeper learning experience with manageable scope, but requires careful subset selection for YAML to avoid overwhelming complexity</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Parsing Techniques Learned</th>\n<th>Implementation Complexity</th>\n<th>Real-World Utility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Many formats shallow</td>\n<td>Surface-level pattern matching</td>\n<td>Low per format</td>\n<td>Broad but shallow</td>\n</tr>\n<tr>\n<td>Three formats deep</td>\n<td>Line parsing, recursive descent, indentation parsing</td>\n<td>Medium overall</td>\n<td>Focused and practical</td>\n</tr>\n<tr>\n<td>Single format complete</td>\n<td>One technique deeply</td>\n<td>Low overall</td>\n<td>Deep but narrow</td>\n</tr>\n</tbody></table>\n<h3 id=\"non-goals\">Non-Goals</h3>\n<p>The non-goals are equally important as the functional goals, establishing clear boundaries that prevent scope creep and maintain focus on core learning objectives. These exclusions are deliberate choices based on complexity analysis, learning value assessment, and implementation timeline considerations.</p>\n<h4 id=\"advanced-yaml-features\">Advanced YAML Features</h4>\n<p>YAML&#39;s full specification includes numerous advanced features that add significant parsing complexity without proportional educational value for our core parsing learning objectives. We explicitly exclude <strong>multiline string syntax</strong> including folded scalars (<code>&gt;</code>) and literal scalars (<code>|</code>) which require complex whitespace processing rules and line break interpretation logic.</p>\n<p><strong>Flow syntax</strong> beyond basic inline collections is excluded, meaning we don&#39;t support complex nested flow structures, flow mappings with complex keys, or mixed block/flow syntax within the same document. <strong>Anchors and references</strong> (<code>&amp;anchor</code> and <code>*reference</code>) are excluded as they require symbol table management and cycle detection logic that distracts from core parsing concepts.</p>\n<p><strong>Complex implicit typing</strong> rules are simplified to basic cases. We don&#39;t support the full YAML type inference rules that can interpret strings like <code>2.10</code> as version numbers rather than floating-point numbers, or complex date/time parsing beyond basic ISO 8601 formats. <strong>Document directives</strong> (<code>%YAML 1.2</code>) and multiple documents in a single file are outside our scope.</p>\n<p>These exclusions allow us to focus on the fundamental parsing concepts—indentation tracking, nested structure creation, and basic type inference—without getting lost in YAML&#39;s more esoteric features.</p>\n<h4 id=\"performance-optimization-features\">Performance Optimization Features</h4>\n<p>Our parser prioritizes educational clarity over performance optimization, deliberately excluding features that would complicate the core algorithms without teaching fundamentally new parsing concepts. <strong>Streaming parsing</strong> for large files is excluded as it requires complex buffering logic and stateful parsing that obscures the recursive descent algorithms we&#39;re trying to demonstrate.</p>\n<p><strong>Memory optimization</strong> techniques like string interning, parse tree compression, or lazy evaluation are outside scope. The implementation will create full parse trees and nested dictionary structures in memory, accepting higher memory usage in favor of simpler, more understandable algorithms.</p>\n<p><strong>Incremental parsing</strong> and <strong>caching mechanisms</strong> are excluded as they require sophisticated invalidation logic and change detection that distracts from core parsing implementation. Each <code>parse_config()</code> call processes the entire input from scratch.</p>\n<p><strong>Parallel parsing</strong> for multi-document files or concurrent tokenization/parsing pipelines are beyond scope, as the coordination complexity would overshadow the parsing algorithm learning objectives.</p>\n<h4 id=\"production-ready-features\">Production-Ready Features</h4>\n<p>While our parser will handle real configuration files correctly, several production-ready features are explicitly excluded to maintain focus on parsing fundamentals rather than software engineering concerns.</p>\n<p><strong>Schema validation</strong> and <strong>configuration validation</strong> are excluded despite their practical importance. Adding schema support would require a type system definition language, validation rule engines, and comprehensive error reporting for constraint violations. These features teach configuration management rather than parsing techniques.</p>\n<p><strong>Plugin architectures</strong> for custom format support are outside scope. While our design will be extensible in principle, we won&#39;t implement the dynamic loading, registration systems, or interface contracts needed for runtime format plugin support.</p>\n<p><strong>Configuration merging</strong> and <strong>environment variable interpolation</strong> represent configuration management concerns rather than parsing challenges. Features like <code>${ENV_VAR}</code> substitution, file inclusion (<code>!include other.yaml</code>), or configuration inheritance require post-processing logic that operates on parsed results rather than parsing algorithms themselves.</p>\n<p><strong>Backwards compatibility</strong> with legacy format variants is excluded. We implement current standard versions of each format rather than supporting deprecated syntax or vendor-specific extensions.</p>\n<h4 id=\"advanced-error-recovery\">Advanced Error Recovery</h4>\n<p>While error detection and reporting are core goals, sophisticated <strong>error recovery</strong> mechanisms that attempt to continue parsing after syntax errors are excluded. Implementing recovery requires complex synchronization point detection, parser state restoration, and speculative parsing techniques that significantly complicate the core algorithms.</p>\n<p><strong>Syntax error correction</strong> and <strong>&quot;did you mean&quot; suggestions</strong> beyond basic cases are outside scope. We&#39;ll provide meaningful error messages with position information and simple suggestions, but not fuzzy matching or complex correction algorithms.</p>\n<p><strong>Incremental error reporting</strong> during parsing (reporting multiple errors in a single pass) is excluded in favor of <strong>fail-fast behavior</strong> that stops at the first error. This simplifies the parsing logic and error handling flow, though it means users might need multiple parse attempts to discover all syntax errors.</p>\n<blockquote>\n<p><strong>Decision: Educational Focus vs. Production Features</strong></p>\n<ul>\n<li><strong>Context</strong>: Learning projects can either simulate production requirements or focus purely on concept demonstration</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Include production features (schema validation, plugins, performance optimization) for realism</li>\n<li>Exclude production features to maintain focus on core parsing concepts</li>\n<li>Hybrid approach with optional production feature modules</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Option 2 - Exclude production features entirely</li>\n<li><strong>Rationale</strong>: Production features teach software engineering and API design rather than parsing algorithms. Including them would double the implementation complexity while diluting the core learning experience around tokenization, recursive descent parsing, and data structure mapping.</li>\n<li><strong>Consequences</strong>: Cleaner learning experience focused on parsing fundamentals, but resulting parser requires additional work for production use</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Feature Category</th>\n<th>Educational Value</th>\n<th>Implementation Complexity</th>\n<th>Scope Decision</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Core parsing algorithms</td>\n<td>Very High</td>\n<td>Medium</td>\n<td>Include</td>\n</tr>\n<tr>\n<td>Error detection/reporting</td>\n<td>High</td>\n<td>Low-Medium</td>\n<td>Include</td>\n</tr>\n<tr>\n<td>Performance optimization</td>\n<td>Low</td>\n<td>High</td>\n<td>Exclude</td>\n</tr>\n<tr>\n<td>Production features</td>\n<td>Low-Medium</td>\n<td>High</td>\n<td>Exclude</td>\n</tr>\n<tr>\n<td>Advanced error recovery</td>\n<td>Medium</td>\n<td>Very High</td>\n<td>Exclude</td>\n</tr>\n</tbody></table>\n<h4 id=\"format-subset-justification\">Format Subset Justification</h4>\n<p>Each format&#39;s feature exclusions follow specific principles designed to maximize learning value while minimizing implementation burden. The subset selections ensure that students encounter the core challenges of each parsing paradigm without getting overwhelmed by format-specific edge cases.</p>\n<p><strong>INI Subset Rationale:</strong></p>\n<p>INI format exclusions focus on eliminating legacy compatibility issues and vendor-specific extensions that don&#39;t teach fundamental parsing concepts. We exclude <strong>case-insensitive key handling</strong> as it adds string processing complexity without parsing algorithm learning value. <strong>Custom delimiter support</strong> beyond equals and colon is excluded as it requires configurable tokenization rules.</p>\n<p><strong>Escape sequence handling</strong> is limited to the standard set (<code>\\&quot;</code>, <code>\\\\</code>, <code>\\n</code>, <code>\\r</code>, <code>\\t</code>) rather than supporting arbitrary Unicode escape sequences or custom escape definitions. This provides sufficient complexity to learn escape processing without requiring full Unicode lexical analysis.</p>\n<p><strong>TOML Subset Rationale:</strong></p>\n<p>TOML exclusions eliminate the most complex edge cases while retaining comprehensive coverage of the core type system and table structures. <strong>Custom datetime formats</strong> beyond ISO 8601 are excluded, as datetime parsing teaches string pattern matching rather than recursive parsing concepts.</p>\n<p><strong>Complex array syntax</strong> like arrays of arrays of tables with mixed inline/block syntax are excluded as they require extensive lookahead and backtracking logic that obscures the core recursive descent patterns we&#39;re demonstrating.</p>\n<p><strong>Advanced numeric formats</strong> like infinite/NaN float values are excluded as they add IEEE 754 floating-point knowledge requirements without contributing to parsing algorithm understanding.</p>\n<p><strong>YAML Subset Rationale:</strong></p>\n<p>YAML exclusions focus on eliminating the features that make YAML parsing notoriously complex while retaining the indentation-sensitive parsing challenges that provide educational value. <strong>Complex key syntax</strong> including multi-line keys, flow keys, and complex keys (non-string keys) are excluded as they require sophisticated key parsing logic.</p>\n<p><strong>Advanced scalar syntax</strong> including tagged types (<code>!!int</code>, <code>!!str</code>), complex multiline strings, and aggressive implicit typing for version numbers, IP addresses, and other domain-specific formats are excluded. Basic implicit typing for numbers, booleans, and null values is retained as it demonstrates type inference concepts.</p>\n<h4 id=\"unified-interface-goals\">Unified Interface Goals</h4>\n<p>The parser must provide a clean, consistent interface that demonstrates good API design principles while supporting the learning objectives around format abstraction and error handling.</p>\n<table>\n<thead>\n<tr>\n<th>Interface Component</th>\n<th>Requirement</th>\n<th>Learning Objective</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>parse_config()</code> function</td>\n<td>Single entry point for all formats</td>\n<td>API design consistency</td>\n</tr>\n<tr>\n<td>Format auto-detection</td>\n<td>Determine format from content analysis</td>\n<td>Pattern recognition algorithms</td>\n</tr>\n<tr>\n<td>Unified output structure</td>\n<td>Nested dictionaries with consistent key access</td>\n<td>Data structure standardization</td>\n</tr>\n<tr>\n<td>Position-aware error reporting</td>\n<td>Line/column information for all parse errors</td>\n<td>Error context generation</td>\n</tr>\n<tr>\n<td>Type-preserved values</td>\n<td>Maintain numeric, boolean, and string types appropriately</td>\n<td>Type system handling</td>\n</tr>\n</tbody></table>\n<p>The unified output format must preserve type information where formats provide it explicitly (TOML), infer types where formats expect it (YAML), and provide reasonable defaults where formats are ambiguous (INI). This demonstrates the challenges of <strong>type system impedance mismatch</strong> across different configuration philosophies.</p>\n<p><strong>Format Detection Strategy Goals:</strong></p>\n<p>Automatic format detection teaches pattern recognition and heuristic algorithm development. The detection algorithm analyzes content characteristics to identify format signatures: INI files typically contain <code>[section]</code> headers or <code>key=value</code> patterns, TOML files contain explicit tables or arrays with TOML-specific syntax like <code>[[array.of.tables]]</code>, and YAML files exhibit significant indentation patterns and colon-space separators.</p>\n<p>The format detection must be robust enough to handle edge cases like empty files, files with only comments, or files that could plausibly match multiple formats. The algorithm should prefer more specific format indicators over general ones, defaulting to the most permissive format (INI) when detection is ambiguous.</p>\n<h4 id=\"error-handling-and-diagnostics-goals\">Error Handling and Diagnostics Goals</h4>\n<p>Comprehensive error handling serves dual purposes: demonstrating proper error management techniques and providing a debugging-friendly development experience. Our error handling goals focus on three categories of errors that teach different aspects of parser error management.</p>\n<p><strong>Lexical Errors (<code>TokenError</code>):</strong></p>\n<p>Lexical errors occur during tokenization when character sequences cannot be converted into valid tokens. Examples include unterminated string literals, invalid escape sequences, or malformed numeric literals. These errors must include precise position information and suggest specific fixes like adding closing quotes or correcting escape syntax.</p>\n<p><strong>Syntax Errors (<code>SyntaxError</code>):</strong></p>\n<p>Syntax errors occur when token sequences don&#39;t match the expected grammar rules. Examples include missing section closing brackets in INI files, invalid table header syntax in TOML files, or incorrect indentation patterns in YAML files. These errors require context-aware messaging that explains what was expected versus what was encountered.</p>\n<p><strong>Structure Errors (<code>StructureError</code>):</strong></p>\n<p>Structure errors occur when parsed content violates logical consistency rules like duplicate key definitions, conflicting type assignments, or circular reference creation. These errors teach the difference between syntactic validity and semantic correctness.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Example Triggers</th>\n<th>Required Context</th>\n<th>Learning Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TokenError</code></td>\n<td>Unterminated strings, invalid escapes</td>\n<td>Character position, surrounding text</td>\n<td>Lexical analysis debugging</td>\n</tr>\n<tr>\n<td><code>SyntaxError</code></td>\n<td>Grammar violations, unexpected tokens</td>\n<td>Token position, expected vs actual</td>\n<td>Grammar rule enforcement</td>\n</tr>\n<tr>\n<td><code>StructureError</code></td>\n<td>Duplicate keys, type conflicts</td>\n<td>Logical position, conflicting definitions</td>\n<td>Semantic validation</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-structure-mapping-goals\">Data Structure Mapping Goals</h4>\n<p>The parser must demonstrate effective techniques for mapping disparate source formats to a unified internal representation. This <strong>nested structure mapping</strong> teaches fundamental data transformation concepts that apply beyond configuration parsing to any data integration challenge.</p>\n<p><strong>Hierarchical Structure Creation:</strong></p>\n<p>All three formats must map to consistent nested dictionary structures despite using different syntax for hierarchy expression. INI sections become dictionary keys, TOML tables become nested dictionary paths, and YAML indentation becomes nested dictionary nesting. This mapping consistency allows consuming applications to navigate configuration hierarchies using uniform key access patterns.</p>\n<p><strong>Type Preservation and Conversion:</strong></p>\n<p>The mapping must preserve semantic information where possible while providing consistent access patterns. TOML&#39;s explicit typing is preserved directly, YAML&#39;s implicit typing is resolved during parsing, and INI&#39;s string-based values undergo basic type inference for numeric and boolean patterns.</p>\n<p><strong>List and Dictionary Distinction:</strong></p>\n<p>The parser must clearly distinguish between configuration structures that represent ordered lists versus unordered mappings, preserving this distinction in the output format. TOML arrays become Python lists, YAML sequences become Python lists, and INI multi-value keys (where supported) become Python lists.</p>\n<blockquote>\n<p>The fundamental principle guiding our scope decisions is <strong>learning amplification</strong>—every included feature should teach a distinct parsing concept or reinforce previously learned concepts through varied application. Features that add complexity without educational payoff are ruthlessly excluded, while features that demonstrate core parsing principles are included even when they increase implementation effort.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation approach balances educational clarity with practical functionality, using Python&#39;s strengths in string processing and dynamic typing while demonstrating parsing concepts that transfer to other languages.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>String Processing</td>\n<td>Built-in <code>str</code> methods with manual indexing</td>\n<td>Regular expressions with <code>re</code> module</td>\n</tr>\n<tr>\n<td>Data Structures</td>\n<td>Native <code>dict</code> and <code>list</code> with manual type checking</td>\n<td><code>typing</code> module with type hints throughout</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Custom exception classes with string messages</td>\n<td>Rich context objects with position tracking</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Built-in <code>unittest</code> module</td>\n<td><code>pytest</code> with fixtures and parametrization</td>\n</tr>\n<tr>\n<td>File I/O</td>\n<td>Simple <code>open()</code> and <code>read()</code> operations</td>\n<td><code>pathlib</code> with encoding detection</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>The project organization reflects the component separation discussed in the high-level architecture, with clear boundaries between format-specific implementations and shared infrastructure:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config-parser/\n  src/\n    config_parser/\n      __init__.py              ← public API exports\n      core/\n        __init__.py\n        tokenizer.py           ← BaseTokenizer and TokenType definitions\n        errors.py              ← ParseError hierarchy\n        position.py            ← Position tracking utilities\n      formats/\n        __init__.py\n        ini_parser.py          ← INI-specific parsing logic\n        toml_parser.py         ← TOML tokenizer and parser\n        yaml_parser.py         ← YAML subset parser\n      utils/\n        __init__.py\n        format_detection.py    ← Automatic format detection\n        type_inference.py      ← Shared type conversion utilities\n  tests/\n    unit/\n      test_tokenizer.py\n      test_ini_parser.py\n      test_toml_parser.py\n      test_yaml_parser.py\n    integration/\n      test_unified_interface.py\n      test_format_detection.py\n    fixtures/\n      sample_configs/         ← test configuration files\n        basic.ini\n        complex.toml\n        nested.yaml\n  examples/\n    basic_usage.py\n    error_handling_demo.py\n    format_comparison.py</code></pre></div>\n\n<h4 id=\"core-infrastructure-starter-code\">Core Infrastructure Starter Code</h4>\n<p>The following infrastructure provides the foundation for all format-specific parsers, handling position tracking, error management, and basic tokenization concepts:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core parsing infrastructure providing position tracking, error handling,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">and base tokenization functionality for all configuration format parsers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Any, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token types recognized across all supported configuration formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NEWLINE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BLOCK_SEQUENCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVALID</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks position in source text for error reporting and debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual lexical unit with type, value, and position information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_text: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all configuration parsing errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggestion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._format_message())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_message</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Format error message with position and suggestion information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement error message formatting with position context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include suggestion text when available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add visual indicators for error location</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Lexical analysis errors during tokenization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Grammar violations during parsing phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Logical consistency violations in parsed structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EOF_MARKER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base tokenizer providing common functionality for all formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: List[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_position</span><span style=\"color:#E1E4E8\">(self) -> Position:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns current parsing position for error reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Position(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.position)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look ahead at character without consuming it.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pos </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> offset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> pos </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> EOF_MARKER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[pos]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume current character and update position tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> EOF_MARKER</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        char </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.source[</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.position]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> char</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main tokenization entry point. Override in format-specific tokenizers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement format-specific tokenization logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle whitespace according to format semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recognize format-specific token patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Build token list with position tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Skip whitespace characters that don't carry semantic meaning.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">in</span><span style=\"color:#9ECBFF\"> ' </span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.peek() </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> EOF_MARKER</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.advance()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse quoted string with escape sequence processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume opening quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Process characters until closing quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle escape sequences (\\n, \\t, \\\", \\\\, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle unterminated string error case</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return STRING token with unescaped value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse numeric literal with type inference.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect digit characters and decimal points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle scientific notation (1e5, 1.2e-3)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect integer vs float based on decimal point presence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle number format errors (multiple decimal points)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return NUMBER token with converted Python value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_error_context</span><span style=\"color:#E1E4E8\">(source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Position, context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate visual error context showing source location with line numbers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.split(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, position.line </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> context_lines </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(lines), position.line </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> context_lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(start_line, end_line):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line_num </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line_content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> lines[i] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(lines) </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        marker </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" -> \"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"    \"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">marker</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">line_num</span><span style=\"color:#F97583\">:4d</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> | </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">line_content</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pointer </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" \"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(marker) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 7</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> position.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"^\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            context.append(pointer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(context)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Public API function signatures (implement in respective milestone modules)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse_config</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, format: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main entry point for configuration parsing with automatic format detection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement format detection when format=None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Delegate to appropriate format-specific parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle parsing errors with rich context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return unified nested dictionary structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_format</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Analyze content characteristics to determine configuration format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for INI-specific patterns ([sections], key=value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for TOML-specific patterns ([[tables]], explicit arrays)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for YAML-specific patterns (indentation, key: value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return format string or raise detection error</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"parser-implementation-skeleton\">Parser Implementation Skeleton</h4>\n<p>Each format-specific parser builds on the base tokenizer infrastructure while implementing parsing algorithms appropriate to the format&#39;s syntactic structure:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Format-specific parser skeletons demonstrating different parsing approaches.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Students implement the </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#9ECBFF\"> sections using concepts from respective milestones.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> INIParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Line-based parser for INI format configuration files.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content.split(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line_index </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_section </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse INI content into nested dictionary structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize result dict and set current_section to global</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Iterate through lines, skipping empty lines and comments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect section headers and update current_section</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse key-value pairs and add to current section</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle multi-line continuations with backslash endings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return completed nested dictionary structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_section_header</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract section name from [section] line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate bracket syntax and extract section name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Support nested sections with dot notation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle whitespace around section names</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_key_value_pair</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse key=value or key: value into (key, value) tuple.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Split on = or : delimiter (handle multiple = in value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Trim whitespace from key and value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle quoted values with escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Strip inline comments from unquoted values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TOMLParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Recursive descent parser for TOML format with full tokenization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TOMLTokenizer(content)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tokenizer.tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.token_index </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse TOML tokens into nested dictionary structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize result dictionary and current table context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Process tokens sequentially, dispatching by token type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle table headers and array-of-tables headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse key-value assignments with dotted key support</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle inline tables and inline arrays recursively</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate no key redefinition errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_table_header</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse [table.name] header and return dotted table path.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume opening bracket token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect identifier and dot tokens for table path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume closing bracket token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate table path doesn't conflict with existing keys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_inline_table</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse {key1 = value1, key2 = value2} inline table syntax.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume opening brace token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse key-value pairs separated by commas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle nested inline tables and arrays recursively</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consume closing brace token and return dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Stack-based parser for YAML subset with indentation sensitivity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content.split(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line_index </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.indentation_stack </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse YAML content using indentation-based nesting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize result and indentation tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Process lines, calculating indentation levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle indentation increases (push to stack, nest deeper)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle indentation decreases (pop from stack, return to parent)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Parse mappings (key: value) and sequences (- item)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle flow syntax for inline collections</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_mapping_line</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, indent_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse 'key: value' mapping line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Split on first colon character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Trim whitespace from key and value parts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle quoted keys and values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Perform type inference on value part</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_indentation</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate indentation level and validate consistency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Count leading spaces (forbid tabs in YAML)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate indentation is consistent with previous levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return indentation level for stack management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>Each milestone includes specific verification points to ensure correct implementation before proceeding to the next phase:</p>\n<p><strong>Milestone 1 Checkpoint (INI Parser):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/unit/test_ini_parser.py -v</code></li>\n<li>Expected: All basic INI parsing tests pass including sections, comments, quoted strings</li>\n<li>Manual verification: Parse sample INI file and verify nested dictionary structure</li>\n<li>Debug check: Print parsed structure and confirm section nesting is correct</li>\n</ul>\n<p><strong>Milestone 2 Checkpoint (TOML Tokenizer):</strong></p>\n<ul>\n<li>Run: <code>python -c &quot;from config_parser.formats.toml_parser import TOMLTokenizer; t = TOMLTokenizer(&#39;key = \\&quot;value\\&quot;&#39;); print(t.tokenize())&quot;</code></li>\n<li>Expected: List of tokens including IDENTIFIER, EQUALS, STRING, EOF with correct values</li>\n<li>Manual verification: Tokenize complex TOML sample and verify all token types are recognized</li>\n<li>Debug check: Verify position tracking is accurate by checking token position fields</li>\n</ul>\n<p><strong>Milestone 3 Checkpoint (TOML Parser):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/unit/test_toml_parser.py -v</code></li>\n<li>Expected: TOML parsing tests pass including tables, arrays-of-tables, dotted keys</li>\n<li>Manual verification: Parse complex TOML file with nested tables and verify structure</li>\n<li>Debug check: Test table redefinition error handling works correctly</li>\n</ul>\n<p><strong>Milestone 4 Checkpoint (YAML Parser):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/unit/test_yaml_parser.py -v</code></li>\n<li>Expected: YAML subset tests pass including indented blocks, sequences, mappings</li>\n<li>Manual verification: Parse nested YAML structure and verify indentation handling</li>\n<li>Debug check: Test indentation validation correctly rejects malformed input</li>\n</ul>\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>String Processing Efficiency:</strong>\nUse string slicing (<code>source[start:end]</code>) rather than character-by-character processing where possible. Python&#39;s string methods like <code>str.startswith()</code>, <code>str.strip()</code>, and <code>str.split()</code> are optimized for common parsing operations.</p>\n<p><strong>Type Inference Implementation:</strong>\nLeverage Python&#39;s dynamic typing for simple type inference: use <code>int()</code>, <code>float()</code>, and <code>bool()</code> conversion functions with try-catch blocks to detect appropriate types. For YAML boolean inference, check against common boolean string representations: <code>{&#39;true&#39;, &#39;false&#39;, &#39;yes&#39;, &#39;no&#39;, &#39;on&#39;, &#39;off&#39;}</code>.</p>\n<p><strong>Error Context Generation:</strong>\nUse <code>str.splitlines(keepends=True)</code> to preserve original line endings when generating error context displays. This maintains consistent character position calculations across different line ending styles.</p>\n<p><strong>Dictionary Path Creation:</strong>\nFor nested dictionary creation from dotted paths like <code>server.database.host</code>, use a helper function that creates intermediate dictionary levels as needed:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> set_nested_dict_value</span><span style=\"color:#E1E4E8\">(target_dict: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">, dotted_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Set value in nested dictionary using dotted path notation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Split path on dots</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Navigate/create intermediate dictionary levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set final key to value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle conflicts with existing non-dict values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Position Tracking Accuracy:</strong>\nMaintain separate line and column counters during tokenization. Increment line counter on <code>\\n</code> characters and reset column to 1. Increment column counter for all other characters including <code>\\r</code> and <code>\\t</code>. Track absolute byte offset separately for efficient source text slicing.</p>\n<h4 id=\"common-implementation-pitfalls\">Common Implementation Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Inconsistent Position Tracking</strong>\nForgetting to update position information during tokenization leads to meaningless error messages. Always call <code>current_position()</code> before creating tokens, and ensure <code>advance()</code> properly updates line, column, and offset counters. Test position tracking by deliberately introducing syntax errors and verifying error messages point to the correct location.</p>\n<p>⚠️ <strong>Pitfall: Format Detection False Positives</strong>\nSimple format detection can misidentify files when formats share syntax elements. A TOML file with only key-value pairs might be detected as INI. Use multiple detection criteria and prefer more specific format indicators. Test detection with edge cases like empty files, comment-only files, and ambiguous minimal examples.</p>\n<p>⚠️ <strong>Pitfall: Type Inference Inconsistency</strong>\nDifferent formats have different type inference rules. INI typically treats everything as strings unless explicitly converted, TOML has explicit typing, and YAML aggressively infers types. Implement format-specific type inference rather than trying to unify the rules, as the differences are fundamental to each format&#39;s design philosophy.</p>\n<p>⚠️ <strong>Pitfall: Error Recovery Complexity</strong>\nAttempting sophisticated error recovery in a learning project leads to complex parser state management that obscures the core parsing algorithms. Use fail-fast error handling that stops at the first error with detailed diagnostics rather than trying to continue parsing after errors.</p>\n<p>The implementation approach emphasizes correctness and educational clarity over performance optimization or advanced features. Focus on making the core parsing algorithms clear and well-tested before considering any optimizations or additional features beyond the defined scope.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - this section establishes the foundational architecture that supports all format-specific implementations</p>\n</blockquote>\n<p>Configuration file parsing requires a carefully designed architecture that can handle three fundamentally different syntactic approaches while maintaining clean separation of concerns. Think of this architecture as a universal translator that needs to understand multiple languages (INI, TOML, YAML) but always produces the same kind of output (nested dictionaries). The key challenge lies in designing components that are both flexible enough to handle format-specific complexities and structured enough to share common parsing infrastructure.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Component Architecture\"></p>\n<p>The architecture follows a classic <strong>two-phase parsing approach</strong>: lexical analysis (tokenization) followed by syntactic analysis (parsing). However, unlike traditional compilers that target machine code, our parser targets human-readable data structures. This creates unique design challenges around <strong>impedance mismatch</strong> - bridging the gap between human-friendly configuration syntax and machine-friendly nested dictionaries.</p>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>The system divides parsing responsibilities across five core components, each with distinct concerns and well-defined interfaces. This separation enables independent development and testing while maintaining clear data flow boundaries.</p>\n<p><strong>Format Detection Component</strong> serves as the system&#39;s entry point, automatically identifying which parser to invoke based on content analysis. Think of it as a postal sorting machine that examines envelope characteristics to determine the correct destination. This component analyzes syntactic markers, structural patterns, and format-specific signatures to make routing decisions without requiring full parsing.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Content Analysis</td>\n<td>Examines file content for format-specific signatures</td>\n<td>Raw configuration text</td>\n<td>Confidence scores per format</td>\n</tr>\n<tr>\n<td>Heuristic Application</td>\n<td>Applies weighted heuristics for ambiguous cases</td>\n<td>Syntactic patterns</td>\n<td>Format identification</td>\n</tr>\n<tr>\n<td>Fallback Strategy</td>\n<td>Handles cases where format cannot be determined</td>\n<td>Analysis results</td>\n<td>Default format selection</td>\n</tr>\n<tr>\n<td>Error Context</td>\n<td>Provides meaningful errors when detection fails</td>\n<td>Failed analysis state</td>\n<td>Diagnostic information</td>\n</tr>\n</tbody></table>\n<p><strong>Base Tokenizer Component</strong> provides the foundation for lexical analysis across all formats. Rather than implementing three separate tokenizers, we use inheritance and composition to share common functionality while allowing format-specific extensions. This component handles the universal aspects of tokenization: character stream management, position tracking, and basic token creation.</p>\n<table>\n<thead>\n<tr>\n<th>Core Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>current_position()</code></td>\n<td>None</td>\n<td><code>Position</code></td>\n<td>Returns current line, column, and offset</td>\n</tr>\n<tr>\n<td><code>peek(offset=0)</code></td>\n<td><code>offset: int</code></td>\n<td><code>str</code></td>\n<td>Lookahead without consuming characters</td>\n</tr>\n<tr>\n<td><code>advance()</code></td>\n<td>None</td>\n<td><code>str</code></td>\n<td>Consume character and update position</td>\n</tr>\n<tr>\n<td><code>skip_whitespace()</code></td>\n<td>None</td>\n<td><code>None</code></td>\n<td>Skip spaces and tabs (not newlines)</td>\n</tr>\n<tr>\n<td><code>read_string_literal(quote_char)</code></td>\n<td><code>quote_char: str</code></td>\n<td><code>Token</code></td>\n<td>Parse quoted string with escape processing</td>\n</tr>\n<tr>\n<td><code>read_number()</code></td>\n<td>None</td>\n<td><code>Token</code></td>\n<td>Parse numeric literal with type inference</td>\n</tr>\n</tbody></table>\n<p><strong>Format-Specific Parser Components</strong> (INI, TOML, YAML) handle the syntactic analysis phase, converting token streams into structured data. Each parser implements a different parsing strategy optimized for its format&#39;s characteristics: line-based parsing for INI, recursive descent for TOML, and indentation-sensitive parsing for YAML.</p>\n<p>The parsers share a common responsibility pattern but implement dramatically different algorithms:</p>\n<table>\n<thead>\n<tr>\n<th>Parser Type</th>\n<th>Strategy</th>\n<th>Primary Challenge</th>\n<th>Key Data Structure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI Parser</td>\n<td>Line-based sequential</td>\n<td>Section and key organization</td>\n<td>Flat section map</td>\n</tr>\n<tr>\n<td>TOML Parser</td>\n<td>Recursive descent</td>\n<td>Table nesting and type system</td>\n<td>Symbol table with dotted keys</td>\n</tr>\n<tr>\n<td>YAML Parser</td>\n<td>Indentation stack</td>\n<td>Block structure and implicit typing</td>\n<td>Indentation level stack</td>\n</tr>\n</tbody></table>\n<p><strong>Error Handling Component</strong> coordinates error detection, enrichment, and reporting across all parsing phases. This component transforms low-level parsing failures into actionable user feedback with context and suggestions. Think of it as a medical diagnostic system that not only identifies symptoms but also suggests treatments.</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Detection Phase</th>\n<th>Enrichment Strategy</th>\n<th>User Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>TokenError</code></td>\n<td>Lexical analysis</td>\n<td>Character-level context</td>\n<td>Syntax highlighting</td>\n</tr>\n<tr>\n<td><code>SyntaxError</code></td>\n<td>Structural parsing</td>\n<td>Grammar rule violations</td>\n<td>Format guidance</td>\n</tr>\n<tr>\n<td><code>StructureError</code></td>\n<td>Semantic validation</td>\n<td>Logical inconsistencies</td>\n<td>Schema suggestions</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The key architectural decision is treating tokenization and parsing as separate concerns connected by a well-defined token stream interface. This enables us to swap tokenizers independently of parsers and simplifies testing by allowing isolated validation of lexical analysis separate from structural parsing.</p>\n</blockquote>\n<p><strong>Architecture Decision: Shared Tokenizer Base vs Format-Specific Tokenizers</strong></p>\n<blockquote>\n<p><strong>Decision: Shared Base Tokenizer with Format-Specific Extensions</strong></p>\n<ul>\n<li><strong>Context</strong>: Configuration formats share many tokenization patterns (strings, numbers, identifiers) but have format-specific requirements (TOML datetime literals, YAML flow syntax, INI comment styles)</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Completely separate tokenizers for each format</li>\n<li>Single universal tokenizer handling all formats</li>\n<li>Base tokenizer class with format-specific extensions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Base tokenizer class with format-specific extensions</li>\n<li><strong>Rationale</strong>: Maximizes code reuse for common patterns while preserving flexibility for format-specific requirements. Reduces testing burden and maintenance overhead compared to separate tokenizers, while avoiding the complexity explosion of a single universal tokenizer trying to handle all format variations</li>\n<li><strong>Consequences</strong>: Requires careful interface design to support extension points. Creates inheritance relationships that must be managed, but significantly reduces code duplication and provides consistent position tracking and error reporting across formats</li>\n</ul>\n</blockquote>\n<p><strong>Architecture Decision: Token Stream vs Parse Tree Interface</strong></p>\n<blockquote>\n<p><strong>Decision: Token Stream Interface Between Components</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to connect tokenizers with parsers while maintaining component independence and testability</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Direct character stream parsing (no separate tokenization)</li>\n<li>Full parse tree generation before data structure creation</li>\n<li>Token stream interface with lazy evaluation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Token stream interface with lazy evaluation</li>\n<li><strong>Rationale</strong>: Token streams provide the right level of abstraction - more structured than character streams but lighter than full parse trees. Enables independent testing of tokenization logic and parser logic. Supports efficient memory usage through lazy token generation</li>\n<li><strong>Consequences</strong>: Requires careful token type design to support all formats. Creates additional interface complexity but enables better separation of concerns and more focused testing</li>\n</ul>\n</blockquote>\n<h3 id=\"recommended-module-structure\">Recommended Module Structure</h3>\n<p>The codebase organization reflects the architectural separation of concerns while promoting code reuse and maintainability. The structure supports independent development of format-specific components while sharing common infrastructure.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config_parser/\n├── __init__.py                 ← Main public API and format detection\n├── core/                       ← Shared infrastructure\n│   ├── __init__.py\n│   ├── tokens.py              ← Token types and base tokenizer\n│   ├── errors.py              ← Error hierarchy and context generation\n│   ├── positions.py           ← Position tracking utilities\n│   └── base_parser.py         ← Common parsing utilities\n├── formats/                    ← Format-specific implementations\n│   ├── __init__.py\n│   ├── ini/\n│   │   ├── __init__.py\n│   │   ├── tokenizer.py       ← INI-specific tokenization\n│   │   ├── parser.py          ← INI parsing logic\n│   │   └── validator.py       ← INI-specific validation\n│   ├── toml/\n│   │   ├── __init__.py\n│   │   ├── tokenizer.py       ← TOML tokenization with datetime/multiline\n│   │   ├── parser.py          ← Recursive descent TOML parser\n│   │   ├── tables.py          ← Table and array-of-tables handling\n│   │   └── validator.py       ← TOML semantic validation\n│   └── yaml/\n│       ├── __init__.py\n│       ├── tokenizer.py       ← YAML tokenization with indentation\n│       ├── parser.py          ← Indentation-sensitive parser\n│       ├── flow.py            ← Flow syntax handling\n│       └── validator.py       ← YAML subset validation\n├── utils/                      ← Utility functions\n│   ├── __init__.py\n│   ├── detection.py           ← Format detection heuristics\n│   ├── type_inference.py      ← Automatic type conversion\n│   └── string_utils.py        ← String processing utilities\n└── tests/                      ← Comprehensive test suite\n    ├── __init__.py\n    ├── unit/                  ← Component-level tests\n    ├── integration/           ← Cross-component tests\n    ├── fixtures/              ← Test configuration files\n    └── benchmarks/            ← Performance tests</code></pre></div>\n\n<p><strong>Module Responsibility Distribution</strong> ensures each module has a single, well-defined purpose while maintaining clear dependency relationships:</p>\n<table>\n<thead>\n<tr>\n<th>Module Category</th>\n<th>Primary Responsibility</th>\n<th>Dependencies</th>\n<th>Extension Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>core/</code></td>\n<td>Shared parsing infrastructure</td>\n<td>None (foundation layer)</td>\n<td>Token types, error types</td>\n</tr>\n<tr>\n<td><code>formats/</code></td>\n<td>Format-specific parsing logic</td>\n<td>Depends on <code>core/</code></td>\n<td>New format implementations</td>\n</tr>\n<tr>\n<td><code>utils/</code></td>\n<td>Cross-cutting utilities</td>\n<td>Depends on <code>core/</code></td>\n<td>Detection heuristics</td>\n</tr>\n<tr>\n<td>Main API</td>\n<td>Public interface and orchestration</td>\n<td>All modules</td>\n<td>Configuration options</td>\n</tr>\n</tbody></table>\n<p><strong>Architecture Decision: Flat vs Nested Module Structure</strong></p>\n<blockquote>\n<p><strong>Decision: Nested Module Structure with Format Separation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to organize format-specific code while maintaining discoverability and avoiding namespace pollution</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Flat structure with format prefixes (ini_parser.py, toml_parser.py)</li>\n<li>Single formats.py module with all implementations</li>\n<li>Nested structure with format-specific packages</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Nested structure with format-specific packages</li>\n<li><strong>Rationale</strong>: Each format has multiple related files (tokenizer, parser, validator), making packages natural organizational units. Supports independent development and testing of formats. Easier to add new formats without impacting existing code. Cleaner imports and namespace management</li>\n<li><strong>Consequences</strong>: Slightly more complex import paths but much better organization. Enables format-specific testing and development workflows. Supports future packaging of formats as separate distributions if needed</li>\n</ul>\n</blockquote>\n<p>The module structure supports several key <strong>extensibility patterns</strong>:</p>\n<p><strong>Format Addition Pattern</strong>: Adding new formats requires only creating a new package under <code>formats/</code> with the standard tokenizer/parser/validator structure. The main API automatically discovers new formats through package introspection.</p>\n<p><strong>Component Extension Pattern</strong>: Each core component can be extended through inheritance or composition. For example, adding new token types requires extending the <code>TokenType</code> enum and updating format-specific tokenizers to recognize them.</p>\n<p><strong>Utility Integration Pattern</strong>: Cross-cutting concerns like caching, streaming, or validation can be added through the <code>utils/</code> package without impacting core parsing logic.</p>\n<p><strong>Dependency Management</strong> follows a strict layered architecture:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>┌─────────────────┐\n│   Main API      │ ← Public interface, format detection\n├─────────────────┤\n│   Formats/      │ ← Format-specific parsers and tokenizers\n├─────────────────┤\n│   Utils/        │ ← Cross-cutting utilities and helpers\n├─────────────────┤\n│   Core/         │ ← Foundation: tokens, errors, base classes\n└─────────────────┘</code></pre></div>\n\n<p>This layered approach ensures that core components remain stable while format-specific implementations can evolve independently. Dependencies flow downward only - core components never depend on format-specific code, ensuring reusability and testability.</p>\n<p><strong>Architecture Decision: Package-Level vs Module-Level Organization</strong></p>\n<blockquote>\n<p><strong>Decision: Package-Level Organization for Format Implementations</strong></p>\n<ul>\n<li><strong>Context</strong>: Format implementations require multiple related modules (tokenizer, parser, validator) that should be grouped together</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>All format code in single modules (ini.py, toml.py, yaml.py)</li>\n<li>Package per format with internal module separation</li>\n<li>Mixed approach with simple formats as modules, complex as packages</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Package per format with internal module separation</li>\n<li><strong>Rationale</strong>: Even simple formats like INI benefit from separation of tokenization and parsing concerns. Consistent structure makes the codebase more predictable. Packages provide natural extension points for format-specific utilities and validation. Supports independent testing strategies per format</li>\n<li><strong>Consequences</strong>: More files and directories but much better organization. Enables format-specific development workflows and testing strategies. Makes it easier to contribute format-specific improvements without understanding other formats</li>\n</ul>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The following guidance provides concrete steps for implementing the high-level architecture using Python, focusing on establishing the foundational structure that will support all format-specific implementations.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenization</td>\n<td>Manual string parsing with position tracking</td>\n<td>Regular expressions with compiled patterns</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Basic exception hierarchy with string messages</td>\n<td>Rich error objects with context and suggestions</td>\n</tr>\n<tr>\n<td>Type Inference</td>\n<td>Simple isinstance checks and string conversion</td>\n<td>Configurable type system with custom converters</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>unittest with manual test data</td>\n<td>pytest with property-based testing</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Direct implementation focused on correctness</td>\n<td>Caching, lazy evaluation, and memory optimization</td>\n</tr>\n</tbody></table>\n<p><strong>B. Core Infrastructure Starter Code</strong></p>\n<p><strong>Position Tracking (<code>core/positions.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Position tracking utilities for parsing error reporting.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a position in the source text with line, column, and offset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_error_context</span><span style=\"color:#E1E4E8\">(source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Position, context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate visual error context showing the error location in source text.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.split(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(lines):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"Invalid position\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Calculate context window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, position.line </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> context_lines </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(lines), position.line </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> context_lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context_parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(start_line, end_line):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line_num </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line_content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> lines[i]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        prefix </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \">>> \"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"    \"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context_parts.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prefix</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">line_num</span><span style=\"color:#F97583\">:4d</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> | </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">line_content</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add pointer line for error position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pointer </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" \"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(prefix) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 7</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> position.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"^\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            context_parts.append(pointer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(context_parts)</span></span></code></pre></div>\n\n<p><strong>Error Hierarchy (<code>core/errors.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Error types for configuration parsing with context and suggestions.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parsing errors with position and suggestion support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.message]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            parts.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\" at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suggestion:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            parts.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Suggestion: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.suggestion</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">.join(parts)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors during lexical analysis (tokenization).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors during syntactic analysis (parsing structure).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors in logical structure (semantic validation).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Token System (<code>core/tokens.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Token definitions and base tokenizer for all configuration formats.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .positions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Position</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All token types needed across INI, TOML, and YAML formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Literal values</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto() </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Operators and punctuation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()         </span><span style=\"color:#6A737D\"># =</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()          </span><span style=\"color:#6A737D\"># :</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()          </span><span style=\"color:#6A737D\"># ,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()            </span><span style=\"color:#6A737D\"># .</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Structure markers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()  </span><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()    </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()    </span><span style=\"color:#6A737D\"># [ (in value context)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()      </span><span style=\"color:#6A737D\"># ] (in value context)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()   </span><span style=\"color:#6A737D\"># {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()     </span><span style=\"color:#6A737D\"># }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Whitespace and control</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NEWLINE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()         </span><span style=\"color:#6A737D\"># YAML indentation increase</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()         </span><span style=\"color:#6A737D\"># YAML indentation decrease</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BLOCK_SEQUENCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto() </span><span style=\"color:#6A737D\"># YAML list item marker (-)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Special tokens</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVALID</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A single token with type, value, position, and original text.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_text: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Foundation tokenizer providing common functionality for all formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: List[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_position</span><span style=\"color:#E1E4E8\">(self) -> Position:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns current parsing position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return Position object with current line, column, offset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Lookahead without consuming characters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return character at position + offset, or EOF_MARKER if beyond end</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle negative offsets for lookbehind</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume character and update position tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Get current character before advancing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update self.position, handle newline for line/column tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return consumed character or EOF_MARKER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Skip spaces and tabs but preserve newlines for structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Advance while current character is space or tab</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Do NOT skip newlines - they're structurally significant</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse quoted string with escape sequence processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Advance past opening quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Build string value while processing escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle \\n, \\t, \\r, \\\\, \\\", \\' escape sequences  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect unterminated strings and create appropriate error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return STRING token with processed value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse numeric literal with automatic type inference.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect digits, handle decimal points for floats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle scientific notation (1e5, 1E-3)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle integer underscores in TOML (1_000_000)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert to appropriate Python type (int, float)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return NUMBER token with converted value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main tokenization entry point - implemented by format-specific subclasses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: This is the main loop - format-specific tokenizers override this</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call appropriate read_* methods based on current character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle format-specific token types and syntax</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return complete list of tokens including final EOF</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Subclasses must implement tokenize()\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Useful constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EOF_MARKER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">'</span></span></code></pre></div>\n\n<p><strong>C. Main API Structure (<code>__init__.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Main configuration parser API with automatic format detection.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .utils.detection </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> detect_format</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .formats.ini.parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> INIParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .formats.toml.parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TOMLParser  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .formats.yaml.parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> YAMLParser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse_config</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, format: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main configuration parsing entry point with automatic format detection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        content: Raw configuration file content</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        format: Optional format override ('ini', 'toml', 'yaml')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Nested dictionary representing the configuration structure</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ParseError: When content cannot be parsed in the specified/detected format</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If format not specified, call detect_format(content)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create appropriate parser instance based on format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call parser.parse(content) and return result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Wrap parser-specific errors in generic ParseError with context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Export main API</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__all__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'parse_config'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'detect_format'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ParseError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'TokenError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'SyntaxError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'StructureError'</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<p><strong>D. Format Detection Starter (<code>utils/detection.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Automatic configuration format detection using content analysis.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_format</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Detect configuration format from content using weighted heuristics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Format string: 'ini', 'toml', or 'yaml'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'ini'</span><span style=\"color:#E1E4E8\">: _score_ini(content),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'toml'</span><span style=\"color:#E1E4E8\">: _score_toml(content), </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'yaml'</span><span style=\"color:#E1E4E8\">: _score_yaml(content)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Return format with highest confidence score</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(scores.keys(), </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> k: scores[k])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _score_ini</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate confidence score for INI format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    score </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for [section] headers - strong INI indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for key=value pairs - moderate indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for ; comments - moderate INI indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Penalize TOML-specific syntax like [[array]]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Penalize YAML-specific syntax like list items with -</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> score</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _score_toml</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate confidence score for TOML format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    score </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for [[array.of.tables]] - very strong TOML indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for dotted.keys = value - strong TOML indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for inline tables { key = value } - strong indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for TOML datetime formats - moderate indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Penalize YAML indentation patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> score</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _score_yaml</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate confidence score for YAML format.\"\"\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    score </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze indentation patterns - strong YAML indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for list items with - prefix - strong indicator  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for key: value with colon - moderate indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look for flow syntax [list] or {map} - moderate indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Penalize INI [sections] and TOML [[arrays]]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> score</span></span></code></pre></div>\n\n<p><strong>E. Development Workflow Recommendations</strong></p>\n<p><strong>Phase 1 - Foundation Setup</strong>:</p>\n<ol>\n<li>Implement the core infrastructure (positions, errors, tokens) completely</li>\n<li>Create empty format packages with proper <code>__init__.py</code> files</li>\n<li>Implement basic format detection with simple heuristics</li>\n<li>Set up comprehensive testing structure with fixtures</li>\n</ol>\n<p><strong>Phase 2 - Format Implementation Order</strong>:</p>\n<ol>\n<li>Start with INI parser (simplest format, builds confidence)</li>\n<li>Move to TOML tokenizer (introduces complex tokenization concepts)  </li>\n<li>Implement TOML parser (most complex parsing logic)</li>\n<li>Finish with YAML parser (different paradigm, indentation-sensitive)</li>\n</ol>\n<p><strong>Phase 3 - Integration and Polish</strong>:</p>\n<ol>\n<li>Connect all formats through main API</li>\n<li>Enhance format detection with real-world test cases</li>\n<li>Add comprehensive error handling with context</li>\n<li>Performance optimization and edge case handling</li>\n</ol>\n<p><strong>F. Common Architecture Pitfalls</strong></p>\n<p>⚠️ <strong>Pitfall: Mixing Tokenization and Parsing Logic</strong>\nMany implementations blur the line between lexical and syntactic analysis, making both components harder to test and debug. Keep tokenizers focused purely on character-to-token conversion without understanding structural meaning.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Error Position Tracking</strong>\nFailing to maintain accurate position information throughout the parsing pipeline makes debugging nearly impossible. Every token must carry position information, and every error must reference the relevant position.</p>\n<p>⚠️ <strong>Pitfall: Format-Specific Code in Shared Components</strong>\nPutting TOML-specific logic in the base tokenizer or YAML-specific handling in error reporting breaks the architectural separation. Keep shared components truly format-agnostic.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Interface Design</strong>\nDesigning token types or error interfaces that work for one format but need special cases for others indicates insufficient upfront analysis. Design interfaces to handle the union of all format requirements from the beginning.</p>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - defines the core data structures used throughout all parsing implementations</p>\n</blockquote>\n<p>The data model forms the foundation of our multi-format configuration parser, defining how we represent information as it flows through the parsing pipeline. Think of the data model as the common language that all components speak - just as different human languages can express the same concepts using different words and grammar, our INI, TOML, and YAML parsers must ultimately produce the same structured data despite processing fundamentally different syntactic approaches.</p>\n<p>The data model consists of three primary layers, each serving a distinct purpose in the parsing pipeline. The <strong>token layer</strong> represents the lowest-level meaningful units extracted from raw text - individual symbols, keywords, literals, and operators that carry semantic meaning. The <strong>parse tree layer</strong> provides an intermediate structural representation that captures the syntactic relationships between tokens while remaining close to the original format&#39;s grammar. Finally, the <strong>unified output layer</strong> presents a consistent nested dictionary structure that client code can work with regardless of which configuration format was parsed.</p>\n<p>This layered approach addresses one of the core challenges in multi-format parsing: the <strong>impedance mismatch</strong> between human-readable configuration syntax and machine-processable data structures. Each format has evolved different conventions for expressing hierarchy, data types, and structural relationships. INI files use section headers with flat key-value pairs, TOML employs table definitions with dotted key paths, and YAML relies on indentation-sensitive block structures. Our data model must bridge these syntactic differences while preserving the semantic intent of each format.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Ftoken-types.svg\" alt=\"Token Type Hierarchy\"></p>\n<h3 id=\"token-type-definitions\">Token Type Definitions</h3>\n<p>The tokenization layer transforms character streams into typed tokens that carry both lexical information and positional context. Understanding tokenization requires thinking of it as <strong>pattern recognition with state tracking</strong> - the tokenizer examines character sequences and classifies them into meaningful categories while maintaining awareness of where each token originated in the source text.</p>\n<p>Every token in our system carries four essential pieces of information: its semantic type (what kind of language construct it represents), its processed value (the meaningful content after handling escapes and type conversion), its source position (for error reporting), and its raw text (the exact character sequence from the original input). This comprehensive token representation enables sophisticated error reporting and supports advanced features like syntax highlighting or source-to-output mapping.</p>\n<p>The <code>TokenType</code> enumeration defines all possible token categories needed across INI, TOML, and YAML formats. This unified token vocabulary allows us to share tokenization logic where formats overlap while maintaining format-specific token types for unique constructs.</p>\n<table>\n<thead>\n<tr>\n<th>Token Type</th>\n<th>Description</th>\n<th>Example Raw Text</th>\n<th>Processed Value</th>\n<th>Used By</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STRING</code></td>\n<td>Quoted or unquoted string literal</td>\n<td><code>&quot;hello\\nworld&quot;</code></td>\n<td><code>hello\\nworld</code></td>\n<td>All formats</td>\n</tr>\n<tr>\n<td><code>NUMBER</code></td>\n<td>Numeric literal (integer or float)</td>\n<td><code>42</code>, <code>3.14</code>, <code>1_000</code></td>\n<td><code>42</code>, <code>3.14</code>, <code>1000</code></td>\n<td>TOML, YAML</td>\n</tr>\n<tr>\n<td><code>BOOLEAN</code></td>\n<td>Boolean literal</td>\n<td><code>true</code>, <code>false</code>, <code>yes</code>, <code>no</code></td>\n<td><code>True</code>, <code>False</code></td>\n<td>TOML, YAML</td>\n</tr>\n<tr>\n<td><code>IDENTIFIER</code></td>\n<td>Unquoted name or key</td>\n<td><code>hostname</code>, <code>database</code></td>\n<td><code>hostname</code>, <code>database</code></td>\n<td>All formats</td>\n</tr>\n<tr>\n<td><code>EQUALS</code></td>\n<td>Assignment operator</td>\n<td><code>=</code></td>\n<td><code>=</code></td>\n<td>INI, TOML</td>\n</tr>\n<tr>\n<td><code>COLON</code></td>\n<td>Key-value separator</td>\n<td><code>:</code></td>\n<td><code>:</code></td>\n<td>INI, YAML</td>\n</tr>\n<tr>\n<td><code>NEWLINE</code></td>\n<td>Line terminator</td>\n<td><code>\\n</code>, <code>\\r\\n</code></td>\n<td><code>\\n</code></td>\n<td>All formats</td>\n</tr>\n<tr>\n<td><code>EOF</code></td>\n<td>End of file marker</td>\n<td>(none)</td>\n<td><code>EOF_MARKER</code></td>\n<td>All formats</td>\n</tr>\n<tr>\n<td><code>COMMENT</code></td>\n<td>Comment text</td>\n<td><code># This is a comment</code></td>\n<td><code>This is a comment</code></td>\n<td>All formats</td>\n</tr>\n<tr>\n<td><code>SECTION_START</code></td>\n<td>INI section opening</td>\n<td><code>[</code></td>\n<td><code>[</code></td>\n<td>INI</td>\n</tr>\n<tr>\n<td><code>SECTION_END</code></td>\n<td>INI section closing</td>\n<td><code>]</code></td>\n<td><code>]</code></td>\n<td>INI</td>\n</tr>\n<tr>\n<td><code>ARRAY_START</code></td>\n<td>Array opening bracket</td>\n<td><code>[</code></td>\n<td><code>[</code></td>\n<td>TOML, YAML</td>\n</tr>\n<tr>\n<td><code>ARRAY_END</code></td>\n<td>Array closing bracket</td>\n<td><code>]</code></td>\n<td><code>]</code></td>\n<td>TOML, YAML</td>\n</tr>\n<tr>\n<td><code>OBJECT_START</code></td>\n<td>Object opening brace</td>\n<td><code>{</code></td>\n<td><code>{</code></td>\n<td>TOML, YAML</td>\n</tr>\n<tr>\n<td><code>OBJECT_END</code></td>\n<td>Object closing brace</td>\n<td><code>}</code></td>\n<td><code>}</code></td>\n<td>TOML, YAML</td>\n</tr>\n<tr>\n<td><code>COMMA</code></td>\n<td>Element separator</td>\n<td><code>,</code></td>\n<td><code>,</code></td>\n<td>TOML, YAML</td>\n</tr>\n<tr>\n<td><code>DOT</code></td>\n<td>Dotted key separator</td>\n<td><code>.</code></td>\n<td><code>.</code></td>\n<td>TOML</td>\n</tr>\n<tr>\n<td><code>INDENT</code></td>\n<td>Increased indentation level</td>\n<td>(whitespace)</td>\n<td>indent level</td>\n<td>YAML</td>\n</tr>\n<tr>\n<td><code>DEDENT</code></td>\n<td>Decreased indentation level</td>\n<td>(whitespace)</td>\n<td>dedent count</td>\n<td>YAML</td>\n</tr>\n<tr>\n<td><code>BLOCK_SEQUENCE</code></td>\n<td>YAML sequence marker</td>\n<td><code>-</code></td>\n<td><code>-</code></td>\n<td>YAML</td>\n</tr>\n<tr>\n<td><code>INVALID</code></td>\n<td>Malformed or unrecognized token</td>\n<td><code>@#$%</code></td>\n<td>(original text)</td>\n<td>All formats</td>\n</tr>\n</tbody></table>\n<p>The <code>Position</code> structure tracks location information for every token, enabling precise error reporting and source mapping. Position tracking must handle the complexities of different line ending conventions (Unix <code>\\n</code>, Windows <code>\\r\\n</code>, classic Mac <code>\\r</code>) while maintaining accurate column counting in the presence of tab characters and Unicode code points.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>line</code></td>\n<td><code>int</code></td>\n<td>Line number (1-based) in source text</td>\n</tr>\n<tr>\n<td><code>column</code></td>\n<td><code>int</code></td>\n<td>Column number (1-based) in source line</td>\n</tr>\n<tr>\n<td><code>offset</code></td>\n<td><code>int</code></td>\n<td>Absolute character offset (0-based) from start of input</td>\n</tr>\n</tbody></table>\n<p>The <code>Token</code> structure represents individual lexical units with their complete context. The separation between <code>value</code> and <code>raw_text</code> enables clean processing of escape sequences, type conversion, and normalization while preserving the original source text for error reporting and debugging.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>type</code></td>\n<td><code>TokenType</code></td>\n<td>Semantic category of this token</td>\n</tr>\n<tr>\n<td><code>value</code></td>\n<td><code>Any</code></td>\n<td>Processed value after escape handling and type conversion</td>\n</tr>\n<tr>\n<td><code>position</code></td>\n<td><code>Position</code></td>\n<td>Source location where this token originated</td>\n</tr>\n<tr>\n<td><code>raw_text</code></td>\n<td><code>str</code></td>\n<td>Exact character sequence from input (before processing)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The distinction between <code>value</code> and <code>raw_text</code> in tokens is crucial for debugging and error reporting. When a user writes <code>&quot;hello\\nworld&quot;</code> in their configuration file, they intend the string value <code>hello\\nworld</code> (with a literal newline), but error messages should reference the original <code>&quot;hello\\nworld&quot;</code> text they actually typed.</p>\n</blockquote>\n<p><strong>Context sensitivity</strong> presents one of the most significant challenges in tokenization across multiple formats. The same character sequence can represent different token types depending on its syntactic context. For example, the character <code>[</code> represents a section header in INI files, an array literal in TOML expressions, a table array definition in TOML headers, and a flow sequence in YAML. Our tokenizer design must either maintain sufficient context to disambiguate these cases or defer disambiguation to the parsing layer.</p>\n<blockquote>\n<p><strong>Decision: Context-Sensitive vs Context-Free Tokenization</strong></p>\n<ul>\n<li><strong>Context</strong>: Different formats assign different meanings to the same characters depending on syntactic position</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Context-sensitive tokenizer that tracks parsing state and emits different tokens based on context</li>\n<li>Context-free tokenizer that emits generic tokens and lets parsers handle disambiguation</li>\n<li>Format-specific tokenizers with no shared token vocabulary</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Context-free tokenizer with parser-level disambiguation</li>\n<li><strong>Rationale</strong>: Context-free tokenization is simpler to implement, test, and debug. Parser-level disambiguation allows each format parser to apply its own interpretation rules without complex tokenizer state management.</li>\n<li><strong>Consequences</strong>: Parsers must handle some ambiguous tokens, but tokenization logic remains clean and format-agnostic where possible.</li>\n</ul>\n</blockquote>\n<p><strong>Lexical ambiguity</strong> occurs when the same character sequence could represent multiple token types even within a single format. TOML demonstrates this complexity with its multiple string literal syntaxes: basic strings (<code>&quot;hello&quot;</code>), literal strings (<code>&#39;hello&#39;</code>), multi-line basic strings (<code>&quot;&quot;&quot;hello&quot;&quot;&quot;</code>), and multi-line literal strings (<code>&#39;&#39;&#39;hello&#39;&#39;&#39;</code>). Each syntax has different rules for escape sequence processing and line handling.</p>\n<p>String literal tokenization requires careful state machine implementation to handle nested quotes, escape sequences, and multi-line continuation. The tokenizer must track whether it&#39;s inside a quoted string, which quote character initiated the string, whether escape processing is active, and how to handle embedded newlines.</p>\n<table>\n<thead>\n<tr>\n<th>String Type</th>\n<th>Quote Style</th>\n<th>Escape Processing</th>\n<th>Multi-line Support</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TOML Basic</td>\n<td><code>&quot;...&quot;</code></td>\n<td>Yes (backslash escapes)</td>\n<td>Single line only</td>\n<td><code>&quot;hello\\nworld&quot;</code></td>\n</tr>\n<tr>\n<td>TOML Literal</td>\n<td><code>&#39;...&#39;</code></td>\n<td>No (raw text)</td>\n<td>Single line only</td>\n<td><code>&#39;C:\\Users\\name&#39;</code></td>\n</tr>\n<tr>\n<td>TOML Multi-Basic</td>\n<td><code>&quot;&quot;&quot;...&quot;&quot;&quot;</code></td>\n<td>Yes (backslash escapes)</td>\n<td>Yes (preserve newlines)</td>\n<td><code>&quot;&quot;&quot;line 1\\nline 2&quot;&quot;&quot;</code></td>\n</tr>\n<tr>\n<td>TOML Multi-Literal</td>\n<td><code>&#39;&#39;&#39;...&#39;&#39;&#39;</code></td>\n<td>No (raw text)</td>\n<td>Yes (preserve newlines)</td>\n<td><code>&#39;&#39;&#39;raw\\text\\here&#39;&#39;&#39;</code></td>\n</tr>\n<tr>\n<td>YAML Double</td>\n<td><code>&quot;...&quot;</code></td>\n<td>Yes (limited escape set)</td>\n<td>With continuation</td>\n<td><code>&quot;folded\\nstring&quot;</code></td>\n</tr>\n<tr>\n<td>YAML Single</td>\n<td><code>&#39;...&#39;</code></td>\n<td>Limited (only <code>&#39;&#39;</code> → <code>&#39;</code>)</td>\n<td>With continuation</td>\n<td><code>&#39;don&#39;&#39;t escape&#39;</code></td>\n</tr>\n<tr>\n<td>INI Quoted</td>\n<td><code>&quot;...&quot;</code> or <code>&#39;...&#39;</code></td>\n<td>Format-dependent</td>\n<td>Usually single line</td>\n<td><code>&quot;value with spaces&quot;</code></td>\n</tr>\n</tbody></table>\n<p><strong>Type inference</strong> during tokenization attempts to classify numeric and boolean literals without requiring explicit type annotations. This automatic classification simplifies the user experience but introduces complexity in handling edge cases and format-specific type systems.</p>\n<p>Numeric type inference must distinguish between integers, floating-point numbers, and special numeric formats like TOML&#39;s integer underscores (<code>1_000_000</code>) or YAML&#39;s sexagesimal numbers (<code>90:30:15</code>). Boolean inference varies significantly between formats - YAML recognizes <code>yes</code>, <code>no</code>, <code>on</code>, <code>off</code>, <code>true</code>, and <code>false</code>, while TOML only accepts <code>true</code> and <code>false</code>.</p>\n<blockquote>\n<p><strong>Decision: Tokenization-Level vs Parse-Level Type Inference</strong></p>\n<ul>\n<li><strong>Context</strong>: Determining whether <code>&quot;42&quot;</code>, <code>42</code>, <code>true</code>, and <code>&quot;true&quot;</code> represent different data types</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Tokenizer performs type inference and emits strongly-typed tokens</li>\n<li>Tokenizer emits raw values and parsers handle type inference</li>\n<li>No type inference - everything remains as strings until explicit conversion</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Tokenizer performs basic type inference for clearly-typed literals</li>\n<li><strong>Rationale</strong>: Type information at the token level simplifies parser logic and enables better error messages. Ambiguous cases can still be deferred to parsers.</li>\n<li><strong>Consequences</strong>: Tokenizer becomes slightly more complex but parsers can focus on structural analysis rather than type classification.</li>\n</ul>\n</blockquote>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fdata-structure-mapping.svg\" alt=\"Data Structure Transformation\"></p>\n<h3 id=\"parse-tree-structure\">Parse Tree Structure</h3>\n<p>The parse tree provides an intermediate structural representation that captures the hierarchical relationships between tokens while remaining faithful to each format&#39;s grammatical structure. Think of the parse tree as a <strong>syntax-aware scaffolding</strong> that organizes tokens according to their grammatical roles before transforming them into the final unified data structure.</p>\n<p>Parse trees serve several critical functions in our architecture. They provide a stable interface between format-specific parsing logic and the unified output generation. They enable sophisticated error recovery by maintaining partial structural information even when parsing fails. They support advanced features like preserving comments, maintaining key ordering, or implementing syntax-aware editing operations.</p>\n<p>The parse tree structure must accommodate the diverse grammatical approaches of our target formats while providing sufficient commonality for shared processing logic. INI files have a simple two-level hierarchy of sections containing key-value pairs. TOML supports arbitrary nesting through dotted keys and table definitions, with complex rules for table redefinition and array-of-tables. YAML uses indentation-sensitive block structures with implicit type inference and multiple syntactic styles for the same logical constructs.</p>\n<blockquote>\n<p><strong>Decision: Format-Specific vs Unified Parse Tree Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: Balancing the need for format-faithful representation with shared processing logic</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single unified parse tree structure that all formats must map to</li>\n<li>Format-specific parse tree structures with conversion to unified output</li>\n<li>Hybrid approach with common base structures and format-specific extensions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach with common base node types and format-specific specializations</li>\n<li><strong>Rationale</strong>: Enables shared processing logic for common constructs while allowing format-specific optimizations and features</li>\n<li><strong>Consequences</strong>: Moderate complexity increase but maximum flexibility for format-specific requirements</li>\n</ul>\n</blockquote>\n<p>The base parse tree node structure provides common functionality needed across all formats. Every node tracks its source position for error reporting, maintains references to its constituent tokens for source mapping, and provides a consistent interface for tree traversal and manipulation.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>node_type</code></td>\n<td><code>str</code></td>\n<td>Discriminator indicating specific node subtype</td>\n</tr>\n<tr>\n<td><code>position</code></td>\n<td><code>Position</code></td>\n<td>Source location of this syntactic construct</td>\n</tr>\n<tr>\n<td><code>tokens</code></td>\n<td><code>List[Token]</code></td>\n<td>All tokens that contribute to this node</td>\n</tr>\n<tr>\n<td><code>children</code></td>\n<td><code>List[ParseNode]</code></td>\n<td>Child nodes in syntax tree</td>\n</tr>\n<tr>\n<td><code>metadata</code></td>\n<td><code>dict</code></td>\n<td>Format-specific annotations and processing hints</td>\n</tr>\n</tbody></table>\n<p><strong>Section nodes</strong> represent logical groupings of configuration data. In INI files, sections correspond directly to <code>[section_name]</code> headers. In TOML, sections represent table definitions including nested tables from dotted keys and array-of-tables. YAML doesn&#39;t have explicit sections, but top-level mapping keys function similarly.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>section_name</code></td>\n<td><code>str</code></td>\n<td>Fully-qualified section identifier</td>\n</tr>\n<tr>\n<td><code>key_path</code></td>\n<td><code>List[str]</code></td>\n<td>Hierarchical path components for nested sections</td>\n</tr>\n<tr>\n<td><code>is_array_element</code></td>\n<td><code>bool</code></td>\n<td>True if this section represents an array-of-tables entry</td>\n</tr>\n<tr>\n<td><code>key_value_pairs</code></td>\n<td><code>List[KeyValueNode]</code></td>\n<td>Direct key-value assignments within this section</td>\n</tr>\n</tbody></table>\n<p><strong>Key-value nodes</strong> represent individual configuration assignments. These nodes must handle the variety of assignment operators (<code>=</code>, <code>:</code>), value types, and structural patterns used across formats.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>key</code></td>\n<td><code>str</code></td>\n<td>Configuration parameter name</td>\n</tr>\n<tr>\n<td><code>key_path</code></td>\n<td><code>List[str]</code></td>\n<td>Dotted key components for nested assignment</td>\n</tr>\n<tr>\n<td><code>value</code></td>\n<td><code>ValueNode</code></td>\n<td>Assigned value (scalar, array, or nested structure)</td>\n</tr>\n<tr>\n<td><code>assignment_operator</code></td>\n<td><code>str</code></td>\n<td>Operator used for assignment (<code>=</code>, <code>:</code>)</td>\n</tr>\n<tr>\n<td><code>inline_comment</code></td>\n<td><code>Optional[str]</code></td>\n<td>Comment appearing on same line as assignment</td>\n</tr>\n</tbody></table>\n<p><strong>Value nodes</strong> represent the diverse data types and structures supported across configuration formats. Value nodes must accommodate scalars (strings, numbers, booleans), collections (arrays, objects), and format-specific constructs like TOML inline tables or YAML flow sequences.</p>\n<table>\n<thead>\n<tr>\n<th>Value Node Type</th>\n<th>Description</th>\n<th>Example Source</th>\n<th>Parsed Structure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ScalarValue</code></td>\n<td>Single atomic value</td>\n<td><code>hostname = &quot;server1&quot;</code></td>\n<td><code>{&quot;type&quot;: &quot;scalar&quot;, &quot;value&quot;: &quot;server1&quot;}</code></td>\n</tr>\n<tr>\n<td><code>ArrayValue</code></td>\n<td>Ordered list of values</td>\n<td><code>ports = [80, 443, 8080]</code></td>\n<td><code>{&quot;type&quot;: &quot;array&quot;, &quot;elements&quot;: [80, 443, 8080]}</code></td>\n</tr>\n<tr>\n<td><code>ObjectValue</code></td>\n<td>Key-value mapping</td>\n<td><code>{name = &quot;test&quot;, port = 80}</code></td>\n<td><code>{&quot;type&quot;: &quot;object&quot;, &quot;fields&quot;: {...}}</code></td>\n</tr>\n<tr>\n<td><code>NestedValue</code></td>\n<td>Reference to nested section</td>\n<td><code>database.connection.host</code></td>\n<td><code>{&quot;type&quot;: &quot;nested&quot;, &quot;path&quot;: [&quot;database&quot;, &quot;connection&quot;, &quot;host&quot;]}</code></td>\n</tr>\n</tbody></table>\n<p><strong>Comment nodes</strong> preserve documentation and annotations from the source configuration. Comment handling varies significantly between formats - INI and TOML support both line comments and inline comments, while YAML has more complex comment association rules.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>comment_text</code></td>\n<td><code>str</code></td>\n<td>Comment content (without comment markers)</td>\n</tr>\n<tr>\n<td><code>comment_style</code></td>\n<td><code>str</code></td>\n<td>Comment syntax used (<code>#</code>, <code>;</code>, <code>//</code>)</td>\n</tr>\n<tr>\n<td><code>attachment</code></td>\n<td><code>str</code></td>\n<td>How comment relates to nearby content (<code>above</code>, <code>inline</code>, <code>below</code>)</td>\n</tr>\n<tr>\n<td><code>associated_node</code></td>\n<td><code>Optional[ParseNode]</code></td>\n<td>Parse node this comment documents</td>\n</tr>\n</tbody></table>\n<p><strong>Array-of-tables nodes</strong> handle TOML&#39;s unique <code>[[table.name]]</code> syntax for creating arrays of structured objects. This construct has no direct equivalent in INI or YAML, requiring specialized handling in the parse tree.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>table_path</code></td>\n<td><code>List[str]</code></td>\n<td>Hierarchical path to the array location</td>\n</tr>\n<tr>\n<td><code>entries</code></td>\n<td><code>List[SectionNode]</code></td>\n<td>Individual table entries in the array</td>\n</tr>\n<tr>\n<td><code>entry_order</code></td>\n<td><code>List[int]</code></td>\n<td>Explicit ordering for array elements</td>\n</tr>\n</tbody></table>\n<p>The parse tree construction process varies significantly between formats due to their different grammatical approaches. <strong>INI parsing</strong> follows a simple line-oriented algorithm: scan for section headers, collect key-value pairs until the next section, handle comments and continuation lines. <strong>TOML parsing</strong> requires recursive descent techniques to handle nested table definitions, dotted key expansion, and the complex interaction between explicit tables and implicit tables created by dotted keys. <strong>YAML parsing</strong> uses an indentation-sensitive algorithm with a stack-based approach to track nesting levels and handle the transition between different indentation depths.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Parse trees serve as a crucial debugging tool during implementation. When parsing produces incorrect output, examining the parse tree structure often reveals whether the problem lies in tokenization (wrong tokens), parsing logic (incorrect tree structure), or output conversion (correct tree, wrong transformation).</p>\n</blockquote>\n<h3 id=\"unified-output-format\">Unified Output Format</h3>\n<p>The unified output format provides a consistent nested dictionary structure that client applications can work with regardless of which configuration format was originally parsed. Think of the unified output as a <strong>format-agnostic data contract</strong> - it abstracts away the syntactic differences between INI, TOML, and YAML while preserving the semantic intent of the original configuration.</p>\n<p>The unified format addresses one of the core value propositions of our multi-format parser: enabling applications to support multiple configuration formats without implementing format-specific processing logic. A web application can read database connection parameters from <code>config.ini</code>, <code>config.toml</code>, or <code>config.yaml</code> using identical code, with the parser handling the format detection and conversion automatically.</p>\n<p>The output structure consists of nested Python dictionaries with string keys and mixed-type values. This representation aligns with JSON semantics while supporting additional data types like datetime objects, large integers, and explicit type annotations that some configuration formats provide.</p>\n<blockquote>\n<p><strong>Decision: Output Data Structure Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Choosing the unified representation that balances simplicity, type safety, and format compatibility</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pure JSON-compatible structure (strings, numbers, booleans, arrays, objects only)</li>\n<li>Python-native structure with full type system support (datetime, decimal, custom types)</li>\n<li>Hybrid structure with JSON compatibility and optional type annotations</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Python-native structure with rich type support</li>\n<li><strong>Rationale</strong>: Configuration files often contain dates, file paths, and numeric values that benefit from proper typing. Applications can serialize to JSON if needed.</li>\n<li><strong>Consequences</strong>: Output is more useful for Python applications but requires explicit conversion for JSON export.</li>\n</ul>\n</blockquote>\n<p><strong>Nested structure mapping</strong> transforms the various hierarchical representations used by different formats into a consistent nested dictionary structure. INI sections become top-level dictionary keys, TOML tables and dotted keys create nested dictionaries, and YAML block structures map directly to nested dictionaries and arrays.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Source</th>\n<th>Unified Output Structure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI: <code>[database]</code> <code>host=localhost</code></td>\n<td><code>{&quot;database&quot;: {&quot;host&quot;: &quot;localhost&quot;}}</code></td>\n</tr>\n<tr>\n<td>TOML: <code>database.host = &quot;localhost&quot;</code></td>\n<td><code>{&quot;database&quot;: {&quot;host&quot;: &quot;localhost&quot;}}</code></td>\n</tr>\n<tr>\n<td>YAML: <code>database:</code> <code>  host: localhost</code></td>\n<td><code>{&quot;database&quot;: {&quot;host&quot;: &quot;localhost&quot;}}</code></td>\n</tr>\n</tbody></table>\n<p><strong>Type preservation</strong> maintains the semantic intent of typed literals while providing consistent behavior across formats. Numbers remain as integers or floats, booleans are represented as Python <code>True</code>/<code>False</code>, dates and times use Python <code>datetime</code> objects, and strings preserve their exact content including Unicode characters.</p>\n<p>The type mapping process must handle format-specific type systems while producing consistent output. TOML has an explicit type system with integers, floats, booleans, strings, arrays, and tables. YAML performs implicit type inference that can produce surprising results (<code>yes</code> becomes <code>True</code>, <code>1.0</code> becomes a float, <code>010</code> might become octal). INI files typically treat everything as strings unless explicit conversion is applied.</p>\n<table>\n<thead>\n<tr>\n<th>Source Format</th>\n<th>Type System</th>\n<th>Example Literal</th>\n<th>Unified Output Type</th>\n<th>Unified Output Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TOML</td>\n<td>Explicit typing</td>\n<td><code>port = 8080</code></td>\n<td><code>int</code></td>\n<td><code>8080</code></td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Implicit inference</td>\n<td><code>port: 8080</code></td>\n<td><code>int</code></td>\n<td><code>8080</code></td>\n</tr>\n<tr>\n<td>INI</td>\n<td>String-based</td>\n<td><code>port=8080</code></td>\n<td><code>str</code> or <code>int</code></td>\n<td><code>&quot;8080&quot;</code> or <code>8080</code></td>\n</tr>\n<tr>\n<td>TOML</td>\n<td>Boolean literals</td>\n<td><code>enabled = true</code></td>\n<td><code>bool</code></td>\n<td><code>True</code></td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Boolean inference</td>\n<td><code>enabled: yes</code></td>\n<td><code>bool</code></td>\n<td><code>True</code></td>\n</tr>\n<tr>\n<td>INI</td>\n<td>String representation</td>\n<td><code>enabled=true</code></td>\n<td><code>str</code> or <code>bool</code></td>\n<td><code>&quot;true&quot;</code> or <code>True</code></td>\n</tr>\n</tbody></table>\n<p><strong>Array handling</strong> unifies the different approaches formats use for representing ordered collections. TOML uses explicit array syntax with square brackets: <code>ports = [80, 443, 8080]</code>. YAML supports both flow syntax <code>[80, 443, 8080]</code> and block syntax with dash-prefixed items. INI files don&#39;t have native array support, but our parser can recognize comma-separated values or repeated keys as arrays.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Array Syntax</th>\n<th>Source Example</th>\n<th>Unified Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TOML</td>\n<td>Square brackets</td>\n<td><code>ports = [80, 443]</code></td>\n<td><code>{&quot;ports&quot;: [80, 443]}</code></td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Flow sequence</td>\n<td><code>ports: [80, 443]</code></td>\n<td><code>{&quot;ports&quot;: [80, 443]}</code></td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Block sequence</td>\n<td><code>ports:</code> <code>- 80</code> <code>- 443</code></td>\n<td><code>{&quot;ports&quot;: [80, 443]}</code></td>\n</tr>\n<tr>\n<td>INI</td>\n<td>Comma-separated</td>\n<td><code>ports=80,443</code></td>\n<td><code>{&quot;ports&quot;: [80, 443]}</code></td>\n</tr>\n<tr>\n<td>INI</td>\n<td>Repeated keys</td>\n<td><code>port=80</code> <code>port=443</code></td>\n<td><code>{&quot;ports&quot;: [80, 443]}</code></td>\n</tr>\n</tbody></table>\n<p><strong>Object nesting</strong> creates hierarchical dictionary structures from the various nesting mechanisms used by different formats. The unified output uses string keys and supports arbitrary nesting depth limited only by Python&#39;s recursion limits.</p>\n<p>TOML dotted keys like <code>database.connection.host = &quot;localhost&quot;</code> create nested dictionaries: <code>{&quot;database&quot;: {&quot;connection&quot;: {&quot;host&quot;: &quot;localhost&quot;}}}</code>. YAML indentation-based nesting maps directly to nested dictionaries. INI section names can be interpreted as namespace separators, so <code>[database.connection]</code> creates similar nesting.</p>\n<p><strong>Key normalization</strong> handles differences in identifier conventions between formats. Some formats are case-sensitive while others are case-insensitive. Key names might use different conventions (camelCase, snake_case, kebab-case) even within the same configuration file.</p>\n<table>\n<thead>\n<tr>\n<th>Normalization Strategy</th>\n<th>Description</th>\n<th>Example Transformation</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>preserve</code></td>\n<td>Maintain exact key names from source</td>\n<td><code>userName</code> → <code>userName</code></td>\n<td>Format-aware applications</td>\n</tr>\n<tr>\n<td><code>lowercase</code></td>\n<td>Convert all keys to lowercase</td>\n<td><code>userName</code> → <code>username</code></td>\n<td>Case-insensitive lookup</td>\n</tr>\n<tr>\n<td><code>snake_case</code></td>\n<td>Convert to underscore convention</td>\n<td><code>userName</code> → <code>user_name</code></td>\n<td>Python applications</td>\n</tr>\n<tr>\n<td><code>kebab_case</code></td>\n<td>Convert to dash convention</td>\n<td><code>userName</code> → <code>user-name</code></td>\n<td>Configuration standards</td>\n</tr>\n</tbody></table>\n<p><strong>Error information preservation</strong> maintains connection between the unified output and the original source text for debugging and error reporting. When possible, the unified output includes metadata about source locations, original formatting, and any type conversions that were applied.</p>\n<p>The output structure supports optional metadata attachment through a special <code>__metadata__</code> key that contains position information, original formatting, comments, and conversion notes. This metadata enables advanced features like round-trip conversion, syntax-aware editing, and precise error reporting.</p>\n<table>\n<thead>\n<tr>\n<th>Metadata Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>source_format</code></td>\n<td><code>str</code></td>\n<td>Original configuration format</td>\n<td><code>&quot;toml&quot;</code>, <code>&quot;yaml&quot;</code>, <code>&quot;ini&quot;</code></td>\n</tr>\n<tr>\n<td><code>source_position</code></td>\n<td><code>Position</code></td>\n<td>Location in original file</td>\n<td><code>{line: 15, column: 8}</code></td>\n</tr>\n<tr>\n<td><code>original_key</code></td>\n<td><code>str</code></td>\n<td>Key name before normalization</td>\n<td><code>&quot;userName&quot;</code> → normalized <code>&quot;user_name&quot;</code></td>\n</tr>\n<tr>\n<td><code>type_conversion</code></td>\n<td><code>str</code></td>\n<td>Applied type conversion</td>\n<td><code>&quot;string_to_int&quot;</code>, <code>&quot;implicit_boolean&quot;</code></td>\n</tr>\n<tr>\n<td><code>comments</code></td>\n<td><code>List[str]</code></td>\n<td>Associated comments</td>\n<td><code>[&quot;# Database configuration&quot;, &quot;# Updated 2024-01-15&quot;]</code></td>\n</tr>\n</tbody></table>\n<p>The unified output format serves as the foundation for advanced features like configuration validation, schema enforcement, and cross-format conversion. Applications can implement configuration schemas that work across multiple input formats, validate required fields and value ranges, and provide meaningful error messages that reference the original source text.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data model implementation requires careful attention to type safety, memory efficiency, and extensibility. Python&#39;s dynamic typing system provides flexibility for handling mixed-type configuration values while its dataclass and enum features enable clean, self-documenting data structures.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Token Types</td>\n<td>Python Enum</td>\n<td>Custom classes with validation</td>\n</tr>\n<tr>\n<td>Position Tracking</td>\n<td>Named tuple</td>\n<td>Dataclass with methods</td>\n</tr>\n<tr>\n<td>Parse Trees</td>\n<td>Dictionary-based</td>\n<td>Custom node classes with inheritance</td>\n</tr>\n<tr>\n<td>Type Conversion</td>\n<td>Built-in functions</td>\n<td>Custom type system with validation</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception classes</td>\n<td>Rich error objects with context</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config_parser/\n  data_model/\n    __init__.py              ← export main classes\n    tokens.py                ← TokenType, Token, Position\n    parse_tree.py            ← Parse tree node classes\n    output.py                ← Unified output utilities\n    errors.py                ← Error classes and context\n    types.py                 ← Type conversion utilities\n  tests/\n    test_tokens.py           ← Token handling tests\n    test_parse_tree.py       ← Parse tree construction tests\n    test_output.py           ← Output format tests</code></pre></div>\n\n<p><strong>Core Token Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional, List, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token types for all supported configuration formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NEWLINE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BLOCK_SEQUENCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVALID</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Source position for error reporting and debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">          # 1-based line number</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">        # 1-based column number  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">        # 0-based absolute offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">frozen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual lexical unit with complete context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType    </span><span style=\"color:#6A737D\"># Semantic category</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Any         </span><span style=\"color:#6A737D\"># Processed value (after escapes, conversion)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Position </span><span style=\"color:#6A737D\"># Source location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">      # Original character sequence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.value</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EOF_MARKER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">'</span></span></code></pre></div>\n\n<p><strong>Parse Tree Node Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parse tree nodes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens: List[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    children: List[</span><span style=\"color:#9ECBFF\">'ParseNode'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert this node to unified output format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SectionNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration section with key-value pairs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    section_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_array_element: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key_value_pairs: List[</span><span style=\"color:#9ECBFF\">'KeyValueNode'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.node_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"section\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> KeyValueNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual configuration assignment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Optional[</span><span style=\"color:#9ECBFF\">'ValueNode'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assignment_operator: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"=\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inline_comment: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.node_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"key_value\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValueNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration value of various types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"scalar\"</span><span style=\"color:#6A737D\">  # scalar, array, object, nested</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processed_value: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.node_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"value\"</span></span></code></pre></div>\n\n<p><strong>Error Handling Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parsing errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggestion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.format_error())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> format_error</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Format error with position and suggestion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suggestion:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">Suggestion: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.suggestion</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error in tokenization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error in parsing phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error in semantic analysis phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_error_context</span><span style=\"color:#E1E4E8\">(source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Position, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate visual error context showing problematic code.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.split(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, position.line </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> context_lines </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(lines), position.line </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> context_lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(start_line, end_line):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line_num </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        marker </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \">>> \"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"    \"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">marker</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">line_num</span><span style=\"color:#F97583\">:3d</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> | </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">lines[i]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add column pointer for error line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pointer </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" \"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> position.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"^\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            context.append(pointer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(context)</span></span></code></pre></div>\n\n<p><strong>Core Tokenizer Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base tokenizer for all configuration formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens: List[Token] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_position</span><span style=\"color:#E1E4E8\">(self) -> Position:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current parsing position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return Position object with current line, column, offset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look ahead without consuming characters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return character at position + offset, or EOF_MARKER if beyond end</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle bounds checking gracefully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume current character and update position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Get current character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update position counter (handle \\n for line tracking)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update column counter (reset on newline, increment otherwise)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return consumed character</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Skip non-semantic whitespace.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Advance through spaces and tabs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Preserve newlines for formats where they're significant</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle different line ending conventions (\\n, \\r\\n, \\r)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse quoted string with escape sequences.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Track starting position for token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect characters until matching quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle escape sequences (\\n, \\t, \\\", \\\\, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle unterminated string error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return STRING token with processed value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse numeric literal (integer or float).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Track starting position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect digits, handle decimal points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle scientific notation (1e10, 2.5E-3)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle format-specific features (TOML underscores: 1_000)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert to appropriate Python type (int or float)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return NUMBER token with converted value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main tokenization entry point.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize token list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Loop through source characters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify token boundaries and types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle format-specific token patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add EOF token at end</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return complete token list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Output Generation Utilities:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> normalize_key</span><span style=\"color:#E1E4E8\">(key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"preserve\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Normalize key names according to specified strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement preserve (no change)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement lowercase conversion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement snake_case conversion (camelCase -> snake_case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement kebab-case conversion (camelCase -> kebab-case)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> merge_nested_dicts</span><span style=\"color:#E1E4E8\">(dict1: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], dict2: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Deep merge two nested dictionaries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle overlapping keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recursively merge nested dictionaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle array values (append vs replace)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Preserve type information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> convert_parse_tree_to_dict</span><span style=\"color:#E1E4E8\">(root: ParseNode) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Convert parse tree to unified output format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Traverse parse tree depth-first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert section nodes to nested dictionaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert key-value nodes to dictionary entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle array-of-tables specially</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply type conversions and normalizations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>dataclasses</code> for clean, self-documenting data structures with automatic <code>__init__</code>, <code>__repr__</code>, and equality methods</li>\n<li>Leverage <code>typing</code> module for precise type annotations that improve IDE support and catch bugs early</li>\n<li>Use <code>enum.auto()</code> for token types to avoid manual value assignment and reduce maintenance</li>\n<li>Consider <code>functools.lru_cache</code> for expensive operations like regex compilation in tokenizers</li>\n<li>Use <code>collections.defaultdict</code> for building nested structures incrementally during parsing</li>\n</ul>\n<p><strong>Milestone Checkpoint:</strong>\nAfter implementing the data model components, verify correct behavior:</p>\n<ol>\n<li><strong>Token Creation</strong>: Create tokens of each type and verify the <code>__str__</code> representation shows useful information</li>\n<li><strong>Position Tracking</strong>: Create positions and verify they format correctly for error messages  </li>\n<li><strong>Parse Tree Construction</strong>: Build a simple parse tree manually and verify <code>to_dict()</code> conversion works</li>\n<li><strong>Error Context</strong>: Generate error context for a sample configuration file and verify the visual formatting</li>\n<li><strong>Type Conversions</strong>: Test numeric and boolean conversion with various input formats</li>\n</ol>\n<p>Run: <code>python -m pytest tests/test_data_model.py -v</code> to verify all data model components work correctly.</p>\n<p><strong>Common Implementation Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Mutable Default Arguments in Dataclasses</strong>\nUsing mutable defaults like <code>tokens: List[Token] = []</code> causes all instances to share the same list. Use <code>field(default_factory=list)</code> instead.</p>\n<p>⚠️ <strong>Pitfall: Position Tracking with Unicode</strong>\nCounting characters as single units breaks with multi-byte Unicode. Use <code>len(text.encode(&#39;utf-8&#39;))</code> for byte offsets if needed, or stick to character-based counting for simplicity.</p>\n<p>⚠️ <strong>Pitfall: Token Value vs Raw Text Confusion</strong> \nMixing up processed values and raw text leads to double-escaping or incorrect error messages. Always use <code>raw_text</code> for error reporting and <code>value</code> for semantic processing.</p>\n<p>⚠️ <strong>Pitfall: Deep Copy Issues with Parse Trees</strong>\nParse tree nodes contain references to other nodes, making deep copying complex. Implement custom <code>copy</code> methods or use immutable structures where possible.</p>\n<p>⚠️ <strong>Pitfall: Enum Comparison Mistakes</strong>\nComparing <code>TokenType.STRING == &quot;STRING&quot;</code> fails because enums don&#39;t equal their string representations. Use <code>token.type == TokenType.STRING</code> or <code>token.type.name == &quot;STRING&quot;</code>.</p>\n<h2 id=\"tokenizer-component-design\">Tokenizer Component Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> TOML Tokenizer, with foundational concepts supporting INI Parser and YAML Subset Parser</p>\n</blockquote>\n<p>The tokenizer serves as the lexical analysis engine that transforms raw character streams into meaningful typed tokens with precise position tracking. Think of the tokenizer as a sophisticated pattern recognition system that must simultaneously solve three fundamental challenges: identifying where meaningful units begin and end in continuous text, classifying what type of semantic meaning each unit carries, and maintaining perfect position tracking for error reporting. Unlike simple string splitting, tokenization must handle <strong>lexical ambiguity</strong> where the same character sequence can mean completely different things depending on context—a quote character might start a string literal, escape another quote, or appear as literal content within a different quoting style.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Ftoken-types.svg\" alt=\"Token Type Hierarchy\"></p>\n<p>The tokenizer operates as a <strong>context-sensitive</strong> state machine that maintains awareness of its current parsing context to resolve these ambiguities. When the tokenizer encounters a quote character, its behavior depends entirely on whether it&#39;s currently inside a string literal, what type of string context it&#39;s in, and what escape rules apply. This context sensitivity becomes particularly complex when handling the different string literal syntaxes across INI, TOML, and YAML formats, where the same character sequence <code>&quot;&quot;&quot;</code> might indicate a multiline string start in TOML but could be three separate quoted empty strings in INI context.</p>\n<p>The tokenizer must also solve the <strong>impedance mismatch</strong> between human-readable configuration syntax and machine-processable token streams. Configuration formats are designed for human readability, using intuitive conventions like indentation for nesting (YAML) or natural key=value syntax (INI). However, parsers need discrete, classified tokens with explicit type information and precise boundary definitions. The tokenizer bridges this gap by applying format-specific lexical rules that preserve the semantic intent while creating the structured token stream that downstream parsers require.</p>\n<h3 id=\"tokenization-mental-model\">Tokenization Mental Model</h3>\n<p>Understanding tokenization requires thinking about it as <strong>pattern recognition with state tracking</strong> rather than simple character classification. Imagine the tokenizer as a careful reader who must simultaneously identify word boundaries, understand context-dependent meanings, and take detailed notes about location and classification for later reference.</p>\n<p>The mental model begins with the concept of a <strong>scanning window</strong> that moves through the character stream one position at a time. At each position, the tokenizer must decide whether the current character continues an existing token, starts a new token, or serves as a delimiter that separates tokens. This decision requires examining not just the current character, but also the <strong>parsing context</strong> maintained in the tokenizer&#39;s state machine. The context tracks information like &quot;currently inside a quoted string,&quot; &quot;at the start of a line,&quot; or &quot;within a comment block.&quot;</p>\n<p>Consider how a human reader processes the TOML line <code>name = &quot;John \\&quot;Doe\\&quot;&quot;</code>. The reader automatically recognizes that the first quote starts a string, the backslash-quote sequence represents an escaped quote within the string content, and the final quote ends the string. The tokenizer must replicate this contextual understanding through explicit state tracking. When it encounters the first quote, it transitions to &quot;inside string literal&quot; state. When it sees the backslash, it transitions to &quot;processing escape sequence&quot; state, reads the escaped quote as literal content, returns to string literal state, and finally recognizes the closing quote as a string terminator.</p>\n<p>The <strong>lookahead</strong> concept provides another essential mental model component. The tokenizer often needs to examine upcoming characters to make correct tokenization decisions. When processing the sequence <code>123.456e-7</code>, the tokenizer must look ahead to determine whether this represents a single floating-point number token or multiple separate tokens. The decimal point alone isn&#39;t sufficient—it must examine the subsequent characters to distinguish between a number continuation and a separate identifier that might follow.</p>\n<p><strong>Position tracking</strong> requires thinking of the tokenizer as maintaining a detailed location journal. Every token must carry precise position information indicating exactly where it appeared in the source text. This information becomes critical for error reporting, IDE integration, and debugging tools. The position tracking must handle format-specific complications like YAML&#39;s significant indentation (where column position affects semantic meaning) and multiline string literals (where internal newlines don&#39;t advance the logical line number for subsequent tokens).</p>\n<p>The <strong>error recovery</strong> mental model treats tokenization errors as opportunities to provide helpful feedback rather than immediate failures. When the tokenizer encounters malformed input like an unterminated string literal, it should attempt to identify the likely intended structure, create appropriate error tokens, and continue processing to find additional issues. Think of this as a careful proofreader who marks errors but continues reading to provide comprehensive feedback rather than stopping at the first mistake.</p>\n<h3 id=\"tokenizer-interface-design\">Tokenizer Interface Design</h3>\n<p>The tokenizer interface must provide a clean abstraction that supports both streaming tokenization (processing tokens one at a time) and batch tokenization (processing entire files), while maintaining consistent error handling and position tracking across all supported formats.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>current_position</code></td>\n<td>none</td>\n<td><code>Position</code></td>\n<td>Returns current parsing position with line, column, and byte offset</td>\n</tr>\n<tr>\n<td><code>peek</code></td>\n<td><code>offset=0</code></td>\n<td><code>str</code></td>\n<td>Look ahead at character without consuming, returns EOF_MARKER at end</td>\n</tr>\n<tr>\n<td><code>advance</code></td>\n<td>none</td>\n<td><code>str</code></td>\n<td>Consume current character, update position tracking, return consumed character</td>\n</tr>\n<tr>\n<td><code>tokenize</code></td>\n<td>none</td>\n<td><code>List[Token]</code></td>\n<td>Main entry point that processes entire input and returns complete token list</td>\n</tr>\n<tr>\n<td><code>next_token</code></td>\n<td>none</td>\n<td><code>Token</code></td>\n<td>Generate and return next token from current position, advance past it</td>\n</tr>\n<tr>\n<td><code>skip_whitespace</code></td>\n<td>none</td>\n<td><code>None</code></td>\n<td>Advance past all non-semantic whitespace characters</td>\n</tr>\n<tr>\n<td><code>read_string_literal</code></td>\n<td><code>quote_char: str</code></td>\n<td><code>Token</code></td>\n<td>Parse quoted string with escape sequence processing</td>\n</tr>\n<tr>\n<td><code>read_number</code></td>\n<td>none</td>\n<td><code>Token</code></td>\n<td>Parse numeric literal with type inference (int/float)</td>\n</tr>\n<tr>\n<td><code>read_identifier</code></td>\n<td>none</td>\n<td><code>Token</code></td>\n<td>Parse unquoted identifier or keyword</td>\n</tr>\n<tr>\n<td><code>read_comment</code></td>\n<td><code>comment_char: str</code></td>\n<td><code>Token</code></td>\n<td>Parse comment from delimiter to end of line</td>\n</tr>\n<tr>\n<td><code>create_error_token</code></td>\n<td><code>message: str, raw_text: str</code></td>\n<td><code>Token</code></td>\n<td>Create INVALID token with error information</td>\n</tr>\n<tr>\n<td><code>is_at_end</code></td>\n<td>none</td>\n<td><code>bool</code></td>\n<td>Check if tokenizer has reached end of input</td>\n</tr>\n</tbody></table>\n<p>The interface design separates concerns between <strong>navigation</strong> methods (<code>peek</code>, <code>advance</code>, <code>current_position</code>) that handle character stream management and <strong>recognition</strong> methods (<code>read_string_literal</code>, <code>read_number</code>) that identify and extract specific token types. This separation allows the recognition methods to focus on format-specific parsing logic while relying on consistent navigation behavior.</p>\n<blockquote>\n<p><strong>Decision: Lookahead Interface Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Different formats require varying amounts of lookahead for tokenization decisions</li>\n<li><strong>Options Considered</strong>: Fixed single-character lookahead, unlimited string lookahead, parameterized offset lookahead</li>\n<li><strong>Decision</strong>: Parameterized offset lookahead with <code>peek(offset=0)</code> method</li>\n<li><strong>Rationale</strong>: Provides flexibility for complex tokenization while maintaining predictable performance characteristics</li>\n<li><strong>Consequences</strong>: Enables proper handling of complex literals like scientific notation while avoiding unbounded memory usage</li>\n</ul>\n</blockquote>\n<p>The <code>BaseTokenizer</code> maintains internal state that supports both the navigation and recognition interfaces:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>source</code></td>\n<td><code>str</code></td>\n<td>Complete input text being tokenized</td>\n</tr>\n<tr>\n<td><code>position</code></td>\n<td><code>int</code></td>\n<td>Current byte offset in source text</td>\n</tr>\n<tr>\n<td><code>line</code></td>\n<td><code>int</code></td>\n<td>Current line number (1-based) for error reporting</td>\n</tr>\n<tr>\n<td><code>column</code></td>\n<td><code>int</code></td>\n<td>Current column number (1-based) for error reporting</td>\n</tr>\n<tr>\n<td><code>tokens</code></td>\n<td><code>List[Token]</code></td>\n<td>Accumulated tokens from tokenization process</td>\n</tr>\n</tbody></table>\n<p>The tokenizer interface supports <strong>streaming processing</strong> through the <code>next_token</code> method, allowing parsers to process tokens incrementally without loading the complete token list into memory. This approach becomes important for large configuration files or embedded systems with memory constraints. The streaming interface maintains the same error handling and position tracking guarantees as batch tokenization.</p>\n<p><strong>Error token creation</strong> represents a critical interface design decision. Rather than throwing exceptions immediately upon encountering malformed input, the tokenizer creates <code>INVALID</code> tokens that carry error information. This approach allows the parser to collect multiple errors in a single pass and provide comprehensive feedback to users. The error tokens include the malformed text, error description, and precise position information.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Ftokenizer-state-machine.svg\" alt=\"Tokenizer State Machine\"></p>\n<p>The interface supports <strong>format-specific customization</strong> through polymorphism. Each format (INI, TOML, YAML) can extend <code>BaseTokenizer</code> and override specific recognition methods while inheriting the common navigation and position tracking behavior. For example, YAML tokenization requires custom <code>skip_whitespace</code> behavior that preserves significant indentation, while TOML needs specialized string literal processing for its multiple quoting styles.</p>\n<h3 id=\"string-literal-handling\">String Literal Handling</h3>\n<p>String literal tokenization represents the most complex aspect of lexical analysis due to the variety of quoting styles, escape sequence processing, and multiline handling requirements across different configuration formats. The complexity stems from <strong>context sensitivity</strong>—the same character sequence can have completely different meanings depending on the current string parsing state.</p>\n<p>The string literal recognition system must handle multiple <strong>quoting styles</strong> with different semantic rules:</p>\n<table>\n<thead>\n<tr>\n<th>Quote Style</th>\n<th>Formats</th>\n<th>Escape Processing</th>\n<th>Multiline Support</th>\n<th>Special Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single quotes <code>&#39;text&#39;</code></td>\n<td>INI, TOML, YAML</td>\n<td>TOML: none, others: basic</td>\n<td>TOML only</td>\n<td>TOML literal strings don&#39;t process escapes</td>\n</tr>\n<tr>\n<td>Double quotes <code>&quot;text&quot;</code></td>\n<td>All formats</td>\n<td>Full escape processing</td>\n<td>No</td>\n<td>Standard escape sequences: <code>\\n</code>, <code>\\t</code>, <code>\\&quot;</code>, <code>\\\\</code></td>\n</tr>\n<tr>\n<td>Triple single <code>&#39;&#39;&#39;text&#39;&#39;&#39;</code></td>\n<td>TOML only</td>\n<td>None</td>\n<td>Yes</td>\n<td>First newline after opening quotes is trimmed</td>\n</tr>\n<tr>\n<td>Triple double <code>&quot;&quot;&quot;text&quot;&quot;&quot;</code></td>\n<td>TOML only</td>\n<td>Full escape processing</td>\n<td>Yes</td>\n<td>First newline after opening quotes is trimmed</td>\n</tr>\n<tr>\n<td>No quotes (bare)</td>\n<td>All formats</td>\n<td>None</td>\n<td>Context-dependent</td>\n<td>YAML flow scalars, INI values, TOML bare keys</td>\n</tr>\n</tbody></table>\n<p>The <strong>escape sequence processing</strong> requires a nested state machine within string literal parsing. When the tokenizer encounters a backslash character inside a double-quoted string, it must transition to escape processing mode and handle various escape sequences:</p>\n<ol>\n<li>The tokenizer identifies the backslash as an escape indicator</li>\n<li>It examines the following character to determine the escape type</li>\n<li>For standard escapes (<code>\\n</code>, <code>\\t</code>, <code>\\r</code>, <code>\\&quot;</code>, <code>\\\\</code>), it converts to the appropriate character</li>\n<li>For Unicode escapes (<code>\\u0041</code>, <code>\\U00000041</code>), it processes the hexadecimal digits</li>\n<li>For invalid escape sequences, it creates an error token with position information</li>\n<li>It returns to normal string content processing mode</li>\n</ol>\n<p><strong>Multiline string handling</strong> introduces additional complexity because the tokenizer must track logical string content while maintaining accurate position information for error reporting. TOML&#39;s triple-quoted strings include special rules about newline handling: the first newline immediately after the opening quotes is automatically trimmed, but subsequent newlines are preserved as content. The tokenizer must implement this logic while correctly updating line and column tracking.</p>\n<blockquote>\n<p><strong>Decision: String Literal State Machine Design</strong></p>\n<ul>\n<li><strong>Context</strong>: String parsing requires handling nested state transitions for quotes, escapes, and multiline content</li>\n<li><strong>Options Considered</strong>: Single-function string parser, recursive state machine, explicit state tracking with switch statements</li>\n<li><strong>Decision</strong>: Explicit state tracking with enumerated states and transition table</li>\n<li><strong>Rationale</strong>: Provides clear visibility into parsing state, enables systematic testing, supports debugging</li>\n<li><strong>Consequences</strong>: More verbose implementation but significantly easier to debug and extend for new quote styles</li>\n</ul>\n</blockquote>\n<p>The string literal parser maintains explicit state through an enumeration:</p>\n<table>\n<thead>\n<tr>\n<th>State</th>\n<th>Description</th>\n<th>Valid Transitions</th>\n<th>Exit Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>NORMAL</code></td>\n<td>Processing regular string content</td>\n<td><code>ESCAPE</code>, <code>QUOTE_END</code></td>\n<td>Closing quote or end of input</td>\n</tr>\n<tr>\n<td><code>ESCAPE</code></td>\n<td>Processing escape sequence</td>\n<td><code>NORMAL</code>, <code>ERROR</code></td>\n<td>Valid escape processed or invalid sequence</td>\n</tr>\n<tr>\n<td><code>UNICODE_ESCAPE</code></td>\n<td>Processing <code>\\uXXXX</code> sequence</td>\n<td><code>NORMAL</code>, <code>ERROR</code></td>\n<td>Four hex digits processed or invalid digit</td>\n</tr>\n<tr>\n<td><code>QUOTE_END</code></td>\n<td>Potential end of multiline string</td>\n<td><code>NORMAL</code>, <code>COMPLETE</code></td>\n<td>Triple quote completed or false alarm</td>\n</tr>\n<tr>\n<td><code>ERROR</code></td>\n<td>Invalid sequence encountered</td>\n<td><code>NORMAL</code>, <code>COMPLETE</code></td>\n<td>Error token created, attempt recovery</td>\n</tr>\n<tr>\n<td><code>COMPLETE</code></td>\n<td>String literal fully parsed</td>\n<td>none</td>\n<td>Return completed token</td>\n</tr>\n</tbody></table>\n<p><strong>Error recovery</strong> in string literal parsing requires special consideration because unterminated strings can consume the entire remaining input. When the tokenizer reaches the end of input while still inside a string literal, it must create an error token that includes all the consumed content and indicates the unterminated string issue. The error token should point to the opening quote position to help users identify where the string began.</p>\n<p>The string literal handler must also manage <strong>performance considerations</strong> for large multiline strings. Rather than concatenating characters one at a time (which creates O(n²) complexity), it should identify string boundaries first, then extract the complete content in a single operation. This approach maintains linear performance even for very large configuration files with substantial multiline content.</p>\n<p><strong>Quote character disambiguation</strong> represents another critical challenge. When the tokenizer encounters a quote character, it must determine whether this starts a single-quoted string, a double-quoted string, or potentially a triple-quoted multiline string. This requires lookahead logic that examines the following characters to make the correct determination:</p>\n<ol>\n<li>Single quote encountered: look ahead two characters to check for triple-quote pattern</li>\n<li>If triple-quote detected: initialize multiline literal string parsing</li>\n<li>If not triple-quote: initialize single-quote string parsing</li>\n<li>Handle edge cases like <code>&#39;&#39;</code> (empty string) vs <code>&#39;&#39;&#39;</code> (multiline string start)</li>\n</ol>\n<h3 id=\"tokenizer-architecture-decisions\">Tokenizer Architecture Decisions</h3>\n<p>The tokenizer architecture must balance <strong>performance</strong>, <strong>maintainability</strong>, and <strong>extensibility</strong> while handling the diverse requirements of INI, TOML, and YAML formats. The key architectural decisions affect how the tokenizer manages state, processes different formats, and integrates with the overall parsing pipeline.</p>\n<blockquote>\n<p><strong>Decision: Single Tokenizer vs Format-Specific Tokenizers</strong></p>\n<ul>\n<li><strong>Context</strong>: Each format has different lexical rules, keywords, and syntactic elements that require specialized handling</li>\n<li><strong>Options Considered</strong>: Single universal tokenizer with format flags, completely separate tokenizers, base tokenizer with format-specific extensions</li>\n<li><strong>Decision</strong>: Base tokenizer with format-specific extensions through inheritance</li>\n<li><strong>Rationale</strong>: Maximizes code reuse for common functionality while allowing format-specific customization where needed</li>\n<li><strong>Consequences</strong>: Shared navigation and position tracking logic, but specialized string and numeric literal handling per format</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Universal tokenizer</td>\n<td>Single codebase, consistent behavior</td>\n<td>Complex branching logic, hard to optimize per format</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Separate tokenizers</td>\n<td>Format-optimized, clear separation</td>\n<td>Code duplication, inconsistent position tracking</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Inheritance-based</td>\n<td>Code reuse + specialization</td>\n<td>Moderate complexity, clear extension points</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<p>The inheritance-based approach creates a <code>BaseTokenizer</code> that handles universal concerns like position tracking, character navigation, and basic token creation. Format-specific tokenizers (<code>INITokenizer</code>, <code>TOMLTokenizer</code>, <code>YAMLTokenizer</code>) extend the base class and override methods that require specialized behavior. This architecture enables sharing common functionality while supporting format-specific requirements like YAML&#39;s indentation-sensitive tokenization or TOML&#39;s complex string literal rules.</p>\n<blockquote>\n<p><strong>Decision: Character Encoding and Unicode Support</strong></p>\n<ul>\n<li><strong>Context</strong>: Configuration files may contain Unicode characters, and different platforms handle encoding differently</li>\n<li><strong>Options Considered</strong>: ASCII-only support, UTF-8 with byte processing, full Unicode with character processing</li>\n<li><strong>Decision</strong>: Full Unicode support with character-based processing and UTF-8 encoding assumption</li>\n<li><strong>Rationale</strong>: Modern configuration files commonly contain Unicode characters for internationalization</li>\n<li><strong>Consequences</strong>: More complex position tracking (byte offset vs character offset), but supports real-world usage</li>\n</ul>\n</blockquote>\n<p>The Unicode support decision affects position tracking implementation. The tokenizer must maintain both <strong>character positions</strong> (for human-readable error messages) and <strong>byte offsets</strong> (for efficient file access). When processing multibyte Unicode characters, the character count and byte count diverge, requiring careful tracking of both metrics.</p>\n<blockquote>\n<p><strong>Decision: Error Recovery Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Malformed configuration files should provide helpful error messages rather than immediate failures</li>\n<li><strong>Options Considered</strong>: Fail-fast on first error, collect errors and continue, attempt automatic correction</li>\n<li><strong>Decision</strong>: Collect errors in special INVALID tokens and continue tokenization when possible</li>\n<li><strong>Rationale</strong>: Allows comprehensive error reporting and better user experience for debugging</li>\n<li><strong>Consequences</strong>: More complex parser logic but significantly better error messages</li>\n</ul>\n</blockquote>\n<p>The error recovery approach creates <code>TokenError</code> objects that carry both the problematic text and contextual information about what was expected. The tokenizer continues processing after creating error tokens, allowing it to identify multiple issues in a single pass. This approach provides much better user experience compared to fail-fast behavior that requires multiple edit-test cycles to identify all issues.</p>\n<blockquote>\n<p><strong>Decision: Memory Management and Token Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Large configuration files could create memory pressure if all tokens are stored simultaneously</li>\n<li><strong>Options Considered</strong>: Store all tokens in memory, streaming tokenization only, hybrid approach with optional storage</li>\n<li><strong>Decision</strong>: Hybrid approach with batch tokenization for normal use and streaming interface for large files</li>\n<li><strong>Rationale</strong>: Most configuration files are small enough for in-memory processing, but large files need streaming</li>\n<li><strong>Consequences</strong>: Two code paths to maintain, but supports both typical usage and edge cases</li>\n</ul>\n</blockquote>\n<p>The memory management decision creates two tokenization modes. The standard <code>tokenize()</code> method processes the entire input and returns a complete token list, suitable for typical configuration files under 1MB. The streaming <code>next_token()</code> interface allows processing arbitrarily large files by generating tokens on demand without storing the complete list.</p>\n<p><strong>State machine implementation</strong> represents another critical architectural decision. The tokenizer uses explicit state enumeration rather than implicit state in nested function calls. This approach provides better debuggability and makes the tokenization process easier to trace and test. Each state transition is explicitly modeled, making the tokenizer&#39;s behavior predictable and testable.</p>\n<h3 id=\"common-tokenizer-pitfalls\">Common Tokenizer Pitfalls</h3>\n<p>Tokenizer implementation involves several subtle but critical pitfalls that frequently trap developers, particularly around escape sequence processing, position tracking accuracy, and state management consistency.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Escape Sequence Processing</strong></p>\n<p>The most common tokenizer mistake involves improper handling of escape sequences within string literals. Developers often implement escape processing that fails to handle edge cases or processes escapes in contexts where they shouldn&#39;t be processed. For example, TOML literal strings (single-quoted) should not process escape sequences—the sequence <code>&#39;C:\\path\\to\\file&#39;</code> should preserve the backslashes literally, not interpret them as escape attempts.</p>\n<p>The error typically manifests when the tokenizer applies escape processing to all quoted strings regardless of quote type. This breaks TOML literal strings and can cause incorrect parsing of file paths, regular expressions, and other content that intentionally contains backslashes. The fix requires checking the string literal type before applying escape processing: only double-quoted strings in TOML should process escapes, while single-quoted strings preserve backslashes literally.</p>\n<p>⚠️ <strong>Pitfall: Position Tracking Inconsistencies</strong></p>\n<p>Position tracking errors create confusing error messages that point to incorrect locations in the source file. The most frequent mistake involves failing to properly update line and column numbers when processing special characters like tabs, carriage returns, and multiline strings. When the tokenizer encounters a tab character, it should advance the column position according to tab width settings (typically 4 or 8 spaces), not increment by one character.</p>\n<p>Another common position tracking error occurs with multiline string literals. When processing TOML triple-quoted strings, the tokenizer must correctly update line numbers for each newline within the string content while ensuring that subsequent tokens have accurate position information. The error typically causes error messages to point to the beginning of the multiline string rather than the actual error location.</p>\n<p>⚠️ <strong>Pitfall: State Machine Inconsistencies</strong></p>\n<p>State management errors in string literal parsing often cause the tokenizer to get &quot;stuck&quot; in a particular state or fail to properly transition between states. A common example involves handling nested quote characters: when processing <code>&quot;He said \\&quot;hello\\&quot;&quot;</code>, the tokenizer must properly transition to escape state when it encounters the backslash, process the escaped quote as content, and return to normal string processing state.</p>\n<p>The error typically occurs when developers implement string parsing with ad-hoc conditional logic rather than explicit state tracking. The fix requires implementing a clear state machine with enumerated states and explicit transition conditions. Each state should have well-defined entry conditions, processing behavior, and exit conditions.</p>\n<p>⚠️ <strong>Pitfall: Lookahead Buffer Management</strong></p>\n<p>Incorrect lookahead implementation can cause the tokenizer to miss token boundaries or incorrectly classify tokens. A common mistake involves implementing <code>peek()</code> that doesn&#39;t properly handle the end-of-file condition, causing array index errors or infinite loops when the tokenizer reaches the end of input.</p>\n<p>Another lookahead error involves modifying the tokenizer state during lookahead operations. The <code>peek()</code> method should be purely observational—it should examine upcoming characters without advancing the current position or changing internal state. Lookahead that accidentally modifies state can cause tokens to be skipped or duplicated.</p>\n<p>⚠️ <strong>Pitfall: Comment Handling Edge Cases</strong></p>\n<p>Comment processing errors frequently occur at line boundaries and in interaction with string literals. A common mistake involves recognizing comment delimiters (<code>#</code> or <code>;</code>) that appear inside string literals as actual comment starts. The line <code>message = &quot;Error #404: Not found&quot;</code> should not treat everything after the <code>#</code> as a comment—the hash character is part of the string content.</p>\n<p>The fix requires checking the current parsing context before processing comment delimiters. Comment recognition should only occur when the tokenizer is not inside a string literal or other quoted context. Additionally, comment processing must properly handle different line ending styles (<code>\\n</code>, <code>\\r\\n</code>, <code>\\r</code>) to ensure comments are correctly terminated.</p>\n<p>⚠️ <strong>Pitfall: Number Format Recognition</strong></p>\n<p>Numeric literal tokenization often fails to handle edge cases like scientific notation, hex literals, or numbers with underscores (allowed in TOML). A common error involves recognizing <code>1.23e-4</code> as three separate tokens (<code>1.23</code>, <code>e</code>, <code>-4</code>) instead of a single floating-point number in scientific notation.</p>\n<p>The fix requires implementing proper lookahead in number recognition. When the tokenizer encounters a digit, it must examine the following characters to determine the complete numeric literal extent. This includes handling decimal points, exponent markers (<code>e</code> or <code>E</code>), sign characters in exponents, and format-specific features like TOML&#39;s underscore separators in large numbers.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The tokenizer implementation requires careful attention to character encoding, state management, and performance optimization. The following guidance provides concrete recommendations for building a robust tokenizer that handles all three configuration formats effectively.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Character Processing</td>\n<td>Basic string indexing with ord()</td>\n<td>Unicode-aware processing with unicodedata module</td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>Explicit state variables</td>\n<td>State machine with enum states and transition table</td>\n</tr>\n<tr>\n<td>Position Tracking</td>\n<td>Simple line/column counters</td>\n<td>Position objects with byte offset, line, column tracking</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception throwing</td>\n<td>Error token collection with context</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/\n├── tokenizer/\n│   ├── __init__.py              ← exports BaseTokenizer, Token, TokenType\n│   ├── base_tokenizer.py        ← BaseTokenizer with core functionality\n│   ├── tokens.py                ← Token, TokenType, Position definitions\n│   ├── ini_tokenizer.py         ← INITokenizer with format-specific rules\n│   ├── toml_tokenizer.py        ← TOMLTokenizer with complex string handling\n│   ├── yaml_tokenizer.py        ← YAMLTokenizer with indentation processing\n│   └── errors.py                ← TokenError and related error types\n├── parsers/                     ← parser components use tokenizer\n└── tests/\n    └── tokenizer/\n        ├── test_base_tokenizer.py\n        ├── test_string_literals.py\n        └── test_position_tracking.py</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Complete token and position definitions that learners can use directly:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NEWLINE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()      </span><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()        </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()        </span><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()          </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()       </span><span style=\"color:#6A737D\"># {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()         </span><span style=\"color:#6A737D\"># }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()             </span><span style=\"color:#6A737D\"># YAML indentation increase</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()             </span><span style=\"color:#6A737D\"># YAML indentation decrease</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BLOCK_SEQUENCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()     </span><span style=\"color:#6A737D\"># YAML - marker</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVALID</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()            </span><span style=\"color:#6A737D\"># Error token</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_text: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Character constants for tokenizer logic</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EOF_MARKER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">WHITESPACE_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> ' </span><span style=\"color:#79B8FF\">\\t\\r</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">NEWLINE_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">QUOTE_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '\"</span><span style=\"color:#79B8FF\">\\'</span><span style=\"color:#9ECBFF\">`'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">NUMBER_START_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '0123456789+-'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">IDENTIFIER_START_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">IDENTIFIER_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> IDENTIFIER_START_CHARS</span><span style=\"color:#F97583\"> +</span><span style=\"color:#9ECBFF\"> '0123456789-'</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p>BaseTokenizer implementation with detailed TODO comments for learner implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_position</span><span style=\"color:#E1E4E8\">(self) -> Position:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns current parsing position with line, column, and byte offset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> Position(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.line, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.column, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.position)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> peek</span><span style=\"color:#E1E4E8\">(self, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Look ahead at character without consuming, returns EOF_MARKER at end.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate target position as self.position + offset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if target position is beyond source length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return EOF_MARKER if beyond end, otherwise return character at target</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle negative offsets by returning EOF_MARKER</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Consume current character, update position tracking, return consumed char.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if at end of source, return EOF_MARKER if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get current character at self.position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Increment self.position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update line and column based on character type:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If newline: increment line, reset column to 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If tab: advance column to next tab stop (typically +4 or +8)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Otherwise: increment column by 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the consumed character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle both \\n and \\r\\n newline styles</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> skip_whitespace</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Advance past all non-semantic whitespace characters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Loop while current character is in WHITESPACE_CHARS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call advance() for each whitespace character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Stop when reaching non-whitespace or EOF_MARKER</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Note: Don't skip newlines - they may be significant tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_string_literal</span><span style=\"color:#E1E4E8\">(self, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse quoted string with escape sequence processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_pos </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_position()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if this might be a triple-quoted string (look ahead 2 chars)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If triple-quoted, call read_multiline_string instead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Advance past opening quote</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize empty content list for building string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Loop until closing quote or EOF:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If escape char (\\), process escape sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If closing quote, break loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If newline in single-quoted string, create error token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Otherwise add character to content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Advance past closing quote (if found)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Join content list and create STRING token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Different quote types have different escape rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> read_number</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse numeric literal with type inference (int/float).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine if number starts with sign (+/-)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Read integer portion (digits, possibly with underscores in TOML)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for decimal point, read fractional portion if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for exponent (e/E), read exponent with optional sign</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Determine if result should be int or float based on format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Convert string to appropriate numeric type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create NUMBER token with converted value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle scientific notation like 1.23e-4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> list[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main tokenization entry point - processes entire input.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize empty tokens list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Loop until position reaches end of source:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Skip whitespace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Identify token type from current character</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Call appropriate read_* method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Add resulting token to list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add final EOF token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return complete token list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use character-based dispatch for token type identification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li>Use <code>str.isdigit()</code>, <code>str.isalpha()</code>, <code>str.isalnum()</code> for character classification</li>\n<li>Handle Unicode properly with <code>len()</code> and string slicing - Python handles UTF-8 correctly</li>\n<li>Use <code>ord()</code> and <code>chr()</code> for escape sequence processing (e.g., <code>\\n</code> → <code>chr(10)</code>)</li>\n<li>Consider using <code>unicodedata.category()</code> for advanced Unicode character classification</li>\n<li>Use <code>enum.auto()</code> for TokenType to avoid manual numbering</li>\n<li>Implement <code>__str__</code> and <code>__repr__</code> methods for debugging Token and Position classes</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the base tokenizer:</p>\n<p><strong>Test Command:</strong> <code>python -m pytest tests/tokenizer/test_base_tokenizer.py -v</code></p>\n<p><strong>Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>test_peek_lookahead PASSED\ntest_advance_position_tracking PASSED  \ntest_string_literal_basic PASSED\ntest_string_literal_escapes PASSED\ntest_number_parsing PASSED\ntest_tokenize_complete_input PASSED</code></pre></div>\n\n<p><strong>Manual Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> BaseTokenizer(</span><span style=\"color:#9ECBFF\">'key = \"value\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer.tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">([</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(token) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> token </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tokens])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: ['IDENTIFIER(key) at line 1, column 1', 'EQUALS(=) at line 1, column 5', ...]</span></span></code></pre></div>\n\n<p><strong>Signs of Problems:</strong></p>\n<ul>\n<li>Position tracking off by one → Check advance() method line/column updates</li>\n<li>Escape sequences not working → Verify quote type checking in read_string_literal</li>\n<li>Infinite loops → Check EOF_MARKER handling in peek() and advance()</li>\n<li>Missing tokens → Verify whitespace skipping doesn&#39;t consume significant characters</li>\n</ul>\n<h2 id=\"ini-parser-component-design\">INI Parser Component Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> INI Parser - implements section-based parsing with key-value pairs and comment handling</p>\n</blockquote>\n<p>The INI parser represents our entry point into configuration file parsing, serving as the foundation for understanding core parsing concepts before tackling more complex formats. Think of INI parsing as learning to drive in an empty parking lot before venturing onto busy highways—the fundamental principles of tokenization, state management, and data structure mapping are all present, but in their simplest possible form.</p>\n<p>The elegance of INI files lies in their human-readable simplicity: sections enclosed in square brackets, key-value pairs separated by equals signs or colons, and comments prefixed with semicolons or hash symbols. However, this apparent simplicity conceals several parsing challenges that make INI an excellent learning platform for understanding configuration file processing.</p>\n<h3 id=\"ini-parsing-mental-model-understanding-ini-as-section-based-key-value-organization\">INI Parsing Mental Model: Understanding INI as Section-Based Key-Value Organization</h3>\n<p>Understanding INI parsing requires thinking about it as a <strong>hierarchical filing system</strong> where documents are organized into labeled folders. Each section header acts like a manila folder tab, and the key-value pairs underneath are the documents filed within that folder. The parser&#39;s job is to walk through this filing system sequentially, creating digital folders and filing away documents in the correct locations.</p>\n<p>The mental model extends further when we consider that some documents might exist outside of any folder (global keys), some folders might be referenced multiple times throughout the filing system, and some documents might have annotations (comments) that need to be preserved or ignored based on configuration.</p>\n<p>This hierarchical organization creates several parsing contexts that must be tracked simultaneously. The <strong>current section context</strong> determines where newly encountered key-value pairs should be stored. The <strong>comment context</strong> affects whether a line should be processed or ignored. The <strong>value context</strong> influences how the right-hand side of assignments should be interpreted and type-converted.</p>\n<p>Consider this fundamental INI structure that demonstrates the key parsing contexts:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code># Global configuration\ndebug = true\nport = 8080\n\n[database]\nhost = localhost\nport = 5432  # Different from global port\nname = &quot;my app&quot;\n\n[database.connection]\ntimeout = 30</code></pre></div>\n\n<p>The parser must recognize that we have three distinct namespaces: global scope, <code>database</code> section, and <code>database.connection</code> section. Keys can be repeated across namespaces (like <code>port</code>) without conflict. The final data structure should reflect this hierarchical organization through nested dictionaries.</p>\n<p>The <strong>line-oriented processing model</strong> is crucial for INI parsing. Unlike formats that might span arbitrary boundaries, INI parsing can be conceptualized as processing one logical line at a time. Each line falls into one of several categories: blank lines (ignored), comment lines (ignored or preserved), section headers (context switches), key-value assignments (data storage), or continuation lines (value extension).</p>\n<p>This line-oriented approach creates a natural state machine where the parser maintains current context and applies different processing rules based on line classification. The beauty of this model is its simplicity—most INI parsing bugs stem from incorrectly classifying lines or failing to maintain proper context across line boundaries.</p>\n<h3 id=\"ini-parsing-algorithm-step-by-step-process-for-handling-sections-keys-values-and-comments\">INI Parsing Algorithm: Step-by-Step Process for Handling Sections, Keys, Values, and Comments</h3>\n<p>The INI parsing algorithm follows a systematic approach that mirrors how humans naturally read these files: top to bottom, line by line, building understanding of structure as we encounter section boundaries. The algorithm maintains parsing state across line boundaries while making context-sensitive decisions about how to interpret each line.</p>\n<p>The core algorithm can be broken down into distinct phases that handle different aspects of the parsing process:</p>\n<ol>\n<li><p><strong>Preprocessing and Line Classification</strong>: The raw input is split into logical lines (handling line continuation if supported), and each line is classified by its syntactic structure. This phase identifies section headers, key-value pairs, comments, and blank lines through pattern recognition.</p>\n</li>\n<li><p><strong>Context Management</strong>: Based on line classification, the parser updates its current parsing context. Section headers cause context switches, while key-value pairs are processed within the current context. This phase maintains the section stack and current namespace information.</p>\n</li>\n<li><p><strong>Value Processing</strong>: Key-value pairs undergo value extraction, type inference, and storage within the appropriate section. This phase handles quoted strings, escape sequences, and basic type coercion.</p>\n</li>\n<li><p><strong>Structure Building</strong>: The parsed data is organized into the final nested dictionary structure, creating intermediate sections as needed and handling key path resolution for dotted notation.</p>\n</li>\n</ol>\n<p>Here&#39;s the detailed step-by-step algorithm for INI parsing:</p>\n<ol>\n<li><p><strong>Initialize parsing state</strong> by creating an empty result dictionary, setting current section to global scope (empty string), and preparing line processing infrastructure including line number tracking and error context accumulation.</p>\n</li>\n<li><p><strong>Split input into logical lines</strong> while preserving line number information for error reporting. Handle different line ending conventions (CRLF, LF) and optionally support line continuation with backslash escapes if that feature is desired.</p>\n</li>\n<li><p><strong>For each logical line, classify its syntactic type</strong> by examining leading characters after whitespace trimming. Lines starting with <code>[</code> are section headers, lines containing <code>=</code> or <code>:</code> are key-value pairs, lines starting with <code>;</code> or <code>#</code> are comments, and empty lines are ignored.</p>\n</li>\n<li><p><strong>Process section headers</strong> by extracting the section name from between square brackets, validating the syntax, and updating the current section context. Create nested dictionary structure if the section name contains dots (like <code>database.connection</code>).</p>\n</li>\n<li><p><strong>Process key-value pairs</strong> by splitting on the first occurrence of <code>=</code> or <code>:</code>, trimming whitespace from both sides, and applying value processing rules. Store the processed key-value pair in the current section of the result dictionary.</p>\n</li>\n<li><p><strong>Handle value processing</strong> by detecting quoted strings and processing escape sequences, applying type inference to convert strings to appropriate Python types (integers, floats, booleans), and handling special cases like empty values or values containing the assignment operator.</p>\n</li>\n<li><p><strong>Manage inline comments</strong> by detecting comment markers after values and either preserving them as metadata or ignoring them based on parser configuration. Be careful not to treat comment markers inside quoted strings as actual comments.</p>\n</li>\n<li><p><strong>Handle error recovery</strong> by collecting syntax errors with line number information while continuing to parse subsequent lines when possible. This allows reporting multiple errors in a single parse run rather than failing on the first error.</p>\n</li>\n<li><p><strong>Finalize the result structure</strong> by ensuring all sections exist in the final dictionary (even empty sections), applying any post-processing transformations like key normalization, and validating the overall structure for consistency.</p>\n</li>\n<li><p><strong>Return the parsed configuration</strong> as a nested dictionary where top-level keys represent sections (with an empty string key for global values) and values are dictionaries containing the key-value pairs for each section.</p>\n</li>\n</ol>\n<p>The algorithm maintains several pieces of state throughout this process: the current section context (determining where new keys are stored), accumulated errors for comprehensive reporting, line number information for error context, and the growing result dictionary that represents the parsed structure.</p>\n<h3 id=\"ini-architecture-decisions-decisions-around-global-keys-comment-handling-and-value-type-inference\">INI Architecture Decisions: Decisions Around Global Keys, Comment Handling, and Value Type Inference</h3>\n<p>The INI parser requires several architectural decisions that significantly impact both implementation complexity and user experience. These decisions represent trade-offs between simplicity, compatibility, and functionality. Each choice has downstream consequences for how the parser behaves in edge cases and how well it integrates with the broader configuration parsing system.</p>\n<blockquote>\n<p><strong>Decision: Global Key Handling Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: INI files often contain key-value pairs before any section headers, but different parsers handle these differently—some reject them, others create an implicit section, others treat them as truly global.</li>\n<li><strong>Options Considered</strong>: Reject global keys as syntax errors, create implicit &quot;DEFAULT&quot; section, store in special global namespace</li>\n<li><strong>Decision</strong>: Store global keys in the result dictionary with empty string as section key, accessible as <code>result[&quot;&quot;]</code></li>\n<li><strong>Rationale</strong>: This approach maintains the section-based mental model while accommodating real-world INI files that use global configuration. It&#39;s explicit (no hidden &quot;DEFAULT&quot; section) and predictable (always accessible the same way).</li>\n<li><strong>Consequences</strong>: Users must check for the empty string key to access global values, but the behavior is consistent and discoverable.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Global Key Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Compatibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Reject as error</td>\n<td>Simple, enforces structure</td>\n<td>Breaks with common INI files</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Implicit DEFAULT section</td>\n<td>Familiar to ConfigParser users</td>\n<td>Hidden behavior, not obvious</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Empty string key</td>\n<td>Explicit, predictable</td>\n<td>Slightly awkward access pattern</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Comment Preservation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Comments serve different purposes—some are documentation that should be preserved, others are temporary annotations. Different use cases benefit from different handling approaches.</li>\n<li><strong>Options Considered</strong>: Always ignore comments, always preserve as metadata, configurable preservation with default ignore</li>\n<li><strong>Decision</strong>: Default to ignoring comments with optional preservation mode that stores them as metadata in a parallel structure</li>\n<li><strong>Rationale</strong>: Most configuration consumers don&#39;t need comment data and benefit from cleaner output, but preserving comments enables round-trip editing and documentation tools.</li>\n<li><strong>Consequences</strong>: Simple use cases get clean data, advanced use cases can opt into comment preservation with additional complexity.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Comment Strategy</th>\n<th>Memory Usage</th>\n<th>Output Complexity</th>\n<th>Use Case Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Always ignore</td>\n<td>Low</td>\n<td>Simple</td>\n<td>Basic config loading</td>\n</tr>\n<tr>\n<td>Always preserve</td>\n<td>High</td>\n<td>Complex</td>\n<td>Documentation tools</td>\n</tr>\n<tr>\n<td>Configurable</td>\n<td>Variable</td>\n<td>Medium</td>\n<td>Both use cases</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Value Type Inference Rules</strong></p>\n<ul>\n<li><strong>Context</strong>: INI files store everything as strings, but users expect automatic conversion to appropriate Python types. Different inference rules affect both usability and predictability.</li>\n<li><strong>Options Considered</strong>: No type inference (all strings), aggressive inference with boolean/number detection, conservative inference with opt-in parsing</li>\n<li><strong>Decision</strong>: Conservative type inference that converts obvious numbers and booleans while preserving strings for ambiguous values</li>\n<li><strong>Rationale</strong>: Automatic conversion improves usability for common cases while avoiding surprising conversions that might break application logic.</li>\n<li><strong>Consequences</strong>: Users get convenient type conversion for clear cases but maintain control over ambiguous values through explicit quoting.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Inference Level</th>\n<th>Conversion Examples</th>\n<th>Surprising Cases</th>\n<th>User Control</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>None</td>\n<td>All strings</td>\n<td>None</td>\n<td>Full</td>\n</tr>\n<tr>\n<td>Aggressive</td>\n<td>&quot;yes&quot;→true, &quot;1.0&quot;→float</td>\n<td>Many edge cases</td>\n<td>Limited</td>\n</tr>\n<tr>\n<td>Conservative</td>\n<td>&quot;123&quot;→int, &quot;true&quot;→bool</td>\n<td>Minimal</td>\n<td>Good balance</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Section Nesting Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Some INI files use dotted section names to imply hierarchy, while others treat dots as literal characters. The parser must choose how to interpret these patterns.</li>\n<li><strong>Options Considered</strong>: Always flatten section names, always create nested structure from dots, configurable nesting with sensible default</li>\n<li><strong>Decision</strong>: Create nested dictionary structure from dotted section names by default, with option to disable nesting for literal interpretation</li>\n<li><strong>Rationale</strong>: Nested structure matches user expectations for modern configuration and provides better integration with other formats, while the option preserves compatibility with legacy systems.</li>\n<li><strong>Consequences</strong>: Default behavior creates intuitive nested access patterns, but users working with legacy systems can opt out of nesting interpretation.</li>\n</ul>\n</blockquote>\n<p>The type inference implementation uses a priority-based approach that attempts conversions in order of specificity. Integer conversion is attempted first (using Python&#39;s <code>int()</code> function), followed by float conversion, then boolean conversion using a predefined mapping of string values to boolean states. Only if all conversions fail does the value remain as a string.</p>\n<table>\n<thead>\n<tr>\n<th>Value Pattern</th>\n<th>Inference Result</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>123</code></td>\n<td><code>int(123)</code></td>\n<td>Clear integer literal</td>\n</tr>\n<tr>\n<td><code>12.34</code></td>\n<td><code>float(12.34)</code></td>\n<td>Clear float literal</td>\n</tr>\n<tr>\n<td><code>true</code>, <code>yes</code>, <code>on</code></td>\n<td><code>True</code></td>\n<td>Common boolean representations</td>\n</tr>\n<tr>\n<td><code>false</code>, <code>no</code>, <code>off</code></td>\n<td><code>False</code></td>\n<td>Common boolean representations</td>\n</tr>\n<tr>\n<td><code>&quot;123&quot;</code></td>\n<td><code>str(&quot;123&quot;)</code></td>\n<td>Quoted values bypass inference</td>\n</tr>\n<tr>\n<td><code>1.0.0</code></td>\n<td><code>str(&quot;1.0.0&quot;)</code></td>\n<td>Version strings stay as strings</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-ini-parsing-pitfalls-issues-with-inline-comments-quoted-values-and-section-nesting\">Common INI Parsing Pitfalls: Issues with Inline Comments, Quoted Values, and Section Nesting</h3>\n<p>INI parsing appears deceptively simple, leading many developers to underestimate the edge cases and subtle behaviors that can cause parsing failures or incorrect data extraction. Understanding these common pitfalls helps build robust parsers and debug issues when they arise.</p>\n<p>⚠️ <strong>Pitfall: Inline Comment Detection Breaking on Quoted Values</strong></p>\n<p>The most frequent mistake in INI parsing occurs when handling inline comments that appear after values. Naive implementations split lines on comment characters (<code>;</code> or <code>#</code>) without considering whether those characters appear inside quoted strings.</p>\n<p>Consider this problematic line: <code>url = &quot;postgres://user:pass#123@host/db&quot; ; connection string</code>. A naive parser might treat <code>#123@host/db&quot; ; connection string</code> as a comment, corrupting the URL value. The correct behavior requires tracking quote state while scanning for comment markers.</p>\n<p>This issue manifests in several ways: quoted passwords containing special characters get truncated, file paths with hash symbols get corrupted, and SQL connection strings become malformed. The fix requires implementing stateful scanning that only recognizes comment markers outside of quoted contexts.</p>\n<p>The robust approach processes each character sequentially, maintaining quote state and only treating comment markers as significant when not inside string literals. This requires handling escape sequences within quotes to properly track quote boundaries.</p>\n<p>⚠️ <strong>Pitfall: Assignment Operator Confusion in Values</strong></p>\n<p>Another common error involves handling assignment operators (<code>=</code> or <code>:</code>) that appear within values rather than as key-value separators. Splitting lines on the first occurrence of these characters seems correct, but many implementations split on all occurrences, breaking values that contain assignment operators.</p>\n<p>For example: <code>expression = x = y + z</code> should result in key <code>&quot;expression&quot;</code> with value <code>&quot;x = y + z&quot;</code>, not key <code>&quot;expression&quot;</code> with value <code>&quot;x&quot;</code> and some confused parsing of the remainder. The correct implementation splits only on the first occurrence of the assignment operator.</p>\n<p>This pitfall extends to handling multiple assignment operators in configuration values. Mathematical expressions, code snippets, and connection strings frequently contain these characters as content rather than syntax.</p>\n<p>⚠️ <strong>Pitfall: Section Name Validation and Escaping</strong></p>\n<p>Section headers require careful validation to prevent malformed input from corrupting the parser state or creating invalid nested structures. Common mistakes include accepting section names with unmatched brackets, allowing empty section names, or failing to handle escaped characters in section names.</p>\n<p>Consider these problematic section headers: <code>[section[nested]]</code>, <code>[]</code>, <code>[section\\nname]</code>. Each represents a different failure mode—nested brackets might indicate user confusion about syntax, empty names create ambiguous dictionary keys, and embedded newlines suggest multiline parsing errors.</p>\n<p>The robust approach validates section names against a clear syntax specification, rejects malformed headers with helpful error messages, and handles any necessary escape sequences consistently with the overall parsing approach.</p>\n<p>⚠️ <strong>Pitfall: Global Key Context Management</strong></p>\n<p>Managing the parsing context when transitioning between global keys and sectioned keys creates several failure modes. The most common involves incorrectly attributing keys to sections when encountering mixed global and sectioned content.</p>\n<p>Consider this structure:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">ini</span><pre class=\"arch-pre shiki-highlighted\"><code>global_key = value\n[section]\nsection_key = value\nanother_global = value</code></pre></div>\n\n<p>Incorrect implementations might attribute <code>another_global</code> to <code>[section]</code> because they fail to implement proper context switching. Keys appearing after sections should remain in their current section unless explicitly moved by another section header.</p>\n<p>The fix requires maintaining clear section context throughout parsing and only changing context when section headers are encountered. Global keys appearing after sections indicate either user error (should be warned about) or intentional global configuration (should be supported).</p>\n<p>⚠️ <strong>Pitfall: Value Trimming and Whitespace Semantics</strong></p>\n<p>Different approaches to whitespace handling create inconsistent behavior across different INI files. Some implementations trim all whitespace from values, others preserve it exactly, and still others apply complex rules about when trimming should occur.</p>\n<p>The challenge emerges with values like <code>name = &quot; John Doe &quot;</code> where the spaces might be significant (indicating exact string content) or artifacts of formatting (should be trimmed). Quoted values suggest the spaces are intentional, but unquoted values are ambiguous.</p>\n<p>A consistent approach defines clear rules: unquoted values have leading and trailing whitespace trimmed, quoted values preserve all internal whitespace including leading/trailing spaces, and the quoting characters themselves are removed during processing.</p>\n<p>⚠️ <strong>Pitfall: Line Continuation and Multiline Value Handling</strong></p>\n<p>Some INI dialects support line continuation with backslash characters or multiline values with specific syntax. Implementing this incorrectly breaks both simple and complex configurations.</p>\n<p>The most common error involves treating continuation characters inside quoted strings as actual continuation markers rather than literal content. Another frequent mistake is failing to properly join continued lines while preserving meaningful whitespace.</p>\n<p>For implementations that choose to support continuation, the logic must carefully distinguish between syntactic continuation (backslash at line end) and literal backslashes that happen to appear at line boundaries.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The INI parser implementation focuses on building robust line-by-line processing while maintaining clean separation between tokenization, parsing, and data structure construction. This foundation prepares developers for more complex parsing challenges in TOML and YAML formats.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Line Processing</td>\n<td><code>str.splitlines()</code> with manual iteration</td>\n<td><code>io.StringIO</code> with buffered reading</td>\n</tr>\n<tr>\n<td>Pattern Recognition</td>\n<td>String methods (<code>startswith</code>, <code>find</code>, <code>strip</code>)</td>\n<td><code>re</code> module with compiled patterns</td>\n</tr>\n<tr>\n<td>Value Type Conversion</td>\n<td>Manual <code>int()</code>, <code>float()</code>, <code>bool()</code> attempts</td>\n<td><code>ast.literal_eval()</code> for safe evaluation</td>\n</tr>\n<tr>\n<td>Error Collection</td>\n<td>Simple list of error strings</td>\n<td>Structured <code>ParseError</code> objects with position</td>\n</tr>\n<tr>\n<td>String Parsing</td>\n<td>Character-by-character state machine</td>\n<td><code>shlex</code> module for shell-like parsing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config_parser/\n├── __init__.py\n├── common/\n│   ├── __init__.py\n│   ├── types.py              # Token, Position, ParseError definitions\n│   ├── tokenizer.py          # BaseTokenizer (from previous section)\n│   └── utils.py              # Shared utilities\n├── ini/\n│   ├── __init__.py\n│   ├── parser.py             # INIParser class - main implementation\n│   ├── validator.py          # INI-specific validation rules\n│   └── test_ini.py           # INI parser tests\n├── toml/                     # Future TOML implementation\n├── yaml/                     # Future YAML implementation\n└── main.py                   # CLI interface and format detection</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>File: <code>common/types.py</code></strong> (Complete implementation for shared types)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Shared type definitions for configuration parsers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">These types provide the foundation for all format-specific implementations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, auto</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, List, Optional, Dict, Union</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Token types used across all configuration formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NUMBER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto() </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IDENTIFIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EQUALS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COLON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NEWLINE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EOF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()      </span><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SECTION_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()        </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()        </span><span style=\"color:#6A737D\"># [</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()          </span><span style=\"color:#6A737D\"># ]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_START</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()       </span><span style=\"color:#6A737D\"># {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OBJECT_END</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()         </span><span style=\"color:#6A737D\"># }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMMA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DOT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEDENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BLOCK_SEQUENCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()     </span><span style=\"color:#6A737D\"># YAML list marker</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INVALID</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> auto()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Position information for tokens and errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"A lexical token with type, value, and position information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: Position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_text: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.type.name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">(</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.value</span><span style=\"color:#F97583\">!r</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parsing errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggestion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._format_message())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _format_message</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">msg</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> at </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suggestion:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            msg </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">msg</span><span style=\"color:#79B8FF\">}\\n</span><span style=\"color:#9ECBFF\">Suggestion: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.suggestion</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> msg</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error during tokenization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error in syntax structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error in logical structure (e.g., duplicate keys).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Constants used across parsers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EOF_MARKER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\0</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">WHITESPACE_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> ' </span><span style=\"color:#79B8FF\">\\t\\r</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">NEWLINE_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">QUOTE_CHARS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> '\"</span><span style=\"color:#79B8FF\">\\'</span><span style=\"color:#9ECBFF\">`'</span></span></code></pre></div>\n\n<p><strong>File: <code>common/utils.py</code></strong> (Complete utility functions)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Utility functions shared across all configuration parsers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Position, ParseError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> current_position</span><span style=\"color:#E1E4E8\">(source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Position:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate line and column from string offset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines_before </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source[:offset].count(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> lines_before </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> offset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_newline </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.rfind(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, offset)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> offset </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> last_newline </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> Position(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">lines_before </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        column</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">column </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        offset</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_error_context</span><span style=\"color:#E1E4E8\">(source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Position, context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate visual error context showing problematic line with pointer.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source.splitlines()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> lines </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(lines):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"Error context unavailable\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, position.line </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> context_lines </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(lines), position.line </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> context_lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    context_parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(start_line, end_line):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        line_num </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        prefix </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \">>> \"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"    \"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context_parts.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prefix</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">line_num</span><span style=\"color:#F97583\">:4d</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">lines[i]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add pointer line for error location</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> line_num </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position.line:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pointer_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" \"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(prefix) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> position.column </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"^\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            context_parts.append(pointer_line)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(context_parts)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> normalize_key</span><span style=\"color:#E1E4E8\">(key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"lowercase\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Apply key normalization strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"lowercase\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> key.lower().strip()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"preserve\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> key.strip()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> strategy </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"snake_case\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> key.lower().replace(</span><span style=\"color:#9ECBFF\">\"-\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"_\"</span><span style=\"color:#E1E4E8\">).replace(</span><span style=\"color:#9ECBFF\">\" \"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"_\"</span><span style=\"color:#E1E4E8\">).strip()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unknown normalization strategy: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">strategy</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> merge_nested_dicts</span><span style=\"color:#E1E4E8\">(dict1: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], dict2: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Deep merge nested dictionaries, with dict2 taking precedence.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dict1.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> dict2.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(result[key], </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(value, </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> merge_nested_dicts(result[key], value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_format</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Automatically detect configuration format from content.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content.strip()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> content:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"ini\"</span><span style=\"color:#6A737D\">  # Default for empty files</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [line.strip() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> content.splitlines() </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> line.strip()]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> lines:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"ini\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # YAML indicators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(line.startswith(</span><span style=\"color:#9ECBFF\">'- '</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> ': '</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> line.startswith(</span><span style=\"color:#9ECBFF\">'['</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           for</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> lines):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for YAML-specific patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(line </span><span style=\"color:#F97583\">and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> line[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].isspace() </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> ': '</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> lines):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"yaml\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # TOML indicators  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(line.startswith(</span><span style=\"color:#9ECBFF\">'[['</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> ' = '</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> '\"'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> line </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">           for</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> lines):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"toml\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Default to INI</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"ini\"</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>File: <code>ini/parser.py</code></strong> (Skeleton for learner implementation)</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">INI format configuration parser.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Implements line-based parsing for section headers and key-value pairs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..common.types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Position, ParseError, </span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">, StructureError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..common.utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> current_position, normalize_key, create_error_context</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> INIParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Parser for INI format configuration files.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Section headers: [section.name]  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Key-value pairs: key = value, key: value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Comments: ; comment, # comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Quoted values: key = \"value with spaces\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Type inference: automatic conversion to int, float, bool</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, preserve_comments: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 enable_nesting: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 key_normalization: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"preserve\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.preserve_comments </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> preserve_comments</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enable_nesting </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> enable_nesting  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.key_normalization </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> key_normalization</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors: List[ParseError] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse INI format configuration content.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns nested dictionary with sections as keys.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Global keys are stored under empty string key.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize parsing state (result dict, current section, line tracking)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Split content into logical lines while preserving line numbers  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each line, classify its type (section, key-value, comment, blank)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Process each line type with appropriate handler method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle any accumulated errors and return result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self._classify_line() to determine line type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Track current section context throughout parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _classify_line</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line_num: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Classify a line as section, keyvalue, comment, or blank.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns (line_type, processed_content) tuple.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        line_type is one of: 'section', 'keyvalue', 'comment', 'blank'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Strip whitespace and check for blank lines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for comment lines (starting with ; or #)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for section headers (enclosed in [])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for key-value pairs (containing = or :)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return appropriate classification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Look for patterns after stripping whitespace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Be careful about quoted strings containing special characters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_section_header</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line_num: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Extract section name from header line like [section.name].</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns the section name, creating nested structure if dots present.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract content between [ and ] brackets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate section name (not empty, valid characters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle dotted names for nested sections if nesting enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return normalized section name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check for unmatched brackets and report errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Consider whether to allow spaces in section names</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_key_value_pair</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line_num: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, current_section: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               result: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse key-value pair and store in result dictionary.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handles both = and : delimiters, quoted values, inline comments.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find the assignment operator (= or :) - use first occurrence only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Split line into key and value parts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Process value part (handle quotes, inline comments, type inference)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store in appropriate section of result dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle any parsing errors gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self._parse_value() to handle complex value processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Be careful about assignment operators inside quoted strings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_value</span><span style=\"color:#E1E4E8\">(self, value_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line_num: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[Any, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse value string, handling quotes, escapes, and type inference.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns (processed_value, inline_comment) tuple.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Detect and handle quoted strings (preserve exact content)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find inline comments (but not inside quoted strings)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply type inference to unquoted values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return processed value and any inline comment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Need to track quote state when looking for comment markers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle escape sequences in quoted strings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _infer_type</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Convert string value to appropriate Python type.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Attempts int, float, bool conversion in order.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Try integer conversion first (for values like \"123\")</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Try float conversion (for values like \"12.34\")  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Try boolean conversion (true, false, yes, no, on, off)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Fall back to string if all conversions fail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use try/except for conversion attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Define boolean value mapping constants</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_nested_section</span><span style=\"color:#E1E4E8\">(self, result: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], section_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Create nested dictionary structure for dotted section names.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        For section \"database.connection\", creates result[\"database\"][\"connection\"].</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns the innermost dictionary where keys should be stored.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Split section path on dots if nesting enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Walk through path parts, creating nested dicts as needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return reference to innermost dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle case where intermediate path conflicts with existing keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check if intermediate keys are already non-dict values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Consider whether empty section names are valid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Additional helper functions for string processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _find_comment_start</span><span style=\"color:#E1E4E8\">(value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, start_pos: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Find start of inline comment, respecting quoted string boundaries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement stateful scanning for comment markers outside quotes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _process_quoted_string</span><span style=\"color:#E1E4E8\">(quoted_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, quote_char: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Process escape sequences in quoted string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle \\n, \\t, \\\\, \\\", \\' escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Python Implementation Notes:</strong></p>\n<ul>\n<li>Use <code>str.partition(&#39;=&#39;)</code> instead of <code>str.split(&#39;=&#39;, 1)</code> for cleaner key-value splitting</li>\n<li>The <code>configparser</code> module in standard library provides reference behavior, but implement from scratch for learning</li>\n<li><code>str.strip()</code> removes all whitespace; <code>str.lstrip(&#39; \\t&#39;)</code> removes only spaces and tabs</li>\n<li>For boolean inference: <code>{&#39;true&#39;: True, &#39;false&#39;: False, &#39;yes&#39;: True, &#39;no&#39;: False, &#39;on&#39;: True, &#39;off&#39;: False}</code></li>\n<li>Use <code>collections.defaultdict(dict)</code> to automatically create nested sections</li>\n<li>Regular expressions are overkill for INI parsing—string methods are sufficient and more readable</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result[section][key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._infer_type(value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">except</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">ValueError</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">TypeError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Invalid value for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                       position</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">current_position(content, line_start_offset),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                       suggestion</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Check value format or use quotes for literal strings\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    self</span><span style=\"color:#E1E4E8\">.errors.append(error)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result[section][key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value  </span><span style=\"color:#6A737D\"># Fall back to string value</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the INI parser, verify correct behavior with these test cases:</p>\n<p><strong>Test Command:</strong> <code>python -m pytest ini/test_ini.py -v</code></p>\n<p><strong>Manual Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ini.parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> INIParser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> INIParser()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Global config</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">debug = true  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">port = 8080</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">[database]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">host = localhost</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">port = 5432</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">password = \"secret#123\"  ; inline comment</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">[database.pool]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">min_connections = 5</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">max_connections = 20</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected structure:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> config[</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"debug\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"port\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">8080</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> config[</span><span style=\"color:#9ECBFF\">\"database\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"host\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"localhost\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> config[</span><span style=\"color:#9ECBFF\">\"database\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"port\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5432</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> config[</span><span style=\"color:#9ECBFF\">\"database\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"password\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"secret#123\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> config[</span><span style=\"color:#9ECBFF\">\"database\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"pool\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"min_connections\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 5</span></span></code></pre></div>\n\n<p><strong>Success Indicators:</strong></p>\n<ul>\n<li>Global keys accessible via <code>config[&quot;&quot;]</code></li>\n<li>Section nesting creates proper dictionary hierarchy  </li>\n<li>Type inference converts <code>&quot;true&quot;</code> to <code>True</code>, <code>&quot;123&quot;</code> to <code>123</code></li>\n<li>Quoted strings preserve exact content including special characters</li>\n<li>Inline comments after values are ignored (unless preservation enabled)</li>\n<li>Parser continues after errors and collects multiple issues</li>\n</ul>\n<p><strong>Common Issues to Debug:</strong></p>\n<ul>\n<li>If all values are strings: Check <code>_infer_type()</code> implementation</li>\n<li>If comments appear in values: Check quote state tracking in <code>_parse_value()</code></li>\n<li>If section nesting fails: Verify <code>_create_nested_section()</code> path splitting</li>\n<li>If global keys missing: Ensure empty string key exists in result dictionary</li>\n</ul>\n<h2 id=\"toml-parser-component-design\">TOML Parser Component Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> TOML Tokenizer, TOML Parser - builds advanced tokenization capabilities and implements recursive descent parsing for tables, arrays, and complex type system</p>\n</blockquote>\n<p>The TOML parser represents the most sophisticated component in our configuration parsing system, handling the complex interplay between explicit type systems, nested table structures, and array-of-tables syntax. Unlike the line-based simplicity of INI parsing, TOML parsing requires a full recursive descent approach that can manage hierarchical document structures while maintaining strict rules about key redefinition and table organization. This component demonstrates advanced parsing techniques including lookahead parsing, symbol table management, and context-sensitive grammar handling that form the foundation for understanding modern configuration language implementation.</p>\n<p>The complexity of TOML parsing stems from its ambitious goal of being both human-readable and unambiguous for machine processing. While this creates a more robust configuration format, it introduces significant parsing challenges around table inheritance, dotted key expansion, and the distinction between inline tables and table headers. Understanding these concepts provides essential insight into how modern configuration languages balance expressiveness with parsing complexity.</p>\n<h3 id=\"toml-parsing-mental-model\">TOML Parsing Mental Model</h3>\n<p>Think of TOML parsing as <strong>architectural blueprint interpretation</strong> — you&#39;re reading a structured document that defines a building (your data structure) through a combination of room declarations (tables), furniture lists (arrays), and detailed specifications (key-value pairs). Just as an architect must understand that declaring a &quot;kitchen.island&quot; doesn&#39;t just create an island but also ensures the kitchen room exists, TOML parsing must understand that dotted keys implicitly create the table hierarchy they reference.</p>\n<p>The key insight is that TOML operates on two levels simultaneously: the <strong>document level</strong> where you&#39;re declaring tables and organizing structure, and the <strong>value level</strong> where you&#39;re assigning specific data to keys. Unlike INI files where sections are simple containers, TOML tables have complex relationships — they can be nested, referenced by dotted paths, and must follow strict rules about redefinition and ordering.</p>\n<p>Consider this mental model: TOML parsing is like <strong>managing a hierarchical filing system</strong> where each table declaration creates a folder, each key-value pair files a document, and dotted notation creates nested folder structures. The parser must ensure that no document is filed twice in the same location (key redefinition error) and that folder creation follows proper hierarchy rules. Array-of-tables syntax is like creating multiple folders with the same name but different numeric suffixes — <code>reports-1/</code>, <code>reports-2/</code>, etc.</p>\n<p>The <strong>explicit type system</strong> in TOML means the parser acts as both a filing clerk and a data validator. Unlike YAML&#39;s implicit typing where &quot;yes&quot; might become a boolean, TOML requires that strings look like strings (<code>&quot;hello&quot;</code>), integers look like integers (<code>42</code>), and dates follow specific formats (<code>1979-05-27T07:32:00Z</code>). This eliminates parsing ambiguity but requires sophisticated tokenization to distinguish between different literal formats.</p>\n<blockquote>\n<p>The critical parsing insight is that TOML tables exist in a <strong>global namespace</strong> where dotted keys and table headers must be coordinated. When you write <code>physical.color = &quot;orange&quot;</code> followed by <code>[physical.size]</code>, the parser must recognize that both reference the same <code>physical</code> table and merge appropriately.</p>\n</blockquote>\n<h3 id=\"recursive-descent-algorithm\">Recursive Descent Algorithm</h3>\n<p>Recursive descent parsing provides the natural approach for handling TOML&#39;s nested structure by mapping grammatical rules directly to function calls. Each syntactic construct (table, key-value pair, array, inline table) corresponds to a parsing function that can call other parsing functions to handle nested elements. This creates a call stack that mirrors the document&#39;s hierarchical structure.</p>\n<p>The <strong>core recursive descent pattern</strong> follows this structure: each parsing function consumes tokens that match its grammatical rule, recursively calls other parsing functions for nested elements, and returns a parse tree node representing the matched structure. For TOML, the top-level parser alternates between table declarations and key-value assignments, calling specialized functions for each construct type.</p>\n<p>Here&#39;s the algorithmic breakdown for TOML recursive descent parsing:</p>\n<ol>\n<li><strong>Initialize parsing state</strong> with an empty symbol table for tracking defined keys and tables, current table context pointing to the root, and token stream positioned at the beginning</li>\n<li><strong>Main parsing loop</strong> examines the current token to determine the construct type — section header brackets indicate table declarations, identifiers suggest key-value pairs, and EOF terminates parsing</li>\n<li><strong>Table declaration processing</strong> extracts the table path from bracketed notation, validates that the path doesn&#39;t conflict with existing definitions, creates the table hierarchy if needed, and updates the current table context</li>\n<li><strong>Key-value pair processing</strong> parses the key (which may be dotted), validates the key isn&#39;t already defined in the current table, parses the value recursively based on its type, and stores the key-value mapping</li>\n<li><strong>Dotted key expansion</strong> treats dotted keys like nested table creation, ensuring each path segment creates a table if it doesn&#39;t exist, and placing the final key-value pair in the deepest table</li>\n<li><strong>Value parsing delegation</strong> examines the token type and calls appropriate specialized parsers — <code>parse_string()</code> for quoted literals, <code>parse_array()</code> for bracket notation, <code>parse_inline_table()</code> for brace notation</li>\n<li><strong>Array-of-tables handling</strong> creates a new table instance and appends it to an array stored under the table name, allowing multiple table definitions with the same path</li>\n<li><strong>Error recovery</strong> captures parsing failures with position information, attempts to resynchronize at the next table boundary or key-value pair, and continues parsing to collect multiple error reports</li>\n</ol>\n<p>The <strong>recursive call pattern</strong> emerges naturally from TOML&#39;s grammar. When <code>parse_value()</code> encounters an array, it calls <code>parse_array()</code>, which repeatedly calls <code>parse_value()</code> for each element. When parsing inline tables, <code>parse_inline_table()</code> calls <code>parse_key_value_pair()</code> for each entry. This recursive structure handles arbitrary nesting depth without explicit stack management.</p>\n<p><strong>Lookahead parsing</strong> becomes essential for disambiguating TOML constructs. Distinguishing between table headers <code>[table]</code> and array-of-tables <code>[[table]]</code> requires examining two tokens ahead. Similarly, dotted keys require lookahead to determine whether <code>a.b.c</code> represents a single dotted key or the start of a more complex structure.</p>\n<blockquote>\n<p>The recursive descent approach naturally handles TOML&#39;s context sensitivity — each parsing function maintains the current table context and key path, allowing nested calls to understand their position in the document hierarchy.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Parsing Function</th>\n<th>Purpose</th>\n<th>Recursive Calls</th>\n<th>Returns</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>parse_document()</code></td>\n<td>Main entry point, processes top-level constructs</td>\n<td><code>parse_table_header()</code>, <code>parse_key_value_pair()</code></td>\n<td><code>ParseNode</code> with document structure</td>\n</tr>\n<tr>\n<td><code>parse_table_header()</code></td>\n<td>Handles <code>[table]</code> and <code>[[table]]</code> declarations</td>\n<td><code>parse_key_path()</code></td>\n<td><code>SectionNode</code> with table information</td>\n</tr>\n<tr>\n<td><code>parse_key_value_pair()</code></td>\n<td>Processes key assignments</td>\n<td><code>parse_key_path()</code>, <code>parse_value()</code></td>\n<td><code>KeyValueNode</code> with key and value</td>\n</tr>\n<tr>\n<td><code>parse_value()</code></td>\n<td>Dispatches to type-specific parsers</td>\n<td><code>parse_array()</code>, <code>parse_inline_table()</code>, <code>parse_string()</code></td>\n<td><code>ValueNode</code> with typed value</td>\n</tr>\n<tr>\n<td><code>parse_array()</code></td>\n<td>Handles <code>[item1, item2, item3]</code> syntax</td>\n<td><code>parse_value()</code> for each element</td>\n<td><code>ValueNode</code> with array contents</td>\n</tr>\n<tr>\n<td><code>parse_inline_table()</code></td>\n<td>Handles <code>{key = value, key2 = value2}</code> syntax</td>\n<td><code>parse_key_value_pair()</code> for each entry</td>\n<td><code>ValueNode</code> with table contents</td>\n</tr>\n<tr>\n<td><code>parse_key_path()</code></td>\n<td>Handles dotted keys like <code>a.b.c</code></td>\n<td>None (terminal parser)</td>\n<td>List of key segments</td>\n</tr>\n</tbody></table>\n<h3 id=\"table-and-array-of-tables-logic\">Table and Array-of-Tables Logic</h3>\n<p>TOML&#39;s table system represents one of the most complex aspects of configuration file parsing, requiring careful management of hierarchical namespaces, implicit table creation, and the distinction between table headers and array-of-tables declarations. The parser must maintain a global view of the document structure while processing local key-value assignments.</p>\n<p><strong>Table hierarchy management</strong> operates through a symbol table that tracks all defined tables and keys. When the parser encounters a table header like <code>[database.connection]</code>, it must verify that neither <code>database</code> nor <code>database.connection</code> has been previously defined as a non-table value, create any missing intermediate tables, and establish the new current context for subsequent key-value pairs. This process involves both validation and construction phases.</p>\n<p>The <strong>implicit table creation</strong> rule states that dotted keys automatically create the necessary table hierarchy. When processing <code>server.host = &quot;localhost&quot;</code> before any <code>[server]</code> declaration, the parser must create an implicit <code>server</code> table and place the <code>host</code> key within it. This implicit creation must be tracked separately from explicit table declarations to handle later conflicts correctly.</p>\n<p><strong>Array-of-tables syntax</strong> using double brackets <code>[[database.servers]]</code> creates a fundamentally different structure than regular tables. Each array-of-tables declaration appends a new table instance to an array stored under that key path. The parser must distinguish between extending an existing array-of-tables and conflicting with a previously defined single table or value.</p>\n<p>Here&#39;s the detailed algorithm for table and array-of-tables processing:</p>\n<ol>\n<li><strong>Table header detection</strong> recognizes bracket notation and determines whether single brackets indicate a table declaration or double brackets indicate array-of-tables entry</li>\n<li><strong>Path validation</strong> checks that the table path doesn&#39;t conflict with existing definitions — a path can&#39;t be redefined as a different type, and array-of-tables paths must be consistent</li>\n<li><strong>Intermediate table creation</strong> ensures all parent tables in a dotted path exist, creating them implicitly if necessary, and marking them as implicit to allow later explicit redefinition</li>\n<li><strong>Array-of-tables instantiation</strong> creates a new table instance for double-bracket notation, appends it to the array stored under that path, and sets the new table as the current parsing context</li>\n<li><strong>Context switching</strong> updates the current table pointer to reflect the newly declared or accessed table, allowing subsequent key-value pairs to be stored in the correct location</li>\n<li><strong>Conflict detection</strong> validates that new table declarations don&#39;t redefine existing keys or tables, that array-of-tables declarations are consistent with previous usage, and that implicit tables can be explicitly redefined</li>\n</ol>\n<p><strong>Dotted key expansion</strong> requires special handling because it creates table structure inline with value assignment. When processing <code>physical.color = &quot;orange&quot;</code>, the parser must create a <code>physical</code> table if it doesn&#39;t exist, then assign the <code>color</code> key within that table. This expansion must respect the same conflict rules as explicit table declarations.</p>\n<p>The <strong>table redefinition rules</strong> create complex validation requirements. A table can only be explicitly defined once, but keys can be added to tables from multiple locations. Array-of-tables can be extended with multiple <code>[[array.name]]</code> declarations, but the same path can&#39;t mix array-of-tables and regular table syntax.</p>\n<blockquote>\n<p>The key insight is that TOML tables form a <strong>global namespace</strong> where all dotted paths must be mutually consistent. Unlike INI sections that are independent containers, TOML tables are interconnected through their hierarchical relationships and shared namespace.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Table Operation</th>\n<th>Validation Required</th>\n<th>Action Taken</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>[explicit.table]</code></td>\n<td>Path not previously defined as table</td>\n<td>Create table, set as current context</td>\n<td>Path exists as value or different table type</td>\n</tr>\n<tr>\n<td><code>[[array.of.tables]]</code></td>\n<td>Path used consistently for arrays</td>\n<td>Create table, append to array</td>\n<td>Path exists as single table or value</td>\n</tr>\n<tr>\n<td>Dotted key assignment</td>\n<td>Intermediate paths are compatible</td>\n<td>Create implicit tables, assign value</td>\n<td>Intermediate path conflicts with existing value</td>\n</tr>\n<tr>\n<td>Inline table <code>{k=v}</code></td>\n<td>Table contents don&#39;t conflict</td>\n<td>Create table with all key-value pairs</td>\n<td>Keys conflict with existing definitions in scope</td>\n</tr>\n</tbody></table>\n<h3 id=\"toml-architecture-decisions\">TOML Architecture Decisions</h3>\n<p>The TOML parser&#39;s architecture must balance parsing complexity with maintainability, requiring careful decisions about tokenization depth, error handling strategies, and data structure representation. These architectural choices significantly impact both implementation difficulty and runtime performance.</p>\n<blockquote>\n<p><strong>Decision: Two-Pass vs Single-Pass Parsing</strong></p>\n<ul>\n<li><strong>Context</strong>: TOML&#39;s global namespace and table redefinition rules require comprehensive validation that may benefit from multiple parsing phases</li>\n<li><strong>Options Considered</strong>: Single-pass parsing with complex state tracking, two-pass parsing with structure building then validation, three-pass parsing with tokenization, structure building, and validation phases</li>\n<li><strong>Decision</strong>: Single-pass parsing with comprehensive symbol table management</li>\n<li><strong>Rationale</strong>: Single-pass parsing provides better memory efficiency and simpler error reporting, while comprehensive symbol tables can handle the required validation without multiple document traversals. The complexity of multi-pass coordination outweighs the benefits for our educational implementation.</li>\n<li><strong>Consequences</strong>: Requires sophisticated symbol table implementation but provides linear time complexity and straightforward error position reporting</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Parsing Approach</th>\n<th>Memory Usage</th>\n<th>Time Complexity</th>\n<th>Error Reporting Quality</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single-pass with symbol table</td>\n<td>O(n)</td>\n<td>O(n)</td>\n<td>Excellent (exact positions)</td>\n<td>High (complex state management)</td>\n</tr>\n<tr>\n<td>Two-pass parsing</td>\n<td>O(2n)</td>\n<td>O(2n)</td>\n<td>Good (requires position tracking)</td>\n<td>Medium (separate phases)</td>\n</tr>\n<tr>\n<td>Three-pass parsing</td>\n<td>O(3n)</td>\n<td>O(3n)</td>\n<td>Excellent (multiple validation passes)</td>\n<td>Low (simple individual passes)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Recursive Descent vs Parser Generator</strong></p>\n<ul>\n<li><strong>Context</strong>: TOML&#39;s grammar complexity could be handled by hand-written recursive descent or automated parser generation tools</li>\n<li><strong>Options Considered</strong>: Hand-written recursive descent parser, ANTLR or similar parser generator, hybrid approach with generated lexer and hand-written parser</li>\n<li><strong>Decision</strong>: Hand-written recursive descent parser</li>\n<li><strong>Rationale</strong>: Educational value of understanding parsing mechanics outweighs the convenience of generated parsers. Recursive descent provides clear mapping from grammar rules to code, making debugging and extension more accessible to learners.</li>\n<li><strong>Consequences</strong>: Higher implementation effort but better learning outcomes and complete control over error messages and recovery strategies</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Token Stream vs Character Stream Parsing</strong></p>\n<ul>\n<li><strong>Context</strong>: The parser can operate on pre-tokenized input from the tokenizer component or consume characters directly during parsing</li>\n<li><strong>Options Considered</strong>: Pre-tokenized input with full lookahead, character stream with on-demand tokenization, hybrid approach with token buffering</li>\n<li><strong>Decision</strong>: Pre-tokenized input with bounded lookahead</li>\n<li><strong>Rationale</strong>: Separation of lexical analysis and syntactic analysis improves modularity and debugging. Pre-tokenization enables better error messages with token-level position tracking and simplifies parser logic by eliminating character-level concerns.</li>\n<li><strong>Consequences</strong>: Higher memory usage for token storage but cleaner parser implementation and superior error reporting capabilities</li>\n</ul>\n</blockquote>\n<p><strong>Symbol table architecture</strong> requires careful consideration of how to represent the hierarchical namespace and track different types of definitions (explicit tables, implicit tables, array-of-tables, values). The symbol table must support efficient lookup, conflict detection, and context switching as the parser moves between different table scopes.</p>\n<p>The chosen approach uses a <strong>nested dictionary structure</strong> mirroring the final output format, with additional metadata tracking the definition type and location for each entry. This provides O(1) lookup for conflict detection while building the final data structure incrementally during parsing.</p>\n<blockquote>\n<p><strong>Decision: Parse Tree vs Direct Output Generation</strong></p>\n<ul>\n<li><strong>Context</strong>: The parser can build an intermediate parse tree structure or generate the final nested dictionary directly during parsing</li>\n<li><strong>Options Considered</strong>: Full parse tree with separate transformation phase, direct output generation during parsing, hybrid approach with minimal intermediate representation</li>\n<li><strong>Decision</strong>: Direct output generation with conflict tracking metadata</li>\n<li><strong>Rationale</strong>: Direct generation reduces memory usage and eliminates an additional transformation pass. The conflict tracking metadata provides necessary validation capabilities without full parse tree overhead.</li>\n<li><strong>Consequences</strong>: More complex parsing logic but better performance and memory efficiency for large configuration files</li>\n</ul>\n</blockquote>\n<p><strong>Error recovery strategy</strong> determines how the parser responds to syntax errors and whether it attempts to continue parsing to report multiple errors. The architecture must balance comprehensive error reporting with parsing simplicity and reasonable recovery behavior.</p>\n<p>The implemented approach uses <strong>panic-mode recovery</strong> where the parser skips tokens until it reaches a synchronization point (typically the start of the next table or key-value pair), then resumes parsing. This provides multiple error reports while maintaining reasonable parsing state consistency.</p>\n<h3 id=\"common-toml-parsing-pitfalls\">Common TOML Parsing Pitfalls</h3>\n<p>Understanding the frequent mistakes in TOML parsing implementation helps avoid subtle bugs that can produce incorrect results or confusing error messages. These pitfalls often arise from the interaction between TOML&#39;s powerful features and the complexity they introduce in parser implementation.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Table Redefinition Rules</strong></p>\n<p>Many implementations incorrectly allow table redefinition or fail to properly distinguish between extending a table and redefining it. In TOML, once a table is explicitly declared with <code>[table.name]</code>, it cannot be redeclared, but additional keys can be added from other locations through dotted key notation.</p>\n<p>The error manifests when parsing documents like:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>[server]\nhost = &quot;localhost&quot;\n\n[server]  # This should be an error - table redefinition\nport = 8080</code></pre></div>\n\n<p><strong>Why it&#39;s wrong</strong>: TOML specification explicitly forbids table redefinition to eliminate ambiguity about key placement and table structure. Allowing redefinition makes document interpretation dependent on declaration order.</p>\n<p><strong>How to fix</strong>: Maintain a set of explicitly declared table paths and check each new table declaration against this set. Distinguish between explicit table declarations (<code>[table]</code>) and implicit table creation through dotted keys.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Array-of-Tables Mixing</strong></p>\n<p>Implementations often fail to properly validate that array-of-tables declarations are used consistently. Mixing <code>[table]</code> and <code>[[table]]</code> syntax for the same path should produce an error, but many parsers incorrectly allow this or handle it inconsistently.</p>\n<p><strong>Why it&#39;s wrong</strong>: The distinction between single tables and array-of-tables is fundamental to TOML&#39;s type system. Mixing these creates ambiguous document interpretation and violates the specification&#39;s clarity goals.</p>\n<p><strong>How to fix</strong>: Track whether each table path is used as a single table or array-of-tables and validate consistency across all declarations. Maintain separate metadata for table type alongside the symbol table entries.</p>\n<p>⚠️ <strong>Pitfall: Improper Dotted Key Expansion</strong></p>\n<p>Many implementations incorrectly handle the interaction between dotted keys and table declarations. When processing <code>a.b.c = &quot;value&quot;</code>, the parser must create implicit tables for <code>a</code> and <code>a.b</code>, but these implicit tables must be compatible with later explicit table declarations.</p>\n<p>The complexity arises in sequences like:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>physical.color = &quot;orange&quot;  # Creates implicit [physical] table\n[physical.dimensions]      # Must extend the existing physical table\n[physical]                 # Should be error - can't redefine implicit table explicitly</code></pre></div>\n\n<p><strong>Why it&#39;s wrong</strong>: Incorrect dotted key expansion violates TOML&#39;s namespace consistency rules and can lead to key placement in wrong table locations.</p>\n<p><strong>How to fix</strong>: Mark implicitly created tables with metadata indicating their creation method, allow extension through additional dotted keys or sub-table declarations, but forbid explicit redeclaration of implicit tables.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Inline Table Validation</strong></p>\n<p>Inline tables using <code>{key = value, key2 = value2}</code> syntax have special scoping rules that many parsers implement incorrectly. Inline tables must be completely defined in their declaration and cannot be extended later through dotted keys or additional table declarations.</p>\n<p><strong>Why it&#39;s wrong</strong>: TOML treats inline tables as atomic units that cannot be modified after declaration, ensuring document clarity and preventing ambiguous key placement.</p>\n<p><strong>How to fix</strong>: Mark inline tables as immutable in the symbol table and validate that no subsequent operations attempt to add keys to or modify inline table contents.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Unicode and Escape Sequence Handling</strong></p>\n<p>TOML&#39;s string literals support complex escape sequences including Unicode code points, and many implementations handle these incorrectly or incompletely. This is particularly problematic for multiline strings where escape sequence rules differ between basic strings and literal strings.</p>\n<p><strong>Why it&#39;s wrong</strong>: Incorrect escape handling can corrupt string values, cause parsing failures on valid documents, or create security vulnerabilities through improper Unicode handling.</p>\n<p><strong>How to fix</strong>: Implement complete escape sequence processing according to TOML specification, handle Unicode normalization properly, and validate escape sequences during tokenization rather than during parsing.</p>\n<p>⚠️ <strong>Pitfall: Poor Error Message Context</strong></p>\n<p>TOML parsing errors often involve complex interactions between different parts of the document (table redefinition, key conflicts, type mismatches), but many implementations provide error messages that don&#39;t include sufficient context to understand the problem.</p>\n<p><strong>Why it&#39;s wrong</strong>: Poor error messages make debugging configuration files extremely difficult, especially for users who may not understand TOML&#39;s complex rules about table definitions and key conflicts.</p>\n<p><strong>How to fix</strong>: Include both the error location and the conflicting previous definition location in error messages. Provide specific information about what rules were violated and suggestions for fixing the problem.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Ftoml-parsing-sequence.svg\" alt=\"TOML Table Parsing Sequence\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The TOML parser implementation requires sophisticated recursive descent techniques combined with comprehensive symbol table management. This guidance provides the foundation for building a parser that correctly handles TOML&#39;s complex table hierarchy and type system while maintaining clear separation between tokenization and parsing concerns.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Recursive Descent Framework</td>\n<td>Manual recursive function calls with error handling</td>\n<td>Parser combinator library with automatic backtracking</td>\n</tr>\n<tr>\n<td>Symbol Table</td>\n<td>Nested dictionaries with metadata annotations</td>\n<td>Custom symbol table class with scope management</td>\n</tr>\n<tr>\n<td>Type System</td>\n<td>Python native types with isinstance() checking</td>\n<td>Custom TOML type classes with validation methods</td>\n</tr>\n<tr>\n<td>Error Reporting</td>\n<td>Exception-based with position information</td>\n<td>Error accumulation with multiple error reporting</td>\n</tr>\n<tr>\n<td>Token Consumption</td>\n<td>Linear token stream with index tracking</td>\n<td>Token stream class with lookahead buffers</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  parsers/\n    base_parser.py              ← Abstract base for all parsers\n    toml_parser.py              ← Main TOML recursive descent parser\n    toml_types.py               ← TOML-specific type definitions and validation\n  tokenizers/\n    toml_tokenizer.py           ← TOML tokenizer (from previous milestone)\n  data_structures/\n    parse_nodes.py              ← ParseNode hierarchy definitions\n    symbol_table.py             ← Symbol table for conflict tracking\n  tests/\n    test_toml_parser.py         ← Comprehensive TOML parser tests\n    fixtures/\n      toml_examples/            ← TOML test files for each feature\n        basic_tables.toml\n        array_of_tables.toml\n        dotted_keys.toml\n        inline_structures.toml</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Symbol Table Implementation (<code>data_structures/symbol_table.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DefinitionType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXPLICIT_TABLE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"explicit_table\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IMPLICIT_TABLE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"implicit_table\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARRAY_OF_TABLES</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"array_of_tables\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VALUE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"value\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INLINE_TABLE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"inline_table\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DefinitionInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    definition_type: DefinitionType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position: </span><span style=\"color:#9ECBFF\">'Position'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    is_mutable: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SymbolTable</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks all defined keys and tables for conflict detection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.definitions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, DefinitionInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_table_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_definition</span><span style=\"color:#E1E4E8\">(self, key_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], definition_type: DefinitionType, position: </span><span style=\"color:#9ECBFF\">'Position'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a new definition and validate for conflicts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        path_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#E1E4E8\">.join(key_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        existing </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.definitions.get(path_str)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> existing:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Validate compatibility based on TOML rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> definition_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> DefinitionType.</span><span style=\"color:#79B8FF\">EXPLICIT_TABLE</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> existing.definition_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> DefinitionType.</span><span style=\"color:#79B8FF\">IMPLICIT_TABLE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Can explicitly define an implicit table</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.definitions[path_str] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DefinitionInfo(definition_type, position)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> definition_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> DefinitionType.</span><span style=\"color:#79B8FF\">ARRAY_OF_TABLES</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> existing.definition_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> DefinitionType.</span><span style=\"color:#79B8FF\">ARRAY_OF_TABLES</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Can extend array of tables</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#E1E4E8\"> StructureError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot redefine </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">path_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> as </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">definition_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, position)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.definitions[path_str] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DefinitionInfo(definition_type, position)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_nested_structure</span><span style=\"color:#E1E4E8\">(self, key_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create nested dictionary structure for the given path.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> segment </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> key_path[:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> segment </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> current:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                current[segment] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current[segment]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> current</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_current_table</span><span style=\"color:#E1E4E8\">(self, table_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set the current table context for key-value assignments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_table_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> table_path.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_current_table</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the dictionary representing the current table.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_table_path:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> segment </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_table_path:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> segment </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> current:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                current[segment] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current[segment]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> current</span></span></code></pre></div>\n\n<p><strong>TOML Type Validation (<code>parsers/toml_types.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, date, time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Union, List, Dict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TOMLTypeValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates and converts TOML literal values to appropriate Python types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Regex patterns for TOML literals</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTEGER_PATTERN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^[+-]</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">0</span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\">[1-9](?:</span><span style=\"color:#DBEDFF\">_</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">\\d)</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FLOAT_PATTERN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^[+-]</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">0</span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\">[1-9](?:</span><span style=\"color:#DBEDFF\">_</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">\\d)</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">)(?:</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(?:\\d(?:</span><span style=\"color:#DBEDFF\">_</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">\\d)</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">(?:[eE][+-]</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">\\d(?:</span><span style=\"color:#DBEDFF\">_</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">\\d)</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN_PATTERN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^(?:</span><span style=\"color:#DBEDFF\">true</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">false</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DATETIME_PATTERN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^\\d</span><span style=\"color:#F97583\">{4}</span><span style=\"color:#DBEDFF\">-</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#DBEDFF\">-</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#DBEDFF\">T</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#DBEDFF\">:</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#DBEDFF\">:</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#DBEDFF\">Z</span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\">[+-]\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#DBEDFF\">:</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_and_convert</span><span style=\"color:#E1E4E8\">(cls, token: </span><span style=\"color:#9ECBFF\">'Token'</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert token value to appropriate Python type based on TOML rules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._process_string_literal(token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">NUMBER</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._process_numeric_literal(token)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> token.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TokenType.</span><span style=\"color:#79B8FF\">BOOLEAN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token.value </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'true'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> token.value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_string_literal</span><span style=\"color:#E1E4E8\">(cls, token: </span><span style=\"color:#9ECBFF\">'Token'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Process string literal with proper escape sequence handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        raw_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> token.raw_text</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> raw_value.startswith(</span><span style=\"color:#9ECBFF\">'\"\"\"'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> raw_value.startswith(</span><span style=\"color:#9ECBFF\">\"'''\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Multiline string - handle special rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._process_multiline_string(raw_value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> raw_value.startswith(</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Basic string - process escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._process_basic_string(raw_value[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])  </span><span style=\"color:#6A737D\"># Remove quotes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> raw_value.startswith(</span><span style=\"color:#9ECBFF\">\"'\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Literal string - no escape processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> raw_value[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Remove quotes, no escape processing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> raw_value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_basic_string</span><span style=\"color:#E1E4E8\">(cls, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Process escape sequences in basic strings.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle standard escape sequences: \\n, \\t, \\r, \\\\, \\\", etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        escape_map </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'n'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'t'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\t</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'b'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\b</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'f'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\f</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'/'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        i </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(content):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> content[i] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(content):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                next_char </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content[i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> next_char </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> escape_map:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    result.append(escape_map[next_char])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    i </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                elif</span><span style=\"color:#E1E4E8\"> next_char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'u'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(content):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Unicode escape sequence \\uXXXX</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    unicode_hex </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content[i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">:i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    result.append(</span><span style=\"color:#79B8FF\">chr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(unicode_hex, </span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    i </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 6</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                elif</span><span style=\"color:#E1E4E8\"> next_char </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'U'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 9</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(content):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Unicode escape sequence \\UXXXXXXXX</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    unicode_hex </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content[i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">:i</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    result.append(</span><span style=\"color:#79B8FF\">chr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(unicode_hex, </span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    i </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Invalid escape sequence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    result.append(content[i])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    i </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result.append(content[i])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                i </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> ''</span><span style=\"color:#E1E4E8\">.join(result)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Main TOML Parser (<code>parsers/toml_parser.py</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..tokenizers.base_tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Token, TokenType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_structures.parse_nodes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseNode, SectionNode, KeyValueNode, ValueNode</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..data_structures.symbol_table </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SymbolTable, DefinitionType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .base_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseParser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TOMLParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseParser</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Recursive descent parser for TOML configuration format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokens: List[Token]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(tokens)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.symbol_table </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SymbolTable()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_token_index </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main TOML parsing entry point - implements recursive descent algorithm.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize tokenizer and generate token stream from content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Reset parser state (symbol table, current position, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Enter main parsing loop - process document-level constructs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle end-of-file and validate final document state  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return the nested dictionary structure from symbol table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Main loop alternates between table declarations and key-value pairs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_document_level_construct</span><span style=\"color:#E1E4E8\">(self) -> Optional[ParseNode]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse top-level TOML constructs (tables, key-value pairs, comments).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Skip whitespace and comments to find next meaningful token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for EOF condition and return None if document complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Examine current token to determine construct type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Dispatch to appropriate parsing method based on token type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle unexpected tokens with informative error messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use lookahead to distinguish [table] from [[array-of-tables]]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_table_header</span><span style=\"color:#E1E4E8\">(self) -> SectionNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse [table] and [[table.name]] declarations with hierarchy validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine if single bracket [table] or double bracket [[table]]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse the table path (may be dotted like table.subtable.name)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate table path doesn't conflict with existing definitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Register new table definition in symbol table with proper type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create table hierarchy and set as current parsing context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle array-of-tables by creating new table instance and appending</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Array-of-tables creates multiple table instances under same path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_table_path</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse dotted table paths like 'database.connection.pool'.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start with first identifier token as initial path segment  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for DOT token indicating additional path segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consume DOT and parse next identifier in the path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Continue until no more DOT tokens found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate each path segment is a valid identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of path segments for table creation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle quoted identifiers that may contain special characters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_key_value_pair</span><span style=\"color:#E1E4E8\">(self) -> KeyValueNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse key assignments including dotted keys and complex values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse the key path (may be dotted like 'server.database.host')</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Consume EQUALS token and validate assignment operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse the value recursively based on its type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle dotted key expansion - create implicit tables as needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate key doesn't already exist in target table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store key-value mapping in appropriate table location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Dotted keys create nested table structure automatically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_value</span><span style=\"color:#E1E4E8\">(self) -> ValueNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Dispatch to type-specific value parsers based on token examination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Examine current token type to determine value category</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Dispatch to appropriate specialized parser method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle arrays by calling parse_array() for bracket notation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle inline tables by calling parse_inline_table() for braces</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle literal values (strings, numbers, booleans, dates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate parsed value conforms to TOML type system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use lookahead for ambiguous cases like negative numbers vs expressions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_array</span><span style=\"color:#E1E4E8\">(self) -> ValueNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse array literals [item1, item2, item3] with mixed type support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Consume opening ARRAY_START bracket token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle empty array case (immediate closing bracket)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse first array element by recursively calling parse_value()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Loop to parse additional elements separated by COMMA tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle trailing commas (optional in TOML arrays)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Consume closing ARRAY_END bracket and validate structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create ValueNode with array contents and return</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: TOML allows mixed-type arrays unlike some formats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_inline_table</span><span style=\"color:#E1E4E8\">(self) -> ValueNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse inline table syntax {key1 = value1, key2 = value2}.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Consume opening OBJECT_START brace token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle empty table case (immediate closing brace)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse first key-value pair within the inline table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Loop to parse additional pairs separated by COMMA tokens  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate inline table keys don't conflict with each other</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Consume closing OBJECT_END brace and create table structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Mark inline table as immutable in symbol table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Inline tables cannot be extended after declaration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_table_redefinition</span><span style=\"color:#E1E4E8\">(self, table_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], definition_type: DefinitionType) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate new table definition against TOML redefinition rules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert table path to string representation for lookup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if path already exists in symbol table definitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply TOML rules for valid redefinition cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Allow explicit definition of previously implicit tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Allow extension of array-of-tables with consistent syntax</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Raise StructureError for invalid redefinition attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Different definition types have different redefinition rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> expand_dotted_key</span><span style=\"color:#E1E4E8\">(self, key_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], value: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create nested table structure for dotted key assignment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate each segment of key path for conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create implicit tables for intermediate path segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Register implicit table definitions in symbol table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Navigate to or create the target table for value assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate final key doesn't already exist in target table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store the key-value pair in the appropriate nested location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Dotted keys can create deeply nested structures automatically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing TOML tokenizer integration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_toml_parser.py::TestTOMLTokenizerIntegration</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected: All token types correctly recognized, string literals properly parsed, numeric values tokenized with correct types.</p>\n<p><strong>After implementing basic table parsing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from parsers.toml_parser import TOMLParser</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">result = TOMLParser().parse('[server]\\nhost = </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">localhost</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">\\nport = 8080')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Success:', result == {'server': {'host': 'localhost', 'port': 8080}})</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n<p>Expected: <code>Success: True</code> with proper nested dictionary structure.</p>\n<p><strong>After implementing array-of-tables:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from parsers.toml_parser import TOMLParser</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">toml_content = '''</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">[[database.servers]]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">ip = </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">192.168.1.1</span><span style=\"color:#79B8FF\">\\\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">dc = </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">eqdc10</span><span style=\"color:#79B8FF\">\\\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">[[database.servers]]  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">ip = </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">192.168.1.2</span><span style=\"color:#79B8FF\">\\\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">dc = </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">eqdc10</span><span style=\"color:#79B8FF\">\\\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'''</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">result = TOMLParser().parse(toml_content)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Array length:', len(result['database']['servers']))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('First server IP:', result['database']['servers'][0]['ip'])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n<p>Expected: <code>Array length: 2</code> and <code>First server IP: 192.168.1.1</code>.</p>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;Key redefinition error&quot; on valid TOML</td>\n<td>Incorrect table conflict detection</td>\n<td>Check symbol table state when error occurs</td>\n<td>Review table redefinition validation rules</td>\n</tr>\n<tr>\n<td>Array-of-tables creates single table instead of array</td>\n<td>Missing double-bracket detection logic</td>\n<td>Examine tokenizer output for bracket tokens</td>\n<td>Implement proper lookahead for <code>[[</code> vs <code>[</code></td>\n</tr>\n<tr>\n<td>Dotted keys create incorrect nesting</td>\n<td>Improper key path expansion</td>\n<td>Trace key path parsing and table creation</td>\n<td>Debug dotted key expansion algorithm step by step</td>\n</tr>\n<tr>\n<td>Inline tables allow extension (should be immutable)</td>\n<td>Missing immutability enforcement</td>\n<td>Check if inline tables marked as immutable</td>\n<td>Add immutability validation in symbol table</td>\n</tr>\n<tr>\n<td>Complex nested structures parse incorrectly</td>\n<td>Recursive descent state management issues</td>\n<td>Print parser state at each recursive call</td>\n<td>Review context switching and current table tracking</td>\n</tr>\n<tr>\n<td>Unicode strings corrupted during parsing</td>\n<td>Incorrect escape sequence processing</td>\n<td>Test with known Unicode test cases</td>\n<td>Debug string literal processing in type validator</td>\n</tr>\n</tbody></table>\n<h2 id=\"yaml-parser-component-design\">YAML Parser Component Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> YAML Subset Parser - implements indentation-sensitive parsing for mappings, sequences, and scalar values with automatic type inference</p>\n</blockquote>\n<p>The YAML parser represents the most conceptually different parsing challenge in our configuration file parser suite. While INI parsing follows line-based rules and TOML parsing uses explicit delimiters, YAML parsing requires understanding <strong>indentation-driven hierarchical structure</strong> where whitespace itself carries semantic meaning. This fundamental difference demands a completely different parsing approach built around stack-based nesting management and context-sensitive interpretation.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fyaml-indentation-stack.svg\" alt=\"YAML Indentation Stack Management\"></p>\n<h3 id=\"yaml-parsing-mental-model\">YAML Parsing Mental Model</h3>\n<p>Think of YAML parsing like reading a well-structured outline or book table of contents. The indentation level tells you exactly where you are in the hierarchy - a chapter, section, subsection, or bullet point. Just as you would track your current nesting level while reading an outline, the YAML parser maintains a <strong>stack of indentation contexts</strong> that grows and shrinks as the document flows through different nesting levels.</p>\n<p>Consider this mental model: imagine you&#39;re organizing physical file folders on a desk. Each level of indentation represents placing a folder inside another folder. When you encounter a line with less indentation, you&#39;re &quot;backing out&quot; of nested folders until you reach the appropriate parent folder. When you encounter deeper indentation, you&#39;re &quot;diving into&quot; a subfolder structure. The parser maintains this folder stack in memory, always knowing exactly which &quot;folder&quot; (context) it&#39;s currently processing.</p>\n<p>The <strong>indentation-driven hierarchical structure</strong> means that YAML parsing is fundamentally about tracking these context transitions. Unlike TOML where table boundaries are explicit (<code>[table.name]</code>) or INI where sections are clearly marked (<code>[section]</code>), YAML structure emerges from the spatial relationship between lines. This creates both elegance and complexity - the format reads naturally to humans but requires sophisticated state tracking for machines.</p>\n<p>The parser must continuously answer three questions: &quot;What nesting level am I at?&quot;, &quot;Am I entering a deeper level or returning to a shallower one?&quot;, and &quot;What type of structure am I building at this level?&quot; This context sensitivity means that the same line can mean completely different things depending on the indentation stack state when it&#39;s encountered.</p>\n<h3 id=\"indentation-based-parsing-algorithm\">Indentation-Based Parsing Algorithm</h3>\n<p>The <strong>stack-based approach</strong> forms the algorithmic foundation of YAML parsing. The parser maintains an <strong>indentation stack</strong> where each stack frame represents a nesting level with its indentation amount, structure type (mapping or sequence), and the data being built at that level. This approach handles the complex dance of tracking when structures begin, continue, and end based purely on whitespace changes.</p>\n<p>The core algorithm operates through a <strong>structure transition</strong> state machine that processes each logical line by comparing its indentation against the current stack state:</p>\n<ol>\n<li><p><strong>Tokenize the logical line</strong> into its component parts: indentation amount, content type (key-value pair, sequence item, or scalar), and the actual content values. This tokenization must handle both block-style syntax (indentation-based) and flow-style syntax (bracket and brace delimited inline structures).</p>\n</li>\n<li><p><strong>Calculate indentation level</strong> by counting leading whitespace characters. YAML strictly forbids mixing tabs and spaces for indentation, so this calculation must validate consistency and reject mixed whitespace. The indentation amount determines the structural relationship to previously parsed content.</p>\n</li>\n<li><p><strong>Compare current indentation to stack state</strong> to determine the structural transition type. If indentation increases, we&#39;re entering a nested structure. If indentation decreases, we&#39;re exiting one or more nested structures. If indentation matches the top of stack, we&#39;re continuing the current structure at the same level.</p>\n</li>\n<li><p><strong>Handle stack transitions</strong> based on the indentation comparison. For increased indentation, push a new stack frame for the nested structure. For decreased indentation, pop stack frames until reaching the matching indentation level, completing any nested structures during the unwinding process.</p>\n</li>\n<li><p><strong>Process the line content</strong> within the appropriate structural context. Key-value pairs create or extend mapping structures. Sequence items (lines starting with dash) create or extend list structures. Scalar values get processed through type inference and stored in the current structure.</p>\n</li>\n<li><p><strong>Validate structural consistency</strong> by ensuring that the indentation level matches exactly with a previous level when returning to a shallower nesting. YAML prohibits &quot;dedenting&quot; to indentation levels that were never established, preventing malformed structural transitions.</p>\n</li>\n<li><p><strong>Update parser state</strong> by modifying the current stack frame&#39;s data structure with the processed line content and preparing for the next line. The stack top always represents the active parsing context for subsequent lines.</p>\n</li>\n</ol>\n<p>The <strong>context-sensitive interpretation</strong> means that identical content can create different structural results depending on the current stack state. A line containing <code>name: value</code> creates a top-level mapping entry when the stack is empty, adds a mapping entry to the current mapping when inside a mapping context, or creates a mapping value for a sequence item when inside a sequence context.</p>\n<p><strong>Stack frame management</strong> requires careful attention to data structure references. Each stack frame must maintain a reference to the actual data structure being built (dictionary for mappings, list for sequences) so that modifications during parsing affect the final output structure. The stack stores these references, not copies of the data.</p>\n<p>The algorithm handles <strong>implicit structure creation</strong> by automatically determining structure types from content patterns. When encountering a key-value pair at a new indentation level, the parser creates a mapping structure. When encountering a sequence item (dash-prefixed line), the parser creates a sequence structure. This implicit creation eliminates the need for explicit structure declarations like TOML&#39;s table headers.</p>\n<h3 id=\"yaml-type-inference-logic\">YAML Type Inference Logic</h3>\n<p>YAML&#39;s <strong>automatic type detection and conversion</strong> system attempts to interpret scalar values as their most natural data types rather than treating everything as strings. This type inference creates intuitive behavior for users but introduces complexity in parsing logic that must recognize and convert various literal patterns.</p>\n<p>The <strong>scalar type inference</strong> follows a precedence hierarchy that examines string content against increasingly specific patterns. The inference engine processes each scalar value through this decision tree:</p>\n<table>\n<thead>\n<tr>\n<th>Pattern Type</th>\n<th>Examples</th>\n<th>Inferred Type</th>\n<th>Conversion Logic</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Boolean literals</td>\n<td><code>true</code>, <code>false</code>, <code>yes</code>, <code>no</code>, <code>on</code>, <code>off</code></td>\n<td>Boolean</td>\n<td>Case-insensitive string matching against known boolean values</td>\n</tr>\n<tr>\n<td>Integer literals</td>\n<td><code>42</code>, <code>-17</code>, <code>0x1A</code>, <code>0o755</code>, <code>0b1010</code></td>\n<td>Integer</td>\n<td>Regex pattern matching with base detection (decimal, hex, octal, binary)</td>\n</tr>\n<tr>\n<td>Float literals</td>\n<td><code>3.14</code>, <code>-2.5e10</code>, <code>.5</code>, <code>1.2e-3</code></td>\n<td>Float</td>\n<td>Scientific notation support with decimal point or exponent indicators</td>\n</tr>\n<tr>\n<td>Date/time literals</td>\n<td><code>2023-12-31</code>, <code>12:30:45</code>, <code>2023-12-31T12:30:45Z</code></td>\n<td>Date/Time</td>\n<td>ISO 8601 format detection with timezone support</td>\n</tr>\n<tr>\n<td>Null literals</td>\n<td><code>null</code>, <code>~</code>, empty value</td>\n<td>Null/None</td>\n<td>Explicit null markers or empty values in certain contexts</td>\n</tr>\n<tr>\n<td>String fallback</td>\n<td>Everything else</td>\n<td>String</td>\n<td>Default case when no other patterns match</td>\n</tr>\n</tbody></table>\n<p>The <strong>boolean inference rules</strong> recognize multiple conventional representations that users commonly expect to work as boolean values. The parser must handle case variations and cultural differences in boolean representation. Values like <code>TRUE</code>, <code>False</code>, <code>YES</code>, <code>No</code> all map to appropriate boolean values through case-insensitive comparison.</p>\n<p><strong>Numeric type inference</strong> involves pattern recognition that distinguishes integers from floating-point numbers while supporting alternative numeric bases. Integer patterns include standard decimal notation, hexadecimal (0x prefix), octal (0o prefix), and binary (0b prefix) representations. Float patterns recognize decimal points, scientific notation with e/E exponents, and special float values like infinity and NaN.</p>\n<p>The <strong>string escaping and quoting</strong> system provides explicit control over type inference when automatic detection produces incorrect results. Single-quoted strings (<code>&#39;value&#39;</code>) preserve literal content without escape sequence processing and prevent type inference. Double-quoted strings (<code>&quot;value&quot;</code>) allow escape sequences like <code>\\n</code>, <code>\\t</code>, <code>\\&quot;</code>, and <code>\\\\</code> while still preventing type inference. Unquoted strings undergo full type inference processing.</p>\n<p><strong>Multiline string handling</strong> supports both literal block scalars (using <code>|</code> indicator) and folded block scalars (using <code>&gt;</code> indicator). Literal blocks preserve line breaks and indentation exactly as written. Folded blocks collapse line breaks within paragraphs into spaces while preserving paragraph breaks indicated by blank lines.</p>\n<p>Type inference must handle <strong>edge cases</strong> where the same string representation could reasonably map to multiple types. For example, <code>01:30:00</code> could represent a time value or a string. The parser uses context clues and follows YAML specification precedence rules to resolve these ambiguities consistently.</p>\n<p>The <strong>inference validation</strong> process ensures that converted values make semantic sense and handles conversion errors gracefully. Invalid date formats, numeric overflows, and malformed patterns should produce clear error messages rather than silent failures or unexpected type assignments.</p>\n<h3 id=\"yaml-architecture-decisions\">YAML Architecture Decisions</h3>\n<p>The architecture decisions for YAML parsing center on managing the complexity of indentation-sensitive parsing while maintaining reasonable performance and error handling capabilities. These decisions directly impact both implementation complexity and user experience.</p>\n<blockquote>\n<p><strong>Decision: YAML Subset Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Full YAML specification includes advanced features like anchors, aliases, complex multiline syntax, and document streams that significantly increase implementation complexity without corresponding learning value for configuration parsing</li>\n<li><strong>Options Considered</strong>: Full YAML 1.2 specification, Common subset (mappings/sequences/scalars), Minimal subset (basic nesting only)</li>\n<li><strong>Decision</strong>: Implement common subset covering indentation-based mappings, sequences, basic scalars, and simple flow syntax</li>\n<li><strong>Rationale</strong>: The common subset covers 95% of real-world YAML configuration usage while keeping implementation complexity manageable for learning purposes. Advanced features like anchors and multi-document streams are rarely used in configuration files</li>\n<li><strong>Consequences</strong>: Users cannot use advanced YAML features, but implementation remains focused on core parsing concepts without getting lost in specification edge cases</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Subset Option</th>\n<th>Features Included</th>\n<th>Implementation Complexity</th>\n<th>Real-World Coverage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full YAML 1.2</td>\n<td>All specification features</td>\n<td>Very High</td>\n<td>100%</td>\n</tr>\n<tr>\n<td>Common Subset</td>\n<td>Mappings, sequences, scalars, basic flow</td>\n<td>Medium</td>\n<td>95%</td>\n</tr>\n<tr>\n<td>Minimal Subset</td>\n<td>Basic nesting only</td>\n<td>Low</td>\n<td>60%</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Flow Syntax Support Level</strong></p>\n<ul>\n<li><strong>Context</strong>: YAML supports both block syntax (indentation-based) and flow syntax (JSON-like with brackets and braces). Flow syntax can appear inline within block structures, creating parsing complexity</li>\n<li><strong>Options Considered</strong>: No flow syntax support, Basic flow arrays/objects only, Full flow syntax with nesting</li>\n<li><strong>Decision</strong>: Support basic flow syntax for inline arrays and objects without deep nesting</li>\n<li><strong>Rationale</strong>: Basic flow syntax like <code>[item1, item2, item3]</code> and <code>{key: value, key2: value2}</code> appears frequently in real configurations and provides valuable parsing experience without excessive complexity</li>\n<li><strong>Consequences</strong>: Enables common inline syntax patterns while avoiding complex flow/block interaction edge cases that would complicate the parsing algorithm significantly</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Indentation Validation Strictness</strong></p>\n<ul>\n<li><strong>Context</strong>: YAML requires consistent indentation patterns but allows flexibility in indentation amounts. Parser must decide how strictly to enforce indentation rules</li>\n<li><strong>Options Considered</strong>: Strict validation (exact indentation matching), Flexible validation (consistent increases/decreases), Lenient validation (best-effort interpretation)</li>\n<li><strong>Decision</strong>: Implement strict validation with clear error messages for indentation violations</li>\n<li><strong>Rationale</strong>: Strict validation helps users learn proper YAML formatting and prevents subtle bugs from inconsistent indentation. Clear error messages make the strictness helpful rather than frustrating</li>\n<li><strong>Consequences</strong>: Parser rejects malformed YAML that other parsers might accept, but provides better learning experience and more predictable behavior</li>\n</ul>\n</blockquote>\n<p>The <strong>multiline string handling</strong> decision affects both parsing complexity and user experience with complex string values in configuration files:</p>\n<blockquote>\n<p><strong>Decision: Multiline String Support</strong></p>\n<ul>\n<li><strong>Context</strong>: YAML multiline strings use block scalar indicators (<code>|</code> and <code>&gt;</code>) with complex indentation and line-ending rules that significantly complicate parsing logic</li>\n<li><strong>Options Considered</strong>: No multiline support, Basic literal blocks only, Full block scalar support with indicators</li>\n<li><strong>Decision</strong>: Support basic literal block scalars with <code>|</code> indicator for exact preservation</li>\n<li><strong>Rationale</strong>: Configuration files commonly need multiline strings for templates, SQL queries, or documentation. Literal blocks cover the most important use case with manageable parsing complexity</li>\n<li><strong>Consequences</strong>: Users can include multiline configuration values naturally, but advanced folding and block scalar features are not available</li>\n</ul>\n</blockquote>\n<p><strong>Error recovery strategy</strong> determines how the parser behaves when encountering invalid YAML structure:</p>\n<table>\n<thead>\n<tr>\n<th>Recovery Strategy</th>\n<th>Behavior on Error</th>\n<th>User Experience</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Fail Fast</td>\n<td>Stop on first error</td>\n<td>Clear failure point</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Continue Parsing</td>\n<td>Collect multiple errors</td>\n<td>Better error overview</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Best-effort Recovery</td>\n<td>Attempt to continue with assumptions</td>\n<td>May mask real errors</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Error Recovery Approach</strong></p>\n<ul>\n<li><strong>Context</strong>: YAML parsing can fail in multiple ways (indentation errors, type conversion failures, syntax violations) and the parser must decide whether to stop immediately or attempt continued parsing</li>\n<li><strong>Options Considered</strong>: Fail-fast on first error, Continue parsing to collect multiple errors, Best-effort recovery with assumptions</li>\n<li><strong>Decision</strong>: Implement fail-fast approach with rich error context and suggestions for fix</li>\n<li><strong>Rationale</strong>: YAML&#39;s indentation sensitivity means that early errors often cascade into misleading later errors. Failing fast with clear context helps users fix the actual root cause rather than getting confused by secondary errors</li>\n<li><strong>Consequences</strong>: Users see one error at a time but get high-quality error messages that directly address the parsing failure cause</li>\n</ul>\n</blockquote>\n<h3 id=\"common-yaml-parsing-pitfalls\">Common YAML Parsing Pitfalls</h3>\n<p>YAML parsing introduces unique challenges that frequently trip up both parser implementers and users. Understanding these pitfalls helps create more robust parsers and better error messages.</p>\n<p>⚠️ <strong>Pitfall: Tab vs Space Indentation Mixing</strong></p>\n<p>The most common YAML parsing error occurs when users mix tab and space characters for indentation. YAML specification explicitly prohibits this mixing, but many text editors make the distinction invisible to users. The parser encounters what appears to be consistent indentation but actually represents inconsistent whitespace character usage.</p>\n<p>This manifests as parsing errors where the parser reports indentation level mismatches despite the visual appearance of correct indentation. Users often spend significant time checking their indentation manually without realizing the issue lies in invisible character differences.</p>\n<p><strong>Detection approach</strong>: During tokenization, track whether tabs or spaces are used for indentation and reject documents that mix both. Provide error messages that specifically identify the character type and line location where mixing occurs.</p>\n<p><strong>Recovery strategy</strong>: Report the exact character position and provide suggestions to convert all indentation to spaces (the recommended YAML convention) or use editor settings to visualize whitespace characters.</p>\n<p>⚠️ <strong>Pitfall: Implicit Type Conversion Surprises</strong></p>\n<p>YAML&#39;s aggressive type inference creates unexpected behavior when string values accidentally match type inference patterns. Common examples include version numbers like <code>1.0</code> becoming floats instead of strings, boolean-like words becoming boolean values, and numeric strings being converted to integers.</p>\n<p>Consider a configuration containing <code>version: 1.20</code> where the user expects string <code>&quot;1.20&quot;</code> but gets float <code>1.2</code> due to type inference. Similarly, values like <code>yes</code>, <code>no</code>, <code>on</code>, <code>off</code> automatically become boolean values even when users intended string values.</p>\n<p><strong>Detection approach</strong>: Implement type inference warnings or provide explicit control through quoted string syntax. When encountering common problematic patterns, consider generating warnings that inform users about automatic type conversion.</p>\n<p><strong>Recovery strategy</strong>: Document type inference rules clearly and show users how to use quoted strings (<code>&quot;1.20&quot;</code>) to force string type when automatic inference produces incorrect results. Consider providing a strict mode that requires explicit typing for ambiguous values.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Indentation Level Returns</strong></p>\n<p>When returning to a previous indentation level, YAML requires exact matching with a previously established level. Users often &quot;dedent&quot; to indentation amounts that were never used, creating invalid structure transitions that the parser cannot interpret.</p>\n<p>This occurs when users inconsistently indent nested structures, such as using 2 spaces for the first level, 4 spaces for the second level, and then 3 spaces when attempting to return to an intermediate level that was never established.</p>\n<p><strong>Error pattern example</strong>: Starting with 0 spaces (root), going to 2 spaces (level 1), then 6 spaces (level 2), and attempting to return to 4 spaces (invalid - level never existed).</p>\n<p><strong>Detection approach</strong>: Maintain a stack of established indentation levels and validate that any dedent operation returns to exactly one of those levels. Reject documents where dedent attempts target indentation amounts that don&#39;t match the stack.</p>\n<p><strong>Recovery strategy</strong>: Provide error messages showing the established indentation levels and suggest valid dedent targets. Include visual representation of the indentation stack state to help users understand the structural context.</p>\n<p>⚠️ <strong>Pitfall: Flow Syntax Context Confusion</strong></p>\n<p>When YAML parsers support both block syntax (indentation-based) and flow syntax (bracket/brace delimited), users can create confusing combinations where flow syntax appears within block contexts or vice versa. The parser must correctly handle context switches between these two syntactic modes.</p>\n<p>Complex cases arise when arrays or objects defined using flow syntax contain values that look like block syntax, or when block structures attempt to contain flow-syntax elements without proper nesting understanding.</p>\n<p><strong>Context switching errors</strong>: Users might start an array with flow syntax <code>[item1,</code> and then attempt to continue with block syntax indentation for subsequent items, creating syntactically invalid combinations.</p>\n<p><strong>Detection approach</strong>: Track parsing context to distinguish between block and flow modes. Validate that syntax elements are appropriate for the current context and provide clear errors when context violations occur.</p>\n<p><strong>Recovery strategy</strong>: Explain the difference between block and flow syntax clearly in error messages. Suggest consistent syntax usage and provide examples of correct flow-within-block or block-within-flow patterns.</p>\n<p>⚠️ <strong>Pitfall: Scalar Value Ambiguity in Lists</strong></p>\n<p>YAML sequence items (list elements) can contain either simple scalar values or complex nested structures. The parser must correctly determine whether a sequence item is a simple value or the beginning of a nested structure, especially when the distinction isn&#39;t immediately clear from the first line.</p>\n<p>Ambiguity occurs with patterns like:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>items:\n  - name: first item\n    description: a complex item\n  - simple item\n  - name: third item</code></pre></div>\n\n<p>The parser must recognize that some sequence items are simple scalars while others are complex mappings, even though the determination might require lookahead parsing to make the distinction.</p>\n<p><strong>Detection approach</strong>: Use lookahead parsing to examine subsequent lines when processing sequence items. If the next line has greater indentation and contains a key-value pattern, treat the sequence item as a complex mapping. Otherwise, treat it as a simple scalar.</p>\n<p><strong>Recovery strategy</strong>: When ambiguity exists, provide clear error messages that explain how the parser interpreted the structure and suggest formatting changes to make the intent explicit.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The YAML parser implementation requires careful balance between handling indentation complexity and maintaining clean, debuggable code structure. The stack-based parsing approach translates naturally to object-oriented design with clear separation between tokenization, structure tracking, and value processing responsibilities.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommendation for Learning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Indentation Tracking</td>\n<td>Simple integer stack</td>\n<td>Structured context objects</td>\n<td>Structured context objects</td>\n</tr>\n<tr>\n<td>Type Inference</td>\n<td>Basic regex patterns</td>\n<td>Comprehensive type system</td>\n<td>Basic regex with clear rules</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exception-based</td>\n<td>Result/Option types</td>\n<td>Exception-based with context</td>\n</tr>\n<tr>\n<td>Flow Syntax</td>\n<td>Skip entirely</td>\n<td>Full bracket/brace parsing</td>\n<td>Basic inline arrays/objects</td>\n</tr>\n<tr>\n<td>String Processing</td>\n<td>Basic quote handling</td>\n<td>Full escape sequence support</td>\n<td>Full escape sequences</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The YAML parser should integrate cleanly with the existing configuration parser architecture while maintaining clear separation of indentation-specific logic:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config-parser/\n  parsers/\n    yaml_parser.py              ← Main YAML parser entry point\n    yaml_tokenizer.py           ← YAML-specific tokenization\n    yaml_structures.py          ← Stack frame and context objects\n    yaml_types.py               ← Type inference logic\n  tests/\n    test_yaml_parser.py         ← Comprehensive YAML tests\n    fixtures/\n      yaml/                     ← Test YAML files with edge cases\n  examples/\n    yaml_parsing_example.py     ← Demonstration of parser usage</code></pre></div>\n\n<p>This structure separates the indentation-sensitive parsing logic from the general tokenization framework while maintaining integration with the overall parser architecture.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Complete indentation stack management infrastructure that handles the complex state tracking required for YAML parsing:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLStructureType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Types of YAML structures that can be nested.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MAPPING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"mapping\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQUENCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sequence\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DOCUMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"document\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IndentationFrame</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents one level of nesting in the indentation stack.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    indent_level: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    structure_type: YAMLStructureType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data: Union[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], List[Any]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line_number: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_mapping_entry</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add a key-value pair to this mapping frame.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.structure_type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> YAMLStructureType.</span><span style=\"color:#79B8FF\">MAPPING</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot add mapping entry to </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.structure_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_sequence_item</span><span style=\"color:#E1E4E8\">(self, item: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add an item to this sequence frame.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.structure_type </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> YAMLStructureType.</span><span style=\"color:#79B8FF\">SEQUENCE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot add sequence item to </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.structure_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data.append(item)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IndentationStack</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages the stack of indentation contexts during YAML parsing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stack: List[IndentationFrame] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.established_levels: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Track valid dedent levels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_level</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the current indentation level.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].indent_level </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> current_frame</span><span style=\"color:#E1E4E8\">(self) -> Optional[IndentationFrame]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the current stack frame.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> push_frame</span><span style=\"color:#E1E4E8\">(self, indent_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, structure_type: YAMLStructureType, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> IndentationFrame:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Push a new indentation frame onto the stack.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> structure_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> YAMLStructureType.</span><span style=\"color:#79B8FF\">MAPPING</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> structure_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> YAMLStructureType.</span><span style=\"color:#79B8FF\">SEQUENCE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        frame </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> IndentationFrame(indent_level, structure_type, data, line_number)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stack.append(frame)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> indent_level </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.established_levels:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.established_levels.append(indent_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> frame</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pop_to_level</span><span style=\"color:#E1E4E8\">(self, target_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> List[IndentationFrame]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Pop frames until reaching the target indentation level.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> target_level </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.established_levels:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> StructureError(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Invalid dedent to level </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">target_level</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">. Valid levels: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.established_levels</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                position</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                suggestion</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Use one of these indentation levels: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.established_levels</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        popped_frames </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].indent_level </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> target_level:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            popped_frames.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.stack.pop())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> popped_frames</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_root_data</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get the root document data structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].data </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.stack </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Type inference utilities for YAML scalar processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLTypeInference</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles automatic type detection and conversion for YAML scalars.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN_VALUES</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'true'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'false'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'yes'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'no'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'on'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'off'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'y'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'n'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NULL_VALUES</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'null'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'~'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> infer_type</span><span style=\"color:#E1E4E8\">(cls, value_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert string value to appropriate Python type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(value_str, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> value_str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Remove leading/trailing whitespace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        clean_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value_str.strip()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle empty values and null</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> clean_value </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">NULL_VALUES</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle boolean values (case insensitive)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lower_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> clean_value.lower()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> lower_value </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">BOOLEAN_VALUES</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">BOOLEAN_VALUES</span><span style=\"color:#E1E4E8\">[lower_value]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Handle numeric values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Try integer first</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#F97583\"> not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> clean_value </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> 'e'</span><span style=\"color:#F97583\"> not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> lower_value:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(clean_value)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Try float</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(clean_value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return as string if no other type matches</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> clean_value</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Complete error handling infrastructure for YAML-specific errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLIndentationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">StructureError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error in YAML indentation structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, current_levels: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            message</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">message,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            position</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">Position(</span><span style=\"color:#FFAB70\">line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">line_number, </span><span style=\"color:#FFAB70\">column</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            suggestion</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Valid indentation levels are: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">current_levels</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_levels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_levels</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLTypeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error in YAML type inference or conversion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>The main YAML parser implementation with detailed TODO comments mapping to the algorithm steps:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"YAML subset parser with indentation-based structure tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stack </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> IndentationStack()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.type_inference </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> YAMLTypeInference()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_line </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse YAML content into nested dictionary structure.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            content: Raw YAML text content</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Parsed configuration as nested dictionary</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            YAMLIndentationError: For indentation structure violations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            YAMLTypeError: For type inference failures</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            SyntaxError: For general syntax violations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Split content into logical lines, handling line continuations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize document root frame on indentation stack</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Process each logical line through indentation analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle context transitions based on indentation changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Parse line content based on current structural context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate final document structure completeness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return root data structure from stack</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self._process_line() for each line after indentation analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Track line numbers for error reporting throughout parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _analyze_line_indentation</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze line indentation and extract content.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            line: Raw line text</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            line_number: Line number for error reporting</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Tuple of (indentation_level, content_text)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            YAMLIndentationError: For invalid indentation patterns</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Count leading whitespace characters (spaces only, reject tabs)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate indentation character consistency (no tab mixing)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract content portion after removing indentation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return indentation level and cleaned content</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use enumerate() to track character positions for error reporting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Raise YAMLIndentationError for tab characters with helpful message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_indentation_transition</span><span style=\"color:#E1E4E8\">(self, current_indent: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle stack transitions based on indentation level changes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            current_indent: Indentation level of current line</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            line_number: Current line number for error context</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compare current indentation to stack top level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If deeper indentation, prepare for nested structure (don't push yet)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If same indentation, continue current structure context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If shallower indentation, pop stack frames to matching level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate dedent targets against established indentation levels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.stack.pop_to_level() for dedent operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Defer frame pushing until content type is determined</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_line_content</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, indent_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Process line content based on YAML syntax patterns.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            content: Line content after indentation removal</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            indent_level: Indentation level of this line</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            line_number: Line number for error reporting</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Skip empty lines and comment lines (starting with #)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Detect line type: mapping (key:), sequence (-), or scalar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle mapping entries by parsing key-value pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle sequence items by parsing list elements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle scalar values with type inference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create appropriate stack frames for nested structures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use regex patterns to identify mapping vs sequence vs scalar</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Call self._parse_mapping_line() or self._parse_sequence_line()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_mapping_line</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, indent_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse a mapping line (key: value format).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Split content on first colon to separate key and value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate key format and handle quoted keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Trim whitespace from key and value portions  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If no current mapping frame, create one at this indent level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If value is empty, prepare for nested structure on next line</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If value present, apply type inference and store mapping entry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Handle inline flow syntax in values (basic arrays/objects)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.stack.current_frame() to check current context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Empty values after colon indicate nested structure follows</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _parse_sequence_line</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, indent_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse a sequence line (- item format).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Remove leading dash and whitespace to get item content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If no current sequence frame, create one at this indent level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If item content is empty, prepare for nested structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If item content present, apply type inference and add to sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle complex sequence items that are mappings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Sequence items can contain nested mappings or other sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use lookahead to determine if item is scalar or complex structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p>Python-specific techniques for effective YAML parser implementation:</p>\n<ul>\n<li><strong>String processing</strong>: Use <code>str.lstrip()</code> to remove leading whitespace, but manually count characters to distinguish tabs from spaces</li>\n<li><strong>Stack management</strong>: Python lists work perfectly as stacks with <code>append()</code> and <code>pop()</code> operations</li>\n<li><strong>Type inference</strong>: Use <code>isinstance()</code> checks and <code>try/except</code> blocks for numeric conversion attempts  </li>\n<li><strong>Regular expressions</strong>: Import <code>re</code> module for pattern matching mapping syntax (<code>key:</code>) and sequence syntax (<code>-</code>)</li>\n<li><strong>Error context</strong>: Use <code>enumerate()</code> when iterating over lines to maintain line number tracking</li>\n<li><strong>Unicode handling</strong>: Python 3 strings handle Unicode automatically, but be aware of Unicode whitespace characters beyond ASCII space and tab</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the YAML parser component, verify functionality with these concrete tests:</p>\n<p><strong>Basic parsing verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from parsers.yaml_parser import YAMLParser</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">parser = YAMLParser()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">result = parser.parse('name: test\\nage: 25\\nactive: true')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Parsed:', result)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">assert result == {'name': 'test', 'age': 25, 'active': True}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ Basic parsing works')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Indentation structure testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from parsers.yaml_parser import YAMLParser</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">yaml_content = '''</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">database:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  host: localhost</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  port: 5432</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  credentials:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    username: admin</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    password: secret</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">'''</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">parser = YAMLParser()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">result = parser.parse(yaml_content.strip())</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Nested result:', result)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">assert result['database']['credentials']['username'] == 'admin'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('✓ Nested indentation works')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Expected behavior verification:</strong></p>\n<ul>\n<li>Parser should handle 2-space, 4-space, or consistent indentation amounts</li>\n<li>Type inference should convert <code>25</code> to integer, <code>true</code> to boolean, quoted strings to strings</li>\n<li>Indentation errors should provide helpful error messages showing valid indent levels</li>\n<li>Mixed tab/space indentation should be rejected with clear error explaining the problem</li>\n</ul>\n<p><strong>Signs of implementation problems:</strong></p>\n<ul>\n<li>&quot;KeyError&quot; exceptions usually indicate missing stack frame management</li>\n<li>&quot;Indentation level mismatch&quot; errors suggest issues with dedent level validation  </li>\n<li>Type conversion errors indicate problems in the type inference logic</li>\n<li>Stack overflow suggests infinite recursion in nested structure handling</li>\n</ul>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - this section defines how all components work together to create a unified parsing system</p>\n</blockquote>\n<p>Think of the complete parsing system as an assembly line in a specialized translation factory. Raw configuration files enter at one end as unstructured text, pass through a series of specialized workstations (format detection, tokenization, parsing, and structure conversion), and emerge as clean, standardized nested dictionaries. Each workstation has a specific expertise and passes its refined output to the next station, with quality control checkpoints that can halt the entire line if defects are detected. The beauty of this assembly line is that while each workstation uses different techniques internally, they all communicate through standardized interfaces, allowing the line to handle three completely different &quot;product types&quot; (INI, TOML, YAML) while producing identical output formats.</p>\n<p>The interactions between components follow a carefully orchestrated pipeline where data flows through well-defined interfaces, errors are enriched with context as they propagate upward, and format detection automatically routes content to the appropriate specialized parser. This design enables clean separation of concerns while maintaining a unified experience for users of the parsing library.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Component Architecture\"></p>\n<h3 id=\"complete-parsing-pipeline\">Complete Parsing Pipeline</h3>\n<p>The complete parsing pipeline represents the end-to-end journey from raw configuration file content to fully processed nested dictionary structures. This pipeline consists of several distinct phases, each with specific responsibilities and well-defined interfaces for data exchange.</p>\n<p>Think of this pipeline as a sophisticated mail sorting facility. Raw mail (configuration content) arrives in various formats and languages (INI, TOML, YAML). The facility first examines each piece to determine its origin language and routing requirements (format detection). Next, specialized language experts break down the content into meaningful units (tokenization). Then, grammar specialists organize these units into logical structures (parsing). Finally, translators convert everything into a common internal language (unified output format) that the rest of the organization can understand. At each stage, quality inspectors check for problems and can route defective items to error handling processes.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fparsing-pipeline.svg\" alt=\"Complete Parsing Pipeline Flow\"></p>\n<h4 id=\"pipeline-phase-breakdown\">Pipeline Phase Breakdown</h4>\n<p>The parsing pipeline operates through five distinct phases, each building upon the output of the previous phase:</p>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Input</th>\n<th>Output</th>\n<th>Primary Responsibility</th>\n<th>Error Types</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Content Ingestion</td>\n<td>File path or raw string</td>\n<td>String content with encoding normalization</td>\n<td>Read file content, handle encoding, normalize line endings</td>\n<td>IO errors, encoding errors</td>\n</tr>\n<tr>\n<td>Format Detection</td>\n<td>String content</td>\n<td>Format identifier (ini/toml/yaml)</td>\n<td>Analyze content patterns to determine format</td>\n<td>Ambiguous format errors</td>\n</tr>\n<tr>\n<td>Tokenization</td>\n<td>String content + format</td>\n<td>List of typed tokens with positions</td>\n<td>Break content into meaningful lexical units</td>\n<td>Lexical errors, invalid characters</td>\n</tr>\n<tr>\n<td>Parsing</td>\n<td>Token stream + format</td>\n<td>Parse tree structure</td>\n<td>Build hierarchical structure from tokens</td>\n<td>Syntax errors, structure errors</td>\n</tr>\n<tr>\n<td>Output Conversion</td>\n<td>Parse tree</td>\n<td>Unified dictionary structure</td>\n<td>Convert format-specific structures to common representation</td>\n<td>Type conversion errors, structure mapping errors</td>\n</tr>\n</tbody></table>\n<p>The main entry point <code>parse_config(content, format=None)</code> orchestrates this entire pipeline, handling the coordination between phases and managing error propagation. When a format is not explicitly specified, the pipeline automatically detects the format before proceeding to tokenization.</p>\n<h4 id=\"data-transformation-flow\">Data Transformation Flow</h4>\n<p>Each phase transforms data from one representation to another, with specific rules governing these transformations:</p>\n<p><strong>Content Ingestion Transformation</strong>: Raw file content undergoes normalization to ensure consistent processing regardless of source encoding or line ending conventions. The ingestion phase converts all content to UTF-8 encoding and normalizes line endings to Unix-style newlines. This normalization prevents encoding-related parsing errors in later phases.</p>\n<p><strong>Format Detection Transformation</strong>: Normalized content is analyzed using format-specific heuristics to produce a format identifier. This transformation is read-only - the content remains unchanged, but metadata about its format is added to guide subsequent processing decisions.</p>\n<p><strong>Tokenization Transformation</strong>: String content is decomposed into a sequence of typed tokens, each carrying a specific semantic meaning and position information. This transformation is lossy in the sense that whitespace semantics and comment placement may be simplified, but all semantically meaningful content is preserved with enhanced type information.</p>\n<p><strong>Parsing Transformation</strong>: The token stream is restructured into a hierarchical parse tree that reflects the logical organization of the configuration data. This transformation handles the complex task of resolving nested structures, managing scope transitions, and validating structural consistency according to format-specific rules.</p>\n<p><strong>Output Conversion Transformation</strong>: Format-specific parse tree structures are normalized into a unified nested dictionary representation. This transformation handles impedance mismatch between different format paradigms, ensuring that sections, tables, and mappings all produce equivalent dictionary structures.</p>\n<h4 id=\"pipeline-state-management\">Pipeline State Management</h4>\n<p>The parsing pipeline maintains state information that flows between phases to enable error reporting, debugging, and incremental processing:</p>\n<table>\n<thead>\n<tr>\n<th>State Component</th>\n<th>Purpose</th>\n<th>Maintained By</th>\n<th>Used By</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Source Position</td>\n<td>Track current location in original content</td>\n<td>Tokenizer</td>\n<td>Parser, Error Reporter</td>\n</tr>\n<tr>\n<td>Format Context</td>\n<td>Remember detected format and configuration options</td>\n<td>Format Detector</td>\n<td>Tokenizer, Parser</td>\n</tr>\n<tr>\n<td>Symbol Tables</td>\n<td>Track defined keys and tables for conflict detection</td>\n<td>Parser</td>\n<td>Parser (validation)</td>\n</tr>\n<tr>\n<td>Error Context</td>\n<td>Accumulate errors with source positions</td>\n<td>All phases</td>\n<td>Error Reporter</td>\n</tr>\n<tr>\n<td>Parse Stack</td>\n<td>Maintain parsing context for nested structures</td>\n<td>Parser</td>\n<td>Parser (recursive descent)</td>\n</tr>\n</tbody></table>\n<p>The pipeline uses a <strong>context object</strong> that flows through all phases, accumulating information and providing shared services. This context object implements the following interface:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>get_position()</code></td>\n<td>None</td>\n<td><code>Position</code></td>\n<td>Get current source position</td>\n</tr>\n<tr>\n<td><code>set_position(pos)</code></td>\n<td><code>pos: Position</code></td>\n<td>None</td>\n<td>Update current source position</td>\n</tr>\n<tr>\n<td><code>add_error(error)</code></td>\n<td><code>error: ParseError</code></td>\n<td>None</td>\n<td>Record error with current context</td>\n</tr>\n<tr>\n<td><code>get_errors()</code></td>\n<td>None</td>\n<td><code>List[ParseError]</code></td>\n<td>Retrieve all accumulated errors</td>\n</tr>\n<tr>\n<td><code>get_format()</code></td>\n<td>None</td>\n<td><code>str</code></td>\n<td>Get detected format identifier</td>\n</tr>\n<tr>\n<td><code>set_format(fmt)</code></td>\n<td><code>fmt: str</code></td>\n<td>None</td>\n<td>Set format for pipeline</td>\n</tr>\n<tr>\n<td><code>get_options()</code></td>\n<td>None</td>\n<td><code>dict</code></td>\n<td>Get format-specific parsing options</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The context object serves as both a communication channel and a memory mechanism, allowing later phases to access information discovered by earlier phases while maintaining loose coupling between components.</p>\n</blockquote>\n<h4 id=\"error-recovery-in-pipeline\">Error Recovery in Pipeline</h4>\n<p>The parsing pipeline implements <strong>graceful degradation</strong> where errors in one phase don&#39;t necessarily halt the entire pipeline. Each phase can choose to continue processing despite encountering errors, allowing the collection of multiple error reports in a single parsing attempt:</p>\n<ol>\n<li><p><strong>Content Ingestion</strong>: IO errors or encoding errors immediately halt the pipeline, as no further processing is possible without readable content.</p>\n</li>\n<li><p><strong>Format Detection</strong>: Ambiguous format detection doesn&#39;t halt the pipeline; instead, the system defaults to INI format (simplest) and continues, recording a warning about the format assumption.</p>\n</li>\n<li><p><strong>Tokenization</strong>: Invalid characters or lexical errors are recorded as <code>TokenError</code> instances, but tokenization continues by inserting <code>INVALID</code> tokens and resuming at the next recognizable boundary.</p>\n</li>\n<li><p><strong>Parsing</strong>: Syntax errors are recorded as <code>SyntaxError</code> instances, and the parser attempts to resynchronize by finding the next stable parsing boundary (section header, top-level key, etc.).</p>\n</li>\n<li><p><strong>Output Conversion</strong>: Type conversion errors are recorded as warnings, with problematic values preserved as strings in the final output.</p>\n</li>\n</ol>\n<p>This error recovery strategy enables the pipeline to provide comprehensive feedback about multiple issues in a single parsing attempt, significantly improving the developer experience when debugging configuration files.</p>\n<h3 id=\"format-detection-strategy\">Format Detection Strategy</h3>\n<p>Format detection serves as the intelligent routing mechanism that automatically determines which specialized parser should handle incoming configuration content. Think of format detection as an experienced librarian who can instantly recognize whether a book is written in English, French, or German just by glancing at a few pages - they don&#39;t need to read the entire book, just identify the distinctive patterns that reveal the language.</p>\n<p>The format detection system analyzes content using a combination of <strong>syntactic signatures</strong> and <strong>statistical patterns</strong> to make confident format determinations even with partial or ambiguous content. This approach is crucial because configuration files often contain overlapping syntactic elements (all three formats support key-value pairs and comments), requiring sophisticated heuristics to distinguish between them.</p>\n<h4 id=\"detection-algorithm-strategy\">Detection Algorithm Strategy</h4>\n<p>The <code>detect_format(content)</code> function implements a <strong>multi-pass analysis strategy</strong> that examines different aspects of the content to build confidence in format identification:</p>\n<ol>\n<li><strong>Signature Scanning Pass</strong>: Look for unambiguous format-specific signatures that immediately identify the format</li>\n<li><strong>Pattern Analysis Pass</strong>: Analyze the frequency and distribution of format-specific patterns</li>\n<li><strong>Conflict Resolution Pass</strong>: Resolve ambiguities using format-specific tie-breaking rules</li>\n<li><strong>Confidence Assessment Pass</strong>: Evaluate the reliability of the detection and flag uncertain cases</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Detection Pass</th>\n<th>TOML Signatures</th>\n<th>INI Signatures</th>\n<th>YAML Signatures</th>\n<th>Weight</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Signature Scanning</td>\n<td><code>[[array.of.tables]]</code>, <code>key.dotted.path =</code>, <code>{ inline = &quot;table&quot; }</code></td>\n<td><code>[section]</code> without dotted paths, <code>key=value</code> without quotes</td>\n<td><code>key: value</code>, <code>- list item</code>, indentation-based nesting</td>\n<td>High (90% confidence)</td>\n</tr>\n<tr>\n<td>Pattern Analysis</td>\n<td>Quoted keys, date-time literals, mixed-type arrays</td>\n<td>Semicolon comments, unquoted values, flat sections</td>\n<td>Flow syntax <code>{}[]</code>, implicit type inference patterns</td>\n<td>Medium (70% confidence)</td>\n</tr>\n<tr>\n<td>Conflict Resolution</td>\n<td>Complex table structures, type-specific literals</td>\n<td>Simple key-value with minimal nesting</td>\n<td>Indentation consistency, mapping vs sequence patterns</td>\n<td>Low (50% confidence)</td>\n</tr>\n</tbody></table>\n<p>The detection algorithm processes these passes sequentially, stopping early if high-confidence signatures are found, or continuing through all passes for ambiguous content.</p>\n<h4 id=\"format-specific-detection-rules\">Format-Specific Detection Rules</h4>\n<p>Each format has distinctive characteristics that enable reliable detection:</p>\n<p><strong>TOML Detection Signatures</strong>:</p>\n<ul>\n<li><strong>Definitive signatures</strong>: Double-bracket array-of-tables syntax <code>[[database.servers]]</code>, dotted table paths <code>[tool.poetry.dependencies]</code>, and inline tables <code>person = { name = &quot;Tom&quot;, age = 30 }</code></li>\n<li><strong>Strong indicators</strong>: Quoted keys using double or single quotes, date-time literals with timezone information, and mixed-type arrays with explicit type annotations</li>\n<li><strong>Supporting patterns</strong>: Underscores in numeric literals <code>1_000_000</code>, multi-line strings with triple quotes, and boolean values using lowercase <code>true/false</code></li>\n</ul>\n<p><strong>INI Detection Signatures</strong>:</p>\n<ul>\n<li><strong>Definitive signatures</strong>: Simple bracketed sections <code>[Section Name]</code> without dots, semicolon-prefixed comments <code>;comment</code>, and unquoted string values</li>\n<li><strong>Strong indicators</strong>: Key-value pairs using colon syntax <code>key: value</code>, global keys appearing before any section headers, and minimal use of nested structures</li>\n<li><strong>Supporting patterns</strong>: Quoted string values using double quotes only, numeric values without type prefixes, and flat organizational structure</li>\n</ul>\n<p><strong>YAML Detection Signatures</strong>:</p>\n<ul>\n<li><strong>Definitive signatures</strong>: Indentation-based nesting with consistent spaces, list items prefixed with dash and space <code>- item</code>, and mapping syntax using colon-space <code>key: value</code></li>\n<li><strong>Strong indicators</strong>: Flow syntax mixing with block syntax, implicit type inference for <code>yes/no/on/off</code>, and multi-document separators <code>---</code></li>\n<li><strong>Supporting patterns</strong>: Quoted strings using single or double quotes interchangeably, null values represented as <code>~</code> or <code>null</code>, and folded/literal string syntax <code>|</code> and <code>&gt;</code></li>\n</ul>\n<blockquote>\n<p><strong>Decision: Multi-Pass Detection Algorithm</strong></p>\n<ul>\n<li><strong>Context</strong>: Single-pass detection often fails on ambiguous files that mix format conventions or have minimal content</li>\n<li><strong>Options Considered</strong>: Single regex-based detection, machine learning classifier, multi-pass heuristic analysis</li>\n<li><strong>Decision</strong>: Multi-pass heuristic analysis with signature scanning and pattern analysis</li>\n<li><strong>Rationale</strong>: Provides high accuracy without external dependencies, handles edge cases gracefully, and offers explainable detection results</li>\n<li><strong>Consequences</strong>: More complex implementation but significantly better accuracy on real-world configuration files with mixed conventions</li>\n</ul>\n</blockquote>\n<h4 id=\"detection-confidence-and-fallback-strategy\">Detection Confidence and Fallback Strategy</h4>\n<p>The format detection system provides <strong>confidence scoring</strong> for its determinations, allowing the parsing pipeline to make informed decisions about how to handle uncertain cases:</p>\n<table>\n<thead>\n<tr>\n<th>Confidence Level</th>\n<th>Score Range</th>\n<th>Action Taken</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High Confidence</td>\n<td>90-100%</td>\n<td>Proceed with detected format</td>\n<td>Standard error reporting</td>\n</tr>\n<tr>\n<td>Medium Confidence</td>\n<td>70-89%</td>\n<td>Proceed with warning logged</td>\n<td>Enhanced error context</td>\n</tr>\n<tr>\n<td>Low Confidence</td>\n<td>50-69%</td>\n<td>Proceed with user notification</td>\n<td>Suggest manual format specification</td>\n</tr>\n<tr>\n<td>Ambiguous</td>\n<td>Below 50%</td>\n<td>Default to INI with warning</td>\n<td>Multiple format attempt on failure</td>\n</tr>\n</tbody></table>\n<p>The fallback strategy implements <strong>graceful degradation</strong> where low-confidence detections still allow parsing to proceed, but with enhanced error reporting that includes format detection uncertainty in error messages.</p>\n<h4 id=\"ambiguity-resolution-techniques\">Ambiguity Resolution Techniques</h4>\n<p>When content exhibits patterns consistent with multiple formats, the detection system applies format-specific tie-breaking rules:</p>\n<p><strong>INI vs TOML Resolution</strong>: INI format takes precedence when content uses simple bracketed sections without dotted paths and avoids TOML-specific features like inline tables or complex data types. TOML format takes precedence when dotted notation appears in section names or when complex data structures are present.</p>\n<p><strong>YAML vs INI Resolution</strong>: YAML format takes precedence when consistent indentation-based nesting is detected or when list syntax with dash prefixes appears. INI format takes precedence when content is predominantly flat with minimal nesting and uses semicolon-style comments.</p>\n<p><strong>TOML vs YAML Resolution</strong>: This represents the most complex ambiguity case. TOML format takes precedence when section headers use bracket notation and when explicit type annotations appear. YAML format takes precedence when indentation-based structure dominates and when implicit type inference patterns are prevalent.</p>\n<h3 id=\"error-information-flow\">Error Information Flow</h3>\n<p>Error handling in the configuration parser implements a <strong>enriched propagation model</strong> where errors are detected at their source, progressively enhanced with context information as they move up the component stack, and finally transformed into user-friendly diagnostic messages. Think of this as a hospital&#39;s patient information system: when a problem is first detected in a lab test, it starts with raw technical data, but as it moves through specialists and ultimately to the patient&#39;s doctor, each step adds context, interpretation, and actionable guidance until the final message is both technically accurate and humanly comprehensible.</p>\n<p>The error information flow ensures that users receive not just notification that something went wrong, but specific guidance about what the problem is, where it occurred, why it happened, and how to fix it. This comprehensive error reporting significantly reduces debugging time and improves the developer experience when working with configuration files.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Detection and Reporting Flow\"></p>\n<h4 id=\"error-detection-points\">Error Detection Points</h4>\n<p>Errors can originate from multiple points in the parsing pipeline, each with different characteristics and context requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Detection Point</th>\n<th>Error Types</th>\n<th>Available Context</th>\n<th>Enhancement Needed</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Content Ingestion</td>\n<td>IO errors, encoding errors</td>\n<td>File path, system error codes</td>\n<td>User-friendly file access guidance</td>\n</tr>\n<tr>\n<td>Format Detection</td>\n<td>Ambiguous format, unknown patterns</td>\n<td>Content sample, detection confidence scores</td>\n<td>Format suggestion with reasoning</td>\n</tr>\n<tr>\n<td>Tokenization</td>\n<td>Invalid characters, malformed literals</td>\n<td>Character position, token context</td>\n<td>Visual indication of problem location</td>\n</tr>\n<tr>\n<td>Parsing</td>\n<td>Syntax errors, structure violations</td>\n<td>Token stream, parsing state</td>\n<td>Grammar explanation and fix suggestions</td>\n</tr>\n<tr>\n<td>Type Conversion</td>\n<td>Invalid values, type mismatches</td>\n<td>Value context, expected type</td>\n<td>Type conversion guidance and examples</td>\n</tr>\n</tbody></table>\n<p>Each detection point creates errors using format-specific error types that inherit from the base <code>ParseError</code> class, ensuring consistent interface while enabling specialized handling.</p>\n<h4 id=\"error-context-enrichment\">Error Context Enrichment</h4>\n<p>As errors propagate up through the component stack, each level adds contextual information that makes the error more actionable for end users:</p>\n<p><strong>Position Enhancement</strong>: Raw character offsets from tokenization are converted to human-readable line and column numbers. The <code>current_position(source, offset)</code> function calculates these coordinates and creates <code>Position</code> objects that track line, column, and absolute offset information.</p>\n<p><strong>Source Context Addition</strong>: The <code>create_error_context(source, position, context_lines=2)</code> function generates visual error context by extracting surrounding lines from the source content and highlighting the specific problem location. This creates the familiar &quot;caret pointer&quot; display that shows exactly where the error occurred.</p>\n<p><strong>Suggestion Generation</strong>: Each component adds format-specific suggestions based on common error patterns. For example, TOML parsing errors include suggestions about table redefinition rules, while YAML parsing errors include guidance about indentation requirements.</p>\n<p><strong>Error Classification Refinement</strong>: Generic parsing errors are reclassified into specific error types (<code>TokenError</code>, <code>SyntaxError</code>, <code>StructureError</code>) that enable targeted error handling and user guidance.</p>\n<h4 id=\"error-propagation-strategy\">Error Propagation Strategy</h4>\n<p>The error propagation system implements <strong>structured error accumulation</strong> where multiple errors can be collected and reported together, rather than halting on the first error encountered:</p>\n<table>\n<thead>\n<tr>\n<th>Propagation Level</th>\n<th>Responsibilities</th>\n<th>Error Transformation</th>\n<th>Context Added</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Component Level</td>\n<td>Detect and classify errors</td>\n<td>Raw errors → typed errors</td>\n<td>Component-specific context</td>\n</tr>\n<tr>\n<td>Parser Level</td>\n<td>Coordinate error collection</td>\n<td>Individual errors → error collections</td>\n<td>Parsing state context</td>\n</tr>\n<tr>\n<td>Pipeline Level</td>\n<td>Aggregate cross-component errors</td>\n<td>Component errors → unified reports</td>\n<td>Source file context</td>\n</tr>\n<tr>\n<td>API Level</td>\n<td>Format for end-user consumption</td>\n<td>Technical errors → user guidance</td>\n<td>Help text and examples</td>\n</tr>\n</tbody></table>\n<p>The propagation strategy uses <strong>error aggregation objects</strong> that collect related errors and provide unified reporting:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Error Report Structure:\n- Primary Error: The main issue that prevented successful parsing\n- Secondary Errors: Related issues that may have contributed to the primary error  \n- Context Information: Source location, surrounding content, parsing state\n- Suggestions: Actionable guidance for resolving the error\n- Related Documentation: Links to format specifications or examples</code></pre></div>\n\n<h4 id=\"error-recovery-and-continuation\">Error Recovery and Continuation</h4>\n<p>The error handling system implements <strong>intelligent recovery</strong> that allows parsing to continue after encountering errors, enabling the collection of multiple error reports in a single parsing attempt:</p>\n<p><strong>Tokenization Recovery</strong>: When invalid characters are encountered, the tokenizer inserts <code>INVALID</code> tokens and attempts to resynchronize at the next recognizable token boundary. This allows parsing to continue and potentially identify additional errors downstream.</p>\n<p><strong>Parsing Recovery</strong>: When syntax errors occur, parsers attempt to resynchronize at stable parsing boundaries such as section headers, top-level keys, or significant structural markers. The recovery strategy varies by format:</p>\n<ul>\n<li><strong>INI Recovery</strong>: Resynchronize at section headers <code>[section]</code> or line boundaries for key-value pairs</li>\n<li><strong>TOML Recovery</strong>: Resynchronize at table declarations, top-level keys, or array-of-tables markers  </li>\n<li><strong>YAML Recovery</strong>: Resynchronize at consistent indentation levels or document boundaries</li>\n</ul>\n<p><strong>Structure Recovery</strong>: When structural errors occur (like conflicting table definitions in TOML), the parser records the conflict but continues processing other parts of the file, allowing detection of multiple structural issues.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Error recovery is particularly valuable during configuration development and debugging, where users benefit from seeing all issues at once rather than fixing them one at a time through multiple parsing attempts.</p>\n</blockquote>\n<h4 id=\"user-friendly-error-formatting\">User-Friendly Error Formatting</h4>\n<p>The final stage of error information flow transforms technical error details into user-friendly diagnostic messages that follow established conventions for compiler and parser error reporting:</p>\n<p><strong>Error Message Structure</strong>:</p>\n<ol>\n<li><strong>Problem Summary</strong>: One-line description of what went wrong</li>\n<li><strong>Location Information</strong>: File, line, and column where the error occurred  </li>\n<li><strong>Source Context</strong>: Visual display of the problematic source with error highlighting</li>\n<li><strong>Explanation</strong>: Detailed description of why this is an error</li>\n<li><strong>Suggestions</strong>: Specific guidance on how to fix the problem</li>\n<li><strong>Related Information</strong>: Links to documentation or similar error examples</li>\n</ol>\n<p><strong>Visual Error Context Example</strong>:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Error: Invalid table redefinition in TOML\n  → config.toml:15:1\n\n   13 | [database]\n   14 | host = &quot;localhost&quot;  \n   15 | [database]\n      | ^^^^^^^^^^\n   16 | port = 5432\n\nExplanation: Table 'database' has already been defined on line 13. TOML does not allow redefining tables once they have been created.\n\nSuggestion: Use dotted notation [database.connection] to create subtables, or merge the key-value pairs into the existing [database] section.</code></pre></div>\n\n<p>This formatting approach provides immediate visual identification of the problem location, clear explanation of the underlying issue, and actionable guidance for resolution.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fdata-structure-mapping.svg\" alt=\"Data Structure Transformation\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The interactions and data flow components require careful coordination between multiple parsing subsystems. This section provides the foundational infrastructure and integration patterns needed to build a cohesive parsing pipeline.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Orchestration</td>\n<td>Function-based pipeline with explicit error passing</td>\n<td>Class-based pipeline with context objects and middleware</td>\n</tr>\n<tr>\n<td>Format Detection</td>\n<td>Regex-based pattern matching with scoring</td>\n<td>Statistical analysis with machine learning features</td>\n</tr>\n<tr>\n<td>Error Reporting</td>\n<td>String-based error messages with position info</td>\n<td>Rich error objects with suggestions and context</td>\n</tr>\n<tr>\n<td>Context Management</td>\n<td>Global variables or parameter threading</td>\n<td>Context objects with scoped state management</td>\n</tr>\n<tr>\n<td>Data Flow Coordination</td>\n<td>Direct function calls with return values</td>\n<td>Event-driven architecture with observer pattern</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-module-structure\">Recommended Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config_parser/\n  __init__.py                    ← Public API exports\n  pipeline/\n    __init__.py\n    core.py                     ← Main pipeline orchestration\n    context.py                  ← Pipeline context management\n    errors.py                   ← Error handling infrastructure\n  detection/\n    __init__.py\n    detector.py                 ← Format detection logic\n    signatures.py              ← Format-specific signatures\n    heuristics.py              ← Detection heuristics\n  parsers/\n    __init__.py\n    ini_parser.py              ← INI format parser\n    toml_parser.py             ← TOML format parser  \n    yaml_parser.py             ← YAML format parser\n    base.py                    ← Common parser interface\n  tokenizer/\n    __init__.py\n    tokenizer.py               ← Main tokenization engine\n    tokens.py                  ← Token definitions\n  utils/\n    __init__.py\n    position.py                ← Position tracking utilities\n    conversion.py              ← Data structure conversion\n  tests/\n    test_pipeline.py           ← Integration tests\n    test_detection.py          ← Format detection tests\n    test_errors.py             ← Error handling tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Pipeline Context Management</strong> (Complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># pipeline/context.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Central context object that flows through the entire parsing pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_content: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_position: Optional[</span><span style=\"color:#9ECBFF\">'Position'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    detected_format: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    options: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors: List[ParseError] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warnings: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_position</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'Position'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current source position, creating default if none exists.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_position </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            from</span><span style=\"color:#E1E4E8\"> ..utils.position </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.current_position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Position(</span><span style=\"color:#FFAB70\">line</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">column</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_position</span><span style=\"color:#E1E4E8\">(self, pos: </span><span style=\"color:#9ECBFF\">'Position'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update current source position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pos</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_error</span><span style=\"color:#E1E4E8\">(self, error: ParseError) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record error with current context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> error.position </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#F97583\"> and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_position </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            error.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors.append(error)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_warning</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record warning message.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.warnings.append(message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> has_errors</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if any errors have been recorded.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.errors) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_errors</span><span style=\"color:#E1E4E8\">(self) -> List[ParseError]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve all accumulated errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.errors.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_format</span><span style=\"color:#E1E4E8\">(self) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get detected format identifier.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.detected_format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_format</span><span style=\"color:#E1E4E8\">(self, fmt: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set format for pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.detected_format </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_options</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get format-specific parsing options.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.options.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_option</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set format-specific parsing option.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.options[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># pipeline/core.py  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .context </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseContext</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseError, ConfigurationError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..detection.detector </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> detect_format</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..parsers.ini_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> INIParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..parsers.toml_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TOMLParser  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..parsers.yaml_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> YAMLParser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse_config</span><span style=\"color:#E1E4E8\">(content: Union[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Path], format: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Main configuration parsing entry point.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        content: Configuration content as string or file path</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        format: Optional format specification ('ini', 'toml', 'yaml')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Nested dictionary representing the configuration structure</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ConfigurationError: When parsing fails with detailed error information</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Handle content input - if Path, read file; if str, use directly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create ParseContext with content and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect format if not explicitly provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Route to appropriate parser based on detected format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle parsing errors and create comprehensive error reports</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return unified dictionary structure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _read_content</span><span style=\"color:#E1E4E8\">(source: Union[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Path]) -> tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Read configuration content from file or use string directly.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns (content, file_path) tuple.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement file reading with encoding detection and normalization</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _create_parser</span><span style=\"color:#E1E4E8\">(format_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'BaseParser'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create appropriate parser instance based on format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Factory method for parser creation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _handle_parsing_errors</span><span style=\"color:#E1E4E8\">(context: ParseContext) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Process accumulated errors and create user-friendly reports.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Transform technical errors into user guidance</span></span></code></pre></div>\n\n<p><strong>Error Infrastructure</strong> (Complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># pipeline/errors.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Source position information for error reporting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parsing errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggestion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.position:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.position</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.suggestion:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Suggestion: </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.suggestion</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error during tokenization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error during syntax analysis phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error during structure building phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConfigurationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"High-level configuration parsing failure with multiple error details.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, errors: List[ParseError], source_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source_path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create summary message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Configuration parsing failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">errors[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Configuration parsing failed with </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(errors)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> errors\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> format_detailed_report</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate comprehensive error report for user display.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create detailed multi-error report with source context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_error_context</span><span style=\"color:#E1E4E8\">(source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Position, context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate visual error context showing source location.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Split source into lines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate which lines to show (position +/- context_lines) </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Format with line numbers and error indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return formatted context string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Format Detection Implementation</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># detection/detector.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Tuple, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FormatConfidence</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HIGH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 90</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MEDIUM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 70</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOW</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AMBIGUOUS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 25</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_format</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Automatically detect configuration file format from content.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        content: Raw configuration file content</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Format identifier ('ini', 'toml', 'yaml')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ValueError: When format cannot be determined with confidence</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Run signature scanning pass for definitive format markers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run pattern analysis pass for format-specific characteristics  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run conflict resolution pass for ambiguous cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Evaluate confidence and return format or raise error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use _scan_format_signatures() for each format, combine scores</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _scan_format_signatures</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Scan for definitive format signatures and return confidence scores.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns dict with format names as keys, confidence scores as values.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Define regex patterns for TOML signatures ([[tables]], dotted.keys, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Define regex patterns for INI signatures ([sections], key=value, etc.) </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Define regex patterns for YAML signatures (key:, -, indentation, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Count matches for each format and calculate confidence scores</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return scores dict for conflict resolution</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _analyze_format_patterns</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Analyze statistical patterns for format identification.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns confidence adjustments based on pattern frequency.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement pattern frequency analysis</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> _resolve_format_conflicts</span><span style=\"color:#E1E4E8\">(scores: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Apply tie-breaking rules when multiple formats have similar scores.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns (format_name, final_confidence_score).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement conflict resolution logic</span></span></code></pre></div>\n\n<p><strong>Pipeline Integration Points</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># parsers/base.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..pipeline.context </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseContext</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseParser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base interface for all format-specific parsers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, context: ParseContext):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse content and return unified dictionary structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_format_name</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return format identifier for this parser.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_error</span><span style=\"color:#E1E4E8\">(self, error: </span><span style=\"color:#9ECBFF\">'ParseError'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add error to parsing context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context.add_error(error)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_warning</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add warning to parsing context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context.add_warning(message)</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Python Implementation Notes</strong>:</p>\n<ul>\n<li>Use <code>pathlib.Path</code> for file handling to maintain cross-platform compatibility</li>\n<li>Implement error context generation using <code>str.splitlines()</code> and list slicing for performance</li>\n<li>Use <code>dataclasses</code> for structured error objects to reduce boilerplate</li>\n<li>Consider using <code>typing.Union</code> for flexible input types (string content vs file paths)</li>\n<li>Use <code>enum.Enum</code> for format identifiers and confidence levels to prevent typos</li>\n</ul>\n<p><strong>Error Handling Patterns</strong>:</p>\n<ul>\n<li>Use context managers for file operations to ensure proper cleanup</li>\n<li>Implement error accumulation using lists rather than raising immediately </li>\n<li>Use custom exception hierarchies to enable targeted error handling</li>\n<li>Include position information in all error objects for debugging</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the interactions and data flow components, verify the integration:</p>\n<p><strong>Integration Test Command</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_pipeline.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected Behavior</strong>:</p>\n<ol>\n<li><p><strong>Format Detection</strong>: Create test files with clear format signatures. The detector should identify each format with high confidence.</p>\n</li>\n<li><p><strong>Pipeline Flow</strong>: Test the complete pipeline with valid files of each format. All should produce equivalent nested dictionary outputs.</p>\n</li>\n<li><p><strong>Error Propagation</strong>: Test with deliberately broken configuration files. Errors should include source positions, context, and helpful suggestions.</p>\n</li>\n<li><p><strong>Multi-Error Collection</strong>: Test with files containing multiple errors. The parser should report all errors found, not just the first one.</p>\n</li>\n</ol>\n<p><strong>Manual Verification Steps</strong>:</p>\n<ol>\n<li>Create a simple test script that calls <code>parse_config()</code> with sample files</li>\n<li>Verify that INI, TOML, and YAML files with equivalent content produce identical dictionary outputs</li>\n<li>Test error reporting by introducing syntax errors - verify that error messages include line numbers and suggestions</li>\n<li>Test format detection by removing format hints - verify correct automatic detection</li>\n</ol>\n<p><strong>Success Indicators</strong>:</p>\n<ul>\n<li>All three parsers integrate cleanly through the common pipeline</li>\n<li>Error messages include visual source context with line numbers</li>\n<li>Format detection works reliably on real configuration files</li>\n<li>Pipeline handles both file paths and string content seamlessly</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Format detection always returns INI</td>\n<td>Detection patterns too restrictive</td>\n<td>Print intermediate scores from each format scanner</td>\n<td>Adjust pattern weights and add more signature patterns</td>\n</tr>\n<tr>\n<td>Errors missing source positions</td>\n<td>Position not propagated through pipeline</td>\n<td>Add logging to track position updates</td>\n<td>Ensure position is set in context before creating errors</td>\n</tr>\n<tr>\n<td>Pipeline crashes on malformed files</td>\n<td>Missing error recovery in component</td>\n<td>Add try/catch around each pipeline phase</td>\n<td>Implement graceful error handling with continuation</td>\n</tr>\n<tr>\n<td>Inconsistent output between formats</td>\n<td>Structure conversion differences</td>\n<td>Compare parse trees before conversion</td>\n<td>Standardize conversion logic in base parser class</td>\n</tr>\n<tr>\n<td>Memory usage grows with file size</td>\n<td>Error context keeping full source</td>\n<td>Profile memory usage during parsing</td>\n<td>Limit error context to relevant lines only</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive error detection, reporting, and recovery strategies across all formats</p>\n</blockquote>\n<p>Configuration file parsing operates in an inherently error-prone environment where malformed input, ambiguous syntax, and edge cases are common. Think of error handling in parsing as building a safety net with multiple layers—each component must detect problems at its level, enrich the error information with context, and decide whether to recover gracefully or halt processing. Unlike typical application errors that occur during runtime, parsing errors must provide educational feedback to users who are editing configuration files, often by hand, and need specific guidance about what went wrong and how to fix it.</p>\n<p>The complexity of error handling in our multi-format parser stems from the fundamental differences in how INI, TOML, and YAML express structure and meaning. Each format has distinct failure modes, from INI&#39;s simple line-based issues to TOML&#39;s complex table redefinition conflicts to YAML&#39;s indentation-driven structural problems. Our error handling system must provide format-specific diagnostics while maintaining consistent error reporting patterns across all parsers.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fparsing-pipeline.svg\" alt=\"Complete Parsing Pipeline Flow\"></p>\n<h3 id=\"error-classification\">Error Classification</h3>\n<p>Parsing errors fall into distinct categories based on where they occur in the processing pipeline and what type of problem they represent. Understanding these categories helps design appropriate detection strategies and recovery mechanisms for each error type.</p>\n<p><strong>Lexical Errors (Token-Level)</strong></p>\n<p>Lexical errors occur during tokenization when the character stream cannot be converted into valid tokens. These represent the most fundamental parsing failures, where the input violates basic format syntax rules at the character level. Think of lexical errors as problems a human would catch when reading character by character—invalid escape sequences, unterminated strings, or illegal characters in specific contexts.</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Description</th>\n<th>Common Causes</th>\n<th>Detection Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unterminated String</td>\n<td>String literal missing closing quote</td>\n<td>Multiline strings without proper delimiters, escaped quotes at string end</td>\n<td>Track quote nesting depth, validate at line/file boundaries</td>\n</tr>\n<tr>\n<td>Invalid Escape Sequence</td>\n<td>Backslash followed by invalid character</td>\n<td>Incorrect escape codes like <code>\\q</code> instead of <code>\\n</code></td>\n<td>Pattern matching during string tokenization</td>\n</tr>\n<tr>\n<td>Illegal Character</td>\n<td>Character not allowed in current context</td>\n<td>Unicode characters in identifiers, special chars in numbers</td>\n<td>Context-aware character validation</td>\n</tr>\n<tr>\n<td>Malformed Number</td>\n<td>Invalid numeric literal format</td>\n<td>Multiple decimal points, invalid scientific notation</td>\n<td>Regex validation and numeric conversion attempts</td>\n</tr>\n<tr>\n<td>Invalid Unicode</td>\n<td>Malformed UTF-8 sequences or invalid codepoints</td>\n<td>File encoding issues, manual hex editing</td>\n<td>UTF-8 validation during character reading</td>\n</tr>\n</tbody></table>\n<p>The tokenizer must detect these errors immediately when encountered because they prevent meaningful token creation. However, the challenge lies in providing helpful error messages that explain not just what character is invalid, but why it&#39;s invalid in the current parsing context.</p>\n<p><strong>Syntactic Errors (Grammar-Level)</strong></p>\n<p>Syntactic errors occur when tokens are valid individually but don&#39;t follow the grammar rules of the target format. These errors represent violations of format-specific structure rules—like missing equals signs in key-value pairs or incorrect bracket matching in arrays. Think of syntactic errors as problems a human would catch when reading for grammatical structure rather than individual words.</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Description</th>\n<th>Detection Method</th>\n<th>Recovery Options</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing Assignment Operator</td>\n<td>Key without equals or colon separator</td>\n<td>Parser expects EQUALS or COLON after key token</td>\n<td>Skip line, assume section header, insert default operator</td>\n</tr>\n<tr>\n<td>Unmatched Brackets</td>\n<td>Opening bracket without corresponding close</td>\n<td>Bracket stack tracking during parsing</td>\n<td>Insert missing bracket, truncate at boundary</td>\n</tr>\n<tr>\n<td>Invalid Key Format</td>\n<td>Key contains illegal characters or structure</td>\n<td>Key validation against format rules</td>\n<td>Sanitize key, reject entry, quote if possible</td>\n</tr>\n<tr>\n<td>Malformed Table Header</td>\n<td>Section header syntax violations</td>\n<td>Bracket counting, path validation</td>\n<td>Skip header, use previous section, create default</td>\n</tr>\n<tr>\n<td>Invalid Array Structure</td>\n<td>Mixed types or malformed array syntax</td>\n<td>Type consistency checking during array parsing</td>\n<td>Convert to strings, truncate at error, skip malformed elements</td>\n</tr>\n</tbody></table>\n<p>Syntactic errors are particularly challenging because the parser has successfully tokenized the input but cannot interpret the token sequence according to format rules. The decision of whether to recover or abort depends on how fundamental the syntactic violation is to the document structure.</p>\n<p><strong>Structural Errors (Semantic-Level)</strong></p>\n<p>Structural errors occur when syntax is correct but the resulting data structure violates format-specific semantic rules. These are the most complex errors to detect because they require understanding the global document structure and format-specific constraints. Think of structural errors as problems that become apparent only when viewing the entire document—like trying to redefine a table that already exists or creating circular references in nested structures.</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Format</th>\n<th>Description</th>\n<th>Detection Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table Redefinition</td>\n<td>TOML</td>\n<td>Attempting to redefine existing table</td>\n<td>Symbol table tracking with conflict detection</td>\n</tr>\n<tr>\n<td>Key Path Conflict</td>\n<td>TOML</td>\n<td>Dotted key conflicts with existing table</td>\n<td>Path resolution with type checking</td>\n</tr>\n<tr>\n<td>Indentation Inconsistency</td>\n<td>YAML</td>\n<td>Mixed indentation levels or tab/space mixing</td>\n<td>Indentation tracking with established level validation</td>\n</tr>\n<tr>\n<td>Circular Structure</td>\n<td>All</td>\n<td>Self-referencing nested structures</td>\n<td>Depth tracking and path cycle detection</td>\n</tr>\n<tr>\n<td>Type Inconsistency</td>\n<td>TOML/YAML</td>\n<td>Array elements with incompatible types</td>\n<td>Type inference tracking during array construction</td>\n</tr>\n<tr>\n<td>Invalid Section Nesting</td>\n<td>INI</td>\n<td>Sections that create impossible hierarchies</td>\n<td>Section path validation during nested structure creation</td>\n</tr>\n</tbody></table>\n<p>Structural errors require maintaining parsing state across the entire document and applying format-specific validation rules as the data structure is constructed. These errors often indicate deeper misunderstandings of format semantics rather than simple syntax mistakes.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The error classification hierarchy mirrors the parsing pipeline—lexical errors block tokenization, syntactic errors block parse tree construction, and structural errors block final data structure creation. Each layer must decide whether to recover locally or propagate the error upward with additional context.</p>\n</blockquote>\n<p><strong>Context-Dependent Errors</strong></p>\n<p>Some errors can only be detected by examining the relationship between different parts of the document or by understanding the broader parsing context. These errors represent violations of format-specific rules that span multiple lines or sections.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Description</th>\n<th>Examples</th>\n<th>Detection Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cross-Reference Violations</td>\n<td>References to undefined or invalid targets</td>\n<td>TOML array-of-tables referencing undefined tables</td>\n<td>Symbol table validation after parsing completion</td>\n</tr>\n<tr>\n<td>Scope Violations</td>\n<td>Content appearing in invalid contexts</td>\n<td>Global INI keys after section definitions</td>\n<td>Context stack validation during parsing</td>\n</tr>\n<tr>\n<td>Ordering Violations</td>\n<td>Content appearing in wrong sequence</td>\n<td>YAML mapping keys out of alphabetical order (when required)</td>\n<td>Sequence tracking with format-specific rules</td>\n</tr>\n<tr>\n<td>Dependency Violations</td>\n<td>Missing prerequisites for current content</td>\n<td>Nested structure without parent definition</td>\n<td>Dependency graph validation</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Error Classification Granularity</strong></p>\n<ul>\n<li><strong>Context</strong>: Errors can be classified at different levels of detail, from broad categories to specific error codes</li>\n<li><strong>Options Considered</strong>: Simple three-tier system (lexical/syntactic/structural), detailed error codes for each format, hybrid approach with categories and subcodes</li>\n<li><strong>Decision</strong>: Hierarchical classification with main categories and format-specific subcategories</li>\n<li><strong>Rationale</strong>: Allows consistent error handling patterns while preserving format-specific diagnostic information</li>\n<li><strong>Consequences</strong>: Enables both generic error recovery strategies and specialized format-specific handling</li>\n</ul>\n</blockquote>\n<h3 id=\"error-message-design\">Error Message Design</h3>\n<p>Effective error messages in configuration parsing must bridge the gap between technical parsing details and user-friendly guidance. The target audience includes both developers integrating configuration files and system administrators editing configuration by hand. Think of error messages as being written by an experienced colleague who understands both the technical requirements and the user&#39;s likely mental model of the configuration format.</p>\n<p><strong>Error Message Components</strong></p>\n<p>Every parsing error message should contain specific components that collectively provide enough information for the user to understand what went wrong and how to fix it. The challenge lies in presenting this information clearly without overwhelming the user with implementation details.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Example Content</th>\n<th>Design Guidelines</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Problem Summary</td>\n<td>Clear statement of what went wrong</td>\n<td>&quot;Unterminated string literal&quot;</td>\n<td>Use domain language, avoid parser internals</td>\n</tr>\n<tr>\n<td>Location Context</td>\n<td>Where the error occurred</td>\n<td>&quot;Line 23, column 15&quot;</td>\n<td>Provide both line/column and visual context</td>\n</tr>\n<tr>\n<td>Syntax Context</td>\n<td>Surrounding code for visual reference</td>\n<td>Show 2-3 lines around error position</td>\n<td>Highlight exact error position with markers</td>\n</tr>\n<tr>\n<td>Explanation</td>\n<td>Why this is considered an error</td>\n<td>&quot;String literals must end with matching quote&quot;</td>\n<td>Reference format specification rules</td>\n</tr>\n<tr>\n<td>Suggestion</td>\n<td>Specific guidance for fixing the problem</td>\n<td>&quot;Add closing quote or use multiline string syntax&quot;</td>\n<td>Provide actionable steps, not vague advice</td>\n</tr>\n<tr>\n<td>Related Information</td>\n<td>Links to relevant documentation or similar errors</td>\n<td>&quot;See TOML string specification section 4.2&quot;</td>\n<td>Help users understand broader context</td>\n</tr>\n</tbody></table>\n<p>The <code>create_error_context</code> function standardizes the visual presentation of error location by showing source lines with position markers and highlighting the specific character or token where the error occurred.</p>\n<p><strong>Format-Specific Error Messaging</strong></p>\n<p>Each configuration format has characteristic error patterns that require specialized messaging approaches. The error message must reflect the user&#39;s mental model of how the format works, not the internal parsing implementation.</p>\n<p><strong>INI Format Error Messages</strong></p>\n<p>INI errors typically involve line-based parsing issues that are relatively straightforward to diagnose and fix. The mental model for INI users is simple: sections contain key-value pairs, and comments are ignored. Error messages should reinforce this simplicity while providing specific guidance.</p>\n<table>\n<thead>\n<tr>\n<th>Error Scenario</th>\n<th>Technical Issue</th>\n<th>User-Friendly Message</th>\n<th>Recovery Guidance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing section header</td>\n<td>Global key without section context</td>\n<td>&quot;Key-value pair &#39;database.host&#39; appears before any section header&quot;</td>\n<td>&quot;Move this line inside a [section] or add a [DEFAULT] section above it&quot;</td>\n</tr>\n<tr>\n<td>Malformed assignment</td>\n<td>Line without equals or colon</td>\n<td>&quot;Line &#39;database host localhost&#39; is not recognized as section header or key-value pair&quot;</td>\n<td>&quot;Add &#39;=&#39; or &#39;:&#39; between key and value: &#39;database.host = localhost&#39;&quot;</td>\n</tr>\n<tr>\n<td>Invalid section name</td>\n<td>Section header with illegal characters</td>\n<td>&quot;Section header contains invalid characters: &#39;[data&lt;&gt;base]&#39;&quot;</td>\n<td>&quot;Remove special characters: &#39;[database]&#39; or quote if necessary&quot;</td>\n</tr>\n<tr>\n<td>Inline comment confusion</td>\n<td>Equals sign within quoted value</td>\n<td>&quot;Assignment appears to contain multiple &#39;=&#39; characters&quot;</td>\n<td>&quot;Quote the entire value: &#39;url = &quot;<a href=\"http://example.com?id=123%5C\">http://example.com?id=123\\</a>&quot;&#39;&quot;</td>\n</tr>\n</tbody></table>\n<p><strong>TOML Format Error Messages</strong></p>\n<p>TOML errors are often complex because of the format&#39;s rich type system and table semantics. Users frequently struggle with table redefinition rules and dotted key expansion. Error messages must explain not just what&#39;s wrong but how TOML&#39;s global namespace works.</p>\n<table>\n<thead>\n<tr>\n<th>Error Scenario</th>\n<th>Technical Issue</th>\n<th>User-Friendly Message</th>\n<th>Explanation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table redefinition</td>\n<td>Explicit table conflicts with previous definition</td>\n<td>&quot;Cannot redefine table [database] (previously defined at line 15)&quot;</td>\n<td>&quot;TOML tables can only be defined once. Use dotted keys to add more values or create a [database.connection] subtable&quot;</td>\n</tr>\n<tr>\n<td>Dotted key conflict</td>\n<td>Dotted key path conflicts with existing table</td>\n<td>&quot;Key &#39;database.host&#39; conflicts with table [database.host] at line 8&quot;</td>\n<td>&quot;A key path cannot contain both a value and a subtable. Choose either &#39;database.host = &quot;value&quot;&#39; or &#39;[database.host]&#39; with sub-keys&quot;</td>\n</tr>\n<tr>\n<td>Array-of-tables confusion</td>\n<td>Mixed array and table syntax</td>\n<td>&quot;Cannot mix array-of-tables [[servers]] with regular table [servers]&quot;</td>\n<td>&quot;Use either [[servers]] for multiple server entries or [servers] for a single server configuration, not both&quot;</td>\n</tr>\n<tr>\n<td>Type inconsistency</td>\n<td>Array contains mixed types</td>\n<td>&quot;Array contains both integer (5) and string (&quot;five&quot;) values&quot;</td>\n<td>&quot;TOML arrays must contain values of the same type. Use strings for all values or create separate arrays&quot;</td>\n</tr>\n</tbody></table>\n<p><strong>YAML Format Error Messages</strong></p>\n<p>YAML errors frequently involve indentation and implicit structure creation. Users often struggle with YAML&#39;s context-sensitive parsing where the same content can mean different things depending on indentation and surrounding structure.</p>\n<table>\n<thead>\n<tr>\n<th>Error Scenario</th>\n<th>Technical Issue</th>\n<th>User-Friendly Message</th>\n<th>Indentation Guidance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Indentation inconsistency</td>\n<td>Current line doesn&#39;t match established levels</td>\n<td>&quot;Indentation of 3 spaces doesn&#39;t match any previous level (expected 0, 2, or 4)&quot;</td>\n<td>&quot;Use consistent indentation increments. Choose either 2 or 4 spaces per level and stick with it&quot;</td>\n</tr>\n<tr>\n<td>Mixed tabs and spaces</td>\n<td>Line contains both tab and space characters</td>\n<td>&quot;Line contains both tabs and spaces for indentation&quot;</td>\n<td>&quot;YAML forbids mixing tabs and spaces. Use only spaces for indentation&quot;</td>\n</tr>\n<tr>\n<td>Structural ambiguity</td>\n<td>Content could be interpreted multiple ways</td>\n<td>&quot;Mapping key &#39;items&#39; appears at same level as sequence item&quot;</td>\n<td>&quot;Indent the mapping key further to make it part of the sequence item, or outdent to make it a sibling&quot;</td>\n</tr>\n<tr>\n<td>Type inference surprise</td>\n<td>Value converted to unexpected type</td>\n<td>&quot;Value &#39;yes&#39; was interpreted as boolean true, not string&quot;</td>\n<td>&quot;Quote string values that might be interpreted as other types: items: &quot;yes&quot;&quot;</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Error Message Verbosity</strong></p>\n<ul>\n<li><strong>Context</strong>: Error messages can range from terse technical descriptions to verbose explanations with examples</li>\n<li><strong>Options Considered</strong>: Brief messages with error codes, verbose messages with full explanations, configurable verbosity levels</li>\n<li><strong>Decision</strong>: Verbose messages by default with option to reduce verbosity</li>\n<li><strong>Rationale</strong>: Configuration files are often edited manually by users who need educational feedback, not just error identification</li>\n<li><strong>Consequences</strong>: Larger error output but significantly improved user experience for configuration debugging</li>\n</ul>\n</blockquote>\n<p><strong>Error Context Generation</strong></p>\n<p>The <code>create_error_context</code> function generates visual representations of error locations that help users quickly identify and fix problems. This function must handle various edge cases while providing consistent output formatting.</p>\n<table>\n<thead>\n<tr>\n<th>Context Scenario</th>\n<th>Challenge</th>\n<th>Solution Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error at line beginning</td>\n<td>No preceding content for context</td>\n<td>Show line with position marker at start</td>\n</tr>\n<tr>\n<td>Error at line end</td>\n<td>May be continuation or termination issue</td>\n<td>Show line with marker and indicate if more content expected</td>\n</tr>\n<tr>\n<td>Error in long line</td>\n<td>Line too wide for terminal display</td>\n<td>Truncate line intelligently, keeping error position visible</td>\n</tr>\n<tr>\n<td>Error at file boundary</td>\n<td>Beginning or end of file</td>\n<td>Show available context, indicate file boundary</td>\n</tr>\n<tr>\n<td>Multi-line error</td>\n<td>Error spans multiple lines</td>\n<td>Show all affected lines with range indicators</td>\n</tr>\n</tbody></table>\n<p>The visual error context uses consistent formatting conventions: line numbers in brackets, position markers with carets or arrows, and highlighting of the specific characters involved in the error. This standardization helps users quickly parse error output across different error types.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Detection and Reporting Flow\"></p>\n<h3 id=\"error-recovery-approaches\">Error Recovery Approaches</h3>\n<p>Error recovery in parsing determines whether the system continues processing after encountering errors, and if so, how it attempts to interpret subsequent input. The fundamental tension in error recovery is between providing comprehensive error reporting (finding multiple problems in one pass) and maintaining parsing accuracy (not generating spurious errors due to incorrect recovery assumptions).</p>\n<p><strong>Recovery Strategy Classification</strong></p>\n<p>Different types of parsing errors require different recovery strategies based on their scope and impact on subsequent parsing. The recovery approach must consider both the likelihood of successful continued parsing and the value of finding additional errors in the same document.</p>\n<p><strong>Panic Mode Recovery</strong></p>\n<p>Panic mode recovery involves discarding input tokens until reaching a known synchronization point where parsing can resume reliably. This approach works well for format-specific landmarks that clearly indicate structure boundaries.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Synchronization Points</th>\n<th>Recovery Strategy</th>\n<th>Reliability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI</td>\n<td>Section headers, blank lines</td>\n<td>Skip to next <code>[section]</code> or end of current section</td>\n<td>High - sections are independent</td>\n</tr>\n<tr>\n<td>TOML</td>\n<td>Table headers, top-level assignments</td>\n<td>Skip to next <code>[table]</code> or unindented key assignment</td>\n<td>Medium - may skip related content</td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Document separators, unindented lines</td>\n<td>Skip to next document <code>---</code> or zero-indentation content</td>\n<td>Low - indentation context is critical</td>\n</tr>\n</tbody></table>\n<p>Panic mode recovery is most effective when the error occurs in a self-contained syntactic unit that can be safely skipped without affecting the interpretation of subsequent content. The challenge lies in determining which synchronization points are truly safe versus which might lead to misinterpretation of later content.</p>\n<p><strong>Error Production Recovery</strong></p>\n<p>Error production recovery involves inserting assumed content (like missing punctuation) or making reasonable assumptions about user intent to continue parsing. This approach attempts to &quot;fix&quot; the input automatically while recording what assumptions were made.</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Recovery Action</th>\n<th>Assumption Made</th>\n<th>Risk Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing assignment operator</td>\n<td>Insert <code>=</code> between key and value</td>\n<td>User intended key-value pair</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Missing closing quote</td>\n<td>Insert quote at line end</td>\n<td>String was intended to be single-line</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Missing closing bracket</td>\n<td>Insert bracket at section end</td>\n<td>User forgot to close array/table</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Inconsistent indentation</td>\n<td>Assume closest matching level</td>\n<td>User made spacing mistake</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<p>Error production recovery requires careful consideration of how likely the assumed fix is to match user intent. Conservative recovery (making minimal assumptions) is generally safer than aggressive recovery (making complex structural assumptions).</p>\n<p><strong>Phrase-Level Recovery</strong></p>\n<p>Phrase-level recovery attempts to identify the boundaries of malformed constructs and skip only the minimal amount of content necessary to resume parsing. This approach requires understanding the syntactic structure of the format to identify phrase boundaries.</p>\n<table>\n<thead>\n<tr>\n<th>Format Construct</th>\n<th>Boundary Indicators</th>\n<th>Recovery Scope</th>\n<th>Success Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI key-value pair</td>\n<td>Line boundaries, section headers</td>\n<td>Single line or continuation sequence</td>\n<td>Next line parses as valid key-value or section</td>\n</tr>\n<tr>\n<td>TOML value expression</td>\n<td>Commas, newlines, closing brackets</td>\n<td>Value portion of assignment</td>\n<td>Assignment key is preserved, next token is valid</td>\n</tr>\n<tr>\n<td>YAML mapping entry</td>\n<td>Indentation changes, sequence markers</td>\n<td>Single key-value pair</td>\n<td>Indentation context remains consistent</td>\n</tr>\n<tr>\n<td>YAML sequence item</td>\n<td>Sequence markers, indentation outdent</td>\n<td>Single list item</td>\n<td>List structure is maintained</td>\n</tr>\n</tbody></table>\n<p>The effectiveness of phrase-level recovery depends on the format&#39;s syntactic regularity. Formats with clear delimiters and boundaries support more reliable phrase-level recovery than formats with context-dependent parsing rules.</p>\n<p><strong>Intelligent Recovery Strategies</strong></p>\n<p>Advanced recovery strategies use knowledge about common error patterns and format-specific semantics to make informed decisions about how to continue parsing after errors.</p>\n<p><strong>Format-Specific Recovery Patterns</strong></p>\n<p>Each configuration format has characteristic error patterns that suggest specific recovery approaches based on observed user behavior and format complexity.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Common Error Pattern</th>\n<th>Intelligent Recovery</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI</td>\n<td>Quoted value with escaped quotes</td>\n<td>Scan for next unescaped quote or line boundary</td>\n<td>Nested quotes are often user intent, not syntax errors</td>\n</tr>\n<tr>\n<td>TOML</td>\n<td>Array with trailing comma</td>\n<td>Accept trailing comma, continue parsing</td>\n<td>Trailing commas are common in other languages</td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Inconsistent indentation increment</td>\n<td>Calculate greatest common divisor of observed indentations</td>\n<td>Users often pick inconsistent but mathematically related indentations</td>\n</tr>\n<tr>\n<td>All</td>\n<td>Unicode encoding issues</td>\n<td>Attempt multiple encoding interpretations</td>\n<td>Files are often edited with different tools that handle encoding differently</td>\n</tr>\n</tbody></table>\n<p>Intelligent recovery leverages understanding of how users actually create configuration files, including common mistakes and patterns from other similar formats or programming languages.</p>\n<p><strong>Multi-Pass Recovery Analysis</strong></p>\n<p>For complex errors that affect document structure, a multi-pass approach can provide better recovery than single-pass strategies. The first pass identifies structural issues, and subsequent passes attempt parsing with different recovery assumptions.</p>\n<table>\n<thead>\n<tr>\n<th>Pass Number</th>\n<th>Analysis Focus</th>\n<th>Recovery Goals</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pass 1</td>\n<td>Strict parsing with no recovery</td>\n<td>Identify all definite errors</td>\n<td>Clear error classification and location</td>\n</tr>\n<tr>\n<td>Pass 2</td>\n<td>Conservative recovery assumptions</td>\n<td>Parse maximum safe content</td>\n<td>No spurious errors introduced</td>\n</tr>\n<tr>\n<td>Pass 3</td>\n<td>Aggressive recovery with user feedback</td>\n<td>Extract any possible valid content</td>\n<td>User can validate recovery assumptions</td>\n</tr>\n</tbody></table>\n<p>Multi-pass analysis is particularly valuable for YAML documents where indentation errors can cascade and affect the interpretation of large portions of the document.</p>\n<blockquote>\n<p><strong>Decision: Recovery Aggressiveness</strong></p>\n<ul>\n<li><strong>Context</strong>: Recovery strategies range from conservative (minimal assumptions) to aggressive (maximal content extraction)</li>\n<li><strong>Options Considered</strong>: Always halt on first error, conservative recovery with user confirmation, aggressive recovery with detailed assumption logging</li>\n<li><strong>Decision</strong>: Conservative recovery by default with option for aggressive recovery mode</li>\n<li><strong>Rationale</strong>: Configuration files often contain critical system settings where incorrect assumptions can cause operational problems</li>\n<li><strong>Consequences</strong>: May require multiple parse attempts to extract maximum content, but reduces risk of silent misinterpretation</li>\n</ul>\n</blockquote>\n<p><strong>Graceful Degradation Patterns</strong></p>\n<p>When errors prevent complete parsing of a configuration document, graceful degradation strategies attempt to extract partial information that might still be valuable to the application.</p>\n<table>\n<thead>\n<tr>\n<th>Degradation Level</th>\n<th>Content Extracted</th>\n<th>Application Guidance</th>\n<th>Risk Assessment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Section-Level</td>\n<td>Complete valid sections only</td>\n<td>Use partial configuration with defaults for missing sections</td>\n<td>Low - missing sections are explicit</td>\n</tr>\n<tr>\n<td>Key-Level</td>\n<td>Valid key-value pairs within sections</td>\n<td>Skip malformed keys, preserve valid ones</td>\n<td>Medium - key relationships might be broken</td>\n</tr>\n<tr>\n<td>Value-Level</td>\n<td>Keys with parseable values</td>\n<td>Use string fallback for unparseable values</td>\n<td>High - type mismatches can cause runtime errors</td>\n</tr>\n</tbody></table>\n<p>Graceful degradation must provide clear information to the calling application about what content was successfully parsed and what assumptions or defaults are being applied for missing or malformed content.</p>\n<p><strong>Error Recovery State Management</strong></p>\n<p>Effective error recovery requires maintaining additional parsing state to track recovery decisions and their impact on subsequent parsing. This state helps determine when recovery assumptions prove incorrect and parsing should be abandoned.</p>\n<table>\n<thead>\n<tr>\n<th>State Information</th>\n<th>Purpose</th>\n<th>Update Triggers</th>\n<th>Decision Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Recovery assumption log</td>\n<td>Track what fixes were assumed</td>\n<td>Each recovery action</td>\n<td>Final validation of assumptions</td>\n</tr>\n<tr>\n<td>Confidence level tracking</td>\n<td>Measure parsing reliability</td>\n<td>Each successful/failed parse decision</td>\n<td>Threshold for abandoning recovery</td>\n</tr>\n<tr>\n<td>Structural integrity markers</td>\n<td>Validate document structure</td>\n<td>Major structure completion</td>\n<td>Consistency checking of recovered content</td>\n</tr>\n<tr>\n<td>Alternative interpretation stack</td>\n<td>Track parsing alternatives</td>\n<td>Ambiguous syntax encounters</td>\n<td>Backtracking to alternative interpretations</td>\n</tr>\n</tbody></table>\n<p>This state management enables the parser to make informed decisions about when recovery is likely to succeed versus when the accumulated uncertainty makes continued parsing unreliable.</p>\n<blockquote>\n<p><strong>Common Pitfalls in Error Recovery</strong></p>\n<p>⚠️ <strong>Pitfall: Over-Aggressive Recovery</strong>\nRecovery logic that makes too many assumptions about user intent can silently convert malformed configuration into incorrect but syntactically valid results. This is particularly dangerous in configuration parsing where silent errors can cause production system failures.</p>\n<p>⚠️ <strong>Pitfall: Recovery Error Cascades</strong>\nIncorrect recovery from one error can cause subsequent parsing to misinterpret valid content as erroneous. Each recovery decision must be validated against following content to detect when recovery assumptions are incorrect.</p>\n<p>⚠️ <strong>Pitfall: Context Loss During Recovery</strong>\nRecovery strategies that discard too much parsing context (like jumping to the next section) can lose important structural information needed to correctly interpret subsequent content. Context preservation is critical for maintaining parsing accuracy.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Recovery Behavior</strong>\nSimilar errors in different parts of the document should trigger consistent recovery behavior. Inconsistent recovery can confuse users and make error patterns harder to recognize and fix systematically.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The error handling implementation requires careful coordination between all parsing components to ensure consistent error detection, enriched propagation, and appropriate recovery strategies. The following guidance provides concrete implementations for the core error handling infrastructure.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Types</td>\n<td>Simple inheritance with base <code>ParseError</code> class</td>\n<td>Rich error taxonomy with error codes and metadata</td>\n</tr>\n<tr>\n<td>Error Context</td>\n<td>String formatting with manual line extraction</td>\n<td>Template-based error messages with structured context</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Fixed recovery strategies per error type</td>\n<td>Configurable recovery policies with success tracking</td>\n</tr>\n<tr>\n<td>Error Reporting</td>\n<td>Direct exception raising with message</td>\n<td>Structured error collection with severity levels</td>\n</tr>\n<tr>\n<td>Position Tracking</td>\n<td>Line/column counters during parsing</td>\n<td>Full source mapping with character ranges</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">config_parser</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── errors</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py              </span><span style=\"color:#6A737D\"># Error type exports</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── base_errors.py           </span><span style=\"color:#6A737D\"># ParseError, TokenError, SyntaxError, StructureError</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── error_context.py         </span><span style=\"color:#6A737D\"># create_error_context, position tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── error_recovery.py        </span><span style=\"color:#6A737D\"># Recovery strategies and state management</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── format_specific.py       </span><span style=\"color:#6A737D\"># INI, TOML, YAML specific error types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── parsers</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── base_parser.py           </span><span style=\"color:#6A737D\"># BaseParser with error handling integration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── ini_parser.py            </span><span style=\"color:#6A737D\"># INI parser with error recovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── toml_parser.py           </span><span style=\"color:#6A737D\"># TOML parser with error recovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── yaml_parser.py           </span><span style=\"color:#6A737D\"># YAML parser with error recovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">└── tests</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ├── test_error_handling.py   </span><span style=\"color:#6A737D\"># Error detection and recovery tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    └── error_test_cases</span><span style=\"color:#F97583\">/</span><span style=\"color:#6A737D\">         # Test configuration files with various errors</span></span></code></pre></div>\n\n<p><strong>Core Error Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Position</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a position in the source text with line, column, and offset information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    line: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    offset: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __str__</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"line </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.line</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, column </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.column</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParseError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all parsing errors with rich context information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> suggestion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.format_message())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> format_message</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Format error message with position and suggestion information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create user-friendly error message combining message, position, and suggestion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include position information in human-readable format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Append suggestion as actionable guidance when available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use consistent formatting for all error types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors that occur during tokenization (lexical analysis).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 invalid_text: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.invalid_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> invalid_text</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, position, suggestion)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#79B8FF\"> SyntaxError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors that occur when tokens don't follow format grammar rules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 expected_tokens: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, actual_token: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected_tokens </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_tokens </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.actual_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> actual_token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, position, suggestion)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructureError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ParseError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Errors that occur when document structure violates format semantics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 conflicting_position: Optional[Position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.conflicting_position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conflicting_position</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, position, suggestion)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_error_context</span><span style=\"color:#E1E4E8\">(source_content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, position: Position, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate visual error context showing source lines around error position.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Split source content into lines and identify target line</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate start and end line numbers for context window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format line numbers with consistent width and padding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add position marker (caret or arrow) pointing to exact error location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle edge cases: file start/end, very long lines, empty lines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return formatted string with line numbers, content, and position marker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Error Recovery Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Callable, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RecoveryStrategy</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Available error recovery strategies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HALT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"halt\"</span><span style=\"color:#6A737D\">                    # Stop parsing on first error</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PANIC_MODE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"panic_mode\"</span><span style=\"color:#6A737D\">        # Skip to synchronization point</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ERROR_PRODUCTION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"error_production\"</span><span style=\"color:#6A737D\">  # Insert assumed content</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PHRASE_LEVEL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"phrase_level\"</span><span style=\"color:#6A737D\">    # Skip minimal syntactic unit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RecoveryDecision</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a recovery decision made during parsing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    strategy: RecoveryStrategy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assumption: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">                  # What assumption was made</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#6A737D\">                # Confidence in recovery (0.0-1.0)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tokens_skipped: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">             # Number of tokens skipped</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    content_inserted: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">           # Any content inserted by recovery</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorRecoveryState</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks error recovery state during parsing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    decisions: List[RecoveryDecision] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.7</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_consecutive_recoveries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    consecutive_recovery_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_continue_recovery</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if parsing should continue after current error.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if consecutive recovery count exceeds maximum</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate average confidence of recent recovery decisions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return False if confidence drops below threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider total number of recovery attempts in document</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_recovery</span><span style=\"color:#E1E4E8\">(self, decision: RecoveryDecision) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record a recovery decision and update state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Append decision to decisions list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update consecutive recovery count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Adjust confidence thresholds based on recovery success patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Reset consecutive count on successful parsing between errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorRecoveryManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages error recovery strategies and decisions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_strategies: Dict[</span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">, Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.synchronization_points </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'ini'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#85E89D;font-weight:bold\">\\[</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\]</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^\\s</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">$</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">],           </span><span style=\"color:#6A737D\"># Section headers, blank lines</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'toml'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#85E89D;font-weight:bold\">\\[</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#85E89D;font-weight:bold\">\\]</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^[a-zA-Z].</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">=</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">],   </span><span style=\"color:#6A737D\"># Table headers, top-level keys</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'yaml'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^</span><span style=\"color:#DBEDFF\">---</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">^[</span><span style=\"color:#F97583\">^</span><span style=\"color:#79B8FF\">\\s]</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">]            </span><span style=\"color:#6A737D\"># Document separators, unindented content</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_recovery_strategy</span><span style=\"color:#E1E4E8\">(self, error_type: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  strategy_func: Callable) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a recovery strategy for specific error type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store strategy function for error type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that strategy function has correct signature</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Allow multiple strategies per error type with priority ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> attempt_recovery</span><span style=\"color:#E1E4E8\">(self, error: ParseError, parser_state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        format_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[RecoveryDecision]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Attempt to recover from parsing error using appropriate strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Determine recovery strategy based on error type and parser state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if recovery is advisable based on current recovery state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute recovery strategy and capture decision information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate recovery assumptions against subsequent content when possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return RecoveryDecision with strategy details and confidence level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Format-Specific Error Handling</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> INIParsingError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">SyntaxError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Specialized error for INI format parsing issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, line_number: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, line_content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 suggestion: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        position </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Position(</span><span style=\"color:#FFAB70\">line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">line_number, </span><span style=\"color:#FFAB70\">column</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">offset</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.line_content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line_content</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, position, </span><span style=\"color:#FFAB70\">suggestion</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">suggestion)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TOMLTableRedefinitionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">StructureError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error for TOML table redefinition conflicts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, table_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], original_position: Position,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 redefinition_position: Position):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        table_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \".\"</span><span style=\"color:#E1E4E8\">.join(table_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Cannot redefine table [</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">]\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Use dotted keys to add values or create subtable like [</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">.subtable]\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, redefinition_position, original_position, suggestion)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLIndentationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">StructureError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Error for YAML indentation inconsistencies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, current_indent: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, expected_indents: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 position: Position):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_indent </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_indent</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.expected_indents </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> expected_indents</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Indentation of </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">current_indent</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> spaces doesn't match established levels\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        suggestion </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Use one of these indentation levels: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_indents</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message, position, </span><span style=\"color:#FFAB70\">suggestion</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">suggestion)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_format_specific_error</span><span style=\"color:#E1E4E8\">(format_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> ParseError:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Factory function for creating format-specific error instances.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Dispatch to appropriate error class based on format and error type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract relevant context information for error class constructor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate format-appropriate error message and suggestion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return properly constructed error instance with full context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Error Collection and Reporting</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConfigurationError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Top-level error containing all parsing errors for a configuration file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors: List[ParseError]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recovery_decisions: List[RecoveryDecision] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> has_fatal_errors</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if errors prevent using any configuration content.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Classify errors by severity (fatal vs recoverable)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if any StructureError or unrecovered TokenError present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider recovery decision confidence in fatal error determination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> format_error_report</span><span style=\"color:#E1E4E8\">(self, include_suggestions: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          include_context: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate comprehensive error report for user consumption.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Group related errors together (same line, same construct)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Sort errors by position (line number, then column)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format each error with context and suggestions when requested</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include recovery decision summary if any recoveries were attempted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Provide document-level guidance for common error patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorCollector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Collects and manages errors during parsing process.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors: List[ParseError] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_decisions: List[RecoveryDecision] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_error</span><span style=\"color:#E1E4E8\">(self, error: ParseError) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add error to collection with automatic position tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Append error to errors list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure error has position information when available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for duplicate errors at same position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Maintain errors in position order for consistent reporting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> has_errors</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if any errors have been collected.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.errors) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_configuration_error</span><span style=\"color:#E1E4E8\">(self) -> ConfigurationError:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create final ConfigurationError with all collected information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return ConfigurationError with current errors and recovery decisions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include source path information when available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Sort errors by position for consistent presentation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints</strong></p>\n<p><strong>Milestone 1 - INI Error Handling</strong>: Test error detection for malformed INI files:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test with malformed INI file containing various error types</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_ini_error_handling.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify error messages are helpful and include suggestions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check that recovery allows parsing of valid sections despite errors</span></span></code></pre></div>\n\n<p><strong>Milestone 2-3 - TOML Error Handling</strong>: Test complex TOML error scenarios:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test TOML table redefinition and dotted key conflicts</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_toml_error_handling.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify symbol table conflict detection works correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check that error messages explain TOML's global namespace rules</span></span></code></pre></div>\n\n<p><strong>Milestone 4 - YAML Error Handling</strong>: Test indentation and structure errors:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test YAML indentation consistency and type inference</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_yaml_error_handling.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify indentation error messages suggest specific fixes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check that recovery maintains structural integrity</span></span></code></pre></div>\n\n<p><strong>Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error messages without position info</td>\n<td>Position not tracked during parsing</td>\n<td>Add logging to position tracking functions</td>\n<td>Ensure every token creation updates position</td>\n</tr>\n<tr>\n<td>Generic error messages</td>\n<td>Error context not enriched during propagation</td>\n<td>Check error creation calls for context info</td>\n<td>Pass parser state to error constructors</td>\n</tr>\n<tr>\n<td>Recovery causes spurious errors</td>\n<td>Recovery assumptions don&#39;t match actual content</td>\n<td>Log recovery decisions and validate against following tokens</td>\n<td>Add recovery validation logic</td>\n</tr>\n<tr>\n<td>Same error reported multiple times</td>\n<td>Error detection in multiple parsing phases</td>\n<td>Check error deduplication in error collector</td>\n<td>Filter duplicate errors by position and type</td>\n</tr>\n<tr>\n<td>Confusing error suggestions</td>\n<td>Suggestions don&#39;t match actual error context</td>\n<td>Review format-specific error message templates</td>\n<td>Improve suggestion logic with context awareness</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive testing approach ensuring correct implementation and validation across all parsing components</p>\n</blockquote>\n<p>Testing a multi-format configuration parser requires a sophisticated approach that validates correctness across fundamentally different syntactic paradigms while ensuring each component maintains its specific behavioral guarantees. Think of testing configuration parsers like quality assurance for a universal translator - we must verify that each language (format) is correctly understood, that the translation process preserves meaning accurately, and that error conditions are handled gracefully across all supported languages. The challenge lies in designing test strategies that capture both the unique characteristics of each format and the unified behavior expected from the complete parsing system.</p>\n<p>The testing strategy must address multiple layers of complexity: lexical analysis correctness across different tokenization approaches, syntactic parsing accuracy for nested structures, semantic validation of type inference and data conversion, and integration behavior when components work together. Each milestone introduces specific testing requirements that build upon previous foundations while introducing new edge cases and validation needs.</p>\n<h3 id=\"test-categories-and-coverage\">Test Categories and Coverage</h3>\n<p>Configuration parser testing requires a multi-dimensional approach that validates behavior across different abstraction levels, error conditions, and format-specific requirements. The testing matrix spans from low-level tokenization correctness to high-level integration scenarios, ensuring comprehensive coverage of the parsing pipeline.</p>\n<p><strong>Unit Testing Coverage</strong> forms the foundation of our testing strategy, focusing on individual component behavior in isolation. Each component must be tested against its specific responsibilities and interface contracts. Unit tests provide fast feedback during development and enable confident refactoring by establishing behavioral baselines.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Test Focus</th>\n<th>Key Scenarios</th>\n<th>Coverage Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>BaseTokenizer</code></td>\n<td>Token generation accuracy</td>\n<td>String literals, escape sequences, position tracking</td>\n<td>All token types, edge cases, malformed input</td>\n</tr>\n<tr>\n<td><code>INIParser</code></td>\n<td>Section parsing, key-value extraction</td>\n<td>Global keys, nested sections, comment handling</td>\n<td>All INI syntax variants, whitespace handling</td>\n</tr>\n<tr>\n<td><code>TOMLParser</code></td>\n<td>Table creation, type inference</td>\n<td>Dotted keys, array-of-tables, inline structures</td>\n<td>TOML specification compliance, conflict detection</td>\n</tr>\n<tr>\n<td><code>YAMLParser</code></td>\n<td>Indentation tracking, structure building</td>\n<td>Block syntax, flow syntax, type conversion</td>\n<td>Indentation edge cases, mixed content types</td>\n</tr>\n<tr>\n<td>Format Detection</td>\n<td>Format identification accuracy</td>\n<td>Ambiguous content, mixed syntax</td>\n<td>High confidence detection, fallback behavior</td>\n</tr>\n</tbody></table>\n<p><strong>Integration Testing Coverage</strong> validates component interactions and end-to-end parsing behavior. Integration tests ensure that the tokenizer-parser pipeline produces correct results and that error information flows properly between components. These tests catch impedance mismatches between components and validate the complete parsing workflow.</p>\n<table>\n<thead>\n<tr>\n<th>Integration Scenario</th>\n<th>Test Focus</th>\n<th>Validation Points</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File-to-Dictionary Pipeline</td>\n<td>Complete parsing accuracy</td>\n<td>Input file → parsed structure consistency</td>\n<td>Malformed files, encoding issues, large files</td>\n</tr>\n<tr>\n<td>Cross-Format Consistency</td>\n<td>Equivalent configurations produce same output</td>\n<td>Semantic equivalence across INI/TOML/YAML</td>\n<td>Format-specific limitations, type coercion differences</td>\n</tr>\n<tr>\n<td>Error Context Propagation</td>\n<td>Error information enrichment</td>\n<td>Position accuracy, helpful messages</td>\n<td>Multi-level errors, recovery context</td>\n</tr>\n<tr>\n<td>Format Detection Integration</td>\n<td>Automatic format selection</td>\n<td>Correct parser selection, confidence levels</td>\n<td>Ambiguous content, unknown formats</td>\n</tr>\n</tbody></table>\n<p><strong>Property-Based Testing Coverage</strong> validates parser behavior across broad input spaces by generating test cases that explore edge conditions and invariant properties. Property-based tests are particularly valuable for parsers because they can discover subtle bugs in tokenization state machines and recursive parsing algorithms.</p>\n<table>\n<thead>\n<tr>\n<th>Property Category</th>\n<th>Invariant Properties</th>\n<th>Generation Strategy</th>\n<th>Validation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tokenization Roundtrip</td>\n<td><code>tokenize(detokenize(tokens)) == tokens</code></td>\n<td>Random token sequences</td>\n<td>Token equality, position consistency</td>\n</tr>\n<tr>\n<td>Parse Tree Structure</td>\n<td>Valid input produces well-formed tree</td>\n<td>Grammar-compliant generation</td>\n<td>Structure validation, parent-child consistency</td>\n</tr>\n<tr>\n<td>Type Inference Consistency</td>\n<td>Same semantic value infers same type</td>\n<td>Value variation generation</td>\n<td>Type stability, conversion accuracy</td>\n</tr>\n<tr>\n<td>Error Recovery Consistency</td>\n<td>Errors don&#39;t corrupt subsequent parsing</td>\n<td>Malformed input injection</td>\n<td>State isolation, recovery effectiveness</td>\n</tr>\n</tbody></table>\n<p><strong>Regression Testing Coverage</strong> ensures that fixes and enhancements don&#39;t break existing functionality. Regression tests capture specific bugs that were discovered and fixed, preventing their reintroduction. This category grows over time as edge cases are discovered and resolved.</p>\n<table>\n<thead>\n<tr>\n<th>Regression Category</th>\n<th>Source of Test Cases</th>\n<th>Maintenance Strategy</th>\n<th>Update Triggers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bug Fix Validation</td>\n<td>Discovered parsing failures</td>\n<td>Permanent test retention</td>\n<td>Every bug fix adds test case</td>\n</tr>\n<tr>\n<td>Edge Case Preservation</td>\n<td>Boundary condition discoveries</td>\n<td>Categorized edge case library</td>\n<td>Format specification updates</td>\n</tr>\n<tr>\n<td>Performance Regression</td>\n<td>Performance degradation incidents</td>\n<td>Benchmark integration</td>\n<td>Significant algorithmic changes</td>\n</tr>\n<tr>\n<td>Compatibility Maintenance</td>\n<td>Version upgrade issues</td>\n<td>Cross-version test suite</td>\n<td>Dependency updates, format changes</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight:</strong> The test categorization strategy ensures that each type of failure is caught by the most appropriate testing approach. Unit tests catch component logic errors quickly, integration tests catch interface mismatches, property-based tests discover edge cases, and regression tests prevent backsliding.</p>\n</blockquote>\n<p><strong>Error Condition Testing Coverage</strong> validates parser behavior when encountering malformed input, resource constraints, and exceptional conditions. Error testing is particularly critical for parsers because they must handle arbitrary user input gracefully while providing helpful feedback for fixing configuration issues.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Test Scenarios</th>\n<th>Expected Behavior</th>\n<th>Recovery Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Syntax Errors</td>\n<td>Invalid characters, malformed structures</td>\n<td>Specific error messages, accurate positions</td>\n<td>Parsing continuation, error accumulation</td>\n</tr>\n<tr>\n<td>Type Errors</td>\n<td>Invalid type conversions, incompatible values</td>\n<td>Type-specific error details, suggested fixes</td>\n<td>Type inference fallbacks, default handling</td>\n</tr>\n<tr>\n<td>Structure Errors</td>\n<td>Invalid nesting, conflicting definitions</td>\n<td>Structural context, conflict locations</td>\n<td>Partial structure preservation, data recovery</td>\n</tr>\n<tr>\n<td>Resource Errors</td>\n<td>Large files, deep nesting, memory pressure</td>\n<td>Graceful degradation, resource reporting</td>\n<td>Streaming alternatives, size limits</td>\n</tr>\n</tbody></table>\n<p><strong>Format-Specific Testing Coverage</strong> addresses the unique characteristics and edge cases of each supported configuration format. Format-specific tests ensure compliance with format specifications and handle format-specific corner cases that don&#39;t apply to other formats.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Specific Test Areas</th>\n<th>Critical Edge Cases</th>\n<th>Compliance Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI</td>\n<td>Section headers, key-value variants, comment styles</td>\n<td>Global keys, inline comments, quoted values</td>\n<td>Loose INI standard interpretations</td>\n</tr>\n<tr>\n<td>TOML</td>\n<td>Table definitions, array-of-tables, type system</td>\n<td>Table redefinition, dotted key conflicts, datetime formats</td>\n<td>TOML v1.0.0 specification compliance</td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Indentation semantics, flow/block mixing, implicit typing</td>\n<td>Tab vs space indentation, implicit type surprises</td>\n<td>YAML subset specification adherence</td>\n</tr>\n</tbody></table>\n<h3 id=\"milestone-verification-points\">Milestone Verification Points</h3>\n<p>Each implementation milestone requires specific verification criteria that confirm successful completion of core functionality before proceeding to more advanced features. Milestone verification provides concrete checkpoints that validate both functional correctness and implementation quality.</p>\n<p><strong>Milestone 1: INI Parser Verification</strong> establishes the foundation for configuration parsing by validating line-based parsing, section organization, and comment handling. INI parser verification focuses on the fundamental concepts of section-based organization and key-value extraction that underlie more complex parsing scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Success Criteria</th>\n<th>Test Validation</th>\n<th>Implementation Quality</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Section Header Parsing</td>\n<td><code>[section]</code> creates nested dictionary entry</td>\n<td>Section names extracted correctly, nested structure created</td>\n<td>Bracket validation, whitespace handling, invalid section recovery</td>\n</tr>\n<tr>\n<td>Key-Value Processing</td>\n<td>Both <code>key=value</code> and <code>key: value</code> supported</td>\n<td>Values parsed with whitespace trimming, type inference applied</td>\n<td>Quote handling, escape sequence processing, inline comment separation</td>\n</tr>\n<tr>\n<td>Comment Handling</td>\n<td><code>;</code> and <code>#</code> comments ignored appropriately</td>\n<td>Comment lines skipped, inline comments preserved/ignored based on configuration</td>\n<td>Comment detection accuracy, mixed comment style handling</td>\n</tr>\n<tr>\n<td>Global Key Support</td>\n<td>Keys before first section handled correctly</td>\n<td>Global namespace created, keys accessible in output structure</td>\n<td>Global section naming, namespace organization</td>\n</tr>\n</tbody></table>\n<p>The INI parser milestone verification requires comprehensive testing of edge cases that commonly trip up implementations. Testing must validate that the parser handles keys outside of sections appropriately, processes inline comments correctly without breaking on <code>=</code> characters inside quoted strings, and supports both common INI delimiters (<code>=</code> and <code>:</code>) with consistent behavior.</p>\n<p><strong>Milestone Verification Test Suite for INI Parser:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Case Category</th>\n<th>Test Scenarios</th>\n<th>Expected Results</th>\n<th>Failure Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Structure</td>\n<td>Simple sections with key-value pairs</td>\n<td>Nested dictionary with section keys</td>\n<td>Missing sections, flat structure, incorrect nesting</td>\n</tr>\n<tr>\n<td>Edge Cases</td>\n<td>Empty sections, keys with no values, quoted strings with delimiters</td>\n<td>Appropriate defaults, quote processing, delimiter isolation</td>\n<td>Parsing failures, incorrect value extraction, quote handling errors</td>\n</tr>\n<tr>\n<td>Comment Processing</td>\n<td>Line comments, inline comments, mixed comment styles</td>\n<td>Comments ignored or preserved based on parser configuration</td>\n<td>Comment content included in values, parsing errors on comment lines</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Malformed section headers, invalid key syntax</td>\n<td>Specific error messages, parsing continuation</td>\n<td>Parser crashes, generic error messages, parsing termination</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 2: TOML Tokenizer Verification</strong> validates lexical analysis capabilities required for structured format parsing. TOML tokenizer verification focuses on accurate token generation, complex string handling, and position tracking that enables meaningful error reporting.</p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Success Criteria</th>\n<th>Token Accuracy</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Token Types</td>\n<td>All TOML grammar elements tokenized correctly</td>\n<td><code>STRING</code>, <code>NUMBER</code>, <code>BOOLEAN</code>, <code>IDENTIFIER</code> tokens generated accurately</td>\n<td>Invalid token detection, position tracking, recovery strategies</td>\n</tr>\n<tr>\n<td>String Literal Handling</td>\n<td>Basic, literal, and multiline strings processed correctly</td>\n<td>Escape sequence processing, quote type differentiation, multiline joining</td>\n<td>Quote mismatch detection, escape sequence validation, multiline boundary handling</td>\n</tr>\n<tr>\n<td>Numeric Processing</td>\n<td>Integers, floats, dates, times recognized as distinct types</td>\n<td>Type-specific token generation, format validation, underscore handling</td>\n<td>Invalid numeric formats, overflow detection, format compliance</td>\n</tr>\n<tr>\n<td>Position Tracking</td>\n<td>Line and column numbers accurate for all tokens</td>\n<td><code>Position</code> objects reflect true source locations</td>\n<td>Position drift, multiline position errors, tab handling</td>\n</tr>\n</tbody></table>\n<p>TOML tokenizer verification requires extensive testing of string literal variants because TOML supports multiple string syntaxes with different escape processing rules. The tokenizer must correctly differentiate between basic strings (which process escapes) and literal strings (which treat backslashes literally), while handling multiline variants of both string types according to TOML specification rules.</p>\n<p><strong>Milestone Verification Test Suite for TOML Tokenizer:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Case Category</th>\n<th>Token Scenarios</th>\n<th>Validation Approach</th>\n<th>Quality Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>String Variants</td>\n<td>Basic strings with escapes, literal strings, multiline strings</td>\n<td>Token type accuracy, value processing correctness</td>\n<td>Escape sequence handling, quote processing, newline normalization</td>\n</tr>\n<tr>\n<td>Numeric Types</td>\n<td>Integers with underscores, scientific notation floats, ISO dates</td>\n<td>Type-specific token generation, format compliance</td>\n<td>Underscore handling, exponent processing, datetime parsing</td>\n</tr>\n<tr>\n<td>Structural Elements</td>\n<td>Brackets, dots, equals, commas in various combinations</td>\n<td>Token sequence accuracy, delimiter recognition</td>\n<td>Tokenization order, structural token identification</td>\n</tr>\n<tr>\n<td>Error Conditions</td>\n<td>Unterminated strings, invalid numeric formats, illegal characters</td>\n<td>Error token generation, position accuracy, recovery behavior</td>\n<td>Error context quality, position precision, tokenization continuation</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 3: TOML Parser Verification</strong> validates recursive descent parsing capabilities for complex nested structures. TOML parser verification focuses on table creation, array-of-tables handling, and conflict detection that ensures TOML specification compliance.</p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Success Criteria</th>\n<th>Structure Validation</th>\n<th>Conflict Detection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table Parsing</td>\n<td><code>[table]</code> and <code>[table.subtable]</code> create correct nested structure</td>\n<td>Dictionary nesting accuracy, key path resolution</td>\n<td>Table redefinition detection, implicit table conflicts</td>\n</tr>\n<tr>\n<td>Array-of-Tables</td>\n<td><code>[[array.of.tables]]</code> creates list of dictionary entries</td>\n<td>Array structure creation, element organization</td>\n<td>Array redefinition as table, mixed array/table conflicts</td>\n</tr>\n<tr>\n<td>Dotted Key Expansion</td>\n<td><code>physical.color = &#39;orange&#39;</code> creates nested structure</td>\n<td>Automatic structure creation, key path processing</td>\n<td>Conflicting key definitions, type mismatches</td>\n</tr>\n<tr>\n<td>Inline Structures</td>\n<td>Inline tables <code>{key = value}</code> and arrays <code>[1, 2, 3]</code> parsed correctly</td>\n<td>Nested structure creation, type inference</td>\n<td>Syntax error recovery, nesting validation</td>\n</tr>\n</tbody></table>\n<p>TOML parser verification requires sophisticated conflict detection testing because TOML has complex rules about table redefinition and key conflicts. The parser must detect when a dotted key attempts to redefine an existing table, when a table is defined multiple times, and when array-of-tables syntax conflicts with existing definitions.</p>\n<p><strong>Milestone Verification Test Suite for TOML Parser:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Case Category</th>\n<th>Parsing Scenarios</th>\n<th>Structure Validation</th>\n<th>Error Detection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table Hierarchies</td>\n<td>Nested table definitions, dotted table paths, implicit table creation</td>\n<td>Nesting accuracy, path resolution, structure completeness</td>\n<td>Table redefinition detection, path conflict identification</td>\n</tr>\n<tr>\n<td>Value Types</td>\n<td>Strings, numbers, booleans, arrays, inline tables</td>\n<td>Type inference accuracy, nested structure creation</td>\n<td>Type conversion errors, syntax validation</td>\n</tr>\n<tr>\n<td>Complex Structures</td>\n<td>Mixed arrays, nested inline tables, array-of-tables with complex values</td>\n<td>Deep structure accuracy, reference integrity</td>\n<td>Circular reference detection, depth validation</td>\n</tr>\n<tr>\n<td>Specification Compliance</td>\n<td>Edge cases from TOML specification, conflict scenarios</td>\n<td>Specification adherence, edge case handling</td>\n<td>Standard compliance validation, error message quality</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 4: YAML Subset Parser Verification</strong> validates indentation-sensitive parsing for hierarchical block structures. YAML parser verification focuses on indentation stack management, structure type inference, and scalar type conversion.</p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Success Criteria</th>\n<th>Structure Management</th>\n<th>Type Inference</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Indentation Processing</td>\n<td>Block structure determined correctly from indentation</td>\n<td>Stack-based nesting, level transitions, structure preservation</td>\n<td>Indentation error detection, tab vs space validation</td>\n</tr>\n<tr>\n<td>Mapping Processing</td>\n<td><code>key: value</code> pairs create dictionary entries</td>\n<td>Dictionary structure, nested mappings, key uniqueness</td>\n<td>Key conflict detection, value processing</td>\n</tr>\n<tr>\n<td>Sequence Processing</td>\n<td><code>- item</code> lists create ordered arrays</td>\n<td>Array structure, nested sequences, mixed content</td>\n<td>Item processing, nesting validation</td>\n</tr>\n<tr>\n<td>Flow Syntax</td>\n<td><code>[list]</code> and <code>{map}</code> inline syntax parsed correctly</td>\n<td>Inline structure creation, flow/block mixing</td>\n<td>Syntax validation, nesting consistency</td>\n</tr>\n</tbody></table>\n<p>YAML parser verification requires extensive indentation testing because YAML&#39;s indentation sensitivity creates numerous edge cases around tab handling, inconsistent indentation levels, and mixed indentation styles. The parser must maintain strict indentation validation while providing helpful error messages for common indentation mistakes.</p>\n<p><strong>Milestone Verification Test Suite for YAML Parser:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Case Category</th>\n<th>Indentation Scenarios</th>\n<th>Structure Validation</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Block Structures</td>\n<td>Consistent indentation, nested mappings and sequences</td>\n<td>Structure accuracy, nesting preservation</td>\n<td>Indentation error detection, level validation</td>\n</tr>\n<tr>\n<td>Mixed Content</td>\n<td>Mappings containing sequences, sequences containing mappings</td>\n<td>Content type handling, structure transitions</td>\n<td>Type conflict detection, mixed content validation</td>\n</tr>\n<tr>\n<td>Scalar Processing</td>\n<td>Quoted strings, unquoted strings, numeric values, booleans</td>\n<td>Type inference accuracy, value processing</td>\n<td>Type conversion errors, ambiguous value handling</td>\n</tr>\n<tr>\n<td>Edge Cases</td>\n<td>Empty documents, single-item structures, deeply nested content</td>\n<td>Minimal structure handling, depth management</td>\n<td>Edge case recovery, structure validation</td>\n</tr>\n</tbody></table>\n<h3 id=\"test-data-strategy\">Test Data Strategy</h3>\n<p>Effective configuration parser testing requires carefully curated test datasets that systematically explore the input space while covering critical edge cases and error conditions. The test data strategy must balance comprehensive coverage with maintainable test organization, ensuring that test cases remain understandable and debuggable as the test suite grows.</p>\n<p><strong>Structured Test Data Organization</strong> provides a systematic approach to organizing test cases across multiple dimensions: format types, feature complexity, error conditions, and edge cases. The organization strategy enables efficient test maintenance and comprehensive coverage validation.</p>\n<table>\n<thead>\n<tr>\n<th>Test Data Category</th>\n<th>Organization Principle</th>\n<th>File Structure</th>\n<th>Content Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Golden Path Cases</td>\n<td>Common usage patterns for each format</td>\n<td><code>test-data/golden/[format]/[feature].ext</code></td>\n<td>Real-world configuration examples, typical use cases</td>\n</tr>\n<tr>\n<td>Edge Case Library</td>\n<td>Boundary conditions and corner cases</td>\n<td><code>test-data/edge-cases/[format]/[category]/</code></td>\n<td>Minimal reproducible cases, focused edge conditions</td>\n</tr>\n<tr>\n<td>Error Case Collection</td>\n<td>Invalid input scenarios</td>\n<td><code>test-data/errors/[format]/[error-type]/</code></td>\n<td>Systematic error exploration, recovery validation</td>\n</tr>\n<tr>\n<td>Cross-Format Equivalence</td>\n<td>Semantically equivalent configurations</td>\n<td><code>test-data/equivalence/[scenario]/</code></td>\n<td>Same logical configuration in multiple formats</td>\n</tr>\n</tbody></table>\n<p>The structured organization enables systematic test coverage analysis and makes it easy to add new test cases as edge cases are discovered. Each category serves a specific testing purpose and can be processed with appropriate test harness logic.</p>\n<p><strong>Golden Path Test Data Strategy</strong> focuses on realistic configuration scenarios that represent common usage patterns. Golden path tests validate that the parser handles typical use cases correctly and produces expected output structures. These tests serve as acceptance criteria and regression protection for core functionality.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Golden Path Scenarios</th>\n<th>Test Data Characteristics</th>\n<th>Validation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>INI</td>\n<td>Application settings, database configuration, service parameters</td>\n<td>Multiple sections, varied value types, mixed comment styles</td>\n<td>Structure accuracy, value processing, type inference</td>\n</tr>\n<tr>\n<td>TOML</td>\n<td>Package configuration, build settings, service definitions</td>\n<td>Nested tables, arrays, mixed value types, complex structures</td>\n<td>Nesting validation, type system compliance, specification adherence</td>\n</tr>\n<tr>\n<td>YAML</td>\n<td>Application config, deployment descriptors, data serialization</td>\n<td>Block structures, sequences, mappings, mixed content types</td>\n<td>Indentation handling, type inference, structure preservation</td>\n</tr>\n</tbody></table>\n<p>Golden path test data should represent configurations that users would actually write, not contrived examples that exist only to test specific features. The test data should include realistic key names, appropriate value ranges, and natural organization patterns that reflect how each format is typically used.</p>\n<p><strong>Edge Case Test Data Strategy</strong> systematically explores boundary conditions and corner cases that often reveal parsing bugs. Edge case testing requires carefully constructed minimal examples that isolate specific problematic conditions without introducing additional complexity.</p>\n<table>\n<thead>\n<tr>\n<th>Edge Case Category</th>\n<th>Test Data Design</th>\n<th>Validation Focus</th>\n<th>Coverage Goals</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Empty Content</td>\n<td>Empty files, whitespace-only files, comment-only files</td>\n<td>Parser initialization, minimal input handling</td>\n<td>Zero-content graceful handling</td>\n</tr>\n<tr>\n<td>Boundary Values</td>\n<td>Maximum nesting depth, longest strings, largest numbers</td>\n<td>Resource handling, algorithmic limits</td>\n<td>Performance boundaries, memory usage</td>\n</tr>\n<tr>\n<td>Whitespace Sensitivity</td>\n<td>Mixed tabs/spaces, trailing whitespace, Unicode whitespace</td>\n<td>Whitespace processing, normalization</td>\n<td>Whitespace semantic preservation</td>\n</tr>\n<tr>\n<td>Unicode Complexity</td>\n<td>Non-ASCII characters, emoji, combining characters, RTL text</td>\n<td>Unicode handling, encoding processing</td>\n<td>International character support</td>\n</tr>\n</tbody></table>\n<p>Edge case test data must be constructed systematically to ensure comprehensive coverage of boundary conditions. Each edge case should focus on a single problematic condition to make failures easy to diagnose and fix.</p>\n<p><strong>Error Case Test Data Strategy</strong> validates parser behavior when encountering invalid input by providing systematic exploration of error conditions. Error case testing ensures that parsers fail gracefully and provide helpful error messages that enable users to fix their configuration files.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Test Data Design</th>\n<th>Error Validation</th>\n<th>Recovery Testing</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Syntax Errors</td>\n<td>Invalid characters, malformed structures, missing delimiters</td>\n<td>Error message quality, position accuracy</td>\n<td>Parsing continuation, error accumulation</td>\n</tr>\n<tr>\n<td>Type Errors</td>\n<td>Invalid type conversions, incompatible value assignments</td>\n<td>Type-specific error reporting, conversion failure handling</td>\n<td>Type inference fallbacks, default value handling</td>\n</tr>\n<tr>\n<td>Structure Errors</td>\n<td>Invalid nesting, conflicting definitions, circular references</td>\n<td>Structural validation, conflict detection</td>\n<td>Partial structure preservation, data salvage</td>\n</tr>\n<tr>\n<td>Resource Errors</td>\n<td>Extremely large files, deeply nested structures, memory pressure</td>\n<td>Resource exhaustion handling, graceful degradation</td>\n<td>Resource limit enforcement, streaming alternatives</td>\n</tr>\n</tbody></table>\n<p>Error case test data should be constructed to trigger specific error conditions while remaining understandable to developers debugging parsing failures. Each error case should include expected error messages and recovery behavior validation.</p>\n<p><strong>Cross-Format Equivalence Test Strategy</strong> validates that semantically equivalent configurations produce consistent results across different formats. Equivalence testing ensures that format choice doesn&#39;t affect application behavior when configurations represent the same logical settings.</p>\n<table>\n<thead>\n<tr>\n<th>Equivalence Scenario</th>\n<th>Format Variations</th>\n<th>Semantic Validation</th>\n<th>Difference Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Key-Value</td>\n<td>INI sections, TOML tables, YAML mappings</td>\n<td>Value equality, structure equivalence</td>\n<td>Format-specific limitations, type coercion differences</td>\n</tr>\n<tr>\n<td>Nested Structures</td>\n<td>INI dotted sections, TOML nested tables, YAML block nesting</td>\n<td>Hierarchy preservation, access path consistency</td>\n<td>Nesting depth limits, structure representation differences</td>\n</tr>\n<tr>\n<td>Array Handling</td>\n<td>INI repeated keys, TOML arrays, YAML sequences</td>\n<td>Array content equality, ordering preservation</td>\n<td>Format array support differences, mixed type handling</td>\n</tr>\n<tr>\n<td>Type Representation</td>\n<td>Format-specific type syntax, implicit vs explicit typing</td>\n<td>Type consistency, conversion accuracy</td>\n<td>Type system differences, inference variations</td>\n</tr>\n</tbody></table>\n<p>Cross-format equivalence testing helps validate that the unified output format successfully abstracts away format differences while preserving semantic meaning. These tests catch cases where format-specific processing introduces unintended behavioral differences.</p>\n<p><strong>Test Data Maintenance Strategy</strong> ensures that test datasets remain current, comprehensive, and maintainable as the parser implementation evolves. Maintenance strategy addresses test data organization, update procedures, and quality validation.</p>\n<table>\n<thead>\n<tr>\n<th>Maintenance Aspect</th>\n<th>Strategy Approach</th>\n<th>Automation Support</th>\n<th>Quality Assurance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test Case Discovery</td>\n<td>Systematic exploration of input space, bug-driven case addition</td>\n<td>Automated test case generation, property-based case discovery</td>\n<td>Coverage analysis, gap identification</td>\n</tr>\n<tr>\n<td>Data Quality Validation</td>\n<td>Format compliance checking, expected result verification</td>\n<td>Automated validation pipelines, consistency checking</td>\n<td>Regular audit procedures, quality metrics</td>\n</tr>\n<tr>\n<td>Update Procedures</td>\n<td>Version control integration, change tracking, regression prevention</td>\n<td>Automated update validation, backwards compatibility testing</td>\n<td>Change impact analysis, regression detection</td>\n</tr>\n<tr>\n<td>Organization Maintenance</td>\n<td>Consistent categorization, clear naming conventions, duplicate elimination</td>\n<td>Automated organization validation, duplicate detection</td>\n<td>Regular cleanup procedures, organization audits</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Insight:</strong> Test data strategy success depends on systematic organization and comprehensive coverage rather than large volumes of ad-hoc test cases. Well-organized test data enables efficient debugging, comprehensive validation, and maintainable test suites.</p>\n</blockquote>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Fparsing-pipeline.svg\" alt=\"Complete Parsing Pipeline Flow\"></p>\n<p>The test data strategy must support both automated testing workflows and manual debugging scenarios. Test data should be organized to enable easy identification of relevant test cases, efficient test execution, and clear failure diagnosis when tests fail.</p>\n<p><strong>Performance Test Data Strategy</strong> validates parser behavior under resource pressure and ensures that algorithmic complexity remains acceptable for realistic input sizes. Performance testing requires carefully constructed datasets that stress specific performance characteristics without introducing artificial complexity.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Aspect</th>\n<th>Test Data Characteristics</th>\n<th>Measurement Focus</th>\n<th>Validation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Size Scaling</td>\n<td>Incrementally larger configuration files</td>\n<td>Memory usage, parsing time, throughput</td>\n<td>Linear scaling behavior, memory efficiency</td>\n</tr>\n<tr>\n<td>Nesting Depth</td>\n<td>Increasingly deep nested structures</td>\n<td>Stack usage, recursion handling, algorithm complexity</td>\n<td>Graceful degradation, depth limit handling</td>\n</tr>\n<tr>\n<td>Key Volume</td>\n<td>Large numbers of keys and sections</td>\n<td>Hash table performance, lookup efficiency</td>\n<td>Consistent access times, memory organization</td>\n</tr>\n<tr>\n<td>String Processing</td>\n<td>Large string values, complex escape sequences</td>\n<td>String processing efficiency, memory allocation</td>\n<td>String handling optimization, garbage collection impact</td>\n</tr>\n</tbody></table>\n<p>Performance test data should reflect realistic scaling scenarios that applications might encounter, rather than pathological cases designed to break the parser. The focus should be on ensuring acceptable performance for reasonable input sizes while establishing clear limits for extreme cases.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test Framework</td>\n<td><code>pytest</code> with fixtures and parametrize</td>\n<td><code>pytest</code> with <code>hypothesis</code> for property-based testing</td>\n</tr>\n<tr>\n<td>Test Data Management</td>\n<td>JSON files with expected results</td>\n<td>YAML test definitions with embedded test cases</td>\n</tr>\n<tr>\n<td>Coverage Analysis</td>\n<td><code>coverage.py</code> for line coverage</td>\n<td><code>coverage.py</code> + <code>pytest-cov</code> with branch coverage</td>\n</tr>\n<tr>\n<td>Performance Testing</td>\n<td>Simple timing with <code>time.time()</code></td>\n<td><code>pytest-benchmark</code> with statistical analysis</td>\n</tr>\n<tr>\n<td>Test Organization</td>\n<td>Directory-based test separation</td>\n<td><code>pytest</code> marks and custom test collections</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config-parser/\n  tests/\n    unit/\n      test_tokenizer.py          ← BaseTokenizer unit tests\n      test_ini_parser.py         ← INIParser specific tests\n      test_toml_parser.py        ← TOMLParser specific tests\n      test_yaml_parser.py        ← YAMLParser specific tests\n    integration/\n      test_parsing_pipeline.py   ← End-to-end parsing tests\n      test_format_detection.py   ← Format detection integration\n      test_error_handling.py     ← Cross-component error flow\n    data/\n      golden/\n        ini/                     ← Golden path INI test files\n        toml/                    ← Golden path TOML test files\n        yaml/                    ← Golden path YAML test files\n      edge-cases/\n        tokenizer/               ← Tokenization edge cases\n        parsing/                 ← Parser-specific edge cases\n      errors/\n        syntax/                  ← Syntax error test cases\n        structure/               ← Structure error test cases\n      equivalence/\n        basic-config/            ← Cross-format equivalent configs\n    conftest.py                  ← Shared test fixtures\n    test_utils.py                ← Testing utility functions</code></pre></div>\n\n<p><strong>Test Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/conftest.py - Shared test fixtures and utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestCase</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single parser test case with input, expected output, and metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_content: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_output: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_errors: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    format_hint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestDataLoader</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Loads and manages test data files with caching and validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, test_data_root: Path):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.test_data_root </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> test_data_root</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_test_cases</span><span style=\"color:#E1E4E8\">(self, category: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, format_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load test cases from organized test data directory structure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build path based on category and optional format_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Scan directory for test case files (.json, .yaml, .toml, .ini)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load each file and create TestCase objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Cache loaded test cases for performance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate test case structure and expected results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_equivalence_set</span><span style=\"color:#E1E4E8\">(self, scenario_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TestCase]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load cross-format equivalent test cases for validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load test cases for all formats in equivalence scenario</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that expected outputs are semantically equivalent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return dictionary mapping format -> TestCase</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_data_loader</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Provides TestDataLoader instance for all tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_data_root </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(</span><span style=\"color:#79B8FF\">__file__</span><span style=\"color:#E1E4E8\">).parent </span><span style=\"color:#F97583\">/</span><span style=\"color:#9ECBFF\"> \"data\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> TestDataLoader(test_data_root)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parser_factory</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Factory for creating parser instances with test configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_parser</span><span style=\"color:#E1E4E8\">(format_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">options):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"ini\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            from</span><span style=\"color:#E1E4E8\"> config_parser.ini_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> INIParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> INIParser(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">options)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"toml\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            from</span><span style=\"color:#E1E4E8\"> config_parser.toml_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TOMLParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> TOMLParser(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">options)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"yaml\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            from</span><span style=\"color:#E1E4E8\"> config_parser.yaml_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> YAMLParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> YAMLParser(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">options)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unknown format: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">format_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> create_parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> assert_equivalent_structures</span><span style=\"color:#E1E4E8\">(actual: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], expected: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"root\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Deep comparison of nested dictionary structures with helpful error messages.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compare dictionary keys, reporting missing/extra keys with path context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Recursively compare nested dictionaries and lists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle type coercion differences (e.g., \"1\" vs 1) appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Provide detailed error messages with path information for failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> normalize_test_output</span><span style=\"color:#E1E4E8\">(output: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Normalize parser output for cross-format comparison.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Sort dictionary keys recursively for consistent comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Normalize string representations of numbers and booleans</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle format-specific type differences (e.g., date objects vs strings)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Remove format-specific metadata that shouldn't affect equivalence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Unit Test Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_tokenizer.py - Tokenizer unit tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> config_parser.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseTokenizer, Token, TokenType, Position</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestBaseTokenizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Unit tests for tokenizer functionality across all formats.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_basic_token_generation</span><span style=\"color:#E1E4E8\">(self, test_data_loader):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate basic tokenization for all supported token types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load tokenizer test cases with expected token sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create tokenizer instance and tokenize test input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare generated tokens with expected tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate token types, values, and position information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test edge cases like empty input, whitespace-only, comments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_string_literal_handling</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test complex string literal parsing including escapes and multiline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        test_cases </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'basic_string'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"Hello World\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Hello World\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'escape_sequences'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"Line 1</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">nLine 2</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">t</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Line 1</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Line 2</span><span style=\"color:#79B8FF\">\\t\\\"</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'multiline_string'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'\"\"\"Line 1</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Line 2\"\"\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Line 1</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Line 2\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'literal_string'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"'No</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">nEscape'\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"No</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">nEscape\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> test_name, input_text, expected_value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> test_cases:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create tokenizer with string input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Tokenize and extract string token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate token type is STRING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate processed value matches expected result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate position tracking through string processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_position_tracking</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate accurate line and column tracking during tokenization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        multiline_input </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"\"line1 = \"value1\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        [section]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        line3 = 123\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize multiline input and collect all tokens</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that line numbers increment correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate that column numbers reset after newlines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate position accuracy for tokens spanning multiple characters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test position tracking with tabs, Unicode characters, and mixed line endings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_error_recovery</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test tokenizer behavior with invalid characters and malformed input.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_cases </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"unterminated string </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">never ends\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"invalid </span><span style=\"color:#79B8FF\">\\x00</span><span style=\"color:#9ECBFF\"> null character\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"unicode handling test 🚀\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"mixed quotes 'started with single </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">ended with double\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> error_input </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> error_cases:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create tokenizer with malformed input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Tokenize and expect appropriate error token generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate error position accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify tokenizer continues processing after errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test that subsequent valid tokens are processed correctly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_ini_parser.py - INI parser unit tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestINIParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Unit tests for INI-specific parsing functionality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.mark.parametrize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"test_case\"</span><span style=\"color:#E1E4E8\">, [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pytest.param(TestCase(</span><span style=\"color:#9ECBFF\">\"basic_sections\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"[section1]</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">key=value\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             {</span><span style=\"color:#9ECBFF\">\"section1\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"key\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">}}), </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"basic\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pytest.param(TestCase(</span><span style=\"color:#9ECBFF\">\"global_keys\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"global=value</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">[section]</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">local=value\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             {</span><span style=\"color:#9ECBFF\">\"global\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"section\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"local\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">}}), </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"global\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_section_parsing</span><span style=\"color:#E1E4E8\">(self, test_case, parser_factory):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test section header parsing and nested structure creation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser_factory(</span><span style=\"color:#9ECBFF\">\"ini\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse test case input content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate section structure matches expected output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify nested dictionary organization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test section name extraction and normalization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate global key handling before first section</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_comment_handling</span><span style=\"color:#E1E4E8\">(self, parser_factory):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test comment processing with different comment styles.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ini_content </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ; This is a semicolon comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        # This is a hash comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        [section]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        key1 = value1  ; inline semicolon comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        key2 = value2  # inline hash comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ; Another comment</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        key3 = value3</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse INI content with mixed comment styles</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify comments are ignored during parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate inline comments don't affect value processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test comment handling configuration options</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify structure contains only actual key-value pairs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_value_processing</span><span style=\"color:#E1E4E8\">(self, parser_factory):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test key-value parsing including quotes, escapes, and type inference.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        test_cases </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'unquoted_string'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'key = value'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'value'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'quoted_string'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'key = \"quoted value\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'quoted value'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'number_inference'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'key = 123'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">123</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'boolean_inference'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'key = true'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (</span><span style=\"color:#9ECBFF\">'escaped_quotes'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'key = \"say </span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"hello</span><span style=\"color:#79B8FF\">\\\\</span><span style=\"color:#9ECBFF\">\"\"'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'say \"hello\"'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser_factory(</span><span style=\"color:#9ECBFF\">\"ini\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> test_name, ini_line, expected_value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> test_cases:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse single key-value line in section context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract parsed value from result structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate value matches expected result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify type inference worked correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test whitespace trimming and quote processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span></code></pre></div>\n\n<p><strong>Integration Test Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/integration/test_parsing_pipeline.py - End-to-end integration tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestParsingPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Integration tests for complete parsing workflow.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_file_to_dictionary_pipeline</span><span style=\"color:#E1E4E8\">(self, test_data_loader):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test complete pipeline from configuration files to parsed dictionaries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> format_type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'ini'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'toml'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'yaml'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            golden_cases </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> test_data_loader.load_test_cases(</span><span style=\"color:#9ECBFF\">'golden'</span><span style=\"color:#E1E4E8\">, format_type)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> test_case </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> golden_cases:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create parser for format type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse test case input content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate output structure matches expected result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify no errors occurred during parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test with different parser configuration options</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_cross_format_equivalence</span><span style=\"color:#E1E4E8\">(self, test_data_loader, parser_factory):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate semantically equivalent configurations produce consistent results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        equivalence_scenarios </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'basic_config'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'nested_structures'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'array_handling'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'type_inference'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> scenario </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> equivalence_scenarios:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            equivalent_cases </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> test_data_loader.load_equivalence_set(scenario)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse same logical configuration in all supported formats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Normalize output structures for comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate semantic equivalence across formats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Document and validate acceptable format differences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test edge cases where equivalence might not be perfect</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_error_context_propagation</span><span style=\"color:#E1E4E8\">(self, parser_factory):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test error information flow from tokenizer through parser to user.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        malformed_configs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'ini'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'[broken section</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">key without section'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'toml'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'[table.redefinition]</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">value = 1</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">[table]</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">conflict = 2'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'yaml'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'mapping:</span><span style=\"color:#79B8FF\">\\n\\t</span><span style=\"color:#9ECBFF\">item: value</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">  other: value'</span><span style=\"color:#6A737D\">  # mixed tabs/spaces</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> format_type, malformed_content </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> malformed_configs.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser_factory(format_type)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse malformed content and expect parsing errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate error messages contain helpful context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify error positions are accurate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test error recovery and continued parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate error message quality and actionability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints:</strong></p>\n<p><strong>Milestone 1 Checkpoint - INI Parser:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run INI parser tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_ini_parser.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_section_parsing[basic] PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_section_parsing[global] PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_comment_handling PASSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># test_value_processing PASSED</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification:</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from config_parser.ini_parser import INIParser</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">parser = INIParser()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">result = parser.parse('[database]\\nhost = localhost\\nport = 5432\\n# comment line\\nuser = admin')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print('Parsed structure:', result)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\"># Should output: {'database': {'host': 'localhost', 'port': 5432, 'user': 'admin'}}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint - TOML Tokenizer:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run tokenizer tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_tokenizer.py::TestBaseTokenizer::test_basic_token_generation</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual tokenization verification:</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from config_parser.tokenizer import BaseTokenizer</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tokenizer = BaseTokenizer('[table]\\nkey = </span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">value with </span><span style=\"color:#79B8FF\">\\\\\\\\</span><span style=\"color:#9ECBFF\"> escapes</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tokens = tokenizer.tokenize()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">for token in tokens:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    print(f'{token.type.name}: {token.value} at {token.position.line}:{token.position.column}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Performance Testing Integration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/performance/test_parser_performance.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestParserPerformance</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Performance validation for parser components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.mark.benchmark</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_file_size_scaling</span><span style=\"color:#E1E4E8\">(self, parser_factory, benchmark):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate parsing performance scales appropriately with file size.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate configuration files of increasing sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Benchmark parsing time for each size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate linear scaling characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Measure memory usage during parsing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Establish performance baselines for regression testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_nesting_depth_limits</span><span style=\"color:#E1E4E8\">(self, parser_factory):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test parser behavior with deeply nested structures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_depth </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate deeply nested test configurations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse configurations at increasing depth levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Measure parsing time and memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Identify practical depth limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate graceful handling of extreme nesting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive debugging strategies, symptom diagnosis, and troubleshooting techniques for successful parsing implementation</p>\n</blockquote>\n<p>Debugging configuration parsers presents unique challenges that differ significantly from typical application debugging. Unlike business logic where errors often manifest as obvious behavioral inconsistencies, parsing errors frequently emerge as subtle structural misalignments, tokenization edge cases, or context-sensitive interpretation failures. The multi-format nature of our parser amplifies these challenges by introducing format-specific edge cases alongside shared infrastructure bugs.</p>\n<p>Think of debugging a parser as forensic investigation rather than traditional problem-solving. When a parser fails, you must trace backwards through multiple layers: the final output structure, the parse tree construction, the tokenization process, and the original character-by-character scanning. Each layer introduces potential failure points, and symptoms at the output level often obscure root causes buried deep in the tokenization logic. This section provides systematic approaches for conducting this forensic analysis effectively.</p>\n<p>The complexity stems from parsing&#39;s inherently contextual nature. The same character sequence &quot;key = value&quot; might be valid in INI format but invalid in YAML context, while multiline strings behave completely differently across TOML and YAML. Context sensitivity means bugs often manifest inconsistently - working perfectly for simple cases but failing mysteriously when nesting levels change, indentation patterns shift, or specific character combinations appear.</p>\n<h3 id=\"common-bug-symptoms-and-causes\">Common Bug Symptoms and Causes</h3>\n<p>Understanding the symptom-to-cause mapping for parsing bugs accelerates debugging by directing investigation toward the most likely root causes. Parsing failures typically fall into distinct categories with characteristic symptoms that point to specific implementation areas.</p>\n<p><img src=\"/api/project/config-parser/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Detection and Reporting Flow\"></p>\n<h4 id=\"structural-misrepresentation-symptoms\">Structural Misrepresentation Symptoms</h4>\n<p><strong>Missing Nested Structure</strong>: When configuration content contains hierarchical structure but the parsed output flattens it into a single level, the bug typically originates in path expansion logic. For INI parsers, this manifests when dotted section names like <code>[database.connection.pool]</code> create a flat key instead of nested dictionaries. The root cause usually lies in <code>create_nested_section</code> not properly splitting section paths or <code>merge_nested_dicts</code> failing to create intermediate levels.</p>\n<p><strong>Incorrect Nesting Depth</strong>: Output shows wrong nesting levels - either too shallow or too deep. In YAML parsers, this symptom points to indentation stack management failures. The <code>IndentationStack</code> might incorrectly calculate target levels during <code>pop_to_level</code> operations, or <code>_handle_indentation_transition</code> might push frames when it should maintain current level. TOML parsers exhibit this when dotted key expansion in <code>expand_dotted_key</code> creates too many intermediate tables.</p>\n<p><strong>Key Collision Overwriting</strong>: Later-defined keys silently overwrite earlier ones instead of generating conflicts. This indicates insufficient symbol table tracking in TOML parsers. The <code>SymbolTable</code> should register every definition through <code>register_definition</code> and validate against redefinition rules. Missing validation allows conflicting definitions to proceed unchecked.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Format Context</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Flat output for nested syntax</td>\n<td>INI dotted sections</td>\n<td>Section path splitting failure</td>\n<td><code>create_nested_section</code> logic</td>\n</tr>\n<tr>\n<td>Wrong nesting depth</td>\n<td>YAML indentation</td>\n<td>Stack management errors</td>\n<td><code>IndentationStack</code> operations</td>\n</tr>\n<tr>\n<td>Silent key overwrites</td>\n<td>TOML dotted keys</td>\n<td>Missing conflict detection</td>\n<td><code>SymbolTable</code> validation</td>\n</tr>\n<tr>\n<td>Array structure lost</td>\n<td>TOML array-of-tables</td>\n<td>Table vs array confusion</td>\n<td><code>parse_table_header</code> logic</td>\n</tr>\n</tbody></table>\n<h4 id=\"value-processing-failures\">Value Processing Failures</h4>\n<p><strong>Type Conversion Errors</strong>: Values appear as strings when they should be numbers, booleans, or other types. This symptom indicates failures in type inference logic. Each format&#39;s <code>infer_type</code> method might contain incomplete boolean detection, numeric parsing edge cases, or missing null value recognition. YAML&#39;s implicit typing is particularly susceptible due to complex conversion rules.</p>\n<p><strong>String Escape Sequence Issues</strong>: Literal backslashes appear in output instead of processed escapes, or escape sequences cause parsing to fail entirely. The root cause lies in <code>read_string_literal</code> implementation within the tokenizer. Multi-format parsers must handle different escape rules: INI uses minimal escaping, TOML has basic and literal string variants, while YAML has complex folding rules.</p>\n<p><strong>Multiline Value Corruption</strong>: Multiline strings split incorrectly, lose formatting, or include unintended content. This points to line continuation logic failures. Each format handles multilines differently: INI uses backslash continuation, TOML has multiline string delimiters, YAML uses folding indicators. The tokenizer&#39;s state machine must track multiline context accurately.</p>\n<p><strong>Inline Comment Contamination</strong>: Values include comment text that should be stripped. This indicates inadequate comment detection during value parsing. The <code>process_key_value_pair</code> logic must identify comment boundaries within lines while respecting quoted string contexts where hash or semicolon characters are literal.</p>\n<h4 id=\"tokenization-boundary-errors\">Tokenization Boundary Errors</h4>\n<p><strong>Token Splitting Failures</strong>: Single logical units split across multiple tokens, or multiple units merged into single tokens. This symptom reveals boundary detection errors in the tokenizer. Complex tokens like dotted identifiers, numeric literals with underscores, or quoted strings with embedded delimiters require sophisticated boundary logic.</p>\n<p><strong>Context Sensitivity Violations</strong>: Same character sequences tokenized differently in identical contexts, or differently in contexts where they should be identical. This points to inadequate context tracking in <code>BaseTokenizer</code>. State machines must maintain consistent context interpretation across equivalent parsing situations.</p>\n<p><strong>Position Tracking Inaccuracies</strong>: Error messages report wrong line/column positions, making debugging extremely difficult. The <code>Position</code> calculation logic in <code>current_position</code> must accurately track line breaks, character offsets, and column advancement through all tokenization paths including multiline constructs.</p>\n<h4 id=\"format-detection-ambiguities\">Format Detection Ambiguities</h4>\n<p><strong>Wrong Format Selection</strong>: Parser selects incorrect format, causing interpretation failures. The <code>detect_format</code> algorithm relies on syntactic signatures that distinguish formats. Ambiguous content might match multiple format patterns, requiring confidence scoring to select the most appropriate parser.</p>\n<p><strong>Format Switching Mid-Parse</strong>: Parser begins with one format interpretation then shifts to another, causing structural inconsistencies. This indicates insufficient lookahead in format detection. The analysis window must examine enough content to establish format identity before committing to specific parsing logic.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The most challenging bugs combine multiple symptom categories. For example, a TOML array-of-tables with complex nested structure might simultaneously exhibit tokenization boundary errors (array brackets), structural misrepresentation (table nesting), and value processing failures (nested values). Systematic diagnosis must examine each layer independently before considering interactions.</p>\n</blockquote>\n<h4 id=\"common-error-propagation-patterns\">Common Error Propagation Patterns</h4>\n<p>Parsing errors often cascade through multiple system layers, making root cause identification challenging. Understanding propagation patterns helps focus debugging efforts on primary failure points rather than secondary symptoms.</p>\n<p><strong>Tokenization → Structure Cascade</strong>: Incorrect tokenization creates valid but wrong tokens that produce structurally incorrect parse trees. For example, a quoted string boundary missed by the tokenizer appears as separate IDENTIFIER and STRING tokens. The parser successfully processes these tokens but creates wrong structure. Diagnosis must verify token correctness before examining structural logic.</p>\n<p><strong>Context → Value Cascade</strong>: Wrong parsing context causes correct tokens to be interpreted inappropriately. A YAML scalar appears in sequence context but gets processed as mapping key, leading to structural errors. The tokenization is correct, but context tracking failures cause misinterpretation.</p>\n<p><strong>Format → Everything Cascade</strong>: Incorrect format detection causes the wrong parser to process content, leading to systematic failures across all processing layers. An INI file processed by the TOML parser generates numerous errors because fundamental syntax assumptions are violated. This pattern requires format detection validation as the first debugging step.</p>\n<h3 id=\"debugging-techniques-for-parsers\">Debugging Techniques for Parsers</h3>\n<p>Effective parser debugging requires specialized techniques that differ from general software debugging. The multi-layered nature of parsing - from character scanning through tokenization to structure building - demands systematic approaches for isolating failures at each level.</p>\n<h4 id=\"layered-diagnosis-methodology\">Layered Diagnosis Methodology</h4>\n<p><strong>Layer 1: Character Stream Analysis</strong>: Begin debugging at the most fundamental level by examining the raw input character stream. Many parsing failures originate from assumptions about input encoding, line ending conventions, or whitespace handling that prove incorrect for specific content.</p>\n<p>Create character-level inspection by implementing a diagnostic scanner that reveals non-printable characters, mixed encodings, and whitespace variations. Unicode normalization issues, byte order marks, or mixed line ending styles (Unix LF vs Windows CRLF) frequently cause parsing failures that manifest as mysterious structural errors.</p>\n<p>Position tracking verification requires tracing through the character stream manually to confirm that <code>current_position</code> calculations match actual line and column positions. Off-by-one errors in position tracking make all subsequent error reporting unreliable, severely hampering debugging efforts.</p>\n<p><strong>Layer 2: Token Stream Validation</strong>: After confirming character stream integrity, examine the token stream produced by the tokenizer. This intermediate representation reveals whether lexical analysis correctly identifies meaningful units within the character stream.</p>\n<p>Token boundary verification involves examining each token&#39;s <code>raw_text</code> field against its <code>position</code> information to confirm accurate extraction. Boundary errors often appear as tokens that include extra characters, exclude expected characters, or split logical units incorrectly.</p>\n<p>Token type accuracy checking ensures that the tokenizer assigns correct <code>TokenType</code> values to extracted text. A numeric literal incorrectly classified as IDENTIFIER causes downstream parsing failures that appear as type conversion errors rather than tokenization bugs.</p>\n<p>Context tracking validation confirms that tokenization state changes appropriately as content context shifts. Multiline strings, comment regions, and format-specific constructs require state machine transitions that maintain consistent interpretation rules.</p>\n<p><strong>Layer 3: Parse Tree Structure Inspection</strong>: With correct tokenization confirmed, examine the intermediate parse tree structure built by format-specific parsers. This layer reveals whether structural interpretation correctly translates token sequences into hierarchical representations.</p>\n<p>Node hierarchy verification checks that parent-child relationships in the parse tree match expected nesting from the original content. Missing intermediate nodes, incorrect nesting depths, or orphaned nodes indicate structural interpretation failures.</p>\n<p>Node metadata accuracy ensures that <code>ParseNode</code> instances contain correct position information, token references, and format-specific metadata. Inaccurate metadata complicates error reporting and downstream processing.</p>\n<p><strong>Layer 4: Final Output Validation</strong>: The final debugging layer examines the conversion from parse tree to the unified output format. This transformation must preserve all structural relationships while normalizing format-specific representations into consistent dictionary structures.</p>\n<p>Key path accuracy verification confirms that nested dictionary keys match expected hierarchical paths from the original content. The <code>convert_parse_tree_to_dict</code> transformation must maintain all structural relationships without introducing spurious levels or losing intended nesting.</p>\n<p>Value conversion correctness ensures that the type inference and conversion process produces appropriate Python types for each value. String-to-type conversion errors often appear as unexpected string values in contexts where numbers or booleans are expected.</p>\n<h4 id=\"incremental-complexity-testing\">Incremental Complexity Testing</h4>\n<p><strong>Minimal Reproduction Construction</strong>: When debugging complex parsing failures, construct minimal examples that reproduce the same symptoms with the simplest possible input. This technique isolates the specific conditions that trigger failures without the complexity of realistic configuration files.</p>\n<p>Start with single-line examples that exhibit the problematic behavior. If a complex nested TOML structure fails to parse correctly, create a minimal table definition that shows the same structural issues. Gradually increase complexity while maintaining the failure symptom to identify the exact complexity threshold where problems appear.</p>\n<p>Format isolation testing processes the same logical content through different format parsers to identify format-specific versus general infrastructure bugs. If the same nested structure parses correctly in YAML but fails in TOML, the issue lies in TOML-specific logic rather than shared infrastructure.</p>\n<p><strong>Progressive Feature Addition</strong>: Build parser functionality incrementally, validating correct behavior at each step before adding complexity. This approach prevents multiple bugs from interacting and obscuring individual failure points.</p>\n<p>Begin with basic key-value parsing without nesting, comments, or complex values. Confirm perfect behavior for simple cases before introducing sectioning, then nesting, then complex value types. Each addition point becomes a checkpoint for isolating newly introduced bugs.</p>\n<p>Feature interaction testing examines combinations of parser features that work individually but fail when combined. Comments within multiline strings, nested structures with dotted keys, or array-of-tables with inline values represent interaction points where subtle bugs frequently emerge.</p>\n<h4 id=\"state-inspection-and-tracing\">State Inspection and Tracing</h4>\n<p><strong>Parser State Snapshots</strong>: Implement diagnostic capabilities that capture complete parser state at critical processing points. State snapshots provide detailed views of internal parser condition when failures occur.</p>\n<p>The <code>ParseContext</code> should support diagnostic mode that records position progression, tokenization state, parse tree construction steps, and error accumulation. This information enables post-mortem analysis of parsing sessions to identify the exact point where processing diverged from expected behavior.</p>\n<p>Symbol table inspection for TOML parsing reveals the current definition state, helping identify redefinition conflicts or missing implicit table creation. The <code>SymbolTable</code> diagnostic interface should expose all registered definitions with their types and positions.</p>\n<p>Indentation stack analysis for YAML parsing shows the current nesting context and established indentation levels. The <code>IndentationStack</code> diagnostic view helps identify incorrect stack operations that lead to structural misinterpretation.</p>\n<p><strong>Token Stream Replay</strong>: Implement token stream recording and replay capabilities that enable re-processing specific token sequences with different parser configurations or enhanced diagnostics enabled.</p>\n<p>Token stream serialization captures the complete sequence of tokens produced for problematic input, enabling offline analysis and regression testing. Replay functionality allows processing the same token stream multiple times with different diagnostic settings or parser modifications.</p>\n<p>Interactive token inspection provides step-by-step token stream examination with parser state inspection at each token boundary. This technique helps identify the specific token where parsing logic makes incorrect decisions.</p>\n<h4 id=\"error-message-archaeological-analysis\">Error Message Archaeological Analysis</h4>\n<p><strong>Error Context Reconstruction</strong>: Parser error messages often provide insufficient context for effective debugging. Implement enhanced error context generation that shows not just the immediate error location but the parsing context leading to the failure.</p>\n<p>The <code>create_error_context</code> function should include previous parsing decisions, current parser state, and upcoming tokens to provide comprehensive situational awareness. Context windows should adapt to the specific error type - structural errors need broader context than tokenization errors.</p>\n<p>Error correlation analysis identifies relationships between multiple errors that stem from single root causes. Cascading failures often generate numerous error messages that obscure the primary issue. Group related errors and present root cause analysis rather than symptom catalogs.</p>\n<p><strong>Error Evolution Tracking</strong>: Track how error conditions evolve during parsing to understand failure progression. Some errors represent recoverable conditions that become fatal due to inadequate error recovery, while others indicate fundamental structural problems.</p>\n<p>Recovery decision analysis examines the effectiveness of error recovery strategies by tracking parsing progress after recovery attempts. The <code>ErrorRecoveryState</code> should maintain metrics on recovery success rates and provide feedback for recovery strategy refinement.</p>\n<h3 id=\"debugging-tools-and-inspection\">Debugging Tools and Inspection</h3>\n<p>Effective parser debugging requires specialized tooling that provides visibility into the multi-layered parsing process. Unlike general application debugging where breakpoints and variable inspection suffice, parser debugging demands tools that can trace through character streams, token sequences, and structural transformations while maintaining context awareness.</p>\n<h4 id=\"interactive-parser-inspection-framework\">Interactive Parser Inspection Framework</h4>\n<p><strong>Token-by-Token Stepping Interface</strong>: Implement an interactive debugging interface that allows stepping through tokenization one token at a time while inspecting complete parser state. This capability proves essential for understanding how specific character sequences translate into token streams and how parser decisions evolve.</p>\n<p>The stepping interface should display the current character position, upcoming character sequences, tokenizer state machine status, and accumulated token stream. At each step, inspect the decision logic for token boundary detection, type classification, and value extraction. This granular visibility reveals tokenization edge cases that are invisible in batch processing.</p>\n<p>Context-aware display formatting presents different views optimized for different parsing phases. During tokenization, emphasize character boundaries and state transitions. During structural parsing, highlight nesting relationships and symbol table evolution. During error recovery, focus on recovery decision points and confidence metrics.</p>\n<p><strong>Parse Tree Visualization Tools</strong>: Develop visual representations of intermediate parse tree structures that reveal hierarchical relationships and node metadata. Text-based tree displays work well for automated testing, but interactive visual tools provide superior debugging capability for complex structures.</p>\n<p>Node inspection capabilities should expose all <code>ParseNode</code> fields including position information, token references, child relationships, and format-specific metadata. Interactive expansion and collapse of subtrees helps manage complexity when debugging large configuration files.</p>\n<p>Comparative tree visualization shows differences between expected and actual parse tree structures, highlighting specific nodes where structure diverges from expectations. This capability accelerates debugging of structural interpretation issues.</p>\n<h4 id=\"tokenization-analysis-toolkit\">Tokenization Analysis Toolkit</h4>\n<p><strong>Character Stream Inspector</strong>: Build diagnostic tools that reveal character-level details often hidden by text editors and terminal displays. Many parsing bugs stem from invisible characters, mixed encodings, or non-standard whitespace that standard tools don&#39;t reveal clearly.</p>\n<p>The inspector should display hexadecimal character codes alongside visual representations, highlight different whitespace types distinctly, and identify potential encoding issues. Unicode normalization problems, zero-width characters, and mixed line ending styles become immediately visible.</p>\n<p>Position mapping verification tools trace the relationship between character stream offsets and calculated line/column positions. Generate position maps that can be compared against expected values to identify off-by-one errors or miscalculated boundaries.</p>\n<p><strong>Token Stream Analysis Suite</strong>: Develop comprehensive tools for examining token streams in detail, including sequence analysis, boundary verification, and context tracking validation.</p>\n<p>Token sequence comparison tools examine token streams produced from similar input to identify inconsistencies in tokenization behavior. This capability helps identify context sensitivity bugs where equivalent content tokenizes differently in different parsing contexts.</p>\n<p>Boundary accuracy verification reconstructs original character sequences from token <code>raw_text</code> fields and position information, comparing against the original input to identify extraction errors. Gaps or overlaps in token coverage indicate boundary detection problems.</p>\n<p>Token type distribution analysis reveals patterns in token classification that can identify systematic errors in type detection logic. Unexpected type distributions often point to classification rules that behave differently than intended.</p>\n<h4 id=\"parser-state-inspection-utilities\">Parser State Inspection Utilities</h4>\n<p><strong>Symbol Table Diagnostic Views</strong>: For TOML parsing, implement comprehensive symbol table inspection that reveals definition conflicts, implicit table creation, and key path resolution. The <code>SymbolTable</code> diagnostic interface should support queries about definition history and conflict analysis.</p>\n<p>Definition timeline views show the sequence of key and table definitions with their positions and types. This information helps identify redefinition violations and understand how implicit table creation interacts with explicit definitions.</p>\n<p>Conflict detection analysis explains why specific key combinations are invalid, providing detailed explanations of TOML&#39;s redefinition rules. When debugging table conflicts, show the complete definition history that led to the conflict condition.</p>\n<p><strong>Indentation Stack Analysis</strong>: For YAML parsing, develop tools that visualize indentation stack evolution and validate stack operations. The <code>IndentationStack</code> diagnostic interface should expose stack frame details and transition logic.</p>\n<p>Stack frame inspection shows the complete frame history including indentation levels, structure types, and line numbers where frames were established. This information helps identify incorrect stack operations that lead to nesting errors.</p>\n<p>Indentation level analysis validates that calculated indentation levels match established patterns and identifies ambiguous indentation that could be interpreted multiple ways. Level transition visualization shows how stack operations respond to indentation changes.</p>\n<h4 id=\"error-analysis-and-recovery-tools\">Error Analysis and Recovery Tools</h4>\n<p><strong>Error Pattern Recognition</strong>: Implement tools that analyze error patterns across multiple parsing attempts to identify systematic issues in parser logic or input handling. Pattern recognition helps distinguish between input-specific errors and parser implementation bugs.</p>\n<p>Error clustering analysis groups similar errors by type, location patterns, and context to identify common failure modes. This analysis helps prioritize parser improvements by focusing on the most frequent error categories.</p>\n<p>Recovery effectiveness measurement tracks the success rate of different error recovery strategies and provides data for refining recovery logic. The <code>ErrorRecoveryState</code> metrics help optimize recovery decision-making.</p>\n<p><strong>Diagnostic Trace Generation</strong>: Develop comprehensive tracing that captures the complete parsing decision sequence for post-mortem analysis. Traces should include character scanning, tokenization decisions, parser state transitions, and error handling decisions.</p>\n<p>Execution path analysis identifies the specific code paths followed during parsing, helping isolate bugs to particular logic branches. This information proves especially valuable for debugging complex conditional logic in recursive descent parsers.</p>\n<p>Decision point logging records the rationale for parser decisions at critical points, providing insight into why specific interpretation paths were chosen. This information helps identify cases where parser logic makes reasonable but incorrect decisions based on insufficient context.</p>\n<h4 id=\"performance-and-scalability-analysis\">Performance and Scalability Analysis</h4>\n<p><strong>Parsing Performance Profiler</strong>: Implement profiling tools that identify performance bottlenecks in parsing logic, focusing on operations that scale poorly with input size or complexity.</p>\n<p>Time allocation analysis shows how parsing time distributes across different operations: tokenization, structural analysis, value conversion, and output generation. This information helps identify optimization opportunities and scalability concerns.</p>\n<p>Memory usage tracking reveals memory allocation patterns and identifies potential memory leaks or excessive allocation in parsing logic. Parser implementations should maintain bounded memory usage regardless of input complexity.</p>\n<p><strong>Scalability Stress Testing</strong>: Develop tools that generate configuration files of varying sizes and complexity to test parser behavior under stress conditions. Scalability testing reveals performance degradation patterns and memory usage growth.</p>\n<p>Input complexity graduation creates test cases with systematically increasing complexity: nesting depth, table count, array size, and string length. This approach identifies complexity thresholds where parser performance degrades significantly.</p>\n<p>Resource consumption monitoring tracks CPU usage, memory allocation, and parsing time across different input categories to establish performance baselines and identify regression conditions.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical tools and techniques for implementing comprehensive debugging capabilities for your configuration file parser. The focus is on building diagnostic infrastructure that integrates seamlessly with your parser implementation while providing powerful debugging capabilities.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Diagnostic Output</td>\n<td>Print statements with structured formatting</td>\n<td>Rich library for terminal formatting and interactive displays</td>\n</tr>\n<tr>\n<td>State Inspection</td>\n<td>JSON serialization of parser state</td>\n<td>Custom inspection framework with interactive browsing</td>\n</tr>\n<tr>\n<td>Token Visualization</td>\n<td>Plain text token stream dumps</td>\n<td>HTML/web-based token inspector with syntax highlighting</td>\n</tr>\n<tr>\n<td>Error Analysis</td>\n<td>Basic error categorization and counting</td>\n<td>Statistical analysis with matplotlib for error pattern visualization</td>\n</tr>\n<tr>\n<td>Interactive Debugging</td>\n<td>Command-line REPL with parser commands</td>\n<td>Web-based debugging interface with real-time visualization</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>config-parser/\n  src/\n    parser/\n      core/\n        tokenizer.py              ← BaseTokenizer with diagnostic capabilities\n        errors.py                 ← Error types and diagnostic functions\n        context.py                ← ParseContext with debugging support\n      formats/\n        ini_parser.py             ← INI parser with diagnostic hooks\n        toml_parser.py            ← TOML parser with symbol table inspection\n        yaml_parser.py            ← YAML parser with stack analysis\n      debugging/\n        __init__.py               ← Public debugging API\n        diagnostic_tools.py       ← Core diagnostic infrastructure\n        token_inspector.py        ← Token stream analysis tools\n        state_inspector.py        ← Parser state inspection utilities\n        error_analyzer.py         ← Error pattern analysis and reporting\n        interactive_debugger.py   ← Interactive debugging interface\n      testing/\n        debug_test_cases.py       ← Test cases specifically for debugging scenarios\n        diagnostic_fixtures.py    ← Test fixtures with known debugging patterns\n  examples/\n    debug_examples/\n      problematic_configs/        ← Example files that demonstrate common bugs\n      debugging_sessions.py       ← Example debugging workflows</code></pre></div>\n\n<h4 id=\"core-diagnostic-infrastructure\">Core Diagnostic Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core diagnostic infrastructure for configuration parser debugging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides comprehensive state inspection, error analysis, and interactive debugging capabilities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> traceback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DiagnosticLevel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MINIMAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"minimal\"</span><span style=\"color:#6A737D\">      # Basic error information only</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STANDARD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"standard\"</span><span style=\"color:#6A737D\">    # Include context and suggestions</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPREHENSIVE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"comprehensive\"</span><span style=\"color:#6A737D\">  # Full state inspection and traces</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTERACTIVE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"interactive\"</span><span style=\"color:#6A737D\">      # Enable interactive debugging features</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DiagnosticConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    level: DiagnosticLevel </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DiagnosticLevel.</span><span style=\"color:#79B8FF\">STANDARD</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    include_token_stream: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    include_parse_tree: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    include_position_context: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_context_lines: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enable_color_output: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    save_diagnostic_traces: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    trace_output_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParserDiagnostics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive diagnostic system for parser debugging and analysis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Integrates with all parser components to provide detailed inspection capabilities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: DiagnosticConfig </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> DiagnosticConfig()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.diagnostic_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_patterns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.performance_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> capture_parsing_session</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, format_hint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'ParsingSession'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Create a comprehensive diagnostic session for parsing the given content.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns a session object that tracks all parsing operations and state changes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize session with input content and configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up diagnostic hooks for tokenizer, parser, and error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Enable state capture at each major parsing phase</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Configure error tracking and recovery decision logging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return configured session ready for parsing execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_tokenization_issues</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_tokens: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'TokenizationAnalysis'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Perform detailed analysis of tokenization behavior for diagnostic purposes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Identifies boundary issues, type classification problems, and context sensitivity bugs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create tokenizer with full diagnostic logging enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Process content character-by-character with state tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Analyze token boundaries and type classification decisions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compare against expected tokens if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate comprehensive analysis report with identified issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inspect_parser_state</span><span style=\"color:#E1E4E8\">(self, parser_instance: Any, checkpoint_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Capture complete parser state snapshot for detailed inspection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Includes symbol tables, indentation stacks, and all context information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract current position and context from parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Serialize symbol table state (for TOML parser)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Capture indentation stack (for YAML parser)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Include token stream position and lookahead state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Format state information for human-readable inspection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> trace_error_propagation</span><span style=\"color:#E1E4E8\">(self, error: ParseError) -> </span><span style=\"color:#9ECBFF\">'ErrorTrace'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze how errors propagate through parser components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Identifies root causes and distinguishes primary errors from cascading symptoms.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Analyze error context and position information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Trace backwards through parsing decisions leading to error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify related errors that may stem from same root cause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Classify error as primary failure vs cascading symptom</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate trace showing error evolution and propagation path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenizationAnalysis</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Analysis results for tokenization diagnostic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.character_analysis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.token_boundaries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.type_classification_issues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context_sensitivity_violations </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.position_tracking_errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_report</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate comprehensive tokenization analysis report.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Summarize character-level issues found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Detail token boundary problems with examples</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: List type classification errors with corrections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Highlight context sensitivity violations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Format as readable report with recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParsingSession</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive diagnostic session that tracks all aspects of a parsing operation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides detailed visibility into parser behavior for debugging purposes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: DiagnosticConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timeline </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []  </span><span style=\"color:#6A737D\"># Chronological record of parsing operations</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state_snapshots </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># State captures at key points</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []  </span><span style=\"color:#6A737D\"># Complete error tracking</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.performance_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># Timing and resource usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_parsing</span><span style=\"color:#E1E4E8\">(self, parser_class, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">parser_options) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute parsing with full diagnostic tracking enabled.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns both parsing results and comprehensive diagnostic information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize parser with diagnostic hooks enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Track parsing timeline with timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Capture state snapshots at major parsing phases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record all errors and recovery decisions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate final diagnostic report with recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_failure_points</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#9ECBFF\">'FailureAnalysis'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Identify specific points where parsing failed or made incorrect decisions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Provides targeted analysis for debugging specific issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Examine error history for failure patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify decision points that led to incorrect results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Analyze context conditions at each failure point</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate specific recommendations for each identified issue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Prioritize failure points by impact and fixing difficulty</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"token-stream-analysis-tools\">Token Stream Analysis Tools</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Specialized tools for analyzing token stream behavior and identifying tokenization issues.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TokenStreamInspector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Comprehensive token stream analysis with boundary verification and context tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tokenizer_class):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer_class </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer_class</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_token_boundaries</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'BoundaryAnalysis'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Verify token boundary detection accuracy by comparing extracted tokens</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        against original content positions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Tokenize content with position tracking enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Reconstruct original text from token raw_text and positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify gaps or overlaps in token coverage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate position calculations against actual character positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate boundary accuracy report with specific issues highlighted</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_tokenization_contexts</span><span style=\"color:#E1E4E8\">(self, test_cases: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'ContextComparisonAnalysis'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Compare tokenization behavior across different contexts to identify</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        context sensitivity bugs where equivalent content tokenizes differently.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Process each test case through tokenizer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify equivalent content patterns across test cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare token classification for equivalent patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Flag inconsistencies in context-sensitive interpretation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate comparison report highlighting inconsistent behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_string_literal_handling</span><span style=\"color:#E1E4E8\">(self, test_strings: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'StringHandlingAnalysis'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Comprehensive testing of string literal processing including escape sequences,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        multiline handling, and quote character processing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test each string format supported by parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify escape sequence processing accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate multiline string boundary detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test quote character handling and nesting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate report on string processing capabilities and limitations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InteractiveTokenDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Interactive debugging interface for step-by-step token stream analysis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tokenizer_class):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.content </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tokenizer_class(content)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_position </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_interactive_session</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Launch interactive debugging session with command-line interface.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Supports stepping through tokenization and state inspection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize tokenizer with diagnostic mode enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up command processing loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Implement commands: next, peek, inspect, context, reset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Provide help system and command completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Enable state inspection and modification during session</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> step_to_next_token</span><span style=\"color:#E1E4E8\">(self) -> Token:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Advance tokenizer by one token and display detailed processing information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Shows character consumption, state changes, and decision logic.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Capture tokenizer state before processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Advance tokenizer by one token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Display character consumption and boundary detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Show state machine transitions and decision points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Present token result with classification rationale</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"parser-state-inspection-framework\">Parser State Inspection Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Parser state inspection utilities for examining internal parser condition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">during complex parsing operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TOMLParserInspector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Specialized inspection tools for TOML parser state and symbol table analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, parser_instance):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser_instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inspect_symbol_table</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate comprehensive view of current symbol table state including</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        all definitions, their types, and conflict detection status.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract all registered definitions from symbol table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Organize definitions by key path and definition type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify potential conflicts and redefinition violations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Show implicit table creation history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Format as hierarchical view showing definition relationships</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_table_conflicts</span><span style=\"color:#E1E4E8\">(self, table_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'ConflictAnalysis'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze potential conflicts for a given table path and explain</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        TOML redefinition rules in the context of current definitions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Examine existing definitions that overlap with table_path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply TOML redefinition rules to identify conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Explain why conflicts exist with reference to specification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Suggest alternative approaches that avoid conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate educational analysis explaining TOML semantics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> trace_dotted_key_expansion</span><span style=\"color:#E1E4E8\">(self, key_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#9ECBFF\">'ExpansionTrace'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Trace the process of expanding dotted keys into nested structure,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        showing intermediate table creation and final value assignment.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Simulate dotted key expansion process step by step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Show intermediate table creation decisions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify implicit vs explicit table creation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate final structure matches expected nesting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate trace showing complete expansion process</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YAMLParserInspector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Specialized inspection tools for YAML parser indentation stack and type inference.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, parser_instance):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser_instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inspect_indentation_stack</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Provide detailed view of current indentation stack state including</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        all frames, established levels, and nesting context.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract current indentation stack frames</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Show established indentation levels and their contexts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Display structure types and data at each level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Highlight current frame and processing context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Format as visual stack representation with level indicators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_indentation_transition</span><span style=\"color:#E1E4E8\">(self, target_level: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'TransitionAnalysis'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze what stack operations would be required to transition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        to target indentation level and validate transition correctness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate required stack operations for target level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate target level against established level history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify ambiguous transitions that could be interpreted multiple ways</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Show stack state before and after transition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate analysis explaining transition logic and potential issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> trace_type_inference</span><span style=\"color:#E1E4E8\">(self, value_string: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'TypeInferenceTrace'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Trace type inference process for scalar values showing</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        detection logic and conversion decisions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply each type detection rule to value_string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Show rule matching process and precedence handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Explain conversion logic and edge case handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compare against YAML specification requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate trace showing complete inference and conversion process</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 - Basic Diagnostic Infrastructure</strong>: After implementing core diagnostic classes, verify functionality:</p>\n<ul>\n<li>Run <code>python -m parser.debugging.diagnostic_tools --test-basic</code> to validate diagnostic capture</li>\n<li>Test state inspection with simple INI content: capture should show section processing and key-value parsing</li>\n<li>Verify error tracking works by parsing invalid content and checking error accumulation</li>\n</ul>\n<p><strong>Milestone 2 - Token Stream Analysis</strong>: After implementing tokenization analysis tools:</p>\n<ul>\n<li>Test boundary analysis with complex strings containing quotes and escape sequences</li>\n<li>Verify context comparison identifies inconsistent tokenization behavior</li>\n<li>Check that position tracking validation catches off-by-one errors in line/column calculation</li>\n</ul>\n<p><strong>Milestone 3 - Interactive Debugging</strong>: After implementing interactive debugging interface:</p>\n<ul>\n<li>Launch interactive session and step through tokenization of sample content</li>\n<li>Verify state inspection shows accurate tokenizer and parser state at each step</li>\n<li>Test command completion and help system provide useful guidance</li>\n</ul>\n<p><strong>Milestone 4 - Format-Specific Inspection</strong>: After implementing TOML and YAML inspectors:</p>\n<ul>\n<li>Verify symbol table inspection shows all definitions and conflicts for complex TOML content</li>\n<li>Test indentation stack analysis correctly tracks YAML nesting transitions</li>\n<li>Confirm type inference tracing explains scalar conversion decisions accurately</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Diagnostic tools crash on valid input</td>\n<td>Exception in diagnostic code path</td>\n<td>Enable exception logging in diagnostic framework</td>\n<td>Add error handling to diagnostic methods</td>\n</tr>\n<tr>\n<td>State inspection shows empty data</td>\n<td>Diagnostic hooks not properly integrated</td>\n<td>Verify diagnostic capture points in parser code</td>\n<td>Add diagnostic calls at key parsing phases</td>\n</tr>\n<tr>\n<td>Interactive debugger commands fail</td>\n<td>Command parsing or execution errors</td>\n<td>Test command parsing logic with simple inputs</td>\n<td>Implement robust command validation and error reporting</td>\n</tr>\n<tr>\n<td>Token boundary analysis reports false positives</td>\n<td>Position calculation differences</td>\n<td>Compare manual position calculation with automated results</td>\n<td>Align position calculation methods between tools and parser</td>\n</tr>\n<tr>\n<td>Performance degradation with diagnostics enabled</td>\n<td>Excessive diagnostic overhead</td>\n<td>Profile diagnostic operations vs parsing operations</td>\n<td>Optimize diagnostic data collection and disable expensive operations in production</td>\n</tr>\n</tbody></table>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - this section defines how the completed architecture can be extended with new formats, advanced features, and performance optimizations</p>\n</blockquote>\n<p>The configuration file parser architecture we&#39;ve designed provides a solid foundation that can grow with evolving requirements. Think of our current implementation as a well-designed foundation for a building — the structural elements are in place to support additional floors, rooms, and capabilities without requiring reconstruction of the core infrastructure. This extensibility comes from our careful separation of concerns, standardized interfaces, and unified data model that abstracts away format-specific details.</p>\n<p>The extension strategies we&#39;ll explore fall into three categories: adding support for new configuration formats that follow similar parsing paradigms, integrating advanced features that enhance the parser&#39;s capabilities beyond basic configuration reading, and implementing performance optimizations that enable the parser to handle larger files and higher throughput scenarios. Each category requires different architectural considerations and presents unique challenges in maintaining backward compatibility while adding new functionality.</p>\n<p>Our design&#39;s extensibility stems from several key architectural decisions. The <code>BaseTokenizer</code> abstraction provides a consistent interface that new format tokenizers can implement, while the <code>ParseContext</code> mechanism ensures that format-specific parsers can access shared functionality like error reporting and position tracking. The unified output format using nested dictionaries means that new formats only need to map their structures to this common representation, rather than requiring changes throughout the system.</p>\n<h3 id=\"adding-new-configuration-formats\">Adding New Configuration Formats</h3>\n<p>The architecture&#39;s modular design makes adding new configuration formats a straightforward process that follows established patterns. Consider the task of adding JSON5 support — JSON5 extends standard JSON with comments, trailing commas, and more flexible string literals. This format fits naturally into our tokenization-then-parsing pipeline because it shares structural similarities with existing formats while introducing format-specific syntactic features.</p>\n<p><strong>Mental Model for Format Extension</strong>: Think of adding a new format like adding a new language translator to an international conference. The conference already has the infrastructure (microphones, translation booths, audience seating), standardized protocols (when speakers talk, how questions are handled), and a common output format (simultaneous translation in multiple languages). Adding a new language requires creating a translator who understands that specific language but follows the same protocols and produces output in the same standardized format as existing translators.</p>\n<p>The format extension process follows a predictable pattern that leverages our existing infrastructure. New formats require implementing a tokenizer that extends <code>BaseTokenizer</code> and produces tokens using our standardized <code>TokenType</code> enumeration. The format-specific parser must implement the unified <code>parse(content) -&gt; Dict[str, Any]</code> interface and integrate with our <code>ParseContext</code> system for consistent error reporting and position tracking.</p>\n<blockquote>\n<p><strong>Decision: Standardized Format Registration System</strong></p>\n<ul>\n<li><strong>Context</strong>: New formats need to integrate with format detection and parser factory mechanisms</li>\n<li><strong>Options Considered</strong>: Static registration at compile time, dynamic registration via plugin system, configuration-driven format discovery</li>\n<li><strong>Decision</strong>: Static registration with factory method pattern</li>\n<li><strong>Rationale</strong>: Maintains type safety, enables compile-time verification, simplifies deployment while still allowing conditional compilation of format support</li>\n<li><strong>Consequences</strong>: Adding formats requires code changes but provides stronger guarantees about parser availability and reduces runtime complexity</li>\n</ul>\n</blockquote>\n<p>The format registration system uses a factory pattern that maps format identifiers to parser creation functions. Each new format registers itself with the central <code>ConfigurationFormatRegistry</code> that provides format detection hints and parser instantiation services. This approach ensures that format detection logic remains centralized while allowing format-specific parsers to provide their own syntactic signatures for automatic detection.</p>\n<table>\n<thead>\n<tr>\n<th>Format Extension Component</th>\n<th>Responsibilities</th>\n<th>Interface Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Format Tokenizer</td>\n<td>Break character stream into format-specific tokens</td>\n<td>Extend <code>BaseTokenizer</code>, implement <code>tokenize() -&gt; List[Token]</code></td>\n</tr>\n<tr>\n<td>Format Parser</td>\n<td>Convert tokens to unified data structure</td>\n<td>Implement <code>parse(content) -&gt; Dict[str, Any]</code></td>\n</tr>\n<tr>\n<td>Format Detector</td>\n<td>Provide syntactic signatures for format identification</td>\n<td>Implement <code>get_format_signatures() -&gt; List[str]</code></td>\n</tr>\n<tr>\n<td>Format Registry Entry</td>\n<td>Register format with detection and factory systems</td>\n<td>Provide <code>create_parser(**options)</code> factory function</td>\n</tr>\n</tbody></table>\n<p><strong>JSON5 Extension Example</strong>: JSON5 support requires extending our tokenization capabilities to handle JavaScript-style comments and relaxed string literal syntax. The JSON5 tokenizer would recognize <code>//</code> and <code>/* */</code> comment styles, producing <code>COMMENT</code> tokens that the parser ignores. Trailing commas in arrays and objects require lookahead logic to distinguish between legitimate commas and trailing syntax. The parser implementation leverages our existing recursive descent patterns but adds format-specific handling for these syntactic extensions.</p>\n<p>The JSON5 format detection relies on JavaScript-specific syntactic markers like unquoted object keys, single-quoted strings, and hexadecimal number literals. These signatures have high specificity — they&#39;re unlikely to appear in INI, TOML, or YAML files — making format detection reliable. The parser maps JSON5 structures directly to our nested dictionary representation since JSON5&#39;s object-array model aligns perfectly with our unified output format.</p>\n<p><strong>HCL (HashiCorp Configuration Language) Extension</strong>: HCL presents interesting challenges because it supports both JSON-compatible syntax and a more human-readable block-based syntax. The HCL tokenizer must handle this syntactic duality, potentially operating in different modes based on detected syntax style. HCL&#39;s block syntax requires parsing constructs like <code>resource &quot;aws_instance&quot; &quot;example&quot; { ... }</code> which don&#39;t map directly to simple key-value pairs.</p>\n<p>The HCL parser addresses this complexity by treating blocks as nested dictionary structures where block types become keys and block labels become nested keys. For example, the resource block above creates a nested structure: <code>{&quot;resource&quot;: {&quot;aws_instance&quot;: {&quot;example&quot;: { ... }}}}</code>. This mapping preserves HCL&#39;s semantic meaning while fitting into our unified output format.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Format extensions succeed when they embrace our unified data model rather than fighting it. Formats that map naturally to nested dictionaries integrate smoothly, while formats requiring fundamentally different output structures may indicate the need for architectural evolution rather than simple extension.</p>\n</blockquote>\n<p><strong>Advanced Format Considerations</strong>: Some configuration formats present challenges that push the boundaries of our current architecture. Formats with include mechanisms, template engines, or dynamic evaluation require extensions to our parsing pipeline. These advanced formats might require a two-phase parsing approach: first parsing the static structure using existing mechanisms, then post-processing for dynamic features.</p>\n<p>Formats with schema requirements or validation constraints may benefit from integration with our planned schema validation extensions. The format extension architecture should anticipate these needs by providing hooks for post-parse processing and validation phases that can be optionally enabled based on format requirements and user preferences.</p>\n<table>\n<thead>\n<tr>\n<th>Extension Challenge</th>\n<th>Current Architecture Impact</th>\n<th>Recommended Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Include file support</td>\n<td>Requires file system access during parsing</td>\n<td>Add optional <code>FileResolver</code> component to <code>ParseContext</code></td>\n</tr>\n<tr>\n<td>Template/variable expansion</td>\n<td>Single-pass parsing insufficient</td>\n<td>Implement two-phase parsing with post-processing stage</td>\n</tr>\n<tr>\n<td>Schema validation</td>\n<td>No built-in validation framework</td>\n<td>Design validation extension points in parser interface</td>\n</tr>\n<tr>\n<td>Custom data types</td>\n<td>Limited to basic Python types</td>\n<td>Extend value processing with pluggable type converters</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-feature-extensions\">Advanced Feature Extensions</h3>\n<p>The parser architecture provides several extension points for advanced features that enhance functionality beyond basic configuration file reading. These extensions transform the parser from a simple data extraction tool into a comprehensive configuration management system that can handle complex real-world requirements like schema validation, variable interpolation, and modular configuration organization.</p>\n<p><strong>Schema Validation Extension</strong>: Configuration files often require validation against predefined schemas to ensure they contain required fields, use acceptable value ranges, and maintain structural consistency. Think of schema validation like a quality control inspector at a manufacturing plant — it examines each component (configuration section) against predetermined specifications (schema rules) and flags any deviations before the product (configuration data) moves to the next stage (application startup).</p>\n<p>The schema validation extension integrates with our parsing pipeline as a post-processing stage that operates on the unified dictionary output. This design choice preserves the separation between syntactic parsing and semantic validation, allowing schema validation to work consistently across all supported formats. The validation system uses a plugin architecture where different schema languages (JSON Schema, custom validation rules) can be plugged in based on user requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Schema Validation Component</th>\n<th>Purpose</th>\n<th>Integration Point</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Schema Definition Parser</td>\n<td>Parse schema files into validation rules</td>\n<td>Standalone component, loads schema before configuration parsing</td>\n</tr>\n<tr>\n<td>Validation Rule Engine</td>\n<td>Apply validation rules to parsed configuration</td>\n<td>Post-processing stage after format-specific parsing</td>\n</tr>\n<tr>\n<td>Error Enrichment System</td>\n<td>Add schema context to validation errors</td>\n<td>Integrates with existing <code>ParseError</code> hierarchy</td>\n</tr>\n<tr>\n<td>Validation Result Reporter</td>\n<td>Generate comprehensive validation reports</td>\n<td>Extends existing error reporting mechanisms</td>\n</tr>\n</tbody></table>\n<p>The schema validation system maintains validation context that maps back to original source positions, enabling error messages that reference both the configuration file location and the schema requirement that was violated. For example, a validation error might report: &quot;Line 15: Missing required field &#39;database.port&#39; (required by schema section &#39;server-config&#39;)&quot; providing both syntactic and semantic context.</p>\n<p><strong>Variable Interpolation Extension</strong>: Modern configuration management often requires dynamic value substitution where configuration values can reference other configuration values, environment variables, or external data sources. Consider variable interpolation as similar to mail merge in document processing — templates contain placeholders that get filled in with actual values from various sources to produce the final document.</p>\n<p>The variable interpolation extension operates as a post-processing phase that scans the parsed configuration for interpolation expressions and resolves them using configured value sources. The interpolation engine supports multiple expression syntaxes (${var}, {{var}}, #{var}) to accommodate different format conventions and user preferences. Resolution occurs in dependency order, ensuring that variables are resolved before being used in other variable definitions.</p>\n<blockquote>\n<p><strong>Decision: Two-Phase Interpolation Processing</strong></p>\n<ul>\n<li><strong>Context</strong>: Variable interpolation requires dependency resolution and may involve external data sources</li>\n<li><strong>Options Considered</strong>: Single-pass interpolation during parsing, post-processing after parsing, lazy evaluation during configuration access</li>\n<li><strong>Decision</strong>: Post-processing with dependency graph resolution</li>\n<li><strong>Rationale</strong>: Separates concerns, enables cross-format interpolation, allows dependency cycle detection, supports external value sources</li>\n<li><strong>Consequences</strong>: Requires additional processing phase but provides more powerful and reliable interpolation capabilities</li>\n</ul>\n</blockquote>\n<p>The interpolation system builds a dependency graph of variable references and performs topological sorting to determine resolution order. Circular dependencies are detected and reported as configuration errors with suggestions for restructuring. The system supports multiple value sources including configuration values, environment variables, file contents, and external service calls through a pluggable provider interface.</p>\n<p><strong>Include File Extension</strong>: Large configuration systems benefit from modular organization where configuration can be split across multiple files and composed at parse time. Think of include file support like a compiler&#39;s include mechanism — individual source files can reference other files, and the compilation process assembles them into a complete program.</p>\n<p>The include file extension requires coordination between file system access and parsing operations. Include directives are recognized during parsing and trigger recursive parsing of referenced files. The included content is merged into the parent configuration using format-appropriate rules — INI sections merge, TOML tables merge with conflict detection, and YAML structures merge preserving hierarchy.</p>\n<table>\n<thead>\n<tr>\n<th>Include Processing Component</th>\n<th>Responsibilities</th>\n<th>Implementation Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Include Directive Detection</td>\n<td>Recognize format-specific include syntax</td>\n<td>Integrates with format-specific parsers</td>\n</tr>\n<tr>\n<td>File Resolution System</td>\n<td>Resolve relative paths and search paths</td>\n<td>Handles file system access and path normalization</td>\n</tr>\n<tr>\n<td>Recursive Parse Coordination</td>\n<td>Manage parsing of included files</td>\n<td>Prevents infinite recursion, maintains parse context</td>\n</tr>\n<tr>\n<td>Configuration Merging Logic</td>\n<td>Combine included content with parent configuration</td>\n<td>Format-specific merging rules with conflict detection</td>\n</tr>\n</tbody></table>\n<p>Include processing maintains a stack of currently processing files to detect and prevent circular includes. Error reporting preserves the include chain so users can trace errors back through the file inclusion hierarchy. For example: &quot;Error in config.toml (included from main.toml:15): Invalid table definition at line 8&quot;.</p>\n<p><strong>Configuration Template Extensions</strong>: Some deployment scenarios benefit from template-based configuration generation where configuration files are generated from templates with environment-specific values. This extension transforms the parser into a configuration generation system that can produce format-specific output from template definitions.</p>\n<p>The template extension operates in reverse from normal parsing — instead of reading configuration files and producing data structures, it takes data structures and template definitions to produce configuration files in specific formats. This bidirectional capability enables configuration round-tripping and format conversion workflows.</p>\n<p><strong>Advanced Type System Extensions</strong>: The current parser supports basic data types appropriate for configuration files, but some applications require more sophisticated type handling including custom objects, validated enumerations, and computed values. The advanced type system extension provides pluggable type converters that can transform string literals into domain-specific objects during parsing.</p>\n<p>Custom type converters register with the value processing system and provide type detection patterns and conversion logic. For example, a duration converter might recognize strings like &quot;30m&quot;, &quot;2h45m&quot;, &quot;1d&quot; and convert them to appropriate duration objects. The type system maintains type metadata that can be used by schema validation and error reporting systems.</p>\n<h3 id=\"performance-and-scalability-improvements\">Performance and Scalability Improvements</h3>\n<p>As configuration files grow larger and parsing becomes more frequent, performance optimizations become essential for maintaining responsive applications. The current parser architecture provides several optimization opportunities that can dramatically improve performance for large-scale usage scenarios without compromising correctness or maintainability.</p>\n<p><strong>Streaming Parser Implementation</strong>: Large configuration files can cause memory pressure when the entire file is loaded into memory for parsing. Think of streaming parsing like reading a book one page at a time instead of memorizing the entire book before understanding its content — you can begin processing and understanding information as soon as you receive it, rather than waiting for complete input.</p>\n<p>The streaming parser extension modifies our tokenization and parsing pipeline to operate on character streams rather than complete string content. This approach enables processing configuration files that exceed available memory and reduces startup latency for applications that only need specific configuration sections.</p>\n<table>\n<thead>\n<tr>\n<th>Streaming Component</th>\n<th>Current Implementation</th>\n<th>Streaming Enhancement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Processing</td>\n<td>Load complete file into memory string</td>\n<td>Process character stream with buffered reading</td>\n</tr>\n<tr>\n<td>Tokenization</td>\n<td>Generate complete token list</td>\n<td>Yield tokens on-demand with lookahead buffer</td>\n</tr>\n<tr>\n<td>Parsing</td>\n<td>Random access to token stream</td>\n<td>Forward-only parsing with limited backtracking</td>\n</tr>\n<tr>\n<td>Output Generation</td>\n<td>Build complete result dictionary</td>\n<td>Incremental result building with optional section filtering</td>\n</tr>\n</tbody></table>\n<p>Streaming parsing requires careful consideration of lookahead requirements. TOML&#39;s complex syntax occasionally requires significant lookahead to disambiguate constructs, while YAML&#39;s indentation sensitivity requires maintaining context about previous lines. The streaming implementation uses bounded buffers that balance memory usage with parsing capability.</p>\n<p><strong>Incremental Parsing for Configuration Updates</strong>: Applications that monitor configuration files for changes benefit from incremental parsing that can update the parsed representation without re-parsing unchanged sections. Consider incremental parsing similar to smart document editing software that only reformats the paragraph you&#39;re currently editing rather than reformatting the entire document with every keystroke.</p>\n<p>The incremental parsing extension requires maintaining parse tree metadata that maps sections of the output structure back to source file regions. When file changes are detected, the system identifies affected regions and re-parses only those sections, merging updated content with the cached parse results. This approach dramatically reduces parsing overhead for configuration hot-reloading scenarios.</p>\n<blockquote>\n<p><strong>Decision: Section-Level Incremental Granularity</strong></p>\n<ul>\n<li><strong>Context</strong>: Incremental parsing requires balancing granularity with complexity</li>\n<li><strong>Options Considered</strong>: Line-level incremental updates, section-level updates, full file re-parsing with caching</li>\n<li><strong>Decision</strong>: Section-level incremental updates with dependency tracking</li>\n<li><strong>Rationale</strong>: Provides significant performance benefits while maintaining manageable complexity, aligns with natural configuration organization boundaries</li>\n<li><strong>Consequences</strong>: Requires section dependency tracking but offers substantial performance improvements for large configuration files</li>\n</ul>\n</blockquote>\n<p>Incremental parsing maintains dependency information between configuration sections so that changes in one section can trigger re-parsing of dependent sections. For example, changing a TOML table definition might require re-parsing sections that reference that table through dotted key notation.</p>\n<p><strong>Parse Result Caching System</strong>: Applications that parse the same configuration files repeatedly (such as microservices that restart frequently) benefit from caching parsed results indexed by file content hash. Think of parse result caching like a translator who keeps a glossary of previously translated phrases — common translations can be recalled instantly rather than re-performing the translation work.</p>\n<p>The caching system computes content hashes for configuration files and stores serialized parse results in a cache (memory-based, disk-based, or distributed cache systems). Cache entries include metadata about parsing options and format detection results to ensure cache hits only occur for equivalent parsing scenarios.</p>\n<p><strong>Parallel Parsing for Multi-File Configurations</strong>: Configuration systems with many include files or modular organization can benefit from parallel parsing where independent configuration files are parsed simultaneously. The parallel parsing extension requires careful coordination to maintain dependency order while maximizing concurrency for independent parsing operations.</p>\n<p>The parallel parsing coordinator analyzes include dependencies to create a parsing task graph where independent branches can be processed concurrently. Results are combined using the same merging logic as sequential include processing, but with coordination points that ensure dependency order is preserved.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Extension</th>\n<th>Memory Impact</th>\n<th>CPU Impact</th>\n<th>Complexity Impact</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Streaming parsing</td>\n<td>Significant reduction</td>\n<td>Slight increase</td>\n<td>Moderate increase</td>\n<td>Very large files</td>\n</tr>\n<tr>\n<td>Incremental parsing</td>\n<td>Moderate increase</td>\n<td>Significant reduction for updates</td>\n<td>High increase</td>\n<td>Frequently changing files</td>\n</tr>\n<tr>\n<td>Result caching</td>\n<td>Moderate increase</td>\n<td>Significant reduction for repeated parsing</td>\n<td>Low increase</td>\n<td>Repeated parsing scenarios</td>\n</tr>\n<tr>\n<td>Parallel parsing</td>\n<td>No significant change</td>\n<td>Significant reduction</td>\n<td>High increase</td>\n<td>Multi-file configurations</td>\n</tr>\n</tbody></table>\n<p><strong>Memory Optimization Strategies</strong>: The current parser creates rich object hierarchies for tokens, parse nodes, and error tracking that provide excellent debugging capabilities but consume significant memory for large configuration files. Memory optimization extensions provide configurable trade-offs between memory usage and debugging capability.</p>\n<p>Lightweight parsing modes eliminate intermediate parse trees and error context information, producing only the final dictionary output. Token pooling reuses token objects to reduce garbage collection pressure. String interning reduces memory usage for repeated configuration keys and common values. These optimizations can reduce memory usage by 50-80% for typical configuration files while maintaining parsing correctness.</p>\n<p><strong>Async/Await Integration</strong>: Modern applications increasingly use asynchronous programming models where I/O operations should not block execution threads. The async parsing extension provides asynchronous versions of parsing operations that integrate cleanly with async/await frameworks while maintaining the same functionality and error handling characteristics.</p>\n<p>Async parsing is particularly valuable when combined with streaming parsing and include file processing, where file I/O operations can be overlapped with parsing work. The async implementation maintains the same error handling and progress reporting capabilities while enabling better resource utilization in async applications.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations for Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>New Format Support</td>\n<td>Direct parser implementation</td>\n<td>Plugin architecture with dynamic loading</td>\n</tr>\n<tr>\n<td>Schema Validation</td>\n<td>JSON Schema library integration</td>\n<td>Custom validation engine with DSL</td>\n</tr>\n<tr>\n<td>Variable Interpolation</td>\n<td>String template substitution</td>\n<td>Expression evaluator with function support</td>\n</tr>\n<tr>\n<td>Include Files</td>\n<td>Recursive parsing with simple merging</td>\n<td>Dependency graph with conflict resolution</td>\n</tr>\n<tr>\n<td>Streaming Parsing</td>\n<td>Generator-based token production</td>\n<td>Coroutine-based parser with backtracking</td>\n</tr>\n<tr>\n<td>Caching</td>\n<td>In-memory dictionary cache</td>\n<td>Redis/disk-based cache with TTL</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Extension Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  config_parser/\n    core/                      ← existing core components\n      tokenizer.py\n      parsers/\n        ini_parser.py\n        toml_parser.py\n        yaml_parser.py\n    extensions/                ← extension system\n      __init__.py             ← extension registry\n      formats/                ← new format support\n        json5_parser.py\n        hcl_parser.py\n        format_registry.py\n      features/               ← advanced features\n        schema_validator.py\n        variable_interpolator.py\n        include_processor.py\n      performance/            ← performance extensions\n        streaming_parser.py\n        incremental_parser.py\n        cache_manager.py\n    examples/                 ← extension examples\n      custom_format_example.py\n      validation_example.py</code></pre></div>\n\n<p><strong>Extension Registry Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FORMAT_PARSER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"format_parser\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FEATURE_PROCESSOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"feature_processor\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PERFORMANCE_OPTIMIZER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"performance_optimizer\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extension_type: ExtensionType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    factory: Callable[</span><span style=\"color:#79B8FF\">...</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config_schema: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConfigurationExtensionRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._extensions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ExtensionInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._format_detectors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable[[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_extension</span><span style=\"color:#E1E4E8\">(self, extension_info: ExtensionInfo) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a new extension with the configuration system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._validate_dependencies(extension_info.dependencies)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._extensions[extension_info.name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> extension_info</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> extension_info.extension_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> ExtensionType.</span><span style=\"color:#79B8FF\">FORMAT_PARSER</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Format parsers should provide detection capabilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            detector </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(extension_info.factory(), </span><span style=\"color:#9ECBFF\">'detect_format_confidence'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> detector:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._format_detectors[extension_info.name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> detector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_extension</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[ExtensionInfo]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._extensions.get(name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_parser</span><span style=\"color:#E1E4E8\">(self, format_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">options) -> Optional[Any]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        extension </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_extension(format_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> extension </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> extension.extension_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> ExtensionType.</span><span style=\"color:#79B8FF\">FORMAT_PARSER</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> extension.factory(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">options)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_format</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        best_format </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        best_confidence </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> format_name, detector </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._format_detectors.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            confidence </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> detector(content)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> confidence </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> best_confidence:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                best_confidence </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> confidence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                best_format </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> format_name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> best_format </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> best_confidence </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.7</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global registry instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">extension_registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ConfigurationExtensionRegistry()</span></span></code></pre></div>\n\n<p><strong>Format Extension Skeleton (JSON5 Example):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> config_parser.core.tokenizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseTokenizer, Token, TokenType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> config_parser.core.base_parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseParser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JSON5Tokenizer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseTokenizer</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(source)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize JSON5-specific tokenization state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up comment recognition patterns (// and /* */)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Configure string literal handling for single quotes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> tokenize</span><span style=\"color:#E1E4E8\">(self) -> List[Token]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Implement main tokenization loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle JavaScript-style comments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Process unquoted object keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle trailing commas in arrays/objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Support hexadecimal number literals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JSON5Parser</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseParser</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">options):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tokenizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize parser state for JSON5 syntax</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up recursive descent parsing methods</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self, content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create JSON5 tokenizer instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Tokenize input content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse JSON5 structure using recursive descent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Convert to unified dictionary format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle parsing errors with appropriate context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_format_confidence</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Look for JavaScript-style comments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for unquoted object keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect trailing commas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return confidence score (0.0 - 1.0)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Registration with extension system</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">json5_extension </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExtensionInfo(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"json5\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    extension_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">ExtensionType.</span><span style=\"color:#79B8FF\">FORMAT_PARSER</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    version</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"1.0.0\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    dependencies</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">JSON5Parser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">extension_registry.register_extension(json5_extension)</span></span></code></pre></div>\n\n<p><strong>Schema Validation Extension Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> config_parser.core.errors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParseError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValidationRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(self, value: Any, path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return list of validation error messages, empty if valid.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SchemaValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, schema_definition: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.schema </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schema_definition</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rules: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ValidationRule] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse schema definition into validation rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Build rule hierarchy for nested validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up error message templates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_configuration</span><span style=\"color:#E1E4E8\">(self, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> List[ParseError]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Traverse configuration structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply appropriate validation rules at each level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect validation errors with path context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate helpful error messages with suggestions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of validation errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_custom_rule</span><span style=\"color:#E1E4E8\">(self, path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, rule: ValidationRule):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Register custom validation rule for specific configuration path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Integrate with existing rule hierarchy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Integration point for adding validation to parsing pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> parse_with_validation</span><span style=\"color:#E1E4E8\">(content: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         format_hint: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse configuration using standard pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load schema definition if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create validator instance with schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Run validation on parsed configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Report validation errors alongside parsing errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Performance Extension Checkpoints:</strong></p>\n<p>After implementing streaming parsing:</p>\n<ul>\n<li>Test with configuration files larger than available memory (generate test files &gt; 1GB)</li>\n<li>Verify memory usage remains constant regardless of file size: <code>python -m memory_profiler your_streaming_test.py</code></li>\n<li>Expected behavior: Memory usage should plateau at buffer size rather than growing with file size</li>\n<li>Performance benchmark: Parse 100MB configuration file in under 30 seconds with less than 50MB memory usage</li>\n</ul>\n<p>After implementing incremental parsing:</p>\n<ul>\n<li>Create test scenario with 1000-section configuration file</li>\n<li>Modify single section and measure re-parse time</li>\n<li>Expected behavior: Re-parse time should be &lt; 5% of full parse time for single section changes</li>\n<li>Verify correctness: Output should be identical to full re-parse for all test cases</li>\n</ul>\n<p>After implementing caching system:</p>\n<ul>\n<li>Test cache hit rates with repeated parsing of same content</li>\n<li>Expected behavior: Cache hit should be &gt; 100x faster than full parsing</li>\n<li>Verify cache invalidation: Content changes should produce cache misses</li>\n<li>Test command: <code>python -m pytest tests/performance/test_caching.py -v --benchmark-only</code></li>\n</ul>\n<p><strong>Common Extension Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Breaking Unified Output Format</strong>\nNew format parsers sometimes introduce format-specific data types in the output dictionary, breaking compatibility with existing code that expects standard Python types. Always convert format-specific types to basic Python types (dict, list, str, int, float, bool) in the final output.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Error Reporting</strong>\nExtension components may implement their own error handling that doesn&#39;t integrate with the existing <code>ParseError</code> hierarchy, leading to inconsistent error messages and loss of source position information. Always use the established error classes and ensure position information propagates correctly.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in Streaming Parsers</strong>\nStreaming parsers can accumulate state that isn&#39;t properly cleaned up, especially when parsing fails partway through large files. Implement proper cleanup in finally blocks and consider using context managers for resource management.</p>\n<p>⚠️ <strong>Pitfall: Schema Validation Performance</strong>\nComplex schema validation can become a performance bottleneck, especially with deeply nested configurations. Profile validation performance and consider implementing validation rule caching or lazy validation strategies for large configurations.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive terminology reference supporting all implementation phases</p>\n</blockquote>\n<p>A comprehensive configuration file parser requires precise vocabulary to communicate complex parsing concepts effectively. This glossary serves as the authoritative reference for all technical terms, parsing concepts, and domain-specific vocabulary used throughout the design document. Understanding these terms is essential for implementing the parser architecture and communicating about parsing challenges.</p>\n<p>The terminology is organized into logical categories to support different aspects of the learning journey. Each definition includes context about where the term appears in the implementation pipeline and why it matters for configuration parsing specifically.</p>\n<h3 id=\"parsing-fundamentals\">Parsing Fundamentals</h3>\n<p><strong>Tokenization</strong> refers to the process of breaking a character stream into meaningful lexical units called tokens. Think of tokenization as converting a continuous stream of characters into a sequence of building blocks that the parser can understand and manipulate. Each token represents a syntactic element like a string, number, operator, or delimiter that carries semantic meaning within the configuration format.</p>\n<p><strong>Recursive descent</strong> describes a parsing methodology where the parser uses function calls to handle nested grammatical structures. Each function in the parser corresponds to a grammatical rule, and when the parser encounters nested content, it calls the appropriate function recursively. This approach naturally handles the hierarchical structure found in configuration files, where tables can contain subtables, arrays can contain other arrays, and values can be complex nested structures.</p>\n<p><strong>Lookahead parsing</strong> involves examining upcoming tokens in the input stream before making parsing decisions. The parser maintains the ability to inspect future tokens without consuming them, allowing it to choose the correct parsing path based on upcoming context. This technique is crucial for resolving syntactic ambiguities and implementing robust error recovery.</p>\n<p><strong>Parse tree</strong> represents the intermediate structural representation of parsed syntax before conversion to the final data structure. The parse tree captures the grammatical structure of the input, maintaining information about how the parser interpreted each syntactic element. This intermediate representation allows for validation, transformation, and debugging before generating the unified output format.</p>\n<p><strong>Unified output format</strong> describes the consistent nested dictionary structure that all parsers produce regardless of input format. This standardization allows applications to work with configuration data uniformly, regardless of whether the source was INI, TOML, or YAML. The unified format uses Python dictionaries and lists to represent all hierarchical structures and data types.</p>\n<h3 id=\"tokenization-concepts\">Tokenization Concepts</h3>\n<p><strong>Lexical ambiguity</strong> occurs when the same character sequence can mean different things depending on the parsing context. For example, the sequence <code>&quot;key&quot;</code> might be a quoted string literal in one context but part of a larger quoted phrase in another context. Tokenizers must maintain sufficient context to resolve these ambiguities correctly.</p>\n<p><strong>Context sensitivity</strong> describes how the meaning of input changes based on surrounding parsing context. The same character sequence might tokenize as different token types depending on what the parser has already encountered. For instance, a colon character has different meanings in INI key-value pairs versus YAML mapping syntax.</p>\n<p><strong>Whitespace semantics</strong> refers to how different configuration formats treat whitespace as either meaningful or ignorable. INI files generally ignore whitespace around delimiters, while YAML uses indentation as syntactically significant for defining structure. Understanding these semantics is crucial for correct tokenization.</p>\n<p><strong>String literal handling</strong> encompasses the complex logic required to process quoted strings, escape sequences, and multiline variants across different formats. Each format has different rules for string delimiters, escape sequences, and multiline continuation, requiring format-specific tokenization logic.</p>\n<p><strong>Scanning window</strong> describes the tokenizer&#39;s moving view through the character stream as it identifies token boundaries. The tokenizer maintains position information and looks ahead through this window to determine where each token begins and ends.</p>\n<h3 id=\"data-structure-concepts\">Data Structure Concepts</h3>\n<table>\n<thead>\n<tr>\n<th>Concept</th>\n<th>Description</th>\n<th>Usage Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Token Type Definitions</strong></td>\n<td>Enumerated values representing all possible token categories across formats</td>\n<td>Tokenization phase for classifying lexical units</td>\n</tr>\n<tr>\n<td><strong>Parse Node Structure</strong></td>\n<td>Hierarchical representation capturing grammatical relationships</td>\n<td>Intermediate parsing phase before final output</td>\n</tr>\n<tr>\n<td><strong>Position Tracking</strong></td>\n<td>Line and column information for error reporting and debugging</td>\n<td>Throughout tokenization and parsing for diagnostics</td>\n</tr>\n<tr>\n<td><strong>Symbol Table Management</strong></td>\n<td>Registry of defined keys and tables for conflict detection</td>\n<td>TOML parsing to enforce redefinition rules</td>\n</tr>\n<tr>\n<td><strong>Indentation Frame</strong></td>\n<td>Stack element tracking YAML nesting context</td>\n<td>YAML parsing for managing hierarchical structure</td>\n</tr>\n</tbody></table>\n<h3 id=\"ini-format-concepts\">INI Format Concepts</h3>\n<p><strong>Section-based organization</strong> describes INI&#39;s hierarchical structure where named sections contain key-value pairs. Each section creates a namespace that groups related configuration values, and the parser must track the current section context when processing key-value assignments.</p>\n<p><strong>Line-based parsing</strong> refers to INI&#39;s approach of processing input one logical line at a time. The parser examines each line to determine whether it&#39;s a section header, key-value pair, comment, or continuation, then processes it according to its type.</p>\n<p><strong>Global keys</strong> are key-value pairs that appear before any section headers in INI files. These keys belong to a default or global namespace and require special handling in the parser&#39;s section tracking logic.</p>\n<p><strong>Dotted notation</strong> allows section names to use dot separators to create nested dictionary structures in the output. The parser must expand these dotted sections into hierarchical data structures during processing.</p>\n<h3 id=\"toml-format-concepts\">TOML Format Concepts</h3>\n<p><strong>Table redefinition rules</strong> are TOML&#39;s constraints preventing conflicting table declarations within the same document. Once a table path is defined explicitly, it cannot be redefined using a different declaration type, and the parser must track all definitions to enforce these rules.</p>\n<p><strong>Array-of-tables</strong> represents TOML&#39;s syntax for creating arrays of table instances using double bracket notation <code>[[table.name]]</code>. Each array-of-tables declaration creates a new table instance and appends it to an array at the specified path.</p>\n<p><strong>Dotted key expansion</strong> is the automatic creation of nested table structure from dotted key notation like <code>physical.color = &quot;orange&quot;</code>. The parser must create intermediate tables as needed while respecting existing table definitions.</p>\n<p><strong>Inline tables</strong> use TOML&#39;s single-line table syntax <code>{key = value, key2 = value2}</code> to define complete table structures within expressions. These tables cannot be modified after creation and require special parsing logic.</p>\n<p><strong>Implicit table creation</strong> occurs when dotted keys reference table paths that don&#39;t exist yet. The parser automatically creates the necessary intermediate table structure while tracking these implicit creations for conflict detection.</p>\n<p><strong>Symbol table management</strong> involves tracking all defined keys and tables throughout TOML parsing to detect conflicts and validate redefinition rules. The symbol table maintains metadata about how each key path was defined and where conflicts might occur.</p>\n<h3 id=\"yaml-format-concepts\">YAML Format Concepts</h3>\n<p><strong>Indentation-driven hierarchical structure</strong> describes YAML&#39;s fundamental approach of using whitespace to define nesting relationships. The amount of indentation determines the hierarchical level of each element, and the parser must maintain strict tracking of indentation levels.</p>\n<p><strong>Stack-based approach</strong> refers to the parsing methodology that uses a stack data structure to track current nesting contexts. As indentation increases, the parser pushes new contexts onto the stack, and as indentation decreases, it pops contexts back to the appropriate level.</p>\n<p><strong>Indentation stack</strong> is the specific stack data structure that tracks current nesting contexts and established indentation levels. The stack contains frames representing each nesting level with information about the expected structure type and data being built.</p>\n<p><strong>Structure transitions</strong> are changes in nesting level that require stack operations to maintain correct parsing context. When the parser encounters different indentation levels, it must push or pop stack frames to match the new structure.</p>\n<p><strong>Scalar type inference</strong> involves automatically converting string literals to appropriate Python data types based on their format and content. YAML&#39;s implicit typing system requires the parser to recognize patterns like numbers, booleans, and null values.</p>\n<p><strong>Flow syntax</strong> refers to YAML&#39;s JSON-like inline syntax using brackets and braces for lists and mappings. This syntax provides an alternative to block syntax and requires different parsing logic.</p>\n<p><strong>Block syntax</strong> is YAML&#39;s primary indentation-based syntax for defining hierarchical structures using whitespace and line breaks rather than explicit delimiters.</p>\n<h3 id=\"error-handling-concepts\">Error Handling Concepts</h3>\n<p><strong>Error recovery</strong> describes the parser&#39;s ability to continue processing after encountering errors to find additional issues in the same parsing pass. Rather than stopping at the first error, the parser attempts to understand enough context to continue and report multiple problems.</p>\n<p><strong>Panic mode recovery</strong> involves discarding input tokens until reaching a known synchronization point where parsing can safely resume. This strategy helps the parser skip over malformed content and continue processing subsequent sections.</p>\n<p><strong>Error production recovery</strong> inserts assumed content when the parser encounters unexpected input but can infer what was likely intended. This approach allows parsing to continue while recording the assumption made.</p>\n<p><strong>Phrase-level recovery</strong> skips minimal malformed syntactic units while preserving as much surrounding structure as possible. This fine-grained approach minimizes the impact of local errors on overall parsing success.</p>\n<p><strong>Graceful degradation</strong> ensures the system continues operating despite errors, providing reduced functionality rather than complete failure. The parser returns partial results when possible, allowing applications to work with the correctly parsed portions.</p>\n<p><strong>Structured error accumulation</strong> involves collecting multiple related errors for unified reporting rather than stopping at each individual problem. This approach provides comprehensive feedback about all issues in a single parsing pass.</p>\n<h3 id=\"architecture-concepts\">Architecture Concepts</h3>\n<p><strong>Component responsibilities</strong> define what each parser component owns in terms of data, processing, and state management. Clear responsibility boundaries prevent overlap and ensure each component has a well-defined purpose within the overall architecture.</p>\n<p><strong>Nested structure mapping</strong> describes the process of creating hierarchical data structures from various flat syntactic representations. Different formats express hierarchy differently, but all must map to the same nested dictionary output format.</p>\n<p><strong>Format detection</strong> involves automatically identifying configuration format from content analysis without explicit format specification. The system analyzes syntactic patterns and structural characteristics to determine the appropriate parser.</p>\n<p><strong>Impedance mismatch</strong> refers to fundamental incompatibilities between different format paradigms that complicate unified processing. For example, YAML&#39;s indentation sensitivity conflicts with INI&#39;s line-based approach, requiring different parsing strategies.</p>\n<h3 id=\"testing-and-debugging-concepts\">Testing and Debugging Concepts</h3>\n<p><strong>Milestone verification points</strong> are concrete checkpoints that confirm successful completion of each implementation milestone. These checkpoints provide measurable criteria for evaluating progress and identifying implementation gaps.</p>\n<p><strong>Golden path test data</strong> represents realistic configuration scenarios that reflect common usage patterns. This test data validates that the parser handles typical use cases correctly and produces expected results.</p>\n<p><strong>Edge case test data</strong> explores boundary conditions and corner cases that might reveal parsing bugs or unexpected behavior. This comprehensive testing approach ensures robust handling of unusual but valid input.</p>\n<p><strong>Cross-format equivalence testing</strong> validates that semantically equivalent configurations produce consistent results regardless of their source format. This testing ensures the unified output format truly provides format independence.</p>\n<p><strong>Layered diagnosis methodology</strong> provides a systematic debugging approach that examines parsing problems at multiple levels: character processing, tokenization, parse tree construction, and final output generation.</p>\n<p><strong>Token stream replay</strong> involves recording and re-processing token sequences for diagnostic purposes. This technique allows detailed analysis of tokenization behavior and helps identify the root causes of parsing failures.</p>\n<p><strong>Interactive parser inspection framework</strong> provides tools for step-by-step visibility into parsing operations. These debugging tools allow developers to examine parser state, token streams, and decision points interactively.</p>\n<h3 id=\"performance-and-scalability-concepts\">Performance and Scalability Concepts</h3>\n<p><strong>Streaming parsing</strong> refers to processing large configuration files without loading the entire content into memory. This approach enables handling of arbitrarily large files by processing content in chunks as it becomes available.</p>\n<p><strong>Incremental parsing</strong> updates parsed results when only part of a file changes, avoiding complete re-parsing of unchanged content. This optimization is valuable for applications that monitor configuration files for changes.</p>\n<p><strong>Parse result caching</strong> stores previously parsed results to avoid re-parsing unchanged files. The caching system must handle cache invalidation correctly when source files are modified.</p>\n<p><strong>Extension architecture</strong> describes the system design that supports pluggable extensions for new formats, features, and optimizations. The architecture provides well-defined extension points without requiring modifications to core parsing logic.</p>\n<h3 id=\"advanced-feature-concepts\">Advanced Feature Concepts</h3>\n<p><strong>Schema validation</strong> involves validating parsed configuration against predefined rules and constraints. This feature ensures configuration correctness beyond just syntactic validity, checking semantic rules and value constraints.</p>\n<p><strong>Variable interpolation</strong> provides dynamic value substitution in configuration files, allowing references to other configuration values or environment variables. This feature requires additional parsing passes to resolve dependencies correctly.</p>\n<p><strong>Include file processing</strong> supports modular configuration through file inclusion mechanisms. The parser must handle file resolution, circular dependency detection, and scope management for included content.</p>\n<p><strong>Dependency graph resolution</strong> ensures variables and includes are processed in the correct order when complex interdependencies exist. The parser must build and traverse dependency graphs to handle these relationships correctly.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation of a comprehensive configuration parser requires careful attention to terminology consistency and conceptual clarity. Each term in this glossary represents a specific aspect of parsing theory or implementation practice that has precise meaning within the context of configuration file processing.</p>\n<p>When implementing parser components, developers should use this terminology consistently to ensure clear communication about design decisions and implementation challenges. The terms provide a shared vocabulary for discussing parsing problems, architectural choices, and debugging strategies.</p>\n<p>The glossary also serves as a reference for understanding the parsing literature and related tools. Many of these terms have broader applications in compiler design and language processing, making them valuable for developers who want to explore more advanced parsing topics.</p>\n<h4 id=\"key-term-categories\">Key Term Categories</h4>\n<p>The terminology is organized into several key categories that correspond to different aspects of the parsing implementation:</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Focus Area</th>\n<th>Key Terms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Parsing Fundamentals</strong></td>\n<td>Core concepts applicable to all parsers</td>\n<td>Tokenization, recursive descent, lookahead parsing</td>\n</tr>\n<tr>\n<td><strong>Format-Specific Concepts</strong></td>\n<td>Terms specific to INI, TOML, or YAML</td>\n<td>Section-based organization, table redefinition, indentation-driven structure</td>\n</tr>\n<tr>\n<td><strong>Error Handling</strong></td>\n<td>Error detection, recovery, and reporting</td>\n<td>Panic mode recovery, graceful degradation, structured error accumulation</td>\n</tr>\n<tr>\n<td><strong>Architecture</strong></td>\n<td>System design and component interaction</td>\n<td>Component responsibilities, format detection, impedance mismatch</td>\n</tr>\n<tr>\n<td><strong>Testing and Debugging</strong></td>\n<td>Validation and troubleshooting</td>\n<td>Milestone verification, cross-format equivalence, layered diagnosis</td>\n</tr>\n<tr>\n<td><strong>Advanced Features</strong></td>\n<td>Extensions and optimizations</td>\n<td>Schema validation, variable interpolation, streaming parsing</td>\n</tr>\n</tbody></table>\n<h4 id=\"terminology-evolution\">Terminology Evolution</h4>\n<p>As the parser implementation progresses through different milestones, developers will encounter these terms in specific contexts that reinforce their meanings through practical application. The INI parser milestone introduces fundamental concepts like tokenization and line-based parsing, while the TOML parser milestone adds complexity with recursive descent and symbol table management.</p>\n<p>The YAML parser milestone demonstrates how terminology evolves to handle different parsing paradigms, introducing concepts like indentation-driven structure and stack-based approaches. Throughout all milestones, error handling terminology becomes increasingly important as the parser implementations become more sophisticated and robust.</p>\n","toc":[{"level":1,"text":"Config File Parser: Design Document","id":"config-file-parser-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Why Configuration Parsing is Hard","id":"why-configuration-parsing-is-hard"},{"level":3,"text":"Format Comparison Analysis","id":"format-comparison-analysis"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":4,"text":"Core Format Support Requirements","id":"core-format-support-requirements"},{"level":4,"text":"Unified Interface Requirements","id":"unified-interface-requirements"},{"level":4,"text":"Development and Learning Goals","id":"development-and-learning-goals"},{"level":3,"text":"Non-Goals","id":"non-goals"},{"level":4,"text":"Advanced YAML Features","id":"advanced-yaml-features"},{"level":4,"text":"Performance Optimization Features","id":"performance-optimization-features"},{"level":4,"text":"Production-Ready Features","id":"production-ready-features"},{"level":4,"text":"Advanced Error Recovery","id":"advanced-error-recovery"},{"level":4,"text":"Format Subset Justification","id":"format-subset-justification"},{"level":4,"text":"Unified Interface Goals","id":"unified-interface-goals"},{"level":4,"text":"Error Handling and Diagnostics Goals","id":"error-handling-and-diagnostics-goals"},{"level":4,"text":"Data Structure Mapping Goals","id":"data-structure-mapping-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Core Infrastructure Starter Code","id":"core-infrastructure-starter-code"},{"level":4,"text":"Parser Implementation Skeleton","id":"parser-implementation-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Common Implementation Pitfalls","id":"common-implementation-pitfalls"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":3,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Token Type Definitions","id":"token-type-definitions"},{"level":3,"text":"Parse Tree Structure","id":"parse-tree-structure"},{"level":3,"text":"Unified Output Format","id":"unified-output-format"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Tokenizer Component Design","id":"tokenizer-component-design"},{"level":3,"text":"Tokenization Mental Model","id":"tokenization-mental-model"},{"level":3,"text":"Tokenizer Interface Design","id":"tokenizer-interface-design"},{"level":3,"text":"String Literal Handling","id":"string-literal-handling"},{"level":3,"text":"Tokenizer Architecture Decisions","id":"tokenizer-architecture-decisions"},{"level":3,"text":"Common Tokenizer Pitfalls","id":"common-tokenizer-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"INI Parser Component Design","id":"ini-parser-component-design"},{"level":3,"text":"INI Parsing Mental Model: Understanding INI as Section-Based Key-Value Organization","id":"ini-parsing-mental-model-understanding-ini-as-section-based-key-value-organization"},{"level":3,"text":"INI Parsing Algorithm: Step-by-Step Process for Handling Sections, Keys, Values, and Comments","id":"ini-parsing-algorithm-step-by-step-process-for-handling-sections-keys-values-and-comments"},{"level":3,"text":"INI Architecture Decisions: Decisions Around Global Keys, Comment Handling, and Value Type Inference","id":"ini-architecture-decisions-decisions-around-global-keys-comment-handling-and-value-type-inference"},{"level":3,"text":"Common INI Parsing Pitfalls: Issues with Inline Comments, Quoted Values, and Section Nesting","id":"common-ini-parsing-pitfalls-issues-with-inline-comments-quoted-values-and-section-nesting"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"TOML Parser Component Design","id":"toml-parser-component-design"},{"level":3,"text":"TOML Parsing Mental Model","id":"toml-parsing-mental-model"},{"level":3,"text":"Recursive Descent Algorithm","id":"recursive-descent-algorithm"},{"level":3,"text":"Table and Array-of-Tables Logic","id":"table-and-array-of-tables-logic"},{"level":3,"text":"TOML Architecture Decisions","id":"toml-architecture-decisions"},{"level":3,"text":"Common TOML Parsing Pitfalls","id":"common-toml-parsing-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"YAML Parser Component Design","id":"yaml-parser-component-design"},{"level":3,"text":"YAML Parsing Mental Model","id":"yaml-parsing-mental-model"},{"level":3,"text":"Indentation-Based Parsing Algorithm","id":"indentation-based-parsing-algorithm"},{"level":3,"text":"YAML Type Inference Logic","id":"yaml-type-inference-logic"},{"level":3,"text":"YAML Architecture Decisions","id":"yaml-architecture-decisions"},{"level":3,"text":"Common YAML Parsing Pitfalls","id":"common-yaml-parsing-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Complete Parsing Pipeline","id":"complete-parsing-pipeline"},{"level":4,"text":"Pipeline Phase Breakdown","id":"pipeline-phase-breakdown"},{"level":4,"text":"Data Transformation Flow","id":"data-transformation-flow"},{"level":4,"text":"Pipeline State Management","id":"pipeline-state-management"},{"level":4,"text":"Error Recovery in Pipeline","id":"error-recovery-in-pipeline"},{"level":3,"text":"Format Detection Strategy","id":"format-detection-strategy"},{"level":4,"text":"Detection Algorithm Strategy","id":"detection-algorithm-strategy"},{"level":4,"text":"Format-Specific Detection Rules","id":"format-specific-detection-rules"},{"level":4,"text":"Detection Confidence and Fallback Strategy","id":"detection-confidence-and-fallback-strategy"},{"level":4,"text":"Ambiguity Resolution Techniques","id":"ambiguity-resolution-techniques"},{"level":3,"text":"Error Information Flow","id":"error-information-flow"},{"level":4,"text":"Error Detection Points","id":"error-detection-points"},{"level":4,"text":"Error Context Enrichment","id":"error-context-enrichment"},{"level":4,"text":"Error Propagation Strategy","id":"error-propagation-strategy"},{"level":4,"text":"Error Recovery and Continuation","id":"error-recovery-and-continuation"},{"level":4,"text":"User-Friendly Error Formatting","id":"user-friendly-error-formatting"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Error Classification","id":"error-classification"},{"level":3,"text":"Error Message Design","id":"error-message-design"},{"level":3,"text":"Error Recovery Approaches","id":"error-recovery-approaches"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Test Categories and Coverage","id":"test-categories-and-coverage"},{"level":3,"text":"Milestone Verification Points","id":"milestone-verification-points"},{"level":3,"text":"Test Data Strategy","id":"test-data-strategy"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Common Bug Symptoms and Causes","id":"common-bug-symptoms-and-causes"},{"level":4,"text":"Structural Misrepresentation Symptoms","id":"structural-misrepresentation-symptoms"},{"level":4,"text":"Value Processing Failures","id":"value-processing-failures"},{"level":4,"text":"Tokenization Boundary Errors","id":"tokenization-boundary-errors"},{"level":4,"text":"Format Detection Ambiguities","id":"format-detection-ambiguities"},{"level":4,"text":"Common Error Propagation Patterns","id":"common-error-propagation-patterns"},{"level":3,"text":"Debugging Techniques for Parsers","id":"debugging-techniques-for-parsers"},{"level":4,"text":"Layered Diagnosis Methodology","id":"layered-diagnosis-methodology"},{"level":4,"text":"Incremental Complexity Testing","id":"incremental-complexity-testing"},{"level":4,"text":"State Inspection and Tracing","id":"state-inspection-and-tracing"},{"level":4,"text":"Error Message Archaeological Analysis","id":"error-message-archaeological-analysis"},{"level":3,"text":"Debugging Tools and Inspection","id":"debugging-tools-and-inspection"},{"level":4,"text":"Interactive Parser Inspection Framework","id":"interactive-parser-inspection-framework"},{"level":4,"text":"Tokenization Analysis Toolkit","id":"tokenization-analysis-toolkit"},{"level":4,"text":"Parser State Inspection Utilities","id":"parser-state-inspection-utilities"},{"level":4,"text":"Error Analysis and Recovery Tools","id":"error-analysis-and-recovery-tools"},{"level":4,"text":"Performance and Scalability Analysis","id":"performance-and-scalability-analysis"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Core Diagnostic Infrastructure","id":"core-diagnostic-infrastructure"},{"level":4,"text":"Token Stream Analysis Tools","id":"token-stream-analysis-tools"},{"level":4,"text":"Parser State Inspection Framework","id":"parser-state-inspection-framework"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Adding New Configuration Formats","id":"adding-new-configuration-formats"},{"level":3,"text":"Advanced Feature Extensions","id":"advanced-feature-extensions"},{"level":3,"text":"Performance and Scalability Improvements","id":"performance-and-scalability-improvements"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Parsing Fundamentals","id":"parsing-fundamentals"},{"level":3,"text":"Tokenization Concepts","id":"tokenization-concepts"},{"level":3,"text":"Data Structure Concepts","id":"data-structure-concepts"},{"level":3,"text":"INI Format Concepts","id":"ini-format-concepts"},{"level":3,"text":"TOML Format Concepts","id":"toml-format-concepts"},{"level":3,"text":"YAML Format Concepts","id":"yaml-format-concepts"},{"level":3,"text":"Error Handling Concepts","id":"error-handling-concepts"},{"level":3,"text":"Architecture Concepts","id":"architecture-concepts"},{"level":3,"text":"Testing and Debugging Concepts","id":"testing-and-debugging-concepts"},{"level":3,"text":"Performance and Scalability Concepts","id":"performance-and-scalability-concepts"},{"level":3,"text":"Advanced Feature Concepts","id":"advanced-feature-concepts"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Key Term Categories","id":"key-term-categories"},{"level":4,"text":"Terminology Evolution","id":"terminology-evolution"}],"title":"Config File Parser: Design Document","markdown":"# Config File Parser: Design Document\n\n\n## Overview\n\nA multi-format configuration file parser that supports INI, TOML, and YAML formats through unified parsing architecture. The key challenge is designing flexible tokenization and parsing components that can handle fundamentally different syntactic approaches while maintaining clean separation between lexical analysis and structural interpretation.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones (foundational understanding for INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser)\n\nConfiguration file parsing represents one of the most deceptively complex challenges in software engineering. What appears to be a straightforward task—reading structured text and converting it into usable data structures—quickly reveals layers of intricate problems that demand sophisticated solutions. This complexity stems not from any single difficult algorithm, but from the intersection of multiple challenging domains: lexical analysis, syntax parsing, type inference, error recovery, and the fundamental differences between how humans write configuration files and how computers process structured data.\n\nThe core challenge lies in bridging the gap between human-friendly configuration syntax and machine-readable data structures while maintaining the flexibility and expressiveness that makes configuration files valuable in the first place. Configuration formats evolved to solve different problems for different communities, resulting in fundamentally incompatible approaches to representing the same underlying data. Building a unified parser that handles multiple formats requires understanding not just their syntactic differences, but the philosophical approaches that shaped their design.\n\n### Why Configuration Parsing is Hard\n\nThink of configuration file parsing like being a universal translator at the United Nations, but instead of translating between human languages, you're translating between different ways of thinking about structured data. Each configuration format represents a different cultural approach to organizing information—INI files reflect the Windows registry mindset of hierarchical sections, TOML embodies the programmer's desire for explicit types and unambiguous syntax, while YAML embraces the document author's preference for visual structure and minimal punctuation.\n\nThe fundamental challenge begins with **lexical ambiguity**—the same character sequence can mean entirely different things depending on context and format. Consider the innocent-looking string `key = \"value\"`. In INI format, this is a straightforward key-value assignment where the quotes are likely part of the value. In TOML, those quotes create a basic string with potential escape sequence processing. In YAML, this same line might be invalid because YAML prefers `key: value` syntax, or it could be interpreted as a mapping with a complex key containing an equals sign. A robust parser must not only recognize these differences but switch between entirely different parsing strategies based on format detection.\n\n**Context sensitivity** presents another layer of complexity that novice parser implementers consistently underestimate. The meaning of characters and tokens changes dramatically based on their surrounding context. In TOML, the sequence `[[database]]` creates an array of tables entry, but `[database]` creates a simple table header, while `[ \"database\" ]` might be part of an inline array containing a single string. The parser must maintain sophisticated state tracking to distinguish between these contexts, often requiring lookahead parsing or backtracking when initial assumptions prove incorrect.\n\n**Whitespace semantics** vary dramatically between formats in ways that create subtle but critical parsing challenges. INI files generally treat whitespace as optional padding around meaningful content, allowing liberal spacing that gets trimmed during processing. TOML follows similar principles but with stricter rules around string literals and multiline constructs. YAML, however, makes whitespace semantically meaningful—indentation levels determine nesting structure, and the difference between two spaces and four spaces can completely change the resulting data structure. A parser architecture must accommodate both whitespace-agnostic and whitespace-sensitive parsing modes, often within the same parsing session when handling mixed format scenarios.\n\nThe **type inference problem** reveals another dimension of complexity that extends beyond simple syntax parsing. Configuration files exist primarily for human authorship, which means they optimize for writing convenience rather than parsing simplicity. Humans write `timeout = 30` expecting the parser to understand this represents a numeric value, not a string containing digits. They write `enabled = yes` expecting boolean interpretation, and `created = 2023-10-15T14:30:00Z` expecting datetime parsing. Each format handles type inference differently—TOML provides explicit type syntax but still requires inference for basic literals, YAML attempts aggressive type inference that can surprise users, while INI files traditionally treat everything as strings, leaving type interpretation to the application layer.\n\n**Error recovery and reporting** becomes exponentially more complex in configuration parsing because configuration files are primarily authored by humans, not generated by tools. Human-authored content contains creative deviations from formal grammar, partial completions during editing, and errors that reflect misunderstanding of format rules rather than simple typos. A parser must not only detect errors but provide meaningful feedback that helps users understand both what went wrong and how to fix it. This requires maintaining enough parsing context to generate suggestions, tracking line and column positions through complex tokenization processes, and distinguishing between recoverable syntax errors and fundamental format violations.\n\n> **Critical Insight**: Configuration parsing difficulty stems not from complex algorithms but from the impedance mismatch between human-friendly syntax and machine-processable structure. Each format makes different trade-offs in this space, requiring parsers to switch between fundamentally different processing paradigms.\n\nThe **nested structure mapping problem** presents unique challenges that distinguish configuration parsing from simpler key-value processing. Modern configuration formats support deeply nested data structures that must be constructed incrementally as parsing proceeds. TOML's dotted key syntax like `database.connection.pool.size = 10` requires creating intermediate dictionary structures on demand while detecting conflicts with previously defined keys. YAML's indentation-based nesting requires stack-based tracking of scope levels with complex rules for when blocks end and new structures begin. INI files, despite their apparent simplicity, introduce nesting through section headers that create hierarchical organization.\n\n**Format detection and switching** adds another layer of complexity that becomes critical in real-world applications. Users rarely want to specify the configuration format explicitly—they expect parsers to automatically detect whether a file is INI, TOML, or YAML based on content analysis. This detection must happen early enough to select the appropriate parsing strategy but late enough to have sufficient content for reliable identification. False positives in format detection can lead to confusing error messages when content gets parsed with the wrong grammar rules.\n\n**Unicode and encoding handling** permeates every aspect of configuration parsing but often gets overlooked until problems emerge. Configuration files exist in international contexts with multi-byte character encodings, right-to-left text, combining characters, and normalization requirements. String literal processing must handle escape sequences that can introduce arbitrary Unicode code points, while maintaining proper character counting for error position reporting. Different formats have varying levels of Unicode sophistication—YAML requires full Unicode support for identifiers, while INI files traditionally assume ASCII-compatible encodings.\n\n### Format Comparison Analysis\n\nUnderstanding the specific characteristics and parsing requirements of each configuration format provides the foundation for designing a unified parsing architecture. Each format evolved to solve different problems, resulting in distinct approaches to syntax, semantics, and complexity management that directly influence parser design decisions.\n\n| Format Aspect | INI | TOML | YAML Subset |\n|---------------|-----|------|-------------|\n| **Primary Design Goal** | Simple Windows registry-style configuration | Unambiguous configuration with explicit types | Human-readable documents with minimal syntax |\n| **Syntax Philosophy** | Section-based key-value pairs | Programming language inspired explicit syntax | Indentation-based visual hierarchy |\n| **Parsing Complexity** | Low - line-based processing | High - recursive descent with lookahead | Medium - indentation-sensitive with context |\n| **Type System** | Implicit string-based | Explicit with inference | Aggressive implicit inference |\n| **Nesting Support** | Section-based hierarchy only | Full nested structures with dotted keys | Arbitrary depth through indentation |\n| **Comment Handling** | Semicolon and hash prefixes | Hash prefix only | Hash prefix with flow considerations |\n| **String Literals** | Basic quoting with simple escapes | Multiple string types with complex rules | Quoted, unquoted, and multiline variants |\n| **Error Recovery** | Forgiving - skip malformed lines | Strict - fail on ambiguity | Context-sensitive validation |\n\n**INI Format Parsing Characteristics** reflect its origins as a simple configuration mechanism for early PC software. The format prioritizes ease of manual editing over sophisticated data representation, resulting in parsing requirements that favor line-by-line processing with minimal lookahead. INI parsers can process files streaming fashion, making decisions about each line independently based on simple pattern matching. This simplicity, however, creates ambiguities that require careful design decisions around edge cases.\n\nThe section-based organization of INI files creates a natural parsing structure where the parser maintains minimal state—primarily the current section name and accumulated key-value pairs. Section headers use bracket syntax `[section_name]` that's visually distinct from key-value pairs, enabling reliable pattern-based identification. Key-value pairs support both equals and colon separators (`key = value` and `key: value`) with whitespace trimming, but this flexibility introduces edge cases around keys or values that contain these separator characters.\n\nINI comment handling appears straightforward but contains subtle complexities that impact parser design. Comments can begin with semicolons or hash symbols and extend to the end of the line, but the interaction between comments and quoted strings requires careful consideration. A line like `path = \"C:\\Program Files\\App\" ; comment` must distinguish between semicolons inside quoted strings and comment-starting semicolons, requiring some form of quoted string parsing even in this simple format.\n\n**TOML Format Parsing Characteristics** represent the opposite extreme from INI simplicity, embracing explicit syntax that eliminates ambiguity at the cost of parsing complexity. TOML requires full tokenization with lookahead parsing, recursive descent for nested structures, and sophisticated type inference that respects explicit type annotations while handling implicit cases gracefully.\n\nThe tokenization requirements for TOML include multiple string literal types that each follow different processing rules. Basic strings use double quotes with escape sequence processing, literal strings use single quotes with minimal processing, and multiline variants of both types have complex rules for handling leading and trailing whitespace. Integer literals can include underscores for readability (`1_000_000`), floating-point numbers support scientific notation, and the format includes native boolean and datetime types that require specialized parsing logic.\n\nTOML's table structure creates some of the most complex parsing challenges in configuration file processing. Simple tables use `[table_name]` syntax similar to INI sections, but nested tables use dotted notation `[table.subtable.deep]` that requires creating intermediate structures dynamically. Array-of-tables syntax `[[table_name]]` creates list structures containing dictionaries, with complex rules about when new entries are created versus when existing entries are extended. Dotted key assignments like `physical.color = \"orange\"` can create nested structures implicitly, but conflict with explicitly defined tables in ways that require careful validation.\n\n| TOML Parsing Challenge | Complexity Level | Key Difficulty |\n|------------------------|------------------|----------------|\n| **Basic key-value pairs** | Low | Type inference and string literal handling |\n| **Inline tables and arrays** | Medium | Balanced bracket parsing with nested values |\n| **Table headers** | Medium | Nested structure creation and conflict detection |\n| **Array of tables** | High | List management with dictionary entry semantics |\n| **Dotted keys** | High | Implicit structure creation with conflict resolution |\n\n**YAML Subset Parsing Characteristics** introduce indentation-sensitive parsing that requires fundamentally different algorithms from character-delimited formats. YAML parsers must track indentation levels using a stack-based approach, making parsing decisions based on the relationship between current and previous indentation rather than explicit delimiters.\n\nThe indentation sensitivity of YAML creates parsing challenges that don't exist in other formats. The parser must distinguish between spaces and tabs (YAML forbids tabs for indentation), track indentation levels precisely, and determine when indentation changes indicate structure transitions versus continuation of existing structures. A mapping entry like `key: value` at indentation level 2 might start a new mapping, continue an existing sequence, or represent a nested value depending on the preceding context.\n\nYAML's type inference system attempts to automatically detect appropriate types for scalar values, but the rules can surprise users and complicate parsing. The string \"yes\" becomes boolean true, \"1.0\" becomes a floating-point number, and \"2023-10-15\" might become a date object depending on parser configuration. This aggressive inference requires parsers to implement pattern matching against multiple type signatures while providing mechanisms for users to override automatic detection through explicit quoting.\n\nFlow syntax in YAML provides inline alternatives to block structure using familiar bracket and brace notation (`[1, 2, 3]` for sequences and `{key: value}` for mappings). Supporting flow syntax requires YAML parsers to handle both indentation-sensitive block parsing and delimiter-based parsing within the same document, often switching between modes multiple times as parsing proceeds.\n\n> **Decision: Unified Parsing Architecture vs Format-Specific Parsers**\n> - **Context**: Need to support multiple configuration formats with different parsing requirements and complexity levels\n> - **Options Considered**: \n>   1. Completely separate parsers for each format with no shared code\n>   2. Unified parser that handles all formats through extensive configuration\n>   3. Shared tokenization layer with format-specific parsing logic\n> - **Decision**: Shared tokenization foundation with format-specific parsing components\n> - **Rationale**: Balances code reuse for common functionality (string handling, position tracking, error reporting) while acknowledging that parsing strategies differ fundamentally between formats\n> - **Consequences**: Enables consistent error reporting and position tracking across formats, allows incremental implementation starting with simpler formats, but requires careful interface design to accommodate different tokenization needs\n\nThe architectural decision to use shared tokenization with format-specific parsers reflects the reality that while these formats differ significantly in syntax and semantics, they share common low-level requirements around string processing, position tracking, and error context management. This approach allows the implementation to start with the simpler INI format to establish core tokenization patterns, then extend the tokenizer capabilities as needed for TOML's more complex requirements, and finally adapt for YAML's unique indentation-sensitive needs.\n\n**Error Handling Strategy Comparison** reveals how format characteristics influence error detection, recovery, and reporting approaches. Each format's syntax philosophy directly impacts what constitutes an error, how errors can be detected, and what recovery strategies are viable.\n\n| Error Category | INI Approach | TOML Approach | YAML Approach |\n|----------------|--------------|---------------|---------------|\n| **Syntax Errors** | Skip malformed lines, continue processing | Fail fast with precise error location | Context-sensitive validation with recovery hints |\n| **Type Errors** | No type validation (strings only) | Strict type checking with clear messages | Type inference conflicts with override suggestions |\n| **Structure Errors** | Section redefinition warnings | Key redefinition failures with conflict location | Indentation inconsistencies with structure visualization |\n| **Encoding Errors** | Best-effort ASCII compatible processing | UTF-8 validation with byte position reporting | Full Unicode support with normalization guidance |\n\nThis comprehensive comparison of format characteristics and parsing requirements establishes the foundation for designing a unified configuration parsing system that respects each format's unique properties while sharing common infrastructure where beneficial. The next sections will detail how these insights translate into specific architectural decisions and component designs.\n\n### Implementation Guidance\n\nThe implementation of a multi-format configuration parser requires careful technology selection and project organization that accommodates the varying complexity levels of different formats while maintaining clean separation between shared and format-specific functionality.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| **Tokenizer Core** | Regular expressions with manual state tracking | Hand-written state machine with character-level control |\n| **String Processing** | Python built-in string methods with manual escape handling | Custom string literal parser with full Unicode support |\n| **Data Structures** | Native dictionaries and lists with manual nesting | Custom tree structures with metadata tracking |\n| **Error Reporting** | Exception-based with basic line number tracking | Structured error objects with position ranges and suggestions |\n| **File I/O** | Direct file reading with encoding detection | Streaming parser with configurable buffer sizes |\n| **Type Inference** | String-based with manual conversion functions | Plugin-based type detection with user customization |\n\n**B. Recommended File/Module Structure:**\n\n```\nconfig-parser/\n├── src/\n│   ├── __init__.py                    ← public API exports\n│   ├── core/\n│   │   ├── __init__.py               ← shared parsing infrastructure\n│   │   ├── tokenizer.py              ← base tokenization with position tracking\n│   │   ├── errors.py                 ← error types and reporting utilities\n│   │   ├── types.py                  ← token definitions and data structures\n│   │   └── detector.py               ← format detection logic\n│   ├── parsers/\n│   │   ├── __init__.py               ← parser registry and factory\n│   │   ├── ini_parser.py             ← INI format implementation (start here)\n│   │   ├── toml_parser.py            ← TOML format implementation\n│   │   └── yaml_parser.py            ← YAML subset implementation\n│   └── utils/\n│       ├── __init__.py               ← utility functions\n│       ├── string_utils.py           ← string literal processing helpers\n│       └── type_inference.py         ← automatic type detection\n├── tests/\n│   ├── test_data/                    ← sample configuration files\n│   ├── test_tokenizer.py            ← tokenizer unit tests\n│   ├── test_ini_parser.py           ← INI parser tests\n│   ├── test_toml_parser.py          ← TOML parser tests\n│   └── test_yaml_parser.py          ← YAML parser tests\n└── examples/\n    ├── basic_usage.py               ← simple parsing examples\n    └── advanced_features.py        ← error handling and customization\n```\n\n**C. Infrastructure Starter Code:**\n\n```python\n# src/core/types.py - Complete token type definitions\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nclass TokenType(Enum):\n    # Common tokens across all formats\n    STRING = auto()\n    NUMBER = auto()\n    BOOLEAN = auto()\n    IDENTIFIER = auto()\n    EQUALS = auto()\n    COLON = auto()\n    NEWLINE = auto()\n    EOF = auto()\n    COMMENT = auto()\n    \n    # Format-specific tokens\n    SECTION_START = auto()      # [\n    SECTION_END = auto()        # ]\n    ARRAY_START = auto()        # [ in TOML/YAML context\n    ARRAY_END = auto()          # ]\n    OBJECT_START = auto()       # {\n    OBJECT_END = auto()         # }\n    COMMA = auto()\n    DOT = auto()\n    \n    # YAML-specific\n    INDENT = auto()\n    DEDENT = auto()\n    BLOCK_SEQUENCE = auto()     # -\n    \n    # Special tokens\n    INVALID = auto()\n\n@dataclass\nclass Position:\n    \"\"\"Tracks position in source file for error reporting.\"\"\"\n    line: int\n    column: int\n    offset: int\n    \n    def __str__(self) -> str:\n        return f\"line {self.line}, column {self.column}\"\n\n@dataclass\nclass Token:\n    \"\"\"Represents a parsed token with position and value information.\"\"\"\n    type: TokenType\n    value: Any\n    position: Position\n    raw_text: str = \"\"\n    \n    def __str__(self) -> str:\n        return f\"{self.type.name}({self.value}) at {self.position}\"\n```\n\n```python\n# src/core/errors.py - Complete error handling infrastructure\nfrom typing import List, Optional\nfrom .types import Position\n\nclass ParseError(Exception):\n    \"\"\"Base class for all parsing errors with position information.\"\"\"\n    \n    def __init__(self, message: str, position: Optional[Position] = None, \n                 suggestion: Optional[str] = None):\n        self.message = message\n        self.position = position\n        self.suggestion = suggestion\n        super().__init__(self._format_message())\n    \n    def _format_message(self) -> str:\n        msg = self.message\n        if self.position:\n            msg = f\"At {self.position}: {msg}\"\n        if self.suggestion:\n            msg = f\"{msg}\\nSuggestion: {self.suggestion}\"\n        return msg\n\nclass TokenError(ParseError):\n    \"\"\"Errors during tokenization phase.\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Errors in format-specific syntax.\"\"\"\n    pass\n\nclass StructureError(ParseError):\n    \"\"\"Errors in logical structure (key conflicts, invalid nesting).\"\"\"\n    pass\n\ndef create_error_context(source: str, position: Position, \n                        context_lines: int = 2) -> str:\n    \"\"\"Generate visual error context showing problematic source location.\"\"\"\n    lines = source.split('\\n')\n    start_line = max(0, position.line - context_lines - 1)\n    end_line = min(len(lines), position.line + context_lines)\n    \n    context = []\n    for i in range(start_line, end_line):\n        line_num = i + 1\n        marker = \">>>\" if line_num == position.line else \"   \"\n        context.append(f\"{marker} {line_num:3}: {lines[i]}\")\n        \n        # Add column pointer for error line\n        if line_num == position.line:\n            pointer = \" \" * (7 + position.column - 1) + \"^\"\n            context.append(pointer)\n    \n    return '\\n'.join(context)\n```\n\n**D. Core Logic Skeleton Code:**\n\n```python\n# src/core/tokenizer.py - Base tokenizer with TODO implementation points\nfrom typing import Iterator, Optional, List\nfrom .types import Token, TokenType, Position\nfrom .errors import TokenError\n\nclass BaseTokenizer:\n    \"\"\"Base tokenizer providing position tracking and common functionality.\"\"\"\n    \n    def __init__(self, source: str):\n        self.source = source\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n    \n    def current_position(self) -> Position:\n        \"\"\"Get current position for error reporting.\"\"\"\n        return Position(self.line, self.column, self.position)\n    \n    def peek(self, offset: int = 0) -> str:\n        \"\"\"Look ahead at character without consuming it.\"\"\"\n        pos = self.position + offset\n        if pos >= len(self.source):\n            return '\\0'  # EOF marker\n        return self.source[pos]\n    \n    def advance(self) -> str:\n        \"\"\"Consume and return current character, updating position.\"\"\"\n        if self.position >= len(self.source):\n            return '\\0'\n        \n        char = self.source[self.position]\n        self.position += 1\n        \n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n            \n        return char\n    \n    def tokenize(self) -> List[Token]:\n        \"\"\"Main tokenization entry point - implement in subclasses.\"\"\"\n        # TODO 1: Initialize tokenization state\n        # TODO 2: Loop through characters calling appropriate token handlers\n        # TODO 3: Handle end-of-file and return complete token list\n        # TODO 4: Ensure all tokens have proper position information\n        raise NotImplementedError(\"Subclasses must implement tokenize()\")\n    \n    def skip_whitespace(self) -> None:\n        \"\"\"Skip whitespace characters (except newlines in some formats).\"\"\"\n        # TODO: Define which characters count as skippable whitespace\n        # TODO: Handle format-specific whitespace rules (YAML indentation)\n        # TODO: Update position tracking while skipping\n        pass\n    \n    def read_string_literal(self, quote_char: str) -> Token:\n        \"\"\"Parse quoted string with escape sequence handling.\"\"\"\n        # TODO 1: Track starting position for error reporting\n        # TODO 2: Consume opening quote\n        # TODO 3: Process characters until closing quote\n        # TODO 4: Handle escape sequences (\\n, \\t, \\\", \\\\, etc.)\n        # TODO 5: Handle unterminated strings with helpful error messages\n        # TODO 6: Return STRING token with processed value\n        pass\n    \n    def read_number(self) -> Token:\n        \"\"\"Parse numeric literal (integer or float).\"\"\"\n        # TODO 1: Collect digit characters and decimal points\n        # TODO 2: Handle scientific notation (1e5, 1E-3)\n        # TODO 3: Handle format-specific features (TOML underscores)\n        # TODO 4: Distinguish between integers and floats\n        # TODO 5: Validate number format and return NUMBER token\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- **String Processing**: Use Python's `str.translate()` for efficient character mapping during escape sequence processing\n- **Position Tracking**: Consider using `enumerate()` with `splitlines(keepends=True)` for line-aware processing\n- **Regular Expressions**: The `re` module's `finditer()` method provides position information useful for tokenization\n- **Error Context**: Use `textwrap.dedent()` and `textwrap.indent()` for clean error message formatting\n- **Type Inference**: Python's `ast.literal_eval()` can safely evaluate simple literals for type detection\n- **File Encoding**: Always specify encoding explicitly when opening files: `open(filename, 'r', encoding='utf-8')`\n- **Performance**: For large configuration files, consider using `io.StringIO` for string manipulation instead of concatenation\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the basic tokenizer infrastructure:\n\n**Test Command**: `python -m pytest tests/test_tokenizer.py -v`\n\n**Expected Output**: All tokenizer tests should pass, demonstrating proper position tracking, basic token recognition, and error handling.\n\n**Manual Verification**: Create a simple test script:\n```python\nfrom src.core.tokenizer import BaseTokenizer\n\n# Test with a simple input\nsource = 'key = \"value\"\\n# comment\\n'\ntokenizer = BaseTokenizer(source)  # Will fail until you implement a concrete subclass\ntokens = tokenizer.tokenize()\nfor token in tokens:\n    print(f\"{token.type.name}: '{token.value}' at {token.position}\")\n```\n\n**Signs Something is Wrong**:\n- Position tracking jumps or becomes negative → Check advance() method character handling\n- Tokens missing position information → Ensure current_position() called when creating tokens  \n- String parsing fails on quotes → Verify escape sequence handling in read_string_literal()\n- Performance issues on large files → Check for inefficient string concatenation in tokenizer loops\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones (foundational scoping for INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser)\n\nUnderstanding the precise scope of our configuration file parser is essential for making informed architectural decisions and avoiding feature creep during implementation. Think of this section as drawing the boundaries of a map—we need to know not just what territories we'll explore, but also what lies beyond our borders so we don't accidentally wander into complexity quicksand. The goals and non-goals serve as our north star throughout the implementation journey, helping us make consistent decisions when faced with ambiguous requirements or tempting feature additions.\n\nThe challenge in scoping a multi-format parser lies in the **impedance mismatch** between different configuration philosophies. INI files embrace simplicity with flat key-value pairs, TOML aims for human readability with explicit typing, and YAML prioritizes minimalist syntax with implicit typing. Each format makes different trade-offs between expressiveness, readability, and parsing complexity. Our scope must find the sweet spot that captures the essential utility of each format without drowning in edge cases that provide minimal real-world value.\n\n### Functional Goals\n\nThe functional goals define the minimum viable product that delivers meaningful value to users while providing comprehensive learning experiences in parsing techniques. These goals were selected based on analyzing common configuration use cases and identifying the features that appear in 80% of real-world configuration files.\n\n#### Core Format Support Requirements\n\nOur parser must support three distinct configuration formats, each representing a different parsing paradigm. This multi-format approach provides exposure to line-based parsing (INI), recursive descent parsing (TOML), and indentation-sensitive parsing (YAML). The format support is intentionally comprehensive within each format's core feature set rather than attempting to cover exotic edge cases.\n\n| Format | Core Features | Complexity Level | Learning Focus |\n|--------|---------------|------------------|----------------|\n| INI | Sections, key-value pairs, comments | Low | String processing, line parsing |\n| TOML | Tables, arrays, type system, nested structures | High | Recursive descent, tokenization |\n| YAML | Indentation-based nesting, mappings, sequences | Medium | Stack-based parsing, implicit typing |\n\n**INI Format Support Goals:**\n\nThe INI parser serves as our foundational implementation, introducing core parsing concepts without overwhelming complexity. INI files follow a straightforward section-based organization that maps naturally to nested dictionaries. Our implementation must handle section headers using bracket notation `[section.subsection]`, creating appropriate nested dictionary structures. Key-value pairs support both equals and colon delimiters (`key=value` and `key: value`), with automatic whitespace trimming around keys and values.\n\nComment handling requires supporting both semicolon and hash prefixes (`; comment` and `# comment`), with comments allowed on their own lines and inline after values. String value parsing must handle both quoted and unquoted strings, with proper escape sequence processing for quoted strings including `\\\"`, `\\\\`, `\\n`, `\\r`, and `\\t`. The parser should support multi-line value continuation using backslash line endings, properly joining continued lines with space characters.\n\nGlobal key handling—keys that appear before any section header—requires creating an implicit root section or global namespace. This prevents data loss and provides consistent access patterns for simple configuration files that don't use sections.\n\n**TOML Format Support Goals:**\n\nTOML parsing represents the most complex component, introducing full tokenization and recursive descent parsing techniques. The tokenizer must recognize all TOML grammar elements including basic strings (quoted with double quotes), literal strings (quoted with single quotes), and their multiline variants (triple-quoted). Numeric parsing includes integers with optional underscores (`1_000_000`), floats with scientific notation, and hexadecimal/octal/binary integer formats.\n\nTable parsing handles both simple tables `[table.name]` and array-of-tables `[[array.name]]` syntax. Simple tables create nested dictionary structures where dotted paths like `[server.database.connection]` create three levels of nesting. Array-of-tables creates lists of dictionaries, where each occurrence of `[[servers]]` adds a new dictionary entry to the servers array.\n\nInline syntax support includes inline tables `{ key1 = \"value1\", key2 = 42 }` and inline arrays `[ 1, 2, \"three\", true ]` with mixed type support. Dotted key notation like `physical.color = \"orange\"` automatically creates nested dictionary structures without requiring explicit table headers.\n\nThe TOML type system requires proper handling of strings, integers, floats, booleans, datetime values, and nested combinations of arrays and tables. Type inference converts literal syntax to appropriate Python types automatically.\n\n**YAML Subset Support Goals:**\n\nOur YAML implementation focuses on the core block syntax that represents 90% of configuration use cases, deliberately avoiding the more exotic flow syntax variants that add complexity without proportional learning value. The subset includes indentation-based block structure parsing where nesting depth is determined by consistent indentation levels. Mappings use the `key: value` syntax with proper handling of nested mappings through indentation increases.\n\nSequences use the dash prefix syntax (`- item`) for list creation, with support for both simple scalar sequences and complex nested sequences containing mappings or other sequences. Scalar type inference automatically converts unquoted values to appropriate types: numeric strings become integers or floats, `true`/`false`/`yes`/`no` become booleans, and everything else remains strings.\n\nFlow syntax support is limited to basic inline arrays `[item1, item2, item3]` and inline objects `{key1: value1, key2: value2}` to handle simple embedded structures without requiring full flow syntax parsing.\n\n#### Unified Interface Requirements\n\nBeyond format-specific parsing, our system must provide a consistent interface that abstracts away format differences for consuming applications. This unified interface enables applications to work with configuration data without knowing the source format, promoting code reuse and simplifying configuration migration.\n\nThe primary interface is a `parse_config(content, format=None)` function that accepts configuration content as a string and an optional format specifier. When format is not specified, the parser performs **format detection** by examining content characteristics like the presence of section brackets (INI), table headers (TOML), or significant indentation (YAML).\n\nAll parsers return data in a **unified output format** consisting of nested dictionaries with string keys and mixed-type values (strings, numbers, booleans, lists, nested dictionaries). This standardized representation allows consuming code to navigate configuration hierarchies using consistent patterns regardless of source format.\n\nError handling follows a structured approach using custom exception types: `TokenError` for lexical analysis failures, `SyntaxError` for grammar violations, and `StructureError` for logical inconsistencies like duplicate keys or circular references. All errors include position information (`Position` with line, column, and offset fields) and suggested fixes when possible.\n\n#### Development and Learning Goals\n\nSince this project serves as a learning vehicle for parsing concepts, our functional goals include comprehensive educational outcomes alongside practical functionality. The implementation must demonstrate **recursive descent parsing** principles through the TOML parser, showing how complex nested structures can be parsed through function call recursion that mirrors grammar structure.\n\n**Tokenization** concepts are thoroughly explored through the TOML lexer implementation, including state machine management for string literal parsing, lookahead techniques for disambiguating syntax, and position tracking for meaningful error reporting.\n\n**Type inference** and automatic type conversion provide exposure to data type detection algorithms and the trade-offs between explicit and implicit typing systems. The different approaches across INI (mostly strings), TOML (explicit typing), and YAML (aggressive implicit typing) demonstrate the spectrum of type system design decisions.\n\nError recovery and diagnostic message generation represent critical parsing skills that extend beyond configuration files to any language processing task. The implementation must demonstrate how to detect errors early, provide meaningful context, and suggest concrete fixes rather than cryptic failure messages.\n\n> **Decision: Comprehensive Format Coverage vs. Focused Implementation**\n> - **Context**: Configuration parsers can either support many formats shallowly or fewer formats deeply\n> - **Options Considered**: \n>   1. Support 5+ formats (INI, TOML, YAML, JSON, XML) with basic functionality\n>   2. Support 3 formats (INI, TOML, YAML subset) with comprehensive feature coverage\n>   3. Support 1 format (TOML only) with production-ready completeness\n> - **Decision**: Option 2 - Three formats with comprehensive coverage\n> - **Rationale**: Three formats provide exposure to fundamentally different parsing approaches (line-based, recursive descent, indentation-sensitive) while maintaining manageable complexity. Each format teaches distinct concepts without excessive overlap.\n> - **Consequences**: Deeper learning experience with manageable scope, but requires careful subset selection for YAML to avoid overwhelming complexity\n\n| Option | Parsing Techniques Learned | Implementation Complexity | Real-World Utility |\n|--------|---------------------------|--------------------------|-------------------|\n| Many formats shallow | Surface-level pattern matching | Low per format | Broad but shallow |\n| Three formats deep | Line parsing, recursive descent, indentation parsing | Medium overall | Focused and practical |\n| Single format complete | One technique deeply | Low overall | Deep but narrow |\n\n### Non-Goals\n\nThe non-goals are equally important as the functional goals, establishing clear boundaries that prevent scope creep and maintain focus on core learning objectives. These exclusions are deliberate choices based on complexity analysis, learning value assessment, and implementation timeline considerations.\n\n#### Advanced YAML Features\n\nYAML's full specification includes numerous advanced features that add significant parsing complexity without proportional educational value for our core parsing learning objectives. We explicitly exclude **multiline string syntax** including folded scalars (`>`) and literal scalars (`|`) which require complex whitespace processing rules and line break interpretation logic.\n\n**Flow syntax** beyond basic inline collections is excluded, meaning we don't support complex nested flow structures, flow mappings with complex keys, or mixed block/flow syntax within the same document. **Anchors and references** (`&anchor` and `*reference`) are excluded as they require symbol table management and cycle detection logic that distracts from core parsing concepts.\n\n**Complex implicit typing** rules are simplified to basic cases. We don't support the full YAML type inference rules that can interpret strings like `2.10` as version numbers rather than floating-point numbers, or complex date/time parsing beyond basic ISO 8601 formats. **Document directives** (`%YAML 1.2`) and multiple documents in a single file are outside our scope.\n\nThese exclusions allow us to focus on the fundamental parsing concepts—indentation tracking, nested structure creation, and basic type inference—without getting lost in YAML's more esoteric features.\n\n#### Performance Optimization Features\n\nOur parser prioritizes educational clarity over performance optimization, deliberately excluding features that would complicate the core algorithms without teaching fundamentally new parsing concepts. **Streaming parsing** for large files is excluded as it requires complex buffering logic and stateful parsing that obscures the recursive descent algorithms we're trying to demonstrate.\n\n**Memory optimization** techniques like string interning, parse tree compression, or lazy evaluation are outside scope. The implementation will create full parse trees and nested dictionary structures in memory, accepting higher memory usage in favor of simpler, more understandable algorithms.\n\n**Incremental parsing** and **caching mechanisms** are excluded as they require sophisticated invalidation logic and change detection that distracts from core parsing implementation. Each `parse_config()` call processes the entire input from scratch.\n\n**Parallel parsing** for multi-document files or concurrent tokenization/parsing pipelines are beyond scope, as the coordination complexity would overshadow the parsing algorithm learning objectives.\n\n#### Production-Ready Features\n\nWhile our parser will handle real configuration files correctly, several production-ready features are explicitly excluded to maintain focus on parsing fundamentals rather than software engineering concerns.\n\n**Schema validation** and **configuration validation** are excluded despite their practical importance. Adding schema support would require a type system definition language, validation rule engines, and comprehensive error reporting for constraint violations. These features teach configuration management rather than parsing techniques.\n\n**Plugin architectures** for custom format support are outside scope. While our design will be extensible in principle, we won't implement the dynamic loading, registration systems, or interface contracts needed for runtime format plugin support.\n\n**Configuration merging** and **environment variable interpolation** represent configuration management concerns rather than parsing challenges. Features like `${ENV_VAR}` substitution, file inclusion (`!include other.yaml`), or configuration inheritance require post-processing logic that operates on parsed results rather than parsing algorithms themselves.\n\n**Backwards compatibility** with legacy format variants is excluded. We implement current standard versions of each format rather than supporting deprecated syntax or vendor-specific extensions.\n\n#### Advanced Error Recovery\n\nWhile error detection and reporting are core goals, sophisticated **error recovery** mechanisms that attempt to continue parsing after syntax errors are excluded. Implementing recovery requires complex synchronization point detection, parser state restoration, and speculative parsing techniques that significantly complicate the core algorithms.\n\n**Syntax error correction** and **\"did you mean\" suggestions** beyond basic cases are outside scope. We'll provide meaningful error messages with position information and simple suggestions, but not fuzzy matching or complex correction algorithms.\n\n**Incremental error reporting** during parsing (reporting multiple errors in a single pass) is excluded in favor of **fail-fast behavior** that stops at the first error. This simplifies the parsing logic and error handling flow, though it means users might need multiple parse attempts to discover all syntax errors.\n\n> **Decision: Educational Focus vs. Production Features**\n> - **Context**: Learning projects can either simulate production requirements or focus purely on concept demonstration\n> - **Options Considered**: \n>   1. Include production features (schema validation, plugins, performance optimization) for realism\n>   2. Exclude production features to maintain focus on core parsing concepts\n>   3. Hybrid approach with optional production feature modules\n> - **Decision**: Option 2 - Exclude production features entirely\n> - **Rationale**: Production features teach software engineering and API design rather than parsing algorithms. Including them would double the implementation complexity while diluting the core learning experience around tokenization, recursive descent parsing, and data structure mapping.\n> - **Consequences**: Cleaner learning experience focused on parsing fundamentals, but resulting parser requires additional work for production use\n\n| Feature Category | Educational Value | Implementation Complexity | Scope Decision |\n|-----------------|-------------------|--------------------------|----------------|\n| Core parsing algorithms | Very High | Medium | Include |\n| Error detection/reporting | High | Low-Medium | Include |\n| Performance optimization | Low | High | Exclude |\n| Production features | Low-Medium | High | Exclude |\n| Advanced error recovery | Medium | Very High | Exclude |\n\n#### Format Subset Justification\n\nEach format's feature exclusions follow specific principles designed to maximize learning value while minimizing implementation burden. The subset selections ensure that students encounter the core challenges of each parsing paradigm without getting overwhelmed by format-specific edge cases.\n\n**INI Subset Rationale:**\n\nINI format exclusions focus on eliminating legacy compatibility issues and vendor-specific extensions that don't teach fundamental parsing concepts. We exclude **case-insensitive key handling** as it adds string processing complexity without parsing algorithm learning value. **Custom delimiter support** beyond equals and colon is excluded as it requires configurable tokenization rules.\n\n**Escape sequence handling** is limited to the standard set (`\\\"`, `\\\\`, `\\n`, `\\r`, `\\t`) rather than supporting arbitrary Unicode escape sequences or custom escape definitions. This provides sufficient complexity to learn escape processing without requiring full Unicode lexical analysis.\n\n**TOML Subset Rationale:**\n\nTOML exclusions eliminate the most complex edge cases while retaining comprehensive coverage of the core type system and table structures. **Custom datetime formats** beyond ISO 8601 are excluded, as datetime parsing teaches string pattern matching rather than recursive parsing concepts.\n\n**Complex array syntax** like arrays of arrays of tables with mixed inline/block syntax are excluded as they require extensive lookahead and backtracking logic that obscures the core recursive descent patterns we're demonstrating.\n\n**Advanced numeric formats** like infinite/NaN float values are excluded as they add IEEE 754 floating-point knowledge requirements without contributing to parsing algorithm understanding.\n\n**YAML Subset Rationale:**\n\nYAML exclusions focus on eliminating the features that make YAML parsing notoriously complex while retaining the indentation-sensitive parsing challenges that provide educational value. **Complex key syntax** including multi-line keys, flow keys, and complex keys (non-string keys) are excluded as they require sophisticated key parsing logic.\n\n**Advanced scalar syntax** including tagged types (`!!int`, `!!str`), complex multiline strings, and aggressive implicit typing for version numbers, IP addresses, and other domain-specific formats are excluded. Basic implicit typing for numbers, booleans, and null values is retained as it demonstrates type inference concepts.\n\n#### Unified Interface Goals\n\nThe parser must provide a clean, consistent interface that demonstrates good API design principles while supporting the learning objectives around format abstraction and error handling.\n\n| Interface Component | Requirement | Learning Objective |\n|-------------------|-------------|-------------------|\n| `parse_config()` function | Single entry point for all formats | API design consistency |\n| Format auto-detection | Determine format from content analysis | Pattern recognition algorithms |\n| Unified output structure | Nested dictionaries with consistent key access | Data structure standardization |\n| Position-aware error reporting | Line/column information for all parse errors | Error context generation |\n| Type-preserved values | Maintain numeric, boolean, and string types appropriately | Type system handling |\n\nThe unified output format must preserve type information where formats provide it explicitly (TOML), infer types where formats expect it (YAML), and provide reasonable defaults where formats are ambiguous (INI). This demonstrates the challenges of **type system impedance mismatch** across different configuration philosophies.\n\n**Format Detection Strategy Goals:**\n\nAutomatic format detection teaches pattern recognition and heuristic algorithm development. The detection algorithm analyzes content characteristics to identify format signatures: INI files typically contain `[section]` headers or `key=value` patterns, TOML files contain explicit tables or arrays with TOML-specific syntax like `[[array.of.tables]]`, and YAML files exhibit significant indentation patterns and colon-space separators.\n\nThe format detection must be robust enough to handle edge cases like empty files, files with only comments, or files that could plausibly match multiple formats. The algorithm should prefer more specific format indicators over general ones, defaulting to the most permissive format (INI) when detection is ambiguous.\n\n#### Error Handling and Diagnostics Goals\n\nComprehensive error handling serves dual purposes: demonstrating proper error management techniques and providing a debugging-friendly development experience. Our error handling goals focus on three categories of errors that teach different aspects of parser error management.\n\n**Lexical Errors (`TokenError`):**\n\nLexical errors occur during tokenization when character sequences cannot be converted into valid tokens. Examples include unterminated string literals, invalid escape sequences, or malformed numeric literals. These errors must include precise position information and suggest specific fixes like adding closing quotes or correcting escape syntax.\n\n**Syntax Errors (`SyntaxError`):**\n\nSyntax errors occur when token sequences don't match the expected grammar rules. Examples include missing section closing brackets in INI files, invalid table header syntax in TOML files, or incorrect indentation patterns in YAML files. These errors require context-aware messaging that explains what was expected versus what was encountered.\n\n**Structure Errors (`StructureError`):**\n\nStructure errors occur when parsed content violates logical consistency rules like duplicate key definitions, conflicting type assignments, or circular reference creation. These errors teach the difference between syntactic validity and semantic correctness.\n\n| Error Category | Example Triggers | Required Context | Learning Focus |\n|----------------|-----------------|------------------|----------------|\n| `TokenError` | Unterminated strings, invalid escapes | Character position, surrounding text | Lexical analysis debugging |\n| `SyntaxError` | Grammar violations, unexpected tokens | Token position, expected vs actual | Grammar rule enforcement |\n| `StructureError` | Duplicate keys, type conflicts | Logical position, conflicting definitions | Semantic validation |\n\n#### Data Structure Mapping Goals\n\nThe parser must demonstrate effective techniques for mapping disparate source formats to a unified internal representation. This **nested structure mapping** teaches fundamental data transformation concepts that apply beyond configuration parsing to any data integration challenge.\n\n**Hierarchical Structure Creation:**\n\nAll three formats must map to consistent nested dictionary structures despite using different syntax for hierarchy expression. INI sections become dictionary keys, TOML tables become nested dictionary paths, and YAML indentation becomes nested dictionary nesting. This mapping consistency allows consuming applications to navigate configuration hierarchies using uniform key access patterns.\n\n**Type Preservation and Conversion:**\n\nThe mapping must preserve semantic information where possible while providing consistent access patterns. TOML's explicit typing is preserved directly, YAML's implicit typing is resolved during parsing, and INI's string-based values undergo basic type inference for numeric and boolean patterns.\n\n**List and Dictionary Distinction:**\n\nThe parser must clearly distinguish between configuration structures that represent ordered lists versus unordered mappings, preserving this distinction in the output format. TOML arrays become Python lists, YAML sequences become Python lists, and INI multi-value keys (where supported) become Python lists.\n\n> The fundamental principle guiding our scope decisions is **learning amplification**—every included feature should teach a distinct parsing concept or reinforce previously learned concepts through varied application. Features that add complexity without educational payoff are ruthlessly excluded, while features that demonstrate core parsing principles are included even when they increase implementation effort.\n\n### Implementation Guidance\n\nThe implementation approach balances educational clarity with practical functionality, using Python's strengths in string processing and dynamic typing while demonstrating parsing concepts that transfer to other languages.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| String Processing | Built-in `str` methods with manual indexing | Regular expressions with `re` module |\n| Data Structures | Native `dict` and `list` with manual type checking | `typing` module with type hints throughout |\n| Error Handling | Custom exception classes with string messages | Rich context objects with position tracking |\n| Testing Framework | Built-in `unittest` module | `pytest` with fixtures and parametrization |\n| File I/O | Simple `open()` and `read()` operations | `pathlib` with encoding detection |\n\n#### Recommended Project Structure\n\nThe project organization reflects the component separation discussed in the high-level architecture, with clear boundaries between format-specific implementations and shared infrastructure:\n\n```\nconfig-parser/\n  src/\n    config_parser/\n      __init__.py              ← public API exports\n      core/\n        __init__.py\n        tokenizer.py           ← BaseTokenizer and TokenType definitions\n        errors.py              ← ParseError hierarchy\n        position.py            ← Position tracking utilities\n      formats/\n        __init__.py\n        ini_parser.py          ← INI-specific parsing logic\n        toml_parser.py         ← TOML tokenizer and parser\n        yaml_parser.py         ← YAML subset parser\n      utils/\n        __init__.py\n        format_detection.py    ← Automatic format detection\n        type_inference.py      ← Shared type conversion utilities\n  tests/\n    unit/\n      test_tokenizer.py\n      test_ini_parser.py\n      test_toml_parser.py\n      test_yaml_parser.py\n    integration/\n      test_unified_interface.py\n      test_format_detection.py\n    fixtures/\n      sample_configs/         ← test configuration files\n        basic.ini\n        complex.toml\n        nested.yaml\n  examples/\n    basic_usage.py\n    error_handling_demo.py\n    format_comparison.py\n```\n\n#### Core Infrastructure Starter Code\n\nThe following infrastructure provides the foundation for all format-specific parsers, handling position tracking, error management, and basic tokenization concepts:\n\n```python\n\"\"\"\nCore parsing infrastructure providing position tracking, error handling,\nand base tokenization functionality for all configuration format parsers.\n\"\"\"\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Any, Union\n\nclass TokenType(Enum):\n    \"\"\"Token types recognized across all supported configuration formats.\"\"\"\n    STRING = auto()\n    NUMBER = auto()\n    BOOLEAN = auto()\n    IDENTIFIER = auto()\n    EQUALS = auto()\n    COLON = auto()\n    NEWLINE = auto()\n    EOF = auto()\n    COMMENT = auto()\n    SECTION_START = auto()\n    SECTION_END = auto()\n    ARRAY_START = auto()\n    ARRAY_END = auto()\n    OBJECT_START = auto()\n    OBJECT_END = auto()\n    COMMA = auto()\n    DOT = auto()\n    INDENT = auto()\n    DEDENT = auto()\n    BLOCK_SEQUENCE = auto()\n    INVALID = auto()\n\n@dataclass\nclass Position:\n    \"\"\"Tracks position in source text for error reporting and debugging.\"\"\"\n    line: int\n    column: int\n    offset: int\n\n@dataclass\nclass Token:\n    \"\"\"Individual lexical unit with type, value, and position information.\"\"\"\n    type: TokenType\n    value: Any\n    position: Position\n    raw_text: str\n\nclass ParseError(Exception):\n    \"\"\"Base exception for all configuration parsing errors.\"\"\"\n    def __init__(self, message: str, position: Optional[Position] = None, \n                 suggestion: Optional[str] = None):\n        self.message = message\n        self.position = position\n        self.suggestion = suggestion\n        super().__init__(self._format_message())\n    \n    def _format_message(self) -> str:\n        \"\"\"Format error message with position and suggestion information.\"\"\"\n        # TODO: Implement error message formatting with position context\n        # TODO: Include suggestion text when available\n        # TODO: Add visual indicators for error location\n        pass\n\nclass TokenError(ParseError):\n    \"\"\"Lexical analysis errors during tokenization phase.\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Grammar violations during parsing phase.\"\"\"\n    pass\n\nclass StructureError(ParseError):\n    \"\"\"Logical consistency violations in parsed structure.\"\"\"\n    pass\n\nEOF_MARKER = '\\0'\n\nclass BaseTokenizer:\n    \"\"\"Base tokenizer providing common functionality for all formats.\"\"\"\n    \n    def __init__(self, source: str):\n        self.source = source\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n    \n    def current_position(self) -> Position:\n        \"\"\"Returns current parsing position for error reporting.\"\"\"\n        return Position(self.line, self.column, self.position)\n    \n    def peek(self, offset: int = 0) -> str:\n        \"\"\"Look ahead at character without consuming it.\"\"\"\n        pos = self.position + offset\n        if pos >= len(self.source):\n            return EOF_MARKER\n        return self.source[pos]\n    \n    def advance(self) -> str:\n        \"\"\"Consume current character and update position tracking.\"\"\"\n        if self.position >= len(self.source):\n            return EOF_MARKER\n        \n        char = self.source[self.position]\n        self.position += 1\n        \n        if char == '\\n':\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n        \n        return char\n    \n    def tokenize(self) -> List[Token]:\n        \"\"\"Main tokenization entry point. Override in format-specific tokenizers.\"\"\"\n        # TODO: Implement format-specific tokenization logic\n        # TODO: Handle whitespace according to format semantics\n        # TODO: Recognize format-specific token patterns\n        # TODO: Build token list with position tracking\n        pass\n    \n    def skip_whitespace(self) -> None:\n        \"\"\"Skip whitespace characters that don't carry semantic meaning.\"\"\"\n        while self.peek() in ' \\t' and self.peek() != EOF_MARKER:\n            self.advance()\n    \n    def read_string_literal(self, quote_char: str) -> Token:\n        \"\"\"Parse quoted string with escape sequence processing.\"\"\"\n        # TODO: Consume opening quote\n        # TODO: Process characters until closing quote\n        # TODO: Handle escape sequences (\\n, \\t, \\\", \\\\, etc.)\n        # TODO: Handle unterminated string error case\n        # TODO: Return STRING token with unescaped value\n        pass\n    \n    def read_number(self) -> Token:\n        \"\"\"Parse numeric literal with type inference.\"\"\"\n        # TODO: Collect digit characters and decimal points\n        # TODO: Handle scientific notation (1e5, 1.2e-3)\n        # TODO: Detect integer vs float based on decimal point presence\n        # TODO: Handle number format errors (multiple decimal points)\n        # TODO: Return NUMBER token with converted Python value\n        pass\n\ndef create_error_context(source: str, position: Position, context_lines: int = 2) -> str:\n    \"\"\"Generate visual error context showing source location with line numbers.\"\"\"\n    lines = source.split('\\n')\n    start_line = max(0, position.line - context_lines - 1)\n    end_line = min(len(lines), position.line + context_lines)\n    \n    context = []\n    for i in range(start_line, end_line):\n        line_num = i + 1\n        line_content = lines[i] if i < len(lines) else \"\"\n        marker = \" -> \" if line_num == position.line else \"    \"\n        context.append(f\"{marker}{line_num:4d} | {line_content}\")\n        \n        if line_num == position.line:\n            pointer = \" \" * (len(marker) + 7 + position.column - 1) + \"^\"\n            context.append(pointer)\n    \n    return \"\\n\".join(context)\n\n# Public API function signatures (implement in respective milestone modules)\ndef parse_config(content: str, format: Optional[str] = None) -> dict:\n    \"\"\"Main entry point for configuration parsing with automatic format detection.\"\"\"\n    # TODO: Implement format detection when format=None\n    # TODO: Delegate to appropriate format-specific parser\n    # TODO: Handle parsing errors with rich context\n    # TODO: Return unified nested dictionary structure\n    pass\n\ndef detect_format(content: str) -> str:\n    \"\"\"Analyze content characteristics to determine configuration format.\"\"\"\n    # TODO: Look for INI-specific patterns ([sections], key=value)\n    # TODO: Look for TOML-specific patterns ([[tables]], explicit arrays)\n    # TODO: Look for YAML-specific patterns (indentation, key: value)\n    # TODO: Return format string or raise detection error\n    pass\n```\n\n#### Parser Implementation Skeleton\n\nEach format-specific parser builds on the base tokenizer infrastructure while implementing parsing algorithms appropriate to the format's syntactic structure:\n\n```python\n\"\"\"\nFormat-specific parser skeletons demonstrating different parsing approaches.\nStudents implement the TODO sections using concepts from respective milestones.\n\"\"\"\n\nclass INIParser:\n    \"\"\"Line-based parser for INI format configuration files.\"\"\"\n    \n    def __init__(self, content: str):\n        self.lines = content.split('\\n')\n        self.line_index = 0\n        self.current_section = None\n        self.result = {}\n    \n    def parse(self) -> dict:\n        \"\"\"Parse INI content into nested dictionary structure.\"\"\"\n        # TODO 1: Initialize result dict and set current_section to global\n        # TODO 2: Iterate through lines, skipping empty lines and comments\n        # TODO 3: Detect section headers and update current_section\n        # TODO 4: Parse key-value pairs and add to current section\n        # TODO 5: Handle multi-line continuations with backslash endings\n        # TODO 6: Return completed nested dictionary structure\n        pass\n    \n    def _parse_section_header(self, line: str) -> str:\n        \"\"\"Extract section name from [section] line.\"\"\"\n        # TODO: Validate bracket syntax and extract section name\n        # TODO: Support nested sections with dot notation\n        # TODO: Handle whitespace around section names\n        pass\n    \n    def _parse_key_value_pair(self, line: str) -> tuple:\n        \"\"\"Parse key=value or key: value into (key, value) tuple.\"\"\"\n        # TODO: Split on = or : delimiter (handle multiple = in value)\n        # TODO: Trim whitespace from key and value\n        # TODO: Handle quoted values with escape sequences\n        # TODO: Strip inline comments from unquoted values\n        pass\n\nclass TOMLParser:\n    \"\"\"Recursive descent parser for TOML format with full tokenization.\"\"\"\n    \n    def __init__(self, content: str):\n        self.tokenizer = TOMLTokenizer(content)\n        self.tokens = self.tokenizer.tokenize()\n        self.token_index = 0\n        self.result = {}\n    \n    def parse(self) -> dict:\n        \"\"\"Parse TOML tokens into nested dictionary structure.\"\"\"\n        # TODO 1: Initialize result dictionary and current table context\n        # TODO 2: Process tokens sequentially, dispatching by token type\n        # TODO 3: Handle table headers and array-of-tables headers\n        # TODO 4: Parse key-value assignments with dotted key support\n        # TODO 5: Handle inline tables and inline arrays recursively\n        # TODO 6: Validate no key redefinition errors\n        pass\n    \n    def _parse_table_header(self) -> str:\n        \"\"\"Parse [table.name] header and return dotted table path.\"\"\"\n        # TODO: Consume opening bracket token\n        # TODO: Collect identifier and dot tokens for table path\n        # TODO: Consume closing bracket token\n        # TODO: Validate table path doesn't conflict with existing keys\n        pass\n    \n    def _parse_inline_table(self) -> dict:\n        \"\"\"Parse {key1 = value1, key2 = value2} inline table syntax.\"\"\"\n        # TODO: Consume opening brace token\n        # TODO: Parse key-value pairs separated by commas\n        # TODO: Handle nested inline tables and arrays recursively\n        # TODO: Consume closing brace token and return dictionary\n        pass\n\nclass YAMLParser:\n    \"\"\"Stack-based parser for YAML subset with indentation sensitivity.\"\"\"\n    \n    def __init__(self, content: str):\n        self.lines = content.split('\\n')\n        self.line_index = 0\n        self.indentation_stack = [0]\n        self.result = {}\n    \n    def parse(self) -> dict:\n        \"\"\"Parse YAML content using indentation-based nesting.\"\"\"\n        # TODO 1: Initialize result and indentation tracking\n        # TODO 2: Process lines, calculating indentation levels\n        # TODO 3: Handle indentation increases (push to stack, nest deeper)\n        # TODO 4: Handle indentation decreases (pop from stack, return to parent)\n        # TODO 5: Parse mappings (key: value) and sequences (- item)\n        # TODO 6: Handle flow syntax for inline collections\n        pass\n    \n    def _parse_mapping_line(self, line: str, indent_level: int) -> tuple:\n        \"\"\"Parse 'key: value' mapping line.\"\"\"\n        # TODO: Split on first colon character\n        # TODO: Trim whitespace from key and value parts\n        # TODO: Handle quoted keys and values\n        # TODO: Perform type inference on value part\n        pass\n    \n    def _calculate_indentation(self, line: str) -> int:\n        \"\"\"Calculate indentation level and validate consistency.\"\"\"\n        # TODO: Count leading spaces (forbid tabs in YAML)\n        # TODO: Validate indentation is consistent with previous levels\n        # TODO: Return indentation level for stack management\n        pass\n```\n\n#### Milestone Checkpoints\n\nEach milestone includes specific verification points to ensure correct implementation before proceeding to the next phase:\n\n**Milestone 1 Checkpoint (INI Parser):**\n- Run: `python -m pytest tests/unit/test_ini_parser.py -v`\n- Expected: All basic INI parsing tests pass including sections, comments, quoted strings\n- Manual verification: Parse sample INI file and verify nested dictionary structure\n- Debug check: Print parsed structure and confirm section nesting is correct\n\n**Milestone 2 Checkpoint (TOML Tokenizer):**\n- Run: `python -c \"from config_parser.formats.toml_parser import TOMLTokenizer; t = TOMLTokenizer('key = \\\"value\\\"'); print(t.tokenize())\"`\n- Expected: List of tokens including IDENTIFIER, EQUALS, STRING, EOF with correct values\n- Manual verification: Tokenize complex TOML sample and verify all token types are recognized\n- Debug check: Verify position tracking is accurate by checking token position fields\n\n**Milestone 3 Checkpoint (TOML Parser):**\n- Run: `python -m pytest tests/unit/test_toml_parser.py -v`\n- Expected: TOML parsing tests pass including tables, arrays-of-tables, dotted keys\n- Manual verification: Parse complex TOML file with nested tables and verify structure\n- Debug check: Test table redefinition error handling works correctly\n\n**Milestone 4 Checkpoint (YAML Parser):**\n- Run: `python -m pytest tests/unit/test_yaml_parser.py -v`\n- Expected: YAML subset tests pass including indented blocks, sequences, mappings\n- Manual verification: Parse nested YAML structure and verify indentation handling\n- Debug check: Test indentation validation correctly rejects malformed input\n\n#### Language-Specific Implementation Hints\n\n**String Processing Efficiency:**\nUse string slicing (`source[start:end]`) rather than character-by-character processing where possible. Python's string methods like `str.startswith()`, `str.strip()`, and `str.split()` are optimized for common parsing operations.\n\n**Type Inference Implementation:**\nLeverage Python's dynamic typing for simple type inference: use `int()`, `float()`, and `bool()` conversion functions with try-catch blocks to detect appropriate types. For YAML boolean inference, check against common boolean string representations: `{'true', 'false', 'yes', 'no', 'on', 'off'}`.\n\n**Error Context Generation:**\nUse `str.splitlines(keepends=True)` to preserve original line endings when generating error context displays. This maintains consistent character position calculations across different line ending styles.\n\n**Dictionary Path Creation:**\nFor nested dictionary creation from dotted paths like `server.database.host`, use a helper function that creates intermediate dictionary levels as needed:\n\n```python\ndef set_nested_dict_value(target_dict: dict, dotted_path: str, value: Any) -> None:\n    \"\"\"Set value in nested dictionary using dotted path notation.\"\"\"\n    # TODO: Split path on dots\n    # TODO: Navigate/create intermediate dictionary levels\n    # TODO: Set final key to value\n    # TODO: Handle conflicts with existing non-dict values\n    pass\n```\n\n**Position Tracking Accuracy:**\nMaintain separate line and column counters during tokenization. Increment line counter on `\\n` characters and reset column to 1. Increment column counter for all other characters including `\\r` and `\\t`. Track absolute byte offset separately for efficient source text slicing.\n\n#### Common Implementation Pitfalls\n\n⚠️ **Pitfall: Inconsistent Position Tracking**\nForgetting to update position information during tokenization leads to meaningless error messages. Always call `current_position()` before creating tokens, and ensure `advance()` properly updates line, column, and offset counters. Test position tracking by deliberately introducing syntax errors and verifying error messages point to the correct location.\n\n⚠️ **Pitfall: Format Detection False Positives**\nSimple format detection can misidentify files when formats share syntax elements. A TOML file with only key-value pairs might be detected as INI. Use multiple detection criteria and prefer more specific format indicators. Test detection with edge cases like empty files, comment-only files, and ambiguous minimal examples.\n\n⚠️ **Pitfall: Type Inference Inconsistency**\nDifferent formats have different type inference rules. INI typically treats everything as strings unless explicitly converted, TOML has explicit typing, and YAML aggressively infers types. Implement format-specific type inference rather than trying to unify the rules, as the differences are fundamental to each format's design philosophy.\n\n⚠️ **Pitfall: Error Recovery Complexity**\nAttempting sophisticated error recovery in a learning project leads to complex parser state management that obscures the core parsing algorithms. Use fail-fast error handling that stops at the first error with detailed diagnostics rather than trying to continue parsing after errors.\n\nThe implementation approach emphasizes correctness and educational clarity over performance optimization or advanced features. Focus on making the core parsing algorithms clear and well-tested before considering any optimizations or additional features beyond the defined scope.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - this section establishes the foundational architecture that supports all format-specific implementations\n\nConfiguration file parsing requires a carefully designed architecture that can handle three fundamentally different syntactic approaches while maintaining clean separation of concerns. Think of this architecture as a universal translator that needs to understand multiple languages (INI, TOML, YAML) but always produces the same kind of output (nested dictionaries). The key challenge lies in designing components that are both flexible enough to handle format-specific complexities and structured enough to share common parsing infrastructure.\n\n![System Component Architecture](./diagrams/system-architecture.svg)\n\nThe architecture follows a classic **two-phase parsing approach**: lexical analysis (tokenization) followed by syntactic analysis (parsing). However, unlike traditional compilers that target machine code, our parser targets human-readable data structures. This creates unique design challenges around **impedance mismatch** - bridging the gap between human-friendly configuration syntax and machine-friendly nested dictionaries.\n\n### Component Responsibilities\n\nThe system divides parsing responsibilities across five core components, each with distinct concerns and well-defined interfaces. This separation enables independent development and testing while maintaining clear data flow boundaries.\n\n**Format Detection Component** serves as the system's entry point, automatically identifying which parser to invoke based on content analysis. Think of it as a postal sorting machine that examines envelope characteristics to determine the correct destination. This component analyzes syntactic markers, structural patterns, and format-specific signatures to make routing decisions without requiring full parsing.\n\n| Responsibility | Description | Input | Output |\n|----------------|-------------|-------|---------|\n| Content Analysis | Examines file content for format-specific signatures | Raw configuration text | Confidence scores per format |\n| Heuristic Application | Applies weighted heuristics for ambiguous cases | Syntactic patterns | Format identification |\n| Fallback Strategy | Handles cases where format cannot be determined | Analysis results | Default format selection |\n| Error Context | Provides meaningful errors when detection fails | Failed analysis state | Diagnostic information |\n\n**Base Tokenizer Component** provides the foundation for lexical analysis across all formats. Rather than implementing three separate tokenizers, we use inheritance and composition to share common functionality while allowing format-specific extensions. This component handles the universal aspects of tokenization: character stream management, position tracking, and basic token creation.\n\n| Core Method | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `current_position()` | None | `Position` | Returns current line, column, and offset |\n| `peek(offset=0)` | `offset: int` | `str` | Lookahead without consuming characters |\n| `advance()` | None | `str` | Consume character and update position |\n| `skip_whitespace()` | None | `None` | Skip spaces and tabs (not newlines) |\n| `read_string_literal(quote_char)` | `quote_char: str` | `Token` | Parse quoted string with escape processing |\n| `read_number()` | None | `Token` | Parse numeric literal with type inference |\n\n**Format-Specific Parser Components** (INI, TOML, YAML) handle the syntactic analysis phase, converting token streams into structured data. Each parser implements a different parsing strategy optimized for its format's characteristics: line-based parsing for INI, recursive descent for TOML, and indentation-sensitive parsing for YAML.\n\nThe parsers share a common responsibility pattern but implement dramatically different algorithms:\n\n| Parser Type | Strategy | Primary Challenge | Key Data Structure |\n|-------------|----------|-------------------|-------------------|\n| INI Parser | Line-based sequential | Section and key organization | Flat section map |\n| TOML Parser | Recursive descent | Table nesting and type system | Symbol table with dotted keys |\n| YAML Parser | Indentation stack | Block structure and implicit typing | Indentation level stack |\n\n**Error Handling Component** coordinates error detection, enrichment, and reporting across all parsing phases. This component transforms low-level parsing failures into actionable user feedback with context and suggestions. Think of it as a medical diagnostic system that not only identifies symptoms but also suggests treatments.\n\n| Error Type | Detection Phase | Enrichment Strategy | User Impact |\n|------------|----------------|-------------------|-------------|\n| `TokenError` | Lexical analysis | Character-level context | Syntax highlighting |\n| `SyntaxError` | Structural parsing | Grammar rule violations | Format guidance |\n| `StructureError` | Semantic validation | Logical inconsistencies | Schema suggestions |\n\n> **Design Insight**: The key architectural decision is treating tokenization and parsing as separate concerns connected by a well-defined token stream interface. This enables us to swap tokenizers independently of parsers and simplifies testing by allowing isolated validation of lexical analysis separate from structural parsing.\n\n**Architecture Decision: Shared Tokenizer Base vs Format-Specific Tokenizers**\n\n> **Decision: Shared Base Tokenizer with Format-Specific Extensions**\n> - **Context**: Configuration formats share many tokenization patterns (strings, numbers, identifiers) but have format-specific requirements (TOML datetime literals, YAML flow syntax, INI comment styles)\n> - **Options Considered**:\n>   1. Completely separate tokenizers for each format\n>   2. Single universal tokenizer handling all formats\n>   3. Base tokenizer class with format-specific extensions\n> - **Decision**: Base tokenizer class with format-specific extensions\n> - **Rationale**: Maximizes code reuse for common patterns while preserving flexibility for format-specific requirements. Reduces testing burden and maintenance overhead compared to separate tokenizers, while avoiding the complexity explosion of a single universal tokenizer trying to handle all format variations\n> - **Consequences**: Requires careful interface design to support extension points. Creates inheritance relationships that must be managed, but significantly reduces code duplication and provides consistent position tracking and error reporting across formats\n\n**Architecture Decision: Token Stream vs Parse Tree Interface**\n\n> **Decision: Token Stream Interface Between Components**\n> - **Context**: Need to connect tokenizers with parsers while maintaining component independence and testability\n> - **Options Considered**:\n>   1. Direct character stream parsing (no separate tokenization)\n>   2. Full parse tree generation before data structure creation\n>   3. Token stream interface with lazy evaluation\n> - **Decision**: Token stream interface with lazy evaluation\n> - **Rationale**: Token streams provide the right level of abstraction - more structured than character streams but lighter than full parse trees. Enables independent testing of tokenization logic and parser logic. Supports efficient memory usage through lazy token generation\n> - **Consequences**: Requires careful token type design to support all formats. Creates additional interface complexity but enables better separation of concerns and more focused testing\n\n### Recommended Module Structure\n\nThe codebase organization reflects the architectural separation of concerns while promoting code reuse and maintainability. The structure supports independent development of format-specific components while sharing common infrastructure.\n\n```\nconfig_parser/\n├── __init__.py                 ← Main public API and format detection\n├── core/                       ← Shared infrastructure\n│   ├── __init__.py\n│   ├── tokens.py              ← Token types and base tokenizer\n│   ├── errors.py              ← Error hierarchy and context generation\n│   ├── positions.py           ← Position tracking utilities\n│   └── base_parser.py         ← Common parsing utilities\n├── formats/                    ← Format-specific implementations\n│   ├── __init__.py\n│   ├── ini/\n│   │   ├── __init__.py\n│   │   ├── tokenizer.py       ← INI-specific tokenization\n│   │   ├── parser.py          ← INI parsing logic\n│   │   └── validator.py       ← INI-specific validation\n│   ├── toml/\n│   │   ├── __init__.py\n│   │   ├── tokenizer.py       ← TOML tokenization with datetime/multiline\n│   │   ├── parser.py          ← Recursive descent TOML parser\n│   │   ├── tables.py          ← Table and array-of-tables handling\n│   │   └── validator.py       ← TOML semantic validation\n│   └── yaml/\n│       ├── __init__.py\n│       ├── tokenizer.py       ← YAML tokenization with indentation\n│       ├── parser.py          ← Indentation-sensitive parser\n│       ├── flow.py            ← Flow syntax handling\n│       └── validator.py       ← YAML subset validation\n├── utils/                      ← Utility functions\n│   ├── __init__.py\n│   ├── detection.py           ← Format detection heuristics\n│   ├── type_inference.py      ← Automatic type conversion\n│   └── string_utils.py        ← String processing utilities\n└── tests/                      ← Comprehensive test suite\n    ├── __init__.py\n    ├── unit/                  ← Component-level tests\n    ├── integration/           ← Cross-component tests\n    ├── fixtures/              ← Test configuration files\n    └── benchmarks/            ← Performance tests\n```\n\n**Module Responsibility Distribution** ensures each module has a single, well-defined purpose while maintaining clear dependency relationships:\n\n| Module Category | Primary Responsibility | Dependencies | Extension Points |\n|------------------|----------------------|-------------|------------------|\n| `core/` | Shared parsing infrastructure | None (foundation layer) | Token types, error types |\n| `formats/` | Format-specific parsing logic | Depends on `core/` | New format implementations |\n| `utils/` | Cross-cutting utilities | Depends on `core/` | Detection heuristics |\n| Main API | Public interface and orchestration | All modules | Configuration options |\n\n**Architecture Decision: Flat vs Nested Module Structure**\n\n> **Decision: Nested Module Structure with Format Separation**\n> - **Context**: Need to organize format-specific code while maintaining discoverability and avoiding namespace pollution\n> - **Options Considered**:\n>   1. Flat structure with format prefixes (ini_parser.py, toml_parser.py)\n>   2. Single formats.py module with all implementations\n>   3. Nested structure with format-specific packages\n> - **Decision**: Nested structure with format-specific packages\n> - **Rationale**: Each format has multiple related files (tokenizer, parser, validator), making packages natural organizational units. Supports independent development and testing of formats. Easier to add new formats without impacting existing code. Cleaner imports and namespace management\n> - **Consequences**: Slightly more complex import paths but much better organization. Enables format-specific testing and development workflows. Supports future packaging of formats as separate distributions if needed\n\nThe module structure supports several key **extensibility patterns**:\n\n**Format Addition Pattern**: Adding new formats requires only creating a new package under `formats/` with the standard tokenizer/parser/validator structure. The main API automatically discovers new formats through package introspection.\n\n**Component Extension Pattern**: Each core component can be extended through inheritance or composition. For example, adding new token types requires extending the `TokenType` enum and updating format-specific tokenizers to recognize them.\n\n**Utility Integration Pattern**: Cross-cutting concerns like caching, streaming, or validation can be added through the `utils/` package without impacting core parsing logic.\n\n**Dependency Management** follows a strict layered architecture:\n\n```\n┌─────────────────┐\n│   Main API      │ ← Public interface, format detection\n├─────────────────┤\n│   Formats/      │ ← Format-specific parsers and tokenizers\n├─────────────────┤\n│   Utils/        │ ← Cross-cutting utilities and helpers\n├─────────────────┤\n│   Core/         │ ← Foundation: tokens, errors, base classes\n└─────────────────┘\n```\n\nThis layered approach ensures that core components remain stable while format-specific implementations can evolve independently. Dependencies flow downward only - core components never depend on format-specific code, ensuring reusability and testability.\n\n**Architecture Decision: Package-Level vs Module-Level Organization**\n\n> **Decision: Package-Level Organization for Format Implementations**\n> - **Context**: Format implementations require multiple related modules (tokenizer, parser, validator) that should be grouped together\n> - **Options Considered**:\n>   1. All format code in single modules (ini.py, toml.py, yaml.py)\n>   2. Package per format with internal module separation\n>   3. Mixed approach with simple formats as modules, complex as packages\n> - **Decision**: Package per format with internal module separation\n> - **Rationale**: Even simple formats like INI benefit from separation of tokenization and parsing concerns. Consistent structure makes the codebase more predictable. Packages provide natural extension points for format-specific utilities and validation. Supports independent testing strategies per format\n> - **Consequences**: More files and directories but much better organization. Enables format-specific development workflows and testing strategies. Makes it easier to contribute format-specific improvements without understanding other formats\n\n### Implementation Guidance\n\nThe following guidance provides concrete steps for implementing the high-level architecture using Python, focusing on establishing the foundational structure that will support all format-specific implementations.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Tokenization | Manual string parsing with position tracking | Regular expressions with compiled patterns |\n| Error Handling | Basic exception hierarchy with string messages | Rich error objects with context and suggestions |\n| Type Inference | Simple isinstance checks and string conversion | Configurable type system with custom converters |\n| Testing | unittest with manual test data | pytest with property-based testing |\n| Performance | Direct implementation focused on correctness | Caching, lazy evaluation, and memory optimization |\n\n**B. Core Infrastructure Starter Code**\n\n**Position Tracking (`core/positions.py`)**:\n```python\n\"\"\"Position tracking utilities for parsing error reporting.\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass(frozen=True)\nclass Position:\n    \"\"\"Represents a position in the source text with line, column, and offset.\"\"\"\n    line: int\n    column: int\n    offset: int\n    \n    def __str__(self) -> str:\n        return f\"line {self.line}, column {self.column}\"\n\ndef create_error_context(source: str, position: Position, context_lines: int = 2) -> str:\n    \"\"\"Generate visual error context showing the error location in source text.\"\"\"\n    lines = source.split('\\n')\n    if position.line <= 0 or position.line > len(lines):\n        return \"Invalid position\"\n    \n    # Calculate context window\n    start_line = max(0, position.line - context_lines - 1)\n    end_line = min(len(lines), position.line + context_lines)\n    \n    context_parts = []\n    for i in range(start_line, end_line):\n        line_num = i + 1\n        line_content = lines[i]\n        prefix = \">>> \" if line_num == position.line else \"    \"\n        context_parts.append(f\"{prefix}{line_num:4d} | {line_content}\")\n        \n        # Add pointer line for error position\n        if line_num == position.line:\n            pointer = \" \" * (len(prefix) + 7 + position.column - 1) + \"^\"\n            context_parts.append(pointer)\n    \n    return \"\\n\".join(context_parts)\n```\n\n**Error Hierarchy (`core/errors.py`)**:\n```python\n\"\"\"Error types for configuration parsing with context and suggestions.\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass ParseError(Exception):\n    \"\"\"Base class for all parsing errors with position and suggestion support.\"\"\"\n    message: str\n    position: Optional[Position] = None\n    suggestion: Optional[str] = None\n    \n    def __str__(self) -> str:\n        parts = [self.message]\n        if self.position:\n            parts.append(f\" at {self.position}\")\n        if self.suggestion:\n            parts.append(f\"\\nSuggestion: {self.suggestion}\")\n        return \"\".join(parts)\n\n@dataclass\nclass TokenError(ParseError):\n    \"\"\"Errors during lexical analysis (tokenization).\"\"\"\n    pass\n\n@dataclass\nclass SyntaxError(ParseError):\n    \"\"\"Errors during syntactic analysis (parsing structure).\"\"\"\n    pass\n\n@dataclass\nclass StructureError(ParseError):\n    \"\"\"Errors in logical structure (semantic validation).\"\"\"\n    pass\n```\n\n**Token System (`core/tokens.py`)**:\n```python\n\"\"\"Token definitions and base tokenizer for all configuration formats.\"\"\"\n\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Any, List, Optional\nfrom .positions import Position\n\nclass TokenType(Enum):\n    \"\"\"All token types needed across INI, TOML, and YAML formats.\"\"\"\n    # Literal values\n    STRING = auto()\n    NUMBER = auto() \n    BOOLEAN = auto()\n    IDENTIFIER = auto()\n    \n    # Operators and punctuation\n    EQUALS = auto()         # =\n    COLON = auto()          # :\n    COMMA = auto()          # ,\n    DOT = auto()            # .\n    \n    # Structure markers\n    SECTION_START = auto()  # [\n    SECTION_END = auto()    # ]\n    ARRAY_START = auto()    # [ (in value context)\n    ARRAY_END = auto()      # ] (in value context)\n    OBJECT_START = auto()   # {\n    OBJECT_END = auto()     # }\n    \n    # Whitespace and control\n    NEWLINE = auto()\n    INDENT = auto()         # YAML indentation increase\n    DEDENT = auto()         # YAML indentation decrease\n    BLOCK_SEQUENCE = auto() # YAML list item marker (-)\n    \n    # Special tokens\n    COMMENT = auto()\n    EOF = auto()\n    INVALID = auto()\n\n@dataclass\nclass Token:\n    \"\"\"A single token with type, value, position, and original text.\"\"\"\n    type: TokenType\n    value: Any\n    position: Position\n    raw_text: str\n\nclass BaseTokenizer:\n    \"\"\"Foundation tokenizer providing common functionality for all formats.\"\"\"\n    \n    def __init__(self, source: str):\n        self.source = source\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n    \n    def current_position(self) -> Position:\n        \"\"\"Returns current parsing position.\"\"\"\n        # TODO: Return Position object with current line, column, offset\n        pass\n    \n    def peek(self, offset: int = 0) -> str:\n        \"\"\"Lookahead without consuming characters.\"\"\"\n        # TODO: Return character at position + offset, or EOF_MARKER if beyond end\n        # TODO: Handle negative offsets for lookbehind\n        pass\n    \n    def advance(self) -> str:\n        \"\"\"Consume character and update position tracking.\"\"\"\n        # TODO: Get current character before advancing\n        # TODO: Update self.position, handle newline for line/column tracking\n        # TODO: Return consumed character or EOF_MARKER\n        pass\n    \n    def skip_whitespace(self) -> None:\n        \"\"\"Skip spaces and tabs but preserve newlines for structure.\"\"\"\n        # TODO: Advance while current character is space or tab\n        # TODO: Do NOT skip newlines - they're structurally significant\n        pass\n    \n    def read_string_literal(self, quote_char: str) -> Token:\n        \"\"\"Parse quoted string with escape sequence processing.\"\"\"\n        # TODO: Advance past opening quote\n        # TODO: Build string value while processing escape sequences\n        # TODO: Handle \\n, \\t, \\r, \\\\, \\\", \\' escape sequences  \n        # TODO: Detect unterminated strings and create appropriate error\n        # TODO: Return STRING token with processed value\n        pass\n    \n    def read_number(self) -> Token:\n        \"\"\"Parse numeric literal with automatic type inference.\"\"\"\n        # TODO: Collect digits, handle decimal points for floats\n        # TODO: Handle scientific notation (1e5, 1E-3)\n        # TODO: Handle integer underscores in TOML (1_000_000)\n        # TODO: Convert to appropriate Python type (int, float)\n        # TODO: Return NUMBER token with converted value\n        pass\n    \n    def tokenize(self) -> List[Token]:\n        \"\"\"Main tokenization entry point - implemented by format-specific subclasses.\"\"\"\n        # TODO: This is the main loop - format-specific tokenizers override this\n        # TODO: Call appropriate read_* methods based on current character\n        # TODO: Handle format-specific token types and syntax\n        # TODO: Return complete list of tokens including final EOF\n        raise NotImplementedError(\"Subclasses must implement tokenize()\")\n\n# Useful constants\nEOF_MARKER = '\\0'\n```\n\n**C. Main API Structure (`__init__.py`)**:\n```python\n\"\"\"Main configuration parser API with automatic format detection.\"\"\"\n\nfrom typing import Optional, Dict, Any\nfrom .utils.detection import detect_format\nfrom .formats.ini.parser import INIParser\nfrom .formats.toml.parser import TOMLParser  \nfrom .formats.yaml.parser import YAMLParser\n\ndef parse_config(content: str, format: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Main configuration parsing entry point with automatic format detection.\n    \n    Args:\n        content: Raw configuration file content\n        format: Optional format override ('ini', 'toml', 'yaml')\n    \n    Returns:\n        Nested dictionary representing the configuration structure\n        \n    Raises:\n        ParseError: When content cannot be parsed in the specified/detected format\n    \"\"\"\n    # TODO: If format not specified, call detect_format(content)\n    # TODO: Create appropriate parser instance based on format\n    # TODO: Call parser.parse(content) and return result\n    # TODO: Wrap parser-specific errors in generic ParseError with context\n    pass\n\n# Export main API\n__all__ = ['parse_config', 'detect_format', 'ParseError', 'TokenError', 'SyntaxError', 'StructureError']\n```\n\n**D. Format Detection Starter (`utils/detection.py`)**:\n```python\n\"\"\"Automatic configuration format detection using content analysis.\"\"\"\n\nimport re\nfrom typing import Dict\n\ndef detect_format(content: str) -> str:\n    \"\"\"Detect configuration format from content using weighted heuristics.\n    \n    Returns:\n        Format string: 'ini', 'toml', or 'yaml'\n    \"\"\"\n    scores = {\n        'ini': _score_ini(content),\n        'toml': _score_toml(content), \n        'yaml': _score_yaml(content)\n    }\n    \n    # Return format with highest confidence score\n    return max(scores.keys(), key=lambda k: scores[k])\n\ndef _score_ini(content: str) -> float:\n    \"\"\"Calculate confidence score for INI format.\"\"\"\n    score = 0.0\n    # TODO: Look for [section] headers - strong INI indicator\n    # TODO: Look for key=value pairs - moderate indicator\n    # TODO: Look for ; comments - moderate INI indicator\n    # TODO: Penalize TOML-specific syntax like [[array]]\n    # TODO: Penalize YAML-specific syntax like list items with -\n    return score\n\ndef _score_toml(content: str) -> float:\n    \"\"\"Calculate confidence score for TOML format.\"\"\"\n    score = 0.0  \n    # TODO: Look for [[array.of.tables]] - very strong TOML indicator\n    # TODO: Look for dotted.keys = value - strong TOML indicator\n    # TODO: Look for inline tables { key = value } - strong indicator\n    # TODO: Look for TOML datetime formats - moderate indicator\n    # TODO: Penalize YAML indentation patterns\n    return score\n\ndef _score_yaml(content: str) -> float:\n    \"\"\"Calculate confidence score for YAML format.\"\"\"  \n    score = 0.0\n    # TODO: Analyze indentation patterns - strong YAML indicator\n    # TODO: Look for list items with - prefix - strong indicator  \n    # TODO: Look for key: value with colon - moderate indicator\n    # TODO: Look for flow syntax [list] or {map} - moderate indicator\n    # TODO: Penalize INI [sections] and TOML [[arrays]]\n    return score\n```\n\n**E. Development Workflow Recommendations**\n\n**Phase 1 - Foundation Setup**:\n1. Implement the core infrastructure (positions, errors, tokens) completely\n2. Create empty format packages with proper `__init__.py` files\n3. Implement basic format detection with simple heuristics\n4. Set up comprehensive testing structure with fixtures\n\n**Phase 2 - Format Implementation Order**:\n1. Start with INI parser (simplest format, builds confidence)\n2. Move to TOML tokenizer (introduces complex tokenization concepts)  \n3. Implement TOML parser (most complex parsing logic)\n4. Finish with YAML parser (different paradigm, indentation-sensitive)\n\n**Phase 3 - Integration and Polish**:\n1. Connect all formats through main API\n2. Enhance format detection with real-world test cases\n3. Add comprehensive error handling with context\n4. Performance optimization and edge case handling\n\n**F. Common Architecture Pitfalls**\n\n⚠️ **Pitfall: Mixing Tokenization and Parsing Logic**\nMany implementations blur the line between lexical and syntactic analysis, making both components harder to test and debug. Keep tokenizers focused purely on character-to-token conversion without understanding structural meaning.\n\n⚠️ **Pitfall: Inconsistent Error Position Tracking**\nFailing to maintain accurate position information throughout the parsing pipeline makes debugging nearly impossible. Every token must carry position information, and every error must reference the relevant position.\n\n⚠️ **Pitfall: Format-Specific Code in Shared Components**\nPutting TOML-specific logic in the base tokenizer or YAML-specific handling in error reporting breaks the architectural separation. Keep shared components truly format-agnostic.\n\n⚠️ **Pitfall: Inadequate Interface Design**\nDesigning token types or error interfaces that work for one format but need special cases for others indicates insufficient upfront analysis. Design interfaces to handle the union of all format requirements from the beginning.\n\n\n## Data Model\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - defines the core data structures used throughout all parsing implementations\n\nThe data model forms the foundation of our multi-format configuration parser, defining how we represent information as it flows through the parsing pipeline. Think of the data model as the common language that all components speak - just as different human languages can express the same concepts using different words and grammar, our INI, TOML, and YAML parsers must ultimately produce the same structured data despite processing fundamentally different syntactic approaches.\n\nThe data model consists of three primary layers, each serving a distinct purpose in the parsing pipeline. The **token layer** represents the lowest-level meaningful units extracted from raw text - individual symbols, keywords, literals, and operators that carry semantic meaning. The **parse tree layer** provides an intermediate structural representation that captures the syntactic relationships between tokens while remaining close to the original format's grammar. Finally, the **unified output layer** presents a consistent nested dictionary structure that client code can work with regardless of which configuration format was parsed.\n\nThis layered approach addresses one of the core challenges in multi-format parsing: the **impedance mismatch** between human-readable configuration syntax and machine-processable data structures. Each format has evolved different conventions for expressing hierarchy, data types, and structural relationships. INI files use section headers with flat key-value pairs, TOML employs table definitions with dotted key paths, and YAML relies on indentation-sensitive block structures. Our data model must bridge these syntactic differences while preserving the semantic intent of each format.\n\n![Token Type Hierarchy](./diagrams/token-types.svg)\n\n### Token Type Definitions\n\nThe tokenization layer transforms character streams into typed tokens that carry both lexical information and positional context. Understanding tokenization requires thinking of it as **pattern recognition with state tracking** - the tokenizer examines character sequences and classifies them into meaningful categories while maintaining awareness of where each token originated in the source text.\n\nEvery token in our system carries four essential pieces of information: its semantic type (what kind of language construct it represents), its processed value (the meaningful content after handling escapes and type conversion), its source position (for error reporting), and its raw text (the exact character sequence from the original input). This comprehensive token representation enables sophisticated error reporting and supports advanced features like syntax highlighting or source-to-output mapping.\n\nThe `TokenType` enumeration defines all possible token categories needed across INI, TOML, and YAML formats. This unified token vocabulary allows us to share tokenization logic where formats overlap while maintaining format-specific token types for unique constructs.\n\n| Token Type | Description | Example Raw Text | Processed Value | Used By |\n|------------|-------------|------------------|-----------------|---------|\n| `STRING` | Quoted or unquoted string literal | `\"hello\\nworld\"` | `hello\\nworld` | All formats |\n| `NUMBER` | Numeric literal (integer or float) | `42`, `3.14`, `1_000` | `42`, `3.14`, `1000` | TOML, YAML |\n| `BOOLEAN` | Boolean literal | `true`, `false`, `yes`, `no` | `True`, `False` | TOML, YAML |\n| `IDENTIFIER` | Unquoted name or key | `hostname`, `database` | `hostname`, `database` | All formats |\n| `EQUALS` | Assignment operator | `=` | `=` | INI, TOML |\n| `COLON` | Key-value separator | `:` | `:` | INI, YAML |\n| `NEWLINE` | Line terminator | `\\n`, `\\r\\n` | `\\n` | All formats |\n| `EOF` | End of file marker | (none) | `EOF_MARKER` | All formats |\n| `COMMENT` | Comment text | `# This is a comment` | `This is a comment` | All formats |\n| `SECTION_START` | INI section opening | `[` | `[` | INI |\n| `SECTION_END` | INI section closing | `]` | `]` | INI |\n| `ARRAY_START` | Array opening bracket | `[` | `[` | TOML, YAML |\n| `ARRAY_END` | Array closing bracket | `]` | `]` | TOML, YAML |\n| `OBJECT_START` | Object opening brace | `{` | `{` | TOML, YAML |\n| `OBJECT_END` | Object closing brace | `}` | `}` | TOML, YAML |\n| `COMMA` | Element separator | `,` | `,` | TOML, YAML |\n| `DOT` | Dotted key separator | `.` | `.` | TOML |\n| `INDENT` | Increased indentation level | (whitespace) | indent level | YAML |\n| `DEDENT` | Decreased indentation level | (whitespace) | dedent count | YAML |\n| `BLOCK_SEQUENCE` | YAML sequence marker | `-` | `-` | YAML |\n| `INVALID` | Malformed or unrecognized token | `@#$%` | (original text) | All formats |\n\nThe `Position` structure tracks location information for every token, enabling precise error reporting and source mapping. Position tracking must handle the complexities of different line ending conventions (Unix `\\n`, Windows `\\r\\n`, classic Mac `\\r`) while maintaining accurate column counting in the presence of tab characters and Unicode code points.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `line` | `int` | Line number (1-based) in source text |\n| `column` | `int` | Column number (1-based) in source line |\n| `offset` | `int` | Absolute character offset (0-based) from start of input |\n\nThe `Token` structure represents individual lexical units with their complete context. The separation between `value` and `raw_text` enables clean processing of escape sequences, type conversion, and normalization while preserving the original source text for error reporting and debugging.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `type` | `TokenType` | Semantic category of this token |\n| `value` | `Any` | Processed value after escape handling and type conversion |\n| `position` | `Position` | Source location where this token originated |\n| `raw_text` | `str` | Exact character sequence from input (before processing) |\n\n> **Design Insight**: The distinction between `value` and `raw_text` in tokens is crucial for debugging and error reporting. When a user writes `\"hello\\nworld\"` in their configuration file, they intend the string value `hello\\nworld` (with a literal newline), but error messages should reference the original `\"hello\\nworld\"` text they actually typed.\n\n**Context sensitivity** presents one of the most significant challenges in tokenization across multiple formats. The same character sequence can represent different token types depending on its syntactic context. For example, the character `[` represents a section header in INI files, an array literal in TOML expressions, a table array definition in TOML headers, and a flow sequence in YAML. Our tokenizer design must either maintain sufficient context to disambiguate these cases or defer disambiguation to the parsing layer.\n\n> **Decision: Context-Sensitive vs Context-Free Tokenization**\n> - **Context**: Different formats assign different meanings to the same characters depending on syntactic position\n> - **Options Considered**: \n>   1. Context-sensitive tokenizer that tracks parsing state and emits different tokens based on context\n>   2. Context-free tokenizer that emits generic tokens and lets parsers handle disambiguation\n>   3. Format-specific tokenizers with no shared token vocabulary\n> - **Decision**: Context-free tokenizer with parser-level disambiguation\n> - **Rationale**: Context-free tokenization is simpler to implement, test, and debug. Parser-level disambiguation allows each format parser to apply its own interpretation rules without complex tokenizer state management.\n> - **Consequences**: Parsers must handle some ambiguous tokens, but tokenization logic remains clean and format-agnostic where possible.\n\n**Lexical ambiguity** occurs when the same character sequence could represent multiple token types even within a single format. TOML demonstrates this complexity with its multiple string literal syntaxes: basic strings (`\"hello\"`), literal strings (`'hello'`), multi-line basic strings (`\"\"\"hello\"\"\"`), and multi-line literal strings (`'''hello'''`). Each syntax has different rules for escape sequence processing and line handling.\n\nString literal tokenization requires careful state machine implementation to handle nested quotes, escape sequences, and multi-line continuation. The tokenizer must track whether it's inside a quoted string, which quote character initiated the string, whether escape processing is active, and how to handle embedded newlines.\n\n| String Type | Quote Style | Escape Processing | Multi-line Support | Example |\n|-------------|-------------|-------------------|-------------------|---------|\n| TOML Basic | `\"...\"` | Yes (backslash escapes) | Single line only | `\"hello\\nworld\"` |\n| TOML Literal | `'...'` | No (raw text) | Single line only | `'C:\\Users\\name'` |\n| TOML Multi-Basic | `\"\"\"...\"\"\"` | Yes (backslash escapes) | Yes (preserve newlines) | `\"\"\"line 1\\nline 2\"\"\"` |\n| TOML Multi-Literal | `'''...'''` | No (raw text) | Yes (preserve newlines) | `'''raw\\text\\here'''` |\n| YAML Double | `\"...\"` | Yes (limited escape set) | With continuation | `\"folded\\nstring\"` |\n| YAML Single | `'...'` | Limited (only `''` → `'`) | With continuation | `'don''t escape'` |\n| INI Quoted | `\"...\"` or `'...'` | Format-dependent | Usually single line | `\"value with spaces\"` |\n\n**Type inference** during tokenization attempts to classify numeric and boolean literals without requiring explicit type annotations. This automatic classification simplifies the user experience but introduces complexity in handling edge cases and format-specific type systems.\n\nNumeric type inference must distinguish between integers, floating-point numbers, and special numeric formats like TOML's integer underscores (`1_000_000`) or YAML's sexagesimal numbers (`90:30:15`). Boolean inference varies significantly between formats - YAML recognizes `yes`, `no`, `on`, `off`, `true`, and `false`, while TOML only accepts `true` and `false`.\n\n> **Decision: Tokenization-Level vs Parse-Level Type Inference**\n> - **Context**: Determining whether `\"42\"`, `42`, `true`, and `\"true\"` represent different data types\n> - **Options Considered**:\n>   1. Tokenizer performs type inference and emits strongly-typed tokens\n>   2. Tokenizer emits raw values and parsers handle type inference\n>   3. No type inference - everything remains as strings until explicit conversion\n> - **Decision**: Tokenizer performs basic type inference for clearly-typed literals\n> - **Rationale**: Type information at the token level simplifies parser logic and enables better error messages. Ambiguous cases can still be deferred to parsers.\n> - **Consequences**: Tokenizer becomes slightly more complex but parsers can focus on structural analysis rather than type classification.\n\n![Data Structure Transformation](./diagrams/data-structure-mapping.svg)\n\n### Parse Tree Structure\n\nThe parse tree provides an intermediate structural representation that captures the hierarchical relationships between tokens while remaining faithful to each format's grammatical structure. Think of the parse tree as a **syntax-aware scaffolding** that organizes tokens according to their grammatical roles before transforming them into the final unified data structure.\n\nParse trees serve several critical functions in our architecture. They provide a stable interface between format-specific parsing logic and the unified output generation. They enable sophisticated error recovery by maintaining partial structural information even when parsing fails. They support advanced features like preserving comments, maintaining key ordering, or implementing syntax-aware editing operations.\n\nThe parse tree structure must accommodate the diverse grammatical approaches of our target formats while providing sufficient commonality for shared processing logic. INI files have a simple two-level hierarchy of sections containing key-value pairs. TOML supports arbitrary nesting through dotted keys and table definitions, with complex rules for table redefinition and array-of-tables. YAML uses indentation-sensitive block structures with implicit type inference and multiple syntactic styles for the same logical constructs.\n\n> **Decision: Format-Specific vs Unified Parse Tree Structure**\n> - **Context**: Balancing the need for format-faithful representation with shared processing logic\n> - **Options Considered**:\n>   1. Single unified parse tree structure that all formats must map to\n>   2. Format-specific parse tree structures with conversion to unified output\n>   3. Hybrid approach with common base structures and format-specific extensions\n> - **Decision**: Hybrid approach with common base node types and format-specific specializations\n> - **Rationale**: Enables shared processing logic for common constructs while allowing format-specific optimizations and features\n> - **Consequences**: Moderate complexity increase but maximum flexibility for format-specific requirements\n\nThe base parse tree node structure provides common functionality needed across all formats. Every node tracks its source position for error reporting, maintains references to its constituent tokens for source mapping, and provides a consistent interface for tree traversal and manipulation.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `node_type` | `str` | Discriminator indicating specific node subtype |\n| `position` | `Position` | Source location of this syntactic construct |\n| `tokens` | `List[Token]` | All tokens that contribute to this node |\n| `children` | `List[ParseNode]` | Child nodes in syntax tree |\n| `metadata` | `dict` | Format-specific annotations and processing hints |\n\n**Section nodes** represent logical groupings of configuration data. In INI files, sections correspond directly to `[section_name]` headers. In TOML, sections represent table definitions including nested tables from dotted keys and array-of-tables. YAML doesn't have explicit sections, but top-level mapping keys function similarly.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `section_name` | `str` | Fully-qualified section identifier |\n| `key_path` | `List[str]` | Hierarchical path components for nested sections |\n| `is_array_element` | `bool` | True if this section represents an array-of-tables entry |\n| `key_value_pairs` | `List[KeyValueNode]` | Direct key-value assignments within this section |\n\n**Key-value nodes** represent individual configuration assignments. These nodes must handle the variety of assignment operators (`=`, `:`), value types, and structural patterns used across formats.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `key` | `str` | Configuration parameter name |\n| `key_path` | `List[str]` | Dotted key components for nested assignment |\n| `value` | `ValueNode` | Assigned value (scalar, array, or nested structure) |\n| `assignment_operator` | `str` | Operator used for assignment (`=`, `:`) |\n| `inline_comment` | `Optional[str]` | Comment appearing on same line as assignment |\n\n**Value nodes** represent the diverse data types and structures supported across configuration formats. Value nodes must accommodate scalars (strings, numbers, booleans), collections (arrays, objects), and format-specific constructs like TOML inline tables or YAML flow sequences.\n\n| Value Node Type | Description | Example Source | Parsed Structure |\n|-----------------|-------------|----------------|------------------|\n| `ScalarValue` | Single atomic value | `hostname = \"server1\"` | `{\"type\": \"scalar\", \"value\": \"server1\"}` |\n| `ArrayValue` | Ordered list of values | `ports = [80, 443, 8080]` | `{\"type\": \"array\", \"elements\": [80, 443, 8080]}` |\n| `ObjectValue` | Key-value mapping | `{name = \"test\", port = 80}` | `{\"type\": \"object\", \"fields\": {...}}` |\n| `NestedValue` | Reference to nested section | `database.connection.host` | `{\"type\": \"nested\", \"path\": [\"database\", \"connection\", \"host\"]}` |\n\n**Comment nodes** preserve documentation and annotations from the source configuration. Comment handling varies significantly between formats - INI and TOML support both line comments and inline comments, while YAML has more complex comment association rules.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `comment_text` | `str` | Comment content (without comment markers) |\n| `comment_style` | `str` | Comment syntax used (`#`, `;`, `//`) |\n| `attachment` | `str` | How comment relates to nearby content (`above`, `inline`, `below`) |\n| `associated_node` | `Optional[ParseNode]` | Parse node this comment documents |\n\n**Array-of-tables nodes** handle TOML's unique `[[table.name]]` syntax for creating arrays of structured objects. This construct has no direct equivalent in INI or YAML, requiring specialized handling in the parse tree.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `table_path` | `List[str]` | Hierarchical path to the array location |\n| `entries` | `List[SectionNode]` | Individual table entries in the array |\n| `entry_order` | `List[int]` | Explicit ordering for array elements |\n\nThe parse tree construction process varies significantly between formats due to their different grammatical approaches. **INI parsing** follows a simple line-oriented algorithm: scan for section headers, collect key-value pairs until the next section, handle comments and continuation lines. **TOML parsing** requires recursive descent techniques to handle nested table definitions, dotted key expansion, and the complex interaction between explicit tables and implicit tables created by dotted keys. **YAML parsing** uses an indentation-sensitive algorithm with a stack-based approach to track nesting levels and handle the transition between different indentation depths.\n\n> **Design Insight**: Parse trees serve as a crucial debugging tool during implementation. When parsing produces incorrect output, examining the parse tree structure often reveals whether the problem lies in tokenization (wrong tokens), parsing logic (incorrect tree structure), or output conversion (correct tree, wrong transformation).\n\n### Unified Output Format\n\nThe unified output format provides a consistent nested dictionary structure that client applications can work with regardless of which configuration format was originally parsed. Think of the unified output as a **format-agnostic data contract** - it abstracts away the syntactic differences between INI, TOML, and YAML while preserving the semantic intent of the original configuration.\n\nThe unified format addresses one of the core value propositions of our multi-format parser: enabling applications to support multiple configuration formats without implementing format-specific processing logic. A web application can read database connection parameters from `config.ini`, `config.toml`, or `config.yaml` using identical code, with the parser handling the format detection and conversion automatically.\n\nThe output structure consists of nested Python dictionaries with string keys and mixed-type values. This representation aligns with JSON semantics while supporting additional data types like datetime objects, large integers, and explicit type annotations that some configuration formats provide.\n\n> **Decision: Output Data Structure Design**\n> - **Context**: Choosing the unified representation that balances simplicity, type safety, and format compatibility\n> - **Options Considered**:\n>   1. Pure JSON-compatible structure (strings, numbers, booleans, arrays, objects only)\n>   2. Python-native structure with full type system support (datetime, decimal, custom types)\n>   3. Hybrid structure with JSON compatibility and optional type annotations\n> - **Decision**: Python-native structure with rich type support\n> - **Rationale**: Configuration files often contain dates, file paths, and numeric values that benefit from proper typing. Applications can serialize to JSON if needed.\n> - **Consequences**: Output is more useful for Python applications but requires explicit conversion for JSON export.\n\n**Nested structure mapping** transforms the various hierarchical representations used by different formats into a consistent nested dictionary structure. INI sections become top-level dictionary keys, TOML tables and dotted keys create nested dictionaries, and YAML block structures map directly to nested dictionaries and arrays.\n\n| Configuration Source | Unified Output Structure |\n|---------------------|--------------------------|\n| INI: `[database]` `host=localhost` | `{\"database\": {\"host\": \"localhost\"}}` |\n| TOML: `database.host = \"localhost\"` | `{\"database\": {\"host\": \"localhost\"}}` |\n| YAML: `database:` `  host: localhost` | `{\"database\": {\"host\": \"localhost\"}}` |\n\n**Type preservation** maintains the semantic intent of typed literals while providing consistent behavior across formats. Numbers remain as integers or floats, booleans are represented as Python `True`/`False`, dates and times use Python `datetime` objects, and strings preserve their exact content including Unicode characters.\n\nThe type mapping process must handle format-specific type systems while producing consistent output. TOML has an explicit type system with integers, floats, booleans, strings, arrays, and tables. YAML performs implicit type inference that can produce surprising results (`yes` becomes `True`, `1.0` becomes a float, `010` might become octal). INI files typically treat everything as strings unless explicit conversion is applied.\n\n| Source Format | Type System | Example Literal | Unified Output Type | Unified Output Value |\n|---------------|-------------|-----------------|-------------------|---------------------|\n| TOML | Explicit typing | `port = 8080` | `int` | `8080` |\n| YAML | Implicit inference | `port: 8080` | `int` | `8080` |\n| INI | String-based | `port=8080` | `str` or `int` | `\"8080\"` or `8080` |\n| TOML | Boolean literals | `enabled = true` | `bool` | `True` |\n| YAML | Boolean inference | `enabled: yes` | `bool` | `True` |\n| INI | String representation | `enabled=true` | `str` or `bool` | `\"true\"` or `True` |\n\n**Array handling** unifies the different approaches formats use for representing ordered collections. TOML uses explicit array syntax with square brackets: `ports = [80, 443, 8080]`. YAML supports both flow syntax `[80, 443, 8080]` and block syntax with dash-prefixed items. INI files don't have native array support, but our parser can recognize comma-separated values or repeated keys as arrays.\n\n| Format | Array Syntax | Source Example | Unified Output |\n|--------|-------------|----------------|----------------|\n| TOML | Square brackets | `ports = [80, 443]` | `{\"ports\": [80, 443]}` |\n| YAML | Flow sequence | `ports: [80, 443]` | `{\"ports\": [80, 443]}` |\n| YAML | Block sequence | `ports:` `- 80` `- 443` | `{\"ports\": [80, 443]}` |\n| INI | Comma-separated | `ports=80,443` | `{\"ports\": [80, 443]}` |\n| INI | Repeated keys | `port=80` `port=443` | `{\"ports\": [80, 443]}` |\n\n**Object nesting** creates hierarchical dictionary structures from the various nesting mechanisms used by different formats. The unified output uses string keys and supports arbitrary nesting depth limited only by Python's recursion limits.\n\nTOML dotted keys like `database.connection.host = \"localhost\"` create nested dictionaries: `{\"database\": {\"connection\": {\"host\": \"localhost\"}}}`. YAML indentation-based nesting maps directly to nested dictionaries. INI section names can be interpreted as namespace separators, so `[database.connection]` creates similar nesting.\n\n**Key normalization** handles differences in identifier conventions between formats. Some formats are case-sensitive while others are case-insensitive. Key names might use different conventions (camelCase, snake_case, kebab-case) even within the same configuration file.\n\n| Normalization Strategy | Description | Example Transformation | Use Case |\n|----------------------|-------------|----------------------|----------|\n| `preserve` | Maintain exact key names from source | `userName` → `userName` | Format-aware applications |\n| `lowercase` | Convert all keys to lowercase | `userName` → `username` | Case-insensitive lookup |\n| `snake_case` | Convert to underscore convention | `userName` → `user_name` | Python applications |\n| `kebab_case` | Convert to dash convention | `userName` → `user-name` | Configuration standards |\n\n**Error information preservation** maintains connection between the unified output and the original source text for debugging and error reporting. When possible, the unified output includes metadata about source locations, original formatting, and any type conversions that were applied.\n\nThe output structure supports optional metadata attachment through a special `__metadata__` key that contains position information, original formatting, comments, and conversion notes. This metadata enables advanced features like round-trip conversion, syntax-aware editing, and precise error reporting.\n\n| Metadata Field | Type | Description | Example |\n|----------------|------|-------------|---------|\n| `source_format` | `str` | Original configuration format | `\"toml\"`, `\"yaml\"`, `\"ini\"` |\n| `source_position` | `Position` | Location in original file | `{line: 15, column: 8}` |\n| `original_key` | `str` | Key name before normalization | `\"userName\"` → normalized `\"user_name\"` |\n| `type_conversion` | `str` | Applied type conversion | `\"string_to_int\"`, `\"implicit_boolean\"` |\n| `comments` | `List[str]` | Associated comments | `[\"# Database configuration\", \"# Updated 2024-01-15\"]` |\n\nThe unified output format serves as the foundation for advanced features like configuration validation, schema enforcement, and cross-format conversion. Applications can implement configuration schemas that work across multiple input formats, validate required fields and value ranges, and provide meaningful error messages that reference the original source text.\n\n### Implementation Guidance\n\nThe data model implementation requires careful attention to type safety, memory efficiency, and extensibility. Python's dynamic typing system provides flexibility for handling mixed-type configuration values while its dataclass and enum features enable clean, self-documenting data structures.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Token Types | Python Enum | Custom classes with validation |\n| Position Tracking | Named tuple | Dataclass with methods |\n| Parse Trees | Dictionary-based | Custom node classes with inheritance |\n| Type Conversion | Built-in functions | Custom type system with validation |\n| Error Handling | Exception classes | Rich error objects with context |\n\n**Recommended File Structure:**\n```\nconfig_parser/\n  data_model/\n    __init__.py              ← export main classes\n    tokens.py                ← TokenType, Token, Position\n    parse_tree.py            ← Parse tree node classes\n    output.py                ← Unified output utilities\n    errors.py                ← Error classes and context\n    types.py                 ← Type conversion utilities\n  tests/\n    test_tokens.py           ← Token handling tests\n    test_parse_tree.py       ← Parse tree construction tests\n    test_output.py           ← Output format tests\n```\n\n**Core Token Infrastructure (Complete Implementation):**\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, List, Union\n\nclass TokenType(Enum):\n    \"\"\"Token types for all supported configuration formats.\"\"\"\n    STRING = auto()\n    NUMBER = auto()\n    BOOLEAN = auto()\n    IDENTIFIER = auto()\n    EQUALS = auto()\n    COLON = auto()\n    NEWLINE = auto()\n    EOF = auto()\n    COMMENT = auto()\n    SECTION_START = auto()\n    SECTION_END = auto()\n    ARRAY_START = auto()\n    ARRAY_END = auto()\n    OBJECT_START = auto()\n    OBJECT_END = auto()\n    COMMA = auto()\n    DOT = auto()\n    INDENT = auto()\n    DEDENT = auto()\n    BLOCK_SEQUENCE = auto()\n    INVALID = auto()\n\n@dataclass(frozen=True)\nclass Position:\n    \"\"\"Source position for error reporting and debugging.\"\"\"\n    line: int          # 1-based line number\n    column: int        # 1-based column number  \n    offset: int        # 0-based absolute offset\n    \n    def __str__(self) -> str:\n        return f\"line {self.line}, column {self.column}\"\n\n@dataclass(frozen=True)\nclass Token:\n    \"\"\"Individual lexical unit with complete context.\"\"\"\n    type: TokenType    # Semantic category\n    value: Any         # Processed value (after escapes, conversion)\n    position: Position # Source location\n    raw_text: str      # Original character sequence\n    \n    def __str__(self) -> str:\n        return f\"{self.type.name}({self.value!r}) at {self.position}\"\n\n# Constants\nEOF_MARKER = '\\0'\n```\n\n**Parse Tree Node Infrastructure (Complete Implementation):**\n```python\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional, Union\nfrom abc import ABC, abstractmethod\n\n@dataclass\nclass ParseNode(ABC):\n    \"\"\"Base class for all parse tree nodes.\"\"\"\n    node_type: str\n    position: Position\n    tokens: List[Token] = field(default_factory=list)\n    children: List['ParseNode'] = field(default_factory=list)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    @abstractmethod\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert this node to unified output format.\"\"\"\n        pass\n\n@dataclass\nclass SectionNode(ParseNode):\n    \"\"\"Configuration section with key-value pairs.\"\"\"\n    section_name: str = \"\"\n    key_path: List[str] = field(default_factory=list)\n    is_array_element: bool = False\n    key_value_pairs: List['KeyValueNode'] = field(default_factory=list)\n    \n    def __post_init__(self):\n        self.node_type = \"section\"\n\n@dataclass  \nclass KeyValueNode(ParseNode):\n    \"\"\"Individual configuration assignment.\"\"\"\n    key: str = \"\"\n    key_path: List[str] = field(default_factory=list)\n    value: Optional['ValueNode'] = None\n    assignment_operator: str = \"=\"\n    inline_comment: Optional[str] = None\n    \n    def __post_init__(self):\n        self.node_type = \"key_value\"\n\n@dataclass\nclass ValueNode(ParseNode):\n    \"\"\"Configuration value of various types.\"\"\"\n    value_type: str = \"scalar\"  # scalar, array, object, nested\n    processed_value: Any = None\n    \n    def __post_init__(self):\n        self.node_type = \"value\"\n```\n\n**Error Handling Infrastructure (Complete Implementation):**\n```python\nfrom typing import Optional, List\n\nclass ParseError(Exception):\n    \"\"\"Base class for all parsing errors.\"\"\"\n    def __init__(self, message: str, position: Optional[Position] = None, \n                 suggestion: Optional[str] = None):\n        self.message = message\n        self.position = position  \n        self.suggestion = suggestion\n        super().__init__(self.format_error())\n    \n    def format_error(self) -> str:\n        \"\"\"Format error with position and suggestion.\"\"\"\n        result = self.message\n        if self.position:\n            result = f\"{result} at {self.position}\"\n        if self.suggestion:\n            result = f\"{result}\\nSuggestion: {self.suggestion}\"\n        return result\n\nclass TokenError(ParseError):\n    \"\"\"Error in tokenization phase.\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Error in parsing phase.\"\"\"\n    pass\n    \nclass StructureError(ParseError):\n    \"\"\"Error in semantic analysis phase.\"\"\"\n    pass\n\ndef create_error_context(source: str, position: Position, \n                        context_lines: int = 2) -> str:\n    \"\"\"Generate visual error context showing problematic code.\"\"\"\n    lines = source.split('\\n')\n    start_line = max(0, position.line - context_lines - 1)\n    end_line = min(len(lines), position.line + context_lines)\n    \n    context = []\n    for i in range(start_line, end_line):\n        line_num = i + 1\n        marker = \">>> \" if line_num == position.line else \"    \"\n        context.append(f\"{marker}{line_num:3d} | {lines[i]}\")\n        \n        # Add column pointer for error line\n        if line_num == position.line:\n            pointer = \" \" * (8 + position.column - 1) + \"^\"\n            context.append(pointer)\n    \n    return \"\\n\".join(context)\n```\n\n**Core Tokenizer Skeleton:**\n```python\nclass BaseTokenizer:\n    \"\"\"Base tokenizer for all configuration formats.\"\"\"\n    \n    def __init__(self, source: str):\n        self.source = source\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n    \n    def current_position(self) -> Position:\n        \"\"\"Get current parsing position.\"\"\"\n        # TODO: Return Position object with current line, column, offset\n        pass\n    \n    def peek(self, offset: int = 0) -> str:\n        \"\"\"Look ahead without consuming characters.\"\"\"\n        # TODO: Return character at position + offset, or EOF_MARKER if beyond end\n        # TODO: Handle bounds checking gracefully\n        pass\n    \n    def advance(self) -> str:\n        \"\"\"Consume current character and update position.\"\"\"\n        # TODO: Get current character\n        # TODO: Update position counter (handle \\n for line tracking)\n        # TODO: Update column counter (reset on newline, increment otherwise)\n        # TODO: Return consumed character\n        pass\n    \n    def skip_whitespace(self) -> None:\n        \"\"\"Skip non-semantic whitespace.\"\"\"\n        # TODO: Advance through spaces and tabs\n        # TODO: Preserve newlines for formats where they're significant\n        # TODO: Handle different line ending conventions (\\n, \\r\\n, \\r)\n        pass\n    \n    def read_string_literal(self, quote_char: str) -> Token:\n        \"\"\"Parse quoted string with escape sequences.\"\"\"\n        # TODO: Track starting position for token\n        # TODO: Collect characters until matching quote\n        # TODO: Handle escape sequences (\\n, \\t, \\\", \\\\, etc.)\n        # TODO: Handle unterminated string error\n        # TODO: Return STRING token with processed value\n        pass\n    \n    def read_number(self) -> Token:\n        \"\"\"Parse numeric literal (integer or float).\"\"\"\n        # TODO: Track starting position\n        # TODO: Collect digits, handle decimal points\n        # TODO: Handle scientific notation (1e10, 2.5E-3)\n        # TODO: Handle format-specific features (TOML underscores: 1_000)\n        # TODO: Convert to appropriate Python type (int or float)\n        # TODO: Return NUMBER token with converted value\n        pass\n    \n    def tokenize(self) -> List[Token]:\n        \"\"\"Main tokenization entry point.\"\"\"\n        # TODO: Initialize token list\n        # TODO: Loop through source characters\n        # TODO: Identify token boundaries and types\n        # TODO: Handle format-specific token patterns\n        # TODO: Add EOF token at end\n        # TODO: Return complete token list\n        pass\n```\n\n**Output Generation Utilities:**\n```python\ndef normalize_key(key: str, strategy: str = \"preserve\") -> str:\n    \"\"\"Normalize key names according to specified strategy.\"\"\"\n    # TODO: Implement preserve (no change)\n    # TODO: Implement lowercase conversion\n    # TODO: Implement snake_case conversion (camelCase -> snake_case)\n    # TODO: Implement kebab-case conversion (camelCase -> kebab-case)\n    pass\n\ndef merge_nested_dicts(dict1: Dict[str, Any], dict2: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Deep merge two nested dictionaries.\"\"\"\n    # TODO: Handle overlapping keys\n    # TODO: Recursively merge nested dictionaries\n    # TODO: Handle array values (append vs replace)\n    # TODO: Preserve type information\n    pass\n\ndef convert_parse_tree_to_dict(root: ParseNode) -> Dict[str, Any]:\n    \"\"\"Convert parse tree to unified output format.\"\"\"\n    # TODO: Traverse parse tree depth-first\n    # TODO: Convert section nodes to nested dictionaries\n    # TODO: Convert key-value nodes to dictionary entries\n    # TODO: Handle array-of-tables specially\n    # TODO: Apply type conversions and normalizations\n    pass\n```\n\n**Language-Specific Hints:**\n- Use `dataclasses` for clean, self-documenting data structures with automatic `__init__`, `__repr__`, and equality methods\n- Leverage `typing` module for precise type annotations that improve IDE support and catch bugs early\n- Use `enum.auto()` for token types to avoid manual value assignment and reduce maintenance\n- Consider `functools.lru_cache` for expensive operations like regex compilation in tokenizers\n- Use `collections.defaultdict` for building nested structures incrementally during parsing\n\n**Milestone Checkpoint:**\nAfter implementing the data model components, verify correct behavior:\n\n1. **Token Creation**: Create tokens of each type and verify the `__str__` representation shows useful information\n2. **Position Tracking**: Create positions and verify they format correctly for error messages  \n3. **Parse Tree Construction**: Build a simple parse tree manually and verify `to_dict()` conversion works\n4. **Error Context**: Generate error context for a sample configuration file and verify the visual formatting\n5. **Type Conversions**: Test numeric and boolean conversion with various input formats\n\nRun: `python -m pytest tests/test_data_model.py -v` to verify all data model components work correctly.\n\n**Common Implementation Pitfalls:**\n\n⚠️ **Pitfall: Mutable Default Arguments in Dataclasses**\nUsing mutable defaults like `tokens: List[Token] = []` causes all instances to share the same list. Use `field(default_factory=list)` instead.\n\n⚠️ **Pitfall: Position Tracking with Unicode**\nCounting characters as single units breaks with multi-byte Unicode. Use `len(text.encode('utf-8'))` for byte offsets if needed, or stick to character-based counting for simplicity.\n\n⚠️ **Pitfall: Token Value vs Raw Text Confusion** \nMixing up processed values and raw text leads to double-escaping or incorrect error messages. Always use `raw_text` for error reporting and `value` for semantic processing.\n\n⚠️ **Pitfall: Deep Copy Issues with Parse Trees**\nParse tree nodes contain references to other nodes, making deep copying complex. Implement custom `copy` methods or use immutable structures where possible.\n\n⚠️ **Pitfall: Enum Comparison Mistakes**\nComparing `TokenType.STRING == \"STRING\"` fails because enums don't equal their string representations. Use `token.type == TokenType.STRING` or `token.type.name == \"STRING\"`.\n\n\n## Tokenizer Component Design\n\n> **Milestone(s):** TOML Tokenizer, with foundational concepts supporting INI Parser and YAML Subset Parser\n\nThe tokenizer serves as the lexical analysis engine that transforms raw character streams into meaningful typed tokens with precise position tracking. Think of the tokenizer as a sophisticated pattern recognition system that must simultaneously solve three fundamental challenges: identifying where meaningful units begin and end in continuous text, classifying what type of semantic meaning each unit carries, and maintaining perfect position tracking for error reporting. Unlike simple string splitting, tokenization must handle **lexical ambiguity** where the same character sequence can mean completely different things depending on context—a quote character might start a string literal, escape another quote, or appear as literal content within a different quoting style.\n\n![Token Type Hierarchy](./diagrams/token-types.svg)\n\nThe tokenizer operates as a **context-sensitive** state machine that maintains awareness of its current parsing context to resolve these ambiguities. When the tokenizer encounters a quote character, its behavior depends entirely on whether it's currently inside a string literal, what type of string context it's in, and what escape rules apply. This context sensitivity becomes particularly complex when handling the different string literal syntaxes across INI, TOML, and YAML formats, where the same character sequence `\"\"\"` might indicate a multiline string start in TOML but could be three separate quoted empty strings in INI context.\n\nThe tokenizer must also solve the **impedance mismatch** between human-readable configuration syntax and machine-processable token streams. Configuration formats are designed for human readability, using intuitive conventions like indentation for nesting (YAML) or natural key=value syntax (INI). However, parsers need discrete, classified tokens with explicit type information and precise boundary definitions. The tokenizer bridges this gap by applying format-specific lexical rules that preserve the semantic intent while creating the structured token stream that downstream parsers require.\n\n### Tokenization Mental Model\n\nUnderstanding tokenization requires thinking about it as **pattern recognition with state tracking** rather than simple character classification. Imagine the tokenizer as a careful reader who must simultaneously identify word boundaries, understand context-dependent meanings, and take detailed notes about location and classification for later reference.\n\nThe mental model begins with the concept of a **scanning window** that moves through the character stream one position at a time. At each position, the tokenizer must decide whether the current character continues an existing token, starts a new token, or serves as a delimiter that separates tokens. This decision requires examining not just the current character, but also the **parsing context** maintained in the tokenizer's state machine. The context tracks information like \"currently inside a quoted string,\" \"at the start of a line,\" or \"within a comment block.\"\n\nConsider how a human reader processes the TOML line `name = \"John \\\"Doe\\\"\"`. The reader automatically recognizes that the first quote starts a string, the backslash-quote sequence represents an escaped quote within the string content, and the final quote ends the string. The tokenizer must replicate this contextual understanding through explicit state tracking. When it encounters the first quote, it transitions to \"inside string literal\" state. When it sees the backslash, it transitions to \"processing escape sequence\" state, reads the escaped quote as literal content, returns to string literal state, and finally recognizes the closing quote as a string terminator.\n\nThe **lookahead** concept provides another essential mental model component. The tokenizer often needs to examine upcoming characters to make correct tokenization decisions. When processing the sequence `123.456e-7`, the tokenizer must look ahead to determine whether this represents a single floating-point number token or multiple separate tokens. The decimal point alone isn't sufficient—it must examine the subsequent characters to distinguish between a number continuation and a separate identifier that might follow.\n\n**Position tracking** requires thinking of the tokenizer as maintaining a detailed location journal. Every token must carry precise position information indicating exactly where it appeared in the source text. This information becomes critical for error reporting, IDE integration, and debugging tools. The position tracking must handle format-specific complications like YAML's significant indentation (where column position affects semantic meaning) and multiline string literals (where internal newlines don't advance the logical line number for subsequent tokens).\n\nThe **error recovery** mental model treats tokenization errors as opportunities to provide helpful feedback rather than immediate failures. When the tokenizer encounters malformed input like an unterminated string literal, it should attempt to identify the likely intended structure, create appropriate error tokens, and continue processing to find additional issues. Think of this as a careful proofreader who marks errors but continues reading to provide comprehensive feedback rather than stopping at the first mistake.\n\n### Tokenizer Interface Design\n\nThe tokenizer interface must provide a clean abstraction that supports both streaming tokenization (processing tokens one at a time) and batch tokenization (processing entire files), while maintaining consistent error handling and position tracking across all supported formats.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `current_position` | none | `Position` | Returns current parsing position with line, column, and byte offset |\n| `peek` | `offset=0` | `str` | Look ahead at character without consuming, returns EOF_MARKER at end |\n| `advance` | none | `str` | Consume current character, update position tracking, return consumed character |\n| `tokenize` | none | `List[Token]` | Main entry point that processes entire input and returns complete token list |\n| `next_token` | none | `Token` | Generate and return next token from current position, advance past it |\n| `skip_whitespace` | none | `None` | Advance past all non-semantic whitespace characters |\n| `read_string_literal` | `quote_char: str` | `Token` | Parse quoted string with escape sequence processing |\n| `read_number` | none | `Token` | Parse numeric literal with type inference (int/float) |\n| `read_identifier` | none | `Token` | Parse unquoted identifier or keyword |\n| `read_comment` | `comment_char: str` | `Token` | Parse comment from delimiter to end of line |\n| `create_error_token` | `message: str, raw_text: str` | `Token` | Create INVALID token with error information |\n| `is_at_end` | none | `bool` | Check if tokenizer has reached end of input |\n\nThe interface design separates concerns between **navigation** methods (`peek`, `advance`, `current_position`) that handle character stream management and **recognition** methods (`read_string_literal`, `read_number`) that identify and extract specific token types. This separation allows the recognition methods to focus on format-specific parsing logic while relying on consistent navigation behavior.\n\n> **Decision: Lookahead Interface Design**\n> - **Context**: Different formats require varying amounts of lookahead for tokenization decisions\n> - **Options Considered**: Fixed single-character lookahead, unlimited string lookahead, parameterized offset lookahead\n> - **Decision**: Parameterized offset lookahead with `peek(offset=0)` method\n> - **Rationale**: Provides flexibility for complex tokenization while maintaining predictable performance characteristics\n> - **Consequences**: Enables proper handling of complex literals like scientific notation while avoiding unbounded memory usage\n\nThe `BaseTokenizer` maintains internal state that supports both the navigation and recognition interfaces:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `source` | `str` | Complete input text being tokenized |\n| `position` | `int` | Current byte offset in source text |\n| `line` | `int` | Current line number (1-based) for error reporting |\n| `column` | `int` | Current column number (1-based) for error reporting |\n| `tokens` | `List[Token]` | Accumulated tokens from tokenization process |\n\nThe tokenizer interface supports **streaming processing** through the `next_token` method, allowing parsers to process tokens incrementally without loading the complete token list into memory. This approach becomes important for large configuration files or embedded systems with memory constraints. The streaming interface maintains the same error handling and position tracking guarantees as batch tokenization.\n\n**Error token creation** represents a critical interface design decision. Rather than throwing exceptions immediately upon encountering malformed input, the tokenizer creates `INVALID` tokens that carry error information. This approach allows the parser to collect multiple errors in a single pass and provide comprehensive feedback to users. The error tokens include the malformed text, error description, and precise position information.\n\n![Tokenizer State Machine](./diagrams/tokenizer-state-machine.svg)\n\nThe interface supports **format-specific customization** through polymorphism. Each format (INI, TOML, YAML) can extend `BaseTokenizer` and override specific recognition methods while inheriting the common navigation and position tracking behavior. For example, YAML tokenization requires custom `skip_whitespace` behavior that preserves significant indentation, while TOML needs specialized string literal processing for its multiple quoting styles.\n\n### String Literal Handling\n\nString literal tokenization represents the most complex aspect of lexical analysis due to the variety of quoting styles, escape sequence processing, and multiline handling requirements across different configuration formats. The complexity stems from **context sensitivity**—the same character sequence can have completely different meanings depending on the current string parsing state.\n\nThe string literal recognition system must handle multiple **quoting styles** with different semantic rules:\n\n| Quote Style | Formats | Escape Processing | Multiline Support | Special Rules |\n|-------------|---------|------------------|-------------------|---------------|\n| Single quotes `'text'` | INI, TOML, YAML | TOML: none, others: basic | TOML only | TOML literal strings don't process escapes |\n| Double quotes `\"text\"` | All formats | Full escape processing | No | Standard escape sequences: `\\n`, `\\t`, `\\\"`, `\\\\` |\n| Triple single `'''text'''` | TOML only | None | Yes | First newline after opening quotes is trimmed |\n| Triple double `\"\"\"text\"\"\"` | TOML only | Full escape processing | Yes | First newline after opening quotes is trimmed |\n| No quotes (bare) | All formats | None | Context-dependent | YAML flow scalars, INI values, TOML bare keys |\n\nThe **escape sequence processing** requires a nested state machine within string literal parsing. When the tokenizer encounters a backslash character inside a double-quoted string, it must transition to escape processing mode and handle various escape sequences:\n\n1. The tokenizer identifies the backslash as an escape indicator\n2. It examines the following character to determine the escape type\n3. For standard escapes (`\\n`, `\\t`, `\\r`, `\\\"`, `\\\\`), it converts to the appropriate character\n4. For Unicode escapes (`\\u0041`, `\\U00000041`), it processes the hexadecimal digits\n5. For invalid escape sequences, it creates an error token with position information\n6. It returns to normal string content processing mode\n\n**Multiline string handling** introduces additional complexity because the tokenizer must track logical string content while maintaining accurate position information for error reporting. TOML's triple-quoted strings include special rules about newline handling: the first newline immediately after the opening quotes is automatically trimmed, but subsequent newlines are preserved as content. The tokenizer must implement this logic while correctly updating line and column tracking.\n\n> **Decision: String Literal State Machine Design**\n> - **Context**: String parsing requires handling nested state transitions for quotes, escapes, and multiline content\n> - **Options Considered**: Single-function string parser, recursive state machine, explicit state tracking with switch statements\n> - **Decision**: Explicit state tracking with enumerated states and transition table\n> - **Rationale**: Provides clear visibility into parsing state, enables systematic testing, supports debugging\n> - **Consequences**: More verbose implementation but significantly easier to debug and extend for new quote styles\n\nThe string literal parser maintains explicit state through an enumeration:\n\n| State | Description | Valid Transitions | Exit Conditions |\n|-------|-------------|------------------|-----------------|\n| `NORMAL` | Processing regular string content | `ESCAPE`, `QUOTE_END` | Closing quote or end of input |\n| `ESCAPE` | Processing escape sequence | `NORMAL`, `ERROR` | Valid escape processed or invalid sequence |\n| `UNICODE_ESCAPE` | Processing `\\uXXXX` sequence | `NORMAL`, `ERROR` | Four hex digits processed or invalid digit |\n| `QUOTE_END` | Potential end of multiline string | `NORMAL`, `COMPLETE` | Triple quote completed or false alarm |\n| `ERROR` | Invalid sequence encountered | `NORMAL`, `COMPLETE` | Error token created, attempt recovery |\n| `COMPLETE` | String literal fully parsed | none | Return completed token |\n\n**Error recovery** in string literal parsing requires special consideration because unterminated strings can consume the entire remaining input. When the tokenizer reaches the end of input while still inside a string literal, it must create an error token that includes all the consumed content and indicates the unterminated string issue. The error token should point to the opening quote position to help users identify where the string began.\n\nThe string literal handler must also manage **performance considerations** for large multiline strings. Rather than concatenating characters one at a time (which creates O(n²) complexity), it should identify string boundaries first, then extract the complete content in a single operation. This approach maintains linear performance even for very large configuration files with substantial multiline content.\n\n**Quote character disambiguation** represents another critical challenge. When the tokenizer encounters a quote character, it must determine whether this starts a single-quoted string, a double-quoted string, or potentially a triple-quoted multiline string. This requires lookahead logic that examines the following characters to make the correct determination:\n\n1. Single quote encountered: look ahead two characters to check for triple-quote pattern\n2. If triple-quote detected: initialize multiline literal string parsing\n3. If not triple-quote: initialize single-quote string parsing\n4. Handle edge cases like `''` (empty string) vs `'''` (multiline string start)\n\n### Tokenizer Architecture Decisions\n\nThe tokenizer architecture must balance **performance**, **maintainability**, and **extensibility** while handling the diverse requirements of INI, TOML, and YAML formats. The key architectural decisions affect how the tokenizer manages state, processes different formats, and integrates with the overall parsing pipeline.\n\n> **Decision: Single Tokenizer vs Format-Specific Tokenizers**\n> - **Context**: Each format has different lexical rules, keywords, and syntactic elements that require specialized handling\n> - **Options Considered**: Single universal tokenizer with format flags, completely separate tokenizers, base tokenizer with format-specific extensions\n> - **Decision**: Base tokenizer with format-specific extensions through inheritance\n> - **Rationale**: Maximizes code reuse for common functionality while allowing format-specific customization where needed\n> - **Consequences**: Shared navigation and position tracking logic, but specialized string and numeric literal handling per format\n\n| Approach | Pros | Cons | Chosen? |\n|----------|------|------|---------|\n| Universal tokenizer | Single codebase, consistent behavior | Complex branching logic, hard to optimize per format | No |\n| Separate tokenizers | Format-optimized, clear separation | Code duplication, inconsistent position tracking | No |\n| Inheritance-based | Code reuse + specialization | Moderate complexity, clear extension points | **Yes** |\n\nThe inheritance-based approach creates a `BaseTokenizer` that handles universal concerns like position tracking, character navigation, and basic token creation. Format-specific tokenizers (`INITokenizer`, `TOMLTokenizer`, `YAMLTokenizer`) extend the base class and override methods that require specialized behavior. This architecture enables sharing common functionality while supporting format-specific requirements like YAML's indentation-sensitive tokenization or TOML's complex string literal rules.\n\n> **Decision: Character Encoding and Unicode Support**\n> - **Context**: Configuration files may contain Unicode characters, and different platforms handle encoding differently\n> - **Options Considered**: ASCII-only support, UTF-8 with byte processing, full Unicode with character processing\n> - **Decision**: Full Unicode support with character-based processing and UTF-8 encoding assumption\n> - **Rationale**: Modern configuration files commonly contain Unicode characters for internationalization\n> - **Consequences**: More complex position tracking (byte offset vs character offset), but supports real-world usage\n\nThe Unicode support decision affects position tracking implementation. The tokenizer must maintain both **character positions** (for human-readable error messages) and **byte offsets** (for efficient file access). When processing multibyte Unicode characters, the character count and byte count diverge, requiring careful tracking of both metrics.\n\n> **Decision: Error Recovery Strategy**\n> - **Context**: Malformed configuration files should provide helpful error messages rather than immediate failures\n> - **Options Considered**: Fail-fast on first error, collect errors and continue, attempt automatic correction\n> - **Decision**: Collect errors in special INVALID tokens and continue tokenization when possible\n> - **Rationale**: Allows comprehensive error reporting and better user experience for debugging\n> - **Consequences**: More complex parser logic but significantly better error messages\n\nThe error recovery approach creates `TokenError` objects that carry both the problematic text and contextual information about what was expected. The tokenizer continues processing after creating error tokens, allowing it to identify multiple issues in a single pass. This approach provides much better user experience compared to fail-fast behavior that requires multiple edit-test cycles to identify all issues.\n\n> **Decision: Memory Management and Token Storage**\n> - **Context**: Large configuration files could create memory pressure if all tokens are stored simultaneously\n> - **Options Considered**: Store all tokens in memory, streaming tokenization only, hybrid approach with optional storage\n> - **Decision**: Hybrid approach with batch tokenization for normal use and streaming interface for large files\n> - **Rationale**: Most configuration files are small enough for in-memory processing, but large files need streaming\n> - **Consequences**: Two code paths to maintain, but supports both typical usage and edge cases\n\nThe memory management decision creates two tokenization modes. The standard `tokenize()` method processes the entire input and returns a complete token list, suitable for typical configuration files under 1MB. The streaming `next_token()` interface allows processing arbitrarily large files by generating tokens on demand without storing the complete list.\n\n**State machine implementation** represents another critical architectural decision. The tokenizer uses explicit state enumeration rather than implicit state in nested function calls. This approach provides better debuggability and makes the tokenization process easier to trace and test. Each state transition is explicitly modeled, making the tokenizer's behavior predictable and testable.\n\n### Common Tokenizer Pitfalls\n\nTokenizer implementation involves several subtle but critical pitfalls that frequently trap developers, particularly around escape sequence processing, position tracking accuracy, and state management consistency.\n\n⚠️ **Pitfall: Incorrect Escape Sequence Processing**\n\nThe most common tokenizer mistake involves improper handling of escape sequences within string literals. Developers often implement escape processing that fails to handle edge cases or processes escapes in contexts where they shouldn't be processed. For example, TOML literal strings (single-quoted) should not process escape sequences—the sequence `'C:\\path\\to\\file'` should preserve the backslashes literally, not interpret them as escape attempts.\n\nThe error typically manifests when the tokenizer applies escape processing to all quoted strings regardless of quote type. This breaks TOML literal strings and can cause incorrect parsing of file paths, regular expressions, and other content that intentionally contains backslashes. The fix requires checking the string literal type before applying escape processing: only double-quoted strings in TOML should process escapes, while single-quoted strings preserve backslashes literally.\n\n⚠️ **Pitfall: Position Tracking Inconsistencies**\n\nPosition tracking errors create confusing error messages that point to incorrect locations in the source file. The most frequent mistake involves failing to properly update line and column numbers when processing special characters like tabs, carriage returns, and multiline strings. When the tokenizer encounters a tab character, it should advance the column position according to tab width settings (typically 4 or 8 spaces), not increment by one character.\n\nAnother common position tracking error occurs with multiline string literals. When processing TOML triple-quoted strings, the tokenizer must correctly update line numbers for each newline within the string content while ensuring that subsequent tokens have accurate position information. The error typically causes error messages to point to the beginning of the multiline string rather than the actual error location.\n\n⚠️ **Pitfall: State Machine Inconsistencies**\n\nState management errors in string literal parsing often cause the tokenizer to get \"stuck\" in a particular state or fail to properly transition between states. A common example involves handling nested quote characters: when processing `\"He said \\\"hello\\\"\"`, the tokenizer must properly transition to escape state when it encounters the backslash, process the escaped quote as content, and return to normal string processing state.\n\nThe error typically occurs when developers implement string parsing with ad-hoc conditional logic rather than explicit state tracking. The fix requires implementing a clear state machine with enumerated states and explicit transition conditions. Each state should have well-defined entry conditions, processing behavior, and exit conditions.\n\n⚠️ **Pitfall: Lookahead Buffer Management**\n\nIncorrect lookahead implementation can cause the tokenizer to miss token boundaries or incorrectly classify tokens. A common mistake involves implementing `peek()` that doesn't properly handle the end-of-file condition, causing array index errors or infinite loops when the tokenizer reaches the end of input.\n\nAnother lookahead error involves modifying the tokenizer state during lookahead operations. The `peek()` method should be purely observational—it should examine upcoming characters without advancing the current position or changing internal state. Lookahead that accidentally modifies state can cause tokens to be skipped or duplicated.\n\n⚠️ **Pitfall: Comment Handling Edge Cases**\n\nComment processing errors frequently occur at line boundaries and in interaction with string literals. A common mistake involves recognizing comment delimiters (`#` or `;`) that appear inside string literals as actual comment starts. The line `message = \"Error #404: Not found\"` should not treat everything after the `#` as a comment—the hash character is part of the string content.\n\nThe fix requires checking the current parsing context before processing comment delimiters. Comment recognition should only occur when the tokenizer is not inside a string literal or other quoted context. Additionally, comment processing must properly handle different line ending styles (`\\n`, `\\r\\n`, `\\r`) to ensure comments are correctly terminated.\n\n⚠️ **Pitfall: Number Format Recognition**\n\nNumeric literal tokenization often fails to handle edge cases like scientific notation, hex literals, or numbers with underscores (allowed in TOML). A common error involves recognizing `1.23e-4` as three separate tokens (`1.23`, `e`, `-4`) instead of a single floating-point number in scientific notation.\n\nThe fix requires implementing proper lookahead in number recognition. When the tokenizer encounters a digit, it must examine the following characters to determine the complete numeric literal extent. This includes handling decimal points, exponent markers (`e` or `E`), sign characters in exponents, and format-specific features like TOML's underscore separators in large numbers.\n\n### Implementation Guidance\n\nThe tokenizer implementation requires careful attention to character encoding, state management, and performance optimization. The following guidance provides concrete recommendations for building a robust tokenizer that handles all three configuration formats effectively.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Character Processing | Basic string indexing with ord() | Unicode-aware processing with unicodedata module |\n| State Management | Explicit state variables | State machine with enum states and transition table |\n| Position Tracking | Simple line/column counters | Position objects with byte offset, line, column tracking |\n| Error Handling | Exception throwing | Error token collection with context |\n\n**B. Recommended File Structure**\n\n```\nsrc/\n├── tokenizer/\n│   ├── __init__.py              ← exports BaseTokenizer, Token, TokenType\n│   ├── base_tokenizer.py        ← BaseTokenizer with core functionality\n│   ├── tokens.py                ← Token, TokenType, Position definitions\n│   ├── ini_tokenizer.py         ← INITokenizer with format-specific rules\n│   ├── toml_tokenizer.py        ← TOMLTokenizer with complex string handling\n│   ├── yaml_tokenizer.py        ← YAMLTokenizer with indentation processing\n│   └── errors.py                ← TokenError and related error types\n├── parsers/                     ← parser components use tokenizer\n└── tests/\n    └── tokenizer/\n        ├── test_base_tokenizer.py\n        ├── test_string_literals.py\n        └── test_position_tracking.py\n```\n\n**C. Infrastructure Starter Code**\n\nComplete token and position definitions that learners can use directly:\n\n```python\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\n\nclass TokenType(Enum):\n    STRING = auto()\n    NUMBER = auto()\n    BOOLEAN = auto()\n    IDENTIFIER = auto()\n    EQUALS = auto()\n    COLON = auto()\n    NEWLINE = auto()\n    EOF = auto()\n    COMMENT = auto()\n    SECTION_START = auto()      # [\n    SECTION_END = auto()        # ]\n    ARRAY_START = auto()        # [\n    ARRAY_END = auto()          # ]\n    OBJECT_START = auto()       # {\n    OBJECT_END = auto()         # }\n    COMMA = auto()\n    DOT = auto()\n    INDENT = auto()             # YAML indentation increase\n    DEDENT = auto()             # YAML indentation decrease\n    BLOCK_SEQUENCE = auto()     # YAML - marker\n    INVALID = auto()            # Error token\n\n@dataclass\nclass Position:\n    line: int\n    column: int\n    offset: int\n    \n    def __str__(self) -> str:\n        return f\"line {self.line}, column {self.column}\"\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: Any\n    position: Position\n    raw_text: str\n    \n    def __str__(self) -> str:\n        return f\"{self.type.name}({self.value}) at {self.position}\"\n\n# Character constants for tokenizer logic\nEOF_MARKER = '\\0'\nWHITESPACE_CHARS = ' \\t\\r'\nNEWLINE_CHARS = '\\n'\nQUOTE_CHARS = '\"\\'`'\nNUMBER_START_CHARS = '0123456789+-'\nIDENTIFIER_START_CHARS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_'\nIDENTIFIER_CHARS = IDENTIFIER_START_CHARS + '0123456789-'\n```\n\n**D. Core Logic Skeleton Code**\n\nBaseTokenizer implementation with detailed TODO comments for learner implementation:\n\n```python\nclass BaseTokenizer:\n    def __init__(self, source: str):\n        self.source = source\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens = []\n    \n    def current_position(self) -> Position:\n        \"\"\"Returns current parsing position with line, column, and byte offset.\"\"\"\n        return Position(self.line, self.column, self.position)\n    \n    def peek(self, offset: int = 0) -> str:\n        \"\"\"Look ahead at character without consuming, returns EOF_MARKER at end.\"\"\"\n        # TODO 1: Calculate target position as self.position + offset\n        # TODO 2: Check if target position is beyond source length\n        # TODO 3: Return EOF_MARKER if beyond end, otherwise return character at target\n        # Hint: Handle negative offsets by returning EOF_MARKER\n        pass\n    \n    def advance(self) -> str:\n        \"\"\"Consume current character, update position tracking, return consumed char.\"\"\"\n        # TODO 1: Check if at end of source, return EOF_MARKER if so\n        # TODO 2: Get current character at self.position\n        # TODO 3: Increment self.position\n        # TODO 4: Update line and column based on character type:\n        #         - If newline: increment line, reset column to 1\n        #         - If tab: advance column to next tab stop (typically +4 or +8)\n        #         - Otherwise: increment column by 1\n        # TODO 5: Return the consumed character\n        # Hint: Handle both \\n and \\r\\n newline styles\n        pass\n    \n    def skip_whitespace(self) -> None:\n        \"\"\"Advance past all non-semantic whitespace characters.\"\"\"\n        # TODO 1: Loop while current character is in WHITESPACE_CHARS\n        # TODO 2: Call advance() for each whitespace character\n        # TODO 3: Stop when reaching non-whitespace or EOF_MARKER\n        # Note: Don't skip newlines - they may be significant tokens\n        pass\n    \n    def read_string_literal(self, quote_char: str) -> Token:\n        \"\"\"Parse quoted string with escape sequence processing.\"\"\"\n        start_pos = self.current_position()\n        # TODO 1: Check if this might be a triple-quoted string (look ahead 2 chars)\n        # TODO 2: If triple-quoted, call read_multiline_string instead\n        # TODO 3: Advance past opening quote\n        # TODO 4: Initialize empty content list for building string\n        # TODO 5: Loop until closing quote or EOF:\n        #         - If escape char (\\), process escape sequence\n        #         - If closing quote, break loop\n        #         - If newline in single-quoted string, create error token\n        #         - Otherwise add character to content\n        # TODO 6: Advance past closing quote (if found)\n        # TODO 7: Join content list and create STRING token\n        # Hint: Different quote types have different escape rules\n        pass\n    \n    def read_number(self) -> Token:\n        \"\"\"Parse numeric literal with type inference (int/float).\"\"\"\n        # TODO 1: Determine if number starts with sign (+/-)\n        # TODO 2: Read integer portion (digits, possibly with underscores in TOML)\n        # TODO 3: Check for decimal point, read fractional portion if present\n        # TODO 4: Check for exponent (e/E), read exponent with optional sign\n        # TODO 5: Determine if result should be int or float based on format\n        # TODO 6: Convert string to appropriate numeric type\n        # TODO 7: Create NUMBER token with converted value\n        # Hint: Handle scientific notation like 1.23e-4\n        pass\n    \n    def tokenize(self) -> list[Token]:\n        \"\"\"Main tokenization entry point - processes entire input.\"\"\"\n        # TODO 1: Initialize empty tokens list\n        # TODO 2: Loop until position reaches end of source:\n        #         - Skip whitespace\n        #         - Identify token type from current character\n        #         - Call appropriate read_* method\n        #         - Add resulting token to list\n        # TODO 3: Add final EOF token\n        # TODO 4: Return complete token list\n        # Hint: Use character-based dispatch for token type identification\n        pass\n```\n\n**E. Language-Specific Hints**\n\n- Use `str.isdigit()`, `str.isalpha()`, `str.isalnum()` for character classification\n- Handle Unicode properly with `len()` and string slicing - Python handles UTF-8 correctly\n- Use `ord()` and `chr()` for escape sequence processing (e.g., `\\n` → `chr(10)`)\n- Consider using `unicodedata.category()` for advanced Unicode character classification\n- Use `enum.auto()` for TokenType to avoid manual numbering\n- Implement `__str__` and `__repr__` methods for debugging Token and Position classes\n\n**F. Milestone Checkpoint**\n\nAfter implementing the base tokenizer:\n\n**Test Command:** `python -m pytest tests/tokenizer/test_base_tokenizer.py -v`\n\n**Expected Output:**\n```\ntest_peek_lookahead PASSED\ntest_advance_position_tracking PASSED  \ntest_string_literal_basic PASSED\ntest_string_literal_escapes PASSED\ntest_number_parsing PASSED\ntest_tokenize_complete_input PASSED\n```\n\n**Manual Verification:**\n```python\ntokenizer = BaseTokenizer('key = \"value\"')\ntokens = tokenizer.tokenize()\nprint([str(token) for token in tokens])\n# Expected: ['IDENTIFIER(key) at line 1, column 1', 'EQUALS(=) at line 1, column 5', ...]\n```\n\n**Signs of Problems:**\n- Position tracking off by one → Check advance() method line/column updates\n- Escape sequences not working → Verify quote type checking in read_string_literal\n- Infinite loops → Check EOF_MARKER handling in peek() and advance()\n- Missing tokens → Verify whitespace skipping doesn't consume significant characters\n\n\n## INI Parser Component Design\n\n> **Milestone(s):** INI Parser - implements section-based parsing with key-value pairs and comment handling\n\nThe INI parser represents our entry point into configuration file parsing, serving as the foundation for understanding core parsing concepts before tackling more complex formats. Think of INI parsing as learning to drive in an empty parking lot before venturing onto busy highways—the fundamental principles of tokenization, state management, and data structure mapping are all present, but in their simplest possible form.\n\nThe elegance of INI files lies in their human-readable simplicity: sections enclosed in square brackets, key-value pairs separated by equals signs or colons, and comments prefixed with semicolons or hash symbols. However, this apparent simplicity conceals several parsing challenges that make INI an excellent learning platform for understanding configuration file processing.\n\n### INI Parsing Mental Model: Understanding INI as Section-Based Key-Value Organization\n\nUnderstanding INI parsing requires thinking about it as a **hierarchical filing system** where documents are organized into labeled folders. Each section header acts like a manila folder tab, and the key-value pairs underneath are the documents filed within that folder. The parser's job is to walk through this filing system sequentially, creating digital folders and filing away documents in the correct locations.\n\nThe mental model extends further when we consider that some documents might exist outside of any folder (global keys), some folders might be referenced multiple times throughout the filing system, and some documents might have annotations (comments) that need to be preserved or ignored based on configuration.\n\nThis hierarchical organization creates several parsing contexts that must be tracked simultaneously. The **current section context** determines where newly encountered key-value pairs should be stored. The **comment context** affects whether a line should be processed or ignored. The **value context** influences how the right-hand side of assignments should be interpreted and type-converted.\n\nConsider this fundamental INI structure that demonstrates the key parsing contexts:\n\n```\n# Global configuration\ndebug = true\nport = 8080\n\n[database]\nhost = localhost\nport = 5432  # Different from global port\nname = \"my app\"\n\n[database.connection]\ntimeout = 30\n```\n\nThe parser must recognize that we have three distinct namespaces: global scope, `database` section, and `database.connection` section. Keys can be repeated across namespaces (like `port`) without conflict. The final data structure should reflect this hierarchical organization through nested dictionaries.\n\nThe **line-oriented processing model** is crucial for INI parsing. Unlike formats that might span arbitrary boundaries, INI parsing can be conceptualized as processing one logical line at a time. Each line falls into one of several categories: blank lines (ignored), comment lines (ignored or preserved), section headers (context switches), key-value assignments (data storage), or continuation lines (value extension).\n\nThis line-oriented approach creates a natural state machine where the parser maintains current context and applies different processing rules based on line classification. The beauty of this model is its simplicity—most INI parsing bugs stem from incorrectly classifying lines or failing to maintain proper context across line boundaries.\n\n### INI Parsing Algorithm: Step-by-Step Process for Handling Sections, Keys, Values, and Comments\n\nThe INI parsing algorithm follows a systematic approach that mirrors how humans naturally read these files: top to bottom, line by line, building understanding of structure as we encounter section boundaries. The algorithm maintains parsing state across line boundaries while making context-sensitive decisions about how to interpret each line.\n\nThe core algorithm can be broken down into distinct phases that handle different aspects of the parsing process:\n\n1. **Preprocessing and Line Classification**: The raw input is split into logical lines (handling line continuation if supported), and each line is classified by its syntactic structure. This phase identifies section headers, key-value pairs, comments, and blank lines through pattern recognition.\n\n2. **Context Management**: Based on line classification, the parser updates its current parsing context. Section headers cause context switches, while key-value pairs are processed within the current context. This phase maintains the section stack and current namespace information.\n\n3. **Value Processing**: Key-value pairs undergo value extraction, type inference, and storage within the appropriate section. This phase handles quoted strings, escape sequences, and basic type coercion.\n\n4. **Structure Building**: The parsed data is organized into the final nested dictionary structure, creating intermediate sections as needed and handling key path resolution for dotted notation.\n\nHere's the detailed step-by-step algorithm for INI parsing:\n\n1. **Initialize parsing state** by creating an empty result dictionary, setting current section to global scope (empty string), and preparing line processing infrastructure including line number tracking and error context accumulation.\n\n2. **Split input into logical lines** while preserving line number information for error reporting. Handle different line ending conventions (CRLF, LF) and optionally support line continuation with backslash escapes if that feature is desired.\n\n3. **For each logical line, classify its syntactic type** by examining leading characters after whitespace trimming. Lines starting with `[` are section headers, lines containing `=` or `:` are key-value pairs, lines starting with `;` or `#` are comments, and empty lines are ignored.\n\n4. **Process section headers** by extracting the section name from between square brackets, validating the syntax, and updating the current section context. Create nested dictionary structure if the section name contains dots (like `database.connection`).\n\n5. **Process key-value pairs** by splitting on the first occurrence of `=` or `:`, trimming whitespace from both sides, and applying value processing rules. Store the processed key-value pair in the current section of the result dictionary.\n\n6. **Handle value processing** by detecting quoted strings and processing escape sequences, applying type inference to convert strings to appropriate Python types (integers, floats, booleans), and handling special cases like empty values or values containing the assignment operator.\n\n7. **Manage inline comments** by detecting comment markers after values and either preserving them as metadata or ignoring them based on parser configuration. Be careful not to treat comment markers inside quoted strings as actual comments.\n\n8. **Handle error recovery** by collecting syntax errors with line number information while continuing to parse subsequent lines when possible. This allows reporting multiple errors in a single parse run rather than failing on the first error.\n\n9. **Finalize the result structure** by ensuring all sections exist in the final dictionary (even empty sections), applying any post-processing transformations like key normalization, and validating the overall structure for consistency.\n\n10. **Return the parsed configuration** as a nested dictionary where top-level keys represent sections (with an empty string key for global values) and values are dictionaries containing the key-value pairs for each section.\n\nThe algorithm maintains several pieces of state throughout this process: the current section context (determining where new keys are stored), accumulated errors for comprehensive reporting, line number information for error context, and the growing result dictionary that represents the parsed structure.\n\n### INI Architecture Decisions: Decisions Around Global Keys, Comment Handling, and Value Type Inference\n\nThe INI parser requires several architectural decisions that significantly impact both implementation complexity and user experience. These decisions represent trade-offs between simplicity, compatibility, and functionality. Each choice has downstream consequences for how the parser behaves in edge cases and how well it integrates with the broader configuration parsing system.\n\n> **Decision: Global Key Handling Strategy**\n> - **Context**: INI files often contain key-value pairs before any section headers, but different parsers handle these differently—some reject them, others create an implicit section, others treat them as truly global.\n> - **Options Considered**: Reject global keys as syntax errors, create implicit \"DEFAULT\" section, store in special global namespace\n> - **Decision**: Store global keys in the result dictionary with empty string as section key, accessible as `result[\"\"]`\n> - **Rationale**: This approach maintains the section-based mental model while accommodating real-world INI files that use global configuration. It's explicit (no hidden \"DEFAULT\" section) and predictable (always accessible the same way).\n> - **Consequences**: Users must check for the empty string key to access global values, but the behavior is consistent and discoverable.\n\n| Global Key Option | Pros | Cons | Compatibility |\n|------------------|------|------|---------------|\n| Reject as error | Simple, enforces structure | Breaks with common INI files | Low |\n| Implicit DEFAULT section | Familiar to ConfigParser users | Hidden behavior, not obvious | Medium |\n| Empty string key | Explicit, predictable | Slightly awkward access pattern | High |\n\n> **Decision: Comment Preservation Strategy**\n> - **Context**: Comments serve different purposes—some are documentation that should be preserved, others are temporary annotations. Different use cases benefit from different handling approaches.\n> - **Options Considered**: Always ignore comments, always preserve as metadata, configurable preservation with default ignore\n> - **Decision**: Default to ignoring comments with optional preservation mode that stores them as metadata in a parallel structure\n> - **Rationale**: Most configuration consumers don't need comment data and benefit from cleaner output, but preserving comments enables round-trip editing and documentation tools.\n> - **Consequences**: Simple use cases get clean data, advanced use cases can opt into comment preservation with additional complexity.\n\n| Comment Strategy | Memory Usage | Output Complexity | Use Case Support |\n|-----------------|--------------|-------------------|------------------|\n| Always ignore | Low | Simple | Basic config loading |\n| Always preserve | High | Complex | Documentation tools |\n| Configurable | Variable | Medium | Both use cases |\n\n> **Decision: Value Type Inference Rules**\n> - **Context**: INI files store everything as strings, but users expect automatic conversion to appropriate Python types. Different inference rules affect both usability and predictability.\n> - **Options Considered**: No type inference (all strings), aggressive inference with boolean/number detection, conservative inference with opt-in parsing\n> - **Decision**: Conservative type inference that converts obvious numbers and booleans while preserving strings for ambiguous values\n> - **Rationale**: Automatic conversion improves usability for common cases while avoiding surprising conversions that might break application logic.\n> - **Consequences**: Users get convenient type conversion for clear cases but maintain control over ambiguous values through explicit quoting.\n\n| Inference Level | Conversion Examples | Surprising Cases | User Control |\n|----------------|-------------------|------------------|--------------|\n| None | All strings | None | Full |\n| Aggressive | \"yes\"→true, \"1.0\"→float | Many edge cases | Limited |\n| Conservative | \"123\"→int, \"true\"→bool | Minimal | Good balance |\n\n> **Decision: Section Nesting Strategy**\n> - **Context**: Some INI files use dotted section names to imply hierarchy, while others treat dots as literal characters. The parser must choose how to interpret these patterns.\n> - **Options Considered**: Always flatten section names, always create nested structure from dots, configurable nesting with sensible default\n> - **Decision**: Create nested dictionary structure from dotted section names by default, with option to disable nesting for literal interpretation\n> - **Rationale**: Nested structure matches user expectations for modern configuration and provides better integration with other formats, while the option preserves compatibility with legacy systems.\n> - **Consequences**: Default behavior creates intuitive nested access patterns, but users working with legacy systems can opt out of nesting interpretation.\n\nThe type inference implementation uses a priority-based approach that attempts conversions in order of specificity. Integer conversion is attempted first (using Python's `int()` function), followed by float conversion, then boolean conversion using a predefined mapping of string values to boolean states. Only if all conversions fail does the value remain as a string.\n\n| Value Pattern | Inference Result | Rationale |\n|---------------|------------------|-----------|\n| `123` | `int(123)` | Clear integer literal |\n| `12.34` | `float(12.34)` | Clear float literal |\n| `true`, `yes`, `on` | `True` | Common boolean representations |\n| `false`, `no`, `off` | `False` | Common boolean representations |\n| `\"123\"` | `str(\"123\")` | Quoted values bypass inference |\n| `1.0.0` | `str(\"1.0.0\")` | Version strings stay as strings |\n\n### Common INI Parsing Pitfalls: Issues with Inline Comments, Quoted Values, and Section Nesting\n\nINI parsing appears deceptively simple, leading many developers to underestimate the edge cases and subtle behaviors that can cause parsing failures or incorrect data extraction. Understanding these common pitfalls helps build robust parsers and debug issues when they arise.\n\n⚠️ **Pitfall: Inline Comment Detection Breaking on Quoted Values**\n\nThe most frequent mistake in INI parsing occurs when handling inline comments that appear after values. Naive implementations split lines on comment characters (`;` or `#`) without considering whether those characters appear inside quoted strings.\n\nConsider this problematic line: `url = \"postgres://user:pass#123@host/db\" ; connection string`. A naive parser might treat `#123@host/db\" ; connection string` as a comment, corrupting the URL value. The correct behavior requires tracking quote state while scanning for comment markers.\n\nThis issue manifests in several ways: quoted passwords containing special characters get truncated, file paths with hash symbols get corrupted, and SQL connection strings become malformed. The fix requires implementing stateful scanning that only recognizes comment markers outside of quoted contexts.\n\nThe robust approach processes each character sequentially, maintaining quote state and only treating comment markers as significant when not inside string literals. This requires handling escape sequences within quotes to properly track quote boundaries.\n\n⚠️ **Pitfall: Assignment Operator Confusion in Values**\n\nAnother common error involves handling assignment operators (`=` or `:`) that appear within values rather than as key-value separators. Splitting lines on the first occurrence of these characters seems correct, but many implementations split on all occurrences, breaking values that contain assignment operators.\n\nFor example: `expression = x = y + z` should result in key `\"expression\"` with value `\"x = y + z\"`, not key `\"expression\"` with value `\"x\"` and some confused parsing of the remainder. The correct implementation splits only on the first occurrence of the assignment operator.\n\nThis pitfall extends to handling multiple assignment operators in configuration values. Mathematical expressions, code snippets, and connection strings frequently contain these characters as content rather than syntax.\n\n⚠️ **Pitfall: Section Name Validation and Escaping**\n\nSection headers require careful validation to prevent malformed input from corrupting the parser state or creating invalid nested structures. Common mistakes include accepting section names with unmatched brackets, allowing empty section names, or failing to handle escaped characters in section names.\n\nConsider these problematic section headers: `[section[nested]]`, `[]`, `[section\\nname]`. Each represents a different failure mode—nested brackets might indicate user confusion about syntax, empty names create ambiguous dictionary keys, and embedded newlines suggest multiline parsing errors.\n\nThe robust approach validates section names against a clear syntax specification, rejects malformed headers with helpful error messages, and handles any necessary escape sequences consistently with the overall parsing approach.\n\n⚠️ **Pitfall: Global Key Context Management**\n\nManaging the parsing context when transitioning between global keys and sectioned keys creates several failure modes. The most common involves incorrectly attributing keys to sections when encountering mixed global and sectioned content.\n\nConsider this structure:\n```ini\nglobal_key = value\n[section]\nsection_key = value\nanother_global = value\n```\n\nIncorrect implementations might attribute `another_global` to `[section]` because they fail to implement proper context switching. Keys appearing after sections should remain in their current section unless explicitly moved by another section header.\n\nThe fix requires maintaining clear section context throughout parsing and only changing context when section headers are encountered. Global keys appearing after sections indicate either user error (should be warned about) or intentional global configuration (should be supported).\n\n⚠️ **Pitfall: Value Trimming and Whitespace Semantics**\n\nDifferent approaches to whitespace handling create inconsistent behavior across different INI files. Some implementations trim all whitespace from values, others preserve it exactly, and still others apply complex rules about when trimming should occur.\n\nThe challenge emerges with values like `name = \" John Doe \"` where the spaces might be significant (indicating exact string content) or artifacts of formatting (should be trimmed). Quoted values suggest the spaces are intentional, but unquoted values are ambiguous.\n\nA consistent approach defines clear rules: unquoted values have leading and trailing whitespace trimmed, quoted values preserve all internal whitespace including leading/trailing spaces, and the quoting characters themselves are removed during processing.\n\n⚠️ **Pitfall: Line Continuation and Multiline Value Handling**\n\nSome INI dialects support line continuation with backslash characters or multiline values with specific syntax. Implementing this incorrectly breaks both simple and complex configurations.\n\nThe most common error involves treating continuation characters inside quoted strings as actual continuation markers rather than literal content. Another frequent mistake is failing to properly join continued lines while preserving meaningful whitespace.\n\nFor implementations that choose to support continuation, the logic must carefully distinguish between syntactic continuation (backslash at line end) and literal backslashes that happen to appear at line boundaries.\n\n### Implementation Guidance\n\nThe INI parser implementation focuses on building robust line-by-line processing while maintaining clean separation between tokenization, parsing, and data structure construction. This foundation prepares developers for more complex parsing challenges in TOML and YAML formats.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Line Processing | `str.splitlines()` with manual iteration | `io.StringIO` with buffered reading |\n| Pattern Recognition | String methods (`startswith`, `find`, `strip`) | `re` module with compiled patterns |\n| Value Type Conversion | Manual `int()`, `float()`, `bool()` attempts | `ast.literal_eval()` for safe evaluation |\n| Error Collection | Simple list of error strings | Structured `ParseError` objects with position |\n| String Parsing | Character-by-character state machine | `shlex` module for shell-like parsing |\n\n#### Recommended File Structure\n\n```\nconfig_parser/\n├── __init__.py\n├── common/\n│   ├── __init__.py\n│   ├── types.py              # Token, Position, ParseError definitions\n│   ├── tokenizer.py          # BaseTokenizer (from previous section)\n│   └── utils.py              # Shared utilities\n├── ini/\n│   ├── __init__.py\n│   ├── parser.py             # INIParser class - main implementation\n│   ├── validator.py          # INI-specific validation rules\n│   └── test_ini.py           # INI parser tests\n├── toml/                     # Future TOML implementation\n├── yaml/                     # Future YAML implementation\n└── main.py                   # CLI interface and format detection\n```\n\n#### Infrastructure Starter Code\n\n**File: `common/types.py`** (Complete implementation for shared types)\n\n```python\n\"\"\"\nShared type definitions for configuration parsers.\nThese types provide the foundation for all format-specific implementations.\n\"\"\"\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom typing import Any, List, Optional, Dict, Union\n\n\nclass TokenType(Enum):\n    \"\"\"Token types used across all configuration formats.\"\"\"\n    STRING = auto()\n    NUMBER = auto() \n    BOOLEAN = auto()\n    IDENTIFIER = auto()\n    EQUALS = auto()\n    COLON = auto()\n    NEWLINE = auto()\n    EOF = auto()\n    COMMENT = auto()\n    SECTION_START = auto()      # [\n    SECTION_END = auto()        # ]\n    ARRAY_START = auto()        # [\n    ARRAY_END = auto()          # ]\n    OBJECT_START = auto()       # {\n    OBJECT_END = auto()         # }\n    COMMA = auto()\n    DOT = auto()\n    INDENT = auto()\n    DEDENT = auto()\n    BLOCK_SEQUENCE = auto()     # YAML list marker\n    INVALID = auto()\n\n\n@dataclass\nclass Position:\n    \"\"\"Position information for tokens and errors.\"\"\"\n    line: int\n    column: int\n    offset: int\n\n    def __str__(self) -> str:\n        return f\"line {self.line}, column {self.column}\"\n\n\n@dataclass  \nclass Token:\n    \"\"\"A lexical token with type, value, and position information.\"\"\"\n    type: TokenType\n    value: Any\n    position: Position\n    raw_text: str\n\n    def __str__(self) -> str:\n        return f\"{self.type.name}({self.value!r}) at {self.position}\"\n\n\nclass ParseError(Exception):\n    \"\"\"Base class for all parsing errors.\"\"\"\n    def __init__(self, message: str, position: Optional[Position] = None, \n                 suggestion: Optional[str] = None):\n        self.message = message\n        self.position = position\n        self.suggestion = suggestion\n        super().__init__(self._format_message())\n    \n    def _format_message(self) -> str:\n        msg = self.message\n        if self.position:\n            msg = f\"{msg} at {self.position}\"\n        if self.suggestion:\n            msg = f\"{msg}\\nSuggestion: {self.suggestion}\"\n        return msg\n\n\nclass TokenError(ParseError):\n    \"\"\"Error during tokenization phase.\"\"\"\n    pass\n\n\nclass SyntaxError(ParseError):\n    \"\"\"Error in syntax structure.\"\"\"\n    pass\n\n\nclass StructureError(ParseError):\n    \"\"\"Error in logical structure (e.g., duplicate keys).\"\"\"\n    pass\n\n\n# Constants used across parsers\nEOF_MARKER = '\\0'\nWHITESPACE_CHARS = ' \\t\\r'\nNEWLINE_CHARS = '\\n'\nQUOTE_CHARS = '\"\\'`'\n```\n\n**File: `common/utils.py`** (Complete utility functions)\n\n```python\n\"\"\"\nUtility functions shared across all configuration parsers.\n\"\"\"\nfrom typing import Dict, Any, List\nfrom .types import Position, ParseError\n\n\ndef current_position(source: str, offset: int) -> Position:\n    \"\"\"Calculate line and column from string offset.\"\"\"\n    lines_before = source[:offset].count('\\n')\n    if lines_before == 0:\n        column = offset\n    else:\n        last_newline = source.rfind('\\n', 0, offset)\n        column = offset - last_newline - 1\n    \n    return Position(\n        line=lines_before + 1,\n        column=column + 1, \n        offset=offset\n    )\n\n\ndef create_error_context(source: str, position: Position, context_lines: int = 2) -> str:\n    \"\"\"Generate visual error context showing problematic line with pointer.\"\"\"\n    lines = source.splitlines()\n    if not lines or position.line > len(lines):\n        return \"Error context unavailable\"\n    \n    start_line = max(0, position.line - context_lines - 1)\n    end_line = min(len(lines), position.line + context_lines)\n    \n    context_parts = []\n    for i in range(start_line, end_line):\n        line_num = i + 1\n        prefix = \">>> \" if line_num == position.line else \"    \"\n        context_parts.append(f\"{prefix}{line_num:4d}: {lines[i]}\")\n        \n        # Add pointer line for error location\n        if line_num == position.line:\n            pointer_line = \" \" * (len(prefix) + 6 + position.column - 1) + \"^\"\n            context_parts.append(pointer_line)\n    \n    return \"\\n\".join(context_parts)\n\n\ndef normalize_key(key: str, strategy: str = \"lowercase\") -> str:\n    \"\"\"Apply key normalization strategy.\"\"\"\n    if strategy == \"lowercase\":\n        return key.lower().strip()\n    elif strategy == \"preserve\":\n        return key.strip()\n    elif strategy == \"snake_case\":\n        return key.lower().replace(\"-\", \"_\").replace(\" \", \"_\").strip()\n    else:\n        raise ValueError(f\"Unknown normalization strategy: {strategy}\")\n\n\ndef merge_nested_dicts(dict1: Dict[str, Any], dict2: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Deep merge nested dictionaries, with dict2 taking precedence.\"\"\"\n    result = dict1.copy()\n    \n    for key, value in dict2.items():\n        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n            result[key] = merge_nested_dicts(result[key], value)\n        else:\n            result[key] = value\n    \n    return result\n\n\ndef detect_format(content: str) -> str:\n    \"\"\"Automatically detect configuration format from content.\"\"\"\n    content = content.strip()\n    if not content:\n        return \"ini\"  # Default for empty files\n    \n    lines = [line.strip() for line in content.splitlines() if line.strip()]\n    if not lines:\n        return \"ini\"\n    \n    # YAML indicators\n    if any(line.startswith('- ') or ': ' in line and not line.startswith('[') \n           for line in lines):\n        # Check for YAML-specific patterns\n        if any(line and not line[0].isspace() and ': ' in line for line in lines):\n            return \"yaml\"\n    \n    # TOML indicators  \n    if any(line.startswith('[[') or ' = ' in line and '\"' in line \n           for line in lines):\n        return \"toml\"\n    \n    # Default to INI\n    return \"ini\"\n```\n\n#### Core Logic Skeleton Code\n\n**File: `ini/parser.py`** (Skeleton for learner implementation)\n\n```python\n\"\"\"\nINI format configuration parser.\nImplements line-based parsing for section headers and key-value pairs.\n\"\"\"\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom ..common.types import Position, ParseError, SyntaxError, StructureError\nfrom ..common.utils import current_position, normalize_key, create_error_context\n\n\nclass INIParser:\n    \"\"\"\n    Parser for INI format configuration files.\n    \n    Supports:\n    - Section headers: [section.name]  \n    - Key-value pairs: key = value, key: value\n    - Comments: ; comment, # comment\n    - Quoted values: key = \"value with spaces\"\n    - Type inference: automatic conversion to int, float, bool\n    \"\"\"\n    \n    def __init__(self, preserve_comments: bool = False, \n                 enable_nesting: bool = True,\n                 key_normalization: str = \"preserve\"):\n        self.preserve_comments = preserve_comments\n        self.enable_nesting = enable_nesting  \n        self.key_normalization = key_normalization\n        self.errors: List[ParseError] = []\n        \n    def parse(self, content: str) -> Dict[str, Any]:\n        \"\"\"\n        Parse INI format configuration content.\n        \n        Returns nested dictionary with sections as keys.\n        Global keys are stored under empty string key.\n        \"\"\"\n        # TODO 1: Initialize parsing state (result dict, current section, line tracking)\n        # TODO 2: Split content into logical lines while preserving line numbers  \n        # TODO 3: For each line, classify its type (section, key-value, comment, blank)\n        # TODO 4: Process each line type with appropriate handler method\n        # TODO 5: Handle any accumulated errors and return result\n        # Hint: Use self._classify_line() to determine line type\n        # Hint: Track current section context throughout parsing\n        pass\n    \n    def _classify_line(self, line: str, line_num: int) -> Tuple[str, str]:\n        \"\"\"\n        Classify a line as section, keyvalue, comment, or blank.\n        \n        Returns (line_type, processed_content) tuple.\n        line_type is one of: 'section', 'keyvalue', 'comment', 'blank'\n        \"\"\"\n        # TODO 1: Strip whitespace and check for blank lines\n        # TODO 2: Check for comment lines (starting with ; or #)\n        # TODO 3: Check for section headers (enclosed in [])\n        # TODO 4: Check for key-value pairs (containing = or :)\n        # TODO 5: Return appropriate classification\n        # Hint: Look for patterns after stripping whitespace\n        # Hint: Be careful about quoted strings containing special characters\n        pass\n        \n    def _process_section_header(self, line: str, line_num: int) -> str:\n        \"\"\"\n        Extract section name from header line like [section.name].\n        \n        Returns the section name, creating nested structure if dots present.\n        \"\"\"\n        # TODO 1: Extract content between [ and ] brackets\n        # TODO 2: Validate section name (not empty, valid characters)\n        # TODO 3: Handle dotted names for nested sections if nesting enabled\n        # TODO 4: Return normalized section name\n        # Hint: Check for unmatched brackets and report errors\n        # Hint: Consider whether to allow spaces in section names\n        pass\n        \n    def _process_key_value_pair(self, line: str, line_num: int, current_section: str, \n                               result: Dict[str, Any]) -> None:\n        \"\"\"\n        Parse key-value pair and store in result dictionary.\n        \n        Handles both = and : delimiters, quoted values, inline comments.\n        \"\"\"\n        # TODO 1: Find the assignment operator (= or :) - use first occurrence only\n        # TODO 2: Split line into key and value parts\n        # TODO 3: Process value part (handle quotes, inline comments, type inference)\n        # TODO 4: Store in appropriate section of result dictionary\n        # TODO 5: Handle any parsing errors gracefully\n        # Hint: Use self._parse_value() to handle complex value processing\n        # Hint: Be careful about assignment operators inside quoted strings\n        pass\n        \n    def _parse_value(self, value_str: str, line_num: int) -> Tuple[Any, Optional[str]]:\n        \"\"\"\n        Parse value string, handling quotes, escapes, and type inference.\n        \n        Returns (processed_value, inline_comment) tuple.\n        \"\"\"\n        # TODO 1: Detect and handle quoted strings (preserve exact content)\n        # TODO 2: Find inline comments (but not inside quoted strings)  \n        # TODO 3: Apply type inference to unquoted values\n        # TODO 4: Return processed value and any inline comment\n        # Hint: Need to track quote state when looking for comment markers\n        # Hint: Handle escape sequences in quoted strings\n        pass\n        \n    def _infer_type(self, value: str) -> Any:\n        \"\"\"\n        Convert string value to appropriate Python type.\n        \n        Attempts int, float, bool conversion in order.\n        \"\"\"\n        # TODO 1: Try integer conversion first (for values like \"123\")\n        # TODO 2: Try float conversion (for values like \"12.34\")  \n        # TODO 3: Try boolean conversion (true, false, yes, no, on, off)\n        # TODO 4: Fall back to string if all conversions fail\n        # Hint: Use try/except for conversion attempts\n        # Hint: Define boolean value mapping constants\n        pass\n        \n    def _create_nested_section(self, result: Dict[str, Any], section_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Create nested dictionary structure for dotted section names.\n        \n        For section \"database.connection\", creates result[\"database\"][\"connection\"].\n        Returns the innermost dictionary where keys should be stored.\n        \"\"\"\n        # TODO 1: Split section path on dots if nesting enabled\n        # TODO 2: Walk through path parts, creating nested dicts as needed\n        # TODO 3: Return reference to innermost dictionary\n        # TODO 4: Handle case where intermediate path conflicts with existing keys\n        # Hint: Check if intermediate keys are already non-dict values\n        # Hint: Consider whether empty section names are valid\n        pass\n\n# Additional helper functions for string processing\ndef _find_comment_start(value: str, start_pos: int = 0) -> Optional[int]:\n    \"\"\"Find start of inline comment, respecting quoted string boundaries.\"\"\"\n    # TODO: Implement stateful scanning for comment markers outside quotes\n    pass\n\ndef _process_quoted_string(quoted_str: str, quote_char: str) -> str:\n    \"\"\"Process escape sequences in quoted string.\"\"\"\n    # TODO: Handle \\n, \\t, \\\\, \\\", \\' escape sequences\n    pass\n```\n\n#### Language-Specific Hints\n\n**Python Implementation Notes:**\n- Use `str.partition('=')` instead of `str.split('=', 1)` for cleaner key-value splitting\n- The `configparser` module in standard library provides reference behavior, but implement from scratch for learning\n- `str.strip()` removes all whitespace; `str.lstrip(' \\t')` removes only spaces and tabs\n- For boolean inference: `{'true': True, 'false': False, 'yes': True, 'no': False, 'on': True, 'off': False}`\n- Use `collections.defaultdict(dict)` to automatically create nested sections\n- Regular expressions are overkill for INI parsing—string methods are sufficient and more readable\n\n**Error Handling Patterns:**\n```python\ntry:\n    result[section][key] = self._infer_type(value)\nexcept (ValueError, TypeError) as e:\n    error = SyntaxError(f\"Invalid value for {key}: {value}\", \n                       position=current_position(content, line_start_offset),\n                       suggestion=\"Check value format or use quotes for literal strings\")\n    self.errors.append(error)\n    result[section][key] = value  # Fall back to string value\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the INI parser, verify correct behavior with these test cases:\n\n**Test Command:** `python -m pytest ini/test_ini.py -v`\n\n**Manual Verification:**\n```python\nfrom ini.parser import INIParser\n\nparser = INIParser()\nconfig = parser.parse(\"\"\"\n# Global config\ndebug = true  \nport = 8080\n\n[database]\nhost = localhost\nport = 5432\npassword = \"secret#123\"  ; inline comment\n\n[database.pool]\nmin_connections = 5\nmax_connections = 20\n\"\"\")\n\n# Expected structure:\nassert config[\"\"] == {\"debug\": True, \"port\": 8080}\nassert config[\"database\"][\"host\"] == \"localhost\" \nassert config[\"database\"][\"port\"] == 5432\nassert config[\"database\"][\"password\"] == \"secret#123\"\nassert config[\"database\"][\"pool\"][\"min_connections\"] == 5\n```\n\n**Success Indicators:**\n- Global keys accessible via `config[\"\"]`\n- Section nesting creates proper dictionary hierarchy  \n- Type inference converts `\"true\"` to `True`, `\"123\"` to `123`\n- Quoted strings preserve exact content including special characters\n- Inline comments after values are ignored (unless preservation enabled)\n- Parser continues after errors and collects multiple issues\n\n**Common Issues to Debug:**\n- If all values are strings: Check `_infer_type()` implementation\n- If comments appear in values: Check quote state tracking in `_parse_value()`\n- If section nesting fails: Verify `_create_nested_section()` path splitting\n- If global keys missing: Ensure empty string key exists in result dictionary\n\n\n## TOML Parser Component Design\n\n> **Milestone(s):** TOML Tokenizer, TOML Parser - builds advanced tokenization capabilities and implements recursive descent parsing for tables, arrays, and complex type system\n\nThe TOML parser represents the most sophisticated component in our configuration parsing system, handling the complex interplay between explicit type systems, nested table structures, and array-of-tables syntax. Unlike the line-based simplicity of INI parsing, TOML parsing requires a full recursive descent approach that can manage hierarchical document structures while maintaining strict rules about key redefinition and table organization. This component demonstrates advanced parsing techniques including lookahead parsing, symbol table management, and context-sensitive grammar handling that form the foundation for understanding modern configuration language implementation.\n\nThe complexity of TOML parsing stems from its ambitious goal of being both human-readable and unambiguous for machine processing. While this creates a more robust configuration format, it introduces significant parsing challenges around table inheritance, dotted key expansion, and the distinction between inline tables and table headers. Understanding these concepts provides essential insight into how modern configuration languages balance expressiveness with parsing complexity.\n\n### TOML Parsing Mental Model\n\nThink of TOML parsing as **architectural blueprint interpretation** — you're reading a structured document that defines a building (your data structure) through a combination of room declarations (tables), furniture lists (arrays), and detailed specifications (key-value pairs). Just as an architect must understand that declaring a \"kitchen.island\" doesn't just create an island but also ensures the kitchen room exists, TOML parsing must understand that dotted keys implicitly create the table hierarchy they reference.\n\nThe key insight is that TOML operates on two levels simultaneously: the **document level** where you're declaring tables and organizing structure, and the **value level** where you're assigning specific data to keys. Unlike INI files where sections are simple containers, TOML tables have complex relationships — they can be nested, referenced by dotted paths, and must follow strict rules about redefinition and ordering.\n\nConsider this mental model: TOML parsing is like **managing a hierarchical filing system** where each table declaration creates a folder, each key-value pair files a document, and dotted notation creates nested folder structures. The parser must ensure that no document is filed twice in the same location (key redefinition error) and that folder creation follows proper hierarchy rules. Array-of-tables syntax is like creating multiple folders with the same name but different numeric suffixes — `reports-1/`, `reports-2/`, etc.\n\nThe **explicit type system** in TOML means the parser acts as both a filing clerk and a data validator. Unlike YAML's implicit typing where \"yes\" might become a boolean, TOML requires that strings look like strings (`\"hello\"`), integers look like integers (`42`), and dates follow specific formats (`1979-05-27T07:32:00Z`). This eliminates parsing ambiguity but requires sophisticated tokenization to distinguish between different literal formats.\n\n> The critical parsing insight is that TOML tables exist in a **global namespace** where dotted keys and table headers must be coordinated. When you write `physical.color = \"orange\"` followed by `[physical.size]`, the parser must recognize that both reference the same `physical` table and merge appropriately.\n\n### Recursive Descent Algorithm\n\nRecursive descent parsing provides the natural approach for handling TOML's nested structure by mapping grammatical rules directly to function calls. Each syntactic construct (table, key-value pair, array, inline table) corresponds to a parsing function that can call other parsing functions to handle nested elements. This creates a call stack that mirrors the document's hierarchical structure.\n\nThe **core recursive descent pattern** follows this structure: each parsing function consumes tokens that match its grammatical rule, recursively calls other parsing functions for nested elements, and returns a parse tree node representing the matched structure. For TOML, the top-level parser alternates between table declarations and key-value assignments, calling specialized functions for each construct type.\n\nHere's the algorithmic breakdown for TOML recursive descent parsing:\n\n1. **Initialize parsing state** with an empty symbol table for tracking defined keys and tables, current table context pointing to the root, and token stream positioned at the beginning\n2. **Main parsing loop** examines the current token to determine the construct type — section header brackets indicate table declarations, identifiers suggest key-value pairs, and EOF terminates parsing\n3. **Table declaration processing** extracts the table path from bracketed notation, validates that the path doesn't conflict with existing definitions, creates the table hierarchy if needed, and updates the current table context\n4. **Key-value pair processing** parses the key (which may be dotted), validates the key isn't already defined in the current table, parses the value recursively based on its type, and stores the key-value mapping\n5. **Dotted key expansion** treats dotted keys like nested table creation, ensuring each path segment creates a table if it doesn't exist, and placing the final key-value pair in the deepest table\n6. **Value parsing delegation** examines the token type and calls appropriate specialized parsers — `parse_string()` for quoted literals, `parse_array()` for bracket notation, `parse_inline_table()` for brace notation\n7. **Array-of-tables handling** creates a new table instance and appends it to an array stored under the table name, allowing multiple table definitions with the same path\n8. **Error recovery** captures parsing failures with position information, attempts to resynchronize at the next table boundary or key-value pair, and continues parsing to collect multiple error reports\n\nThe **recursive call pattern** emerges naturally from TOML's grammar. When `parse_value()` encounters an array, it calls `parse_array()`, which repeatedly calls `parse_value()` for each element. When parsing inline tables, `parse_inline_table()` calls `parse_key_value_pair()` for each entry. This recursive structure handles arbitrary nesting depth without explicit stack management.\n\n**Lookahead parsing** becomes essential for disambiguating TOML constructs. Distinguishing between table headers `[table]` and array-of-tables `[[table]]` requires examining two tokens ahead. Similarly, dotted keys require lookahead to determine whether `a.b.c` represents a single dotted key or the start of a more complex structure.\n\n> The recursive descent approach naturally handles TOML's context sensitivity — each parsing function maintains the current table context and key path, allowing nested calls to understand their position in the document hierarchy.\n\n| Parsing Function | Purpose | Recursive Calls | Returns |\n|-----------------|---------|----------------|---------|\n| `parse_document()` | Main entry point, processes top-level constructs | `parse_table_header()`, `parse_key_value_pair()` | `ParseNode` with document structure |\n| `parse_table_header()` | Handles `[table]` and `[[table]]` declarations | `parse_key_path()` | `SectionNode` with table information |\n| `parse_key_value_pair()` | Processes key assignments | `parse_key_path()`, `parse_value()` | `KeyValueNode` with key and value |\n| `parse_value()` | Dispatches to type-specific parsers | `parse_array()`, `parse_inline_table()`, `parse_string()` | `ValueNode` with typed value |\n| `parse_array()` | Handles `[item1, item2, item3]` syntax | `parse_value()` for each element | `ValueNode` with array contents |\n| `parse_inline_table()` | Handles `{key = value, key2 = value2}` syntax | `parse_key_value_pair()` for each entry | `ValueNode` with table contents |\n| `parse_key_path()` | Handles dotted keys like `a.b.c` | None (terminal parser) | List of key segments |\n\n### Table and Array-of-Tables Logic\n\nTOML's table system represents one of the most complex aspects of configuration file parsing, requiring careful management of hierarchical namespaces, implicit table creation, and the distinction between table headers and array-of-tables declarations. The parser must maintain a global view of the document structure while processing local key-value assignments.\n\n**Table hierarchy management** operates through a symbol table that tracks all defined tables and keys. When the parser encounters a table header like `[database.connection]`, it must verify that neither `database` nor `database.connection` has been previously defined as a non-table value, create any missing intermediate tables, and establish the new current context for subsequent key-value pairs. This process involves both validation and construction phases.\n\nThe **implicit table creation** rule states that dotted keys automatically create the necessary table hierarchy. When processing `server.host = \"localhost\"` before any `[server]` declaration, the parser must create an implicit `server` table and place the `host` key within it. This implicit creation must be tracked separately from explicit table declarations to handle later conflicts correctly.\n\n**Array-of-tables syntax** using double brackets `[[database.servers]]` creates a fundamentally different structure than regular tables. Each array-of-tables declaration appends a new table instance to an array stored under that key path. The parser must distinguish between extending an existing array-of-tables and conflicting with a previously defined single table or value.\n\nHere's the detailed algorithm for table and array-of-tables processing:\n\n1. **Table header detection** recognizes bracket notation and determines whether single brackets indicate a table declaration or double brackets indicate array-of-tables entry\n2. **Path validation** checks that the table path doesn't conflict with existing definitions — a path can't be redefined as a different type, and array-of-tables paths must be consistent\n3. **Intermediate table creation** ensures all parent tables in a dotted path exist, creating them implicitly if necessary, and marking them as implicit to allow later explicit redefinition\n4. **Array-of-tables instantiation** creates a new table instance for double-bracket notation, appends it to the array stored under that path, and sets the new table as the current parsing context\n5. **Context switching** updates the current table pointer to reflect the newly declared or accessed table, allowing subsequent key-value pairs to be stored in the correct location\n6. **Conflict detection** validates that new table declarations don't redefine existing keys or tables, that array-of-tables declarations are consistent with previous usage, and that implicit tables can be explicitly redefined\n\n**Dotted key expansion** requires special handling because it creates table structure inline with value assignment. When processing `physical.color = \"orange\"`, the parser must create a `physical` table if it doesn't exist, then assign the `color` key within that table. This expansion must respect the same conflict rules as explicit table declarations.\n\nThe **table redefinition rules** create complex validation requirements. A table can only be explicitly defined once, but keys can be added to tables from multiple locations. Array-of-tables can be extended with multiple `[[array.name]]` declarations, but the same path can't mix array-of-tables and regular table syntax.\n\n> The key insight is that TOML tables form a **global namespace** where all dotted paths must be mutually consistent. Unlike INI sections that are independent containers, TOML tables are interconnected through their hierarchical relationships and shared namespace.\n\n| Table Operation | Validation Required | Action Taken | Error Conditions |\n|----------------|-------------------|--------------|------------------|\n| `[explicit.table]` | Path not previously defined as table | Create table, set as current context | Path exists as value or different table type |\n| `[[array.of.tables]]` | Path used consistently for arrays | Create table, append to array | Path exists as single table or value |\n| Dotted key assignment | Intermediate paths are compatible | Create implicit tables, assign value | Intermediate path conflicts with existing value |\n| Inline table `{k=v}` | Table contents don't conflict | Create table with all key-value pairs | Keys conflict with existing definitions in scope |\n\n### TOML Architecture Decisions\n\nThe TOML parser's architecture must balance parsing complexity with maintainability, requiring careful decisions about tokenization depth, error handling strategies, and data structure representation. These architectural choices significantly impact both implementation difficulty and runtime performance.\n\n> **Decision: Two-Pass vs Single-Pass Parsing**\n> - **Context**: TOML's global namespace and table redefinition rules require comprehensive validation that may benefit from multiple parsing phases\n> - **Options Considered**: Single-pass parsing with complex state tracking, two-pass parsing with structure building then validation, three-pass parsing with tokenization, structure building, and validation phases\n> - **Decision**: Single-pass parsing with comprehensive symbol table management\n> - **Rationale**: Single-pass parsing provides better memory efficiency and simpler error reporting, while comprehensive symbol tables can handle the required validation without multiple document traversals. The complexity of multi-pass coordination outweighs the benefits for our educational implementation.\n> - **Consequences**: Requires sophisticated symbol table implementation but provides linear time complexity and straightforward error position reporting\n\n| Parsing Approach | Memory Usage | Time Complexity | Error Reporting Quality | Implementation Complexity |\n|------------------|--------------|-----------------|------------------------|---------------------------|\n| Single-pass with symbol table | O(n) | O(n) | Excellent (exact positions) | High (complex state management) |\n| Two-pass parsing | O(2n) | O(2n) | Good (requires position tracking) | Medium (separate phases) |\n| Three-pass parsing | O(3n) | O(3n) | Excellent (multiple validation passes) | Low (simple individual passes) |\n\n> **Decision: Recursive Descent vs Parser Generator**\n> - **Context**: TOML's grammar complexity could be handled by hand-written recursive descent or automated parser generation tools\n> - **Options Considered**: Hand-written recursive descent parser, ANTLR or similar parser generator, hybrid approach with generated lexer and hand-written parser\n> - **Decision**: Hand-written recursive descent parser\n> - **Rationale**: Educational value of understanding parsing mechanics outweighs the convenience of generated parsers. Recursive descent provides clear mapping from grammar rules to code, making debugging and extension more accessible to learners.\n> - **Consequences**: Higher implementation effort but better learning outcomes and complete control over error messages and recovery strategies\n\n> **Decision: Token Stream vs Character Stream Parsing**\n> - **Context**: The parser can operate on pre-tokenized input from the tokenizer component or consume characters directly during parsing\n> - **Options Considered**: Pre-tokenized input with full lookahead, character stream with on-demand tokenization, hybrid approach with token buffering\n> - **Decision**: Pre-tokenized input with bounded lookahead\n> - **Rationale**: Separation of lexical analysis and syntactic analysis improves modularity and debugging. Pre-tokenization enables better error messages with token-level position tracking and simplifies parser logic by eliminating character-level concerns.\n> - **Consequences**: Higher memory usage for token storage but cleaner parser implementation and superior error reporting capabilities\n\n**Symbol table architecture** requires careful consideration of how to represent the hierarchical namespace and track different types of definitions (explicit tables, implicit tables, array-of-tables, values). The symbol table must support efficient lookup, conflict detection, and context switching as the parser moves between different table scopes.\n\nThe chosen approach uses a **nested dictionary structure** mirroring the final output format, with additional metadata tracking the definition type and location for each entry. This provides O(1) lookup for conflict detection while building the final data structure incrementally during parsing.\n\n> **Decision: Parse Tree vs Direct Output Generation**\n> - **Context**: The parser can build an intermediate parse tree structure or generate the final nested dictionary directly during parsing\n> - **Options Considered**: Full parse tree with separate transformation phase, direct output generation during parsing, hybrid approach with minimal intermediate representation\n> - **Decision**: Direct output generation with conflict tracking metadata\n> - **Rationale**: Direct generation reduces memory usage and eliminates an additional transformation pass. The conflict tracking metadata provides necessary validation capabilities without full parse tree overhead.\n> - **Consequences**: More complex parsing logic but better performance and memory efficiency for large configuration files\n\n**Error recovery strategy** determines how the parser responds to syntax errors and whether it attempts to continue parsing to report multiple errors. The architecture must balance comprehensive error reporting with parsing simplicity and reasonable recovery behavior.\n\nThe implemented approach uses **panic-mode recovery** where the parser skips tokens until it reaches a synchronization point (typically the start of the next table or key-value pair), then resumes parsing. This provides multiple error reports while maintaining reasonable parsing state consistency.\n\n### Common TOML Parsing Pitfalls\n\nUnderstanding the frequent mistakes in TOML parsing implementation helps avoid subtle bugs that can produce incorrect results or confusing error messages. These pitfalls often arise from the interaction between TOML's powerful features and the complexity they introduce in parser implementation.\n\n⚠️ **Pitfall: Ignoring Table Redefinition Rules**\n\nMany implementations incorrectly allow table redefinition or fail to properly distinguish between extending a table and redefining it. In TOML, once a table is explicitly declared with `[table.name]`, it cannot be redeclared, but additional keys can be added from other locations through dotted key notation.\n\nThe error manifests when parsing documents like:\n```\n[server]\nhost = \"localhost\"\n\n[server]  # This should be an error - table redefinition\nport = 8080\n```\n\n**Why it's wrong**: TOML specification explicitly forbids table redefinition to eliminate ambiguity about key placement and table structure. Allowing redefinition makes document interpretation dependent on declaration order.\n\n**How to fix**: Maintain a set of explicitly declared table paths and check each new table declaration against this set. Distinguish between explicit table declarations (`[table]`) and implicit table creation through dotted keys.\n\n⚠️ **Pitfall: Incorrect Array-of-Tables Mixing**\n\nImplementations often fail to properly validate that array-of-tables declarations are used consistently. Mixing `[table]` and `[[table]]` syntax for the same path should produce an error, but many parsers incorrectly allow this or handle it inconsistently.\n\n**Why it's wrong**: The distinction between single tables and array-of-tables is fundamental to TOML's type system. Mixing these creates ambiguous document interpretation and violates the specification's clarity goals.\n\n**How to fix**: Track whether each table path is used as a single table or array-of-tables and validate consistency across all declarations. Maintain separate metadata for table type alongside the symbol table entries.\n\n⚠️ **Pitfall: Improper Dotted Key Expansion**\n\nMany implementations incorrectly handle the interaction between dotted keys and table declarations. When processing `a.b.c = \"value\"`, the parser must create implicit tables for `a` and `a.b`, but these implicit tables must be compatible with later explicit table declarations.\n\nThe complexity arises in sequences like:\n```\nphysical.color = \"orange\"  # Creates implicit [physical] table\n[physical.dimensions]      # Must extend the existing physical table\n[physical]                 # Should be error - can't redefine implicit table explicitly\n```\n\n**Why it's wrong**: Incorrect dotted key expansion violates TOML's namespace consistency rules and can lead to key placement in wrong table locations.\n\n**How to fix**: Mark implicitly created tables with metadata indicating their creation method, allow extension through additional dotted keys or sub-table declarations, but forbid explicit redeclaration of implicit tables.\n\n⚠️ **Pitfall: Inadequate Inline Table Validation**\n\nInline tables using `{key = value, key2 = value2}` syntax have special scoping rules that many parsers implement incorrectly. Inline tables must be completely defined in their declaration and cannot be extended later through dotted keys or additional table declarations.\n\n**Why it's wrong**: TOML treats inline tables as atomic units that cannot be modified after declaration, ensuring document clarity and preventing ambiguous key placement.\n\n**How to fix**: Mark inline tables as immutable in the symbol table and validate that no subsequent operations attempt to add keys to or modify inline table contents.\n\n⚠️ **Pitfall: Insufficient Unicode and Escape Sequence Handling**\n\nTOML's string literals support complex escape sequences including Unicode code points, and many implementations handle these incorrectly or incompletely. This is particularly problematic for multiline strings where escape sequence rules differ between basic strings and literal strings.\n\n**Why it's wrong**: Incorrect escape handling can corrupt string values, cause parsing failures on valid documents, or create security vulnerabilities through improper Unicode handling.\n\n**How to fix**: Implement complete escape sequence processing according to TOML specification, handle Unicode normalization properly, and validate escape sequences during tokenization rather than during parsing.\n\n⚠️ **Pitfall: Poor Error Message Context**\n\nTOML parsing errors often involve complex interactions between different parts of the document (table redefinition, key conflicts, type mismatches), but many implementations provide error messages that don't include sufficient context to understand the problem.\n\n**Why it's wrong**: Poor error messages make debugging configuration files extremely difficult, especially for users who may not understand TOML's complex rules about table definitions and key conflicts.\n\n**How to fix**: Include both the error location and the conflicting previous definition location in error messages. Provide specific information about what rules were violated and suggestions for fixing the problem.\n\n![TOML Table Parsing Sequence](./diagrams/toml-parsing-sequence.svg)\n\n### Implementation Guidance\n\nThe TOML parser implementation requires sophisticated recursive descent techniques combined with comprehensive symbol table management. This guidance provides the foundation for building a parser that correctly handles TOML's complex table hierarchy and type system while maintaining clear separation between tokenization and parsing concerns.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Recursive Descent Framework | Manual recursive function calls with error handling | Parser combinator library with automatic backtracking |\n| Symbol Table | Nested dictionaries with metadata annotations | Custom symbol table class with scope management |\n| Type System | Python native types with isinstance() checking | Custom TOML type classes with validation methods |\n| Error Reporting | Exception-based with position information | Error accumulation with multiple error reporting |\n| Token Consumption | Linear token stream with index tracking | Token stream class with lookahead buffers |\n\n#### Recommended File Structure\n\n```\nproject-root/\n  parsers/\n    base_parser.py              ← Abstract base for all parsers\n    toml_parser.py              ← Main TOML recursive descent parser\n    toml_types.py               ← TOML-specific type definitions and validation\n  tokenizers/\n    toml_tokenizer.py           ← TOML tokenizer (from previous milestone)\n  data_structures/\n    parse_nodes.py              ← ParseNode hierarchy definitions\n    symbol_table.py             ← Symbol table for conflict tracking\n  tests/\n    test_toml_parser.py         ← Comprehensive TOML parser tests\n    fixtures/\n      toml_examples/            ← TOML test files for each feature\n        basic_tables.toml\n        array_of_tables.toml\n        dotted_keys.toml\n        inline_structures.toml\n```\n\n#### Infrastructure Starter Code\n\n**Symbol Table Implementation (`data_structures/symbol_table.py`):**\n```python\nfrom typing import Dict, List, Any, Optional, Set\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass DefinitionType(Enum):\n    EXPLICIT_TABLE = \"explicit_table\"\n    IMPLICIT_TABLE = \"implicit_table\"\n    ARRAY_OF_TABLES = \"array_of_tables\"\n    VALUE = \"value\"\n    INLINE_TABLE = \"inline_table\"\n\n@dataclass\nclass DefinitionInfo:\n    definition_type: DefinitionType\n    position: 'Position'\n    is_mutable: bool = True\n    \nclass SymbolTable:\n    \"\"\"Tracks all defined keys and tables for conflict detection.\"\"\"\n    \n    def __init__(self):\n        self.definitions: Dict[str, DefinitionInfo] = {}\n        self.data: Dict[str, Any] = {}\n        self.current_table_path: List[str] = []\n    \n    def register_definition(self, key_path: List[str], definition_type: DefinitionType, position: 'Position') -> None:\n        \"\"\"Register a new definition and validate for conflicts.\"\"\"\n        path_str = '.'.join(key_path)\n        existing = self.definitions.get(path_str)\n        \n        if existing:\n            # Validate compatibility based on TOML rules\n            if definition_type == DefinitionType.EXPLICIT_TABLE and existing.definition_type == DefinitionType.IMPLICIT_TABLE:\n                # Can explicitly define an implicit table\n                self.definitions[path_str] = DefinitionInfo(definition_type, position)\n                return\n            elif definition_type == DefinitionType.ARRAY_OF_TABLES and existing.definition_type == DefinitionType.ARRAY_OF_TABLES:\n                # Can extend array of tables\n                return\n            else:\n                raise StructureError(f\"Cannot redefine {path_str} as {definition_type}\", position)\n        \n        self.definitions[path_str] = DefinitionInfo(definition_type, position)\n    \n    def create_nested_structure(self, key_path: List[str]) -> Dict[str, Any]:\n        \"\"\"Create nested dictionary structure for the given path.\"\"\"\n        current = self.data\n        for segment in key_path[:-1]:\n            if segment not in current:\n                current[segment] = {}\n            current = current[segment]\n        return current\n    \n    def set_current_table(self, table_path: List[str]) -> None:\n        \"\"\"Set the current table context for key-value assignments.\"\"\"\n        self.current_table_path = table_path.copy()\n    \n    def get_current_table(self) -> Dict[str, Any]:\n        \"\"\"Get the dictionary representing the current table.\"\"\"\n        if not self.current_table_path:\n            return self.data\n        \n        current = self.data\n        for segment in self.current_table_path:\n            if segment not in current:\n                current[segment] = {}\n            current = current[segment]\n        return current\n```\n\n**TOML Type Validation (`parsers/toml_types.py`):**\n```python\nimport re\nfrom datetime import datetime, date, time\nfrom typing import Any, Union, List, Dict\n\nclass TOMLTypeValidator:\n    \"\"\"Validates and converts TOML literal values to appropriate Python types.\"\"\"\n    \n    # Regex patterns for TOML literals\n    INTEGER_PATTERN = re.compile(r'^[+-]?(?:0|[1-9](?:_?\\d)*)$')\n    FLOAT_PATTERN = re.compile(r'^[+-]?(?:0|[1-9](?:_?\\d)*)(?:\\.(?:\\d(?:_?\\d)*)?)?(?:[eE][+-]?\\d(?:_?\\d)*)?$')\n    BOOLEAN_PATTERN = re.compile(r'^(?:true|false)$')\n    DATETIME_PATTERN = re.compile(r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?(?:Z|[+-]\\d{2}:\\d{2})$')\n    \n    @classmethod\n    def validate_and_convert(cls, token: 'Token') -> Any:\n        \"\"\"Convert token value to appropriate Python type based on TOML rules.\"\"\"\n        if token.type == TokenType.STRING:\n            return cls._process_string_literal(token)\n        elif token.type == TokenType.NUMBER:\n            return cls._process_numeric_literal(token)\n        elif token.type == TokenType.BOOLEAN:\n            return token.value == 'true'\n        else:\n            return token.value\n    \n    @classmethod\n    def _process_string_literal(cls, token: 'Token') -> str:\n        \"\"\"Process string literal with proper escape sequence handling.\"\"\"\n        raw_value = token.raw_text\n        \n        if raw_value.startswith('\"\"\"') or raw_value.startswith(\"'''\"):\n            # Multiline string - handle special rules\n            return cls._process_multiline_string(raw_value)\n        elif raw_value.startswith('\"'):\n            # Basic string - process escape sequences\n            return cls._process_basic_string(raw_value[1:-1])  # Remove quotes\n        elif raw_value.startswith(\"'\"):\n            # Literal string - no escape processing\n            return raw_value[1:-1]  # Remove quotes, no escape processing\n        \n        return raw_value\n    \n    @classmethod\n    def _process_basic_string(cls, content: str) -> str:\n        \"\"\"Process escape sequences in basic strings.\"\"\"\n        # Handle standard escape sequences: \\n, \\t, \\r, \\\\, \\\", etc.\n        escape_map = {\n            'n': '\\n', 't': '\\t', 'r': '\\r', '\\\\': '\\\\', '\"': '\"',\n            'b': '\\b', 'f': '\\f', '/': '/'\n        }\n        \n        result = []\n        i = 0\n        while i < len(content):\n            if content[i] == '\\\\' and i + 1 < len(content):\n                next_char = content[i + 1]\n                if next_char in escape_map:\n                    result.append(escape_map[next_char])\n                    i += 2\n                elif next_char == 'u' and i + 5 < len(content):\n                    # Unicode escape sequence \\uXXXX\n                    unicode_hex = content[i+2:i+6]\n                    result.append(chr(int(unicode_hex, 16)))\n                    i += 6\n                elif next_char == 'U' and i + 9 < len(content):\n                    # Unicode escape sequence \\UXXXXXXXX\n                    unicode_hex = content[i+2:i+10]\n                    result.append(chr(int(unicode_hex, 16)))\n                    i += 10\n                else:\n                    # Invalid escape sequence\n                    result.append(content[i])\n                    i += 1\n            else:\n                result.append(content[i])\n                i += 1\n        \n        return ''.join(result)\n```\n\n#### Core Logic Skeleton\n\n**Main TOML Parser (`parsers/toml_parser.py`):**\n```python\nfrom typing import List, Dict, Any, Optional\nfrom ..tokenizers.base_tokenizer import Token, TokenType\nfrom ..data_structures.parse_nodes import ParseNode, SectionNode, KeyValueNode, ValueNode\nfrom ..data_structures.symbol_table import SymbolTable, DefinitionType\nfrom .base_parser import BaseParser\n\nclass TOMLParser(BaseParser):\n    \"\"\"Recursive descent parser for TOML configuration format.\"\"\"\n    \n    def __init__(self, tokens: List[Token]):\n        super().__init__(tokens)\n        self.symbol_table = SymbolTable()\n        self.current_token_index = 0\n    \n    def parse(self, content: str) -> Dict[str, Any]:\n        \"\"\"Main TOML parsing entry point - implements recursive descent algorithm.\"\"\"\n        # TODO 1: Initialize tokenizer and generate token stream from content\n        # TODO 2: Reset parser state (symbol table, current position, etc.)\n        # TODO 3: Enter main parsing loop - process document-level constructs\n        # TODO 4: Handle end-of-file and validate final document state  \n        # TODO 5: Return the nested dictionary structure from symbol table\n        # Hint: Main loop alternates between table declarations and key-value pairs\n        pass\n    \n    def parse_document_level_construct(self) -> Optional[ParseNode]:\n        \"\"\"Parse top-level TOML constructs (tables, key-value pairs, comments).\"\"\"\n        # TODO 1: Skip whitespace and comments to find next meaningful token\n        # TODO 2: Check for EOF condition and return None if document complete\n        # TODO 3: Examine current token to determine construct type\n        # TODO 4: Dispatch to appropriate parsing method based on token type\n        # TODO 5: Handle unexpected tokens with informative error messages\n        # Hint: Use lookahead to distinguish [table] from [[array-of-tables]]\n        pass\n    \n    def parse_table_header(self) -> SectionNode:\n        \"\"\"Parse [table] and [[table.name]] declarations with hierarchy validation.\"\"\"\n        # TODO 1: Determine if single bracket [table] or double bracket [[table]]\n        # TODO 2: Parse the table path (may be dotted like table.subtable.name)\n        # TODO 3: Validate table path doesn't conflict with existing definitions\n        # TODO 4: Register new table definition in symbol table with proper type\n        # TODO 5: Create table hierarchy and set as current parsing context\n        # TODO 6: Handle array-of-tables by creating new table instance and appending\n        # Hint: Array-of-tables creates multiple table instances under same path\n        pass\n    \n    def parse_table_path(self) -> List[str]:\n        \"\"\"Parse dotted table paths like 'database.connection.pool'.\"\"\"\n        # TODO 1: Start with first identifier token as initial path segment  \n        # TODO 2: Check for DOT token indicating additional path segments\n        # TODO 3: Consume DOT and parse next identifier in the path\n        # TODO 4: Continue until no more DOT tokens found\n        # TODO 5: Validate each path segment is a valid identifier\n        # TODO 6: Return list of path segments for table creation\n        # Hint: Handle quoted identifiers that may contain special characters\n        pass\n    \n    def parse_key_value_pair(self) -> KeyValueNode:\n        \"\"\"Parse key assignments including dotted keys and complex values.\"\"\"\n        # TODO 1: Parse the key path (may be dotted like 'server.database.host')\n        # TODO 2: Consume EQUALS token and validate assignment operator\n        # TODO 3: Parse the value recursively based on its type\n        # TODO 4: Handle dotted key expansion - create implicit tables as needed\n        # TODO 5: Validate key doesn't already exist in target table\n        # TODO 6: Store key-value mapping in appropriate table location\n        # Hint: Dotted keys create nested table structure automatically\n        pass\n    \n    def parse_value(self) -> ValueNode:\n        \"\"\"Dispatch to type-specific value parsers based on token examination.\"\"\"\n        # TODO 1: Examine current token type to determine value category\n        # TODO 2: Dispatch to appropriate specialized parser method\n        # TODO 3: Handle arrays by calling parse_array() for bracket notation\n        # TODO 4: Handle inline tables by calling parse_inline_table() for braces\n        # TODO 5: Handle literal values (strings, numbers, booleans, dates)\n        # TODO 6: Validate parsed value conforms to TOML type system\n        # Hint: Use lookahead for ambiguous cases like negative numbers vs expressions\n        pass\n    \n    def parse_array(self) -> ValueNode:\n        \"\"\"Parse array literals [item1, item2, item3] with mixed type support.\"\"\"\n        # TODO 1: Consume opening ARRAY_START bracket token\n        # TODO 2: Handle empty array case (immediate closing bracket)\n        # TODO 3: Parse first array element by recursively calling parse_value()\n        # TODO 4: Loop to parse additional elements separated by COMMA tokens\n        # TODO 5: Handle trailing commas (optional in TOML arrays)\n        # TODO 6: Consume closing ARRAY_END bracket and validate structure\n        # TODO 7: Create ValueNode with array contents and return\n        # Hint: TOML allows mixed-type arrays unlike some formats\n        pass\n    \n    def parse_inline_table(self) -> ValueNode:\n        \"\"\"Parse inline table syntax {key1 = value1, key2 = value2}.\"\"\"\n        # TODO 1: Consume opening OBJECT_START brace token\n        # TODO 2: Handle empty table case (immediate closing brace)\n        # TODO 3: Parse first key-value pair within the inline table\n        # TODO 4: Loop to parse additional pairs separated by COMMA tokens  \n        # TODO 5: Validate inline table keys don't conflict with each other\n        # TODO 6: Consume closing OBJECT_END brace and create table structure\n        # TODO 7: Mark inline table as immutable in symbol table\n        # Hint: Inline tables cannot be extended after declaration\n        pass\n    \n    def validate_table_redefinition(self, table_path: List[str], definition_type: DefinitionType) -> None:\n        \"\"\"Validate new table definition against TOML redefinition rules.\"\"\"\n        # TODO 1: Convert table path to string representation for lookup\n        # TODO 2: Check if path already exists in symbol table definitions\n        # TODO 3: Apply TOML rules for valid redefinition cases\n        # TODO 4: Allow explicit definition of previously implicit tables\n        # TODO 5: Allow extension of array-of-tables with consistent syntax\n        # TODO 6: Raise StructureError for invalid redefinition attempts\n        # Hint: Different definition types have different redefinition rules\n        pass\n    \n    def expand_dotted_key(self, key_path: List[str], value: Any) -> None:\n        \"\"\"Create nested table structure for dotted key assignment.\"\"\"\n        # TODO 1: Validate each segment of key path for conflicts\n        # TODO 2: Create implicit tables for intermediate path segments\n        # TODO 3: Register implicit table definitions in symbol table\n        # TODO 4: Navigate to or create the target table for value assignment\n        # TODO 5: Validate final key doesn't already exist in target table\n        # TODO 6: Store the key-value pair in the appropriate nested location\n        # Hint: Dotted keys can create deeply nested structures automatically\n        pass\n```\n\n#### Milestone Checkpoints\n\n**After implementing TOML tokenizer integration:**\n```bash\npython -m pytest tests/test_toml_parser.py::TestTOMLTokenizerIntegration -v\n```\nExpected: All token types correctly recognized, string literals properly parsed, numeric values tokenized with correct types.\n\n**After implementing basic table parsing:**\n```bash\npython -c \"\nfrom parsers.toml_parser import TOMLParser\nresult = TOMLParser().parse('[server]\\nhost = \\\"localhost\\\"\\nport = 8080')\nprint('Success:', result == {'server': {'host': 'localhost', 'port': 8080}})\n\"\n```\nExpected: `Success: True` with proper nested dictionary structure.\n\n**After implementing array-of-tables:**\n```bash\npython -c \"\nfrom parsers.toml_parser import TOMLParser\ntoml_content = '''\n[[database.servers]]\nip = \\\"192.168.1.1\\\"\ndc = \\\"eqdc10\\\"\n\n[[database.servers]]  \nip = \\\"192.168.1.2\\\"\ndc = \\\"eqdc10\\\"\n'''\nresult = TOMLParser().parse(toml_content)\nprint('Array length:', len(result['database']['servers']))\nprint('First server IP:', result['database']['servers'][0]['ip'])\n\"\n```\nExpected: `Array length: 2` and `First server IP: 192.168.1.1`.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"Key redefinition error\" on valid TOML | Incorrect table conflict detection | Check symbol table state when error occurs | Review table redefinition validation rules |\n| Array-of-tables creates single table instead of array | Missing double-bracket detection logic | Examine tokenizer output for bracket tokens | Implement proper lookahead for `[[` vs `[` |\n| Dotted keys create incorrect nesting | Improper key path expansion | Trace key path parsing and table creation | Debug dotted key expansion algorithm step by step |\n| Inline tables allow extension (should be immutable) | Missing immutability enforcement | Check if inline tables marked as immutable | Add immutability validation in symbol table |\n| Complex nested structures parse incorrectly | Recursive descent state management issues | Print parser state at each recursive call | Review context switching and current table tracking |\n| Unicode strings corrupted during parsing | Incorrect escape sequence processing | Test with known Unicode test cases | Debug string literal processing in type validator |\n\n\n## YAML Parser Component Design\n\n> **Milestone(s):** YAML Subset Parser - implements indentation-sensitive parsing for mappings, sequences, and scalar values with automatic type inference\n\nThe YAML parser represents the most conceptually different parsing challenge in our configuration file parser suite. While INI parsing follows line-based rules and TOML parsing uses explicit delimiters, YAML parsing requires understanding **indentation-driven hierarchical structure** where whitespace itself carries semantic meaning. This fundamental difference demands a completely different parsing approach built around stack-based nesting management and context-sensitive interpretation.\n\n![YAML Indentation Stack Management](./diagrams/yaml-indentation-stack.svg)\n\n### YAML Parsing Mental Model\n\nThink of YAML parsing like reading a well-structured outline or book table of contents. The indentation level tells you exactly where you are in the hierarchy - a chapter, section, subsection, or bullet point. Just as you would track your current nesting level while reading an outline, the YAML parser maintains a **stack of indentation contexts** that grows and shrinks as the document flows through different nesting levels.\n\nConsider this mental model: imagine you're organizing physical file folders on a desk. Each level of indentation represents placing a folder inside another folder. When you encounter a line with less indentation, you're \"backing out\" of nested folders until you reach the appropriate parent folder. When you encounter deeper indentation, you're \"diving into\" a subfolder structure. The parser maintains this folder stack in memory, always knowing exactly which \"folder\" (context) it's currently processing.\n\nThe **indentation-driven hierarchical structure** means that YAML parsing is fundamentally about tracking these context transitions. Unlike TOML where table boundaries are explicit (`[table.name]`) or INI where sections are clearly marked (`[section]`), YAML structure emerges from the spatial relationship between lines. This creates both elegance and complexity - the format reads naturally to humans but requires sophisticated state tracking for machines.\n\nThe parser must continuously answer three questions: \"What nesting level am I at?\", \"Am I entering a deeper level or returning to a shallower one?\", and \"What type of structure am I building at this level?\" This context sensitivity means that the same line can mean completely different things depending on the indentation stack state when it's encountered.\n\n### Indentation-Based Parsing Algorithm\n\nThe **stack-based approach** forms the algorithmic foundation of YAML parsing. The parser maintains an **indentation stack** where each stack frame represents a nesting level with its indentation amount, structure type (mapping or sequence), and the data being built at that level. This approach handles the complex dance of tracking when structures begin, continue, and end based purely on whitespace changes.\n\nThe core algorithm operates through a **structure transition** state machine that processes each logical line by comparing its indentation against the current stack state:\n\n1. **Tokenize the logical line** into its component parts: indentation amount, content type (key-value pair, sequence item, or scalar), and the actual content values. This tokenization must handle both block-style syntax (indentation-based) and flow-style syntax (bracket and brace delimited inline structures).\n\n2. **Calculate indentation level** by counting leading whitespace characters. YAML strictly forbids mixing tabs and spaces for indentation, so this calculation must validate consistency and reject mixed whitespace. The indentation amount determines the structural relationship to previously parsed content.\n\n3. **Compare current indentation to stack state** to determine the structural transition type. If indentation increases, we're entering a nested structure. If indentation decreases, we're exiting one or more nested structures. If indentation matches the top of stack, we're continuing the current structure at the same level.\n\n4. **Handle stack transitions** based on the indentation comparison. For increased indentation, push a new stack frame for the nested structure. For decreased indentation, pop stack frames until reaching the matching indentation level, completing any nested structures during the unwinding process.\n\n5. **Process the line content** within the appropriate structural context. Key-value pairs create or extend mapping structures. Sequence items (lines starting with dash) create or extend list structures. Scalar values get processed through type inference and stored in the current structure.\n\n6. **Validate structural consistency** by ensuring that the indentation level matches exactly with a previous level when returning to a shallower nesting. YAML prohibits \"dedenting\" to indentation levels that were never established, preventing malformed structural transitions.\n\n7. **Update parser state** by modifying the current stack frame's data structure with the processed line content and preparing for the next line. The stack top always represents the active parsing context for subsequent lines.\n\nThe **context-sensitive interpretation** means that identical content can create different structural results depending on the current stack state. A line containing `name: value` creates a top-level mapping entry when the stack is empty, adds a mapping entry to the current mapping when inside a mapping context, or creates a mapping value for a sequence item when inside a sequence context.\n\n**Stack frame management** requires careful attention to data structure references. Each stack frame must maintain a reference to the actual data structure being built (dictionary for mappings, list for sequences) so that modifications during parsing affect the final output structure. The stack stores these references, not copies of the data.\n\nThe algorithm handles **implicit structure creation** by automatically determining structure types from content patterns. When encountering a key-value pair at a new indentation level, the parser creates a mapping structure. When encountering a sequence item (dash-prefixed line), the parser creates a sequence structure. This implicit creation eliminates the need for explicit structure declarations like TOML's table headers.\n\n### YAML Type Inference Logic\n\nYAML's **automatic type detection and conversion** system attempts to interpret scalar values as their most natural data types rather than treating everything as strings. This type inference creates intuitive behavior for users but introduces complexity in parsing logic that must recognize and convert various literal patterns.\n\nThe **scalar type inference** follows a precedence hierarchy that examines string content against increasingly specific patterns. The inference engine processes each scalar value through this decision tree:\n\n| Pattern Type | Examples | Inferred Type | Conversion Logic |\n|--------------|----------|---------------|------------------|\n| Boolean literals | `true`, `false`, `yes`, `no`, `on`, `off` | Boolean | Case-insensitive string matching against known boolean values |\n| Integer literals | `42`, `-17`, `0x1A`, `0o755`, `0b1010` | Integer | Regex pattern matching with base detection (decimal, hex, octal, binary) |\n| Float literals | `3.14`, `-2.5e10`, `.5`, `1.2e-3` | Float | Scientific notation support with decimal point or exponent indicators |\n| Date/time literals | `2023-12-31`, `12:30:45`, `2023-12-31T12:30:45Z` | Date/Time | ISO 8601 format detection with timezone support |\n| Null literals | `null`, `~`, empty value | Null/None | Explicit null markers or empty values in certain contexts |\n| String fallback | Everything else | String | Default case when no other patterns match |\n\nThe **boolean inference rules** recognize multiple conventional representations that users commonly expect to work as boolean values. The parser must handle case variations and cultural differences in boolean representation. Values like `TRUE`, `False`, `YES`, `No` all map to appropriate boolean values through case-insensitive comparison.\n\n**Numeric type inference** involves pattern recognition that distinguishes integers from floating-point numbers while supporting alternative numeric bases. Integer patterns include standard decimal notation, hexadecimal (0x prefix), octal (0o prefix), and binary (0b prefix) representations. Float patterns recognize decimal points, scientific notation with e/E exponents, and special float values like infinity and NaN.\n\nThe **string escaping and quoting** system provides explicit control over type inference when automatic detection produces incorrect results. Single-quoted strings (`'value'`) preserve literal content without escape sequence processing and prevent type inference. Double-quoted strings (`\"value\"`) allow escape sequences like `\\n`, `\\t`, `\\\"`, and `\\\\` while still preventing type inference. Unquoted strings undergo full type inference processing.\n\n**Multiline string handling** supports both literal block scalars (using `|` indicator) and folded block scalars (using `>` indicator). Literal blocks preserve line breaks and indentation exactly as written. Folded blocks collapse line breaks within paragraphs into spaces while preserving paragraph breaks indicated by blank lines.\n\nType inference must handle **edge cases** where the same string representation could reasonably map to multiple types. For example, `01:30:00` could represent a time value or a string. The parser uses context clues and follows YAML specification precedence rules to resolve these ambiguities consistently.\n\nThe **inference validation** process ensures that converted values make semantic sense and handles conversion errors gracefully. Invalid date formats, numeric overflows, and malformed patterns should produce clear error messages rather than silent failures or unexpected type assignments.\n\n### YAML Architecture Decisions\n\nThe architecture decisions for YAML parsing center on managing the complexity of indentation-sensitive parsing while maintaining reasonable performance and error handling capabilities. These decisions directly impact both implementation complexity and user experience.\n\n> **Decision: YAML Subset Selection**\n> - **Context**: Full YAML specification includes advanced features like anchors, aliases, complex multiline syntax, and document streams that significantly increase implementation complexity without corresponding learning value for configuration parsing\n> - **Options Considered**: Full YAML 1.2 specification, Common subset (mappings/sequences/scalars), Minimal subset (basic nesting only)\n> - **Decision**: Implement common subset covering indentation-based mappings, sequences, basic scalars, and simple flow syntax\n> - **Rationale**: The common subset covers 95% of real-world YAML configuration usage while keeping implementation complexity manageable for learning purposes. Advanced features like anchors and multi-document streams are rarely used in configuration files\n> - **Consequences**: Users cannot use advanced YAML features, but implementation remains focused on core parsing concepts without getting lost in specification edge cases\n\n| Subset Option | Features Included | Implementation Complexity | Real-World Coverage |\n|---------------|-------------------|---------------------------|---------------------|\n| Full YAML 1.2 | All specification features | Very High | 100% |\n| Common Subset | Mappings, sequences, scalars, basic flow | Medium | 95% |\n| Minimal Subset | Basic nesting only | Low | 60% |\n\n> **Decision: Flow Syntax Support Level**\n> - **Context**: YAML supports both block syntax (indentation-based) and flow syntax (JSON-like with brackets and braces). Flow syntax can appear inline within block structures, creating parsing complexity\n> - **Options Considered**: No flow syntax support, Basic flow arrays/objects only, Full flow syntax with nesting\n> - **Decision**: Support basic flow syntax for inline arrays and objects without deep nesting\n> - **Rationale**: Basic flow syntax like `[item1, item2, item3]` and `{key: value, key2: value2}` appears frequently in real configurations and provides valuable parsing experience without excessive complexity\n> - **Consequences**: Enables common inline syntax patterns while avoiding complex flow/block interaction edge cases that would complicate the parsing algorithm significantly\n\n> **Decision: Indentation Validation Strictness**\n> - **Context**: YAML requires consistent indentation patterns but allows flexibility in indentation amounts. Parser must decide how strictly to enforce indentation rules\n> - **Options Considered**: Strict validation (exact indentation matching), Flexible validation (consistent increases/decreases), Lenient validation (best-effort interpretation)\n> - **Decision**: Implement strict validation with clear error messages for indentation violations\n> - **Rationale**: Strict validation helps users learn proper YAML formatting and prevents subtle bugs from inconsistent indentation. Clear error messages make the strictness helpful rather than frustrating\n> - **Consequences**: Parser rejects malformed YAML that other parsers might accept, but provides better learning experience and more predictable behavior\n\nThe **multiline string handling** decision affects both parsing complexity and user experience with complex string values in configuration files:\n\n> **Decision: Multiline String Support**\n> - **Context**: YAML multiline strings use block scalar indicators (`|` and `>`) with complex indentation and line-ending rules that significantly complicate parsing logic\n> - **Options Considered**: No multiline support, Basic literal blocks only, Full block scalar support with indicators\n> - **Decision**: Support basic literal block scalars with `|` indicator for exact preservation\n> - **Rationale**: Configuration files commonly need multiline strings for templates, SQL queries, or documentation. Literal blocks cover the most important use case with manageable parsing complexity\n> - **Consequences**: Users can include multiline configuration values naturally, but advanced folding and block scalar features are not available\n\n**Error recovery strategy** determines how the parser behaves when encountering invalid YAML structure:\n\n| Recovery Strategy | Behavior on Error | User Experience | Implementation Complexity |\n|-------------------|-------------------|-----------------|---------------------------|\n| Fail Fast | Stop on first error | Clear failure point | Low |\n| Continue Parsing | Collect multiple errors | Better error overview | Medium |\n| Best-effort Recovery | Attempt to continue with assumptions | May mask real errors | High |\n\n> **Decision: Error Recovery Approach**\n> - **Context**: YAML parsing can fail in multiple ways (indentation errors, type conversion failures, syntax violations) and the parser must decide whether to stop immediately or attempt continued parsing\n> - **Options Considered**: Fail-fast on first error, Continue parsing to collect multiple errors, Best-effort recovery with assumptions\n> - **Decision**: Implement fail-fast approach with rich error context and suggestions for fix\n> - **Rationale**: YAML's indentation sensitivity means that early errors often cascade into misleading later errors. Failing fast with clear context helps users fix the actual root cause rather than getting confused by secondary errors\n> - **Consequences**: Users see one error at a time but get high-quality error messages that directly address the parsing failure cause\n\n### Common YAML Parsing Pitfalls\n\nYAML parsing introduces unique challenges that frequently trip up both parser implementers and users. Understanding these pitfalls helps create more robust parsers and better error messages.\n\n⚠️ **Pitfall: Tab vs Space Indentation Mixing**\n\nThe most common YAML parsing error occurs when users mix tab and space characters for indentation. YAML specification explicitly prohibits this mixing, but many text editors make the distinction invisible to users. The parser encounters what appears to be consistent indentation but actually represents inconsistent whitespace character usage.\n\nThis manifests as parsing errors where the parser reports indentation level mismatches despite the visual appearance of correct indentation. Users often spend significant time checking their indentation manually without realizing the issue lies in invisible character differences.\n\n**Detection approach**: During tokenization, track whether tabs or spaces are used for indentation and reject documents that mix both. Provide error messages that specifically identify the character type and line location where mixing occurs.\n\n**Recovery strategy**: Report the exact character position and provide suggestions to convert all indentation to spaces (the recommended YAML convention) or use editor settings to visualize whitespace characters.\n\n⚠️ **Pitfall: Implicit Type Conversion Surprises**\n\nYAML's aggressive type inference creates unexpected behavior when string values accidentally match type inference patterns. Common examples include version numbers like `1.0` becoming floats instead of strings, boolean-like words becoming boolean values, and numeric strings being converted to integers.\n\nConsider a configuration containing `version: 1.20` where the user expects string `\"1.20\"` but gets float `1.2` due to type inference. Similarly, values like `yes`, `no`, `on`, `off` automatically become boolean values even when users intended string values.\n\n**Detection approach**: Implement type inference warnings or provide explicit control through quoted string syntax. When encountering common problematic patterns, consider generating warnings that inform users about automatic type conversion.\n\n**Recovery strategy**: Document type inference rules clearly and show users how to use quoted strings (`\"1.20\"`) to force string type when automatic inference produces incorrect results. Consider providing a strict mode that requires explicit typing for ambiguous values.\n\n⚠️ **Pitfall: Inconsistent Indentation Level Returns**\n\nWhen returning to a previous indentation level, YAML requires exact matching with a previously established level. Users often \"dedent\" to indentation amounts that were never used, creating invalid structure transitions that the parser cannot interpret.\n\nThis occurs when users inconsistently indent nested structures, such as using 2 spaces for the first level, 4 spaces for the second level, and then 3 spaces when attempting to return to an intermediate level that was never established.\n\n**Error pattern example**: Starting with 0 spaces (root), going to 2 spaces (level 1), then 6 spaces (level 2), and attempting to return to 4 spaces (invalid - level never existed).\n\n**Detection approach**: Maintain a stack of established indentation levels and validate that any dedent operation returns to exactly one of those levels. Reject documents where dedent attempts target indentation amounts that don't match the stack.\n\n**Recovery strategy**: Provide error messages showing the established indentation levels and suggest valid dedent targets. Include visual representation of the indentation stack state to help users understand the structural context.\n\n⚠️ **Pitfall: Flow Syntax Context Confusion**\n\nWhen YAML parsers support both block syntax (indentation-based) and flow syntax (bracket/brace delimited), users can create confusing combinations where flow syntax appears within block contexts or vice versa. The parser must correctly handle context switches between these two syntactic modes.\n\nComplex cases arise when arrays or objects defined using flow syntax contain values that look like block syntax, or when block structures attempt to contain flow-syntax elements without proper nesting understanding.\n\n**Context switching errors**: Users might start an array with flow syntax `[item1,` and then attempt to continue with block syntax indentation for subsequent items, creating syntactically invalid combinations.\n\n**Detection approach**: Track parsing context to distinguish between block and flow modes. Validate that syntax elements are appropriate for the current context and provide clear errors when context violations occur.\n\n**Recovery strategy**: Explain the difference between block and flow syntax clearly in error messages. Suggest consistent syntax usage and provide examples of correct flow-within-block or block-within-flow patterns.\n\n⚠️ **Pitfall: Scalar Value Ambiguity in Lists**\n\nYAML sequence items (list elements) can contain either simple scalar values or complex nested structures. The parser must correctly determine whether a sequence item is a simple value or the beginning of a nested structure, especially when the distinction isn't immediately clear from the first line.\n\nAmbiguity occurs with patterns like:\n```\nitems:\n  - name: first item\n    description: a complex item\n  - simple item\n  - name: third item\n```\n\nThe parser must recognize that some sequence items are simple scalars while others are complex mappings, even though the determination might require lookahead parsing to make the distinction.\n\n**Detection approach**: Use lookahead parsing to examine subsequent lines when processing sequence items. If the next line has greater indentation and contains a key-value pattern, treat the sequence item as a complex mapping. Otherwise, treat it as a simple scalar.\n\n**Recovery strategy**: When ambiguity exists, provide clear error messages that explain how the parser interpreted the structure and suggest formatting changes to make the intent explicit.\n\n### Implementation Guidance\n\nThe YAML parser implementation requires careful balance between handling indentation complexity and maintaining clean, debuggable code structure. The stack-based parsing approach translates naturally to object-oriented design with clear separation between tokenization, structure tracking, and value processing responsibilities.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Recommendation for Learning |\n|-----------|---------------|-----------------|---------------------------|\n| Indentation Tracking | Simple integer stack | Structured context objects | Structured context objects |\n| Type Inference | Basic regex patterns | Comprehensive type system | Basic regex with clear rules |\n| Error Handling | Exception-based | Result/Option types | Exception-based with context |\n| Flow Syntax | Skip entirely | Full bracket/brace parsing | Basic inline arrays/objects |\n| String Processing | Basic quote handling | Full escape sequence support | Full escape sequences |\n\n#### Recommended File Structure\n\nThe YAML parser should integrate cleanly with the existing configuration parser architecture while maintaining clear separation of indentation-specific logic:\n\n```\nconfig-parser/\n  parsers/\n    yaml_parser.py              ← Main YAML parser entry point\n    yaml_tokenizer.py           ← YAML-specific tokenization\n    yaml_structures.py          ← Stack frame and context objects\n    yaml_types.py               ← Type inference logic\n  tests/\n    test_yaml_parser.py         ← Comprehensive YAML tests\n    fixtures/\n      yaml/                     ← Test YAML files with edge cases\n  examples/\n    yaml_parsing_example.py     ← Demonstration of parser usage\n```\n\nThis structure separates the indentation-sensitive parsing logic from the general tokenization framework while maintaining integration with the overall parser architecture.\n\n#### Infrastructure Starter Code\n\nComplete indentation stack management infrastructure that handles the complex state tracking required for YAML parsing:\n\n```python\nfrom typing import List, Dict, Any, Optional, Union\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass YAMLStructureType(Enum):\n    \"\"\"Types of YAML structures that can be nested.\"\"\"\n    MAPPING = \"mapping\"\n    SEQUENCE = \"sequence\"\n    DOCUMENT = \"document\"\n\n@dataclass\nclass IndentationFrame:\n    \"\"\"Represents one level of nesting in the indentation stack.\"\"\"\n    indent_level: int\n    structure_type: YAMLStructureType\n    data: Union[Dict[str, Any], List[Any]]\n    line_number: int\n    \n    def add_mapping_entry(self, key: str, value: Any) -> None:\n        \"\"\"Add a key-value pair to this mapping frame.\"\"\"\n        if self.structure_type != YAMLStructureType.MAPPING:\n            raise ValueError(f\"Cannot add mapping entry to {self.structure_type}\")\n        self.data[key] = value\n    \n    def add_sequence_item(self, item: Any) -> None:\n        \"\"\"Add an item to this sequence frame.\"\"\"\n        if self.structure_type != YAMLStructureType.SEQUENCE:\n            raise ValueError(f\"Cannot add sequence item to {self.structure_type}\")\n        self.data.append(item)\n\nclass IndentationStack:\n    \"\"\"Manages the stack of indentation contexts during YAML parsing.\"\"\"\n    \n    def __init__(self):\n        self.stack: List[IndentationFrame] = []\n        self.established_levels: List[int] = [0]  # Track valid dedent levels\n    \n    def current_level(self) -> int:\n        \"\"\"Get the current indentation level.\"\"\"\n        return self.stack[-1].indent_level if self.stack else 0\n    \n    def current_frame(self) -> Optional[IndentationFrame]:\n        \"\"\"Get the current stack frame.\"\"\"\n        return self.stack[-1] if self.stack else None\n    \n    def push_frame(self, indent_level: int, structure_type: YAMLStructureType, line_number: int) -> IndentationFrame:\n        \"\"\"Push a new indentation frame onto the stack.\"\"\"\n        if structure_type == YAMLStructureType.MAPPING:\n            data = {}\n        elif structure_type == YAMLStructureType.SEQUENCE:\n            data = []\n        else:\n            data = {}\n        \n        frame = IndentationFrame(indent_level, structure_type, data, line_number)\n        self.stack.append(frame)\n        \n        if indent_level not in self.established_levels:\n            self.established_levels.append(indent_level)\n        \n        return frame\n    \n    def pop_to_level(self, target_level: int) -> List[IndentationFrame]:\n        \"\"\"Pop frames until reaching the target indentation level.\"\"\"\n        if target_level not in self.established_levels:\n            raise StructureError(\n                f\"Invalid dedent to level {target_level}. Valid levels: {self.established_levels}\",\n                position=None,\n                suggestion=f\"Use one of these indentation levels: {self.established_levels}\"\n            )\n        \n        popped_frames = []\n        while self.stack and self.stack[-1].indent_level > target_level:\n            popped_frames.append(self.stack.pop())\n        \n        return popped_frames\n    \n    def get_root_data(self) -> Dict[str, Any]:\n        \"\"\"Get the root document data structure.\"\"\"\n        return self.stack[0].data if self.stack else {}\n\n# Type inference utilities for YAML scalar processing\nclass YAMLTypeInference:\n    \"\"\"Handles automatic type detection and conversion for YAML scalars.\"\"\"\n    \n    BOOLEAN_VALUES = {\n        'true': True, 'false': False,\n        'yes': True, 'no': False,\n        'on': True, 'off': False,\n        'y': True, 'n': False\n    }\n    \n    NULL_VALUES = {'null', '~', ''}\n    \n    @classmethod\n    def infer_type(cls, value_str: str) -> Any:\n        \"\"\"Convert string value to appropriate Python type.\"\"\"\n        if not isinstance(value_str, str):\n            return value_str\n        \n        # Remove leading/trailing whitespace\n        clean_value = value_str.strip()\n        \n        # Handle empty values and null\n        if clean_value in cls.NULL_VALUES:\n            return None\n        \n        # Handle boolean values (case insensitive)\n        lower_value = clean_value.lower()\n        if lower_value in cls.BOOLEAN_VALUES:\n            return cls.BOOLEAN_VALUES[lower_value]\n        \n        # Handle numeric values\n        try:\n            # Try integer first\n            if '.' not in clean_value and 'e' not in lower_value:\n                return int(clean_value)\n            # Try float\n            return float(clean_value)\n        except ValueError:\n            pass\n        \n        # Return as string if no other type matches\n        return clean_value\n\n# Complete error handling infrastructure for YAML-specific errors\nclass YAMLIndentationError(StructureError):\n    \"\"\"Error in YAML indentation structure.\"\"\"\n    \n    def __init__(self, message: str, line_number: int, current_levels: List[int]):\n        super().__init__(\n            message=message,\n            position=Position(line=line_number, column=0, offset=0),\n            suggestion=f\"Valid indentation levels are: {current_levels}\"\n        )\n        self.current_levels = current_levels\n\nclass YAMLTypeError(ParseError):\n    \"\"\"Error in YAML type inference or conversion.\"\"\"\n    pass\n```\n\n#### Core Logic Skeleton Code\n\nThe main YAML parser implementation with detailed TODO comments mapping to the algorithm steps:\n\n```python\nclass YAMLParser:\n    \"\"\"YAML subset parser with indentation-based structure tracking.\"\"\"\n    \n    def __init__(self):\n        self.stack = IndentationStack()\n        self.type_inference = YAMLTypeInference()\n        self.current_line = 0\n    \n    def parse(self, content: str) -> Dict[str, Any]:\n        \"\"\"\n        Parse YAML content into nested dictionary structure.\n        \n        Args:\n            content: Raw YAML text content\n            \n        Returns:\n            Parsed configuration as nested dictionary\n            \n        Raises:\n            YAMLIndentationError: For indentation structure violations\n            YAMLTypeError: For type inference failures\n            SyntaxError: For general syntax violations\n        \"\"\"\n        # TODO 1: Split content into logical lines, handling line continuations\n        # TODO 2: Initialize document root frame on indentation stack\n        # TODO 3: Process each logical line through indentation analysis\n        # TODO 4: Handle context transitions based on indentation changes\n        # TODO 5: Parse line content based on current structural context\n        # TODO 6: Validate final document structure completeness\n        # TODO 7: Return root data structure from stack\n        \n        # Hint: Use self._process_line() for each line after indentation analysis\n        # Hint: Track line numbers for error reporting throughout parsing\n        pass\n    \n    def _analyze_line_indentation(self, line: str, line_number: int) -> Tuple[int, str]:\n        \"\"\"\n        Analyze line indentation and extract content.\n        \n        Args:\n            line: Raw line text\n            line_number: Line number for error reporting\n            \n        Returns:\n            Tuple of (indentation_level, content_text)\n            \n        Raises:\n            YAMLIndentationError: For invalid indentation patterns\n        \"\"\"\n        # TODO 1: Count leading whitespace characters (spaces only, reject tabs)\n        # TODO 2: Validate indentation character consistency (no tab mixing)\n        # TODO 3: Extract content portion after removing indentation\n        # TODO 4: Return indentation level and cleaned content\n        \n        # Hint: Use enumerate() to track character positions for error reporting\n        # Hint: Raise YAMLIndentationError for tab characters with helpful message\n        pass\n    \n    def _handle_indentation_transition(self, current_indent: int, line_number: int) -> None:\n        \"\"\"\n        Handle stack transitions based on indentation level changes.\n        \n        Args:\n            current_indent: Indentation level of current line\n            line_number: Current line number for error context\n        \"\"\"\n        # TODO 1: Compare current indentation to stack top level\n        # TODO 2: If deeper indentation, prepare for nested structure (don't push yet)\n        # TODO 3: If same indentation, continue current structure context\n        # TODO 4: If shallower indentation, pop stack frames to matching level\n        # TODO 5: Validate dedent targets against established indentation levels\n        \n        # Hint: Use self.stack.pop_to_level() for dedent operations\n        # Hint: Defer frame pushing until content type is determined\n        pass\n    \n    def _process_line_content(self, content: str, indent_level: int, line_number: int) -> None:\n        \"\"\"\n        Process line content based on YAML syntax patterns.\n        \n        Args:\n            content: Line content after indentation removal\n            indent_level: Indentation level of this line\n            line_number: Line number for error reporting\n        \"\"\"\n        # TODO 1: Skip empty lines and comment lines (starting with #)\n        # TODO 2: Detect line type: mapping (key:), sequence (-), or scalar\n        # TODO 3: Handle mapping entries by parsing key-value pairs\n        # TODO 4: Handle sequence items by parsing list elements\n        # TODO 5: Handle scalar values with type inference\n        # TODO 6: Create appropriate stack frames for nested structures\n        \n        # Hint: Use regex patterns to identify mapping vs sequence vs scalar\n        # Hint: Call self._parse_mapping_line() or self._parse_sequence_line()\n        pass\n    \n    def _parse_mapping_line(self, content: str, indent_level: int, line_number: int) -> None:\n        \"\"\"Parse a mapping line (key: value format).\"\"\"\n        # TODO 1: Split content on first colon to separate key and value\n        # TODO 2: Validate key format and handle quoted keys\n        # TODO 3: Trim whitespace from key and value portions  \n        # TODO 4: If no current mapping frame, create one at this indent level\n        # TODO 5: If value is empty, prepare for nested structure on next line\n        # TODO 6: If value present, apply type inference and store mapping entry\n        # TODO 7: Handle inline flow syntax in values (basic arrays/objects)\n        \n        # Hint: Use self.stack.current_frame() to check current context\n        # Hint: Empty values after colon indicate nested structure follows\n        pass\n    \n    def _parse_sequence_line(self, content: str, indent_level: int, line_number: int) -> None:\n        \"\"\"Parse a sequence line (- item format).\"\"\"\n        # TODO 1: Remove leading dash and whitespace to get item content\n        # TODO 2: If no current sequence frame, create one at this indent level\n        # TODO 3: If item content is empty, prepare for nested structure\n        # TODO 4: If item content present, apply type inference and add to sequence\n        # TODO 5: Handle complex sequence items that are mappings\n        \n        # Hint: Sequence items can contain nested mappings or other sequences\n        # Hint: Use lookahead to determine if item is scalar or complex structure\n        pass\n```\n\n#### Language-Specific Implementation Hints\n\nPython-specific techniques for effective YAML parser implementation:\n\n- **String processing**: Use `str.lstrip()` to remove leading whitespace, but manually count characters to distinguish tabs from spaces\n- **Stack management**: Python lists work perfectly as stacks with `append()` and `pop()` operations\n- **Type inference**: Use `isinstance()` checks and `try/except` blocks for numeric conversion attempts  \n- **Regular expressions**: Import `re` module for pattern matching mapping syntax (`key:`) and sequence syntax (`-`)\n- **Error context**: Use `enumerate()` when iterating over lines to maintain line number tracking\n- **Unicode handling**: Python 3 strings handle Unicode automatically, but be aware of Unicode whitespace characters beyond ASCII space and tab\n\n#### Milestone Checkpoint\n\nAfter implementing the YAML parser component, verify functionality with these concrete tests:\n\n**Basic parsing verification:**\n```bash\npython -c \"\nfrom parsers.yaml_parser import YAMLParser\nparser = YAMLParser()\nresult = parser.parse('name: test\\nage: 25\\nactive: true')\nprint('Parsed:', result)\nassert result == {'name': 'test', 'age': 25, 'active': True}\nprint('✓ Basic parsing works')\n\"\n```\n\n**Indentation structure testing:**\n```bash\npython -c \"\nfrom parsers.yaml_parser import YAMLParser\nyaml_content = '''\ndatabase:\n  host: localhost\n  port: 5432\n  credentials:\n    username: admin\n    password: secret\n'''\nparser = YAMLParser()\nresult = parser.parse(yaml_content.strip())\nprint('Nested result:', result)\nassert result['database']['credentials']['username'] == 'admin'\nprint('✓ Nested indentation works')\n\"\n```\n\n**Expected behavior verification:**\n- Parser should handle 2-space, 4-space, or consistent indentation amounts\n- Type inference should convert `25` to integer, `true` to boolean, quoted strings to strings\n- Indentation errors should provide helpful error messages showing valid indent levels\n- Mixed tab/space indentation should be rejected with clear error explaining the problem\n\n**Signs of implementation problems:**\n- \"KeyError\" exceptions usually indicate missing stack frame management\n- \"Indentation level mismatch\" errors suggest issues with dedent level validation  \n- Type conversion errors indicate problems in the type inference logic\n- Stack overflow suggests infinite recursion in nested structure handling\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - this section defines how all components work together to create a unified parsing system\n\nThink of the complete parsing system as an assembly line in a specialized translation factory. Raw configuration files enter at one end as unstructured text, pass through a series of specialized workstations (format detection, tokenization, parsing, and structure conversion), and emerge as clean, standardized nested dictionaries. Each workstation has a specific expertise and passes its refined output to the next station, with quality control checkpoints that can halt the entire line if defects are detected. The beauty of this assembly line is that while each workstation uses different techniques internally, they all communicate through standardized interfaces, allowing the line to handle three completely different \"product types\" (INI, TOML, YAML) while producing identical output formats.\n\nThe interactions between components follow a carefully orchestrated pipeline where data flows through well-defined interfaces, errors are enriched with context as they propagate upward, and format detection automatically routes content to the appropriate specialized parser. This design enables clean separation of concerns while maintaining a unified experience for users of the parsing library.\n\n![System Component Architecture](./diagrams/system-architecture.svg)\n\n### Complete Parsing Pipeline\n\nThe complete parsing pipeline represents the end-to-end journey from raw configuration file content to fully processed nested dictionary structures. This pipeline consists of several distinct phases, each with specific responsibilities and well-defined interfaces for data exchange.\n\nThink of this pipeline as a sophisticated mail sorting facility. Raw mail (configuration content) arrives in various formats and languages (INI, TOML, YAML). The facility first examines each piece to determine its origin language and routing requirements (format detection). Next, specialized language experts break down the content into meaningful units (tokenization). Then, grammar specialists organize these units into logical structures (parsing). Finally, translators convert everything into a common internal language (unified output format) that the rest of the organization can understand. At each stage, quality inspectors check for problems and can route defective items to error handling processes.\n\n![Complete Parsing Pipeline Flow](./diagrams/parsing-pipeline.svg)\n\n#### Pipeline Phase Breakdown\n\nThe parsing pipeline operates through five distinct phases, each building upon the output of the previous phase:\n\n| Phase | Input | Output | Primary Responsibility | Error Types |\n|-------|--------|---------|----------------------|-------------|\n| Content Ingestion | File path or raw string | String content with encoding normalization | Read file content, handle encoding, normalize line endings | IO errors, encoding errors |\n| Format Detection | String content | Format identifier (ini/toml/yaml) | Analyze content patterns to determine format | Ambiguous format errors |\n| Tokenization | String content + format | List of typed tokens with positions | Break content into meaningful lexical units | Lexical errors, invalid characters |\n| Parsing | Token stream + format | Parse tree structure | Build hierarchical structure from tokens | Syntax errors, structure errors |\n| Output Conversion | Parse tree | Unified dictionary structure | Convert format-specific structures to common representation | Type conversion errors, structure mapping errors |\n\nThe main entry point `parse_config(content, format=None)` orchestrates this entire pipeline, handling the coordination between phases and managing error propagation. When a format is not explicitly specified, the pipeline automatically detects the format before proceeding to tokenization.\n\n#### Data Transformation Flow\n\nEach phase transforms data from one representation to another, with specific rules governing these transformations:\n\n**Content Ingestion Transformation**: Raw file content undergoes normalization to ensure consistent processing regardless of source encoding or line ending conventions. The ingestion phase converts all content to UTF-8 encoding and normalizes line endings to Unix-style newlines. This normalization prevents encoding-related parsing errors in later phases.\n\n**Format Detection Transformation**: Normalized content is analyzed using format-specific heuristics to produce a format identifier. This transformation is read-only - the content remains unchanged, but metadata about its format is added to guide subsequent processing decisions.\n\n**Tokenization Transformation**: String content is decomposed into a sequence of typed tokens, each carrying a specific semantic meaning and position information. This transformation is lossy in the sense that whitespace semantics and comment placement may be simplified, but all semantically meaningful content is preserved with enhanced type information.\n\n**Parsing Transformation**: The token stream is restructured into a hierarchical parse tree that reflects the logical organization of the configuration data. This transformation handles the complex task of resolving nested structures, managing scope transitions, and validating structural consistency according to format-specific rules.\n\n**Output Conversion Transformation**: Format-specific parse tree structures are normalized into a unified nested dictionary representation. This transformation handles impedance mismatch between different format paradigms, ensuring that sections, tables, and mappings all produce equivalent dictionary structures.\n\n#### Pipeline State Management\n\nThe parsing pipeline maintains state information that flows between phases to enable error reporting, debugging, and incremental processing:\n\n| State Component | Purpose | Maintained By | Used By |\n|-----------------|---------|---------------|---------|\n| Source Position | Track current location in original content | Tokenizer | Parser, Error Reporter |\n| Format Context | Remember detected format and configuration options | Format Detector | Tokenizer, Parser |\n| Symbol Tables | Track defined keys and tables for conflict detection | Parser | Parser (validation) |\n| Error Context | Accumulate errors with source positions | All phases | Error Reporter |\n| Parse Stack | Maintain parsing context for nested structures | Parser | Parser (recursive descent) |\n\nThe pipeline uses a **context object** that flows through all phases, accumulating information and providing shared services. This context object implements the following interface:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `get_position()` | None | `Position` | Get current source position |\n| `set_position(pos)` | `pos: Position` | None | Update current source position |\n| `add_error(error)` | `error: ParseError` | None | Record error with current context |\n| `get_errors()` | None | `List[ParseError]` | Retrieve all accumulated errors |\n| `get_format()` | None | `str` | Get detected format identifier |\n| `set_format(fmt)` | `fmt: str` | None | Set format for pipeline |\n| `get_options()` | None | `dict` | Get format-specific parsing options |\n\n> **Design Insight**: The context object serves as both a communication channel and a memory mechanism, allowing later phases to access information discovered by earlier phases while maintaining loose coupling between components.\n\n#### Error Recovery in Pipeline\n\nThe parsing pipeline implements **graceful degradation** where errors in one phase don't necessarily halt the entire pipeline. Each phase can choose to continue processing despite encountering errors, allowing the collection of multiple error reports in a single parsing attempt:\n\n1. **Content Ingestion**: IO errors or encoding errors immediately halt the pipeline, as no further processing is possible without readable content.\n\n2. **Format Detection**: Ambiguous format detection doesn't halt the pipeline; instead, the system defaults to INI format (simplest) and continues, recording a warning about the format assumption.\n\n3. **Tokenization**: Invalid characters or lexical errors are recorded as `TokenError` instances, but tokenization continues by inserting `INVALID` tokens and resuming at the next recognizable boundary.\n\n4. **Parsing**: Syntax errors are recorded as `SyntaxError` instances, and the parser attempts to resynchronize by finding the next stable parsing boundary (section header, top-level key, etc.).\n\n5. **Output Conversion**: Type conversion errors are recorded as warnings, with problematic values preserved as strings in the final output.\n\nThis error recovery strategy enables the pipeline to provide comprehensive feedback about multiple issues in a single parsing attempt, significantly improving the developer experience when debugging configuration files.\n\n### Format Detection Strategy\n\nFormat detection serves as the intelligent routing mechanism that automatically determines which specialized parser should handle incoming configuration content. Think of format detection as an experienced librarian who can instantly recognize whether a book is written in English, French, or German just by glancing at a few pages - they don't need to read the entire book, just identify the distinctive patterns that reveal the language.\n\nThe format detection system analyzes content using a combination of **syntactic signatures** and **statistical patterns** to make confident format determinations even with partial or ambiguous content. This approach is crucial because configuration files often contain overlapping syntactic elements (all three formats support key-value pairs and comments), requiring sophisticated heuristics to distinguish between them.\n\n#### Detection Algorithm Strategy\n\nThe `detect_format(content)` function implements a **multi-pass analysis strategy** that examines different aspects of the content to build confidence in format identification:\n\n1. **Signature Scanning Pass**: Look for unambiguous format-specific signatures that immediately identify the format\n2. **Pattern Analysis Pass**: Analyze the frequency and distribution of format-specific patterns\n3. **Conflict Resolution Pass**: Resolve ambiguities using format-specific tie-breaking rules\n4. **Confidence Assessment Pass**: Evaluate the reliability of the detection and flag uncertain cases\n\n| Detection Pass | TOML Signatures | INI Signatures | YAML Signatures | Weight |\n|----------------|-----------------|----------------|-----------------|---------|\n| Signature Scanning | `[[array.of.tables]]`, `key.dotted.path =`, `{ inline = \"table\" }` | `[section]` without dotted paths, `key=value` without quotes | `key: value`, `- list item`, indentation-based nesting | High (90% confidence) |\n| Pattern Analysis | Quoted keys, date-time literals, mixed-type arrays | Semicolon comments, unquoted values, flat sections | Flow syntax `{}[]`, implicit type inference patterns | Medium (70% confidence) |\n| Conflict Resolution | Complex table structures, type-specific literals | Simple key-value with minimal nesting | Indentation consistency, mapping vs sequence patterns | Low (50% confidence) |\n\nThe detection algorithm processes these passes sequentially, stopping early if high-confidence signatures are found, or continuing through all passes for ambiguous content.\n\n#### Format-Specific Detection Rules\n\nEach format has distinctive characteristics that enable reliable detection:\n\n**TOML Detection Signatures**:\n- **Definitive signatures**: Double-bracket array-of-tables syntax `[[database.servers]]`, dotted table paths `[tool.poetry.dependencies]`, and inline tables `person = { name = \"Tom\", age = 30 }`\n- **Strong indicators**: Quoted keys using double or single quotes, date-time literals with timezone information, and mixed-type arrays with explicit type annotations\n- **Supporting patterns**: Underscores in numeric literals `1_000_000`, multi-line strings with triple quotes, and boolean values using lowercase `true/false`\n\n**INI Detection Signatures**:\n- **Definitive signatures**: Simple bracketed sections `[Section Name]` without dots, semicolon-prefixed comments `;comment`, and unquoted string values\n- **Strong indicators**: Key-value pairs using colon syntax `key: value`, global keys appearing before any section headers, and minimal use of nested structures\n- **Supporting patterns**: Quoted string values using double quotes only, numeric values without type prefixes, and flat organizational structure\n\n**YAML Detection Signatures**:\n- **Definitive signatures**: Indentation-based nesting with consistent spaces, list items prefixed with dash and space `- item`, and mapping syntax using colon-space `key: value`\n- **Strong indicators**: Flow syntax mixing with block syntax, implicit type inference for `yes/no/on/off`, and multi-document separators `---`\n- **Supporting patterns**: Quoted strings using single or double quotes interchangeably, null values represented as `~` or `null`, and folded/literal string syntax `|` and `>`\n\n> **Decision: Multi-Pass Detection Algorithm**\n> - **Context**: Single-pass detection often fails on ambiguous files that mix format conventions or have minimal content\n> - **Options Considered**: Single regex-based detection, machine learning classifier, multi-pass heuristic analysis\n> - **Decision**: Multi-pass heuristic analysis with signature scanning and pattern analysis\n> - **Rationale**: Provides high accuracy without external dependencies, handles edge cases gracefully, and offers explainable detection results\n> - **Consequences**: More complex implementation but significantly better accuracy on real-world configuration files with mixed conventions\n\n#### Detection Confidence and Fallback Strategy\n\nThe format detection system provides **confidence scoring** for its determinations, allowing the parsing pipeline to make informed decisions about how to handle uncertain cases:\n\n| Confidence Level | Score Range | Action Taken | Error Handling |\n|------------------|-------------|--------------|----------------|\n| High Confidence | 90-100% | Proceed with detected format | Standard error reporting |\n| Medium Confidence | 70-89% | Proceed with warning logged | Enhanced error context |\n| Low Confidence | 50-69% | Proceed with user notification | Suggest manual format specification |\n| Ambiguous | Below 50% | Default to INI with warning | Multiple format attempt on failure |\n\nThe fallback strategy implements **graceful degradation** where low-confidence detections still allow parsing to proceed, but with enhanced error reporting that includes format detection uncertainty in error messages.\n\n#### Ambiguity Resolution Techniques\n\nWhen content exhibits patterns consistent with multiple formats, the detection system applies format-specific tie-breaking rules:\n\n**INI vs TOML Resolution**: INI format takes precedence when content uses simple bracketed sections without dotted paths and avoids TOML-specific features like inline tables or complex data types. TOML format takes precedence when dotted notation appears in section names or when complex data structures are present.\n\n**YAML vs INI Resolution**: YAML format takes precedence when consistent indentation-based nesting is detected or when list syntax with dash prefixes appears. INI format takes precedence when content is predominantly flat with minimal nesting and uses semicolon-style comments.\n\n**TOML vs YAML Resolution**: This represents the most complex ambiguity case. TOML format takes precedence when section headers use bracket notation and when explicit type annotations appear. YAML format takes precedence when indentation-based structure dominates and when implicit type inference patterns are prevalent.\n\n### Error Information Flow\n\nError handling in the configuration parser implements a **enriched propagation model** where errors are detected at their source, progressively enhanced with context information as they move up the component stack, and finally transformed into user-friendly diagnostic messages. Think of this as a hospital's patient information system: when a problem is first detected in a lab test, it starts with raw technical data, but as it moves through specialists and ultimately to the patient's doctor, each step adds context, interpretation, and actionable guidance until the final message is both technically accurate and humanly comprehensible.\n\nThe error information flow ensures that users receive not just notification that something went wrong, but specific guidance about what the problem is, where it occurred, why it happened, and how to fix it. This comprehensive error reporting significantly reduces debugging time and improves the developer experience when working with configuration files.\n\n![Error Detection and Reporting Flow](./diagrams/error-handling-flow.svg)\n\n#### Error Detection Points\n\nErrors can originate from multiple points in the parsing pipeline, each with different characteristics and context requirements:\n\n| Detection Point | Error Types | Available Context | Enhancement Needed |\n|-----------------|-------------|-------------------|-------------------|\n| Content Ingestion | IO errors, encoding errors | File path, system error codes | User-friendly file access guidance |\n| Format Detection | Ambiguous format, unknown patterns | Content sample, detection confidence scores | Format suggestion with reasoning |\n| Tokenization | Invalid characters, malformed literals | Character position, token context | Visual indication of problem location |\n| Parsing | Syntax errors, structure violations | Token stream, parsing state | Grammar explanation and fix suggestions |\n| Type Conversion | Invalid values, type mismatches | Value context, expected type | Type conversion guidance and examples |\n\nEach detection point creates errors using format-specific error types that inherit from the base `ParseError` class, ensuring consistent interface while enabling specialized handling.\n\n#### Error Context Enrichment\n\nAs errors propagate up through the component stack, each level adds contextual information that makes the error more actionable for end users:\n\n**Position Enhancement**: Raw character offsets from tokenization are converted to human-readable line and column numbers. The `current_position(source, offset)` function calculates these coordinates and creates `Position` objects that track line, column, and absolute offset information.\n\n**Source Context Addition**: The `create_error_context(source, position, context_lines=2)` function generates visual error context by extracting surrounding lines from the source content and highlighting the specific problem location. This creates the familiar \"caret pointer\" display that shows exactly where the error occurred.\n\n**Suggestion Generation**: Each component adds format-specific suggestions based on common error patterns. For example, TOML parsing errors include suggestions about table redefinition rules, while YAML parsing errors include guidance about indentation requirements.\n\n**Error Classification Refinement**: Generic parsing errors are reclassified into specific error types (`TokenError`, `SyntaxError`, `StructureError`) that enable targeted error handling and user guidance.\n\n#### Error Propagation Strategy\n\nThe error propagation system implements **structured error accumulation** where multiple errors can be collected and reported together, rather than halting on the first error encountered:\n\n| Propagation Level | Responsibilities | Error Transformation | Context Added |\n|------------------|------------------|---------------------|---------------|\n| Component Level | Detect and classify errors | Raw errors → typed errors | Component-specific context |\n| Parser Level | Coordinate error collection | Individual errors → error collections | Parsing state context |\n| Pipeline Level | Aggregate cross-component errors | Component errors → unified reports | Source file context |\n| API Level | Format for end-user consumption | Technical errors → user guidance | Help text and examples |\n\nThe propagation strategy uses **error aggregation objects** that collect related errors and provide unified reporting:\n\n```\nError Report Structure:\n- Primary Error: The main issue that prevented successful parsing\n- Secondary Errors: Related issues that may have contributed to the primary error  \n- Context Information: Source location, surrounding content, parsing state\n- Suggestions: Actionable guidance for resolving the error\n- Related Documentation: Links to format specifications or examples\n```\n\n#### Error Recovery and Continuation\n\nThe error handling system implements **intelligent recovery** that allows parsing to continue after encountering errors, enabling the collection of multiple error reports in a single parsing attempt:\n\n**Tokenization Recovery**: When invalid characters are encountered, the tokenizer inserts `INVALID` tokens and attempts to resynchronize at the next recognizable token boundary. This allows parsing to continue and potentially identify additional errors downstream.\n\n**Parsing Recovery**: When syntax errors occur, parsers attempt to resynchronize at stable parsing boundaries such as section headers, top-level keys, or significant structural markers. The recovery strategy varies by format:\n\n- **INI Recovery**: Resynchronize at section headers `[section]` or line boundaries for key-value pairs\n- **TOML Recovery**: Resynchronize at table declarations, top-level keys, or array-of-tables markers  \n- **YAML Recovery**: Resynchronize at consistent indentation levels or document boundaries\n\n**Structure Recovery**: When structural errors occur (like conflicting table definitions in TOML), the parser records the conflict but continues processing other parts of the file, allowing detection of multiple structural issues.\n\n> **Design Insight**: Error recovery is particularly valuable during configuration development and debugging, where users benefit from seeing all issues at once rather than fixing them one at a time through multiple parsing attempts.\n\n#### User-Friendly Error Formatting\n\nThe final stage of error information flow transforms technical error details into user-friendly diagnostic messages that follow established conventions for compiler and parser error reporting:\n\n**Error Message Structure**:\n1. **Problem Summary**: One-line description of what went wrong\n2. **Location Information**: File, line, and column where the error occurred  \n3. **Source Context**: Visual display of the problematic source with error highlighting\n4. **Explanation**: Detailed description of why this is an error\n5. **Suggestions**: Specific guidance on how to fix the problem\n6. **Related Information**: Links to documentation or similar error examples\n\n**Visual Error Context Example**:\n```\nError: Invalid table redefinition in TOML\n  → config.toml:15:1\n\n   13 | [database]\n   14 | host = \"localhost\"  \n   15 | [database]\n      | ^^^^^^^^^^\n   16 | port = 5432\n\nExplanation: Table 'database' has already been defined on line 13. TOML does not allow redefining tables once they have been created.\n\nSuggestion: Use dotted notation [database.connection] to create subtables, or merge the key-value pairs into the existing [database] section.\n```\n\nThis formatting approach provides immediate visual identification of the problem location, clear explanation of the underlying issue, and actionable guidance for resolution.\n\n![Data Structure Transformation](./diagrams/data-structure-mapping.svg)\n\n### Implementation Guidance\n\nThe interactions and data flow components require careful coordination between multiple parsing subsystems. This section provides the foundational infrastructure and integration patterns needed to build a cohesive parsing pipeline.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Pipeline Orchestration | Function-based pipeline with explicit error passing | Class-based pipeline with context objects and middleware |\n| Format Detection | Regex-based pattern matching with scoring | Statistical analysis with machine learning features |\n| Error Reporting | String-based error messages with position info | Rich error objects with suggestions and context |\n| Context Management | Global variables or parameter threading | Context objects with scoped state management |\n| Data Flow Coordination | Direct function calls with return values | Event-driven architecture with observer pattern |\n\n#### Recommended Module Structure\n\n```\nconfig_parser/\n  __init__.py                    ← Public API exports\n  pipeline/\n    __init__.py\n    core.py                     ← Main pipeline orchestration\n    context.py                  ← Pipeline context management\n    errors.py                   ← Error handling infrastructure\n  detection/\n    __init__.py\n    detector.py                 ← Format detection logic\n    signatures.py              ← Format-specific signatures\n    heuristics.py              ← Detection heuristics\n  parsers/\n    __init__.py\n    ini_parser.py              ← INI format parser\n    toml_parser.py             ← TOML format parser  \n    yaml_parser.py             ← YAML format parser\n    base.py                    ← Common parser interface\n  tokenizer/\n    __init__.py\n    tokenizer.py               ← Main tokenization engine\n    tokens.py                  ← Token definitions\n  utils/\n    __init__.py\n    position.py                ← Position tracking utilities\n    conversion.py              ← Data structure conversion\n  tests/\n    test_pipeline.py           ← Integration tests\n    test_detection.py          ← Format detection tests\n    test_errors.py             ← Error handling tests\n```\n\n#### Infrastructure Starter Code\n\n**Pipeline Context Management** (Complete implementation):\n\n```python\n# pipeline/context.py\nfrom typing import List, Optional, Dict, Any\nfrom dataclasses import dataclass, field\nfrom .errors import ParseError\n\n@dataclass\nclass ParseContext:\n    \"\"\"Central context object that flows through the entire parsing pipeline.\"\"\"\n    \n    source_content: str\n    source_path: Optional[str] = None\n    current_position: Optional['Position'] = None\n    detected_format: Optional[str] = None\n    options: Dict[str, Any] = field(default_factory=dict)\n    errors: List[ParseError] = field(default_factory=list)\n    warnings: List[str] = field(default_factory=list)\n    \n    def get_position(self) -> 'Position':\n        \"\"\"Get current source position, creating default if none exists.\"\"\"\n        if self.current_position is None:\n            from ..utils.position import Position\n            self.current_position = Position(line=1, column=1, offset=0)\n        return self.current_position\n    \n    def set_position(self, pos: 'Position') -> None:\n        \"\"\"Update current source position.\"\"\"\n        self.current_position = pos\n    \n    def add_error(self, error: ParseError) -> None:\n        \"\"\"Record error with current context.\"\"\"\n        if error.position is None and self.current_position is not None:\n            error.position = self.current_position\n        self.errors.append(error)\n    \n    def add_warning(self, message: str) -> None:\n        \"\"\"Record warning message.\"\"\"\n        self.warnings.append(message)\n    \n    def has_errors(self) -> bool:\n        \"\"\"Check if any errors have been recorded.\"\"\"\n        return len(self.errors) > 0\n    \n    def get_errors(self) -> List[ParseError]:\n        \"\"\"Retrieve all accumulated errors.\"\"\"\n        return self.errors.copy()\n    \n    def get_format(self) -> Optional[str]:\n        \"\"\"Get detected format identifier.\"\"\"\n        return self.detected_format\n    \n    def set_format(self, fmt: str) -> None:\n        \"\"\"Set format for pipeline.\"\"\"\n        self.detected_format = fmt\n    \n    def get_options(self) -> Dict[str, Any]:\n        \"\"\"Get format-specific parsing options.\"\"\"\n        return self.options.copy()\n    \n    def set_option(self, key: str, value: Any) -> None:\n        \"\"\"Set format-specific parsing option.\"\"\"\n        self.options[key] = value\n\n# pipeline/core.py  \nfrom typing import Dict, Any, Optional, Union\nfrom pathlib import Path\nfrom .context import ParseContext\nfrom .errors import ParseError, ConfigurationError\nfrom ..detection.detector import detect_format\nfrom ..parsers.ini_parser import INIParser\nfrom ..parsers.toml_parser import TOMLParser  \nfrom ..parsers.yaml_parser import YAMLParser\n\ndef parse_config(content: Union[str, Path], format: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"\n    Main configuration parsing entry point.\n    \n    Args:\n        content: Configuration content as string or file path\n        format: Optional format specification ('ini', 'toml', 'yaml')\n        \n    Returns:\n        Nested dictionary representing the configuration structure\n        \n    Raises:\n        ConfigurationError: When parsing fails with detailed error information\n    \"\"\"\n    # TODO 1: Handle content input - if Path, read file; if str, use directly\n    # TODO 2: Create ParseContext with content and metadata\n    # TODO 3: Detect format if not explicitly provided\n    # TODO 4: Route to appropriate parser based on detected format\n    # TODO 5: Handle parsing errors and create comprehensive error reports\n    # TODO 6: Return unified dictionary structure\n    \ndef _read_content(source: Union[str, Path]) -> tuple[str, Optional[str]]:\n    \"\"\"\n    Read configuration content from file or use string directly.\n    Returns (content, file_path) tuple.\n    \"\"\"\n    # TODO: Implement file reading with encoding detection and normalization\n\ndef _create_parser(format_name: str) -> 'BaseParser':\n    \"\"\"Create appropriate parser instance based on format.\"\"\"\n    # TODO: Factory method for parser creation\n\ndef _handle_parsing_errors(context: ParseContext) -> None:\n    \"\"\"Process accumulated errors and create user-friendly reports.\"\"\"\n    # TODO: Transform technical errors into user guidance\n```\n\n**Error Infrastructure** (Complete implementation):\n\n```python\n# pipeline/errors.py\nfrom typing import Optional, List\nfrom dataclasses import dataclass\n\n@dataclass \nclass Position:\n    \"\"\"Source position information for error reporting.\"\"\"\n    line: int\n    column: int  \n    offset: int\n    \n    def __str__(self) -> str:\n        return f\"{self.line}:{self.column}\"\n\nclass ParseError(Exception):\n    \"\"\"Base class for all parsing errors.\"\"\"\n    \n    def __init__(self, message: str, position: Optional[Position] = None, \n                 suggestion: Optional[str] = None):\n        super().__init__(message)\n        self.message = message\n        self.position = position\n        self.suggestion = suggestion\n    \n    def __str__(self) -> str:\n        result = self.message\n        if self.position:\n            result = f\"Line {self.position}: {result}\"\n        if self.suggestion:\n            result += f\"\\nSuggestion: {self.suggestion}\"\n        return result\n\nclass TokenError(ParseError):\n    \"\"\"Error during tokenization phase.\"\"\"\n    pass\n\nclass SyntaxError(ParseError):\n    \"\"\"Error during syntax analysis phase.\"\"\"\n    pass\n    \nclass StructureError(ParseError):\n    \"\"\"Error during structure building phase.\"\"\"\n    pass\n\nclass ConfigurationError(Exception):\n    \"\"\"High-level configuration parsing failure with multiple error details.\"\"\"\n    \n    def __init__(self, errors: List[ParseError], source_path: Optional[str] = None):\n        self.errors = errors\n        self.source_path = source_path\n        \n        # Create summary message\n        if len(errors) == 1:\n            message = f\"Configuration parsing failed: {errors[0].message}\"\n        else:\n            message = f\"Configuration parsing failed with {len(errors)} errors\"\n            \n        super().__init__(message)\n    \n    def format_detailed_report(self) -> str:\n        \"\"\"Generate comprehensive error report for user display.\"\"\"\n        # TODO: Create detailed multi-error report with source context\n        pass\n\ndef create_error_context(source: str, position: Position, context_lines: int = 2) -> str:\n    \"\"\"Generate visual error context showing source location.\"\"\"\n    # TODO 1: Split source into lines\n    # TODO 2: Calculate which lines to show (position +/- context_lines) \n    # TODO 3: Format with line numbers and error indicator\n    # TODO 4: Return formatted context string\n    pass\n```\n\n#### Core Logic Skeleton\n\n**Format Detection Implementation**:\n\n```python\n# detection/detector.py\nfrom typing import Dict, Tuple, List\nimport re\nfrom enum import Enum\n\nclass FormatConfidence(Enum):\n    HIGH = 90\n    MEDIUM = 70  \n    LOW = 50\n    AMBIGUOUS = 25\n\ndef detect_format(content: str) -> str:\n    \"\"\"\n    Automatically detect configuration file format from content.\n    \n    Args:\n        content: Raw configuration file content\n        \n    Returns:\n        Format identifier ('ini', 'toml', 'yaml')\n        \n    Raises:\n        ValueError: When format cannot be determined with confidence\n    \"\"\"\n    # TODO 1: Run signature scanning pass for definitive format markers\n    # TODO 2: Run pattern analysis pass for format-specific characteristics  \n    # TODO 3: Run conflict resolution pass for ambiguous cases\n    # TODO 4: Evaluate confidence and return format or raise error\n    # Hint: Use _scan_format_signatures() for each format, combine scores\n\ndef _scan_format_signatures(content: str) -> Dict[str, int]:\n    \"\"\"\n    Scan for definitive format signatures and return confidence scores.\n    Returns dict with format names as keys, confidence scores as values.\n    \"\"\"\n    # TODO 1: Define regex patterns for TOML signatures ([[tables]], dotted.keys, etc.)\n    # TODO 2: Define regex patterns for INI signatures ([sections], key=value, etc.) \n    # TODO 3: Define regex patterns for YAML signatures (key:, -, indentation, etc.)\n    # TODO 4: Count matches for each format and calculate confidence scores\n    # TODO 5: Return scores dict for conflict resolution\n\ndef _analyze_format_patterns(content: str) -> Dict[str, int]:\n    \"\"\"\n    Analyze statistical patterns for format identification.\n    Returns confidence adjustments based on pattern frequency.\n    \"\"\"\n    # TODO: Implement pattern frequency analysis\n\ndef _resolve_format_conflicts(scores: Dict[str, int]) -> Tuple[str, int]:\n    \"\"\"\n    Apply tie-breaking rules when multiple formats have similar scores.\n    Returns (format_name, final_confidence_score).\n    \"\"\"\n    # TODO: Implement conflict resolution logic\n```\n\n**Pipeline Integration Points**:\n\n```python\n# parsers/base.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\nfrom ..pipeline.context import ParseContext\n\nclass BaseParser(ABC):\n    \"\"\"Base interface for all format-specific parsers.\"\"\"\n    \n    def __init__(self, context: ParseContext):\n        self.context = context\n    \n    @abstractmethod\n    def parse(self, content: str) -> Dict[str, Any]:\n        \"\"\"Parse content and return unified dictionary structure.\"\"\"\n        pass\n    \n    @abstractmethod  \n    def get_format_name(self) -> str:\n        \"\"\"Return format identifier for this parser.\"\"\"\n        pass\n    \n    def add_error(self, error: 'ParseError') -> None:\n        \"\"\"Add error to parsing context.\"\"\"\n        self.context.add_error(error)\n    \n    def add_warning(self, message: str) -> None:\n        \"\"\"Add warning to parsing context.\"\"\"\n        self.context.add_warning(message)\n```\n\n#### Language-Specific Hints\n\n**Python Implementation Notes**:\n- Use `pathlib.Path` for file handling to maintain cross-platform compatibility\n- Implement error context generation using `str.splitlines()` and list slicing for performance\n- Use `dataclasses` for structured error objects to reduce boilerplate\n- Consider using `typing.Union` for flexible input types (string content vs file paths)\n- Use `enum.Enum` for format identifiers and confidence levels to prevent typos\n\n**Error Handling Patterns**:\n- Use context managers for file operations to ensure proper cleanup\n- Implement error accumulation using lists rather than raising immediately \n- Use custom exception hierarchies to enable targeted error handling\n- Include position information in all error objects for debugging\n\n#### Milestone Checkpoint\n\nAfter implementing the interactions and data flow components, verify the integration:\n\n**Integration Test Command**:\n```bash\npython -m pytest tests/test_pipeline.py -v\n```\n\n**Expected Behavior**:\n1. **Format Detection**: Create test files with clear format signatures. The detector should identify each format with high confidence.\n\n2. **Pipeline Flow**: Test the complete pipeline with valid files of each format. All should produce equivalent nested dictionary outputs.\n\n3. **Error Propagation**: Test with deliberately broken configuration files. Errors should include source positions, context, and helpful suggestions.\n\n4. **Multi-Error Collection**: Test with files containing multiple errors. The parser should report all errors found, not just the first one.\n\n**Manual Verification Steps**:\n1. Create a simple test script that calls `parse_config()` with sample files\n2. Verify that INI, TOML, and YAML files with equivalent content produce identical dictionary outputs\n3. Test error reporting by introducing syntax errors - verify that error messages include line numbers and suggestions\n4. Test format detection by removing format hints - verify correct automatic detection\n\n**Success Indicators**:\n- All three parsers integrate cleanly through the common pipeline\n- Error messages include visual source context with line numbers\n- Format detection works reliably on real configuration files\n- Pipeline handles both file paths and string content seamlessly\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Format detection always returns INI | Detection patterns too restrictive | Print intermediate scores from each format scanner | Adjust pattern weights and add more signature patterns |\n| Errors missing source positions | Position not propagated through pipeline | Add logging to track position updates | Ensure position is set in context before creating errors |\n| Pipeline crashes on malformed files | Missing error recovery in component | Add try/catch around each pipeline phase | Implement graceful error handling with continuation |\n| Inconsistent output between formats | Structure conversion differences | Compare parse trees before conversion | Standardize conversion logic in base parser class |\n| Memory usage grows with file size | Error context keeping full source | Profile memory usage during parsing | Limit error context to relevant lines only |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive error detection, reporting, and recovery strategies across all formats\n\nConfiguration file parsing operates in an inherently error-prone environment where malformed input, ambiguous syntax, and edge cases are common. Think of error handling in parsing as building a safety net with multiple layers—each component must detect problems at its level, enrich the error information with context, and decide whether to recover gracefully or halt processing. Unlike typical application errors that occur during runtime, parsing errors must provide educational feedback to users who are editing configuration files, often by hand, and need specific guidance about what went wrong and how to fix it.\n\nThe complexity of error handling in our multi-format parser stems from the fundamental differences in how INI, TOML, and YAML express structure and meaning. Each format has distinct failure modes, from INI's simple line-based issues to TOML's complex table redefinition conflicts to YAML's indentation-driven structural problems. Our error handling system must provide format-specific diagnostics while maintaining consistent error reporting patterns across all parsers.\n\n![Complete Parsing Pipeline Flow](./diagrams/parsing-pipeline.svg)\n\n### Error Classification\n\nParsing errors fall into distinct categories based on where they occur in the processing pipeline and what type of problem they represent. Understanding these categories helps design appropriate detection strategies and recovery mechanisms for each error type.\n\n**Lexical Errors (Token-Level)**\n\nLexical errors occur during tokenization when the character stream cannot be converted into valid tokens. These represent the most fundamental parsing failures, where the input violates basic format syntax rules at the character level. Think of lexical errors as problems a human would catch when reading character by character—invalid escape sequences, unterminated strings, or illegal characters in specific contexts.\n\n| Error Type | Description | Common Causes | Detection Strategy |\n|------------|-------------|---------------|-------------------|\n| Unterminated String | String literal missing closing quote | Multiline strings without proper delimiters, escaped quotes at string end | Track quote nesting depth, validate at line/file boundaries |\n| Invalid Escape Sequence | Backslash followed by invalid character | Incorrect escape codes like `\\q` instead of `\\n` | Pattern matching during string tokenization |\n| Illegal Character | Character not allowed in current context | Unicode characters in identifiers, special chars in numbers | Context-aware character validation |\n| Malformed Number | Invalid numeric literal format | Multiple decimal points, invalid scientific notation | Regex validation and numeric conversion attempts |\n| Invalid Unicode | Malformed UTF-8 sequences or invalid codepoints | File encoding issues, manual hex editing | UTF-8 validation during character reading |\n\nThe tokenizer must detect these errors immediately when encountered because they prevent meaningful token creation. However, the challenge lies in providing helpful error messages that explain not just what character is invalid, but why it's invalid in the current parsing context.\n\n**Syntactic Errors (Grammar-Level)**\n\nSyntactic errors occur when tokens are valid individually but don't follow the grammar rules of the target format. These errors represent violations of format-specific structure rules—like missing equals signs in key-value pairs or incorrect bracket matching in arrays. Think of syntactic errors as problems a human would catch when reading for grammatical structure rather than individual words.\n\n| Error Type | Description | Detection Method | Recovery Options |\n|------------|-------------|------------------|------------------|\n| Missing Assignment Operator | Key without equals or colon separator | Parser expects EQUALS or COLON after key token | Skip line, assume section header, insert default operator |\n| Unmatched Brackets | Opening bracket without corresponding close | Bracket stack tracking during parsing | Insert missing bracket, truncate at boundary |\n| Invalid Key Format | Key contains illegal characters or structure | Key validation against format rules | Sanitize key, reject entry, quote if possible |\n| Malformed Table Header | Section header syntax violations | Bracket counting, path validation | Skip header, use previous section, create default |\n| Invalid Array Structure | Mixed types or malformed array syntax | Type consistency checking during array parsing | Convert to strings, truncate at error, skip malformed elements |\n\nSyntactic errors are particularly challenging because the parser has successfully tokenized the input but cannot interpret the token sequence according to format rules. The decision of whether to recover or abort depends on how fundamental the syntactic violation is to the document structure.\n\n**Structural Errors (Semantic-Level)**\n\nStructural errors occur when syntax is correct but the resulting data structure violates format-specific semantic rules. These are the most complex errors to detect because they require understanding the global document structure and format-specific constraints. Think of structural errors as problems that become apparent only when viewing the entire document—like trying to redefine a table that already exists or creating circular references in nested structures.\n\n| Error Type | Format | Description | Detection Strategy |\n|------------|--------|-------------|-------------------|\n| Table Redefinition | TOML | Attempting to redefine existing table | Symbol table tracking with conflict detection |\n| Key Path Conflict | TOML | Dotted key conflicts with existing table | Path resolution with type checking |\n| Indentation Inconsistency | YAML | Mixed indentation levels or tab/space mixing | Indentation tracking with established level validation |\n| Circular Structure | All | Self-referencing nested structures | Depth tracking and path cycle detection |\n| Type Inconsistency | TOML/YAML | Array elements with incompatible types | Type inference tracking during array construction |\n| Invalid Section Nesting | INI | Sections that create impossible hierarchies | Section path validation during nested structure creation |\n\nStructural errors require maintaining parsing state across the entire document and applying format-specific validation rules as the data structure is constructed. These errors often indicate deeper misunderstandings of format semantics rather than simple syntax mistakes.\n\n> **Design Insight**: The error classification hierarchy mirrors the parsing pipeline—lexical errors block tokenization, syntactic errors block parse tree construction, and structural errors block final data structure creation. Each layer must decide whether to recover locally or propagate the error upward with additional context.\n\n**Context-Dependent Errors**\n\nSome errors can only be detected by examining the relationship between different parts of the document or by understanding the broader parsing context. These errors represent violations of format-specific rules that span multiple lines or sections.\n\n| Error Category | Description | Examples | Detection Approach |\n|----------------|-------------|----------|-------------------|\n| Cross-Reference Violations | References to undefined or invalid targets | TOML array-of-tables referencing undefined tables | Symbol table validation after parsing completion |\n| Scope Violations | Content appearing in invalid contexts | Global INI keys after section definitions | Context stack validation during parsing |\n| Ordering Violations | Content appearing in wrong sequence | YAML mapping keys out of alphabetical order (when required) | Sequence tracking with format-specific rules |\n| Dependency Violations | Missing prerequisites for current content | Nested structure without parent definition | Dependency graph validation |\n\n> **Decision: Error Classification Granularity**\n> - **Context**: Errors can be classified at different levels of detail, from broad categories to specific error codes\n> - **Options Considered**: Simple three-tier system (lexical/syntactic/structural), detailed error codes for each format, hybrid approach with categories and subcodes\n> - **Decision**: Hierarchical classification with main categories and format-specific subcategories\n> - **Rationale**: Allows consistent error handling patterns while preserving format-specific diagnostic information\n> - **Consequences**: Enables both generic error recovery strategies and specialized format-specific handling\n\n### Error Message Design\n\nEffective error messages in configuration parsing must bridge the gap between technical parsing details and user-friendly guidance. The target audience includes both developers integrating configuration files and system administrators editing configuration by hand. Think of error messages as being written by an experienced colleague who understands both the technical requirements and the user's likely mental model of the configuration format.\n\n**Error Message Components**\n\nEvery parsing error message should contain specific components that collectively provide enough information for the user to understand what went wrong and how to fix it. The challenge lies in presenting this information clearly without overwhelming the user with implementation details.\n\n| Component | Purpose | Example Content | Design Guidelines |\n|-----------|---------|-----------------|-------------------|\n| Problem Summary | Clear statement of what went wrong | \"Unterminated string literal\" | Use domain language, avoid parser internals |\n| Location Context | Where the error occurred | \"Line 23, column 15\" | Provide both line/column and visual context |\n| Syntax Context | Surrounding code for visual reference | Show 2-3 lines around error position | Highlight exact error position with markers |\n| Explanation | Why this is considered an error | \"String literals must end with matching quote\" | Reference format specification rules |\n| Suggestion | Specific guidance for fixing the problem | \"Add closing quote or use multiline string syntax\" | Provide actionable steps, not vague advice |\n| Related Information | Links to relevant documentation or similar errors | \"See TOML string specification section 4.2\" | Help users understand broader context |\n\nThe `create_error_context` function standardizes the visual presentation of error location by showing source lines with position markers and highlighting the specific character or token where the error occurred.\n\n**Format-Specific Error Messaging**\n\nEach configuration format has characteristic error patterns that require specialized messaging approaches. The error message must reflect the user's mental model of how the format works, not the internal parsing implementation.\n\n**INI Format Error Messages**\n\nINI errors typically involve line-based parsing issues that are relatively straightforward to diagnose and fix. The mental model for INI users is simple: sections contain key-value pairs, and comments are ignored. Error messages should reinforce this simplicity while providing specific guidance.\n\n| Error Scenario | Technical Issue | User-Friendly Message | Recovery Guidance |\n|-----------------|-----------------|----------------------|-------------------|\n| Missing section header | Global key without section context | \"Key-value pair 'database.host' appears before any section header\" | \"Move this line inside a [section] or add a [DEFAULT] section above it\" |\n| Malformed assignment | Line without equals or colon | \"Line 'database host localhost' is not recognized as section header or key-value pair\" | \"Add '=' or ':' between key and value: 'database.host = localhost'\" |\n| Invalid section name | Section header with illegal characters | \"Section header contains invalid characters: '[data<>base]'\" | \"Remove special characters: '[database]' or quote if necessary\" |\n| Inline comment confusion | Equals sign within quoted value | \"Assignment appears to contain multiple '=' characters\" | \"Quote the entire value: 'url = \\\"http://example.com?id=123\\\"'\" |\n\n**TOML Format Error Messages**\n\nTOML errors are often complex because of the format's rich type system and table semantics. Users frequently struggle with table redefinition rules and dotted key expansion. Error messages must explain not just what's wrong but how TOML's global namespace works.\n\n| Error Scenario | Technical Issue | User-Friendly Message | Explanation |\n|-----------------|-----------------|----------------------|-------------|\n| Table redefinition | Explicit table conflicts with previous definition | \"Cannot redefine table [database] (previously defined at line 15)\" | \"TOML tables can only be defined once. Use dotted keys to add more values or create a [database.connection] subtable\" |\n| Dotted key conflict | Dotted key path conflicts with existing table | \"Key 'database.host' conflicts with table [database.host] at line 8\" | \"A key path cannot contain both a value and a subtable. Choose either 'database.host = \\\"value\\\"' or '[database.host]' with sub-keys\" |\n| Array-of-tables confusion | Mixed array and table syntax | \"Cannot mix array-of-tables [[servers]] with regular table [servers]\" | \"Use either [[servers]] for multiple server entries or [servers] for a single server configuration, not both\" |\n| Type inconsistency | Array contains mixed types | \"Array contains both integer (5) and string (\\\"five\\\") values\" | \"TOML arrays must contain values of the same type. Use strings for all values or create separate arrays\" |\n\n**YAML Format Error Messages**\n\nYAML errors frequently involve indentation and implicit structure creation. Users often struggle with YAML's context-sensitive parsing where the same content can mean different things depending on indentation and surrounding structure.\n\n| Error Scenario | Technical Issue | User-Friendly Message | Indentation Guidance |\n|-----------------|-----------------|----------------------|----------------------|\n| Indentation inconsistency | Current line doesn't match established levels | \"Indentation of 3 spaces doesn't match any previous level (expected 0, 2, or 4)\" | \"Use consistent indentation increments. Choose either 2 or 4 spaces per level and stick with it\" |\n| Mixed tabs and spaces | Line contains both tab and space characters | \"Line contains both tabs and spaces for indentation\" | \"YAML forbids mixing tabs and spaces. Use only spaces for indentation\" |\n| Structural ambiguity | Content could be interpreted multiple ways | \"Mapping key 'items' appears at same level as sequence item\" | \"Indent the mapping key further to make it part of the sequence item, or outdent to make it a sibling\" |\n| Type inference surprise | Value converted to unexpected type | \"Value 'yes' was interpreted as boolean true, not string\" | \"Quote string values that might be interpreted as other types: items: \\\"yes\\\"\" |\n\n> **Decision: Error Message Verbosity**\n> - **Context**: Error messages can range from terse technical descriptions to verbose explanations with examples\n> - **Options Considered**: Brief messages with error codes, verbose messages with full explanations, configurable verbosity levels\n> - **Decision**: Verbose messages by default with option to reduce verbosity\n> - **Rationale**: Configuration files are often edited manually by users who need educational feedback, not just error identification\n> - **Consequences**: Larger error output but significantly improved user experience for configuration debugging\n\n**Error Context Generation**\n\nThe `create_error_context` function generates visual representations of error locations that help users quickly identify and fix problems. This function must handle various edge cases while providing consistent output formatting.\n\n| Context Scenario | Challenge | Solution Approach |\n|------------------|-----------|-------------------|\n| Error at line beginning | No preceding content for context | Show line with position marker at start |\n| Error at line end | May be continuation or termination issue | Show line with marker and indicate if more content expected |\n| Error in long line | Line too wide for terminal display | Truncate line intelligently, keeping error position visible |\n| Error at file boundary | Beginning or end of file | Show available context, indicate file boundary |\n| Multi-line error | Error spans multiple lines | Show all affected lines with range indicators |\n\nThe visual error context uses consistent formatting conventions: line numbers in brackets, position markers with carets or arrows, and highlighting of the specific characters involved in the error. This standardization helps users quickly parse error output across different error types.\n\n![Error Detection and Reporting Flow](./diagrams/error-handling-flow.svg)\n\n### Error Recovery Approaches\n\nError recovery in parsing determines whether the system continues processing after encountering errors, and if so, how it attempts to interpret subsequent input. The fundamental tension in error recovery is between providing comprehensive error reporting (finding multiple problems in one pass) and maintaining parsing accuracy (not generating spurious errors due to incorrect recovery assumptions).\n\n**Recovery Strategy Classification**\n\nDifferent types of parsing errors require different recovery strategies based on their scope and impact on subsequent parsing. The recovery approach must consider both the likelihood of successful continued parsing and the value of finding additional errors in the same document.\n\n**Panic Mode Recovery**\n\nPanic mode recovery involves discarding input tokens until reaching a known synchronization point where parsing can resume reliably. This approach works well for format-specific landmarks that clearly indicate structure boundaries.\n\n| Format | Synchronization Points | Recovery Strategy | Reliability |\n|--------|------------------------|-------------------|-------------|\n| INI | Section headers, blank lines | Skip to next `[section]` or end of current section | High - sections are independent |\n| TOML | Table headers, top-level assignments | Skip to next `[table]` or unindented key assignment | Medium - may skip related content |\n| YAML | Document separators, unindented lines | Skip to next document `---` or zero-indentation content | Low - indentation context is critical |\n\nPanic mode recovery is most effective when the error occurs in a self-contained syntactic unit that can be safely skipped without affecting the interpretation of subsequent content. The challenge lies in determining which synchronization points are truly safe versus which might lead to misinterpretation of later content.\n\n**Error Production Recovery**\n\nError production recovery involves inserting assumed content (like missing punctuation) or making reasonable assumptions about user intent to continue parsing. This approach attempts to \"fix\" the input automatically while recording what assumptions were made.\n\n| Error Type | Recovery Action | Assumption Made | Risk Level |\n|------------|-----------------|-----------------|------------|\n| Missing assignment operator | Insert `=` between key and value | User intended key-value pair | Low |\n| Missing closing quote | Insert quote at line end | String was intended to be single-line | Medium |\n| Missing closing bracket | Insert bracket at section end | User forgot to close array/table | High |\n| Inconsistent indentation | Assume closest matching level | User made spacing mistake | High |\n\nError production recovery requires careful consideration of how likely the assumed fix is to match user intent. Conservative recovery (making minimal assumptions) is generally safer than aggressive recovery (making complex structural assumptions).\n\n**Phrase-Level Recovery**\n\nPhrase-level recovery attempts to identify the boundaries of malformed constructs and skip only the minimal amount of content necessary to resume parsing. This approach requires understanding the syntactic structure of the format to identify phrase boundaries.\n\n| Format Construct | Boundary Indicators | Recovery Scope | Success Indicators |\n|------------------|-------------------|----------------|-------------------|\n| INI key-value pair | Line boundaries, section headers | Single line or continuation sequence | Next line parses as valid key-value or section |\n| TOML value expression | Commas, newlines, closing brackets | Value portion of assignment | Assignment key is preserved, next token is valid |\n| YAML mapping entry | Indentation changes, sequence markers | Single key-value pair | Indentation context remains consistent |\n| YAML sequence item | Sequence markers, indentation outdent | Single list item | List structure is maintained |\n\nThe effectiveness of phrase-level recovery depends on the format's syntactic regularity. Formats with clear delimiters and boundaries support more reliable phrase-level recovery than formats with context-dependent parsing rules.\n\n**Intelligent Recovery Strategies**\n\nAdvanced recovery strategies use knowledge about common error patterns and format-specific semantics to make informed decisions about how to continue parsing after errors.\n\n**Format-Specific Recovery Patterns**\n\nEach configuration format has characteristic error patterns that suggest specific recovery approaches based on observed user behavior and format complexity.\n\n| Format | Common Error Pattern | Intelligent Recovery | Rationale |\n|--------|---------------------|---------------------|-----------|\n| INI | Quoted value with escaped quotes | Scan for next unescaped quote or line boundary | Nested quotes are often user intent, not syntax errors |\n| TOML | Array with trailing comma | Accept trailing comma, continue parsing | Trailing commas are common in other languages |\n| YAML | Inconsistent indentation increment | Calculate greatest common divisor of observed indentations | Users often pick inconsistent but mathematically related indentations |\n| All | Unicode encoding issues | Attempt multiple encoding interpretations | Files are often edited with different tools that handle encoding differently |\n\nIntelligent recovery leverages understanding of how users actually create configuration files, including common mistakes and patterns from other similar formats or programming languages.\n\n**Multi-Pass Recovery Analysis**\n\nFor complex errors that affect document structure, a multi-pass approach can provide better recovery than single-pass strategies. The first pass identifies structural issues, and subsequent passes attempt parsing with different recovery assumptions.\n\n| Pass Number | Analysis Focus | Recovery Goals | Success Criteria |\n|-------------|----------------|----------------|------------------|\n| Pass 1 | Strict parsing with no recovery | Identify all definite errors | Clear error classification and location |\n| Pass 2 | Conservative recovery assumptions | Parse maximum safe content | No spurious errors introduced |\n| Pass 3 | Aggressive recovery with user feedback | Extract any possible valid content | User can validate recovery assumptions |\n\nMulti-pass analysis is particularly valuable for YAML documents where indentation errors can cascade and affect the interpretation of large portions of the document.\n\n> **Decision: Recovery Aggressiveness**\n> - **Context**: Recovery strategies range from conservative (minimal assumptions) to aggressive (maximal content extraction)\n> - **Options Considered**: Always halt on first error, conservative recovery with user confirmation, aggressive recovery with detailed assumption logging\n> - **Decision**: Conservative recovery by default with option for aggressive recovery mode\n> - **Rationale**: Configuration files often contain critical system settings where incorrect assumptions can cause operational problems\n> - **Consequences**: May require multiple parse attempts to extract maximum content, but reduces risk of silent misinterpretation\n\n**Graceful Degradation Patterns**\n\nWhen errors prevent complete parsing of a configuration document, graceful degradation strategies attempt to extract partial information that might still be valuable to the application.\n\n| Degradation Level | Content Extracted | Application Guidance | Risk Assessment |\n|-------------------|-------------------|---------------------|------------------|\n| Section-Level | Complete valid sections only | Use partial configuration with defaults for missing sections | Low - missing sections are explicit |\n| Key-Level | Valid key-value pairs within sections | Skip malformed keys, preserve valid ones | Medium - key relationships might be broken |\n| Value-Level | Keys with parseable values | Use string fallback for unparseable values | High - type mismatches can cause runtime errors |\n\nGraceful degradation must provide clear information to the calling application about what content was successfully parsed and what assumptions or defaults are being applied for missing or malformed content.\n\n**Error Recovery State Management**\n\nEffective error recovery requires maintaining additional parsing state to track recovery decisions and their impact on subsequent parsing. This state helps determine when recovery assumptions prove incorrect and parsing should be abandoned.\n\n| State Information | Purpose | Update Triggers | Decision Points |\n|------------------|---------|-----------------|-----------------|\n| Recovery assumption log | Track what fixes were assumed | Each recovery action | Final validation of assumptions |\n| Confidence level tracking | Measure parsing reliability | Each successful/failed parse decision | Threshold for abandoning recovery |\n| Structural integrity markers | Validate document structure | Major structure completion | Consistency checking of recovered content |\n| Alternative interpretation stack | Track parsing alternatives | Ambiguous syntax encounters | Backtracking to alternative interpretations |\n\nThis state management enables the parser to make informed decisions about when recovery is likely to succeed versus when the accumulated uncertainty makes continued parsing unreliable.\n\n> **Common Pitfalls in Error Recovery**\n> \n> ⚠️ **Pitfall: Over-Aggressive Recovery**\n> Recovery logic that makes too many assumptions about user intent can silently convert malformed configuration into incorrect but syntactically valid results. This is particularly dangerous in configuration parsing where silent errors can cause production system failures.\n> \n> ⚠️ **Pitfall: Recovery Error Cascades**\n> Incorrect recovery from one error can cause subsequent parsing to misinterpret valid content as erroneous. Each recovery decision must be validated against following content to detect when recovery assumptions are incorrect.\n> \n> ⚠️ **Pitfall: Context Loss During Recovery**\n> Recovery strategies that discard too much parsing context (like jumping to the next section) can lose important structural information needed to correctly interpret subsequent content. Context preservation is critical for maintaining parsing accuracy.\n> \n> ⚠️ **Pitfall: Inconsistent Recovery Behavior**\n> Similar errors in different parts of the document should trigger consistent recovery behavior. Inconsistent recovery can confuse users and make error patterns harder to recognize and fix systematically.\n\n### Implementation Guidance\n\nThe error handling implementation requires careful coordination between all parsing components to ensure consistent error detection, enriched propagation, and appropriate recovery strategies. The following guidance provides concrete implementations for the core error handling infrastructure.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error Types | Simple inheritance with base `ParseError` class | Rich error taxonomy with error codes and metadata |\n| Error Context | String formatting with manual line extraction | Template-based error messages with structured context |\n| Error Recovery | Fixed recovery strategies per error type | Configurable recovery policies with success tracking |\n| Error Reporting | Direct exception raising with message | Structured error collection with severity levels |\n| Position Tracking | Line/column counters during parsing | Full source mapping with character ranges |\n\n**Recommended File Structure**\n\n```python\nconfig_parser/\n├── errors/\n│   ├── __init__.py              # Error type exports\n│   ├── base_errors.py           # ParseError, TokenError, SyntaxError, StructureError\n│   ├── error_context.py         # create_error_context, position tracking\n│   ├── error_recovery.py        # Recovery strategies and state management\n│   └── format_specific.py       # INI, TOML, YAML specific error types\n├── parsers/\n│   ├── base_parser.py           # BaseParser with error handling integration\n│   ├── ini_parser.py            # INI parser with error recovery\n│   ├── toml_parser.py           # TOML parser with error recovery\n│   └── yaml_parser.py           # YAML parser with error recovery\n└── tests/\n    ├── test_error_handling.py   # Error detection and recovery tests\n    └── error_test_cases/         # Test configuration files with various errors\n```\n\n**Core Error Infrastructure**\n\n```python\nfrom typing import Optional, List, Dict, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n@dataclass\nclass Position:\n    \"\"\"Represents a position in the source text with line, column, and offset information.\"\"\"\n    line: int\n    column: int\n    offset: int\n    \n    def __str__(self) -> str:\n        return f\"line {self.line}, column {self.column}\"\n\nclass ParseError(Exception):\n    \"\"\"Base class for all parsing errors with rich context information.\"\"\"\n    \n    def __init__(self, message: str, position: Optional[Position] = None, \n                 suggestion: Optional[str] = None):\n        self.message = message\n        self.position = position\n        self.suggestion = suggestion\n        super().__init__(self.format_message())\n    \n    def format_message(self) -> str:\n        \"\"\"Format error message with position and suggestion information.\"\"\"\n        # TODO: Create user-friendly error message combining message, position, and suggestion\n        # TODO: Include position information in human-readable format\n        # TODO: Append suggestion as actionable guidance when available\n        # TODO: Use consistent formatting for all error types\n        pass\n\nclass TokenError(ParseError):\n    \"\"\"Errors that occur during tokenization (lexical analysis).\"\"\"\n    \n    def __init__(self, message: str, position: Optional[Position] = None, \n                 invalid_text: str = \"\", suggestion: Optional[str] = None):\n        self.invalid_text = invalid_text\n        super().__init__(message, position, suggestion)\n\nclass SyntaxError(ParseError):\n    \"\"\"Errors that occur when tokens don't follow format grammar rules.\"\"\"\n    \n    def __init__(self, message: str, position: Optional[Position] = None,\n                 expected_tokens: List[str] = None, actual_token: str = \"\",\n                 suggestion: Optional[str] = None):\n        self.expected_tokens = expected_tokens or []\n        self.actual_token = actual_token\n        super().__init__(message, position, suggestion)\n\nclass StructureError(ParseError):\n    \"\"\"Errors that occur when document structure violates format semantics.\"\"\"\n    \n    def __init__(self, message: str, position: Optional[Position] = None,\n                 conflicting_position: Optional[Position] = None,\n                 suggestion: Optional[str] = None):\n        self.conflicting_position = conflicting_position\n        super().__init__(message, position, suggestion)\n\ndef create_error_context(source_content: str, position: Position, \n                        context_lines: int = 2) -> str:\n    \"\"\"Generate visual error context showing source lines around error position.\"\"\"\n    # TODO: Split source content into lines and identify target line\n    # TODO: Calculate start and end line numbers for context window\n    # TODO: Format line numbers with consistent width and padding\n    # TODO: Add position marker (caret or arrow) pointing to exact error location\n    # TODO: Handle edge cases: file start/end, very long lines, empty lines\n    # TODO: Return formatted string with line numbers, content, and position marker\n    pass\n```\n\n**Error Recovery Infrastructure**\n\n```python\nfrom enum import Enum\nfrom typing import Dict, List, Callable, Optional\nfrom dataclasses import dataclass, field\n\nclass RecoveryStrategy(Enum):\n    \"\"\"Available error recovery strategies.\"\"\"\n    HALT = \"halt\"                    # Stop parsing on first error\n    PANIC_MODE = \"panic_mode\"        # Skip to synchronization point\n    ERROR_PRODUCTION = \"error_production\"  # Insert assumed content\n    PHRASE_LEVEL = \"phrase_level\"    # Skip minimal syntactic unit\n\n@dataclass\nclass RecoveryDecision:\n    \"\"\"Represents a recovery decision made during parsing.\"\"\"\n    strategy: RecoveryStrategy\n    assumption: str                  # What assumption was made\n    confidence: float                # Confidence in recovery (0.0-1.0)\n    tokens_skipped: int             # Number of tokens skipped\n    content_inserted: str           # Any content inserted by recovery\n\n@dataclass\nclass ErrorRecoveryState:\n    \"\"\"Tracks error recovery state during parsing.\"\"\"\n    decisions: List[RecoveryDecision] = field(default_factory=list)\n    confidence_threshold: float = 0.7\n    max_consecutive_recoveries: int = 5\n    consecutive_recovery_count: int = 0\n    \n    def should_continue_recovery(self) -> bool:\n        \"\"\"Determine if parsing should continue after current error.\"\"\"\n        # TODO: Check if consecutive recovery count exceeds maximum\n        # TODO: Calculate average confidence of recent recovery decisions\n        # TODO: Return False if confidence drops below threshold\n        # TODO: Consider total number of recovery attempts in document\n        pass\n    \n    def record_recovery(self, decision: RecoveryDecision) -> None:\n        \"\"\"Record a recovery decision and update state.\"\"\"\n        # TODO: Append decision to decisions list\n        # TODO: Update consecutive recovery count\n        # TODO: Adjust confidence thresholds based on recovery success patterns\n        # TODO: Reset consecutive count on successful parsing between errors\n        pass\n\nclass ErrorRecoveryManager:\n    \"\"\"Manages error recovery strategies and decisions.\"\"\"\n    \n    def __init__(self):\n        self.recovery_strategies: Dict[type, Callable] = {}\n        self.synchronization_points = {\n            'ini': [r'\\[.*\\]', r'^\\s*$'],           # Section headers, blank lines\n            'toml': [r'\\[.*\\]', r'^[a-zA-Z].*='],   # Table headers, top-level keys\n            'yaml': [r'^---', r'^[^\\s]']            # Document separators, unindented content\n        }\n    \n    def register_recovery_strategy(self, error_type: type, \n                                  strategy_func: Callable) -> None:\n        \"\"\"Register a recovery strategy for specific error type.\"\"\"\n        # TODO: Store strategy function for error type\n        # TODO: Validate that strategy function has correct signature\n        # TODO: Allow multiple strategies per error type with priority ordering\n        pass\n    \n    def attempt_recovery(self, error: ParseError, parser_state: Dict[str, Any],\n                        format_name: str) -> Optional[RecoveryDecision]:\n        \"\"\"Attempt to recover from parsing error using appropriate strategy.\"\"\"\n        # TODO: Determine recovery strategy based on error type and parser state\n        # TODO: Check if recovery is advisable based on current recovery state\n        # TODO: Execute recovery strategy and capture decision information\n        # TODO: Validate recovery assumptions against subsequent content when possible\n        # TODO: Return RecoveryDecision with strategy details and confidence level\n        pass\n```\n\n**Format-Specific Error Handling**\n\n```python\nclass INIParsingError(SyntaxError):\n    \"\"\"Specialized error for INI format parsing issues.\"\"\"\n    \n    def __init__(self, message: str, line_number: int, line_content: str,\n                 suggestion: Optional[str] = None):\n        position = Position(line=line_number, column=1, offset=0)\n        self.line_content = line_content\n        super().__init__(message, position, suggestion=suggestion)\n\nclass TOMLTableRedefinitionError(StructureError):\n    \"\"\"Error for TOML table redefinition conflicts.\"\"\"\n    \n    def __init__(self, table_path: List[str], original_position: Position,\n                 redefinition_position: Position):\n        table_name = \".\".join(table_path)\n        message = f\"Cannot redefine table [{table_name}]\"\n        suggestion = f\"Use dotted keys to add values or create subtable like [{table_name}.subtable]\"\n        super().__init__(message, redefinition_position, original_position, suggestion)\n\nclass YAMLIndentationError(StructureError):\n    \"\"\"Error for YAML indentation inconsistencies.\"\"\"\n    \n    def __init__(self, current_indent: int, expected_indents: List[int],\n                 position: Position):\n        self.current_indent = current_indent\n        self.expected_indents = expected_indents\n        message = f\"Indentation of {current_indent} spaces doesn't match established levels\"\n        suggestion = f\"Use one of these indentation levels: {expected_indents}\"\n        super().__init__(message, position, suggestion=suggestion)\n\ndef create_format_specific_error(format_name: str, error_type: str,\n                               context: Dict[str, Any]) -> ParseError:\n    \"\"\"Factory function for creating format-specific error instances.\"\"\"\n    # TODO: Dispatch to appropriate error class based on format and error type\n    # TODO: Extract relevant context information for error class constructor\n    # TODO: Generate format-appropriate error message and suggestion\n    # TODO: Return properly constructed error instance with full context\n    pass\n```\n\n**Error Collection and Reporting**\n\n```python\n@dataclass\nclass ConfigurationError:\n    \"\"\"Top-level error containing all parsing errors for a configuration file.\"\"\"\n    errors: List[ParseError]\n    source_path: Optional[str] = None\n    recovery_decisions: List[RecoveryDecision] = field(default_factory=list)\n    \n    def has_fatal_errors(self) -> bool:\n        \"\"\"Check if errors prevent using any configuration content.\"\"\"\n        # TODO: Classify errors by severity (fatal vs recoverable)\n        # TODO: Return True if any StructureError or unrecovered TokenError present\n        # TODO: Consider recovery decision confidence in fatal error determination\n        pass\n    \n    def format_error_report(self, include_suggestions: bool = True,\n                          include_context: bool = True) -> str:\n        \"\"\"Generate comprehensive error report for user consumption.\"\"\"\n        # TODO: Group related errors together (same line, same construct)\n        # TODO: Sort errors by position (line number, then column)\n        # TODO: Format each error with context and suggestions when requested\n        # TODO: Include recovery decision summary if any recoveries were attempted\n        # TODO: Provide document-level guidance for common error patterns\n        pass\n\nclass ErrorCollector:\n    \"\"\"Collects and manages errors during parsing process.\"\"\"\n    \n    def __init__(self, source_path: Optional[str] = None):\n        self.errors: List[ParseError] = []\n        self.source_path = source_path\n        self.recovery_decisions: List[RecoveryDecision] = []\n    \n    def add_error(self, error: ParseError) -> None:\n        \"\"\"Add error to collection with automatic position tracking.\"\"\"\n        # TODO: Append error to errors list\n        # TODO: Ensure error has position information when available\n        # TODO: Check for duplicate errors at same position\n        # TODO: Maintain errors in position order for consistent reporting\n        pass\n    \n    def has_errors(self) -> bool:\n        \"\"\"Check if any errors have been collected.\"\"\"\n        return len(self.errors) > 0\n    \n    def create_configuration_error(self) -> ConfigurationError:\n        \"\"\"Create final ConfigurationError with all collected information.\"\"\"\n        # TODO: Return ConfigurationError with current errors and recovery decisions\n        # TODO: Include source path information when available\n        # TODO: Sort errors by position for consistent presentation\n        pass\n```\n\n**Milestone Checkpoints**\n\n**Milestone 1 - INI Error Handling**: Test error detection for malformed INI files:\n```bash\n# Test with malformed INI file containing various error types\npython -m pytest tests/test_ini_error_handling.py -v\n# Verify error messages are helpful and include suggestions\n# Check that recovery allows parsing of valid sections despite errors\n```\n\n**Milestone 2-3 - TOML Error Handling**: Test complex TOML error scenarios:\n```bash\n# Test TOML table redefinition and dotted key conflicts\npython -m pytest tests/test_toml_error_handling.py -v\n# Verify symbol table conflict detection works correctly\n# Check that error messages explain TOML's global namespace rules\n```\n\n**Milestone 4 - YAML Error Handling**: Test indentation and structure errors:\n```bash\n# Test YAML indentation consistency and type inference\npython -m pytest tests/test_yaml_error_handling.py -v\n# Verify indentation error messages suggest specific fixes\n# Check that recovery maintains structural integrity\n```\n\n**Debugging Tips**\n\n| Symptom | Likely Cause | Diagnosis Method | Fix |\n|---------|--------------|------------------|-----|\n| Error messages without position info | Position not tracked during parsing | Add logging to position tracking functions | Ensure every token creation updates position |\n| Generic error messages | Error context not enriched during propagation | Check error creation calls for context info | Pass parser state to error constructors |\n| Recovery causes spurious errors | Recovery assumptions don't match actual content | Log recovery decisions and validate against following tokens | Add recovery validation logic |\n| Same error reported multiple times | Error detection in multiple parsing phases | Check error deduplication in error collector | Filter duplicate errors by position and type |\n| Confusing error suggestions | Suggestions don't match actual error context | Review format-specific error message templates | Improve suggestion logic with context awareness |\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive testing approach ensuring correct implementation and validation across all parsing components\n\nTesting a multi-format configuration parser requires a sophisticated approach that validates correctness across fundamentally different syntactic paradigms while ensuring each component maintains its specific behavioral guarantees. Think of testing configuration parsers like quality assurance for a universal translator - we must verify that each language (format) is correctly understood, that the translation process preserves meaning accurately, and that error conditions are handled gracefully across all supported languages. The challenge lies in designing test strategies that capture both the unique characteristics of each format and the unified behavior expected from the complete parsing system.\n\nThe testing strategy must address multiple layers of complexity: lexical analysis correctness across different tokenization approaches, syntactic parsing accuracy for nested structures, semantic validation of type inference and data conversion, and integration behavior when components work together. Each milestone introduces specific testing requirements that build upon previous foundations while introducing new edge cases and validation needs.\n\n### Test Categories and Coverage\n\nConfiguration parser testing requires a multi-dimensional approach that validates behavior across different abstraction levels, error conditions, and format-specific requirements. The testing matrix spans from low-level tokenization correctness to high-level integration scenarios, ensuring comprehensive coverage of the parsing pipeline.\n\n**Unit Testing Coverage** forms the foundation of our testing strategy, focusing on individual component behavior in isolation. Each component must be tested against its specific responsibilities and interface contracts. Unit tests provide fast feedback during development and enable confident refactoring by establishing behavioral baselines.\n\n| Component | Test Focus | Key Scenarios | Coverage Requirements |\n|-----------|------------|---------------|---------------------|\n| `BaseTokenizer` | Token generation accuracy | String literals, escape sequences, position tracking | All token types, edge cases, malformed input |\n| `INIParser` | Section parsing, key-value extraction | Global keys, nested sections, comment handling | All INI syntax variants, whitespace handling |\n| `TOMLParser` | Table creation, type inference | Dotted keys, array-of-tables, inline structures | TOML specification compliance, conflict detection |\n| `YAMLParser` | Indentation tracking, structure building | Block syntax, flow syntax, type conversion | Indentation edge cases, mixed content types |\n| Format Detection | Format identification accuracy | Ambiguous content, mixed syntax | High confidence detection, fallback behavior |\n\n**Integration Testing Coverage** validates component interactions and end-to-end parsing behavior. Integration tests ensure that the tokenizer-parser pipeline produces correct results and that error information flows properly between components. These tests catch impedance mismatches between components and validate the complete parsing workflow.\n\n| Integration Scenario | Test Focus | Validation Points | Error Conditions |\n|---------------------|------------|-------------------|------------------|\n| File-to-Dictionary Pipeline | Complete parsing accuracy | Input file → parsed structure consistency | Malformed files, encoding issues, large files |\n| Cross-Format Consistency | Equivalent configurations produce same output | Semantic equivalence across INI/TOML/YAML | Format-specific limitations, type coercion differences |\n| Error Context Propagation | Error information enrichment | Position accuracy, helpful messages | Multi-level errors, recovery context |\n| Format Detection Integration | Automatic format selection | Correct parser selection, confidence levels | Ambiguous content, unknown formats |\n\n**Property-Based Testing Coverage** validates parser behavior across broad input spaces by generating test cases that explore edge conditions and invariant properties. Property-based tests are particularly valuable for parsers because they can discover subtle bugs in tokenization state machines and recursive parsing algorithms.\n\n| Property Category | Invariant Properties | Generation Strategy | Validation Approach |\n|-------------------|---------------------|-------------------|-------------------|\n| Tokenization Roundtrip | `tokenize(detokenize(tokens)) == tokens` | Random token sequences | Token equality, position consistency |\n| Parse Tree Structure | Valid input produces well-formed tree | Grammar-compliant generation | Structure validation, parent-child consistency |\n| Type Inference Consistency | Same semantic value infers same type | Value variation generation | Type stability, conversion accuracy |\n| Error Recovery Consistency | Errors don't corrupt subsequent parsing | Malformed input injection | State isolation, recovery effectiveness |\n\n**Regression Testing Coverage** ensures that fixes and enhancements don't break existing functionality. Regression tests capture specific bugs that were discovered and fixed, preventing their reintroduction. This category grows over time as edge cases are discovered and resolved.\n\n| Regression Category | Source of Test Cases | Maintenance Strategy | Update Triggers |\n|--------------------|---------------------|---------------------|-----------------|\n| Bug Fix Validation | Discovered parsing failures | Permanent test retention | Every bug fix adds test case |\n| Edge Case Preservation | Boundary condition discoveries | Categorized edge case library | Format specification updates |\n| Performance Regression | Performance degradation incidents | Benchmark integration | Significant algorithmic changes |\n| Compatibility Maintenance | Version upgrade issues | Cross-version test suite | Dependency updates, format changes |\n\n> **Key Insight:** The test categorization strategy ensures that each type of failure is caught by the most appropriate testing approach. Unit tests catch component logic errors quickly, integration tests catch interface mismatches, property-based tests discover edge cases, and regression tests prevent backsliding.\n\n**Error Condition Testing Coverage** validates parser behavior when encountering malformed input, resource constraints, and exceptional conditions. Error testing is particularly critical for parsers because they must handle arbitrary user input gracefully while providing helpful feedback for fixing configuration issues.\n\n| Error Category | Test Scenarios | Expected Behavior | Recovery Validation |\n|----------------|----------------|-------------------|-------------------|\n| Syntax Errors | Invalid characters, malformed structures | Specific error messages, accurate positions | Parsing continuation, error accumulation |\n| Type Errors | Invalid type conversions, incompatible values | Type-specific error details, suggested fixes | Type inference fallbacks, default handling |\n| Structure Errors | Invalid nesting, conflicting definitions | Structural context, conflict locations | Partial structure preservation, data recovery |\n| Resource Errors | Large files, deep nesting, memory pressure | Graceful degradation, resource reporting | Streaming alternatives, size limits |\n\n**Format-Specific Testing Coverage** addresses the unique characteristics and edge cases of each supported configuration format. Format-specific tests ensure compliance with format specifications and handle format-specific corner cases that don't apply to other formats.\n\n| Format | Specific Test Areas | Critical Edge Cases | Compliance Validation |\n|--------|-------------------|-------------------|---------------------|\n| INI | Section headers, key-value variants, comment styles | Global keys, inline comments, quoted values | Loose INI standard interpretations |\n| TOML | Table definitions, array-of-tables, type system | Table redefinition, dotted key conflicts, datetime formats | TOML v1.0.0 specification compliance |\n| YAML | Indentation semantics, flow/block mixing, implicit typing | Tab vs space indentation, implicit type surprises | YAML subset specification adherence |\n\n### Milestone Verification Points\n\nEach implementation milestone requires specific verification criteria that confirm successful completion of core functionality before proceeding to more advanced features. Milestone verification provides concrete checkpoints that validate both functional correctness and implementation quality.\n\n**Milestone 1: INI Parser Verification** establishes the foundation for configuration parsing by validating line-based parsing, section organization, and comment handling. INI parser verification focuses on the fundamental concepts of section-based organization and key-value extraction that underlie more complex parsing scenarios.\n\n| Verification Category | Success Criteria | Test Validation | Implementation Quality |\n|---------------------|------------------|----------------|----------------------|\n| Section Header Parsing | `[section]` creates nested dictionary entry | Section names extracted correctly, nested structure created | Bracket validation, whitespace handling, invalid section recovery |\n| Key-Value Processing | Both `key=value` and `key: value` supported | Values parsed with whitespace trimming, type inference applied | Quote handling, escape sequence processing, inline comment separation |\n| Comment Handling | `;` and `#` comments ignored appropriately | Comment lines skipped, inline comments preserved/ignored based on configuration | Comment detection accuracy, mixed comment style handling |\n| Global Key Support | Keys before first section handled correctly | Global namespace created, keys accessible in output structure | Global section naming, namespace organization |\n\nThe INI parser milestone verification requires comprehensive testing of edge cases that commonly trip up implementations. Testing must validate that the parser handles keys outside of sections appropriately, processes inline comments correctly without breaking on `=` characters inside quoted strings, and supports both common INI delimiters (`=` and `:`) with consistent behavior.\n\n**Milestone Verification Test Suite for INI Parser:**\n\n| Test Case Category | Test Scenarios | Expected Results | Failure Indicators |\n|-------------------|----------------|------------------|-------------------|\n| Basic Structure | Simple sections with key-value pairs | Nested dictionary with section keys | Missing sections, flat structure, incorrect nesting |\n| Edge Cases | Empty sections, keys with no values, quoted strings with delimiters | Appropriate defaults, quote processing, delimiter isolation | Parsing failures, incorrect value extraction, quote handling errors |\n| Comment Processing | Line comments, inline comments, mixed comment styles | Comments ignored or preserved based on parser configuration | Comment content included in values, parsing errors on comment lines |\n| Error Recovery | Malformed section headers, invalid key syntax | Specific error messages, parsing continuation | Parser crashes, generic error messages, parsing termination |\n\n**Milestone 2: TOML Tokenizer Verification** validates lexical analysis capabilities required for structured format parsing. TOML tokenizer verification focuses on accurate token generation, complex string handling, and position tracking that enables meaningful error reporting.\n\n| Verification Category | Success Criteria | Token Accuracy | Error Handling |\n|---------------------|------------------|----------------|----------------|\n| Basic Token Types | All TOML grammar elements tokenized correctly | `STRING`, `NUMBER`, `BOOLEAN`, `IDENTIFIER` tokens generated accurately | Invalid token detection, position tracking, recovery strategies |\n| String Literal Handling | Basic, literal, and multiline strings processed correctly | Escape sequence processing, quote type differentiation, multiline joining | Quote mismatch detection, escape sequence validation, multiline boundary handling |\n| Numeric Processing | Integers, floats, dates, times recognized as distinct types | Type-specific token generation, format validation, underscore handling | Invalid numeric formats, overflow detection, format compliance |\n| Position Tracking | Line and column numbers accurate for all tokens | `Position` objects reflect true source locations | Position drift, multiline position errors, tab handling |\n\nTOML tokenizer verification requires extensive testing of string literal variants because TOML supports multiple string syntaxes with different escape processing rules. The tokenizer must correctly differentiate between basic strings (which process escapes) and literal strings (which treat backslashes literally), while handling multiline variants of both string types according to TOML specification rules.\n\n**Milestone Verification Test Suite for TOML Tokenizer:**\n\n| Test Case Category | Token Scenarios | Validation Approach | Quality Indicators |\n|-------------------|----------------|-------------------|-------------------|\n| String Variants | Basic strings with escapes, literal strings, multiline strings | Token type accuracy, value processing correctness | Escape sequence handling, quote processing, newline normalization |\n| Numeric Types | Integers with underscores, scientific notation floats, ISO dates | Type-specific token generation, format compliance | Underscore handling, exponent processing, datetime parsing |\n| Structural Elements | Brackets, dots, equals, commas in various combinations | Token sequence accuracy, delimiter recognition | Tokenization order, structural token identification |\n| Error Conditions | Unterminated strings, invalid numeric formats, illegal characters | Error token generation, position accuracy, recovery behavior | Error context quality, position precision, tokenization continuation |\n\n**Milestone 3: TOML Parser Verification** validates recursive descent parsing capabilities for complex nested structures. TOML parser verification focuses on table creation, array-of-tables handling, and conflict detection that ensures TOML specification compliance.\n\n| Verification Category | Success Criteria | Structure Validation | Conflict Detection |\n|---------------------|------------------|-------------------|-------------------|\n| Table Parsing | `[table]` and `[table.subtable]` create correct nested structure | Dictionary nesting accuracy, key path resolution | Table redefinition detection, implicit table conflicts |\n| Array-of-Tables | `[[array.of.tables]]` creates list of dictionary entries | Array structure creation, element organization | Array redefinition as table, mixed array/table conflicts |\n| Dotted Key Expansion | `physical.color = 'orange'` creates nested structure | Automatic structure creation, key path processing | Conflicting key definitions, type mismatches |\n| Inline Structures | Inline tables `{key = value}` and arrays `[1, 2, 3]` parsed correctly | Nested structure creation, type inference | Syntax error recovery, nesting validation |\n\nTOML parser verification requires sophisticated conflict detection testing because TOML has complex rules about table redefinition and key conflicts. The parser must detect when a dotted key attempts to redefine an existing table, when a table is defined multiple times, and when array-of-tables syntax conflicts with existing definitions.\n\n**Milestone Verification Test Suite for TOML Parser:**\n\n| Test Case Category | Parsing Scenarios | Structure Validation | Error Detection |\n|-------------------|------------------|-------------------|----------------|\n| Table Hierarchies | Nested table definitions, dotted table paths, implicit table creation | Nesting accuracy, path resolution, structure completeness | Table redefinition detection, path conflict identification |\n| Value Types | Strings, numbers, booleans, arrays, inline tables | Type inference accuracy, nested structure creation | Type conversion errors, syntax validation |\n| Complex Structures | Mixed arrays, nested inline tables, array-of-tables with complex values | Deep structure accuracy, reference integrity | Circular reference detection, depth validation |\n| Specification Compliance | Edge cases from TOML specification, conflict scenarios | Specification adherence, edge case handling | Standard compliance validation, error message quality |\n\n**Milestone 4: YAML Subset Parser Verification** validates indentation-sensitive parsing for hierarchical block structures. YAML parser verification focuses on indentation stack management, structure type inference, and scalar type conversion.\n\n| Verification Category | Success Criteria | Structure Management | Type Inference |\n|---------------------|------------------|-------------------|----------------|\n| Indentation Processing | Block structure determined correctly from indentation | Stack-based nesting, level transitions, structure preservation | Indentation error detection, tab vs space validation |\n| Mapping Processing | `key: value` pairs create dictionary entries | Dictionary structure, nested mappings, key uniqueness | Key conflict detection, value processing |\n| Sequence Processing | `- item` lists create ordered arrays | Array structure, nested sequences, mixed content | Item processing, nesting validation |\n| Flow Syntax | `[list]` and `{map}` inline syntax parsed correctly | Inline structure creation, flow/block mixing | Syntax validation, nesting consistency |\n\nYAML parser verification requires extensive indentation testing because YAML's indentation sensitivity creates numerous edge cases around tab handling, inconsistent indentation levels, and mixed indentation styles. The parser must maintain strict indentation validation while providing helpful error messages for common indentation mistakes.\n\n**Milestone Verification Test Suite for YAML Parser:**\n\n| Test Case Category | Indentation Scenarios | Structure Validation | Error Handling |\n|-------------------|---------------------|-------------------|----------------|\n| Block Structures | Consistent indentation, nested mappings and sequences | Structure accuracy, nesting preservation | Indentation error detection, level validation |\n| Mixed Content | Mappings containing sequences, sequences containing mappings | Content type handling, structure transitions | Type conflict detection, mixed content validation |\n| Scalar Processing | Quoted strings, unquoted strings, numeric values, booleans | Type inference accuracy, value processing | Type conversion errors, ambiguous value handling |\n| Edge Cases | Empty documents, single-item structures, deeply nested content | Minimal structure handling, depth management | Edge case recovery, structure validation |\n\n### Test Data Strategy\n\nEffective configuration parser testing requires carefully curated test datasets that systematically explore the input space while covering critical edge cases and error conditions. The test data strategy must balance comprehensive coverage with maintainable test organization, ensuring that test cases remain understandable and debuggable as the test suite grows.\n\n**Structured Test Data Organization** provides a systematic approach to organizing test cases across multiple dimensions: format types, feature complexity, error conditions, and edge cases. The organization strategy enables efficient test maintenance and comprehensive coverage validation.\n\n| Test Data Category | Organization Principle | File Structure | Content Strategy |\n|-------------------|----------------------|---------------|------------------|\n| Golden Path Cases | Common usage patterns for each format | `test-data/golden/[format]/[feature].ext` | Real-world configuration examples, typical use cases |\n| Edge Case Library | Boundary conditions and corner cases | `test-data/edge-cases/[format]/[category]/` | Minimal reproducible cases, focused edge conditions |\n| Error Case Collection | Invalid input scenarios | `test-data/errors/[format]/[error-type]/` | Systematic error exploration, recovery validation |\n| Cross-Format Equivalence | Semantically equivalent configurations | `test-data/equivalence/[scenario]/` | Same logical configuration in multiple formats |\n\nThe structured organization enables systematic test coverage analysis and makes it easy to add new test cases as edge cases are discovered. Each category serves a specific testing purpose and can be processed with appropriate test harness logic.\n\n**Golden Path Test Data Strategy** focuses on realistic configuration scenarios that represent common usage patterns. Golden path tests validate that the parser handles typical use cases correctly and produces expected output structures. These tests serve as acceptance criteria and regression protection for core functionality.\n\n| Format | Golden Path Scenarios | Test Data Characteristics | Validation Approach |\n|--------|---------------------|--------------------------|-------------------|\n| INI | Application settings, database configuration, service parameters | Multiple sections, varied value types, mixed comment styles | Structure accuracy, value processing, type inference |\n| TOML | Package configuration, build settings, service definitions | Nested tables, arrays, mixed value types, complex structures | Nesting validation, type system compliance, specification adherence |\n| YAML | Application config, deployment descriptors, data serialization | Block structures, sequences, mappings, mixed content types | Indentation handling, type inference, structure preservation |\n\nGolden path test data should represent configurations that users would actually write, not contrived examples that exist only to test specific features. The test data should include realistic key names, appropriate value ranges, and natural organization patterns that reflect how each format is typically used.\n\n**Edge Case Test Data Strategy** systematically explores boundary conditions and corner cases that often reveal parsing bugs. Edge case testing requires carefully constructed minimal examples that isolate specific problematic conditions without introducing additional complexity.\n\n| Edge Case Category | Test Data Design | Validation Focus | Coverage Goals |\n|-------------------|----------------|------------------|---------------|\n| Empty Content | Empty files, whitespace-only files, comment-only files | Parser initialization, minimal input handling | Zero-content graceful handling |\n| Boundary Values | Maximum nesting depth, longest strings, largest numbers | Resource handling, algorithmic limits | Performance boundaries, memory usage |\n| Whitespace Sensitivity | Mixed tabs/spaces, trailing whitespace, Unicode whitespace | Whitespace processing, normalization | Whitespace semantic preservation |\n| Unicode Complexity | Non-ASCII characters, emoji, combining characters, RTL text | Unicode handling, encoding processing | International character support |\n\nEdge case test data must be constructed systematically to ensure comprehensive coverage of boundary conditions. Each edge case should focus on a single problematic condition to make failures easy to diagnose and fix.\n\n**Error Case Test Data Strategy** validates parser behavior when encountering invalid input by providing systematic exploration of error conditions. Error case testing ensures that parsers fail gracefully and provide helpful error messages that enable users to fix their configuration files.\n\n| Error Category | Test Data Design | Error Validation | Recovery Testing |\n|----------------|----------------|------------------|------------------|\n| Syntax Errors | Invalid characters, malformed structures, missing delimiters | Error message quality, position accuracy | Parsing continuation, error accumulation |\n| Type Errors | Invalid type conversions, incompatible value assignments | Type-specific error reporting, conversion failure handling | Type inference fallbacks, default value handling |\n| Structure Errors | Invalid nesting, conflicting definitions, circular references | Structural validation, conflict detection | Partial structure preservation, data salvage |\n| Resource Errors | Extremely large files, deeply nested structures, memory pressure | Resource exhaustion handling, graceful degradation | Resource limit enforcement, streaming alternatives |\n\nError case test data should be constructed to trigger specific error conditions while remaining understandable to developers debugging parsing failures. Each error case should include expected error messages and recovery behavior validation.\n\n**Cross-Format Equivalence Test Strategy** validates that semantically equivalent configurations produce consistent results across different formats. Equivalence testing ensures that format choice doesn't affect application behavior when configurations represent the same logical settings.\n\n| Equivalence Scenario | Format Variations | Semantic Validation | Difference Handling |\n|---------------------|-------------------|-------------------|-------------------|\n| Simple Key-Value | INI sections, TOML tables, YAML mappings | Value equality, structure equivalence | Format-specific limitations, type coercion differences |\n| Nested Structures | INI dotted sections, TOML nested tables, YAML block nesting | Hierarchy preservation, access path consistency | Nesting depth limits, structure representation differences |\n| Array Handling | INI repeated keys, TOML arrays, YAML sequences | Array content equality, ordering preservation | Format array support differences, mixed type handling |\n| Type Representation | Format-specific type syntax, implicit vs explicit typing | Type consistency, conversion accuracy | Type system differences, inference variations |\n\nCross-format equivalence testing helps validate that the unified output format successfully abstracts away format differences while preserving semantic meaning. These tests catch cases where format-specific processing introduces unintended behavioral differences.\n\n**Test Data Maintenance Strategy** ensures that test datasets remain current, comprehensive, and maintainable as the parser implementation evolves. Maintenance strategy addresses test data organization, update procedures, and quality validation.\n\n| Maintenance Aspect | Strategy Approach | Automation Support | Quality Assurance |\n|-------------------|------------------|-------------------|-------------------|\n| Test Case Discovery | Systematic exploration of input space, bug-driven case addition | Automated test case generation, property-based case discovery | Coverage analysis, gap identification |\n| Data Quality Validation | Format compliance checking, expected result verification | Automated validation pipelines, consistency checking | Regular audit procedures, quality metrics |\n| Update Procedures | Version control integration, change tracking, regression prevention | Automated update validation, backwards compatibility testing | Change impact analysis, regression detection |\n| Organization Maintenance | Consistent categorization, clear naming conventions, duplicate elimination | Automated organization validation, duplicate detection | Regular cleanup procedures, organization audits |\n\n> **Critical Insight:** Test data strategy success depends on systematic organization and comprehensive coverage rather than large volumes of ad-hoc test cases. Well-organized test data enables efficient debugging, comprehensive validation, and maintainable test suites.\n\n![Complete Parsing Pipeline Flow](./diagrams/parsing-pipeline.svg)\n\nThe test data strategy must support both automated testing workflows and manual debugging scenarios. Test data should be organized to enable easy identification of relevant test cases, efficient test execution, and clear failure diagnosis when tests fail.\n\n**Performance Test Data Strategy** validates parser behavior under resource pressure and ensures that algorithmic complexity remains acceptable for realistic input sizes. Performance testing requires carefully constructed datasets that stress specific performance characteristics without introducing artificial complexity.\n\n| Performance Aspect | Test Data Characteristics | Measurement Focus | Validation Criteria |\n|-------------------|--------------------------|-------------------|-------------------|\n| File Size Scaling | Incrementally larger configuration files | Memory usage, parsing time, throughput | Linear scaling behavior, memory efficiency |\n| Nesting Depth | Increasingly deep nested structures | Stack usage, recursion handling, algorithm complexity | Graceful degradation, depth limit handling |\n| Key Volume | Large numbers of keys and sections | Hash table performance, lookup efficiency | Consistent access times, memory organization |\n| String Processing | Large string values, complex escape sequences | String processing efficiency, memory allocation | String handling optimization, garbage collection impact |\n\nPerformance test data should reflect realistic scaling scenarios that applications might encounter, rather than pathological cases designed to break the parser. The focus should be on ensuring acceptable performance for reasonable input sizes while establishing clear limits for extreme cases.\n\n### Implementation Guidance\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Test Framework | `pytest` with fixtures and parametrize | `pytest` with `hypothesis` for property-based testing |\n| Test Data Management | JSON files with expected results | YAML test definitions with embedded test cases |\n| Coverage Analysis | `coverage.py` for line coverage | `coverage.py` + `pytest-cov` with branch coverage |\n| Performance Testing | Simple timing with `time.time()` | `pytest-benchmark` with statistical analysis |\n| Test Organization | Directory-based test separation | `pytest` marks and custom test collections |\n\n**Recommended File Structure:**\n```\nconfig-parser/\n  tests/\n    unit/\n      test_tokenizer.py          ← BaseTokenizer unit tests\n      test_ini_parser.py         ← INIParser specific tests\n      test_toml_parser.py        ← TOMLParser specific tests\n      test_yaml_parser.py        ← YAMLParser specific tests\n    integration/\n      test_parsing_pipeline.py   ← End-to-end parsing tests\n      test_format_detection.py   ← Format detection integration\n      test_error_handling.py     ← Cross-component error flow\n    data/\n      golden/\n        ini/                     ← Golden path INI test files\n        toml/                    ← Golden path TOML test files\n        yaml/                    ← Golden path YAML test files\n      edge-cases/\n        tokenizer/               ← Tokenization edge cases\n        parsing/                 ← Parser-specific edge cases\n      errors/\n        syntax/                  ← Syntax error test cases\n        structure/               ← Structure error test cases\n      equivalence/\n        basic-config/            ← Cross-format equivalent configs\n    conftest.py                  ← Shared test fixtures\n    test_utils.py                ← Testing utility functions\n```\n\n**Test Infrastructure Starter Code:**\n\n```python\n# tests/conftest.py - Shared test fixtures and utilities\nimport pytest\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Union\nfrom dataclasses import dataclass\n\n@dataclass\nclass TestCase:\n    \"\"\"Represents a single parser test case with input, expected output, and metadata.\"\"\"\n    name: str\n    input_content: str\n    expected_output: Dict[str, Any]\n    expected_errors: List[str] = None\n    format_hint: str = None\n    description: str = \"\"\n\nclass TestDataLoader:\n    \"\"\"Loads and manages test data files with caching and validation.\"\"\"\n    \n    def __init__(self, test_data_root: Path):\n        self.test_data_root = test_data_root\n        self._cache = {}\n    \n    def load_test_cases(self, category: str, format_type: str = None) -> List[TestCase]:\n        \"\"\"Load test cases from organized test data directory structure.\"\"\"\n        # TODO 1: Build path based on category and optional format_type\n        # TODO 2: Scan directory for test case files (.json, .yaml, .toml, .ini)\n        # TODO 3: Load each file and create TestCase objects\n        # TODO 4: Cache loaded test cases for performance\n        # TODO 5: Validate test case structure and expected results\n        pass\n    \n    def load_equivalence_set(self, scenario_name: str) -> Dict[str, TestCase]:\n        \"\"\"Load cross-format equivalent test cases for validation.\"\"\"\n        # TODO 1: Load test cases for all formats in equivalence scenario\n        # TODO 2: Validate that expected outputs are semantically equivalent\n        # TODO 3: Return dictionary mapping format -> TestCase\n        pass\n\n@pytest.fixture\ndef test_data_loader():\n    \"\"\"Provides TestDataLoader instance for all tests.\"\"\"\n    test_data_root = Path(__file__).parent / \"data\"\n    return TestDataLoader(test_data_root)\n\n@pytest.fixture\ndef parser_factory():\n    \"\"\"Factory for creating parser instances with test configuration.\"\"\"\n    def create_parser(format_type: str, **options):\n        if format_type == \"ini\":\n            from config_parser.ini_parser import INIParser\n            return INIParser(**options)\n        elif format_type == \"toml\":\n            from config_parser.toml_parser import TOMLParser\n            return TOMLParser(**options)\n        elif format_type == \"yaml\":\n            from config_parser.yaml_parser import YAMLParser\n            return YAMLParser(**options)\n        else:\n            raise ValueError(f\"Unknown format: {format_type}\")\n    return create_parser\n\ndef assert_equivalent_structures(actual: Dict[str, Any], expected: Dict[str, Any], \n                                path: str = \"root\") -> None:\n    \"\"\"Deep comparison of nested dictionary structures with helpful error messages.\"\"\"\n    # TODO 1: Compare dictionary keys, reporting missing/extra keys with path context\n    # TODO 2: Recursively compare nested dictionaries and lists\n    # TODO 3: Handle type coercion differences (e.g., \"1\" vs 1) appropriately\n    # TODO 4: Provide detailed error messages with path information for failures\n    pass\n\ndef normalize_test_output(output: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Normalize parser output for cross-format comparison.\"\"\"\n    # TODO 1: Sort dictionary keys recursively for consistent comparison\n    # TODO 2: Normalize string representations of numbers and booleans\n    # TODO 3: Handle format-specific type differences (e.g., date objects vs strings)\n    # TODO 4: Remove format-specific metadata that shouldn't affect equivalence\n    pass\n```\n\n**Unit Test Skeleton Code:**\n\n```python\n# tests/unit/test_tokenizer.py - Tokenizer unit tests\nimport pytest\nfrom config_parser.tokenizer import BaseTokenizer, Token, TokenType, Position\n\nclass TestBaseTokenizer:\n    \"\"\"Unit tests for tokenizer functionality across all formats.\"\"\"\n    \n    def test_basic_token_generation(self, test_data_loader):\n        \"\"\"Validate basic tokenization for all supported token types.\"\"\"\n        # TODO 1: Load tokenizer test cases with expected token sequences\n        # TODO 2: Create tokenizer instance and tokenize test input\n        # TODO 3: Compare generated tokens with expected tokens\n        # TODO 4: Validate token types, values, and position information\n        # TODO 5: Test edge cases like empty input, whitespace-only, comments\n        pass\n    \n    def test_string_literal_handling(self):\n        \"\"\"Test complex string literal parsing including escapes and multiline.\"\"\"\n        test_cases = [\n            ('basic_string', '\"Hello World\"', \"Hello World\"),\n            ('escape_sequences', '\"Line 1\\\\nLine 2\\\\t\\\\\"\"', \"Line 1\\nLine 2\\t\\\"\"),\n            ('multiline_string', '\"\"\"Line 1\\nLine 2\"\"\"', \"Line 1\\nLine 2\"),\n            ('literal_string', \"'No\\\\nEscape'\", \"No\\\\nEscape\"),\n        ]\n        \n        for test_name, input_text, expected_value in test_cases:\n            # TODO 1: Create tokenizer with string input\n            # TODO 2: Tokenize and extract string token\n            # TODO 3: Validate token type is STRING\n            # TODO 4: Validate processed value matches expected result\n            # TODO 5: Validate position tracking through string processing\n            pass\n    \n    def test_position_tracking(self):\n        \"\"\"Validate accurate line and column tracking during tokenization.\"\"\"\n        multiline_input = \"\"\"line1 = \"value1\"\n        [section]\n        line3 = 123\"\"\"\n        \n        # TODO 1: Tokenize multiline input and collect all tokens\n        # TODO 2: Validate that line numbers increment correctly\n        # TODO 3: Validate that column numbers reset after newlines\n        # TODO 4: Validate position accuracy for tokens spanning multiple characters\n        # TODO 5: Test position tracking with tabs, Unicode characters, and mixed line endings\n        pass\n    \n    def test_error_recovery(self):\n        \"\"\"Test tokenizer behavior with invalid characters and malformed input.\"\"\"\n        error_cases = [\n            \"unterminated string \\\"never ends\",\n            \"invalid \\x00 null character\",\n            \"unicode handling test 🚀\",\n            \"mixed quotes 'started with single \\\"ended with double\",\n        ]\n        \n        for error_input in error_cases:\n            # TODO 1: Create tokenizer with malformed input\n            # TODO 2: Tokenize and expect appropriate error token generation\n            # TODO 3: Validate error position accuracy\n            # TODO 4: Verify tokenizer continues processing after errors\n            # TODO 5: Test that subsequent valid tokens are processed correctly\n            pass\n\n# tests/unit/test_ini_parser.py - INI parser unit tests\nclass TestINIParser:\n    \"\"\"Unit tests for INI-specific parsing functionality.\"\"\"\n    \n    @pytest.mark.parametrize(\"test_case\", [\n        pytest.param(TestCase(\"basic_sections\", \"[section1]\\nkey=value\", \n                             {\"section1\": {\"key\": \"value\"}}), id=\"basic\"),\n        pytest.param(TestCase(\"global_keys\", \"global=value\\n[section]\\nlocal=value\",\n                             {\"global\": \"value\", \"section\": {\"local\": \"value\"}}), id=\"global\"),\n    ])\n    def test_section_parsing(self, test_case, parser_factory):\n        \"\"\"Test section header parsing and nested structure creation.\"\"\"\n        parser = parser_factory(\"ini\")\n        \n        # TODO 1: Parse test case input content\n        # TODO 2: Validate section structure matches expected output\n        # TODO 3: Verify nested dictionary organization\n        # TODO 4: Test section name extraction and normalization\n        # TODO 5: Validate global key handling before first section\n        pass\n    \n    def test_comment_handling(self, parser_factory):\n        \"\"\"Test comment processing with different comment styles.\"\"\"\n        ini_content = \"\"\"\n        ; This is a semicolon comment\n        # This is a hash comment\n        [section]\n        key1 = value1  ; inline semicolon comment\n        key2 = value2  # inline hash comment\n        ; Another comment\n        key3 = value3\n        \"\"\"\n        \n        # TODO 1: Parse INI content with mixed comment styles\n        # TODO 2: Verify comments are ignored during parsing\n        # TODO 3: Validate inline comments don't affect value processing\n        # TODO 4: Test comment handling configuration options\n        # TODO 5: Verify structure contains only actual key-value pairs\n        pass\n    \n    def test_value_processing(self, parser_factory):\n        \"\"\"Test key-value parsing including quotes, escapes, and type inference.\"\"\"\n        test_cases = [\n            ('unquoted_string', 'key = value', 'value'),\n            ('quoted_string', 'key = \"quoted value\"', 'quoted value'),\n            ('number_inference', 'key = 123', 123),\n            ('boolean_inference', 'key = true', True),\n            ('escaped_quotes', 'key = \"say \\\\\"hello\\\\\"\"', 'say \"hello\"'),\n        ]\n        \n        parser = parser_factory(\"ini\")\n        \n        for test_name, ini_line, expected_value in test_cases:\n            # TODO 1: Parse single key-value line in section context\n            # TODO 2: Extract parsed value from result structure\n            # TODO 3: Validate value matches expected result\n            # TODO 4: Verify type inference worked correctly\n            # TODO 5: Test whitespace trimming and quote processing\n            pass\n```\n\n**Integration Test Skeleton Code:**\n\n```python\n# tests/integration/test_parsing_pipeline.py - End-to-end integration tests\nclass TestParsingPipeline:\n    \"\"\"Integration tests for complete parsing workflow.\"\"\"\n    \n    def test_file_to_dictionary_pipeline(self, test_data_loader):\n        \"\"\"Test complete pipeline from configuration files to parsed dictionaries.\"\"\"\n        for format_type in ['ini', 'toml', 'yaml']:\n            golden_cases = test_data_loader.load_test_cases('golden', format_type)\n            \n            for test_case in golden_cases:\n                # TODO 1: Create parser for format type\n                # TODO 2: Parse test case input content\n                # TODO 3: Validate output structure matches expected result\n                # TODO 4: Verify no errors occurred during parsing\n                # TODO 5: Test with different parser configuration options\n                pass\n    \n    def test_cross_format_equivalence(self, test_data_loader, parser_factory):\n        \"\"\"Validate semantically equivalent configurations produce consistent results.\"\"\"\n        equivalence_scenarios = [\n            'basic_config', 'nested_structures', 'array_handling', 'type_inference'\n        ]\n        \n        for scenario in equivalence_scenarios:\n            equivalent_cases = test_data_loader.load_equivalence_set(scenario)\n            \n            # TODO 1: Parse same logical configuration in all supported formats\n            # TODO 2: Normalize output structures for comparison\n            # TODO 3: Validate semantic equivalence across formats\n            # TODO 4: Document and validate acceptable format differences\n            # TODO 5: Test edge cases where equivalence might not be perfect\n            pass\n    \n    def test_error_context_propagation(self, parser_factory):\n        \"\"\"Test error information flow from tokenizer through parser to user.\"\"\"\n        malformed_configs = {\n            'ini': '[broken section\\nkey without section',\n            'toml': '[table.redefinition]\\nvalue = 1\\n[table]\\nconflict = 2',\n            'yaml': 'mapping:\\n\\titem: value\\n  other: value'  # mixed tabs/spaces\n        }\n        \n        for format_type, malformed_content in malformed_configs.items():\n            parser = parser_factory(format_type)\n            \n            # TODO 1: Parse malformed content and expect parsing errors\n            # TODO 2: Validate error messages contain helpful context\n            # TODO 3: Verify error positions are accurate\n            # TODO 4: Test error recovery and continued parsing\n            # TODO 5: Validate error message quality and actionability\n            pass\n```\n\n**Milestone Checkpoints:**\n\n**Milestone 1 Checkpoint - INI Parser:**\n```bash\n# Run INI parser tests\npython -m pytest tests/unit/test_ini_parser.py -v\n\n# Expected output:\n# test_section_parsing[basic] PASSED\n# test_section_parsing[global] PASSED\n# test_comment_handling PASSED\n# test_value_processing PASSED\n\n# Manual verification:\npython -c \"\nfrom config_parser.ini_parser import INIParser\nparser = INIParser()\nresult = parser.parse('[database]\\nhost = localhost\\nport = 5432\\n# comment line\\nuser = admin')\nprint('Parsed structure:', result)\n# Should output: {'database': {'host': 'localhost', 'port': 5432, 'user': 'admin'}}\n\"\n```\n\n**Milestone 2 Checkpoint - TOML Tokenizer:**\n```bash\n# Run tokenizer tests\npython -m pytest tests/unit/test_tokenizer.py::TestBaseTokenizer::test_basic_token_generation -v\n\n# Manual tokenization verification:\npython -c \"\nfrom config_parser.tokenizer import BaseTokenizer\ntokenizer = BaseTokenizer('[table]\\nkey = \\\"value with \\\\\\\\ escapes\\\"')\ntokens = tokenizer.tokenize()\nfor token in tokens:\n    print(f'{token.type.name}: {token.value} at {token.position.line}:{token.position.column}')\n\"\n```\n\n**Performance Testing Integration:**\n```python\n# tests/performance/test_parser_performance.py\nimport pytest\nimport time\nfrom pathlib import Path\n\nclass TestParserPerformance:\n    \"\"\"Performance validation for parser components.\"\"\"\n    \n    @pytest.mark.benchmark\n    def test_file_size_scaling(self, parser_factory, benchmark):\n        \"\"\"Validate parsing performance scales appropriately with file size.\"\"\"\n        # TODO 1: Generate configuration files of increasing sizes\n        # TODO 2: Benchmark parsing time for each size\n        # TODO 3: Validate linear scaling characteristics\n        # TODO 4: Measure memory usage during parsing\n        # TODO 5: Establish performance baselines for regression testing\n        pass\n    \n    def test_nesting_depth_limits(self, parser_factory):\n        \"\"\"Test parser behavior with deeply nested structures.\"\"\"\n        max_depth = 100\n        \n        # TODO 1: Generate deeply nested test configurations\n        # TODO 2: Parse configurations at increasing depth levels\n        # TODO 3: Measure parsing time and memory usage\n        # TODO 4: Identify practical depth limits\n        # TODO 5: Validate graceful handling of extreme nesting\n        pass\n\n```\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive debugging strategies, symptom diagnosis, and troubleshooting techniques for successful parsing implementation\n\nDebugging configuration parsers presents unique challenges that differ significantly from typical application debugging. Unlike business logic where errors often manifest as obvious behavioral inconsistencies, parsing errors frequently emerge as subtle structural misalignments, tokenization edge cases, or context-sensitive interpretation failures. The multi-format nature of our parser amplifies these challenges by introducing format-specific edge cases alongside shared infrastructure bugs.\n\nThink of debugging a parser as forensic investigation rather than traditional problem-solving. When a parser fails, you must trace backwards through multiple layers: the final output structure, the parse tree construction, the tokenization process, and the original character-by-character scanning. Each layer introduces potential failure points, and symptoms at the output level often obscure root causes buried deep in the tokenization logic. This section provides systematic approaches for conducting this forensic analysis effectively.\n\nThe complexity stems from parsing's inherently contextual nature. The same character sequence \"key = value\" might be valid in INI format but invalid in YAML context, while multiline strings behave completely differently across TOML and YAML. Context sensitivity means bugs often manifest inconsistently - working perfectly for simple cases but failing mysteriously when nesting levels change, indentation patterns shift, or specific character combinations appear.\n\n### Common Bug Symptoms and Causes\n\nUnderstanding the symptom-to-cause mapping for parsing bugs accelerates debugging by directing investigation toward the most likely root causes. Parsing failures typically fall into distinct categories with characteristic symptoms that point to specific implementation areas.\n\n![Error Detection and Reporting Flow](./diagrams/error-handling-flow.svg)\n\n#### Structural Misrepresentation Symptoms\n\n**Missing Nested Structure**: When configuration content contains hierarchical structure but the parsed output flattens it into a single level, the bug typically originates in path expansion logic. For INI parsers, this manifests when dotted section names like `[database.connection.pool]` create a flat key instead of nested dictionaries. The root cause usually lies in `create_nested_section` not properly splitting section paths or `merge_nested_dicts` failing to create intermediate levels.\n\n**Incorrect Nesting Depth**: Output shows wrong nesting levels - either too shallow or too deep. In YAML parsers, this symptom points to indentation stack management failures. The `IndentationStack` might incorrectly calculate target levels during `pop_to_level` operations, or `_handle_indentation_transition` might push frames when it should maintain current level. TOML parsers exhibit this when dotted key expansion in `expand_dotted_key` creates too many intermediate tables.\n\n**Key Collision Overwriting**: Later-defined keys silently overwrite earlier ones instead of generating conflicts. This indicates insufficient symbol table tracking in TOML parsers. The `SymbolTable` should register every definition through `register_definition` and validate against redefinition rules. Missing validation allows conflicting definitions to proceed unchecked.\n\n| Symptom | Format Context | Likely Root Cause | Diagnostic Focus |\n|---------|----------------|-------------------|------------------|\n| Flat output for nested syntax | INI dotted sections | Section path splitting failure | `create_nested_section` logic |\n| Wrong nesting depth | YAML indentation | Stack management errors | `IndentationStack` operations |\n| Silent key overwrites | TOML dotted keys | Missing conflict detection | `SymbolTable` validation |\n| Array structure lost | TOML array-of-tables | Table vs array confusion | `parse_table_header` logic |\n\n#### Value Processing Failures\n\n**Type Conversion Errors**: Values appear as strings when they should be numbers, booleans, or other types. This symptom indicates failures in type inference logic. Each format's `infer_type` method might contain incomplete boolean detection, numeric parsing edge cases, or missing null value recognition. YAML's implicit typing is particularly susceptible due to complex conversion rules.\n\n**String Escape Sequence Issues**: Literal backslashes appear in output instead of processed escapes, or escape sequences cause parsing to fail entirely. The root cause lies in `read_string_literal` implementation within the tokenizer. Multi-format parsers must handle different escape rules: INI uses minimal escaping, TOML has basic and literal string variants, while YAML has complex folding rules.\n\n**Multiline Value Corruption**: Multiline strings split incorrectly, lose formatting, or include unintended content. This points to line continuation logic failures. Each format handles multilines differently: INI uses backslash continuation, TOML has multiline string delimiters, YAML uses folding indicators. The tokenizer's state machine must track multiline context accurately.\n\n**Inline Comment Contamination**: Values include comment text that should be stripped. This indicates inadequate comment detection during value parsing. The `process_key_value_pair` logic must identify comment boundaries within lines while respecting quoted string contexts where hash or semicolon characters are literal.\n\n#### Tokenization Boundary Errors\n\n**Token Splitting Failures**: Single logical units split across multiple tokens, or multiple units merged into single tokens. This symptom reveals boundary detection errors in the tokenizer. Complex tokens like dotted identifiers, numeric literals with underscores, or quoted strings with embedded delimiters require sophisticated boundary logic.\n\n**Context Sensitivity Violations**: Same character sequences tokenized differently in identical contexts, or differently in contexts where they should be identical. This points to inadequate context tracking in `BaseTokenizer`. State machines must maintain consistent context interpretation across equivalent parsing situations.\n\n**Position Tracking Inaccuracies**: Error messages report wrong line/column positions, making debugging extremely difficult. The `Position` calculation logic in `current_position` must accurately track line breaks, character offsets, and column advancement through all tokenization paths including multiline constructs.\n\n#### Format Detection Ambiguities\n\n**Wrong Format Selection**: Parser selects incorrect format, causing interpretation failures. The `detect_format` algorithm relies on syntactic signatures that distinguish formats. Ambiguous content might match multiple format patterns, requiring confidence scoring to select the most appropriate parser.\n\n**Format Switching Mid-Parse**: Parser begins with one format interpretation then shifts to another, causing structural inconsistencies. This indicates insufficient lookahead in format detection. The analysis window must examine enough content to establish format identity before committing to specific parsing logic.\n\n> **Design Insight**: The most challenging bugs combine multiple symptom categories. For example, a TOML array-of-tables with complex nested structure might simultaneously exhibit tokenization boundary errors (array brackets), structural misrepresentation (table nesting), and value processing failures (nested values). Systematic diagnosis must examine each layer independently before considering interactions.\n\n#### Common Error Propagation Patterns\n\nParsing errors often cascade through multiple system layers, making root cause identification challenging. Understanding propagation patterns helps focus debugging efforts on primary failure points rather than secondary symptoms.\n\n**Tokenization → Structure Cascade**: Incorrect tokenization creates valid but wrong tokens that produce structurally incorrect parse trees. For example, a quoted string boundary missed by the tokenizer appears as separate IDENTIFIER and STRING tokens. The parser successfully processes these tokens but creates wrong structure. Diagnosis must verify token correctness before examining structural logic.\n\n**Context → Value Cascade**: Wrong parsing context causes correct tokens to be interpreted inappropriately. A YAML scalar appears in sequence context but gets processed as mapping key, leading to structural errors. The tokenization is correct, but context tracking failures cause misinterpretation.\n\n**Format → Everything Cascade**: Incorrect format detection causes the wrong parser to process content, leading to systematic failures across all processing layers. An INI file processed by the TOML parser generates numerous errors because fundamental syntax assumptions are violated. This pattern requires format detection validation as the first debugging step.\n\n### Debugging Techniques for Parsers\n\nEffective parser debugging requires specialized techniques that differ from general software debugging. The multi-layered nature of parsing - from character scanning through tokenization to structure building - demands systematic approaches for isolating failures at each level.\n\n#### Layered Diagnosis Methodology\n\n**Layer 1: Character Stream Analysis**: Begin debugging at the most fundamental level by examining the raw input character stream. Many parsing failures originate from assumptions about input encoding, line ending conventions, or whitespace handling that prove incorrect for specific content.\n\nCreate character-level inspection by implementing a diagnostic scanner that reveals non-printable characters, mixed encodings, and whitespace variations. Unicode normalization issues, byte order marks, or mixed line ending styles (Unix LF vs Windows CRLF) frequently cause parsing failures that manifest as mysterious structural errors.\n\nPosition tracking verification requires tracing through the character stream manually to confirm that `current_position` calculations match actual line and column positions. Off-by-one errors in position tracking make all subsequent error reporting unreliable, severely hampering debugging efforts.\n\n**Layer 2: Token Stream Validation**: After confirming character stream integrity, examine the token stream produced by the tokenizer. This intermediate representation reveals whether lexical analysis correctly identifies meaningful units within the character stream.\n\nToken boundary verification involves examining each token's `raw_text` field against its `position` information to confirm accurate extraction. Boundary errors often appear as tokens that include extra characters, exclude expected characters, or split logical units incorrectly.\n\nToken type accuracy checking ensures that the tokenizer assigns correct `TokenType` values to extracted text. A numeric literal incorrectly classified as IDENTIFIER causes downstream parsing failures that appear as type conversion errors rather than tokenization bugs.\n\nContext tracking validation confirms that tokenization state changes appropriately as content context shifts. Multiline strings, comment regions, and format-specific constructs require state machine transitions that maintain consistent interpretation rules.\n\n**Layer 3: Parse Tree Structure Inspection**: With correct tokenization confirmed, examine the intermediate parse tree structure built by format-specific parsers. This layer reveals whether structural interpretation correctly translates token sequences into hierarchical representations.\n\nNode hierarchy verification checks that parent-child relationships in the parse tree match expected nesting from the original content. Missing intermediate nodes, incorrect nesting depths, or orphaned nodes indicate structural interpretation failures.\n\nNode metadata accuracy ensures that `ParseNode` instances contain correct position information, token references, and format-specific metadata. Inaccurate metadata complicates error reporting and downstream processing.\n\n**Layer 4: Final Output Validation**: The final debugging layer examines the conversion from parse tree to the unified output format. This transformation must preserve all structural relationships while normalizing format-specific representations into consistent dictionary structures.\n\nKey path accuracy verification confirms that nested dictionary keys match expected hierarchical paths from the original content. The `convert_parse_tree_to_dict` transformation must maintain all structural relationships without introducing spurious levels or losing intended nesting.\n\nValue conversion correctness ensures that the type inference and conversion process produces appropriate Python types for each value. String-to-type conversion errors often appear as unexpected string values in contexts where numbers or booleans are expected.\n\n#### Incremental Complexity Testing\n\n**Minimal Reproduction Construction**: When debugging complex parsing failures, construct minimal examples that reproduce the same symptoms with the simplest possible input. This technique isolates the specific conditions that trigger failures without the complexity of realistic configuration files.\n\nStart with single-line examples that exhibit the problematic behavior. If a complex nested TOML structure fails to parse correctly, create a minimal table definition that shows the same structural issues. Gradually increase complexity while maintaining the failure symptom to identify the exact complexity threshold where problems appear.\n\nFormat isolation testing processes the same logical content through different format parsers to identify format-specific versus general infrastructure bugs. If the same nested structure parses correctly in YAML but fails in TOML, the issue lies in TOML-specific logic rather than shared infrastructure.\n\n**Progressive Feature Addition**: Build parser functionality incrementally, validating correct behavior at each step before adding complexity. This approach prevents multiple bugs from interacting and obscuring individual failure points.\n\nBegin with basic key-value parsing without nesting, comments, or complex values. Confirm perfect behavior for simple cases before introducing sectioning, then nesting, then complex value types. Each addition point becomes a checkpoint for isolating newly introduced bugs.\n\nFeature interaction testing examines combinations of parser features that work individually but fail when combined. Comments within multiline strings, nested structures with dotted keys, or array-of-tables with inline values represent interaction points where subtle bugs frequently emerge.\n\n#### State Inspection and Tracing\n\n**Parser State Snapshots**: Implement diagnostic capabilities that capture complete parser state at critical processing points. State snapshots provide detailed views of internal parser condition when failures occur.\n\nThe `ParseContext` should support diagnostic mode that records position progression, tokenization state, parse tree construction steps, and error accumulation. This information enables post-mortem analysis of parsing sessions to identify the exact point where processing diverged from expected behavior.\n\nSymbol table inspection for TOML parsing reveals the current definition state, helping identify redefinition conflicts or missing implicit table creation. The `SymbolTable` diagnostic interface should expose all registered definitions with their types and positions.\n\nIndentation stack analysis for YAML parsing shows the current nesting context and established indentation levels. The `IndentationStack` diagnostic view helps identify incorrect stack operations that lead to structural misinterpretation.\n\n**Token Stream Replay**: Implement token stream recording and replay capabilities that enable re-processing specific token sequences with different parser configurations or enhanced diagnostics enabled.\n\nToken stream serialization captures the complete sequence of tokens produced for problematic input, enabling offline analysis and regression testing. Replay functionality allows processing the same token stream multiple times with different diagnostic settings or parser modifications.\n\nInteractive token inspection provides step-by-step token stream examination with parser state inspection at each token boundary. This technique helps identify the specific token where parsing logic makes incorrect decisions.\n\n#### Error Message Archaeological Analysis\n\n**Error Context Reconstruction**: Parser error messages often provide insufficient context for effective debugging. Implement enhanced error context generation that shows not just the immediate error location but the parsing context leading to the failure.\n\nThe `create_error_context` function should include previous parsing decisions, current parser state, and upcoming tokens to provide comprehensive situational awareness. Context windows should adapt to the specific error type - structural errors need broader context than tokenization errors.\n\nError correlation analysis identifies relationships between multiple errors that stem from single root causes. Cascading failures often generate numerous error messages that obscure the primary issue. Group related errors and present root cause analysis rather than symptom catalogs.\n\n**Error Evolution Tracking**: Track how error conditions evolve during parsing to understand failure progression. Some errors represent recoverable conditions that become fatal due to inadequate error recovery, while others indicate fundamental structural problems.\n\nRecovery decision analysis examines the effectiveness of error recovery strategies by tracking parsing progress after recovery attempts. The `ErrorRecoveryState` should maintain metrics on recovery success rates and provide feedback for recovery strategy refinement.\n\n### Debugging Tools and Inspection\n\nEffective parser debugging requires specialized tooling that provides visibility into the multi-layered parsing process. Unlike general application debugging where breakpoints and variable inspection suffice, parser debugging demands tools that can trace through character streams, token sequences, and structural transformations while maintaining context awareness.\n\n#### Interactive Parser Inspection Framework\n\n**Token-by-Token Stepping Interface**: Implement an interactive debugging interface that allows stepping through tokenization one token at a time while inspecting complete parser state. This capability proves essential for understanding how specific character sequences translate into token streams and how parser decisions evolve.\n\nThe stepping interface should display the current character position, upcoming character sequences, tokenizer state machine status, and accumulated token stream. At each step, inspect the decision logic for token boundary detection, type classification, and value extraction. This granular visibility reveals tokenization edge cases that are invisible in batch processing.\n\nContext-aware display formatting presents different views optimized for different parsing phases. During tokenization, emphasize character boundaries and state transitions. During structural parsing, highlight nesting relationships and symbol table evolution. During error recovery, focus on recovery decision points and confidence metrics.\n\n**Parse Tree Visualization Tools**: Develop visual representations of intermediate parse tree structures that reveal hierarchical relationships and node metadata. Text-based tree displays work well for automated testing, but interactive visual tools provide superior debugging capability for complex structures.\n\nNode inspection capabilities should expose all `ParseNode` fields including position information, token references, child relationships, and format-specific metadata. Interactive expansion and collapse of subtrees helps manage complexity when debugging large configuration files.\n\nComparative tree visualization shows differences between expected and actual parse tree structures, highlighting specific nodes where structure diverges from expectations. This capability accelerates debugging of structural interpretation issues.\n\n#### Tokenization Analysis Toolkit\n\n**Character Stream Inspector**: Build diagnostic tools that reveal character-level details often hidden by text editors and terminal displays. Many parsing bugs stem from invisible characters, mixed encodings, or non-standard whitespace that standard tools don't reveal clearly.\n\nThe inspector should display hexadecimal character codes alongside visual representations, highlight different whitespace types distinctly, and identify potential encoding issues. Unicode normalization problems, zero-width characters, and mixed line ending styles become immediately visible.\n\nPosition mapping verification tools trace the relationship between character stream offsets and calculated line/column positions. Generate position maps that can be compared against expected values to identify off-by-one errors or miscalculated boundaries.\n\n**Token Stream Analysis Suite**: Develop comprehensive tools for examining token streams in detail, including sequence analysis, boundary verification, and context tracking validation.\n\nToken sequence comparison tools examine token streams produced from similar input to identify inconsistencies in tokenization behavior. This capability helps identify context sensitivity bugs where equivalent content tokenizes differently in different parsing contexts.\n\nBoundary accuracy verification reconstructs original character sequences from token `raw_text` fields and position information, comparing against the original input to identify extraction errors. Gaps or overlaps in token coverage indicate boundary detection problems.\n\nToken type distribution analysis reveals patterns in token classification that can identify systematic errors in type detection logic. Unexpected type distributions often point to classification rules that behave differently than intended.\n\n#### Parser State Inspection Utilities\n\n**Symbol Table Diagnostic Views**: For TOML parsing, implement comprehensive symbol table inspection that reveals definition conflicts, implicit table creation, and key path resolution. The `SymbolTable` diagnostic interface should support queries about definition history and conflict analysis.\n\nDefinition timeline views show the sequence of key and table definitions with their positions and types. This information helps identify redefinition violations and understand how implicit table creation interacts with explicit definitions.\n\nConflict detection analysis explains why specific key combinations are invalid, providing detailed explanations of TOML's redefinition rules. When debugging table conflicts, show the complete definition history that led to the conflict condition.\n\n**Indentation Stack Analysis**: For YAML parsing, develop tools that visualize indentation stack evolution and validate stack operations. The `IndentationStack` diagnostic interface should expose stack frame details and transition logic.\n\nStack frame inspection shows the complete frame history including indentation levels, structure types, and line numbers where frames were established. This information helps identify incorrect stack operations that lead to nesting errors.\n\nIndentation level analysis validates that calculated indentation levels match established patterns and identifies ambiguous indentation that could be interpreted multiple ways. Level transition visualization shows how stack operations respond to indentation changes.\n\n#### Error Analysis and Recovery Tools\n\n**Error Pattern Recognition**: Implement tools that analyze error patterns across multiple parsing attempts to identify systematic issues in parser logic or input handling. Pattern recognition helps distinguish between input-specific errors and parser implementation bugs.\n\nError clustering analysis groups similar errors by type, location patterns, and context to identify common failure modes. This analysis helps prioritize parser improvements by focusing on the most frequent error categories.\n\nRecovery effectiveness measurement tracks the success rate of different error recovery strategies and provides data for refining recovery logic. The `ErrorRecoveryState` metrics help optimize recovery decision-making.\n\n**Diagnostic Trace Generation**: Develop comprehensive tracing that captures the complete parsing decision sequence for post-mortem analysis. Traces should include character scanning, tokenization decisions, parser state transitions, and error handling decisions.\n\nExecution path analysis identifies the specific code paths followed during parsing, helping isolate bugs to particular logic branches. This information proves especially valuable for debugging complex conditional logic in recursive descent parsers.\n\nDecision point logging records the rationale for parser decisions at critical points, providing insight into why specific interpretation paths were chosen. This information helps identify cases where parser logic makes reasonable but incorrect decisions based on insufficient context.\n\n#### Performance and Scalability Analysis\n\n**Parsing Performance Profiler**: Implement profiling tools that identify performance bottlenecks in parsing logic, focusing on operations that scale poorly with input size or complexity.\n\nTime allocation analysis shows how parsing time distributes across different operations: tokenization, structural analysis, value conversion, and output generation. This information helps identify optimization opportunities and scalability concerns.\n\nMemory usage tracking reveals memory allocation patterns and identifies potential memory leaks or excessive allocation in parsing logic. Parser implementations should maintain bounded memory usage regardless of input complexity.\n\n**Scalability Stress Testing**: Develop tools that generate configuration files of varying sizes and complexity to test parser behavior under stress conditions. Scalability testing reveals performance degradation patterns and memory usage growth.\n\nInput complexity graduation creates test cases with systematically increasing complexity: nesting depth, table count, array size, and string length. This approach identifies complexity thresholds where parser performance degrades significantly.\n\nResource consumption monitoring tracks CPU usage, memory allocation, and parsing time across different input categories to establish performance baselines and identify regression conditions.\n\n### Implementation Guidance\n\nThis implementation guidance provides practical tools and techniques for implementing comprehensive debugging capabilities for your configuration file parser. The focus is on building diagnostic infrastructure that integrates seamlessly with your parser implementation while providing powerful debugging capabilities.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Diagnostic Output | Print statements with structured formatting | Rich library for terminal formatting and interactive displays |\n| State Inspection | JSON serialization of parser state | Custom inspection framework with interactive browsing |\n| Token Visualization | Plain text token stream dumps | HTML/web-based token inspector with syntax highlighting |\n| Error Analysis | Basic error categorization and counting | Statistical analysis with matplotlib for error pattern visualization |\n| Interactive Debugging | Command-line REPL with parser commands | Web-based debugging interface with real-time visualization |\n\n#### Recommended File/Module Structure\n\n```\nconfig-parser/\n  src/\n    parser/\n      core/\n        tokenizer.py              ← BaseTokenizer with diagnostic capabilities\n        errors.py                 ← Error types and diagnostic functions\n        context.py                ← ParseContext with debugging support\n      formats/\n        ini_parser.py             ← INI parser with diagnostic hooks\n        toml_parser.py            ← TOML parser with symbol table inspection\n        yaml_parser.py            ← YAML parser with stack analysis\n      debugging/\n        __init__.py               ← Public debugging API\n        diagnostic_tools.py       ← Core diagnostic infrastructure\n        token_inspector.py        ← Token stream analysis tools\n        state_inspector.py        ← Parser state inspection utilities\n        error_analyzer.py         ← Error pattern analysis and reporting\n        interactive_debugger.py   ← Interactive debugging interface\n      testing/\n        debug_test_cases.py       ← Test cases specifically for debugging scenarios\n        diagnostic_fixtures.py    ← Test fixtures with known debugging patterns\n  examples/\n    debug_examples/\n      problematic_configs/        ← Example files that demonstrate common bugs\n      debugging_sessions.py       ← Example debugging workflows\n```\n\n#### Core Diagnostic Infrastructure\n\n```python\n\"\"\"\nCore diagnostic infrastructure for configuration parser debugging.\nProvides comprehensive state inspection, error analysis, and interactive debugging capabilities.\n\"\"\"\n\nimport json\nimport traceback\nfrom typing import Dict, List, Any, Optional, Union\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\n\nclass DiagnosticLevel(Enum):\n    MINIMAL = \"minimal\"      # Basic error information only\n    STANDARD = \"standard\"    # Include context and suggestions\n    COMPREHENSIVE = \"comprehensive\"  # Full state inspection and traces\n    INTERACTIVE = \"interactive\"      # Enable interactive debugging features\n\n@dataclass\nclass DiagnosticConfig:\n    level: DiagnosticLevel = DiagnosticLevel.STANDARD\n    include_token_stream: bool = True\n    include_parse_tree: bool = False\n    include_position_context: bool = True\n    max_context_lines: int = 3\n    enable_color_output: bool = True\n    save_diagnostic_traces: bool = False\n    trace_output_path: Optional[str] = None\n\nclass ParserDiagnostics:\n    \"\"\"\n    Comprehensive diagnostic system for parser debugging and analysis.\n    Integrates with all parser components to provide detailed inspection capabilities.\n    \"\"\"\n    \n    def __init__(self, config: DiagnosticConfig = None):\n        self.config = config or DiagnosticConfig()\n        self.diagnostic_data = {}\n        self.error_patterns = []\n        self.performance_metrics = {}\n        \n    def capture_parsing_session(self, content: str, format_hint: str = None) -> 'ParsingSession':\n        \"\"\"\n        Create a comprehensive diagnostic session for parsing the given content.\n        Returns a session object that tracks all parsing operations and state changes.\n        \"\"\"\n        # TODO 1: Initialize session with input content and configuration\n        # TODO 2: Set up diagnostic hooks for tokenizer, parser, and error handling\n        # TODO 3: Enable state capture at each major parsing phase\n        # TODO 4: Configure error tracking and recovery decision logging\n        # TODO 5: Return configured session ready for parsing execution\n        pass\n    \n    def analyze_tokenization_issues(self, content: str, expected_tokens: List[str] = None) -> 'TokenizationAnalysis':\n        \"\"\"\n        Perform detailed analysis of tokenization behavior for diagnostic purposes.\n        Identifies boundary issues, type classification problems, and context sensitivity bugs.\n        \"\"\"\n        # TODO 1: Create tokenizer with full diagnostic logging enabled\n        # TODO 2: Process content character-by-character with state tracking\n        # TODO 3: Analyze token boundaries and type classification decisions\n        # TODO 4: Compare against expected tokens if provided\n        # TODO 5: Generate comprehensive analysis report with identified issues\n        pass\n    \n    def inspect_parser_state(self, parser_instance: Any, checkpoint_name: str = None) -> Dict[str, Any]:\n        \"\"\"\n        Capture complete parser state snapshot for detailed inspection.\n        Includes symbol tables, indentation stacks, and all context information.\n        \"\"\"\n        # TODO 1: Extract current position and context from parser\n        # TODO 2: Serialize symbol table state (for TOML parser)\n        # TODO 3: Capture indentation stack (for YAML parser)\n        # TODO 4: Include token stream position and lookahead state\n        # TODO 5: Format state information for human-readable inspection\n        pass\n    \n    def trace_error_propagation(self, error: ParseError) -> 'ErrorTrace':\n        \"\"\"\n        Analyze how errors propagate through parser components.\n        Identifies root causes and distinguishes primary errors from cascading symptoms.\n        \"\"\"\n        # TODO 1: Analyze error context and position information\n        # TODO 2: Trace backwards through parsing decisions leading to error\n        # TODO 3: Identify related errors that may stem from same root cause\n        # TODO 4: Classify error as primary failure vs cascading symptom\n        # TODO 5: Generate trace showing error evolution and propagation path\n        pass\n\nclass TokenizationAnalysis:\n    \"\"\"Analysis results for tokenization diagnostic operations.\"\"\"\n    \n    def __init__(self):\n        self.character_analysis = []\n        self.token_boundaries = []\n        self.type_classification_issues = []\n        self.context_sensitivity_violations = []\n        self.position_tracking_errors = []\n    \n    def generate_report(self) -> str:\n        \"\"\"Generate comprehensive tokenization analysis report.\"\"\"\n        # TODO 1: Summarize character-level issues found\n        # TODO 2: Detail token boundary problems with examples\n        # TODO 3: List type classification errors with corrections\n        # TODO 4: Highlight context sensitivity violations\n        # TODO 5: Format as readable report with recommendations\n        pass\n\nclass ParsingSession:\n    \"\"\"\n    Comprehensive diagnostic session that tracks all aspects of a parsing operation.\n    Provides detailed visibility into parser behavior for debugging purposes.\n    \"\"\"\n    \n    def __init__(self, content: str, config: DiagnosticConfig):\n        self.content = content\n        self.config = config\n        self.timeline = []  # Chronological record of parsing operations\n        self.state_snapshots = {}  # State captures at key points\n        self.error_history = []  # Complete error tracking\n        self.performance_data = {}  # Timing and resource usage\n        \n    def execute_parsing(self, parser_class, **parser_options) -> Dict[str, Any]:\n        \"\"\"\n        Execute parsing with full diagnostic tracking enabled.\n        Returns both parsing results and comprehensive diagnostic information.\n        \"\"\"\n        # TODO 1: Initialize parser with diagnostic hooks enabled\n        # TODO 2: Track parsing timeline with timestamps\n        # TODO 3: Capture state snapshots at major parsing phases\n        # TODO 4: Record all errors and recovery decisions\n        # TODO 5: Generate final diagnostic report with recommendations\n        pass\n    \n    def analyze_failure_points(self) -> List['FailureAnalysis']:\n        \"\"\"\n        Identify specific points where parsing failed or made incorrect decisions.\n        Provides targeted analysis for debugging specific issues.\n        \"\"\"\n        # TODO 1: Examine error history for failure patterns\n        # TODO 2: Identify decision points that led to incorrect results\n        # TODO 3: Analyze context conditions at each failure point\n        # TODO 4: Generate specific recommendations for each identified issue\n        # TODO 5: Prioritize failure points by impact and fixing difficulty\n        pass\n```\n\n#### Token Stream Analysis Tools\n\n```python\n\"\"\"\nSpecialized tools for analyzing token stream behavior and identifying tokenization issues.\n\"\"\"\n\nclass TokenStreamInspector:\n    \"\"\"\n    Comprehensive token stream analysis with boundary verification and context tracking.\n    \"\"\"\n    \n    def __init__(self, tokenizer_class):\n        self.tokenizer_class = tokenizer_class\n        \n    def analyze_token_boundaries(self, content: str) -> 'BoundaryAnalysis':\n        \"\"\"\n        Verify token boundary detection accuracy by comparing extracted tokens\n        against original content positions.\n        \"\"\"\n        # TODO 1: Tokenize content with position tracking enabled\n        # TODO 2: Reconstruct original text from token raw_text and positions\n        # TODO 3: Identify gaps or overlaps in token coverage\n        # TODO 4: Validate position calculations against actual character positions\n        # TODO 5: Generate boundary accuracy report with specific issues highlighted\n        pass\n    \n    def compare_tokenization_contexts(self, test_cases: List[str]) -> 'ContextComparisonAnalysis':\n        \"\"\"\n        Compare tokenization behavior across different contexts to identify\n        context sensitivity bugs where equivalent content tokenizes differently.\n        \"\"\"\n        # TODO 1: Process each test case through tokenizer\n        # TODO 2: Identify equivalent content patterns across test cases\n        # TODO 3: Compare token classification for equivalent patterns\n        # TODO 4: Flag inconsistencies in context-sensitive interpretation\n        # TODO 5: Generate comparison report highlighting inconsistent behavior\n        pass\n    \n    def validate_string_literal_handling(self, test_strings: List[str]) -> 'StringHandlingAnalysis':\n        \"\"\"\n        Comprehensive testing of string literal processing including escape sequences,\n        multiline handling, and quote character processing.\n        \"\"\"\n        # TODO 1: Test each string format supported by parser\n        # TODO 2: Verify escape sequence processing accuracy\n        # TODO 3: Validate multiline string boundary detection\n        # TODO 4: Test quote character handling and nesting\n        # TODO 5: Generate report on string processing capabilities and limitations\n        pass\n\nclass InteractiveTokenDebugger:\n    \"\"\"\n    Interactive debugging interface for step-by-step token stream analysis.\n    \"\"\"\n    \n    def __init__(self, content: str, tokenizer_class):\n        self.content = content\n        self.tokenizer = tokenizer_class(content)\n        self.current_position = 0\n        \n    def start_interactive_session(self):\n        \"\"\"\n        Launch interactive debugging session with command-line interface.\n        Supports stepping through tokenization and state inspection.\n        \"\"\"\n        # TODO 1: Initialize tokenizer with diagnostic mode enabled\n        # TODO 2: Set up command processing loop\n        # TODO 3: Implement commands: next, peek, inspect, context, reset\n        # TODO 4: Provide help system and command completion\n        # TODO 5: Enable state inspection and modification during session\n        pass\n    \n    def step_to_next_token(self) -> Token:\n        \"\"\"\n        Advance tokenizer by one token and display detailed processing information.\n        Shows character consumption, state changes, and decision logic.\n        \"\"\"\n        # TODO 1: Capture tokenizer state before processing\n        # TODO 2: Advance tokenizer by one token\n        # TODO 3: Display character consumption and boundary detection\n        # TODO 4: Show state machine transitions and decision points\n        # TODO 5: Present token result with classification rationale\n        pass\n```\n\n#### Parser State Inspection Framework\n\n```python\n\"\"\"\nParser state inspection utilities for examining internal parser condition\nduring complex parsing operations.\n\"\"\"\n\nclass TOMLParserInspector:\n    \"\"\"Specialized inspection tools for TOML parser state and symbol table analysis.\"\"\"\n    \n    def __init__(self, parser_instance):\n        self.parser = parser_instance\n        \n    def inspect_symbol_table(self) -> Dict[str, Any]:\n        \"\"\"\n        Generate comprehensive view of current symbol table state including\n        all definitions, their types, and conflict detection status.\n        \"\"\"\n        # TODO 1: Extract all registered definitions from symbol table\n        # TODO 2: Organize definitions by key path and definition type\n        # TODO 3: Identify potential conflicts and redefinition violations\n        # TODO 4: Show implicit table creation history\n        # TODO 5: Format as hierarchical view showing definition relationships\n        pass\n    \n    def analyze_table_conflicts(self, table_path: List[str]) -> 'ConflictAnalysis':\n        \"\"\"\n        Analyze potential conflicts for a given table path and explain\n        TOML redefinition rules in the context of current definitions.\n        \"\"\"\n        # TODO 1: Examine existing definitions that overlap with table_path\n        # TODO 2: Apply TOML redefinition rules to identify conflicts\n        # TODO 3: Explain why conflicts exist with reference to specification\n        # TODO 4: Suggest alternative approaches that avoid conflicts\n        # TODO 5: Generate educational analysis explaining TOML semantics\n        pass\n    \n    def trace_dotted_key_expansion(self, key_path: List[str]) -> 'ExpansionTrace':\n        \"\"\"\n        Trace the process of expanding dotted keys into nested structure,\n        showing intermediate table creation and final value assignment.\n        \"\"\"\n        # TODO 1: Simulate dotted key expansion process step by step\n        # TODO 2: Show intermediate table creation decisions\n        # TODO 3: Identify implicit vs explicit table creation\n        # TODO 4: Validate final structure matches expected nesting\n        # TODO 5: Generate trace showing complete expansion process\n        pass\n\nclass YAMLParserInspector:\n    \"\"\"Specialized inspection tools for YAML parser indentation stack and type inference.\"\"\"\n    \n    def __init__(self, parser_instance):\n        self.parser = parser_instance\n        \n    def inspect_indentation_stack(self) -> Dict[str, Any]:\n        \"\"\"\n        Provide detailed view of current indentation stack state including\n        all frames, established levels, and nesting context.\n        \"\"\"\n        # TODO 1: Extract current indentation stack frames\n        # TODO 2: Show established indentation levels and their contexts\n        # TODO 3: Display structure types and data at each level\n        # TODO 4: Highlight current frame and processing context\n        # TODO 5: Format as visual stack representation with level indicators\n        pass\n    \n    def analyze_indentation_transition(self, target_level: int) -> 'TransitionAnalysis':\n        \"\"\"\n        Analyze what stack operations would be required to transition\n        to target indentation level and validate transition correctness.\n        \"\"\"\n        # TODO 1: Calculate required stack operations for target level\n        # TODO 2: Validate target level against established level history\n        # TODO 3: Identify ambiguous transitions that could be interpreted multiple ways\n        # TODO 4: Show stack state before and after transition\n        # TODO 5: Generate analysis explaining transition logic and potential issues\n        pass\n    \n    def trace_type_inference(self, value_string: str) -> 'TypeInferenceTrace':\n        \"\"\"\n        Trace type inference process for scalar values showing\n        detection logic and conversion decisions.\n        \"\"\"\n        # TODO 1: Apply each type detection rule to value_string\n        # TODO 2: Show rule matching process and precedence handling\n        # TODO 3: Explain conversion logic and edge case handling\n        # TODO 4: Compare against YAML specification requirements\n        # TODO 5: Generate trace showing complete inference and conversion process\n        pass\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 - Basic Diagnostic Infrastructure**: After implementing core diagnostic classes, verify functionality:\n- Run `python -m parser.debugging.diagnostic_tools --test-basic` to validate diagnostic capture\n- Test state inspection with simple INI content: capture should show section processing and key-value parsing\n- Verify error tracking works by parsing invalid content and checking error accumulation\n\n**Milestone 2 - Token Stream Analysis**: After implementing tokenization analysis tools:\n- Test boundary analysis with complex strings containing quotes and escape sequences\n- Verify context comparison identifies inconsistent tokenization behavior\n- Check that position tracking validation catches off-by-one errors in line/column calculation\n\n**Milestone 3 - Interactive Debugging**: After implementing interactive debugging interface:\n- Launch interactive session and step through tokenization of sample content\n- Verify state inspection shows accurate tokenizer and parser state at each step\n- Test command completion and help system provide useful guidance\n\n**Milestone 4 - Format-Specific Inspection**: After implementing TOML and YAML inspectors:\n- Verify symbol table inspection shows all definitions and conflicts for complex TOML content\n- Test indentation stack analysis correctly tracks YAML nesting transitions\n- Confirm type inference tracing explains scalar conversion decisions accurately\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Diagnostic tools crash on valid input | Exception in diagnostic code path | Enable exception logging in diagnostic framework | Add error handling to diagnostic methods |\n| State inspection shows empty data | Diagnostic hooks not properly integrated | Verify diagnostic capture points in parser code | Add diagnostic calls at key parsing phases |\n| Interactive debugger commands fail | Command parsing or execution errors | Test command parsing logic with simple inputs | Implement robust command validation and error reporting |\n| Token boundary analysis reports false positives | Position calculation differences | Compare manual position calculation with automated results | Align position calculation methods between tools and parser |\n| Performance degradation with diagnostics enabled | Excessive diagnostic overhead | Profile diagnostic operations vs parsing operations | Optimize diagnostic data collection and disable expensive operations in production |\n\n\n## Future Extensions\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - this section defines how the completed architecture can be extended with new formats, advanced features, and performance optimizations\n\nThe configuration file parser architecture we've designed provides a solid foundation that can grow with evolving requirements. Think of our current implementation as a well-designed foundation for a building — the structural elements are in place to support additional floors, rooms, and capabilities without requiring reconstruction of the core infrastructure. This extensibility comes from our careful separation of concerns, standardized interfaces, and unified data model that abstracts away format-specific details.\n\nThe extension strategies we'll explore fall into three categories: adding support for new configuration formats that follow similar parsing paradigms, integrating advanced features that enhance the parser's capabilities beyond basic configuration reading, and implementing performance optimizations that enable the parser to handle larger files and higher throughput scenarios. Each category requires different architectural considerations and presents unique challenges in maintaining backward compatibility while adding new functionality.\n\nOur design's extensibility stems from several key architectural decisions. The `BaseTokenizer` abstraction provides a consistent interface that new format tokenizers can implement, while the `ParseContext` mechanism ensures that format-specific parsers can access shared functionality like error reporting and position tracking. The unified output format using nested dictionaries means that new formats only need to map their structures to this common representation, rather than requiring changes throughout the system.\n\n### Adding New Configuration Formats\n\nThe architecture's modular design makes adding new configuration formats a straightforward process that follows established patterns. Consider the task of adding JSON5 support — JSON5 extends standard JSON with comments, trailing commas, and more flexible string literals. This format fits naturally into our tokenization-then-parsing pipeline because it shares structural similarities with existing formats while introducing format-specific syntactic features.\n\n**Mental Model for Format Extension**: Think of adding a new format like adding a new language translator to an international conference. The conference already has the infrastructure (microphones, translation booths, audience seating), standardized protocols (when speakers talk, how questions are handled), and a common output format (simultaneous translation in multiple languages). Adding a new language requires creating a translator who understands that specific language but follows the same protocols and produces output in the same standardized format as existing translators.\n\nThe format extension process follows a predictable pattern that leverages our existing infrastructure. New formats require implementing a tokenizer that extends `BaseTokenizer` and produces tokens using our standardized `TokenType` enumeration. The format-specific parser must implement the unified `parse(content) -> Dict[str, Any]` interface and integrate with our `ParseContext` system for consistent error reporting and position tracking.\n\n> **Decision: Standardized Format Registration System**\n> - **Context**: New formats need to integrate with format detection and parser factory mechanisms\n> - **Options Considered**: Static registration at compile time, dynamic registration via plugin system, configuration-driven format discovery\n> - **Decision**: Static registration with factory method pattern\n> - **Rationale**: Maintains type safety, enables compile-time verification, simplifies deployment while still allowing conditional compilation of format support\n> - **Consequences**: Adding formats requires code changes but provides stronger guarantees about parser availability and reduces runtime complexity\n\nThe format registration system uses a factory pattern that maps format identifiers to parser creation functions. Each new format registers itself with the central `ConfigurationFormatRegistry` that provides format detection hints and parser instantiation services. This approach ensures that format detection logic remains centralized while allowing format-specific parsers to provide their own syntactic signatures for automatic detection.\n\n| Format Extension Component | Responsibilities | Interface Requirements |\n|---------------------------|------------------|----------------------|\n| Format Tokenizer | Break character stream into format-specific tokens | Extend `BaseTokenizer`, implement `tokenize() -> List[Token]` |\n| Format Parser | Convert tokens to unified data structure | Implement `parse(content) -> Dict[str, Any]` |\n| Format Detector | Provide syntactic signatures for format identification | Implement `get_format_signatures() -> List[str]` |\n| Format Registry Entry | Register format with detection and factory systems | Provide `create_parser(**options)` factory function |\n\n**JSON5 Extension Example**: JSON5 support requires extending our tokenization capabilities to handle JavaScript-style comments and relaxed string literal syntax. The JSON5 tokenizer would recognize `//` and `/* */` comment styles, producing `COMMENT` tokens that the parser ignores. Trailing commas in arrays and objects require lookahead logic to distinguish between legitimate commas and trailing syntax. The parser implementation leverages our existing recursive descent patterns but adds format-specific handling for these syntactic extensions.\n\nThe JSON5 format detection relies on JavaScript-specific syntactic markers like unquoted object keys, single-quoted strings, and hexadecimal number literals. These signatures have high specificity — they're unlikely to appear in INI, TOML, or YAML files — making format detection reliable. The parser maps JSON5 structures directly to our nested dictionary representation since JSON5's object-array model aligns perfectly with our unified output format.\n\n**HCL (HashiCorp Configuration Language) Extension**: HCL presents interesting challenges because it supports both JSON-compatible syntax and a more human-readable block-based syntax. The HCL tokenizer must handle this syntactic duality, potentially operating in different modes based on detected syntax style. HCL's block syntax requires parsing constructs like `resource \"aws_instance\" \"example\" { ... }` which don't map directly to simple key-value pairs.\n\nThe HCL parser addresses this complexity by treating blocks as nested dictionary structures where block types become keys and block labels become nested keys. For example, the resource block above creates a nested structure: `{\"resource\": {\"aws_instance\": {\"example\": { ... }}}}`. This mapping preserves HCL's semantic meaning while fitting into our unified output format.\n\n> **Critical Design Insight**: Format extensions succeed when they embrace our unified data model rather than fighting it. Formats that map naturally to nested dictionaries integrate smoothly, while formats requiring fundamentally different output structures may indicate the need for architectural evolution rather than simple extension.\n\n**Advanced Format Considerations**: Some configuration formats present challenges that push the boundaries of our current architecture. Formats with include mechanisms, template engines, or dynamic evaluation require extensions to our parsing pipeline. These advanced formats might require a two-phase parsing approach: first parsing the static structure using existing mechanisms, then post-processing for dynamic features.\n\nFormats with schema requirements or validation constraints may benefit from integration with our planned schema validation extensions. The format extension architecture should anticipate these needs by providing hooks for post-parse processing and validation phases that can be optionally enabled based on format requirements and user preferences.\n\n| Extension Challenge | Current Architecture Impact | Recommended Approach |\n|-------------------|---------------------------|---------------------|\n| Include file support | Requires file system access during parsing | Add optional `FileResolver` component to `ParseContext` |\n| Template/variable expansion | Single-pass parsing insufficient | Implement two-phase parsing with post-processing stage |\n| Schema validation | No built-in validation framework | Design validation extension points in parser interface |\n| Custom data types | Limited to basic Python types | Extend value processing with pluggable type converters |\n\n### Advanced Feature Extensions\n\nThe parser architecture provides several extension points for advanced features that enhance functionality beyond basic configuration file reading. These extensions transform the parser from a simple data extraction tool into a comprehensive configuration management system that can handle complex real-world requirements like schema validation, variable interpolation, and modular configuration organization.\n\n**Schema Validation Extension**: Configuration files often require validation against predefined schemas to ensure they contain required fields, use acceptable value ranges, and maintain structural consistency. Think of schema validation like a quality control inspector at a manufacturing plant — it examines each component (configuration section) against predetermined specifications (schema rules) and flags any deviations before the product (configuration data) moves to the next stage (application startup).\n\nThe schema validation extension integrates with our parsing pipeline as a post-processing stage that operates on the unified dictionary output. This design choice preserves the separation between syntactic parsing and semantic validation, allowing schema validation to work consistently across all supported formats. The validation system uses a plugin architecture where different schema languages (JSON Schema, custom validation rules) can be plugged in based on user requirements.\n\n| Schema Validation Component | Purpose | Integration Point |\n|---------------------------|---------|-------------------|\n| Schema Definition Parser | Parse schema files into validation rules | Standalone component, loads schema before configuration parsing |\n| Validation Rule Engine | Apply validation rules to parsed configuration | Post-processing stage after format-specific parsing |\n| Error Enrichment System | Add schema context to validation errors | Integrates with existing `ParseError` hierarchy |\n| Validation Result Reporter | Generate comprehensive validation reports | Extends existing error reporting mechanisms |\n\nThe schema validation system maintains validation context that maps back to original source positions, enabling error messages that reference both the configuration file location and the schema requirement that was violated. For example, a validation error might report: \"Line 15: Missing required field 'database.port' (required by schema section 'server-config')\" providing both syntactic and semantic context.\n\n**Variable Interpolation Extension**: Modern configuration management often requires dynamic value substitution where configuration values can reference other configuration values, environment variables, or external data sources. Consider variable interpolation as similar to mail merge in document processing — templates contain placeholders that get filled in with actual values from various sources to produce the final document.\n\nThe variable interpolation extension operates as a post-processing phase that scans the parsed configuration for interpolation expressions and resolves them using configured value sources. The interpolation engine supports multiple expression syntaxes (${var}, {{var}}, #{var}) to accommodate different format conventions and user preferences. Resolution occurs in dependency order, ensuring that variables are resolved before being used in other variable definitions.\n\n> **Decision: Two-Phase Interpolation Processing**\n> - **Context**: Variable interpolation requires dependency resolution and may involve external data sources\n> - **Options Considered**: Single-pass interpolation during parsing, post-processing after parsing, lazy evaluation during configuration access\n> - **Decision**: Post-processing with dependency graph resolution\n> - **Rationale**: Separates concerns, enables cross-format interpolation, allows dependency cycle detection, supports external value sources\n> - **Consequences**: Requires additional processing phase but provides more powerful and reliable interpolation capabilities\n\nThe interpolation system builds a dependency graph of variable references and performs topological sorting to determine resolution order. Circular dependencies are detected and reported as configuration errors with suggestions for restructuring. The system supports multiple value sources including configuration values, environment variables, file contents, and external service calls through a pluggable provider interface.\n\n**Include File Extension**: Large configuration systems benefit from modular organization where configuration can be split across multiple files and composed at parse time. Think of include file support like a compiler's include mechanism — individual source files can reference other files, and the compilation process assembles them into a complete program.\n\nThe include file extension requires coordination between file system access and parsing operations. Include directives are recognized during parsing and trigger recursive parsing of referenced files. The included content is merged into the parent configuration using format-appropriate rules — INI sections merge, TOML tables merge with conflict detection, and YAML structures merge preserving hierarchy.\n\n| Include Processing Component | Responsibilities | Implementation Considerations |\n|---------------------------|------------------|------------------------------|\n| Include Directive Detection | Recognize format-specific include syntax | Integrates with format-specific parsers |\n| File Resolution System | Resolve relative paths and search paths | Handles file system access and path normalization |\n| Recursive Parse Coordination | Manage parsing of included files | Prevents infinite recursion, maintains parse context |\n| Configuration Merging Logic | Combine included content with parent configuration | Format-specific merging rules with conflict detection |\n\nInclude processing maintains a stack of currently processing files to detect and prevent circular includes. Error reporting preserves the include chain so users can trace errors back through the file inclusion hierarchy. For example: \"Error in config.toml (included from main.toml:15): Invalid table definition at line 8\".\n\n**Configuration Template Extensions**: Some deployment scenarios benefit from template-based configuration generation where configuration files are generated from templates with environment-specific values. This extension transforms the parser into a configuration generation system that can produce format-specific output from template definitions.\n\nThe template extension operates in reverse from normal parsing — instead of reading configuration files and producing data structures, it takes data structures and template definitions to produce configuration files in specific formats. This bidirectional capability enables configuration round-tripping and format conversion workflows.\n\n**Advanced Type System Extensions**: The current parser supports basic data types appropriate for configuration files, but some applications require more sophisticated type handling including custom objects, validated enumerations, and computed values. The advanced type system extension provides pluggable type converters that can transform string literals into domain-specific objects during parsing.\n\nCustom type converters register with the value processing system and provide type detection patterns and conversion logic. For example, a duration converter might recognize strings like \"30m\", \"2h45m\", \"1d\" and convert them to appropriate duration objects. The type system maintains type metadata that can be used by schema validation and error reporting systems.\n\n### Performance and Scalability Improvements\n\nAs configuration files grow larger and parsing becomes more frequent, performance optimizations become essential for maintaining responsive applications. The current parser architecture provides several optimization opportunities that can dramatically improve performance for large-scale usage scenarios without compromising correctness or maintainability.\n\n**Streaming Parser Implementation**: Large configuration files can cause memory pressure when the entire file is loaded into memory for parsing. Think of streaming parsing like reading a book one page at a time instead of memorizing the entire book before understanding its content — you can begin processing and understanding information as soon as you receive it, rather than waiting for complete input.\n\nThe streaming parser extension modifies our tokenization and parsing pipeline to operate on character streams rather than complete string content. This approach enables processing configuration files that exceed available memory and reduces startup latency for applications that only need specific configuration sections.\n\n| Streaming Component | Current Implementation | Streaming Enhancement |\n|--------------------|----------------------|----------------------|\n| Input Processing | Load complete file into memory string | Process character stream with buffered reading |\n| Tokenization | Generate complete token list | Yield tokens on-demand with lookahead buffer |\n| Parsing | Random access to token stream | Forward-only parsing with limited backtracking |\n| Output Generation | Build complete result dictionary | Incremental result building with optional section filtering |\n\nStreaming parsing requires careful consideration of lookahead requirements. TOML's complex syntax occasionally requires significant lookahead to disambiguate constructs, while YAML's indentation sensitivity requires maintaining context about previous lines. The streaming implementation uses bounded buffers that balance memory usage with parsing capability.\n\n**Incremental Parsing for Configuration Updates**: Applications that monitor configuration files for changes benefit from incremental parsing that can update the parsed representation without re-parsing unchanged sections. Consider incremental parsing similar to smart document editing software that only reformats the paragraph you're currently editing rather than reformatting the entire document with every keystroke.\n\nThe incremental parsing extension requires maintaining parse tree metadata that maps sections of the output structure back to source file regions. When file changes are detected, the system identifies affected regions and re-parses only those sections, merging updated content with the cached parse results. This approach dramatically reduces parsing overhead for configuration hot-reloading scenarios.\n\n> **Decision: Section-Level Incremental Granularity**\n> - **Context**: Incremental parsing requires balancing granularity with complexity\n> - **Options Considered**: Line-level incremental updates, section-level updates, full file re-parsing with caching\n> - **Decision**: Section-level incremental updates with dependency tracking\n> - **Rationale**: Provides significant performance benefits while maintaining manageable complexity, aligns with natural configuration organization boundaries\n> - **Consequences**: Requires section dependency tracking but offers substantial performance improvements for large configuration files\n\nIncremental parsing maintains dependency information between configuration sections so that changes in one section can trigger re-parsing of dependent sections. For example, changing a TOML table definition might require re-parsing sections that reference that table through dotted key notation.\n\n**Parse Result Caching System**: Applications that parse the same configuration files repeatedly (such as microservices that restart frequently) benefit from caching parsed results indexed by file content hash. Think of parse result caching like a translator who keeps a glossary of previously translated phrases — common translations can be recalled instantly rather than re-performing the translation work.\n\nThe caching system computes content hashes for configuration files and stores serialized parse results in a cache (memory-based, disk-based, or distributed cache systems). Cache entries include metadata about parsing options and format detection results to ensure cache hits only occur for equivalent parsing scenarios.\n\n**Parallel Parsing for Multi-File Configurations**: Configuration systems with many include files or modular organization can benefit from parallel parsing where independent configuration files are parsed simultaneously. The parallel parsing extension requires careful coordination to maintain dependency order while maximizing concurrency for independent parsing operations.\n\nThe parallel parsing coordinator analyzes include dependencies to create a parsing task graph where independent branches can be processed concurrently. Results are combined using the same merging logic as sequential include processing, but with coordination points that ensure dependency order is preserved.\n\n| Performance Extension | Memory Impact | CPU Impact | Complexity Impact | Use Case |\n|---------------------|---------------|------------|-------------------|----------|\n| Streaming parsing | Significant reduction | Slight increase | Moderate increase | Very large files |\n| Incremental parsing | Moderate increase | Significant reduction for updates | High increase | Frequently changing files |\n| Result caching | Moderate increase | Significant reduction for repeated parsing | Low increase | Repeated parsing scenarios |\n| Parallel parsing | No significant change | Significant reduction | High increase | Multi-file configurations |\n\n**Memory Optimization Strategies**: The current parser creates rich object hierarchies for tokens, parse nodes, and error tracking that provide excellent debugging capabilities but consume significant memory for large configuration files. Memory optimization extensions provide configurable trade-offs between memory usage and debugging capability.\n\nLightweight parsing modes eliminate intermediate parse trees and error context information, producing only the final dictionary output. Token pooling reuses token objects to reduce garbage collection pressure. String interning reduces memory usage for repeated configuration keys and common values. These optimizations can reduce memory usage by 50-80% for typical configuration files while maintaining parsing correctness.\n\n**Async/Await Integration**: Modern applications increasingly use asynchronous programming models where I/O operations should not block execution threads. The async parsing extension provides asynchronous versions of parsing operations that integrate cleanly with async/await frameworks while maintaining the same functionality and error handling characteristics.\n\nAsync parsing is particularly valuable when combined with streaming parsing and include file processing, where file I/O operations can be overlapped with parsing work. The async implementation maintains the same error handling and progress reporting capabilities while enabling better resource utilization in async applications.\n\n### Implementation Guidance\n\n**Technology Recommendations for Extensions:**\n\n| Extension Category | Simple Option | Advanced Option |\n|------------------|---------------|----------------|\n| New Format Support | Direct parser implementation | Plugin architecture with dynamic loading |\n| Schema Validation | JSON Schema library integration | Custom validation engine with DSL |\n| Variable Interpolation | String template substitution | Expression evaluator with function support |\n| Include Files | Recursive parsing with simple merging | Dependency graph with conflict resolution |\n| Streaming Parsing | Generator-based token production | Coroutine-based parser with backtracking |\n| Caching | In-memory dictionary cache | Redis/disk-based cache with TTL |\n\n**Recommended Extension Structure:**\n```\nproject-root/\n  config_parser/\n    core/                      ← existing core components\n      tokenizer.py\n      parsers/\n        ini_parser.py\n        toml_parser.py\n        yaml_parser.py\n    extensions/                ← extension system\n      __init__.py             ← extension registry\n      formats/                ← new format support\n        json5_parser.py\n        hcl_parser.py\n        format_registry.py\n      features/               ← advanced features\n        schema_validator.py\n        variable_interpolator.py\n        include_processor.py\n      performance/            ← performance extensions\n        streaming_parser.py\n        incremental_parser.py\n        cache_manager.py\n    examples/                 ← extension examples\n      custom_format_example.py\n      validation_example.py\n```\n\n**Extension Registry Infrastructure (Complete Implementation):**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Callable, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ExtensionType(Enum):\n    FORMAT_PARSER = \"format_parser\"\n    FEATURE_PROCESSOR = \"feature_processor\"\n    PERFORMANCE_OPTIMIZER = \"performance_optimizer\"\n\n@dataclass\nclass ExtensionInfo:\n    name: str\n    extension_type: ExtensionType\n    version: str\n    dependencies: List[str]\n    factory: Callable[..., Any]\n    config_schema: Optional[Dict[str, Any]] = None\n\nclass ConfigurationExtensionRegistry:\n    def __init__(self):\n        self._extensions: Dict[str, ExtensionInfo] = {}\n        self._format_detectors: Dict[str, Callable[[str], float]] = {}\n        \n    def register_extension(self, extension_info: ExtensionInfo) -> None:\n        \"\"\"Register a new extension with the configuration system.\"\"\"\n        self._validate_dependencies(extension_info.dependencies)\n        self._extensions[extension_info.name] = extension_info\n        \n        if extension_info.extension_type == ExtensionType.FORMAT_PARSER:\n            # Format parsers should provide detection capabilities\n            detector = getattr(extension_info.factory(), 'detect_format_confidence', None)\n            if detector:\n                self._format_detectors[extension_info.name] = detector\n    \n    def get_extension(self, name: str) -> Optional[ExtensionInfo]:\n        return self._extensions.get(name)\n    \n    def create_parser(self, format_name: str, **options) -> Optional[Any]:\n        extension = self.get_extension(format_name)\n        if extension and extension.extension_type == ExtensionType.FORMAT_PARSER:\n            return extension.factory(**options)\n        return None\n    \n    def detect_format(self, content: str) -> Optional[str]:\n        best_format = None\n        best_confidence = 0.0\n        \n        for format_name, detector in self._format_detectors.items():\n            confidence = detector(content)\n            if confidence > best_confidence:\n                best_confidence = confidence\n                best_format = format_name\n        \n        return best_format if best_confidence > 0.7 else None\n\n# Global registry instance\nextension_registry = ConfigurationExtensionRegistry()\n```\n\n**Format Extension Skeleton (JSON5 Example):**\n\n```python\nfrom typing import Dict, Any, List\nfrom config_parser.core.tokenizer import BaseTokenizer, Token, TokenType\nfrom config_parser.core.base_parser import BaseParser\n\nclass JSON5Tokenizer(BaseTokenizer):\n    def __init__(self, source: str):\n        super().__init__(source)\n        # TODO 1: Initialize JSON5-specific tokenization state\n        # TODO 2: Set up comment recognition patterns (// and /* */)\n        # TODO 3: Configure string literal handling for single quotes\n    \n    def tokenize(self) -> List[Token]:\n        # TODO 1: Implement main tokenization loop\n        # TODO 2: Handle JavaScript-style comments\n        # TODO 3: Process unquoted object keys\n        # TODO 4: Handle trailing commas in arrays/objects\n        # TODO 5: Support hexadecimal number literals\n        pass\n\nclass JSON5Parser(BaseParser):\n    def __init__(self, **options):\n        super().__init__()\n        self.tokenizer = None\n        # TODO 1: Initialize parser state for JSON5 syntax\n        # TODO 2: Set up recursive descent parsing methods\n    \n    def parse(self, content: str) -> Dict[str, Any]:\n        # TODO 1: Create JSON5 tokenizer instance\n        # TODO 2: Tokenize input content\n        # TODO 3: Parse JSON5 structure using recursive descent\n        # TODO 4: Convert to unified dictionary format\n        # TODO 5: Handle parsing errors with appropriate context\n        pass\n    \n    @staticmethod\n    def detect_format_confidence(content: str) -> float:\n        # TODO 1: Look for JavaScript-style comments\n        # TODO 2: Check for unquoted object keys\n        # TODO 3: Detect trailing commas\n        # TODO 4: Return confidence score (0.0 - 1.0)\n        pass\n\n# Registration with extension system\njson5_extension = ExtensionInfo(\n    name=\"json5\",\n    extension_type=ExtensionType.FORMAT_PARSER,\n    version=\"1.0.0\",\n    dependencies=[],\n    factory=JSON5Parser\n)\nextension_registry.register_extension(json5_extension)\n```\n\n**Schema Validation Extension Skeleton:**\n\n```python\nfrom typing import Dict, Any, List, Optional\nfrom abc import ABC, abstractmethod\nfrom config_parser.core.errors import ParseError\n\nclass ValidationRule(ABC):\n    @abstractmethod\n    def validate(self, value: Any, path: str, context: Dict[str, Any]) -> List[str]:\n        \"\"\"Return list of validation error messages, empty if valid.\"\"\"\n        pass\n\nclass SchemaValidator:\n    def __init__(self, schema_definition: Dict[str, Any]):\n        self.schema = schema_definition\n        self.rules: Dict[str, ValidationRule] = {}\n        # TODO 1: Parse schema definition into validation rules\n        # TODO 2: Build rule hierarchy for nested validation\n        # TODO 3: Set up error message templates\n    \n    def validate_configuration(self, config: Dict[str, Any]) -> List[ParseError]:\n        # TODO 1: Traverse configuration structure\n        # TODO 2: Apply appropriate validation rules at each level\n        # TODO 3: Collect validation errors with path context\n        # TODO 4: Generate helpful error messages with suggestions\n        # TODO 5: Return list of validation errors\n        pass\n    \n    def add_custom_rule(self, path: str, rule: ValidationRule):\n        # TODO 1: Register custom validation rule for specific configuration path\n        # TODO 2: Integrate with existing rule hierarchy\n        pass\n\n# Integration point for adding validation to parsing pipeline\ndef parse_with_validation(content: str, schema_path: Optional[str] = None, \n                         format_hint: Optional[str] = None) -> Dict[str, Any]:\n    # TODO 1: Parse configuration using standard pipeline\n    # TODO 2: Load schema definition if provided\n    # TODO 3: Create validator instance with schema\n    # TODO 4: Run validation on parsed configuration\n    # TODO 5: Report validation errors alongside parsing errors\n    pass\n```\n\n**Performance Extension Checkpoints:**\n\nAfter implementing streaming parsing:\n- Test with configuration files larger than available memory (generate test files > 1GB)\n- Verify memory usage remains constant regardless of file size: `python -m memory_profiler your_streaming_test.py`\n- Expected behavior: Memory usage should plateau at buffer size rather than growing with file size\n- Performance benchmark: Parse 100MB configuration file in under 30 seconds with less than 50MB memory usage\n\nAfter implementing incremental parsing:\n- Create test scenario with 1000-section configuration file\n- Modify single section and measure re-parse time\n- Expected behavior: Re-parse time should be < 5% of full parse time for single section changes\n- Verify correctness: Output should be identical to full re-parse for all test cases\n\nAfter implementing caching system:\n- Test cache hit rates with repeated parsing of same content\n- Expected behavior: Cache hit should be > 100x faster than full parsing\n- Verify cache invalidation: Content changes should produce cache misses\n- Test command: `python -m pytest tests/performance/test_caching.py -v --benchmark-only`\n\n**Common Extension Pitfalls:**\n\n⚠️ **Pitfall: Breaking Unified Output Format**\nNew format parsers sometimes introduce format-specific data types in the output dictionary, breaking compatibility with existing code that expects standard Python types. Always convert format-specific types to basic Python types (dict, list, str, int, float, bool) in the final output.\n\n⚠️ **Pitfall: Inconsistent Error Reporting**\nExtension components may implement their own error handling that doesn't integrate with the existing `ParseError` hierarchy, leading to inconsistent error messages and loss of source position information. Always use the established error classes and ensure position information propagates correctly.\n\n⚠️ **Pitfall: Memory Leaks in Streaming Parsers**\nStreaming parsers can accumulate state that isn't properly cleaned up, especially when parsing fails partway through large files. Implement proper cleanup in finally blocks and consider using context managers for resource management.\n\n⚠️ **Pitfall: Schema Validation Performance**\nComplex schema validation can become a performance bottleneck, especially with deeply nested configurations. Profile validation performance and consider implementing validation rule caching or lazy validation strategies for large configurations.\n\n\n## Glossary\n\n> **Milestone(s):** All milestones (INI Parser, TOML Tokenizer, TOML Parser, YAML Subset Parser) - comprehensive terminology reference supporting all implementation phases\n\nA comprehensive configuration file parser requires precise vocabulary to communicate complex parsing concepts effectively. This glossary serves as the authoritative reference for all technical terms, parsing concepts, and domain-specific vocabulary used throughout the design document. Understanding these terms is essential for implementing the parser architecture and communicating about parsing challenges.\n\nThe terminology is organized into logical categories to support different aspects of the learning journey. Each definition includes context about where the term appears in the implementation pipeline and why it matters for configuration parsing specifically.\n\n### Parsing Fundamentals\n\n**Tokenization** refers to the process of breaking a character stream into meaningful lexical units called tokens. Think of tokenization as converting a continuous stream of characters into a sequence of building blocks that the parser can understand and manipulate. Each token represents a syntactic element like a string, number, operator, or delimiter that carries semantic meaning within the configuration format.\n\n**Recursive descent** describes a parsing methodology where the parser uses function calls to handle nested grammatical structures. Each function in the parser corresponds to a grammatical rule, and when the parser encounters nested content, it calls the appropriate function recursively. This approach naturally handles the hierarchical structure found in configuration files, where tables can contain subtables, arrays can contain other arrays, and values can be complex nested structures.\n\n**Lookahead parsing** involves examining upcoming tokens in the input stream before making parsing decisions. The parser maintains the ability to inspect future tokens without consuming them, allowing it to choose the correct parsing path based on upcoming context. This technique is crucial for resolving syntactic ambiguities and implementing robust error recovery.\n\n**Parse tree** represents the intermediate structural representation of parsed syntax before conversion to the final data structure. The parse tree captures the grammatical structure of the input, maintaining information about how the parser interpreted each syntactic element. This intermediate representation allows for validation, transformation, and debugging before generating the unified output format.\n\n**Unified output format** describes the consistent nested dictionary structure that all parsers produce regardless of input format. This standardization allows applications to work with configuration data uniformly, regardless of whether the source was INI, TOML, or YAML. The unified format uses Python dictionaries and lists to represent all hierarchical structures and data types.\n\n### Tokenization Concepts\n\n**Lexical ambiguity** occurs when the same character sequence can mean different things depending on the parsing context. For example, the sequence `\"key\"` might be a quoted string literal in one context but part of a larger quoted phrase in another context. Tokenizers must maintain sufficient context to resolve these ambiguities correctly.\n\n**Context sensitivity** describes how the meaning of input changes based on surrounding parsing context. The same character sequence might tokenize as different token types depending on what the parser has already encountered. For instance, a colon character has different meanings in INI key-value pairs versus YAML mapping syntax.\n\n**Whitespace semantics** refers to how different configuration formats treat whitespace as either meaningful or ignorable. INI files generally ignore whitespace around delimiters, while YAML uses indentation as syntactically significant for defining structure. Understanding these semantics is crucial for correct tokenization.\n\n**String literal handling** encompasses the complex logic required to process quoted strings, escape sequences, and multiline variants across different formats. Each format has different rules for string delimiters, escape sequences, and multiline continuation, requiring format-specific tokenization logic.\n\n**Scanning window** describes the tokenizer's moving view through the character stream as it identifies token boundaries. The tokenizer maintains position information and looks ahead through this window to determine where each token begins and ends.\n\n### Data Structure Concepts\n\n| Concept | Description | Usage Context |\n|---------|-------------|---------------|\n| **Token Type Definitions** | Enumerated values representing all possible token categories across formats | Tokenization phase for classifying lexical units |\n| **Parse Node Structure** | Hierarchical representation capturing grammatical relationships | Intermediate parsing phase before final output |\n| **Position Tracking** | Line and column information for error reporting and debugging | Throughout tokenization and parsing for diagnostics |\n| **Symbol Table Management** | Registry of defined keys and tables for conflict detection | TOML parsing to enforce redefinition rules |\n| **Indentation Frame** | Stack element tracking YAML nesting context | YAML parsing for managing hierarchical structure |\n\n### INI Format Concepts\n\n**Section-based organization** describes INI's hierarchical structure where named sections contain key-value pairs. Each section creates a namespace that groups related configuration values, and the parser must track the current section context when processing key-value assignments.\n\n**Line-based parsing** refers to INI's approach of processing input one logical line at a time. The parser examines each line to determine whether it's a section header, key-value pair, comment, or continuation, then processes it according to its type.\n\n**Global keys** are key-value pairs that appear before any section headers in INI files. These keys belong to a default or global namespace and require special handling in the parser's section tracking logic.\n\n**Dotted notation** allows section names to use dot separators to create nested dictionary structures in the output. The parser must expand these dotted sections into hierarchical data structures during processing.\n\n### TOML Format Concepts\n\n**Table redefinition rules** are TOML's constraints preventing conflicting table declarations within the same document. Once a table path is defined explicitly, it cannot be redefined using a different declaration type, and the parser must track all definitions to enforce these rules.\n\n**Array-of-tables** represents TOML's syntax for creating arrays of table instances using double bracket notation `[[table.name]]`. Each array-of-tables declaration creates a new table instance and appends it to an array at the specified path.\n\n**Dotted key expansion** is the automatic creation of nested table structure from dotted key notation like `physical.color = \"orange\"`. The parser must create intermediate tables as needed while respecting existing table definitions.\n\n**Inline tables** use TOML's single-line table syntax `{key = value, key2 = value2}` to define complete table structures within expressions. These tables cannot be modified after creation and require special parsing logic.\n\n**Implicit table creation** occurs when dotted keys reference table paths that don't exist yet. The parser automatically creates the necessary intermediate table structure while tracking these implicit creations for conflict detection.\n\n**Symbol table management** involves tracking all defined keys and tables throughout TOML parsing to detect conflicts and validate redefinition rules. The symbol table maintains metadata about how each key path was defined and where conflicts might occur.\n\n### YAML Format Concepts\n\n**Indentation-driven hierarchical structure** describes YAML's fundamental approach of using whitespace to define nesting relationships. The amount of indentation determines the hierarchical level of each element, and the parser must maintain strict tracking of indentation levels.\n\n**Stack-based approach** refers to the parsing methodology that uses a stack data structure to track current nesting contexts. As indentation increases, the parser pushes new contexts onto the stack, and as indentation decreases, it pops contexts back to the appropriate level.\n\n**Indentation stack** is the specific stack data structure that tracks current nesting contexts and established indentation levels. The stack contains frames representing each nesting level with information about the expected structure type and data being built.\n\n**Structure transitions** are changes in nesting level that require stack operations to maintain correct parsing context. When the parser encounters different indentation levels, it must push or pop stack frames to match the new structure.\n\n**Scalar type inference** involves automatically converting string literals to appropriate Python data types based on their format and content. YAML's implicit typing system requires the parser to recognize patterns like numbers, booleans, and null values.\n\n**Flow syntax** refers to YAML's JSON-like inline syntax using brackets and braces for lists and mappings. This syntax provides an alternative to block syntax and requires different parsing logic.\n\n**Block syntax** is YAML's primary indentation-based syntax for defining hierarchical structures using whitespace and line breaks rather than explicit delimiters.\n\n### Error Handling Concepts\n\n**Error recovery** describes the parser's ability to continue processing after encountering errors to find additional issues in the same parsing pass. Rather than stopping at the first error, the parser attempts to understand enough context to continue and report multiple problems.\n\n**Panic mode recovery** involves discarding input tokens until reaching a known synchronization point where parsing can safely resume. This strategy helps the parser skip over malformed content and continue processing subsequent sections.\n\n**Error production recovery** inserts assumed content when the parser encounters unexpected input but can infer what was likely intended. This approach allows parsing to continue while recording the assumption made.\n\n**Phrase-level recovery** skips minimal malformed syntactic units while preserving as much surrounding structure as possible. This fine-grained approach minimizes the impact of local errors on overall parsing success.\n\n**Graceful degradation** ensures the system continues operating despite errors, providing reduced functionality rather than complete failure. The parser returns partial results when possible, allowing applications to work with the correctly parsed portions.\n\n**Structured error accumulation** involves collecting multiple related errors for unified reporting rather than stopping at each individual problem. This approach provides comprehensive feedback about all issues in a single parsing pass.\n\n### Architecture Concepts\n\n**Component responsibilities** define what each parser component owns in terms of data, processing, and state management. Clear responsibility boundaries prevent overlap and ensure each component has a well-defined purpose within the overall architecture.\n\n**Nested structure mapping** describes the process of creating hierarchical data structures from various flat syntactic representations. Different formats express hierarchy differently, but all must map to the same nested dictionary output format.\n\n**Format detection** involves automatically identifying configuration format from content analysis without explicit format specification. The system analyzes syntactic patterns and structural characteristics to determine the appropriate parser.\n\n**Impedance mismatch** refers to fundamental incompatibilities between different format paradigms that complicate unified processing. For example, YAML's indentation sensitivity conflicts with INI's line-based approach, requiring different parsing strategies.\n\n### Testing and Debugging Concepts\n\n**Milestone verification points** are concrete checkpoints that confirm successful completion of each implementation milestone. These checkpoints provide measurable criteria for evaluating progress and identifying implementation gaps.\n\n**Golden path test data** represents realistic configuration scenarios that reflect common usage patterns. This test data validates that the parser handles typical use cases correctly and produces expected results.\n\n**Edge case test data** explores boundary conditions and corner cases that might reveal parsing bugs or unexpected behavior. This comprehensive testing approach ensures robust handling of unusual but valid input.\n\n**Cross-format equivalence testing** validates that semantically equivalent configurations produce consistent results regardless of their source format. This testing ensures the unified output format truly provides format independence.\n\n**Layered diagnosis methodology** provides a systematic debugging approach that examines parsing problems at multiple levels: character processing, tokenization, parse tree construction, and final output generation.\n\n**Token stream replay** involves recording and re-processing token sequences for diagnostic purposes. This technique allows detailed analysis of tokenization behavior and helps identify the root causes of parsing failures.\n\n**Interactive parser inspection framework** provides tools for step-by-step visibility into parsing operations. These debugging tools allow developers to examine parser state, token streams, and decision points interactively.\n\n### Performance and Scalability Concepts\n\n**Streaming parsing** refers to processing large configuration files without loading the entire content into memory. This approach enables handling of arbitrarily large files by processing content in chunks as it becomes available.\n\n**Incremental parsing** updates parsed results when only part of a file changes, avoiding complete re-parsing of unchanged content. This optimization is valuable for applications that monitor configuration files for changes.\n\n**Parse result caching** stores previously parsed results to avoid re-parsing unchanged files. The caching system must handle cache invalidation correctly when source files are modified.\n\n**Extension architecture** describes the system design that supports pluggable extensions for new formats, features, and optimizations. The architecture provides well-defined extension points without requiring modifications to core parsing logic.\n\n### Advanced Feature Concepts\n\n**Schema validation** involves validating parsed configuration against predefined rules and constraints. This feature ensures configuration correctness beyond just syntactic validity, checking semantic rules and value constraints.\n\n**Variable interpolation** provides dynamic value substitution in configuration files, allowing references to other configuration values or environment variables. This feature requires additional parsing passes to resolve dependencies correctly.\n\n**Include file processing** supports modular configuration through file inclusion mechanisms. The parser must handle file resolution, circular dependency detection, and scope management for included content.\n\n**Dependency graph resolution** ensures variables and includes are processed in the correct order when complex interdependencies exist. The parser must build and traverse dependency graphs to handle these relationships correctly.\n\n### Implementation Guidance\n\nThe implementation of a comprehensive configuration parser requires careful attention to terminology consistency and conceptual clarity. Each term in this glossary represents a specific aspect of parsing theory or implementation practice that has precise meaning within the context of configuration file processing.\n\nWhen implementing parser components, developers should use this terminology consistently to ensure clear communication about design decisions and implementation challenges. The terms provide a shared vocabulary for discussing parsing problems, architectural choices, and debugging strategies.\n\nThe glossary also serves as a reference for understanding the parsing literature and related tools. Many of these terms have broader applications in compiler design and language processing, making them valuable for developers who want to explore more advanced parsing topics.\n\n#### Key Term Categories\n\nThe terminology is organized into several key categories that correspond to different aspects of the parsing implementation:\n\n| Category | Focus Area | Key Terms |\n|----------|------------|-----------|\n| **Parsing Fundamentals** | Core concepts applicable to all parsers | Tokenization, recursive descent, lookahead parsing |\n| **Format-Specific Concepts** | Terms specific to INI, TOML, or YAML | Section-based organization, table redefinition, indentation-driven structure |\n| **Error Handling** | Error detection, recovery, and reporting | Panic mode recovery, graceful degradation, structured error accumulation |\n| **Architecture** | System design and component interaction | Component responsibilities, format detection, impedance mismatch |\n| **Testing and Debugging** | Validation and troubleshooting | Milestone verification, cross-format equivalence, layered diagnosis |\n| **Advanced Features** | Extensions and optimizations | Schema validation, variable interpolation, streaming parsing |\n\n#### Terminology Evolution\n\nAs the parser implementation progresses through different milestones, developers will encounter these terms in specific contexts that reinforce their meanings through practical application. The INI parser milestone introduces fundamental concepts like tokenization and line-based parsing, while the TOML parser milestone adds complexity with recursive descent and symbol table management.\n\nThe YAML parser milestone demonstrates how terminology evolves to handle different parsing paradigms, introducing concepts like indentation-driven structure and stack-based approaches. Throughout all milestones, error handling terminology becomes increasingly important as the parser implementations become more sophisticated and robust.\n"}