{"html":"<h1 id=\"distributed-cache-design-document\">Distributed Cache: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A distributed cache system using consistent hashing to store and retrieve key-value data across multiple nodes, providing high availability and horizontal scalability. The key architectural challenge is maintaining data consistency and availability while handling node failures and dynamic cluster membership.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides foundational understanding for all milestones, with particular relevance to Milestone 1 (Consistent Hash Ring) and Milestone 3 (Cluster Communication).</p>\n</blockquote>\n<p>Building a distributed cache system presents fundamental challenges that don&#39;t exist in single-node caches. When data must be distributed across multiple machines, we face complex problems around data placement, consistency, fault tolerance, and dynamic cluster membership. This section explores why these challenges are inherent to distributed systems and compares different approaches to solving them.</p>\n<p>The core tension in distributed caching lies between <strong>scalability</strong> and <strong>complexity</strong>. A single-node cache is simple and fast but limited by the memory and processing power of one machine. A distributed cache can scale horizontally by adding more nodes, but introduces coordination overhead, network latency, and partial failure scenarios that fundamentally change the system&#39;s behavior.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<p>The distributed cache we&#39;re building must handle several critical scenarios that don&#39;t exist in single-node systems. When a client requests data, the system must quickly determine which node should handle that request. When nodes are added or removed from the cluster, existing data must be redistributed efficiently. When nodes fail, the system must detect failures quickly and route around them while maintaining data availability. These requirements create a complex web of interdependent design decisions.</p>\n<h3 id=\"mental-model-the-library-system-analogy\">Mental Model: The Library System Analogy</h3>\n<p>To understand distributed caching intuitively, imagine a city library system that has grown from a single building to multiple branches across the city. This analogy helps explain the core concepts and challenges of distributed caching in familiar terms.</p>\n<p><strong>Single Library (Single-Node Cache)</strong>: Initially, the city had one central library where all books were stored. Patrons knew exactly where to go, and librarians could quickly locate any book because they knew the entire collection. The system was simple and efficient, but as the city grew, the single library became overcrowded. Patrons faced long lines, parking became impossible, and the building couldn&#39;t physically hold more books.</p>\n<p><strong>Multiple Branches (Distributed Cache)</strong>: To solve the capacity problem, the city built multiple library branches. Now patrons have several nearby locations, reducing travel time and crowding. However, this creates new challenges: Which branch should stock which books? How do patrons know which branch has the book they want? What happens when a branch temporarily closes for maintenance?</p>\n<p><strong>Book Distribution Strategy (Data Partitioning)</strong>: The library system needs a strategy for deciding which books go to which branch. They could use simple rules like &quot;Branch 1 gets books A-F, Branch 2 gets books G-M, Branch 3 gets books N-Z&quot; (similar to simple hashing). This works well initially, but what happens when they open a new branch? Suddenly they need four branches: A-E, F-J, K-P, Q-Z. This requires moving books from every existing branch - a massive reorganization effort.</p>\n<p><strong>The Smart Distribution System (Consistent Hashing)</strong>: A better approach is to imagine a circular arrangement where branches are positioned around a circle, and books are assigned to the &quot;next&quot; branch clockwise from their position on the circle. When a new branch opens, only books from its immediate neighbors need to be redistributed. This dramatically reduces the reorganization effort while maintaining balanced distribution.</p>\n<p><strong>Popular Books (Hot Keys)</strong>: Some books are extremely popular and get requested frequently. If a popular book is only stored at one branch, that branch becomes overwhelmed while others remain underutilized. The library system can address this by storing multiple copies of popular books at different branches (replication) or by temporarily moving popular books to less busy branches (load balancing).</p>\n<p><strong>Branch Communication (Gossip Protocol)</strong>: Branch librarians need to stay informed about the status of other branches. Is Branch 3 closed for renovations? Did Branch 5 just receive a large shipment of new books? Rather than having a central coordinator make phone calls to every branch (which would be a single point of failure), branches can share information with their neighbors, who then share with their neighbors. This gossip-like communication ensures all branches stay informed even if some communication lines are temporarily down.</p>\n<p><strong>Inter-Library Loans (Request Routing)</strong>: When a patron visits Branch 1 looking for a book that&#39;s actually stored at Branch 4, the librarian doesn&#39;t just say &quot;sorry, we don&#39;t have it.&quot; Instead, they can check their catalog system, identify the correct branch, and either direct the patron there or arrange for the book to be transferred. This is similar to how cache nodes route requests to the correct node in the cluster.</p>\n<p><strong>Backup Copies (Replication)</strong>: Important or popular books might have copies at multiple branches. If Branch 2 is damaged in a flood, patrons can still access those books from other branches that have copies. However, this creates new challenges: If someone returns a book with notes in the margins to Branch 1, how do other branches know about those changes? The library system needs policies for keeping copies synchronized and resolving conflicts.</p>\n<p>This library analogy captures the essential trade-offs of distributed systems: improved capacity and availability through distribution, but increased complexity in coordination, consistency, and failure handling. Just as the library system must balance patron convenience with operational complexity, our distributed cache must balance performance and scalability with system complexity and consistency guarantees.</p>\n<h3 id=\"existing-approaches-comparison\">Existing Approaches Comparison</h3>\n<p>Different strategies exist for distributing data across multiple cache nodes, each with distinct trade-offs in terms of implementation complexity, load distribution, and operational characteristics. Understanding these approaches helps explain why consistent hashing has become the preferred solution for many distributed systems.</p>\n<p><strong>Simple Modulo Hashing</strong> represents the most straightforward approach to data distribution. In this strategy, keys are hashed using a standard hash function, and the resulting hash value is taken modulo the number of nodes to determine placement. For example, with 4 nodes, <code>hash(key) % 4</code> determines which node (0, 1, 2, or 3) should store the key.</p>\n<p>The primary advantage of modulo hashing is its simplicity. The algorithm is trivial to implement, requires no complex data structures, and provides excellent load distribution when the number of nodes remains constant. Key lookup is extremely fast - just a single hash computation and modulo operation.</p>\n<p>However, modulo hashing has a fatal flaw for dynamic systems: <strong>reshuffling on membership changes</strong>. When nodes are added or removed, the modulo value changes, causing most keys to map to different nodes than before. Adding a 5th node to our example changes the modulo from 4 to 5, meaning a key that previously mapped to <code>hash(key) % 4 = 2</code> might now map to <code>hash(key) % 5 = 3</code>. This typically requires redistributing 60-80% of all stored data, creating massive network traffic and temporary inconsistency.</p>\n<p><strong>Range-Based Partitioning</strong> divides the key space into contiguous ranges assigned to different nodes. For example, keys starting with A-F go to Node 1, G-M to Node 2, N-S to Node 3, and T-Z to Node 4. This approach is conceptually simple and works well when key access patterns are uniform across the alphabet.</p>\n<p>Range partitioning excels when queries often involve key ranges (though this is less common in caching scenarios). Adding new nodes requires splitting existing ranges rather than reshuffling all data, which seems like an improvement over modulo hashing.</p>\n<p>However, range partitioning suffers from <strong>hotspot problems</strong> when key distributions are non-uniform. If most cache keys start with &quot;user_&quot; or &quot;session_&quot;, the corresponding node becomes overwhelmed while others remain idle. Additionally, determining optimal range boundaries requires knowledge of key distribution patterns that may change over time.</p>\n<p><strong>Directory-Based Approaches</strong> maintain a separate mapping service that tracks which node stores each key or key range. Clients first query the directory service to determine the correct node, then send their cache request to that node.</p>\n<p>Directory services provide maximum flexibility - keys can be placed on any node based on current load, geographic proximity, or other factors. Load balancing becomes straightforward since the directory can direct requests to less busy nodes. The mapping can be changed at any time without complex redistribution algorithms.</p>\n<p>The downside is <strong>additional complexity and failure modes</strong>. Every cache operation now requires two network calls: one to the directory service and one to the actual cache node. The directory service becomes a potential bottleneck and single point of failure. Keeping directory information consistent across multiple directory replicas introduces the same consistency challenges we&#39;re trying to solve for the cache itself.</p>\n<p><strong>Consistent Hashing</strong> strikes a balance between the simplicity of modulo hashing and the flexibility needed for dynamic cluster membership. The key insight is to arrange both nodes and keys on a circular hash ring, where each key is assigned to the first node encountered when moving clockwise around the ring.</p>\n<p>When nodes are added or removed, only keys stored on adjacent nodes need to be redistributed. For example, if Node B is added between existing nodes A and C, only keys that were previously assigned to C but now hash to a position between A and B need to move. This typically affects only 10-20% of keys for small membership changes.</p>\n<p>Consistent hashing also enables <strong>virtual nodes</strong> - each physical node appears at multiple positions on the ring. This provides better load distribution and reduces the impact of node failures. Instead of each node having exactly one position on the ring, Node 1 might appear at positions 100, 300, 750, and 900. This reduces variance in load distribution and makes the system more resilient to individual node characteristics.</p>\n<p>The following table summarizes the key trade-offs between these approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Load Distribution</th>\n<th>Membership Change Impact</th>\n<th>Implementation Complexity</th>\n<th>Failure Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Modulo Hashing</td>\n<td>Excellent</td>\n<td>60-80% data movement</td>\n<td>Very Low</td>\n<td>Simple - remaining nodes handle increased load</td>\n</tr>\n<tr>\n<td>Range Partitioning</td>\n<td>Poor (hotspots)</td>\n<td>Moderate - range splits</td>\n<td>Low</td>\n<td>Hotspot nodes create cascading failures</td>\n</tr>\n<tr>\n<td>Directory Service</td>\n<td>Excellent</td>\n<td>Low - directory updates only</td>\n<td>High</td>\n<td>Directory becomes single point of failure</td>\n</tr>\n<tr>\n<td>Consistent Hashing</td>\n<td>Good</td>\n<td>10-20% data movement</td>\n<td>Moderate</td>\n<td>Gradual degradation with virtual nodes</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The choice of distribution strategy fundamentally determines the system&#39;s operational characteristics. Simple approaches like modulo hashing work well for static clusters but become operationally expensive when nodes are frequently added or removed. Consistent hashing provides the best balance of operational simplicity and dynamic behavior for most distributed cache scenarios.</p>\n</blockquote>\n<p><strong>Hybrid Approaches</strong> combine multiple strategies to optimize for specific use cases. Some systems use consistent hashing for general distribution but maintain directory entries for frequently accessed keys to enable better load balancing. Others use range partitioning within each consistent hash partition to optimize for workloads that benefit from range queries.</p>\n<p>Amazon&#39;s DynamoDB, for example, uses consistent hashing for initial key distribution but employs additional techniques like virtual node splitting and heat balancing to address load imbalances. Cassandra uses consistent hashing with virtual nodes and adds a replication strategy that considers rack and datacenter topology for improved fault tolerance.</p>\n<p>The key lesson is that data distribution strategy affects every aspect of the system: performance characteristics, operational procedures, failure modes, and implementation complexity. Choosing the right approach requires understanding both the workload characteristics and the operational environment where the system will be deployed.</p>\n<h3 id=\"why-distributed-caching-is-hard\">Why Distributed Caching is Hard</h3>\n<p>Distributed caching appears straightforward on the surface - just spread data across multiple machines and route requests appropriately. However, this simple description masks several fundamental challenges that emerge when transitioning from single-node to multi-node systems. These challenges stem from the <strong>CAP theorem</strong> and the inherent complexities of network communication in distributed systems.</p>\n<p><strong>The Network is Unreliable</strong>: In a single-node cache, all operations happen within a single process, where function calls always succeed or fail immediately. In distributed systems, network communication introduces partial failures, message delays, and message reordering. A request to store a key-value pair might succeed on the target node but the response might be lost on the way back to the client. From the client&#39;s perspective, the operation failed, but from the system&#39;s perspective, it succeeded.</p>\n<p>This ambiguity creates difficult decisions for system designers. Should the client retry the operation (risking duplicate writes) or assume it failed (potentially losing data)? Should the system wait for acknowledgments from all replica nodes (risking timeouts) or proceed with partial success (risking inconsistency)? These decisions require understanding the specific requirements of the application and the acceptable trade-offs between consistency, availability, and performance.</p>\n<p><strong>Distributed State is Hard to Coordinate</strong>: A single-node cache has one authoritative view of what data exists and which operations have completed. In a distributed cache, multiple nodes must coordinate to maintain a consistent view of the system state. Consider a simple scenario where a client wants to increment a counter stored in the cache. In a single-node system, this is atomic: read current value, increment, store new value. In a distributed system with replicas, this operation might succeed on some replicas and fail on others, leaving the system in an inconsistent state where different replicas have different counter values.</p>\n<p>Maintaining consistency across replicas requires sophisticated protocols like <strong>two-phase commit</strong>, <strong>Paxos</strong>, or <strong>Raft</strong>. These protocols ensure all replicas agree on the order of operations and the final state, but they introduce significant complexity and performance overhead. The system must handle scenarios where some replicas are temporarily unavailable, network partitions split the cluster, and nodes recover after extended downtime with potentially stale data.</p>\n<p><strong>Cluster Membership is Dynamic</strong>: Unlike single-node systems where the &quot;cluster&quot; consists of exactly one node, distributed caches must handle nodes joining and leaving the cluster dynamically. New nodes might be added to increase capacity, existing nodes might fail or be taken offline for maintenance, and network partitions might temporarily split the cluster into multiple sub-clusters.</p>\n<p>Each membership change potentially affects data placement, load distribution, and replication strategies. When a node fails, its data becomes unavailable unless replicated elsewhere. When a node joins, existing data must be redistributed to maintain load balance. When a network partition heals, nodes must reconcile potentially conflicting changes made during the partition.</p>\n<p>The challenge is detecting membership changes reliably and quickly. A node might be slow to respond due to high load rather than actual failure. Network congestion might delay heartbeat messages, leading to false failure detections. Automated systems must distinguish between temporary slowdowns and permanent failures while avoiding unnecessary data movement and client disruption.</p>\n<p><strong>Failure Modes Multiply</strong>: Single-node systems have simple failure modes: the process crashes, the machine loses power, or the disk fails. Distributed systems inherit all these failure modes and add many more: network partitions, cascading failures, split-brain scenarios, and partial failures where some operations succeed while others fail.</p>\n<p>Consider what happens when a cache node becomes temporarily unreachable due to network congestion. Should other nodes immediately mark it as failed and begin redistributing its data? If they do, and the node recovers quickly, the redistribution work was unnecessary and created temporary inconsistency. If they don&#39;t, and the node has actually failed, clients experience longer outages waiting for timeouts.</p>\n<p><strong>Split-brain scenarios</strong> occur when network partitions divide the cluster into multiple sub-clusters, each believing it&#39;s the authoritative portion of the system. Both sub-clusters might accept writes for the same keys, creating conflicts that must be resolved when the partition heals. Prevention requires sophisticated quorum protocols that ensure at most one partition can accept writes, but this risks availability during partitions.</p>\n<p><strong>Cascading failures</strong> happen when the failure of one node increases load on remaining nodes, potentially causing them to fail as well. If a 10-node cache cluster suddenly loses 2 nodes, the remaining 8 nodes must handle 25% more load. If they were already operating near capacity, this additional load might cause timeouts, which clients interpret as failures, leading them to retry operations and create even more load.</p>\n<p><strong>Data Consistency vs. Performance Trade-offs</strong>: The CAP theorem formally proves that distributed systems cannot simultaneously provide consistency, availability, and partition tolerance. Cache systems must explicitly choose which guarantees to provide and under what circumstances to sacrifice others.</p>\n<p><strong>Strong consistency</strong> ensures all replicas always return the same value for a given key, but requires coordination overhead that increases latency and reduces availability during network issues. <strong>Eventual consistency</strong> allows temporary divergence between replicas in exchange for better performance and availability, but complicates application logic since reads might return stale data.</p>\n<p><strong>Read-your-writes consistency</strong> guarantees that clients immediately see their own updates, but may see stale data from other clients&#39; updates. This requires sticky sessions or careful coordination between the client and replica nodes. <strong>Monotonic read consistency</strong> ensures clients never see older versions of data than they&#39;ve previously seen, but allows different clients to see different versions simultaneously.</p>\n<p>Each consistency level has different implementation requirements and performance characteristics. Strong consistency typically requires all replicas to agree before completing operations, while eventual consistency can complete operations locally and propagate changes asynchronously. The choice affects not just performance, but also the complexity of client logic and operational procedures.</p>\n<p><strong>Load Distribution Challenges</strong>: Even with perfect hashing algorithms, distributed caches face load imbalances due to variations in data size, access patterns, and node characteristics. Some keys are accessed much more frequently than others (<strong>hot keys</strong>), causing the nodes that store them to become bottlenecks. Some values are much larger than others, causing uneven memory usage across nodes.</p>\n<p><strong>Hot key problems</strong> are particularly challenging because they violate the assumption that load distributes evenly across nodes. A single trending topic on social media might cause millions of cache requests for the same key, overwhelming whichever node happens to store it. Traditional solutions like replication help but don&#39;t eliminate the bottleneck, since all writes still go to the same primary replica.</p>\n<p><strong>Data size variations</strong> can cause some nodes to reach memory limits while others remain half-empty. A few large cached objects (like serialized images or documents) might consume most of a node&#39;s memory, while other nodes store many small key-value pairs. Simple approaches like moving large objects to different nodes help temporarily but don&#39;t solve the fundamental problem of unpredictable data sizes.</p>\n<p><strong>Observability and Debugging Complexity</strong>: Single-node systems can be debugged by examining logs, memory dumps, and profiler output from a single process. Distributed systems require correlating information across multiple nodes, networks, and time zones to understand system behavior.</p>\n<p><strong>Distributed tracing</strong> becomes essential for following requests across multiple nodes and understanding where latency or failures occur. A single client request might touch 5-10 different cache nodes when considering replication, routing, and failure recovery. Understanding why a request was slow requires correlating timing information across all these nodes and accounting for network delays.</p>\n<p><strong>Monitoring and alerting</strong> must account for the fact that partial failures are normal in distributed systems. Traditional monitoring approaches that alert on any node failure would create alert fatigue, since individual node failures should be handled automatically. Instead, monitoring must focus on cluster-level health: overall availability, consistency violations, and degraded performance rather than individual node status.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The fundamental challenge of distributed caching isn&#39;t technical complexity - it&#39;s the explosion of possible system states and failure modes. A single-node cache has predictable behavior and simple failure modes. A distributed cache must handle scenarios where some operations succeed while others fail, some nodes are available while others aren&#39;t, and some data is consistent while other data diverges. Managing this complexity requires careful design decisions about consistency guarantees, failure detection, and recovery procedures.</p>\n</blockquote>\n<p>The distributed cache we&#39;re building addresses these challenges through several key design decisions: consistent hashing for minimal data movement during membership changes, gossip protocols for robust membership detection, configurable consistency levels for application-specific trade-offs, and comprehensive monitoring and observability for operational simplicity. Each of these decisions involves trade-offs, and understanding the alternatives helps inform better choices for specific deployment environments and application requirements.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building a distributed cache requires careful technology choices and architectural decisions that will affect the entire implementation. This section provides concrete recommendations for getting started with a Go-based implementation that balances simplicity with the flexibility to add advanced features later.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<p>The following table outlines technology choices for different system components, providing both simple options for initial implementation and advanced alternatives for production deployments:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Reasoning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transport Protocol</td>\n<td>HTTP REST with JSON (net/http)</td>\n<td>gRPC with Protocol Buffers</td>\n<td>HTTP is easier to debug and test; gRPC provides better performance and type safety</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON (encoding/json)</td>\n<td>Protocol Buffers or MessagePack</td>\n<td>JSON is human-readable and widely supported; binary formats are more efficient</td>\n</tr>\n<tr>\n<td>Node Discovery</td>\n<td>Static configuration file</td>\n<td>etcd, Consul, or mDNS</td>\n<td>Static config is simple for development; service discovery scales better</td>\n</tr>\n<tr>\n<td>Health Checking</td>\n<td>HTTP health endpoints</td>\n<td>Custom UDP heartbeat protocol</td>\n<td>HTTP reuses existing infrastructure; UDP reduces network overhead</td>\n</tr>\n<tr>\n<td>Consistent Hashing</td>\n<td>SHA-256 with standard library</td>\n<td>Jump Consistent Hash or MurmurHash3</td>\n<td>SHA-256 is built-in and secure; specialized hashes may perform better</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard log package</td>\n<td>Structured logging (logrus, zap)</td>\n<td>Standard library requires no dependencies; structured logs enable better analysis</td>\n</tr>\n<tr>\n<td>Metrics</td>\n<td>Simple counters in memory</td>\n<td>Prometheus metrics with HTTP endpoint</td>\n<td>In-memory is zero-dependency; Prometheus integrates with monitoring systems</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON config file</td>\n<td>YAML with environment variable override</td>\n<td>JSON is simple and built-in; YAML is more readable for complex configs</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>Organize the codebase to separate concerns clearly and make it easy to test individual components in isolation:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-cache/\n├── cmd/\n│   ├── cachenode/\n│   │   └── main.go                    ← Entry point for cache node process\n│   └── client/\n│       └── main.go                    ← Example client for testing\n├── internal/\n│   ├── cache/\n│   │   ├── lru.go                     ← LRU cache implementation (Milestone 2)\n│   │   ├── node.go                    ← Cache node with networking\n│   │   └── cache_test.go              ← Unit tests\n│   ├── ring/\n│   │   ├── consistent.go              ← Consistent hash ring (Milestone 1)\n│   │   ├── virtual.go                 ← Virtual node management\n│   │   └── ring_test.go               ← Hash ring tests\n│   ├── cluster/\n│   │   ├── membership.go              ← Node discovery and membership (Milestone 3)\n│   │   ├── gossip.go                  ← Gossip protocol implementation\n│   │   ├── health.go                  ← Health checking\n│   │   └── cluster_test.go            ← Integration tests\n│   ├── replication/\n│   │   ├── strategy.go                ← Replication factor and placement (Milestone 4)\n│   │   ├── quorum.go                  ← Read/write quorum handling\n│   │   ├── conflict.go                ← Conflict resolution\n│   │   └── replication_test.go        ← Replication tests\n│   ├── transport/\n│   │   ├── http.go                    ← HTTP transport layer\n│   │   ├── messages.go                ← Message type definitions\n│   │   └── transport_test.go          ← Transport tests\n│   └── config/\n│       ├── config.go                  ← Configuration loading and validation\n│       └── defaults.go                ← Default configuration values\n├── pkg/\n│   └── api/\n│       ├── client.go                  ← Client library for applications\n│       └── types.go                   ← Public API types\n├── configs/\n│   ├── node1.json                     ← Example node configurations\n│   ├── node2.json\n│   └── cluster.json                   ← Cluster-wide settings\n├── scripts/\n│   ├── start-cluster.sh               ← Development cluster startup\n│   └── test-integration.sh            ← Integration test runner\n├── go.mod\n├── go.sum\n├── README.md\n└── Makefile                           ← Build and test automation</code></pre></div>\n\n<p>This structure separates the core algorithms (<code>internal/ring</code>, <code>internal/cache</code>) from the networking and coordination logic (<code>internal/cluster</code>, <code>internal/transport</code>). The <code>pkg/api</code> directory contains the public interface that client applications will use, while <code>internal/</code> contains implementation details that shouldn&#39;t be imported by external code.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>The following complete implementations handle networking and configuration concerns, allowing learners to focus on the core distributed caching algorithms:</p>\n<p><strong>Configuration Management</strong> (<code>internal/config/config.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeConfig contains all settings for a single cache node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ListenAddress   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"listen_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AdvertiseAddr   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"advertise_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JoinAddresses   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"join_addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxMemoryMB     </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"max_memory_mb\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirtualNodes    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"virtual_nodes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicationFactor </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">         `json:\"replication_factor\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HealthCheckInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"health_check_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GossipInterval  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"gossip_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"request_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from file and environment variables</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Open</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to open config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> file.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    decoder </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">NewDecoder</span><span style=\"color:#E1E4E8\">(file)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> decoder.</span><span style=\"color:#B392F0\">Decode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply defaults for missing values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.MaxMemoryMB </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.MaxMemoryMB </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\"> // 1GB default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.VirtualNodes </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.VirtualNodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 150</span><span style=\"color:#6A737D\"> // Good default for load distribution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.ReplicationFactor </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.ReplicationFactor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.HealthCheckInterval </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.HealthCheckInterval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.GossipInterval </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.GossipInterval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config.RequestTimeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.RequestTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks that configuration values are sensible</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.NodeID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node_id is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ListenAddress </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"listen_address is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.MaxMemoryMB </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max_memory_mb must be at least 1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"virtual_nodes must be at least 1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ReplicationFactor </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"replication_factor must be at least 1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>HTTP Transport Layer</strong> (<code>internal/transport/http.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HTTPTransport handles HTTP communication between cache nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HTTPTransport</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHTTPTransport creates a transport with specified timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHTTPTransport</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendMessage sends a message to a remote node and returns the response</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Serialize message to JSON</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to marshal message: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create HTTP request</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    url </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"http://</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/api/message\"</span><span style=\"color:#E1E4E8\">, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">, url, bytes.</span><span style=\"color:#B392F0\">NewReader</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Send request</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to send request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check response status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"request failed with status </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read response body</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> response </span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> response.</span><span style=\"color:#B392F0\">ReadFrom</span><span style=\"color:#E1E4E8\">(resp.Body); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read response: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> response.</span><span style=\"color:#B392F0\">Bytes</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck sends a health check request to a node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    url </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"http://</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/health\"</span><span style=\"color:#E1E4E8\">, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, url, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create health check request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check returned status </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Message Type Definitions</strong> (<code>internal/transport/messages.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MessageType identifies the type of message being sent</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MessageType</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeGossip</span><span style=\"color:#B392F0\">     MessageType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"gossip\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeGet</span><span style=\"color:#B392F0\">        MessageType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"get\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeSet</span><span style=\"color:#B392F0\">        MessageType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"set\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeDelete</span><span style=\"color:#B392F0\">     MessageType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"delete\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeReplicate</span><span style=\"color:#B392F0\">  MessageType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"replicate\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Message is the envelope for all inter-node communication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Message</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#B392F0\">MessageType</span><span style=\"color:#9ECBFF\"> `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Sender    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"sender\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">   `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data      </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRequest represents a cache get operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetResponse contains the result of a get operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">    `json:\"value,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Found     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetRequest represents a cache set operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">        `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TTL       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"ttl,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetResponse contains the result of a set operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeleteRequest represents a cache delete operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeleteResponse contains the result of a delete operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Found     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GossipMessage carries cluster membership information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GossipMessage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeStates </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#9ECBFF\"> `json:\"node_states\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version    </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">              `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeState represents the state of a single node in the cluster</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"status\"`</span><span style=\"color:#6A737D\">      // \"active\", \"suspected\", \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSeen    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"last_seen\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version     </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">    `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>The following skeletons provide the method signatures and detailed TODO comments for the core components that learners should implement themselves:</p>\n<p><strong>Consistent Hash Ring Interface</strong> (<code>internal/ring/consistent.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha256</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HashRing implements consistent hashing with virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashRing</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    virtualNodes </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ring        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">  // hash position -> node ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sortedKeys  []</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\">           // sorted hash positions for binary search</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">    // set of active nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHashRing creates a new consistent hash ring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">virtualNodes</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        virtualNodes: virtualNodes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ring:        </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sortedKeys:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodes:       </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddNode adds a node to the hash ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This method should add virtual nodes for better load distribution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if node already exists, return early if it does</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add node to the nodes set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create virtual nodes by hashing nodeID + virtual node index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each virtual node, calculate hash position and add to ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Add hash positions to sortedKeys slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Sort the sortedKeys slice for binary search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use fmt.Sprintf(\"%s:%d\", nodeID, i) for virtual node keys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RemoveNode removes a node from the hash ring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RemoveNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if node exists, return early if it doesn't</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Remove node from nodes set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Find all hash positions for this node's virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove hash positions from ring map and sortedKeys slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Sort the sortedKeys slice after removal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: You'll need to iterate through ring to find positions for this node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNode returns the node responsible for storing the given key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Return empty string if ring has no nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate hash of the key using hashKey function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use binary search to find first position >= key hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If no position found, wrap around to first position (ring property)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return node ID at that position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: sort.Search() can help with binary search</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNodes returns the N nodes responsible for a key (for replication)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNodes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Return empty slice if ring has no nodes or count &#x3C;= 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate hash of the key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Find starting position using binary search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Collect up to 'count' unique nodes by walking clockwise around ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return slice of unique node IDs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use a map to track unique nodes since virtual nodes repeat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNodeKeys returns all keys that should be stored on the given node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This is used during rebalancing to determine which keys to migrate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNodeKeys</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">allKeys</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create slice to hold result keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each key in allKeys, check if GetNode(key) == nodeID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If yes, add to result slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return result slice</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// hashKey computes hash value for a key - you can use this helper</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> hashKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha256.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(key))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hasher.</span><span style=\"color:#B392F0\">Sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(hash[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">24</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(hash[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(hash[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(hash[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>LRU Cache Node</strong> (<code>internal/cache/lru.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cache</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">container/list</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CacheEntry represents a single cached item</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpiresAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Size      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LRUCache implements an LRU cache with TTL support and memory limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LRUCache</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                           // maximum memory in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    used       </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                           // current memory usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    items      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#6A737D\">        // key -> list element</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    order      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">List</span><span style=\"color:#6A737D\">                      // LRU order (front = most recent)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Note: list.Element.Value should be *CacheEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLRUCache creates a new LRU cache with the specified capacity in bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLRUCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacityBytes</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        capacity: capacityBytes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        items:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        order:    list.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Get retrieves a value from the cache</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if key exists in items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If not found, return nil, false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Get CacheEntry from list element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check if entry has expired (time.Now().After(ExpiresAt))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If expired, remove entry and return nil, false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Move element to front of list (most recently used)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return value copy, true</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use c.removeElement() helper for expired entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Set stores a value in the cache</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate entry size (len(key) + len(value) + overhead)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate expiration time (time.Now().Add(ttl) if ttl > 0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If key already exists, remove old entry first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Ensure we have enough space by evicting LRU entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create new CacheEntry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Add entry to front of list and update items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update used memory counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use c.evictUntilCapacity() to make space</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Delete removes a key from the cache</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if key exists in items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If found, remove element and return true</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If not found, return false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evictUntilCapacity removes LRU entries until there's enough space</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evictUntilCapacity</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">neededSpace</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: While (used + neededSpace > capacity) and cache not empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get least recently used element (back of list)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove that element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Continue until enough space available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// removeElement removes a specific element from cache</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">removeElement</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">element</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get CacheEntry from element.Value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Remove element from list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove key from items map  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update used memory counter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupExpired removes expired entries (call periodically)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CleanupExpired</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through all entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check expiration time for each entry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove expired entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return count of removed entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Iterate from back to front to avoid invalidating iterators</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Go-Specific Best Practices:</strong></p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for cache operations that are read-heavy. Multiple goroutines can hold read locks simultaneously, improving performance.</li>\n<li>The <code>container/list</code> package provides an efficient doubly-linked list for LRU tracking. Elements can be moved to front in O(1) time.</li>\n<li>Use <code>context.Context</code> for all network operations to enable timeouts and cancellation.</li>\n<li>Prefer <code>time.Time</code> over Unix timestamps for expiration handling - it&#39;s more readable and handles timezone issues automatically.</li>\n<li>Use <code>make(map[string]Type, estimatedSize)</code> when you know the approximate map size to avoid rehashing during growth.</li>\n</ul>\n<p><strong>Memory Management Tips:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Calculate entry size including overhead</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> calculateEntrySize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Account for string header, slice header, and map entry overhead</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    const</span><span style=\"color:#79B8FF\"> mapEntryOverhead</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 32</span><span style=\"color:#6A737D\"> // approximate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(key) </span><span style=\"color:#F97583\">+</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(value) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> mapEntryOverhead)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Create defensive copies of returned values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> copyBytes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">src</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> src </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dst </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(src))</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(dst, src)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> dst</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Networking Best Practices:</strong></p>\n<ul>\n<li>Always set timeouts on HTTP clients: <code>http.Client{Timeout: 10 * time.Second}</code></li>\n<li>Use connection pooling by reusing HTTP clients rather than creating new ones for each request</li>\n<li>Handle partial failures gracefully - a 500 status code from one node shouldn&#39;t crash the entire operation</li>\n<li>Log request/response details at debug level for troubleshooting distributed issues</li>\n</ul>\n<p><strong>Testing Approach:</strong></p>\n<p>Start with unit tests for individual components before building integration tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test individual components</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ./internal/ring/...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ./internal/cache/...</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test with race detector</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#9ECBFF\"> ./internal/cache/...</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test with coverage</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -cover</span><span style=\"color:#9ECBFF\"> ./internal/...</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint (Consistent Hash Ring):</strong>\nAfter implementing the hash ring, verify it works correctly:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run hash ring tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ./internal/ring/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output: Tests should verify even key distribution,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># minimal redistribution on node changes, and correct virtual node behavior</span></span></code></pre></div>\n\n<p>Create a simple test program to verify hash ring behavior:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Test program in cmd/ring-test/main.go</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ring </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ring.</span><span style=\"color:#B392F0\">NewHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">150</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ring.</span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ring.</span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node2\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ring.</span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node3\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Test key distribution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">keys </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"user:1\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"user:2\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"user:3\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"session:abc\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"data:xyz\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _, key </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> keys {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ring.</span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Key </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> -> Node </span><span style=\"color:#79B8FF\">%s\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, key, node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Test node addition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">fmt.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Adding node4...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ring.</span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node4\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _, key </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> keys {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ring.</span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Key </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> -> Node </span><span style=\"color:#79B8FF\">%s\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, key, node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint (Cache Node Implementation):</strong>\nTest LRU cache behavior and memory limits:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run cache tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#9ECBFF\"> ./internal/cache/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test with race detector to check concurrency</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#9ECBFF\"> ./internal/cache/</span></span></code></pre></div>\n\n<p>Verify cache eviction and TTL behavior manually:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">cache </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> cache.</span><span style=\"color:#B392F0\">NewLRUCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// 1KB capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">cache.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key1\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">), time.Hour)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">cache.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key2\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">), time.Hour) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">cache.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key3\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">), time.Hour) </span><span style=\"color:#6A737D\">// Should evict key1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">_, found </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> cache.</span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key1\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// Should be false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">fmt.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"key1 found: </span><span style=\"color:#79B8FF\">%v\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, found)</span></span></code></pre></div>\n\n<p><strong>Signs of Problems and Debugging:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Uneven key distribution</td>\n<td>Not enough virtual nodes</td>\n<td>Check node assignment counts</td>\n<td>Increase virtual node count to 150+</td>\n</tr>\n<tr>\n<td>High memory usage</td>\n<td>Memory not freed on delete</td>\n<td>Add debug logging to removeElement</td>\n<td>Ensure used counter decrements correctly</td>\n</tr>\n<tr>\n<td>Cache misses on existing data</td>\n<td>Race condition in Get/Set</td>\n<td>Run tests with <code>-race</code> flag</td>\n<td>Add proper mutex locking</td>\n</tr>\n<tr>\n<td>Slow cache operations</td>\n<td>Lock contention</td>\n<td>Profile with <code>go tool pprof</code></td>\n<td>Use RWMutex for read-heavy workloads</td>\n</tr>\n<tr>\n<td>Test failures</td>\n<td>Incorrect algorithm</td>\n<td>Step through with debugger</td>\n<td>Verify hash ring traversal logic</td>\n</tr>\n</tbody></table>\n<p>The key to successful implementation is building incrementally and testing each component thoroughly before moving to the next milestone. Focus on getting the core algorithms correct before adding complexity like networking and replication.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the scope and success criteria for all milestones, with particular relevance to overall system design and architecture decisions.</p>\n</blockquote>\n<p>Building a distributed cache system requires careful balance between functionality, performance, and complexity. Think of this like planning a new public transportation system for a growing city. You need to define which neighborhoods you&#39;ll serve (functional goals), how fast and reliable the service must be (non-functional goals), and which advanced features like high-speed rail connections you&#39;ll defer to future phases (non-goals). Clear goal definition prevents scope creep and ensures every architectural decision serves a specific purpose.</p>\n<p>This section establishes the boundaries of our distributed cache implementation, defining what success looks like at each milestone while explicitly excluding features that would complicate the core learning objectives. These goals directly influence every subsequent design decision, from the consistent hashing algorithm to the replication strategy.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p><strong>Functional goals</strong> define the core capabilities our distributed cache must provide to users and applications. These represent the essential operations that make the system useful as a caching solution.</p>\n<table>\n<thead>\n<tr>\n<th>Goal Category</th>\n<th>Specific Capability</th>\n<th>Success Criteria</th>\n<th>Milestone</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key-Value Operations</td>\n<td>GET operation</td>\n<td>Retrieve cached values by key with sub-millisecond latency for cache hits</td>\n<td>2, 3</td>\n</tr>\n<tr>\n<td>Key-Value Operations</td>\n<td>SET operation</td>\n<td>Store key-value pairs with optional TTL, supporting values up to 1MB</td>\n<td>2, 3</td>\n</tr>\n<tr>\n<td>Key-Value Operations</td>\n<td>DELETE operation</td>\n<td>Remove specific keys from cache with immediate consistency</td>\n<td>2, 3</td>\n</tr>\n<tr>\n<td>Data Distribution</td>\n<td>Consistent hashing</td>\n<td>Distribute keys evenly across cluster nodes with minimal reshuffling on membership changes</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Data Distribution</td>\n<td>Virtual nodes</td>\n<td>Support configurable virtual nodes (64-512 per physical node) for improved load balancing</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Cluster Management</td>\n<td>Node discovery</td>\n<td>New nodes automatically join cluster without manual configuration</td>\n<td>3</td>\n</tr>\n<tr>\n<td>Cluster Management</td>\n<td>Failure detection</td>\n<td>Detect and handle node failures within 5 seconds using health checks</td>\n<td>3</td>\n</tr>\n<tr>\n<td>Cluster Management</td>\n<td>Dynamic membership</td>\n<td>Support adding/removing nodes during runtime without service interruption</td>\n<td>1, 3</td>\n</tr>\n<tr>\n<td>Data Persistence</td>\n<td>Memory management</td>\n<td>Enforce configurable memory limits per node with LRU eviction</td>\n<td>2</td>\n</tr>\n<tr>\n<td>Data Persistence</td>\n<td>TTL expiration</td>\n<td>Automatically expire cached entries after configured time-to-live</td>\n<td>2</td>\n</tr>\n<tr>\n<td>Fault Tolerance</td>\n<td>Data replication</td>\n<td>Store configurable number of copies (1-5) for each cached entry</td>\n<td>4</td>\n</tr>\n<tr>\n<td>Fault Tolerance</td>\n<td>Consistency levels</td>\n<td>Support eventual consistency and strong consistency options for operations</td>\n<td>4</td>\n</tr>\n</tbody></table>\n<p>The <strong>primary functional goal</strong> is providing a reliable key-value store that distributes data across multiple nodes while maintaining reasonable performance characteristics. Unlike a single-node cache, our system must handle the complexity of data distribution, node failures, and network partitions while preserving the simple GET/SET/DELETE interface that applications expect.</p>\n<p><strong>Key-value operations</strong> form the foundation of cache functionality. The <code>GET</code> operation must locate the correct node using consistent hashing, handle cases where data might be replicated across multiple nodes, and return results quickly. The <code>SET</code> operation needs to determine replica placement, coordinate writes across multiple nodes when using replication, and respect memory limits through eviction policies. The <code>DELETE</code> operation must remove all replicas consistently and handle scenarios where some replica nodes might be temporarily unavailable.</p>\n<p><strong>Data distribution</strong> through consistent hashing solves the fundamental challenge of mapping keys to nodes in a way that minimizes data movement when cluster membership changes. Traditional modulo hashing would require redistributing most keys when adding or removing nodes, making it impractical for dynamic clusters. Our consistent hashing implementation must support virtual nodes to prevent hotspots when key distributions are uneven or when popular keys would otherwise concentrate on a single physical node.</p>\n<blockquote>\n<p><strong>The critical insight</strong> is that functional goals must work together harmoniously. For example, TTL expiration must interact correctly with replication (expired entries should be removed from all replicas), and LRU eviction must consider the replication factor when calculating memory usage.</p>\n</blockquote>\n<p><strong>Cluster management</strong> capabilities enable the system to adapt to changing infrastructure conditions. Node discovery allows operators to add capacity by simply starting new cache nodes, which then automatically integrate into the existing cluster. Failure detection ensures that unresponsive nodes are quickly removed from the hash ring, preventing clients from being routed to dead nodes. Dynamic membership changes must preserve data availability by triggering appropriate rebalancing and replica migration.</p>\n<p><strong>Data persistence</strong> in our context refers to keeping data available in memory while managing resource constraints. LRU eviction provides a reasonable policy for choosing which entries to remove when memory pressure occurs. TTL expiration handles the common caching pattern where data has a natural lifetime after which it should be automatically cleaned up.</p>\n<p><strong>Fault tolerance</strong> through replication provides data availability when individual nodes fail. Supporting different consistency levels allows applications to choose their preferred trade-off between performance and data consistency. Eventual consistency enables faster operations with the possibility of temporary inconsistencies, while strong consistency guarantees that all replicas agree before operations complete.</p>\n<h3 id=\"non-functional-goals\">Non-Functional Goals</h3>\n<p><strong>Non-functional goals</strong> specify the quality attributes and performance characteristics that make the distributed cache suitable for production use. These goals influence architectural decisions throughout the system design.</p>\n<table>\n<thead>\n<tr>\n<th>Goal Category</th>\n<th>Requirement</th>\n<th>Target Metric</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Performance</td>\n<td>GET latency</td>\n<td>&lt; 1ms for local cache hits, &lt; 5ms for remote hits</td>\n<td>P99 latency under normal load</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>SET latency</td>\n<td>&lt; 2ms for single replica, &lt; 10ms with replication factor 3</td>\n<td>P99 latency under normal load</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>Throughput</td>\n<td>10,000+ operations/second per node</td>\n<td>Sustained load testing</td>\n</tr>\n<tr>\n<td>Scalability</td>\n<td>Cluster size</td>\n<td>Support 10-100 nodes in single cluster</td>\n<td>Linear scaling verification</td>\n</tr>\n<tr>\n<td>Scalability</td>\n<td>Storage capacity</td>\n<td>1GB - 64GB memory per node</td>\n<td>Configurable memory limits</td>\n</tr>\n<tr>\n<td>Scalability</td>\n<td>Key space</td>\n<td>Support millions of keys per cluster</td>\n<td>Hash distribution testing</td>\n</tr>\n<tr>\n<td>Availability</td>\n<td>Node failure tolerance</td>\n<td>Survive failure of minority of nodes (&lt; 50%)</td>\n<td>Chaos testing scenarios</td>\n</tr>\n<tr>\n<td>Availability</td>\n<td>Recovery time</td>\n<td>Return to full service within 30 seconds of failure</td>\n<td>Automated failure injection</td>\n</tr>\n<tr>\n<td>Availability</td>\n<td>Uptime</td>\n<td>99.9% availability under normal conditions</td>\n<td>Extended stability testing</td>\n</tr>\n<tr>\n<td>Consistency</td>\n<td>Replication lag</td>\n<td>Eventual consistency convergence within 1 second</td>\n<td>Anti-entropy effectiveness</td>\n</tr>\n<tr>\n<td>Consistency</td>\n<td>Conflict resolution</td>\n<td>Deterministic resolution of concurrent writes</td>\n<td>Vector clock or timestamp comparison</td>\n</tr>\n<tr>\n<td>Network Efficiency</td>\n<td>Gossip overhead</td>\n<td>&lt; 5% of network bandwidth for membership gossip</td>\n<td>Network monitoring</td>\n</tr>\n<tr>\n<td>Network Efficiency</td>\n<td>Replication traffic</td>\n<td>Minimal unnecessary duplicate transfers</td>\n<td>Request routing optimization</td>\n</tr>\n<tr>\n<td>Resource Usage</td>\n<td>Memory overhead</td>\n<td>&lt; 20% metadata overhead per cached entry</td>\n<td>Memory profiling</td>\n</tr>\n<tr>\n<td>Resource Usage</td>\n<td>CPU utilization</td>\n<td>&lt; 80% CPU under peak load per node</td>\n<td>Performance monitoring</td>\n</tr>\n</tbody></table>\n<p><strong>Performance goals</strong> establish the responsiveness expectations that make the cache useful for latency-sensitive applications. The <code>GET</code> operation latency targets distinguish between local hits (data stored on the receiving node) and remote hits (data retrieved from other cluster nodes). <code>SET</code> operation latency accounts for the additional work required when replicating data across multiple nodes. Throughput goals ensure that individual nodes can handle substantial request volumes without becoming bottlenecks.</p>\n<blockquote>\n<p><strong>Design Principle:</strong> Performance goals must remain achievable even as cluster size grows. This influences decisions like using efficient serialization formats, minimizing network round trips, and choosing algorithms with favorable complexity characteristics.</p>\n</blockquote>\n<p><strong>Scalability goals</strong> define how the system should behave as load and cluster size increase. Supporting 10-100 nodes covers most practical deployment scenarios while avoiding the complexity required for massive clusters with thousands of nodes. Per-node storage capacity targets reflect typical cache deployment patterns where memory is the primary constraint. Key space scalability ensures that the consistent hashing algorithm and data structures can handle realistic cache workloads.</p>\n<p><strong>Availability goals</strong> specify how the system should respond to various failure scenarios. Tolerating minority node failures means the system remains functional when less than half the nodes are unavailable, which covers most common failure patterns like single node crashes or small network partitions. Recovery time goals ensure that temporary failures don&#39;t cause extended service disruption. The 99.9% uptime target allows for planned maintenance windows while maintaining high service reliability.</p>\n<p><strong>Consistency goals</strong> define how the system handles the inherent challenges of distributed data management. Replication lag targets ensure that eventual consistency doesn&#39;t create indefinitely inconsistent states that could confuse applications. Conflict resolution requirements address scenarios where network partitions or timing issues cause concurrent writes to the same key, requiring deterministic resolution to prevent data corruption.</p>\n<p><strong>Network efficiency goals</strong> prevent the distributed coordination mechanisms from overwhelming the network infrastructure. Gossip protocol overhead must remain bounded even as cluster size increases. Replication traffic optimization ensures that replica updates don&#39;t create unnecessary network amplification, particularly important when replication factors are high or when clients frequently update the same keys.</p>\n<p><strong>Resource usage goals</strong> ensure the system remains efficient and cost-effective to operate. Memory overhead limits prevent metadata from consuming excessive space relative to actual cached data. CPU utilization targets leave headroom for load spikes while ensuring efficient use of available processing capacity.</p>\n<blockquote>\n<p><strong>Architecture Decision: Resource Efficiency vs. Feature Richness</strong></p>\n<ul>\n<li><strong>Context</strong>: Distributed systems can implement sophisticated features like advanced conflict resolution, complex consistency models, or rich query capabilities, but these typically increase resource overhead</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Minimal feature set with optimal resource usage</li>\n<li>Rich feature set with higher overhead</li>\n<li>Configurable features allowing runtime trade-offs</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Minimal feature set with clear extension points</li>\n<li><strong>Rationale</strong>: Learning-focused implementation benefits from understanding core concepts deeply rather than surface coverage of many features. Resource efficiency makes the system practical for development environments with limited resources.</li>\n<li><strong>Consequences</strong>: Some advanced features must be implemented as future extensions, but core system remains understandable and efficient</li>\n</ul>\n</blockquote>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p><strong>Explicit non-goals</strong> define capabilities that are deliberately excluded from this implementation to maintain focus on the core learning objectives. These exclusions prevent scope creep and ensure the system remains comprehensible for educational purposes.</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Excluded Feature</th>\n<th>Rationale for Exclusion</th>\n<th>Future Consideration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Persistence</td>\n<td>Disk-based storage</td>\n<td>Adds complexity of durability, crash recovery, and disk I/O management without contributing to distributed systems learning</td>\n<td>Could be added as storage engine abstraction</td>\n</tr>\n<tr>\n<td>Security</td>\n<td>Authentication/authorization</td>\n<td>Security implementation would obscure core distributed caching concepts</td>\n<td>Essential for production deployment</td>\n</tr>\n<tr>\n<td>Security</td>\n<td>Encryption (in-transit/at-rest)</td>\n<td>Cryptographic concerns distract from cache algorithm focus</td>\n<td>Required for production security</td>\n</tr>\n<tr>\n<td>Advanced Features</td>\n<td>Complex queries (range, pattern matching)</td>\n<td>Transforms system from cache to database, different problem domain</td>\n<td>Would require query execution engine</td>\n</tr>\n<tr>\n<td>Advanced Features</td>\n<td>Transactions across keys</td>\n<td>ACID semantics add significant complexity without cache-specific learning value</td>\n<td>Major architectural change required</td>\n</tr>\n<tr>\n<td>Advanced Features</td>\n<td>Compression</td>\n<td>Implementation detail that doesn&#39;t illustrate distributed systems principles</td>\n<td>Straightforward optimization to add later</td>\n</tr>\n<tr>\n<td>Operational Features</td>\n<td>Metrics/monitoring dashboard</td>\n<td>Important for operations but orthogonal to core caching algorithms</td>\n<td>Standard observability patterns apply</td>\n</tr>\n<tr>\n<td>Operational Features</td>\n<td>Configuration hot-reloading</td>\n<td>Operational convenience that adds state management complexity</td>\n<td>Useful operational enhancement</td>\n</tr>\n<tr>\n<td>Operational Features</td>\n<td>Rolling upgrades</td>\n<td>Production deployment concern beyond scope of algorithm implementation</td>\n<td>Requires versioning and compatibility strategy</td>\n</tr>\n<tr>\n<td>Performance Features</td>\n<td>Memory-mapped files</td>\n<td>OS-specific optimization that complicates memory management learning</td>\n<td>Platform-specific enhancement</td>\n</tr>\n<tr>\n<td>Performance Features</td>\n<td>NUMA-aware allocation</td>\n<td>Hardware-specific optimization beyond algorithmic focus</td>\n<td>Performance tuning for specific deployments</td>\n</tr>\n<tr>\n<td>Network Features</td>\n<td>Protocol buffers/advanced serialization</td>\n<td>Serialization format choice doesn&#39;t affect distributed algorithm design</td>\n<td>Easy to substitute different formats</td>\n</tr>\n<tr>\n<td>Network Features</td>\n<td>Connection pooling/multiplexing</td>\n<td>Network optimization orthogonal to cache logic</td>\n<td>Standard networking optimization</td>\n</tr>\n<tr>\n<td>Consistency Features</td>\n<td>Causal consistency</td>\n<td>Advanced consistency model adds significant complexity</td>\n<td>Research-level feature</td>\n</tr>\n<tr>\n<td>Consistency Features</td>\n<td>Byzantine fault tolerance</td>\n<td>Addresses malicious failures rather than crash failures common in cache deployments</td>\n<td>Different threat model entirely</td>\n</tr>\n</tbody></table>\n<p><strong>Persistence exclusions</strong> keep the focus on distributed coordination rather than storage engine implementation. Disk-based storage would require implementing write-ahead logging, crash recovery, data file management, and disk I/O optimization - each substantial topics that don&#39;t contribute to understanding consistent hashing, replication, or cluster membership management. Memory-only storage is sufficient for cache use cases and eliminates entire categories of failure modes and recovery scenarios.</p>\n<p><strong>Security exclusions</strong> prevent cryptographic and access control concerns from obscuring the core distributed systems algorithms. Authentication would require implementing user management, credential verification, and secure session handling. Encryption would add key management, certificate handling, and performance considerations around cryptographic operations. While essential for production systems, these features can be layered onto the core cache implementation without affecting its fundamental design.</p>\n<blockquote>\n<p><strong>Important Clarification:</strong> Excluding security features doesn&#39;t mean ignoring security entirely. The system design should avoid patterns that would make security difficult to add later, such as exposing internal state through APIs or using communication patterns incompatible with encryption.</p>\n</blockquote>\n<p><strong>Advanced feature exclusions</strong> maintain the focus on caching rather than expanding into general-purpose data storage. Complex queries would require implementing query parsing, execution planning, and result aggregation - transforming the system from a cache into a distributed database with entirely different performance characteristics and consistency requirements. Transactions across multiple keys would require distributed locking, deadlock detection, and two-phase commit protocols that deserve their own dedicated implementation and learning focus.</p>\n<p><strong>Operational feature exclusions</strong> recognize that production deployment concerns, while important, are separate from the core algorithmic challenges of distributed caching. Metrics and monitoring can be added using standard observability libraries without affecting the cache implementation. Configuration hot-reloading would require implementing safe configuration validation and state migration. Rolling upgrades would need version compatibility checking and gradual deployment orchestration.</p>\n<p><strong>Performance optimization exclusions</strong> distinguish between algorithmic performance (which is essential) and platform-specific optimizations (which are implementation details). Memory-mapped files, NUMA-aware allocation, and advanced serialization formats can improve performance but don&#39;t change the fundamental algorithms for consistent hashing, replication, or failure detection. These optimizations can be added later without architectural changes.</p>\n<p><strong>Advanced consistency feature exclusions</strong> focus the implementation on the consistency models most relevant to caching workloads. Causal consistency would require implementing vector clocks across all operations and maintaining causal ordering, adding significant complexity for guarantees that cache applications rarely need. Byzantine fault tolerance addresses malicious node behavior rather than the crash failures and network partitions common in cache deployments.</p>\n<blockquote>\n<p><strong>Design Principle:</strong> Non-goals should be revisitable. The system architecture should not preclude adding excluded features in future iterations, but should prioritize making the included features robust and well-designed.</p>\n</blockquote>\n<p>The exclusion of these features doesn&#39;t mean they&#39;re unimportant, but rather that they represent distinct learning domains that would dilute focus from the core distributed caching challenges. Each excluded feature could form the basis for its own educational implementation project with different architectural requirements and trade-offs.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete technical recommendations for implementing the goals defined above, with focus on practical decisions that support the learning objectives.</p>\n<h4 id=\"a-technology-recommendations\">A. Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network Transport</td>\n<td>HTTP REST with JSON (net/http)</td>\n<td>gRPC with Protocol Buffers</td>\n<td>HTTP for learning clarity</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON encoding/decoding</td>\n<td>MessagePack or Protocol Buffers</td>\n<td>JSON for debugging ease</td>\n</tr>\n<tr>\n<td>Hashing Function</td>\n<td>SHA-256 via crypto/sha256</td>\n<td>xxHash or CityHash</td>\n<td>SHA-256 for reliability</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON files with encoding/json</td>\n<td>YAML with structured validation</td>\n<td>JSON for simplicity</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard log package</td>\n<td>Structured logging (logrus, zap)</td>\n<td>Standard log initially</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Built-in testing package</td>\n<td>Testify with assertions</td>\n<td>Built-in testing</td>\n</tr>\n<tr>\n<td>HTTP Client</td>\n<td>Default http.Client</td>\n<td>Custom client with retries</td>\n<td>Default with timeout</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td>sync.Mutex and sync.RWMutex</td>\n<td>Channels and select statements</td>\n<td>Mutexes for data protection</td>\n</tr>\n</tbody></table>\n<p>The <strong>simple options</strong> are recommended for initial implementation because they reduce the learning curve and make debugging easier. JSON serialization allows easy inspection of messages and configuration files. HTTP transport provides familiar REST semantics that are easy to test with standard tools like curl. Standard library packages minimize external dependencies and compilation complexity.</p>\n<h4 id=\"b-recommended-filemodule-structure\">B. Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-cache/\n├── cmd/\n│   └── cache-server/\n│       └── main.go                 ← Server entry point, configuration loading\n├── internal/\n│   ├── config/\n│   │   ├── config.go              ← NodeConfig type and validation\n│   │   └── config_test.go         ← Configuration testing\n│   ├── hash/\n│   │   ├── ring.go                ← HashRing implementation\n│   │   └── ring_test.go           ← Consistent hashing tests\n│   ├── cache/\n│   │   ├── lru.go                 ← LRUCache implementation\n│   │   ├── entry.go               ← CacheEntry type definition\n│   │   └── cache_test.go          ← Cache operation tests\n│   ├── transport/\n│   │   ├── http.go                ← HTTPTransport implementation\n│   │   ├── messages.go            ← Request/Response types\n│   │   └── transport_test.go      ← Network communication tests\n│   ├── gossip/\n│   │   ├── protocol.go            ← Gossip protocol implementation\n│   │   ├── membership.go          ← NodeState management\n│   │   └── gossip_test.go         ← Cluster membership tests\n│   └── node/\n│       ├── node.go                ← Main cache node orchestration\n│       └── node_test.go           ← Integration tests\n├── pkg/\n│   └── client/\n│       ├── client.go              ← Client library for testing\n│       └── client_test.go         ← Client integration tests\n├── configs/\n│   ├── node1.json                 ← Example node configurations\n│   ├── node2.json\n│   └── node3.json\n├── scripts/\n│   ├── start-cluster.sh           ← Development cluster startup\n│   └── test-operations.sh         ← Manual testing scripts\n└── README.md                      ← Setup and usage instructions</code></pre></div>\n\n<p>This structure separates concerns clearly while maintaining the internal/ convention for implementation details that shouldn&#39;t be imported by external packages. The pkg/client/ directory provides a reusable client library for testing and potential external use.</p>\n<h4 id=\"c-infrastructure-starter-code\">C. Infrastructure Starter Code</h4>\n<p><strong>Configuration Management</strong> - Complete implementation for loading and validating node configuration:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/config/config.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeConfig represents complete configuration for a cache node.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// All fields are required unless noted otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"node_id\"`</span><span style=\"color:#6A737D\">              // Unique identifier for this node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ListenAddress       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"listen_address\"`</span><span style=\"color:#6A737D\">       // Address to bind HTTP server (e.g., \":8080\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AdvertiseAddr       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"advertise_address\"`</span><span style=\"color:#6A737D\">    // Address other nodes use to contact this node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JoinAddresses       []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"join_addresses\"`</span><span style=\"color:#6A737D\">       // Bootstrap addresses for cluster discovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxMemoryMB         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"max_memory_mb\"`</span><span style=\"color:#6A737D\">        // Maximum memory usage in megabytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirtualNodes        </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"virtual_nodes\"`</span><span style=\"color:#6A737D\">        // Number of virtual nodes for consistent hashing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicationFactor   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"replication_factor\"`</span><span style=\"color:#6A737D\">   // Number of replicas for each key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HealthCheckInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"health_check_interval\"`</span><span style=\"color:#6A737D\"> // Frequency of health checks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GossipInterval      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"gossip_interval\"`</span><span style=\"color:#6A737D\">      // Frequency of gossip messages</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"request_timeout\"`</span><span style=\"color:#6A737D\">      // Timeout for inter-node requests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from JSON file and validates all required fields.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate ensures all configuration values are reasonable and required fields are present.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.NodeID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node_id is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ListenAddress </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"listen_address is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.AdvertiseAddr </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"advertise_address is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.MaxMemoryMB </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.MaxMemoryMB </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 65536</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max_memory_mb must be between 1 and 65536\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"virtual_nodes must be between 1 and 1000\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ReplicationFactor </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.ReplicationFactor </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"replication_factor must be between 1 and 10\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.HealthCheckInterval </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> time.Second {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health_check_interval must be at least 1 second\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.GossipInterval </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> time.Second {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"gossip_interval must be at least 1 second\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.RequestTimeout </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Millisecond {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"request_timeout must be at least 100ms\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Message Type Definitions</strong> - Complete types for inter-node communication:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/transport/messages.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MessageType constants for different inter-node message types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeGossip</span><span style=\"color:#F97583\">    =</span><span style=\"color:#9ECBFF\"> \"gossip\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeGet</span><span style=\"color:#F97583\">       =</span><span style=\"color:#9ECBFF\"> \"get\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeSet</span><span style=\"color:#F97583\">       =</span><span style=\"color:#9ECBFF\"> \"set\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeDelete</span><span style=\"color:#F97583\">    =</span><span style=\"color:#9ECBFF\"> \"delete\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MessageTypeReplicate</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"replicate\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Message represents the envelope for all inter-node communication.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Message</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"type\"`</span><span style=\"color:#6A737D\">      // MessageType constant</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Sender    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"sender\"`</span><span style=\"color:#6A737D\">    // Node ID of sender</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">   `json:\"timestamp\"`</span><span style=\"color:#6A737D\"> // When message was created</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data      </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span><span style=\"color:#6A737D\">      // Payload specific to message type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Cache operation request/response types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span><span style=\"color:#6A737D\">               // Key to retrieve</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level\"`</span><span style=\"color:#6A737D\"> // \"eventual\" or \"strong\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"key\"`</span><span style=\"color:#6A737D\">       // Requested key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">    `json:\"value\"`</span><span style=\"color:#6A737D\">     // Retrieved value (nil if not found)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Found     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span><span style=\"color:#6A737D\">     // Whether key exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span><span style=\"color:#6A737D\"> // When value was last modified</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"key\"`</span><span style=\"color:#6A737D\">               // Key to store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value            []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">        `json:\"value\"`</span><span style=\"color:#6A737D\">             // Value to store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TTL              </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"ttl\"`</span><span style=\"color:#6A737D\">               // Time-to-live (0 for no expiration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"consistency_level\"`</span><span style=\"color:#6A737D\"> // \"eventual\" or \"strong\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span><span style=\"color:#6A737D\">   // Whether operation succeeded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span><span style=\"color:#6A737D\"> // When value was stored</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span><span style=\"color:#6A737D\">               // Key to delete</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level\"`</span><span style=\"color:#6A737D\"> // \"eventual\" or \"strong\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span><span style=\"color:#6A737D\">   // Whether operation succeeded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Found   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span><span style=\"color:#6A737D\">     // Whether key existed before deletion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span><span style=\"color:#6A737D\"> // When deletion occurred</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"d-core-logic-skeleton-code\">D. Core Logic Skeleton Code</h4>\n<p><strong>Hash Ring Implementation</strong> - Method signatures with detailed TODOs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/hash/ring.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> hash</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HashRing implements consistent hashing with virtual nodes for even key distribution.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashRing</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    virtualNodes </span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">                 // Number of virtual nodes per physical node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ring         </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">   // Hash position -> node ID mapping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sortedKeys   []</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\">            // Sorted hash positions for binary search</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">     // Set of active nodes for membership tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHashRing creates a new consistent hash ring with specified virtual nodes per physical node.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">virtualNodes</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize ring with empty maps and slices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Store virtualNodes parameter for later use in AddNode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return pointer to new HashRing instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddNode adds a physical node to the ring by creating virtual nodes at hash positions.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if node already exists in hr.nodes map - if yes, return early</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add nodeID to hr.nodes set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Loop hr.virtualNodes times to create virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each virtual node, create unique string like \"nodeID:virtualIndex\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Hash the virtual node string to get uint32 position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Add position -> nodeID mapping to hr.ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Add position to hr.sortedKeys slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Sort hr.sortedKeys to maintain binary search property</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sort.Slice(hr.sortedKeys, func(i, j int) bool { return hr.sortedKeys[i] &#x3C; hr.sortedKeys[j] })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNode returns the node responsible for storing the given key using consistent hashing.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Handle empty ring case - return empty string if no nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Hash the key to get uint32 position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use binary search to find first hash position >= key position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If no position found (key hashes beyond last position), wrap to first position  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Look up node ID from hr.ring using the found position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return the node ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sort.Search for binary search on hr.sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>LRU Cache Implementation</strong> - Core eviction logic skeleton:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/cache/lru.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cache</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">container/list</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LRUCache implements thread-safe LRU cache with memory limits and TTL support.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LRUCache</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span><span style=\"color:#6A737D\">           // Protects all fields from concurrent access</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                 // Maximum memory usage in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    used     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                 // Current memory usage in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    items    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#6A737D\"> // Key -> list element mapping for O(1) lookup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    order    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">List</span><span style=\"color:#6A737D\">            // Doubly-linked list for LRU order tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLRUCache creates a new LRU cache with specified memory capacity in bytes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLRUCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacityBytes</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create LRUCache instance with capacity set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize items map and order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set used memory to 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return pointer to cache instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Set stores a key-value pair with optional TTL, evicting entries if necessary to stay under memory limit.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate entry size (key length + value length + CacheEntry overhead)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check if key already exists - if yes, remove old entry from used memory count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create CacheEntry with key, value, size, and expiration time (now + ttl, or zero if no TTL)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: While (used + entry size > capacity) and items exist, call evictOldest()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create list element containing the CacheEntry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Add element to front of order list (most recently used position)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Add key -> element mapping to items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Add entry size to used memory counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Release write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Time{} for zero value when no TTL specified</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evictOldest removes the least recently used entry from cache to free memory.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evictOldest</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get last element from order list (least recently used)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Remove element from order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Extract CacheEntry from element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove key from items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Subtract entry size from used memory counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This method should only be called while holding write lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"e-language-specific-hints\">E. Language-Specific Hints</h4>\n<p><strong>Go-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>crypto/sha256</code> package for consistent hashing: <code>sha256.Sum256([]byte(input))</code> returns [32]byte array</li>\n<li>Convert hash to uint32 using binary encoding: <code>binary.BigEndian.Uint32(hash[:4])</code></li>\n<li>Use <code>sort.Search</code> for binary search on sorted slices: returns insertion index if not found</li>\n<li>Handle JSON marshaling of time.Duration by implementing custom MarshalJSON/UnmarshalJSON methods</li>\n<li>Use <code>sync.RWMutex</code> for cache operations: RLock() for reads, Lock() for writes</li>\n<li>Use <code>container/list</code> for LRU ordering: PushFront() for new items, MoveToFront() for access</li>\n<li>Use <code>http.Server</code> with custom handler functions for REST API endpoints</li>\n<li>Use <code>context.Context</code> with timeout for inter-node requests: <code>context.WithTimeout(context.Background(), timeout)</code></li>\n<li>Use <code>time.Ticker</code> for periodic tasks like gossip and health checks</li>\n<li>Use buffered channels for graceful shutdown: <code>make(chan struct{}, 1)</code> for shutdown signals</li>\n</ul>\n<p><strong>Memory Management Tips:</strong></p>\n<ul>\n<li>Estimate CacheEntry overhead as ~100 bytes (struct fields, map overhead, list element)</li>\n<li>Use <code>unsafe.Sizeof()</code> during development to measure actual struct sizes</li>\n<li>Track memory usage incrementally rather than recalculating from scratch</li>\n<li>Consider string interning for repeated keys to reduce memory usage</li>\n<li>Use <code>runtime.GC()</code> and <code>runtime.ReadMemStats()</code> for memory pressure testing</li>\n</ul>\n<p><strong>Concurrency Patterns:</strong></p>\n<ul>\n<li>Protect hash ring modifications with mutex during node additions/removals</li>\n<li>Use read locks for cache lookups, write locks for modifications</li>\n<li>Implement graceful shutdown by closing channels and waiting for goroutines</li>\n<li>Use atomic operations (<code>sync/atomic</code>) for simple counters like request metrics</li>\n<li>Avoid nested locks by carefully ordering lock acquisition</li>\n</ul>\n<h4 id=\"f-milestone-checkpoints\">F. Milestone Checkpoints</h4>\n<p><strong>After implementing each milestone, verify these specific behaviors:</strong></p>\n<p><strong>Milestone 1 Checkpoint - Consistent Hash Ring:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/hash/...</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected output should show:</p>\n<ul>\n<li>Hash ring distributes 10,000 random keys evenly across 5 nodes (each node gets 15-25% of keys)</li>\n<li>Adding 6th node redistributes only ~17% of existing keys (1/6 of total)</li>\n<li>Removing 1 node redistributes only that node&#39;s keys to successors</li>\n<li>Key lookup returns same node consistently for same key</li>\n</ul>\n<p>Manual verification:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/hash-test/</span><span style=\"color:#79B8FF\"> -nodes=5</span><span style=\"color:#79B8FF\"> -keys=10000</span><span style=\"color:#79B8FF\"> -virtual-nodes=64</span></span></code></pre></div>\n<p>Should output distribution statistics showing balanced key assignment.</p>\n<p><strong>Milestone 2 Checkpoint - Cache Node:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/cache/...</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n<p>Expected behaviors:</p>\n<ul>\n<li>LRU eviction removes oldest entries when memory limit exceeded</li>\n<li>TTL expiration automatically removes expired entries during cleanup</li>\n<li>Concurrent access doesn&#39;t cause data races or panics</li>\n<li>Memory accounting accurately tracks total usage</li>\n</ul>\n<p>Manual verification:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/set</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"key\":\"test\",\"value\":\"aGVsbG8=\"}'</span><span style=\"color:#6A737D\">  # Set key</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/get/test</span><span style=\"color:#6A737D\">  # Retrieve key</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> DELETE</span><span style=\"color:#9ECBFF\"> http://localhost:8080/delete/test</span><span style=\"color:#6A737D\">  # Delete key</span></span></code></pre></div>\n\n<p><strong>Signs of problems and debugging:</strong></p>\n<ul>\n<li>&quot;Uneven distribution&quot; → Check virtual node count (increase to 64-256)</li>\n<li>&quot;Memory leaks&quot; → Verify evictOldest() actually removes from all data structures</li>\n<li>&quot;Race conditions&quot; → Run with <code>go test -race</code> flag to detect data races</li>\n<li>&quot;TTL not working&quot; → Check that CleanupExpired() runs periodically</li>\n</ul>\n<p>This implementation guidance provides a complete foundation for building the distributed cache while ensuring learners focus on the core algorithmic challenges rather than infrastructure setup complexity.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides the architectural foundation for all milestones, with particular relevance to Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication &amp; Consistency).</p>\n</blockquote>\n<p>The high-level architecture of our distributed cache system follows a <strong>peer-to-peer cluster design</strong> where each node is both a cache server and a cluster participant. Think of this like a neighborhood watch system where every household both protects their own property and coordinates with neighbors to maintain security across the entire neighborhood. Each cache node stores data locally while participating in cluster-wide decisions about membership, routing, and replication.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<p>The architecture embraces the principle of <strong>symmetric responsibility</strong> - there are no special coordinator nodes or single points of failure. Every node can handle client requests, participate in gossip communication, and store cached data. This design choice provides inherent fault tolerance but requires sophisticated coordination mechanisms to maintain consistency and handle cluster membership changes.</p>\n<h3 id=\"component-overview\">Component Overview</h3>\n<p>The distributed cache system consists of five primary components that work together to provide a scalable, fault-tolerant caching service. Each component has distinct responsibilities but must coordinate closely with others to maintain system correctness.</p>\n<h4 id=\"cache-node-component\">Cache Node Component</h4>\n<p>The <strong>Cache Node</strong> serves as the fundamental storage unit of the distributed cache, analogous to a smart librarian who not only manages their own collection of books but also knows exactly which other librarians have specific titles. Each cache node implements an LRU cache with TTL support, handling local storage operations while participating in the broader cluster ecosystem.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Interactions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Local Data Storage</td>\n<td>Maintains LRU cache with configurable memory limits</td>\n<td>Interfaces with LRUCache for all storage operations</td>\n</tr>\n<tr>\n<td>TTL Management</td>\n<td>Automatically expires entries after time-to-live</td>\n<td>Runs background cleanup processes, coordinates with request handlers</td>\n</tr>\n<tr>\n<td>Request Processing</td>\n<td>Handles GET/SET/DELETE operations from clients and peers</td>\n<td>Routes requests through Hash Ring, coordinates with replication logic</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Enforces capacity limits through LRU eviction</td>\n<td>Monitors memory usage, triggers eviction when necessary</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Exposes health status for cluster monitoring</td>\n<td>Responds to health checks, reports metrics to gossip protocol</td>\n</tr>\n</tbody></table>\n<p>The Cache Node must handle concurrent operations safely while maintaining LRU ordering and TTL accuracy. It serves as both the primary interface for client requests and a participant in inter-node replication protocols.</p>\n<h4 id=\"hash-ring-component\">Hash Ring Component</h4>\n<p>The <strong>Hash Ring</strong> implements consistent hashing to determine key placement across cluster nodes, functioning like a circular seating chart that automatically adjusts when guests arrive or leave while minimizing the number of people who need to change seats. This component is crucial for maintaining balanced load distribution and minimizing data movement during cluster membership changes.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key Distribution</td>\n<td>Maps cache keys to responsible nodes using consistent hashing</td>\n<td><code>GetNode(key)</code> returns primary node, <code>GetNodes(key, count)</code> returns replica set</td>\n</tr>\n<tr>\n<td>Virtual Node Management</td>\n<td>Maintains multiple hash positions per physical node</td>\n<td><code>AddNode(nodeID)</code> creates virtual nodes, <code>RemoveNode(nodeID)</code> cleans up positions</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>Distributes keys evenly across available nodes</td>\n<td>Uses virtual nodes to reduce hotspots and improve distribution</td>\n</tr>\n<tr>\n<td>Membership Updates</td>\n<td>Handles node addition and removal with minimal key redistribution</td>\n<td>Rebalances ring while preserving existing key assignments where possible</td>\n</tr>\n<tr>\n<td>Routing Logic</td>\n<td>Provides fast lookup for key-to-node mappings</td>\n<td>Maintains sorted key list for O(log n) lookup performance</td>\n</tr>\n</tbody></table>\n<p>The Hash Ring must handle concurrent membership changes while providing consistent routing decisions. It serves as the authoritative source for determining which nodes should store replicas of any given key.</p>\n<h4 id=\"gossip-protocol-component\">Gossip Protocol Component</h4>\n<p>The <strong>Gossip Protocol</strong> manages cluster membership and state propagation, operating like a neighborhood information network where residents periodically share news with their neighbors, ensuring that important information eventually reaches everyone even if some residents are temporarily unavailable. This component enables the cluster to maintain consistent membership views without requiring a central coordinator.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Mechanisms</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node Discovery</td>\n<td>Helps new nodes join the cluster and announce their presence</td>\n<td>Exchanges node lists during gossip rounds, maintains bootstrap addresses</td>\n</tr>\n<tr>\n<td>Failure Detection</td>\n<td>Identifies unresponsive nodes and propagates failure information</td>\n<td>Combines direct health checks with gossip-based failure detection</td>\n</tr>\n<tr>\n<td>State Synchronization</td>\n<td>Ensures all nodes have consistent views of cluster membership</td>\n<td>Spreads node state updates using epidemic-style propagation</td>\n</tr>\n<tr>\n<td>Network Partition Handling</td>\n<td>Maintains cluster coherence during network splits</td>\n<td>Uses version vectors and conflict resolution for state merging</td>\n</tr>\n<tr>\n<td>Convergence Management</td>\n<td>Ensures cluster state eventually converges across all nodes</td>\n<td>Implements anti-entropy mechanisms to repair divergent state</td>\n</tr>\n</tbody></table>\n<p>The Gossip Protocol must balance information freshness with network overhead, providing timely failure detection while avoiding excessive chattiness that could overwhelm the network.</p>\n<h4 id=\"transport-layer-component\">Transport Layer Component</h4>\n<p>The <strong>Transport Layer</strong> handles all inter-node communication, serving as the postal service of the distributed cache that reliably delivers messages between nodes while handling addressing, serialization, and connection management. This component abstracts network complexity from higher-level protocols.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Implementation Details</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Message Serialization</td>\n<td>Converts protocol messages to wire format</td>\n<td>Uses JSON for human readability, supports binary protocols for performance</td>\n</tr>\n<tr>\n<td>Connection Management</td>\n<td>Maintains persistent connections between frequently communicating nodes</td>\n<td>Implements connection pooling, handles reconnection after failures</td>\n</tr>\n<tr>\n<td>Request Routing</td>\n<td>Delivers messages to correct destination nodes</td>\n<td>Supports both point-to-point and broadcast communication patterns</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Manages network failures and timeout scenarios</td>\n<td>Implements retry logic, circuit breakers for failing nodes</td>\n</tr>\n<tr>\n<td>Protocol Support</td>\n<td>Provides unified interface for different message types</td>\n<td>Supports gossip messages, cache operations, replication requests</td>\n</tr>\n</tbody></table>\n<p>The Transport Layer must handle network partitions gracefully while providing reliable message delivery guarantees for critical cluster operations.</p>\n<h4 id=\"client-router-component\">Client Router Component</h4>\n<p>The <strong>Client Router</strong> serves as the primary interface between external clients and the distributed cache cluster, acting like a knowledgeable concierge who directs visitors to the right department while handling special requests and managing visitor experience. This component provides a unified API that abstracts cluster complexity from client applications.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Client Benefits</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Distribution</td>\n<td>Routes client operations to appropriate cluster nodes</td>\n<td>Clients see single cache interface regardless of cluster topology</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>Distributes client load across healthy cluster members</td>\n<td>Prevents hotspots, improves overall cluster utilization</td>\n</tr>\n<tr>\n<td>Consistency Management</td>\n<td>Enforces consistency levels for read and write operations</td>\n<td>Supports configurable consistency from eventual to strong</td>\n</tr>\n<tr>\n<td>Failure Handling</td>\n<td>Manages retries and fallbacks when nodes are unavailable</td>\n<td>Provides high availability despite individual node failures</td>\n</tr>\n<tr>\n<td>Protocol Translation</td>\n<td>Converts client API calls to internal cluster protocols</td>\n<td>Maintains stable client API while allowing internal protocol evolution</td>\n</tr>\n</tbody></table>\n<p>The Client Router must maintain high availability and low latency while providing consistent behavior even during cluster membership changes or partial failures.</p>\n<blockquote>\n<p><strong>Key Architectural Insight</strong>: The peer-to-peer design eliminates single points of failure but requires every component to handle partial information and eventual consistency. Each component must be designed with the assumption that other components might be temporarily unreachable or operating with stale information.</p>\n</blockquote>\n<h3 id=\"recommended-module-structure\">Recommended Module Structure</h3>\n<p>The codebase organization follows a <strong>domain-driven structure</strong> that groups related functionality while maintaining clear separation of concerns. This structure supports incremental development where each milestone builds upon previous components without requiring major refactoring.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>distributed-cache/\n├── cmd/\n│   ├── cache-node/\n│   │   └── main.go                    # Node entry point and CLI handling\n│   └── cache-client/\n│       └── main.go                    # Example client implementation\n├── internal/\n│   ├── hashring/\n│   │   ├── hashring.go               # HashRing implementation\n│   │   ├── virtual_nodes.go          # Virtual node management\n│   │   └── hashring_test.go          # Consistent hashing tests\n│   ├── cache/\n│   │   ├── lru_cache.go              # LRUCache implementation\n│   │   ├── cache_entry.go            # CacheEntry and TTL logic\n│   │   ├── memory_manager.go         # Memory accounting and limits\n│   │   └── cache_test.go             # Cache behavior tests\n│   ├── transport/\n│   │   ├── http_transport.go         # HTTPTransport implementation\n│   │   ├── messages.go               # Message types and serialization\n│   │   └── transport_test.go         # Network communication tests\n│   ├── gossip/\n│   │   ├── protocol.go               # Gossip protocol implementation\n│   │   ├── membership.go             # Node membership management\n│   │   ├── failure_detector.go       # Health checking and failure detection\n│   │   └── gossip_test.go            # Cluster membership tests\n│   ├── replication/\n│   │   ├── replicator.go             # Data replication logic\n│   │   ├── quorum.go                 # Quorum-based operations\n│   │   ├── conflict_resolver.go      # Conflict resolution strategies\n│   │   └── replication_test.go       # Replication and consistency tests\n│   └── node/\n│       ├── cache_node.go             # Main CacheNode orchestration\n│       ├── request_handler.go        # Client request processing\n│       ├── config.go                 # NodeConfig and configuration\n│       └── node_test.go              # Integration tests\n├── pkg/\n│   ├── api/\n│   │   ├── client.go                 # Public client library\n│   │   └── types.go                  # Public API types\n│   └── config/\n│       └── validation.go             # Configuration validation utilities\n├── configs/\n│   ├── node1.json                    # Example node configurations\n│   ├── node2.json\n│   └── cluster.json                  # Cluster-wide settings\n└── scripts/\n    ├── start-cluster.sh              # Development cluster startup\n    └── test-integration.sh           # Integration test runner</code></pre></div>\n\n<blockquote>\n<p><strong>Decision: Internal vs Public Package Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: Go projects typically use <code>internal/</code> for private packages and <code>pkg/</code> for public APIs, but the boundaries affect how components can be tested and reused</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Everything in <code>pkg/</code> for maximum flexibility</li>\n<li>Everything in <code>internal/</code> for maximum encapsulation</li>\n<li>Core logic in <code>internal/</code>, stable APIs in <code>pkg/</code></li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Core implementation in <code>internal/</code>, public client APIs in <code>pkg/</code></li>\n<li><strong>Rationale</strong>: This prevents external dependencies on unstable internals while providing clean client libraries. The <code>internal/</code> boundary forces us to design proper interfaces between components.</li>\n<li><strong>Consequences</strong>: Components can evolve independently, but inter-component communication must go through well-defined interfaces rather than direct struct access.</li>\n</ul>\n</blockquote>\n<p>The module structure supports <strong>dependency injection</strong> patterns where each component receives its dependencies through constructor functions rather than creating them directly. This design enables comprehensive testing and supports different deployment configurations.</p>\n<table>\n<thead>\n<tr>\n<th>Package</th>\n<th>Primary Types</th>\n<th>Dependencies</th>\n<th>Testing Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>internal/hashring</code></td>\n<td><code>HashRing</code></td>\n<td>None (pure algorithm)</td>\n<td>Unit tests with synthetic node sets</td>\n</tr>\n<tr>\n<td><code>internal/cache</code></td>\n<td><code>LRUCache</code>, <code>CacheEntry</code></td>\n<td>None (pure storage)</td>\n<td>Unit tests with memory pressure scenarios</td>\n</tr>\n<tr>\n<td><code>internal/transport</code></td>\n<td><code>HTTPTransport</code>, <code>Message</code></td>\n<td><code>net/http</code>, <code>encoding/json</code></td>\n<td>Unit tests with mock servers</td>\n</tr>\n<tr>\n<td><code>internal/gossip</code></td>\n<td><code>GossipProtocol</code>, <code>NodeState</code></td>\n<td><code>transport</code>, <code>time</code></td>\n<td>Integration tests with multiple nodes</td>\n</tr>\n<tr>\n<td><code>internal/replication</code></td>\n<td><code>Replicator</code>, <code>QuorumConfig</code></td>\n<td><code>hashring</code>, <code>transport</code></td>\n<td>Integration tests with failure injection</td>\n</tr>\n<tr>\n<td><code>internal/node</code></td>\n<td><code>CacheNode</code>, <code>NodeConfig</code></td>\n<td>All other internal packages</td>\n<td>End-to-end tests with real clusters</td>\n</tr>\n</tbody></table>\n<h3 id=\"communication-patterns\">Communication Patterns</h3>\n<p>The distributed cache employs three distinct communication patterns, each optimized for different types of interactions and consistency requirements. These patterns work together to provide both high performance for common operations and strong consistency for critical cluster management tasks.</p>\n<h4 id=\"client-server-request-response-pattern\">Client-Server Request-Response Pattern</h4>\n<p><strong>Client requests</strong> follow a synchronous request-response pattern where external clients communicate with any cluster node to perform cache operations. Think of this like visiting any branch of a bank - you can perform most transactions at any location, and the branch handles the complexity of accessing your account information regardless of where it&#39;s stored.</p>\n<p>The request flow follows this sequence:</p>\n<ol>\n<li><strong>Client Connection</strong>: Client establishes HTTP connection to any available cluster node</li>\n<li><strong>Request Validation</strong>: Receiving node validates request format and authentication</li>\n<li><strong>Key Routing</strong>: Node uses <code>HashRing.GetNode(key)</code> to determine responsible node(s)</li>\n<li><strong>Local vs Remote Handling</strong>: If current node owns the key, handle locally; otherwise forward to correct node</li>\n<li><strong>Replication Coordination</strong>: For write operations, coordinate with replica nodes based on replication factor</li>\n<li><strong>Response Assembly</strong>: Collect responses from required replicas and return unified result to client</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Message Type</th>\n<th>Request Fields</th>\n<th>Response Fields</th>\n<th>Routing Logic</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>GET</td>\n<td><code>Key</code>, <code>ConsistencyLevel</code></td>\n<td><code>Value</code>, <code>Found</code>, <code>Timestamp</code></td>\n<td>Route to primary, query replicas if strong consistency</td>\n</tr>\n<tr>\n<td>SET</td>\n<td><code>Key</code>, <code>Value</code>, <code>TTL</code>, <code>ConsistencyLevel</code></td>\n<td><code>Success</code>, <code>Timestamp</code></td>\n<td>Route to primary and R replicas based on replication factor</td>\n</tr>\n<tr>\n<td>DELETE</td>\n<td><code>Key</code>, <code>ConsistencyLevel</code></td>\n<td><code>Success</code>, <code>Found</code>, <code>Timestamp</code></td>\n<td>Route to all replicas, require quorum confirmation</td>\n</tr>\n</tbody></table>\n<p>The request-response pattern provides <strong>strong consistency guarantees</strong> when required while allowing <strong>eventual consistency</strong> for performance-critical operations. Client libraries can specify consistency levels per operation, trading latency for consistency based on application requirements.</p>\n<h4 id=\"peer-to-peer-gossip-pattern\">Peer-to-Peer Gossip Pattern</h4>\n<p><strong>Gossip communication</strong> uses an asynchronous, epidemic-style protocol where nodes periodically exchange cluster state information with randomly selected peers. This pattern resembles how news spreads in a small town - residents occasionally chat with neighbors, sharing recent updates, and important information eventually reaches everyone even if not everyone talks to everyone else directly.</p>\n<p>The gossip cycle operates on a fixed interval:</p>\n<ol>\n<li><strong>Peer Selection</strong>: Each node randomly selects 1-3 peers for gossip exchange</li>\n<li><strong>State Preparation</strong>: Node prepares <code>GossipMessage</code> containing local view of cluster membership</li>\n<li><strong>Message Exchange</strong>: Nodes exchange gossip messages bidirectionally </li>\n<li><strong>State Merging</strong>: Each node merges received state with local state, resolving conflicts</li>\n<li><strong>Change Detection</strong>: Nodes detect membership changes and trigger appropriate actions</li>\n<li><strong>Convergence Check</strong>: Periodic verification that cluster state has converged across all nodes</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Gossip Phase</th>\n<th>Information Exchanged</th>\n<th>Conflict Resolution</th>\n<th>Side Effects</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Push Phase</td>\n<td>Local node states, version vectors</td>\n<td>Newer version wins, merge concurrent updates</td>\n<td>Update local membership view</td>\n</tr>\n<tr>\n<td>Pull Phase</td>\n<td>Request for specific node information</td>\n<td>Compare timestamps and versions</td>\n<td>Detect missing or stale information</td>\n</tr>\n<tr>\n<td>Anti-Entropy</td>\n<td>Full state synchronization</td>\n<td>Vector clock comparison</td>\n<td>Repair divergent cluster views</td>\n</tr>\n</tbody></table>\n<p>The gossip pattern provides <strong>eventual consistency</strong> for cluster membership while being highly resilient to network partitions and node failures. Information propagates logarithmically - even in a 1000-node cluster, updates typically reach all nodes within a few gossip rounds.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Gossip intervals must be tuned carefully. Too frequent gossip creates network overhead; too infrequent gossip delays failure detection. The sweet spot is typically 1-5 seconds for most clusters, adjusted based on cluster size and network characteristics.</p>\n</blockquote>\n<h4 id=\"replication-coordination-pattern\">Replication Coordination Pattern</h4>\n<p><strong>Replication messages</strong> use a coordinated protocol where one node acts as coordinator for a specific operation, ensuring consistency across multiple replicas. This resembles a meeting facilitator who ensures all participants have their say and records the group&#39;s decision - the coordinator doesn&#39;t dictate the outcome but ensures the process follows agreed-upon rules.</p>\n<p>The coordination sequence varies by operation type:</p>\n<p><strong>Write Coordination (SET operations):</strong></p>\n<ol>\n<li><strong>Coordinator Selection</strong>: Client request receiver becomes coordinator for this operation</li>\n<li><strong>Replica Identification</strong>: Use <code>HashRing.GetNodes(key, replicationFactor)</code> to find replica nodes</li>\n<li><strong>Prepare Phase</strong>: Send <code>SetRequest</code> to all replica nodes with operation details</li>\n<li><strong>Vote Collection</strong>: Collect success/failure responses from replicas within timeout</li>\n<li><strong>Quorum Check</strong>: Verify that sufficient replicas (W nodes) confirmed the operation</li>\n<li><strong>Client Response</strong>: Return success if write quorum achieved, failure otherwise</li>\n<li><strong>Background Repair</strong>: If some replicas failed, schedule repair operation</li>\n</ol>\n<p><strong>Read Coordination (GET operations):</strong></p>\n<ol>\n<li><strong>Coordinator Selection</strong>: Request receiver coordinates read operation</li>\n<li><strong>Consistency Level Check</strong>: Determine if eventual or strong consistency required</li>\n<li><strong>Replica Query</strong>: For strong consistency, query R replicas; for eventual, query 1-2 replicas</li>\n<li><strong>Response Reconciliation</strong>: If multiple responses differ, apply conflict resolution</li>\n<li><strong>Client Response</strong>: Return most recent value based on timestamp or vector clock</li>\n<li><strong>Read Repair</strong>: If inconsistencies detected, update stale replicas in background</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Consistency Level</th>\n<th>Read Quorum (R)</th>\n<th>Write Quorum (W)</th>\n<th>Guarantees</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Eventual</td>\n<td>1</td>\n<td>1</td>\n<td>High availability, eventual consistency</td>\n<td>Possible stale reads</td>\n</tr>\n<tr>\n<td>Strong</td>\n<td>N/2 + 1</td>\n<td>N/2 + 1</td>\n<td>Linearizable consistency</td>\n<td>Higher latency, lower availability</td>\n</tr>\n<tr>\n<td>Quorum</td>\n<td>Configurable</td>\n<td>Configurable</td>\n<td>Tunable consistency-availability balance</td>\n<td>Requires R + W &gt; N for consistency</td>\n</tr>\n</tbody></table>\n<p>The replication pattern enables <strong>configurable consistency levels</strong> while maintaining high availability during partial failures. Applications can choose appropriate consistency levels per operation based on their specific requirements.</p>\n<blockquote>\n<p><strong>Architectural Trade-off</strong>: The three communication patterns operate independently but must be carefully coordinated. For example, gossip protocol changes to cluster membership must be synchronized with hash ring updates to prevent inconsistent routing decisions during replication operations.</p>\n</blockquote>\n<p><strong>Common Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Blocking on Synchronous Operations During Gossip</strong>\nMany implementations make the mistake of performing expensive operations like disk I/O or network calls during gossip message processing, causing gossip rounds to exceed their target intervals. This creates a cascading effect where delayed gossip leads to slower failure detection and potentially stale membership information. The fix is to process gossip messages asynchronously - update local state immediately but defer expensive operations like hash ring rebuilding to background tasks.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Consistency Level Handling</strong>\nA common error is assuming that consistency levels apply uniformly across all operations, leading to situations where a write at &quot;eventual&quot; consistency is immediately followed by a read at &quot;strong&quot; consistency, creating confusing behavior for clients. The solution is to clearly document consistency semantics and provide client libraries that make consistency choices explicit and prevent invalid combinations.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Message Ordering in Replication</strong>\nDevelopers often forget that network messages can arrive out of order, leading to scenarios where a DELETE operation arrives before the corresponding SET operation, causing replicas to diverge. This is particularly problematic during network partitions or high load. The fix is to include vector clocks or Lamport timestamps in all replication messages and apply operations in causal order rather than arrival order.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the distributed cache architecture, including technology choices, starter code, and development checkpoints.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Transport</td>\n<td><code>net/http</code> with JSON</td>\n<td>gRPC with Protocol Buffers</td>\n<td>HTTP/JSON easier to debug, gRPC better performance</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td><code>encoding/json</code></td>\n<td><code>encoding/gob</code> or Protocol Buffers</td>\n<td>JSON human-readable, binary formats more efficient</td>\n</tr>\n<tr>\n<td>Hashing</td>\n<td><code>crypto/sha256</code></td>\n<td><code>github.com/cespare/xxhash</code></td>\n<td>SHA-256 built-in and secure, xxhash faster for non-crypto use</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON files</td>\n<td>YAML with <code>gopkg.in/yaml.v3</code></td>\n<td>JSON simpler, YAML more readable for complex configs</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td><code>log/slog</code> (Go 1.21+)</td>\n<td><code>github.com/rs/zerolog</code></td>\n<td>slog is standard library, zerolog has better performance</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td><code>testing</code> package</td>\n<td><code>github.com/stretchr/testify</code></td>\n<td>Built-in package sufficient, testify adds convenience</td>\n</tr>\n</tbody></table>\n<p>For this implementation, we recommend starting with the simple options to reduce external dependencies and complexity. Advanced options can be adopted later as performance requirements become clear.</p>\n<h4 id=\"starter-infrastructure-code\">Starter Infrastructure Code</h4>\n<p><strong>Configuration Management (<code>internal/node/config.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> node</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeConfig contains all configuration for a cache node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tNodeID              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tListenAddress       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"listen_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tAdvertiseAddr       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"advertise_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tJoinAddresses       []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"join_addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tMaxMemoryMB         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"max_memory_mb\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tVirtualNodes        </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"virtual_nodes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tReplicationFactor   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"replication_factor\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tHealthCheckInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"health_check_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tGossipInterval      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"gossip_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tRequestTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"request_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from JSON file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tdata, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tvar</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config JSON: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid configuration: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks configuration values for correctness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.NodeID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node_id cannot be empty\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.ListenAddress </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"listen_address cannot be empty\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.MaxMemoryMB </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max_memory_mb must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tc.VirtualNodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 150</span><span style=\"color:#6A737D\"> // reasonable default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.ReplicationFactor </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tc.ReplicationFactor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\"> // reasonable default</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.HealthCheckInterval </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tc.HealthCheckInterval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.GossipInterval </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tc.GossipInterval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> c.RequestTimeout </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tc.RequestTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>HTTP Transport Layer (<code>internal/transport/http_transport.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HTTPTransport handles HTTP communication between nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HTTPTransport</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tclient  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\ttimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHTTPTransport creates HTTP transport with specified timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHTTPTransport</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\tclient: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t\tTimeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\t},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t\ttimeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendMessage sends JSON message to remote node and returns response</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tjsonData, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to marshal message: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\turl </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"http://</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/api/message\"</span><span style=\"color:#E1E4E8\">, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\treq, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">, url, bytes.</span><span style=\"color:#B392F0\">NewBuffer</span><span style=\"color:#E1E4E8\">(jsonData))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\treq.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tresp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to send request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"request failed with status </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tbody, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">(resp.Body)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read response: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#E1E4E8\"> body, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck performs health check against remote node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\turl </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"http://</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/health\"</span><span style=\"color:#E1E4E8\">, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\treq, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, url, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create health check request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tresp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check request failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tif</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\t\treturn</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node unhealthy: status </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Message Types (<code>internal/transport/messages.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Message types for inter-node communication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tMessageTypeGossip</span><span style=\"color:#F97583\">    =</span><span style=\"color:#9ECBFF\"> \"gossip\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tMessageTypeGet</span><span style=\"color:#F97583\">       =</span><span style=\"color:#9ECBFF\"> \"get\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tMessageTypeSet</span><span style=\"color:#F97583\">       =</span><span style=\"color:#9ECBFF\"> \"set\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tMessageTypeDelete</span><span style=\"color:#F97583\">    =</span><span style=\"color:#9ECBFF\"> \"delete\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">\tMessageTypeReplicate</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"replicate\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Message is the base message structure for node communication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Message</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tType      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tSender    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"sender\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTimestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">   `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tData      </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Cache operation messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tKey              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tKey       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tValue     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">    `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tFound     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTimestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tKey              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tValue            []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">        `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTTL              </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"consistency_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tSuccess   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTimestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tKey              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tSuccess   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tFound     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tTimestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Gossip protocol messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tNodeID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tAddress  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tStatus   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tLastSeen </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"last_seen\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tVersion  </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">    `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GossipMessage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tNodeStates </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#9ECBFF\"> `json:\"node_states\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tVersion    </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">               `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-component-skeletons\">Core Component Skeletons</h4>\n<p><strong>Hash Ring Core Logic (<code>internal/hashring/hashring.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> hashring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">crypto/sha256</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HashRing implements consistent hashing with virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashRing</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tmutex        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tvirtualNodes </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tring         </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\"> // hash -> node ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tsortedKeys   []</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#6A737D\">          // sorted hash values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tnodes        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">   // active nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHashRing creates new consistent hash ring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">virtualNodes</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO: Initialize HashRing struct with provided virtual node count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO: Create maps for ring, nodes, and slice for sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO: Set mutex and virtualNodes field</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddNode adds node with virtual nodes to ring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\thr.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> hr.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Check if node already exists in hr.nodes map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Add nodeID to hr.nodes map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: For i := 0 to hr.virtualNodes, create virtual node key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Hash virtual node key to get position on ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Add hash->nodeID mapping to hr.ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Rebuild hr.sortedKeys from hr.ring keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: Sort hr.sortedKeys for binary search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Use fmt.Sprintf(\"%s:%d\", nodeID, i) for virtual node keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Use hashKey() helper function for consistent hashing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RemoveNode removes node and virtual nodes from ring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RemoveNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\thr.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> hr.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Check if node exists in hr.nodes map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Remove nodeID from hr.nodes map  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: For i := 0 to hr.virtualNodes, create virtual node key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Hash virtual node key to get position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Delete hash from hr.ring map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Rebuild hr.sortedKeys from remaining hr.ring keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: Sort hr.sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNode returns node responsible for key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\thr.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> hr.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Check if ring is empty, return empty string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Hash the key to get position on ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Use binary search on hr.sortedKeys to find first key >= hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: If no key found, wrap around to first key (ring property)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Return hr.ring[foundKey] to get node ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Use sort.Search() for binary search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Handle wrap-around case when hash > all sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNodes returns N nodes for replication (starting with primary)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNodes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\thr.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> hr.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Get primary node using GetNode(key)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Find starting position in hr.sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Walk clockwise around ring collecting unique nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Handle wrap-around when reaching end of sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Stop when collected 'count' unique nodes or exhausted all nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Return slice of collected node IDs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Use map[string]bool to track already-added nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// hashKey converts string to uint32 hash for ring positioning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> hashKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\th </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha256.</span><span style=\"color:#B392F0\">Sum256</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(key))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\treturn</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">24</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">])</span><span style=\"color:#F97583\">&#x3C;&#x3C;</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#F97583\"> |</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">(h[</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>LRU Cache Core Logic (<code>internal/cache/lru_cache.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cache</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">container/list</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\t\"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CacheEntry represents a cached key-value pair with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tKey       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tValue     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">    `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tExpiresAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"expires_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tSize      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LRUCache implements LRU eviction with TTL support</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LRUCache</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tmutex    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tcapacity </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                    // max memory in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tused     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                    // current memory usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\titems    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#6A737D\"> // key -> list element</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\torder    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">List</span><span style=\"color:#6A737D\">               // LRU order (front = most recent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLRUCache creates LRU cache with memory limit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLRUCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacityBytes</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO: Initialize LRUCache struct with provided capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO: Create items map and order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO: Set used to 0 and mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Get retrieves value and updates LRU order</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tc.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> c.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Look up key in c.items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: If not found, return nil, false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Get CacheEntry from list element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Check if entry has expired using time.Now()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: If expired, remove entry and return nil, false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: If valid, move element to front of c.order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: Return entry.Value, true</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Use c.order.MoveToFront() to update LRU order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Call c.removeElement() helper for cleanup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Set stores value with optional TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tc.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> c.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Calculate entry size: len(key) + len(value) + constant overhead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Create CacheEntry with key, value, size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Set ExpiresAt: time.Now().Add(ttl) if ttl > 0, else zero time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: If key exists, update existing entry and move to front</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: If new key, create list element and add to front</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Update c.items map and c.used memory counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 7: While c.used > c.capacity, evict from back of list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Use c.order.PushFront() for new entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Call c.evictLRU() helper until under capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Delete removes key from cache</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tc.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> c.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Look up key in c.items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: If not found, return false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: Remove element from c.order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: Delete key from c.items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Update c.used counter by subtracting entry size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Return true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupExpired removes expired entries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CleanupExpired</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\tc.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">\tdefer</span><span style=\"color:#E1E4E8\"> c.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">\t</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 1: Initialize counter for removed entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 2: Walk through c.items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 3: For each entry, check if ExpiresAt &#x3C; time.Now()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 4: If expired, remove from both c.items and c.order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 5: Update c.used counter and increment removal counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// TODO 6: Return total number of entries removed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">\t// Hint: Collect keys to remove first, then delete to avoid map iteration issues</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"development-checkpoints\">Development Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint - Hash Ring Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run hash ring tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/hashring/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Keys distribute evenly across nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Virtual nodes improve distribution balance  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Node addition/removal minimally affects existing keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Key lookup performs in O(log n) time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification:</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/test-hashring/main.go</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should show hash ring with virtual nodes and key distribution</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint - Cache Node:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run cache tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/cache/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behaviors:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - LRU eviction removes least recently used entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - TTL cleanup removes expired entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Memory limits enforced through eviction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Concurrent access handled safely</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Load test:</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/cache-benchmark/main.go</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should handle 10K+ ops/sec without data corruption</span></span></code></pre></div>\n\n<p><strong>Language-Specific Implementation Hints:</strong></p>\n<ul>\n<li><strong>Hashing</strong>: Use <code>crypto/sha256</code> for consistent results across nodes. Convert hash to <code>uint32</code> by taking first 4 bytes for ring positioning.</li>\n<li><strong>Concurrency</strong>: Use <code>sync.RWMutex</code> for read-heavy workloads like hash ring lookups. Use <code>sync.Mutex</code> for write-heavy operations like LRU updates.</li>\n<li><strong>Memory Accounting</strong>: Calculate entry size as <code>len(key) + len(value) + 64</code> (64 bytes overhead for metadata and pointers).</li>\n<li><strong>HTTP Timeouts</strong>: Set both client timeout and server timeout. Use <code>context.WithTimeout()</code> for request-level timeouts.</li>\n<li><strong>JSON Serialization</strong>: Use <code>json:&quot;,omitempty&quot;</code> tags to reduce message size. Consider <code>encoding/gob</code> for better performance in internal communication.</li>\n<li><strong>Testing</strong>: Use <code>go test -race</code> to detect race conditions. Use <code>testing.Short()</code> to skip slow tests during development.</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides the data structure foundation for all milestones, with particular relevance to Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication &amp; Consistency).</p>\n</blockquote>\n<h3 id=\"mental-model-the-digital-filing-system\">Mental Model: The Digital Filing System</h3>\n<p>Think of our distributed cache as a sophisticated digital filing system for a large organization with multiple offices. Each piece of information (cache entry) is like a document that needs to be filed, labeled, and easily retrievable. The filing system has several key components:</p>\n<ul>\n<li><strong>Document Labels and Metadata</strong>: Every document has a label (key), contents (value), expiration date (TTL), and storage requirements (size)</li>\n<li><strong>Office Directory</strong>: A master list showing which offices exist, their addresses, and current status</li>\n<li><strong>Filing Rules</strong>: Instructions for which office should store each document based on the label</li>\n<li><strong>Office Coordination</strong>: Each office maintains its own filing cabinet but coordinates with others to ensure documents are properly distributed and backed up</li>\n</ul>\n<p>Just as a physical filing system needs consistent rules for where documents go and how they&#39;re organized, our distributed cache needs well-defined data structures to represent cache entries, node information, and cluster state. These structures form the foundation that all other components build upon.</p>\n<h3 id=\"core-data-types\">Core Data Types</h3>\n<p>The distributed cache system relies on several fundamental data structures that work together to provide distributed storage and retrieval capabilities. Each structure serves a specific purpose and contains carefully chosen fields to support the system&#39;s functionality.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fdata-model-relationships.svg\" alt=\"Data Model and Type Relationships\"></p>\n<p>The core data types form a hierarchy of responsibility, from individual cache entries up to cluster-wide coordination structures. Understanding these relationships is crucial for implementing the distributed cache correctly.</p>\n<h4 id=\"configuration-and-node-identity\">Configuration and Node Identity</h4>\n<p>The <code>NodeConfig</code> structure contains all configuration parameters needed to initialize and run a cache node. This includes network settings, performance parameters, and operational timeouts that control the node&#39;s behavior within the cluster.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NodeID</td>\n<td>string</td>\n<td>Unique identifier for this node in the cluster, typically a UUID or hostname</td>\n</tr>\n<tr>\n<td>ListenAddress</td>\n<td>string</td>\n<td>Address and port where this node accepts incoming requests (e.g., &quot;:8080&quot;)</td>\n</tr>\n<tr>\n<td>AdvertiseAddr</td>\n<td>string</td>\n<td>Address that other nodes should use to contact this node (for NAT/proxy scenarios)</td>\n</tr>\n<tr>\n<td>JoinAddresses</td>\n<td>[]string</td>\n<td>List of existing cluster members to contact during bootstrap</td>\n</tr>\n<tr>\n<td>MaxMemoryMB</td>\n<td>int</td>\n<td>Maximum memory in megabytes this node can use for cache storage</td>\n</tr>\n<tr>\n<td>VirtualNodes</td>\n<td>int</td>\n<td>Number of virtual node positions to create on the hash ring</td>\n</tr>\n<tr>\n<td>ReplicationFactor</td>\n<td>int</td>\n<td>Number of replica copies to maintain for each cache entry</td>\n</tr>\n<tr>\n<td>HealthCheckInterval</td>\n<td>time.Duration</td>\n<td>How frequently to probe other nodes for health status</td>\n</tr>\n<tr>\n<td>GossipInterval</td>\n<td>time.Duration</td>\n<td>How often to exchange cluster state information with peers</td>\n</tr>\n<tr>\n<td>RequestTimeout</td>\n<td>time.Duration</td>\n<td>Maximum time to wait for responses from remote nodes</td>\n</tr>\n</tbody></table>\n<p>The configuration design separates concerns between network identity, resource limits, and operational parameters. The <code>ListenAddress</code> and <code>AdvertiseAddr</code> distinction allows nodes to operate correctly behind load balancers or NAT devices, where the internal listening address differs from the external address other nodes should use.</p>\n<h4 id=\"hash-ring-structure\">Hash Ring Structure</h4>\n<p>The <code>HashRing</code> structure implements consistent hashing for distributing keys across cluster nodes. It maintains the circular hash space and provides efficient lookups to determine which node should handle each key.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>virtualNodes</td>\n<td>int</td>\n<td>Number of virtual positions each physical node occupies on the ring</td>\n</tr>\n<tr>\n<td>ring</td>\n<td>map[uint32]string</td>\n<td>Maps hash positions to node IDs for key-to-node lookups</td>\n</tr>\n<tr>\n<td>sortedKeys</td>\n<td>[]uint32</td>\n<td>Sorted array of all hash positions for binary search during lookups</td>\n</tr>\n<tr>\n<td>nodes</td>\n<td>map[string]bool</td>\n<td>Set of active nodes currently participating in the ring</td>\n</tr>\n</tbody></table>\n<p>The hash ring uses virtual nodes to improve load distribution. Instead of placing each physical node at a single position on the ring, each node occupies multiple virtual positions. This reduces the probability that a single node becomes a hotspot for popular keys and ensures more even distribution when nodes join or leave the cluster.</p>\n<p>The <code>sortedKeys</code> array enables efficient O(log n) lookups using binary search to find the first hash position greater than or equal to a key&#39;s hash value. This position determines which node is responsible for storing that key.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Virtual nodes are critical for load balancing. With only physical nodes on the ring, key distribution can become severely skewed, especially in small clusters. By using 150-500 virtual nodes per physical node, the system achieves much more uniform key distribution and reduces the impact of hot spots.</p>\n</blockquote>\n<h4 id=\"cache-storage-structure\">Cache Storage Structure</h4>\n<p>The <code>LRUCache</code> structure implements the in-memory storage for each node, providing least-recently-used eviction when memory limits are reached. This structure must handle concurrent access from multiple goroutines while maintaining correct LRU ordering.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>mutex</td>\n<td>sync.RWMutex</td>\n<td>Read-write mutex protecting concurrent access to cache state</td>\n</tr>\n<tr>\n<td>capacity</td>\n<td>int64</td>\n<td>Maximum memory in bytes this cache can consume</td>\n</tr>\n<tr>\n<td>used</td>\n<td>int64</td>\n<td>Current memory consumption in bytes across all stored entries</td>\n</tr>\n<tr>\n<td>items</td>\n<td>map[string]*list.Element</td>\n<td>Hash table mapping keys to doubly-linked list elements</td>\n</tr>\n<tr>\n<td>order</td>\n<td>*list.List</td>\n<td>Doubly-linked list maintaining LRU order (most recent at front)</td>\n</tr>\n</tbody></table>\n<p>The LRU cache combines a hash table for O(1) key lookups with a doubly-linked list for O(1) LRU order maintenance. When an item is accessed, it moves to the front of the list. When eviction is needed, items are removed from the back of the list (least recently used).</p>\n<p>The <code>sync.RWMutex</code> allows multiple concurrent readers while ensuring exclusive access for writers. This improves performance for read-heavy workloads, which are common in caching scenarios.</p>\n<h3 id=\"cache-entry-structure\">Cache Entry Structure</h3>\n<p>Each piece of data stored in the distributed cache is represented by a <code>CacheEntry</code> structure. This structure contains not only the key-value pair but also metadata necessary for TTL expiration, memory management, and replication.</p>\n<h4 id=\"entry-data-and-metadata\">Entry Data and Metadata</h4>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key</td>\n<td>string</td>\n<td>The cache key used to identify and retrieve this entry</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>[]byte</td>\n<td>The cached data as a byte array, allowing storage of any serializable data</td>\n</tr>\n<tr>\n<td>ExpiresAt</td>\n<td>time.Time</td>\n<td>Absolute timestamp when this entry should be considered expired</td>\n</tr>\n<tr>\n<td>Size</td>\n<td>int64</td>\n<td>Total memory footprint of this entry in bytes (key + value + metadata)</td>\n</tr>\n</tbody></table>\n<p>The <code>CacheEntry</code> design prioritizes simplicity and efficiency. Keys are strings to provide human-readable identifiers, while values are byte arrays to support any data type that can be serialized. The size field includes the complete memory footprint, not just the value size, enabling accurate memory accounting.</p>\n<h4 id=\"ttl-and-expiration-handling\">TTL and Expiration Handling</h4>\n<p>The <code>ExpiresAt</code> field uses absolute timestamps rather than relative TTL values to avoid clock drift issues and simplify expiration checks. When a TTL is specified during a SET operation, it&#39;s converted to an absolute expiration time based on the local node&#39;s clock.</p>\n<p>A zero value for <code>ExpiresAt</code> (time.Time{}) indicates that the entry never expires. This allows the cache to support both TTL-based and permanent entries using the same data structure.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Using absolute timestamps for expiration avoids the complexity of tracking relative TTL values and updating them as time passes. However, it requires nodes to have reasonably synchronized clocks, which is generally acceptable in data center environments.</p>\n</blockquote>\n<h4 id=\"memory-size-calculation\">Memory Size Calculation</h4>\n<p>The <code>Size</code> field represents the complete memory footprint of the cache entry, including:</p>\n<ol>\n<li>Key string memory (length in bytes plus string header overhead)</li>\n<li>Value byte array memory (slice length plus slice header overhead)</li>\n<li>Metadata structure memory (timestamps, integers, pointers)</li>\n<li>Any alignment padding required by the Go runtime</li>\n</ol>\n<p>Accurate size calculation is essential for enforcing memory limits and making informed eviction decisions. The size is calculated once when the entry is created and stored with the entry to avoid repeated calculations during memory pressure scenarios.</p>\n<h3 id=\"cluster-state-information\">Cluster State Information</h3>\n<p>The distributed cache maintains comprehensive information about cluster membership, node health, and the current state of the hash ring. This information must be kept consistent across all nodes to ensure correct request routing and data placement.</p>\n<h4 id=\"node-state-tracking\">Node State Tracking</h4>\n<p>The <code>NodeState</code> structure represents the current status and metadata for a single node in the cluster. This information is exchanged through gossip protocol messages to maintain eventual consistency of cluster membership.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NodeID</td>\n<td>string</td>\n<td>Unique identifier matching the node&#39;s configured NodeID</td>\n</tr>\n<tr>\n<td>Address</td>\n<td>string</td>\n<td>Network address where the node can be reached for cache requests</td>\n</tr>\n<tr>\n<td>Status</td>\n<td>string</td>\n<td>Current node status: &quot;joining&quot;, &quot;active&quot;, &quot;suspected&quot;, &quot;failed&quot;, or &quot;leaving&quot;</td>\n</tr>\n<tr>\n<td>LastSeen</td>\n<td>time.Time</td>\n<td>Timestamp when this node was last observed to be responsive</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint64</td>\n<td>Monotonic version number incremented when node state changes</td>\n</tr>\n</tbody></table>\n<p>Node states follow a specific lifecycle that helps the cluster distinguish between temporary network issues and permanent failures. The &quot;suspected&quot; state allows the system to continue operating while investigating potential failures, rather than immediately marking nodes as failed due to transient network problems.</p>\n<p>The version number enables conflict resolution when different nodes have conflicting information about the same node. Higher version numbers always take precedence, ensuring that the most recent state information propagates through the cluster.</p>\n<h4 id=\"node-status-state-machine\">Node Status State Machine</h4>\n<p>The node status field follows a well-defined state machine that governs how nodes transition through their lifecycle:</p>\n<table>\n<thead>\n<tr>\n<th>Current Status</th>\n<th>Event</th>\n<th>Next Status</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-</td>\n<td>Node starts and contacts cluster</td>\n<td>joining</td>\n<td>Node is attempting to join the cluster</td>\n</tr>\n<tr>\n<td>joining</td>\n<td>Successfully joined cluster</td>\n<td>active</td>\n<td>Node is fully operational and serving requests</td>\n</tr>\n<tr>\n<td>active</td>\n<td>Health check timeout</td>\n<td>suspected</td>\n<td>Node may have failed but is being investigated</td>\n</tr>\n<tr>\n<td>suspected</td>\n<td>Health check succeeds</td>\n<td>active</td>\n<td>Node has recovered from temporary issue</td>\n</tr>\n<tr>\n<td>suspected</td>\n<td>Failure confirmation timeout</td>\n<td>failed</td>\n<td>Node is confirmed failed and should be removed</td>\n</tr>\n<tr>\n<td>active</td>\n<td>Graceful shutdown initiated</td>\n<td>leaving</td>\n<td>Node is cleanly exiting the cluster</td>\n</tr>\n<tr>\n<td>leaving</td>\n<td>Shutdown complete</td>\n<td>-</td>\n<td>Node is removed from cluster state</td>\n</tr>\n</tbody></table>\n<p>This state machine prevents flapping between active and failed states when nodes experience intermittent connectivity issues. The &quot;suspected&quot; state provides a grace period for recovery while allowing the cluster to continue operating.</p>\n<h4 id=\"cluster-membership-and-gossip\">Cluster Membership and Gossip</h4>\n<p>The <code>GossipMessage</code> structure carries cluster state information between nodes. Each gossip message contains a complete snapshot of the sender&#39;s view of cluster membership, allowing recipients to update their local state and detect inconsistencies.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NodeStates</td>\n<td>map[string]NodeState</td>\n<td>Complete mapping of node IDs to their current state information</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint64</td>\n<td>Overall cluster state version, incremented when any node state changes</td>\n</tr>\n</tbody></table>\n<p>Gossip messages are exchanged periodically between randomly selected pairs of nodes. When a node receives a gossip message, it compares the contained node states with its local view and adopts any information with higher version numbers.</p>\n<p>The gossip protocol ensures that cluster state changes eventually propagate to all nodes, even in the presence of network partitions or node failures. The probabilistic nature of gossip communication provides robustness against individual message failures while maintaining efficiency.</p>\n<h4 id=\"message-transport-and-communication\">Message Transport and Communication</h4>\n<p>All inter-node communication uses a standardized message format that supports multiple operation types and includes metadata for routing and debugging.</p>\n<p>The <code>Message</code> structure provides a common envelope for all inter-node communications:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Type</td>\n<td>string</td>\n<td>Message type identifier (gossip, get, set, delete, replicate)</td>\n</tr>\n<tr>\n<td>Sender</td>\n<td>string</td>\n<td>Node ID of the message originator for routing responses</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When the message was created, useful for timeout and debugging</td>\n</tr>\n<tr>\n<td>Data</td>\n<td>interface{}</td>\n<td>Message payload, type-specific to the message type</td>\n</tr>\n</tbody></table>\n<p>The generic <code>Data</code> field allows the same message structure to carry different payload types without requiring separate message formats for each operation. The actual payload type is determined by the <code>Type</code> field and decoded accordingly.</p>\n<h4 id=\"cache-operation-messages\">Cache Operation Messages</h4>\n<p>Specific message types handle cache operations between nodes. These structures define the format for requests and responses when cache operations need to be forwarded to the appropriate nodes or replicated to backup nodes.</p>\n<p><strong>Get Operation Messages:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>GetRequest</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Key</td>\n<td>string</td>\n<td>Cache key to retrieve</td>\n</tr>\n<tr>\n<td>ConsistencyLevel</td>\n<td>string</td>\n<td>Required consistency level (eventual, consistent, strong)</td>\n</tr>\n<tr>\n<td><strong>GetResponse</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Key</td>\n<td>string</td>\n<td>Echo of the requested key for correlation</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>[]byte</td>\n<td>Retrieved value, nil if not found</td>\n</tr>\n<tr>\n<td>Found</td>\n<td>bool</td>\n<td>Whether the key exists in the cache</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When the value was last modified, used for consistency</td>\n</tr>\n</tbody></table>\n<p><strong>Set Operation Messages:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>SetRequest</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Key</td>\n<td>string</td>\n<td>Cache key to store</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>[]byte</td>\n<td>Data to store in the cache</td>\n</tr>\n<tr>\n<td>TTL</td>\n<td>time.Duration</td>\n<td>Time-to-live for this entry, zero for no expiration</td>\n</tr>\n<tr>\n<td>ConsistencyLevel</td>\n<td>string</td>\n<td>Required consistency level for this write operation</td>\n</tr>\n<tr>\n<td><strong>SetResponse</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Success</td>\n<td>bool</td>\n<td>Whether the operation completed successfully</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When the value was stored, used for conflict resolution</td>\n</tr>\n</tbody></table>\n<p><strong>Delete Operation Messages:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>DeleteRequest</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Key</td>\n<td>string</td>\n<td>Cache key to remove</td>\n</tr>\n<tr>\n<td>ConsistencyLevel</td>\n<td>string</td>\n<td>Required consistency level for this delete operation</td>\n</tr>\n<tr>\n<td><strong>DeleteResponse</strong></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>Success</td>\n<td>bool</td>\n<td>Whether the delete operation completed successfully</td>\n</tr>\n<tr>\n<td>Found</td>\n<td>bool</td>\n<td>Whether the key existed before deletion</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When the deletion was processed</td>\n</tr>\n</tbody></table>\n<p>These message structures support different consistency levels, allowing applications to choose between performance and consistency based on their requirements. The timestamp fields enable conflict resolution when multiple nodes have different versions of the same data.</p>\n<blockquote>\n<p><strong>Architecture Decision: Message Format Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Inter-node communication requires standardized message formats that can handle different operation types while remaining extensible for future features.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Operation-specific message formats with no common structure</li>\n<li>Single message type with operation-specific fields (many optional)</li>\n<li>Common message envelope with operation-specific payloads</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Common message envelope with type-specific payloads</li>\n<li><strong>Rationale</strong>: This approach provides consistency across all operations while avoiding the complexity of a single message with many optional fields. The generic payload field allows strong typing for each operation while maintaining a common transport layer.</li>\n<li><strong>Consequences</strong>: Enables consistent message handling, routing, and debugging while supporting future message types without breaking changes to the transport layer.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>When implementing these data structures, several common mistakes can significantly impact system performance and correctness:</p>\n<p>⚠️ <strong>Pitfall: Insufficient Memory Size Calculation</strong>\nMany developers calculate only the value size when implementing the <code>Size</code> field in <code>CacheEntry</code>, ignoring the key string memory and metadata overhead. This leads to memory limit enforcement that allows 20-30% more memory usage than configured, potentially causing out-of-memory errors. Always calculate the complete memory footprint including string headers, slice headers, and struct overhead.</p>\n<p>⚠️ <strong>Pitfall: Using Relative TTL Instead of Absolute Expiration</strong>\nStoring relative TTL values (like &quot;expires in 5 minutes&quot;) instead of absolute timestamps creates complexity when checking expiration and can lead to incorrect behavior if system time changes. The <code>ExpiresAt</code> field should always contain an absolute timestamp calculated when the entry is created, making expiration checks simple and reliable.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Virtual Node Distribution</strong>\nUsing too few virtual nodes (less than 50 per physical node) leads to poor key distribution and hotspots, especially in small clusters. Using too many virtual nodes (more than 1000) increases memory overhead and hash ring operation costs without significant benefit. The recommended range of 150-500 virtual nodes provides good load balancing without excessive overhead.</p>\n<p>⚠️ <strong>Pitfall: Race Conditions in LRU Cache State</strong>\nForgetting to protect all cache state modifications with the mutex leads to race conditions that can corrupt the LRU order or cause panics when concurrent operations modify the linked list. Every operation that changes the <code>items</code> map, <code>order</code> list, or <code>used</code> counter must hold appropriate locks.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Clock Skew in Distributed Operations</strong>\nUsing local timestamps for distributed operations without considering clock skew between nodes can cause incorrect conflict resolution and consistency issues. While the <code>Timestamp</code> fields in messages use local node time, conflict resolution algorithms must account for reasonable clock differences between nodes.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the data model structures in Go, including complete working code for infrastructure components and skeleton code for core logic that learners should implement themselves.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Serialization</td>\n<td>JSON with encoding/json</td>\n<td>Protocol Buffers with protobuf</td>\n</tr>\n<tr>\n<td>Time Handling</td>\n<td>time.Time with time.Now()</td>\n<td>NTP-synchronized time with clock skew detection</td>\n</tr>\n<tr>\n<td>Memory Tracking</td>\n<td>Manual calculation with unsafe.Sizeof</td>\n<td>Memory profiling with runtime.MemStats</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>sync.RWMutex</td>\n<td>Lock-free data structures with atomic operations</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The data model components should be organized to separate concerns and enable easy testing:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  models/\n    cache_entry.go           ← CacheEntry and related utilities\n    cache_entry_test.go      ← Unit tests for cache entry operations\n    node_config.go           ← NodeConfig loading and validation\n    node_config_test.go      ← Configuration validation tests\n    messages.go              ← All message type definitions\n    cluster_state.go         ← NodeState and cluster membership types\n  ring/\n    hash_ring.go             ← HashRing implementation\n    hash_ring_test.go        ← Hash ring algorithm tests\n  cache/\n    lru_cache.go             ← LRUCache implementation\n    lru_cache_test.go        ← Cache behavior tests</code></pre></div>\n\n<p>This structure separates data definitions from behavioral logic, making it easier to modify structures without affecting algorithms and enabling comprehensive unit testing of each component.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Configuration Loading (complete implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads and validates configuration from a JSON file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config JSON: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid configuration: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks that all configuration values are reasonable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.NodeID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"NodeID cannot be empty\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ListenAddress </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ListenAddress cannot be empty\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.MaxMemoryMB </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"MaxMemoryMB must be positive, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.MaxMemoryMB)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"VirtualNodes should be 50-1000, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.VirtualNodes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ReplicationFactor </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ReplicationFactor must be positive, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.ReplicationFactor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Memory Size Calculation Utilities (complete implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">unsafe</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CalculateEntrySize computes the total memory footprint of a cache entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CalculateEntrySize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // String header + string data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keySize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(unsafe.</span><span style=\"color:#B392F0\">Sizeof</span><span style=\"color:#E1E4E8\">(key)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(key))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Slice header + slice data  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    valueSize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(unsafe.</span><span style=\"color:#B392F0\">Sizeof</span><span style=\"color:#E1E4E8\">(value)) </span><span style=\"color:#F97583\">+</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(value))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // CacheEntry struct overhead</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    structSize </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(unsafe.</span><span style=\"color:#B392F0\">Sizeof</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">{}))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> keySize </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> valueSize </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> structSize</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateCacheEntry creates a new cache entry with proper size calculation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CreateCacheEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> expiresAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ttl </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expiresAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(ttl)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Key:       key,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Value:     value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ExpiresAt: expiresAt,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Size:      </span><span style=\"color:#B392F0\">CalculateEntrySize</span><span style=\"color:#E1E4E8\">(key, value),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsExpired checks if a cache entry has exceeded its TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IsExpired</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> e.ExpiresAt.</span><span style=\"color:#B392F0\">IsZero</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#6A737D\"> // Never expires</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(e.ExpiresAt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Hash Ring Implementation (skeleton with TODOs):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha1</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHashRing creates a new consistent hash ring with the specified virtual nodes per physical node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">virtualNodes</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        virtualNodes: virtualNodes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ring:         </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sortedKeys:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodes:        </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddNode adds a physical node to the hash ring with its virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if node already exists in hr.nodes, return early if so</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add nodeID to hr.nodes map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For i := 0; i &#x3C; hr.virtualNodes; i++, create virtual node positions:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Generate virtual node key as nodeID + \":\" + i (convert i to string)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Hash the virtual node key using hashKey() helper</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add hash position to hr.ring mapping to nodeID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add hash position to hr.sortedKeys slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Sort hr.sortedKeys to maintain ordering for binary search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sort.Slice(hr.sortedKeys, func(i, j int) bool { ... })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RemoveNode removes a physical node and all its virtual nodes from the ring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RemoveNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if node exists in hr.nodes, return early if not</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Remove nodeID from hr.nodes map  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For i := 0; i &#x3C; hr.virtualNodes; i++, remove virtual node positions:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Generate same virtual node key as in AddNode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Hash the virtual node key using hashKey() helper</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Remove hash position from hr.ring map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Find and remove hash position from hr.sortedKeys slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Re-sort hr.sortedKeys after removals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: You'll need to rebuild hr.sortedKeys or carefully remove elements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNode returns the node responsible for storing the given key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hr.sortedKeys) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Hash the key using hashKey() helper</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Use sort.Search to find first position >= key hash in hr.sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If search result >= len(hr.sortedKeys), wrap around to position 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Get the ring position at the found index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return the nodeID from hr.ring[position]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: sort.Search(len(hr.sortedKeys), func(i int) bool { return hr.sortedKeys[i] >= keyHash })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// hashKey generates a hash value for a string key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> hashKey</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hash </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha1.</span><span style=\"color:#B392F0\">Sum</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(key))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(hash[:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>LRU Cache Implementation (skeleton with TODOs):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cache</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">container/list</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLRUCache creates an LRU cache with the specified memory capacity in bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLRUCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacityBytes</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        capacity: capacityBytes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        used:     </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        items:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        order:    list.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Get retrieves a value from the cache and updates its LRU position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lru </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lru.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up key in lru.items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If not found, release read lock and return nil, false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Get the CacheEntry from element.Value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check if entry is expired using IsExpired() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If expired, release read lock, acquire write lock, remove entry, return nil, false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If valid, release read lock, acquire write lock, move element to front of lru.order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Release write lock and return value copy, true</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use lru.order.MoveToFront(element) to update LRU position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Make a copy of the value slice before returning to avoid races</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Set stores a value in the cache with optional TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lru </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lru.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> lru.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create new CacheEntry using CreateCacheEntry helper</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if key already exists in lru.items</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If exists, subtract old entry size from lru.used, remove from lru.order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check if new entry size would exceed capacity (lru.used + entry.Size > lru.capacity)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: While over capacity, evict LRU entries using evictLRU() helper</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Add new entry to front of lru.order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Store list element in lru.items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Add entry size to lru.used counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: list.Element.Value should be *CacheEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evictLRU removes the least recently used entry (helper method)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lru </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evictLRU</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get the back element from lru.order (least recently used)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If list is empty, return false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Get CacheEntry from element.Value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove element from lru.order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Remove entry from lru.items map using entry.Key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Subtract entry.Size from lru.used</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Go-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>crypto/sha1</code> for consistent hashing as it provides good distribution and acceptable performance</li>\n<li>The <code>unsafe.Sizeof()</code> function returns the size of the type structure, not the actual data (for slices/strings)</li>\n<li>Use <code>sort.Search()</code> for efficient binary search in the sorted hash ring positions</li>\n<li><code>container/list</code> provides an efficient doubly-linked list implementation for LRU ordering</li>\n<li><code>sync.RWMutex</code> allows multiple concurrent readers, improving performance for read-heavy workloads</li>\n<li>Always make copies of byte slices when returning from cache to prevent races with the underlying storage</li>\n</ul>\n<p><strong>Memory Management Best Practices:</strong></p>\n<ul>\n<li>Include string header size (typically 16 bytes on 64-bit systems) in addition to string data</li>\n<li>Include slice header size (typically 24 bytes on 64-bit systems) in addition to slice data</li>\n<li>Account for struct padding and alignment when calculating sizes</li>\n<li>Use <code>runtime.GC()</code> and <code>runtime.ReadMemStats()</code> for debugging memory accounting issues</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the data model structures, verify correct behavior with these checks:</p>\n<p><strong>Unit Test Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/models/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/ring/...</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/cache/...</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected Test Behavior:</strong></p>\n<ul>\n<li>Configuration loading should accept valid JSON and reject invalid configurations</li>\n<li>Hash ring should distribute keys evenly across virtual nodes and handle node additions/removals</li>\n<li>LRU cache should correctly enforce memory limits and maintain LRU ordering under concurrent access</li>\n<li>Memory size calculations should account for complete entry footprint</li>\n</ul>\n<p><strong>Manual Verification:</strong></p>\n<ol>\n<li>Create a hash ring with 3 nodes and 100 virtual nodes each</li>\n<li>Add 1000 random keys and verify distribution is reasonably even (no node has &gt;40% of keys)</li>\n<li>Remove one node and verify that only ~33% of keys are remapped to different nodes</li>\n<li>Create an LRU cache with 1MB capacity and fill it with entries until eviction occurs</li>\n<li>Verify that the least recently used entries are evicted first</li>\n</ol>\n<p><strong>Warning Signs:</strong></p>\n<ul>\n<li>Severe key distribution imbalance (one node getting &gt;50% of keys)</li>\n<li>Memory usage significantly exceeding configured limits  </li>\n<li>LRU cache returning expired entries or incorrect eviction order</li>\n<li>Panic or race condition errors under concurrent access</li>\n</ul>\n<h2 id=\"consistent-hash-ring-design\">Consistent Hash Ring Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section covers Milestone 1 (Consistent Hash Ring), providing the foundation for key distribution that enables Milestone 3 (Cluster Communication) and Milestone 4 (Replication &amp; Consistency).</p>\n</blockquote>\n<p>The consistent hash ring forms the backbone of our distributed cache, solving the fundamental challenge of distributing keys across nodes while minimizing data movement when cluster membership changes. This component determines which node stores each cache entry and ensures that adding or removing nodes only affects a small fraction of existing keys.</p>\n<h3 id=\"mental-model-the-circular-table\">Mental Model: The Circular Table</h3>\n<p>Think of consistent hashing as organizing a massive dinner party with a circular table that can magically expand and contract. Instead of assigning guests to numbered seats (which would require reshuffling everyone when the table size changes), we use a different approach.</p>\n<p>Imagine the circular table has positions marked by compass-like coordinates from 0 to 360 degrees. Each guest (cache key) gets assigned a coordinate based on their name&#39;s hash value. Similarly, each waiter (cache node) is assigned multiple positions around the table. When a guest arrives, they sit at the first waiter position they encounter when walking clockwise around the table.</p>\n<p>The magic happens when we add or remove waiters. If we remove waiter Alice (who was stationed at positions 45°, 135°, and 225°), only the guests between the previous waiter and Alice&#39;s positions need to move to the next waiter clockwise. All other guests stay exactly where they are. Similarly, adding a new waiter only affects guests in specific sections of the table.</p>\n<p>This circular table model captures the essence of consistent hashing: locality of change. Unlike traditional modulo hashing (where changing from 3 to 4 servers would reassign roughly 75% of keys), consistent hashing ensures that membership changes only affect keys in the immediate vicinity of the change on the ring.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fconsistent-hash-ring.svg\" alt=\"Consistent Hash Ring Structure\"></p>\n<h3 id=\"virtual-nodes-strategy\">Virtual Nodes Strategy</h3>\n<p>The naive approach of placing each physical node at a single position on the hash ring creates significant problems. Consider a ring with three nodes positioned at hash values 100, 200, and 300. Node A (at position 100) would be responsible for the range from 301 to 100, which covers 160 degrees of the ring. Node B would cover 100 degrees, and Node C would cover only 100 degrees. This uneven distribution means Node A handles 60% more load than the other nodes.</p>\n<p>Virtual nodes solve this distribution problem by giving each physical node multiple positions on the ring. Instead of placing Node A at position 100, we might place it at positions 47, 156, 289, 334, and 401 (mod 360). Each physical node typically manages between 50 and 500 virtual nodes, creating a much more uniform distribution of key ranges.</p>\n<blockquote>\n<p><strong>The fundamental insight is that virtual nodes act like statistical sampling. With only 3 physical nodes, we have 3 data points to distribute load. With 150 virtual nodes (50 per physical node), we have 150 data points, making the distribution much more likely to approach the theoretical ideal.</strong></p>\n</blockquote>\n<p>The virtual nodes strategy provides several critical benefits beyond load balancing. When a physical node fails, its load gets redistributed among all remaining nodes rather than dumping everything on a single successor. If Node A fails and its virtual nodes were interleaved with those of nodes B and C, then approximately half of A&#39;s keys go to B and half go to C, preventing hotspots during failure scenarios.</p>\n<p>Virtual nodes also enable heterogeneous clusters where nodes have different capacities. A powerful node with 32GB of RAM might host 200 virtual nodes, while a smaller node with 8GB hosts only 50 virtual nodes. The ring naturally distributes proportionally more keys to the higher-capacity nodes.</p>\n<table>\n<thead>\n<tr>\n<th>Virtual Node Count</th>\n<th>Distribution Quality</th>\n<th>Memory Overhead</th>\n<th>Rebalance Time</th>\n<th>Recommended Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1-10 per node</td>\n<td>Poor, many hotspots</td>\n<td>Minimal</td>\n<td>Fast</td>\n<td>Never recommended</td>\n</tr>\n<tr>\n<td>50-100 per node</td>\n<td>Good for small clusters</td>\n<td>Low</td>\n<td>Fast</td>\n<td>Development/testing</td>\n</tr>\n<tr>\n<td>100-200 per node</td>\n<td>Excellent</td>\n<td>Moderate</td>\n<td>Moderate</td>\n<td>Production clusters</td>\n</tr>\n<tr>\n<td>500+ per node</td>\n<td>Diminishing returns</td>\n<td>High</td>\n<td>Slow</td>\n<td>Very large clusters only</td>\n</tr>\n</tbody></table>\n<h3 id=\"ring-operations-and-algorithms\">Ring Operations and Algorithms</h3>\n<p>The hash ring implementation centers around three core operations: key lookup, node addition, and node removal. Each operation must maintain the ring&#39;s consistency while providing efficient access patterns for cache operations.</p>\n<h4 id=\"key-lookup-algorithm\">Key Lookup Algorithm</h4>\n<p>The key lookup operation determines which node should handle a given cache key. This operation executes for every cache GET, SET, and DELETE request, making its performance critical to overall system throughput.</p>\n<ol>\n<li><p><strong>Hash the key</strong>: Apply the chosen hash function (typically SHA-1 or MD5) to the cache key, producing a fixed-size hash value. Convert this hash to an unsigned 32-bit integer for ring positioning.</p>\n</li>\n<li><p><strong>Find the position</strong>: The hash value represents the key&#39;s position on the ring. This position falls somewhere between two virtual nodes.</p>\n</li>\n<li><p><strong>Locate successor</strong>: Search the sorted list of virtual node positions to find the first virtual node whose position is greater than or equal to the key&#39;s hash value. This virtual node is the key&#39;s &quot;successor&quot; on the ring.</p>\n</li>\n<li><p><strong>Handle wraparound</strong>: If no virtual node position is greater than the key&#39;s hash (meaning the key hashes to a position after the highest virtual node), wrap around to the first virtual node position. This maintains the ring&#39;s circular property.</p>\n</li>\n<li><p><strong>Return physical node</strong>: Map the responsible virtual node back to its corresponding physical node identifier. This physical node will handle the cache operation for this key.</p>\n</li>\n</ol>\n<p>The lookup operation&#39;s time complexity depends entirely on the search algorithm for step 3. A linear scan through virtual node positions would require O(V) time where V is the total number of virtual nodes across all physical nodes. However, since virtual node positions are stored in sorted order, binary search reduces this to O(log V), making lookups efficient even with hundreds of virtual nodes per physical node.</p>\n<h4 id=\"node-addition-algorithm\">Node Addition Algorithm</h4>\n<p>Adding a new node to the cluster requires careful coordination to maintain data consistency while minimizing service disruption. The addition process involves ring updates, data migration, and membership synchronization.</p>\n<ol>\n<li><p><strong>Generate virtual node positions</strong>: Create the specified number of virtual nodes for the new physical node. Hash the combination of node identifier and virtual node index (e.g., hash(&quot;node-5:vnode-23&quot;)) to determine each virtual node&#39;s ring position.</p>\n</li>\n<li><p><strong>Insert into ring structure</strong>: Add each virtual node position to the sorted position list, maintaining sort order. Update the position-to-node mapping to include the new virtual nodes.</p>\n</li>\n<li><p><strong>Identify affected ranges</strong>: For each virtual node being added, determine the key range it will now own. This range extends from the previous virtual node position (clockwise) up to the new virtual node&#39;s position.</p>\n</li>\n<li><p><strong>Update cluster membership</strong>: Broadcast the membership change to all existing nodes through the gossip protocol, ensuring every node learns about the new member and updates its local ring representation.</p>\n</li>\n<li><p><strong>Migrate affected keys</strong>: The previous owner of each affected key range must transfer the relevant cache entries to the new node. This migration happens gradually to avoid overwhelming the new node or blocking cache operations.</p>\n</li>\n<li><p><strong>Verify migration completion</strong>: Confirm that all affected keys have been successfully transferred and that the new node is responding to cache operations for its assigned ranges.</p>\n</li>\n</ol>\n<p>The addition process requires careful ordering to prevent data loss. The new node must be added to the ring structure and membership before migration begins, ensuring that cache operations for affected keys get routed correctly during the transition period.</p>\n<h4 id=\"node-removal-algorithm\">Node Removal Algorithm</h4>\n<p>Removing a node (whether due to planned maintenance or failure detection) requires redistributing its cache entries while maintaining availability for active keys. The removal process differs significantly depending on whether it&#39;s a graceful shutdown or failure recovery.</p>\n<ol>\n<li><p><strong>Mark node as leaving</strong>: Update the node&#39;s status in the cluster membership to indicate it&#39;s leaving the cluster. This prevents new keys from being assigned to the departing node.</p>\n</li>\n<li><p><strong>Identify successor nodes</strong>: For each virtual node belonging to the departing physical node, find the next virtual node position clockwise on the ring. The physical node owning that successor virtual node will inherit the departing node&#39;s key range.</p>\n</li>\n<li><p><strong>Migrate keys to successors</strong>: Transfer all cache entries owned by the departing node to their respective successor nodes. This migration should prioritize frequently accessed keys to minimize cache misses during the transition.</p>\n</li>\n<li><p><strong>Update ring structure</strong>: Remove all of the departing node&#39;s virtual nodes from the sorted position list and position-to-node mapping.</p>\n</li>\n<li><p><strong>Broadcast membership change</strong>: Notify all remaining cluster members about the node departure through gossip protocol messages, ensuring consistent ring views across the cluster.</p>\n</li>\n<li><p><strong>Clean up resources</strong>: Release any resources associated with the departed node, including connection pools, health check timers, and local state tracking.</p>\n</li>\n</ol>\n<p>For graceful departures, the leaving node can actively participate in migration, pushing its cache entries to successor nodes. For failure scenarios, successor nodes must pull the data from replicas or mark the keys as unavailable until anti-entropy processes restore the data.</p>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The consistent hash ring implementation requires several critical design decisions that affect performance, scalability, and operational complexity. Each decision involves trade-offs between different system qualities.</p>\n<blockquote>\n<p><strong>Decision: Hash Function Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: The hash function must distribute keys uniformly across the ring space while providing consistent results across all nodes in the cluster.</li>\n<li><strong>Options Considered</strong>: MD5, SHA-1, SHA-256, MurmurHash3, xxHash</li>\n<li><strong>Decision</strong>: SHA-1 for production environments, with MD5 as an alternative for development</li>\n<li><strong>Rationale</strong>: SHA-1 provides excellent distribution properties with reasonable computational cost. While cryptographic strength isn&#39;t required for this use case, the uniform distribution is critical. MD5 offers better performance for development/testing scenarios where security isn&#39;t a concern.</li>\n<li><strong>Consequences</strong>: SHA-1 produces 160-bit hashes requiring truncation to 32-bit ring positions, but the distribution quality justifies this overhead. The choice can be made configurable to allow different environments to optimize differently.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Hash Function</th>\n<th>Distribution Quality</th>\n<th>Performance</th>\n<th>Output Size</th>\n<th>Security</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MD5</td>\n<td>Excellent</td>\n<td>Very Fast</td>\n<td>128 bits</td>\n<td>Weak</td>\n<td>Development only</td>\n</tr>\n<tr>\n<td>SHA-1</td>\n<td>Excellent</td>\n<td>Fast</td>\n<td>160 bits</td>\n<td>Moderate</td>\n<td>✓ Production</td>\n</tr>\n<tr>\n<td>SHA-256</td>\n<td>Excellent</td>\n<td>Slower</td>\n<td>256 bits</td>\n<td>Strong</td>\n<td>Overkill</td>\n</tr>\n<tr>\n<td>MurmurHash3</td>\n<td>Good</td>\n<td>Very Fast</td>\n<td>32/128 bits</td>\n<td>None</td>\n<td>Too specialized</td>\n</tr>\n<tr>\n<td>xxHash</td>\n<td>Good</td>\n<td>Very Fast</td>\n<td>32/64 bits</td>\n<td>None</td>\n<td>Too specialized</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Virtual Node Count Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: The number of virtual nodes per physical node affects load distribution, memory overhead, and rebalancing performance.</li>\n<li><strong>Options Considered</strong>: Fixed count (100 per node), proportional to capacity, adaptive based on cluster size</li>\n<li><strong>Decision</strong>: Configurable with intelligent defaults: 150 for small clusters (&lt;10 nodes), 100 for medium clusters (10-50 nodes), 50 for large clusters (50+ nodes)</li>\n<li><strong>Rationale</strong>: Smaller clusters need more virtual nodes per physical node to achieve good distribution. Larger clusters achieve statistical balance with fewer virtual nodes per node, reducing memory overhead and rebalancing time.</li>\n<li><strong>Consequences</strong>: Requires cluster-size-aware configuration, but provides optimal balance between distribution quality and performance across different deployment scales.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Ring Position Storage Structure</strong></p>\n<ul>\n<li><strong>Context</strong>: The ring must support efficient key lookups while allowing dynamic addition and removal of virtual nodes.</li>\n<li><strong>Options Considered</strong>: Sorted slice with binary search, balanced binary tree (red-black), hash table with consistent hashing library</li>\n<li><strong>Decision</strong>: Sorted slice of positions with binary search for lookups, rebuilding on membership changes</li>\n<li><strong>Rationale</strong>: Membership changes are relatively infrequent compared to key lookups. The sorted slice provides O(log n) lookups with excellent cache locality and simple implementation. The rebuild cost on membership changes is acceptable given the infrequency.</li>\n<li><strong>Consequences</strong>: Membership changes require O(n) time to rebuild the sorted slice, but the simplicity and lookup performance outweigh this cost. More complex structures would optimize for the uncommon case at the expense of the common case.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Key Range Assignment Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: When nodes join or leave, the system must determine which keys need migration and coordinate the transfer process.</li>\n<li><strong>Options Considered</strong>: Push-based (departing node sends keys), pull-based (receiving node requests keys), hybrid approach</li>\n<li><strong>Decision</strong>: Hybrid approach - push-based for graceful departures, pull-based for failure scenarios</li>\n<li><strong>Rationale</strong>: Graceful departures can leverage the departing node&#39;s knowledge of its keys for efficient transfer. Failure scenarios require surviving nodes to take initiative and pull data from replicas or other sources.</li>\n<li><strong>Consequences</strong>: Requires more complex implementation but optimizes for both planned and unplanned membership changes. The added complexity is justified by improved performance in each scenario.</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Implementing consistent hashing appears straightforward in theory but contains several subtle traps that can cause severe problems in production environments.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Virtual Node Count</strong></p>\n<p>Many implementations start with too few virtual nodes per physical node, often using just 10-20 virtual nodes. This creates significant load imbalance, with some nodes receiving 2-3x more traffic than others. The problem worsens in small clusters where statistical effects have less opportunity to average out.</p>\n<p><em>Why it&#39;s wrong</em>: With only 20 virtual nodes across a 3-node cluster (60 total), some nodes will own ranges covering 25% of the keyspace while others own only 10%. In a cache workload with uneven key popularity, this translates to proportionally uneven load.</p>\n<p><em>How to fix it</em>: Use at least 50 virtual nodes per physical node for clusters with fewer than 20 nodes. Monitor actual key distribution in production and increase virtual node count if load balancing remains poor. Implement tooling to analyze key distribution across nodes and identify imbalance.</p>\n<p>⚠️ <strong>Pitfall: Hash Function Consistency Across Nodes</strong></p>\n<p>Different nodes using different hash implementations or configurations will place keys at different ring positions, causing cache misses and data inconsistency. This often happens when nodes run different software versions or when hash function libraries change behavior between versions.</p>\n<p><em>Why it&#39;s wrong</em>: If Node A thinks key &quot;user:123&quot; hashes to position 45 (owned by Node B) but Node B thinks the same key hashes to position 67 (owned by Node C), cache operations will fail unpredictably. SET operations might store data on one node while GET operations look for it on another.</p>\n<p><em>How to fix it</em>: Standardize on a specific hash function implementation and version across all nodes. Include hash function choice and version in the cluster configuration. Implement hash compatibility tests in your deployment pipeline. Consider encoding the hash algorithm version in cache entry metadata to detect mismatches.</p>\n<p>⚠️ <strong>Pitfall: Race Conditions During Ring Updates</strong></p>\n<p>Concurrent node additions or failures can create inconsistent ring states if multiple nodes attempt to update the ring simultaneously. This leads to split-brain scenarios where different nodes have different views of key ownership.</p>\n<p><em>Why it&#39;s wrong</em>: If Node A thinks key &quot;session:456&quot; belongs to Node B while Node C thinks it belongs to Node D, cache coherency breaks down. Applications might see cache misses for recently stored data or inconsistent values across requests.</p>\n<p><em>How to fix it</em>: Implement a gossip-based consensus mechanism for ring updates, ensuring all nodes converge to the same ring state. Use version numbers or vector clocks to detect and resolve conflicting ring updates. Add logging and monitoring to detect ring inconsistencies quickly.</p>\n<p>⚠️ <strong>Pitfall: Blocking Operations During Rebalancing</strong></p>\n<p>Some implementations block all cache operations while rebalancing keys after membership changes. This can cause significant service disruption, especially for large datasets or slow migration processes.</p>\n<p><em>Why it&#39;s wrong</em>: Blocking operations defeats the purpose of building a high-availability cache. Applications experience timeouts and degraded performance during routine cluster maintenance or failure recovery.</p>\n<p><em>How to fix it</em>: Implement non-blocking rebalancing where cache operations continue during key migration. Use double-writing (storing keys on both old and new owners temporarily) during transitions. Implement graceful migration with rate limiting to avoid overwhelming nodes with migration traffic.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Hotspot Keys</strong></p>\n<p>Consistent hashing distributes keys uniformly across nodes, but it doesn&#39;t account for keys with vastly different access patterns. A single popular key can create a hotspot that overwhelms one node while others remain idle.</p>\n<p><em>Why it&#39;s wrong</em>: Even perfect key distribution becomes meaningless if 90% of requests target 10% of keys that all happen to live on the same node. The cache becomes limited by the performance of individual nodes rather than scaling with cluster size.</p>\n<p><em>How to fix it</em>: Implement hotspot detection by monitoring per-key access frequencies. For extremely popular keys, consider replicating them to multiple nodes or using a separate hot-key cache layer. Design your key naming scheme to avoid natural hotspots (like time-based prefixes that create temporal locality).</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building a production-ready consistent hash ring requires careful attention to data structures, concurrency, and performance characteristics. The following guidance provides both the foundational infrastructure and the core algorithms that form the heart of your distributed cache.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash Function</td>\n<td>MD5 (crypto/md5)</td>\n<td>SHA-1 (crypto/sha1)</td>\n</tr>\n<tr>\n<td>Data Structure</td>\n<td>Sorted slice + binary search</td>\n<td>Skip list or B-tree</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON (encoding/json)</td>\n<td>Protocol Buffers</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td>RWMutex (sync.RWMutex)</td>\n<td>Lock-free with atomic operations</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Unit tests + table-driven tests</td>\n<td>Property-based testing with rapid</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The hash ring implementation should be organized as a self-contained package with clear separation between core algorithms and supporting utilities:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/hashring/\n  hashring.go              ← Core HashRing implementation\n  hashring_test.go         ← Unit tests and benchmarks\n  virtual_node.go          ← Virtual node management\n  hash_functions.go        ← Pluggable hash function support\n  rebalancer.go           ← Key migration coordination\n  ring_visualizer.go      ← Debugging and monitoring tools\n  \ninternal/hashring/testdata/\n  ring_scenarios.json     ← Test cases for complex scenarios\n  performance_baselines.json ← Performance regression tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>The following hash function abstraction provides a clean interface for different hashing strategies while maintaining consistency across the cluster:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> hashring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/md5</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha1</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">hash</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HashFunction defines the interface for ring position calculation.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashFunction</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Hash</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SHA1Hash provides SHA-1 based hashing for production environments.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SHA1Hash</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#B392F0\">SHA1Hash</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha1.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashBytes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hasher.</span><span style=\"color:#B392F0\">Sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use first 4 bytes of SHA-1 output for ring position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(hashBytes[:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#B392F0\">SHA1Hash</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"sha1\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MD5Hash provides MD5 based hashing for development environments.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MD5Hash</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#B392F0\">MD5Hash</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> md5.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashBytes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hasher.</span><span style=\"color:#B392F0\">Sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use first 4 bytes of MD5 output for ring position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> binary.BigEndian.</span><span style=\"color:#B392F0\">Uint32</span><span style=\"color:#E1E4E8\">(hashBytes[:</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#B392F0\">MD5Hash</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"md5\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHashFunction creates a hash function by name.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashFunction</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">HashFunction</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> name {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"sha1\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> SHA1Hash</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"md5\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> MD5Hash</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unsupported hash function: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>This ring position management utility handles the sorted array operations efficiently:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> hashring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RingPositions manages the sorted list of virtual node positions.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RingPositions</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    positions []</span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRingPositions creates a new ring position manager.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRingPositions</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingPositions</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RingPositions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        positions: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodes:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Add inserts a new position and maintains sorted order.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingPositions</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">position</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rp.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Insert position in sorted order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    insertIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sort.</span><span style=\"color:#B392F0\">Search</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(rp.positions), </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> rp.positions[i] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Expand slice and insert at correct position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.positions </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(rp.positions, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(rp.positions[insertIndex</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:], rp.positions[insertIndex:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.positions[insertIndex] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Map position to node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.nodes[position] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nodeID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Remove deletes a position and updates the mapping.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingPositions</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Remove</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">position</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rp.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Find position index</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sort.</span><span style=\"color:#B392F0\">Search</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(rp.positions), </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> rp.positions[i] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(rp.positions) </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> rp.positions[index] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> position {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Remove from slice</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        copy</span><span style=\"color:#E1E4E8\">(rp.positions[index:], rp.positions[index</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rp.positions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rp.positions[:</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(rp.positions)</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Remove from mapping</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        delete</span><span style=\"color:#E1E4E8\">(rp.nodes, position)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FindSuccessor returns the node responsible for the given position.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingPositions</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">FindSuccessor</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">position</span><span style=\"color:#F97583\"> uint32</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rp.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(rp.positions) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Find first position >= target</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sort.</span><span style=\"color:#B392F0\">Search</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(rp.positions), </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">i</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> rp.positions[i] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> position</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle wraparound case</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(rp.positions) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        index </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successorPosition </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> rp.positions[index]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> rp.nodes[successorPosition], </span><span style=\"color:#79B8FF\">true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetPositions returns a copy of all positions for iteration.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingPositions</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetPositions</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rp.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    positions </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(rp.positions))</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(positions, rp.positions)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> positions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Size returns the number of positions in the ring.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RingPositions</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Size</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rp.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> rp.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(rp.positions)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>The main <code>HashRing</code> implementation should focus on the consistent hashing algorithm while leveraging the supporting infrastructure:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> hashring</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HashRing implements consistent hashing with virtual nodes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashRing</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    virtualNodes </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ring         </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint32</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sortedKeys   []</span><span style=\"color:#F97583\">uint32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashFunc     </span><span style=\"color:#B392F0\">HashFunction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHashRing creates a new consistent hash ring with the specified number of virtual nodes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">virtualNodes</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate that virtualNodes is positive (return error if not)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create HashRing struct with initialized maps and slices</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set default hash function to SHA1Hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return configured HashRing instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use make() to initialize maps with reasonable capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddNode adds a physical node to the ring with its virtual nodes.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock to prevent concurrent modifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if node already exists (skip if duplicate)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each virtual node index (0 to virtualNodes-1):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Create virtual node key by combining nodeID and index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Hash the virtual node key to get ring position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Store position -> nodeID mapping in ring map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add position to sortedKeys slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Sort the sortedKeys slice to maintain order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Mark node as active in nodes map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Release write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Virtual node key format could be \"nodeID:vnodeIndex\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RemoveNode removes a physical node and all its virtual nodes from the ring.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RemoveNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock to prevent concurrent modifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if node exists (skip if not found)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each virtual node index (0 to virtualNodes-1):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Create virtual node key by combining nodeID and index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Hash the virtual node key to get ring position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Remove position from ring map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Find and remove position from sortedKeys slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove node from nodes map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Rebuild sortedKeys slice to remove gaps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Release write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Consider using a temporary slice for efficient removal</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNode returns the node responsible for storing the given key.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock for thread-safe access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if ring is empty (return empty string if no nodes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Hash the key to get its ring position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Use binary search on sortedKeys to find first position >= key position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle wraparound case (if no position >= key position, use first position)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Look up node ID using the found position in ring map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Release read lock and return node ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: sort.Search() provides binary search functionality</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNodes returns N nodes for replication, starting with the primary node.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNodes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock for thread-safe access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate count parameter (should be positive and &#x3C;= total nodes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Hash the key to get starting position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Use binary search to find first virtual node >= key position</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Iterate through sortedKeys starting from found position:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add each unique physical node to result list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Handle wraparound when reaching end of sortedKeys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Stop when we have 'count' unique nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Release read lock and return node list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use a map to track which nodes you've already added</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNodeKeys returns all keys that would be assigned to the specified node.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This method is used during rebalancing to identify keys for migration.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNodeKeys</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">allKeys</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock for consistent ring state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create result slice to collect keys assigned to nodeID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each key in allKeys:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Use GetNode() to find responsible node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If responsible node equals nodeID, add to result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Release read lock and return result slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Consider optimizing for large key sets using position ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This method is primarily used during node migration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRingState returns debugging information about the current ring state.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetRingState</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock for consistent snapshot</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create map to hold ring statistics and state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add total virtual node count, physical node count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add list of active physical nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Add ring position distribution for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Release read lock and return state map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: This information is valuable for monitoring and debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Hash Function Performance</strong>: Go&#39;s <code>crypto/sha1</code> and <code>crypto/md5</code> packages are optimized for performance. For maximum throughput, consider reusing hash instances with <code>Reset()</code> rather than creating new ones for each operation.</p>\n<p><strong>Binary Search Optimization</strong>: The <code>sort.Search()</code> function is highly optimized in Go&#39;s standard library. Use it directly rather than implementing custom binary search logic.</p>\n<p><strong>Memory Pool Usage</strong>: For high-throughput scenarios, consider using <code>sync.Pool</code> to reuse slice allocations in frequently called methods like <code>GetNodes()</code>.</p>\n<p><strong>Slice Growth Strategy</strong>: When building the <code>sortedKeys</code> slice, pre-allocate capacity based on expected virtual node count: <code>make([]uint32, 0, virtualNodes*expectedNodes)</code>.</p>\n<p><strong>Lock Granularity</strong>: The current design uses a single RWMutex for the entire ring. For extremely high-throughput scenarios, consider more granular locking or lock-free approaches using atomic operations.</p>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the consistent hash ring, verify correctness with these specific tests:</p>\n<p><strong>Basic Functionality Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/hashring/</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestHashRing</span></span></code></pre></div>\n\n<p>Expected behavior: All tests pass, showing correct key distribution, node addition/removal, and lookup operations.</p>\n<p><strong>Load Distribution Test</strong>: Create a ring with 3 nodes, add 10,000 randomly generated keys, and verify that each node receives between 25-40% of keys (allowing for statistical variation with virtual nodes).</p>\n<p><strong>Rebalancing Test</strong>: Add a fourth node to the ring and verify that only ~25% of existing keys change ownership. Remove a node and verify that its keys distribute evenly among remaining nodes.</p>\n<p><strong>Signs of Problems</strong>:</p>\n<ul>\n<li><strong>Uneven distribution</strong>: Some nodes getting &gt;50% of keys indicates insufficient virtual nodes</li>\n<li><strong>High rebalancing cost</strong>: &gt;40% of keys moving when adding one node suggests poor hash function or virtual node strategy  </li>\n<li><strong>Lookup inconsistencies</strong>: Same key mapping to different nodes across calls indicates race conditions or sorting bugs</li>\n</ul>\n<p><strong>Performance Benchmark</strong>: Run <code>go test -bench=BenchmarkGetNode</code> and verify lookup operations complete in &lt;1 microsecond for rings with 100+ virtual nodes.</p>\n<h2 id=\"cache-node-implementation\">Cache Node Implementation</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section covers Milestone 2 (Cache Node Implementation), building on the hash ring foundation from Milestone 1 and providing the storage foundation for Milestone 3 (Cluster Communication) and Milestone 4 (Replication &amp; Consistency).</p>\n</blockquote>\n<h3 id=\"mental-model-the-smart-filing-cabinet\">Mental Model: The Smart Filing Cabinet</h3>\n<p>Think of each cache node as a smart filing cabinet in a busy office. Unlike a traditional filing cabinet, this one has several remarkable abilities:</p>\n<p>The cabinet automatically reorganizes itself every time you access a file. When you pull out a document, the cabinet moves it to the front of the most accessible drawer. Files you haven&#39;t used in a while gradually migrate toward the back, and eventually get moved to storage or discarded entirely when the cabinet runs out of space. This is exactly how <strong>LRU eviction</strong> works - the &quot;least recently used&quot; items get pushed out when memory fills up.</p>\n<p>The cabinet also has a built-in timer system. Each file folder has a sticky note with an expiration date. A helpful assistant periodically walks through and removes any files past their expiration - this represents <strong>TTL expiration</strong>. The assistant is careful not to interrupt your work; they clean expired files during quiet moments.</p>\n<p>Finally, the cabinet has a precise scale that weighs every document. It knows exactly how much storage capacity it has, and when new files would exceed the limit, it automatically removes the oldest, least-used files to make room. This is <strong>memory accounting</strong> - tracking the exact memory footprint and enforcing limits through eviction.</p>\n<p>The key insight is that the filing cabinet must remain responsive to your requests even while reorganizing itself and cleaning expired files. This requires careful coordination - the cabinet uses a sophisticated locking system so multiple people can safely access it simultaneously without corrupting the organization system.</p>\n<h3 id=\"lru-eviction-algorithm\">LRU Eviction Algorithm</h3>\n<p>The <strong>Least Recently Used (LRU)</strong> eviction algorithm maintains the invariant that when memory limits are exceeded, the cache removes entries that haven&#39;t been accessed for the longest time. This strategy assumes that recently accessed data is more likely to be accessed again soon - a principle called temporal locality that holds true for most caching workloads.</p>\n<p>The core challenge is efficiently tracking access order while supporting concurrent operations. A naive approach might store timestamps and sort entries by last access time, but this would be prohibitively expensive for high-throughput caches. Instead, we use a combination of a hash map for O(1) lookups and a doubly-linked list for O(1) reordering.</p>\n<h4 id=\"lru-data-structure-design\">LRU Data Structure Design</h4>\n<p>The <code>LRUCache</code> combines two data structures to achieve efficient operations:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>mutex</code></td>\n<td><code>sync.RWMutex</code></td>\n<td>Protects all cache operations from race conditions during concurrent access</td>\n</tr>\n<tr>\n<td><code>capacity</code></td>\n<td><code>int64</code></td>\n<td>Maximum memory in bytes before eviction begins</td>\n</tr>\n<tr>\n<td><code>used</code></td>\n<td><code>int64</code></td>\n<td>Current memory consumption in bytes across all stored entries</td>\n</tr>\n<tr>\n<td><code>items</code></td>\n<td><code>map[string]*list.Element</code></td>\n<td>Hash map providing O(1) key lookup to linked list nodes</td>\n</tr>\n<tr>\n<td><code>order</code></td>\n<td><code>*list.List</code></td>\n<td>Doubly-linked list maintaining LRU order, most recent at front</td>\n</tr>\n</tbody></table>\n<p>The linked list elements contain <code>CacheEntry</code> pointers as their values. This design enables:</p>\n<ul>\n<li>O(1) key lookup through the hash map</li>\n<li>O(1) reordering when entries are accessed (move to front)</li>\n<li>O(1) eviction of least recently used items (remove from back)</li>\n<li>O(1) memory accounting updates</li>\n</ul>\n<h4 id=\"lru-operation-algorithms\">LRU Operation Algorithms</h4>\n<p><strong>Get Operation Algorithm:</strong></p>\n<ol>\n<li>Acquire read lock to prevent concurrent modifications during lookup</li>\n<li>Check if key exists in the items hash map</li>\n<li>If key not found, release lock and return miss</li>\n<li>If key found, upgrade to write lock for reordering</li>\n<li>Extract <code>CacheEntry</code> from the linked list element</li>\n<li>Check if entry has expired based on current time vs <code>ExpiresAt</code></li>\n<li>If expired, remove from both hash map and linked list, update memory accounting</li>\n<li>If valid, move the linked list element to the front (most recently used position)</li>\n<li>Return the entry value and release write lock</li>\n</ol>\n<p><strong>Set Operation Algorithm:</strong></p>\n<ol>\n<li>Acquire write lock for exclusive access during modification</li>\n<li>Calculate memory size of new entry (key length + value length + metadata overhead)</li>\n<li>Check if key already exists in the cache</li>\n<li>If key exists, remove old entry from memory accounting and linked list</li>\n<li>While (current memory usage + new entry size) exceeds capacity, evict LRU entries</li>\n<li>For each eviction: remove element from back of linked list, delete from hash map, subtract from memory usage</li>\n<li>Create new <code>CacheEntry</code> with key, value, expiration time, and size</li>\n<li>Create new linked list element containing the cache entry</li>\n<li>Add element to front of linked list (most recently used position)</li>\n<li>Add key-to-element mapping in hash map</li>\n<li>Update memory usage accounting to include new entry size</li>\n<li>Release write lock</li>\n</ol>\n<p><strong>Delete Operation Algorithm:</strong></p>\n<ol>\n<li>Acquire write lock for exclusive access during modification</li>\n<li>Look up key in hash map to find corresponding linked list element</li>\n<li>If key not found, release lock and return false</li>\n<li>Remove element from linked list using list.Remove()</li>\n<li>Delete key from hash map</li>\n<li>Subtract entry size from current memory usage</li>\n<li>Release write lock and return true</li>\n</ol>\n<h4 id=\"memory-accounting-strategy\">Memory Accounting Strategy</h4>\n<p>Accurate memory accounting prevents the cache from exceeding its configured memory limit and enables precise eviction decisions. The cache tracks memory usage at the granularity of individual cache entries, including both data and metadata overhead.</p>\n<p><strong>Memory Calculation for Cache Entries:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Size Calculation</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key</td>\n<td><code>len(key)</code> bytes</td>\n<td>String storage overhead varies by runtime</td>\n</tr>\n<tr>\n<td>Value</td>\n<td><code>len(value)</code> bytes</td>\n<td>Raw byte slice length</td>\n</tr>\n<tr>\n<td>Metadata</td>\n<td>64 bytes estimate</td>\n<td><code>CacheEntry</code> struct, linked list pointers, map overhead</td>\n</tr>\n<tr>\n<td>Expiration</td>\n<td>8 bytes</td>\n<td><code>time.Time</code> represents nanoseconds as int64</td>\n</tr>\n</tbody></table>\n<p>The 64-byte metadata estimate accounts for Go&#39;s runtime overhead including the <code>CacheEntry</code> struct (key string, value slice header, timestamp, size field), the linked list element (previous/next pointers, value interface), and hash map bucket overhead. This estimate provides a reasonable approximation without the complexity of precise runtime memory introspection.</p>\n<blockquote>\n<p><strong>Critical Design Insight:</strong> Memory accounting must be updated atomically with data structure changes. If memory tracking becomes inconsistent with actual usage, the cache may either exceed its memory limit (causing OOM conditions) or prematurely evict entries (reducing hit rates). All operations that modify cache contents must update the <code>used</code> field within the same critical section.</p>\n</blockquote>\n<h4 id=\"concurrency-and-lock-strategy\">Concurrency and Lock Strategy</h4>\n<p>The cache supports concurrent reads while ensuring data consistency during modifications. The lock strategy balances performance with safety:</p>\n<p><strong>Read Lock Usage (Get Operations):</strong></p>\n<ul>\n<li>Multiple readers can proceed simultaneously for different keys</li>\n<li>Prevents writers from modifying cache structure during reads</li>\n<li>Upgraded to write lock only when reordering is needed (cache hit)</li>\n</ul>\n<p><strong>Write Lock Usage (Set/Delete Operations):</strong></p>\n<ul>\n<li>Exclusive access during cache modifications</li>\n<li>Protects both hash map and linked list consistency</li>\n<li>Covers memory accounting updates</li>\n</ul>\n<p><strong>Lock Upgrade Pattern:</strong>\nThe Get operation uses a lock upgrade pattern - starting with a read lock for the lookup, then upgrading to a write lock only if reordering is needed. This optimization allows multiple concurrent reads of different keys while still maintaining consistency.</p>\n<blockquote>\n<p><strong>Design Trade-off:</strong> Per-key locking could improve concurrency but adds complexity and memory overhead. The single-lock approach provides simpler correctness reasoning at the cost of some parallel access. For most caching workloads, the bottleneck is network I/O rather than lock contention.</p>\n</blockquote>\n<h3 id=\"ttl-and-expiration-handling\">TTL and Expiration Handling</h3>\n<p><strong>Time-to-Live (TTL)</strong> support allows cache entries to automatically expire after a configurable duration, preventing stale data from persisting indefinitely. The expiration system must handle three key requirements: accepting TTL specifications during writes, checking expiration during reads, and periodically cleaning expired entries to reclaim memory.</p>\n<h4 id=\"ttl-storage-and-representation\">TTL Storage and Representation</h4>\n<p>Each <code>CacheEntry</code> contains an <code>ExpiresAt</code> field of type <code>time.Time</code> representing the absolute expiration timestamp. Using absolute timestamps rather than relative durations avoids complications with clock adjustments and provides consistent behavior across system events.</p>\n<p><strong>TTL Specification Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>TTL Value</th>\n<th>ExpiresAt Calculation</th>\n<th>Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>0</code> (no TTL)</td>\n<td><code>time.Time{}</code> (zero value)</td>\n<td>Entry never expires automatically</td>\n</tr>\n<tr>\n<td>Positive duration</td>\n<td><code>time.Now().Add(ttl)</code></td>\n<td>Entry expires after specified duration</td>\n</tr>\n<tr>\n<td>Negative duration</td>\n<td><code>time.Now()</code> (immediate expiration)</td>\n<td>Entry expires immediately (effectively a no-op)</td>\n</tr>\n</tbody></table>\n<p>The zero value of <code>time.Time</code> represents a special case indicating no expiration. This avoids the need for separate boolean flags or nullable timestamp types.</p>\n<h4 id=\"expiration-checking-during-access\">Expiration Checking During Access</h4>\n<p>Every Get operation checks expiration before returning cached data. This <strong>lazy expiration</strong> approach ensures that expired entries are never returned to clients while distributing cleanup costs across normal operations.</p>\n<p><strong>Expiration Check Algorithm:</strong></p>\n<ol>\n<li>Retrieve <code>CacheEntry</code> from cache data structures</li>\n<li>Check if <code>ExpiresAt</code> is the zero value (no expiration configured)</li>\n<li>If expiration is set, compare <code>ExpiresAt</code> with <code>time.Now()</code></li>\n<li>If current time exceeds expiration time, treat as cache miss</li>\n<li>Remove expired entry from hash map, linked list, and update memory accounting</li>\n<li>Return cache miss result to client</li>\n</ol>\n<p>This approach guarantees that clients never receive expired data, even if background cleanup processes fall behind. The cost is minimal since the time comparison and potential cleanup occur only for cache hits.</p>\n<h4 id=\"background-expiration-cleanup\">Background Expiration Cleanup</h4>\n<p>While lazy expiration handles consistency during reads, expired entries can accumulate memory usage if they&#39;re never accessed again. A background cleanup process periodically scans for expired entries and removes them proactively.</p>\n<p><strong>Cleanup Process Design:</strong></p>\n<blockquote>\n<p><strong>Decision: Periodic Scanning vs. Timer-Based Expiration</strong></p>\n<ul>\n<li><strong>Context</strong>: Background cleanup needs to balance memory reclamation with performance overhead</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Periodic full scan of all entries</li>\n<li>Timer-based individual entry expiration</li>\n<li>Probabilistic sampling of entries</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Periodic full scan with configurable interval</li>\n<li><strong>Rationale</strong>: Simple implementation, predictable resource usage, works well with LRU ordering since expired entries tend to migrate toward LRU end</li>\n<li><strong>Consequences</strong>: Some memory overhead between cleanup intervals, but bounded and predictable</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Cleanup Strategy</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Periodic full scan</td>\n<td>Simple, predictable overhead, thorough cleanup</td>\n<td>May pause operations during scan</td>\n<td>✓ Yes</td>\n</tr>\n<tr>\n<td>Individual timers</td>\n<td>Precise expiration timing, distributed overhead</td>\n<td>Complex memory management, timer overhead</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Probabilistic sampling</td>\n<td>Low overhead, scales with cache size</td>\n<td>May miss expired entries, unpredictable cleanup</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p><strong>Cleanup Implementation Algorithm:</strong></p>\n<ol>\n<li>Acquire write lock to prevent concurrent modifications during cleanup</li>\n<li>Create slice to collect keys of expired entries (avoids map modification during iteration)</li>\n<li>Iterate through all entries in the linked list from back to front (LRU order)</li>\n<li>For each entry, check if <code>ExpiresAt</code> is non-zero and less than current time</li>\n<li>If expired, append key to expiration slice and continue</li>\n<li>If non-expired entry found, break iteration (optimization: newer entries likely not expired)</li>\n<li>For each expired key, remove from hash map and linked list, update memory accounting</li>\n<li>Release write lock and return count of expired entries removed</li>\n</ol>\n<p>The cleanup process iterates from the LRU end because expired entries are more likely to be found there - entries that haven&#39;t been accessed recently are both candidates for LRU eviction and more likely to have exceeded their TTL.</p>\n<h4 id=\"ttl-and-memory-management-interaction\">TTL and Memory Management Interaction</h4>\n<p>TTL expiration interacts with LRU eviction in important ways. Both mechanisms remove entries from the cache, but they operate on different criteria - time-based vs. access-based. The system must coordinate these mechanisms to maintain consistency.</p>\n<p><strong>Interaction Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>LRU Behavior</th>\n<th>TTL Behavior</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory pressure, no expired entries</td>\n<td>Evict LRU entries until under capacity</td>\n<td>Continue normal TTL checking</td>\n<td>LRU eviction takes precedence</td>\n</tr>\n<tr>\n<td>Memory pressure, some expired entries</td>\n<td>Would evict LRU entries</td>\n<td>Expired entries available for cleanup</td>\n<td>Remove expired entries first, then LRU if needed</td>\n</tr>\n<tr>\n<td>No memory pressure, expired entries exist</td>\n<td>Normal LRU operation</td>\n<td>Background cleanup runs periodically</td>\n<td>TTL cleanup reclaims memory proactively</td>\n</tr>\n<tr>\n<td>Entry expires while being accessed</td>\n<td>Entry would move to MRU position</td>\n<td>Entry should be treated as expired</td>\n<td>Expiration check prevents MRU promotion</td>\n</tr>\n</tbody></table>\n<p>The key principle is that expiration checks occur before any cache operations, ensuring expired entries are never promoted to the MRU position or returned to clients.</p>\n<h3 id=\"memory-accounting-and-limits\">Memory Accounting and Limits</h3>\n<p>Precise memory management ensures the cache operates within configured limits while providing predictable performance characteristics. The memory accounting system tracks actual memory usage, enforces capacity limits through eviction, and provides visibility into memory utilization patterns.</p>\n<h4 id=\"memory-tracking-granularity\">Memory Tracking Granularity</h4>\n<p>The cache tracks memory usage at the entry level, accounting for both user data and system overhead. This granularity provides accurate enforcement of memory limits while remaining computationally efficient.</p>\n<p><strong>Memory Components per Entry:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Calculation Method</th>\n<th>Example Size</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key string</td>\n<td><code>len(key)</code></td>\n<td>20 bytes</td>\n<td>UTF-8 encoding, varies by content</td>\n</tr>\n<tr>\n<td>Value bytes</td>\n<td><code>len(value)</code></td>\n<td>Variable</td>\n<td>Raw payload size</td>\n</tr>\n<tr>\n<td>Entry struct</td>\n<td>Fixed overhead</td>\n<td>56 bytes</td>\n<td>Key, value slice header, timestamp, size</td>\n</tr>\n<tr>\n<td>List element</td>\n<td>Fixed overhead</td>\n<td>24 bytes</td>\n<td>Previous/next pointers, value interface</td>\n</tr>\n<tr>\n<td>Map overhead</td>\n<td>Estimated</td>\n<td>~16 bytes</td>\n<td>Hash bucket entry, varies with map load factor</td>\n</tr>\n<tr>\n<td><strong>Total</strong></td>\n<td>Sum of components</td>\n<td>116 + data</td>\n<td>Metadata overhead approximately 15% for 1KB values</td>\n</tr>\n</tbody></table>\n<p>The fixed overhead calculation assumes 64-bit pointers and includes Go runtime structures. The map overhead is amortized across entries and varies with the hash map&#39;s load factor and bucket structure.</p>\n<h4 id=\"capacity-enforcement-strategy\">Capacity Enforcement Strategy</h4>\n<p>When adding new entries would exceed the configured memory capacity, the cache must decide which existing entries to remove. The eviction strategy coordinates LRU ordering with memory accounting to maintain both access patterns and memory limits.</p>\n<p><strong>Eviction Decision Algorithm:</strong></p>\n<ol>\n<li>Calculate total memory needed: current usage + new entry size</li>\n<li>If total is within capacity, proceed with insertion without eviction</li>\n<li>If total exceeds capacity, calculate excess memory: (current + new) - capacity</li>\n<li>Begin evicting entries from the LRU end of the linked list</li>\n<li>For each evicted entry, subtract its memory size from current usage</li>\n<li>Continue eviction until excess memory is eliminated</li>\n<li>Insert new entry and update memory accounting</li>\n</ol>\n<blockquote>\n<p><strong>Design Principle</strong>: The cache evicts slightly more entries than strictly necessary to create headroom for future insertions. This reduces the frequency of eviction operations and improves performance for bursty workloads.</p>\n</blockquote>\n<p><strong>Eviction Batch Size Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Eviction Behavior</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Exact eviction</td>\n<td>Remove entries until exactly under limit</td>\n<td>Maximizes memory utilization</td>\n<td>Frequent eviction operations</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Headroom eviction</td>\n<td>Remove extra 10% beyond requirement</td>\n<td>Reduces eviction frequency</td>\n<td>Slightly lower memory utilization</td>\n<td>✓ Yes</td>\n</tr>\n<tr>\n<td>Batch eviction</td>\n<td>Remove fixed number of entries</td>\n<td>Predictable operation cost</td>\n<td>May over-evict or under-evict</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h4 id=\"memory-limit-configuration\">Memory Limit Configuration</h4>\n<p>The memory limit configuration affects both performance and resource utilization. Setting appropriate limits requires understanding the trade-offs between memory usage, cache hit rates, and eviction overhead.</p>\n<p><strong>Configuration Guidelines:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Memory Limit</th>\n<th>Cache Behavior</th>\n<th>Use Case</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&lt; 100 MB</td>\n<td>Frequent eviction, low hit rates</td>\n<td>Development, testing</td>\n<td>Low resource usage, poor performance</td>\n</tr>\n<tr>\n<td>100 MB - 1 GB</td>\n<td>Moderate eviction, good hit rates</td>\n<td>Small production workloads</td>\n<td>Balanced resource usage and performance</td>\n</tr>\n<tr>\n<td>1 GB - 10 GB</td>\n<td>Infrequent eviction, high hit rates</td>\n<td>Large production workloads</td>\n<td>High memory usage, excellent performance</td>\n</tr>\n<tr>\n<td>&gt; 10 GB</td>\n<td>Rare eviction, very high hit rates</td>\n<td>Cache-heavy applications</td>\n<td>Very high memory usage, optimal performance</td>\n</tr>\n</tbody></table>\n<p>The optimal memory limit depends on available system memory, working set size, and access patterns. A general guideline is to allocate 50-70% of available memory to the cache, leaving headroom for the operating system and other processes.</p>\n<h4 id=\"memory-usage-monitoring\">Memory Usage Monitoring</h4>\n<p>The cache provides real-time visibility into memory utilization patterns, enabling capacity planning and performance optimization. Monitoring includes both absolute usage and utilization rates.</p>\n<p><strong>Memory Metrics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Calculation</th>\n<th>Purpose</th>\n<th>Alert Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Used Memory</td>\n<td>Current <code>used</code> field value</td>\n<td>Track absolute consumption</td>\n<td>N/A (informational)</td>\n</tr>\n<tr>\n<td>Memory Utilization</td>\n<td><code>(used / capacity) * 100%</code></td>\n<td>Track capacity percentage</td>\n<td>&gt; 90% (approaching limit)</td>\n</tr>\n<tr>\n<td>Eviction Rate</td>\n<td>Evictions per second</td>\n<td>Monitor pressure</td>\n<td>&gt; 10/sec (high pressure)</td>\n</tr>\n<tr>\n<td>Entry Count</td>\n<td>Length of items map</td>\n<td>Track entry density</td>\n<td>N/A (informational)</td>\n</tr>\n<tr>\n<td>Average Entry Size</td>\n<td><code>used / entry count</code></td>\n<td>Monitor size distribution</td>\n<td>N/A (informational)</td>\n</tr>\n</tbody></table>\n<p>High memory utilization (&gt;90%) indicates that the cache is operating near capacity and may experience frequent evictions. High eviction rates suggest that the working set size exceeds cache capacity and either the memory limit should be increased or the workload characteristics need optimization.</p>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The cache node implementation requires several critical design decisions that affect performance, correctness, and maintainability. Each decision involves trade-offs between competing concerns such as simplicity, performance, and resource usage.</p>\n<h4 id=\"data-structure-selection\">Data Structure Selection</h4>\n<blockquote>\n<p><strong>Decision: Hash Map + Doubly Linked List for LRU</strong></p>\n<ul>\n<li><strong>Context</strong>: Need O(1) access and reordering for high-performance caching</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single hash map with timestamps (simple but requires sorting)</li>\n<li>Hash map + doubly linked list (complex but efficient)</li>\n<li>Hash map + heap/priority queue (moderate complexity, log N reordering)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hash map + doubly linked list combination</li>\n<li><strong>Rationale</strong>: Provides O(1) lookup, insertion, deletion, and reordering. Critical for maintaining consistent performance under load.</li>\n<li><strong>Consequences</strong>: More complex implementation and memory overhead, but predictable performance characteristics essential for production caching.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>Access Time</th>\n<th>Reorder Time</th>\n<th>Memory Overhead</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash map + timestamps</td>\n<td>O(1)</td>\n<td>O(n log n)</td>\n<td>Low</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Hash map + linked list</td>\n<td>O(1)</td>\n<td>O(1)</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Hash map + heap</td>\n<td>O(1)</td>\n<td>O(log n)</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<h4 id=\"concurrency-model\">Concurrency Model</h4>\n<blockquote>\n<p><strong>Decision: Single RWMutex for Cache-Wide Locking</strong></p>\n<ul>\n<li><strong>Context</strong>: Need thread safety for concurrent access while maintaining data structure consistency</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>No locking (unsafe for concurrent access)</li>\n<li>Single mutex for all operations (safe but limits concurrency)</li>\n<li>Single RWMutex allowing concurrent reads (balanced approach)</li>\n<li>Per-key locking with lock striping (complex but highly concurrent)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Single RWMutex for the entire cache</li>\n<li><strong>Rationale</strong>: Balances implementation simplicity with reasonable concurrency. Most cache workloads are network-bound rather than lock-bound.</li>\n<li><strong>Consequences</strong>: Some lock contention under high concurrency, but simpler correctness reasoning and lower memory overhead per key.</li>\n</ul>\n</blockquote>\n<h4 id=\"memory-accounting-precision\">Memory Accounting Precision</h4>\n<blockquote>\n<p><strong>Decision: Estimated Metadata Overhead</strong></p>\n<ul>\n<li><strong>Context</strong>: Need accurate memory accounting without runtime introspection overhead</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Track only user data size (inaccurate)</li>\n<li>Use runtime reflection for exact sizes (accurate but slow)</li>\n<li>Estimate metadata overhead with fixed constants (approximation)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Estimate metadata overhead using fixed per-entry constants</li>\n<li><strong>Rationale</strong>: Provides reasonable accuracy for memory enforcement while maintaining performance. The estimation error is bounded and consistent.</li>\n<li><strong>Consequences</strong>: Memory usage may vary slightly from estimates, but behavior remains predictable and overhead is minimal.</li>\n</ul>\n</blockquote>\n<h4 id=\"ttl-implementation-strategy\">TTL Implementation Strategy</h4>\n<blockquote>\n<p><strong>Decision: Lazy Expiration with Periodic Cleanup</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance data freshness guarantees with performance overhead</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>No expiration support (simple but limited functionality)</li>\n<li>Lazy expiration only (guarantees freshness but may waste memory)</li>\n<li>Active expiration with timers (complex implementation)</li>\n<li>Hybrid lazy + periodic cleanup (balanced approach)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Lazy expiration during access plus periodic background cleanup</li>\n<li><strong>Rationale</strong>: Guarantees that expired data is never served while providing bounded memory usage. Background cleanup prevents memory leaks from unaccessed expired entries.</li>\n<li><strong>Consequences</strong>: Small memory overhead between cleanup intervals, but provides strong correctness guarantees with reasonable performance.</li>\n</ul>\n</blockquote>\n<h4 id=\"error-handling-philosophy\">Error Handling Philosophy</h4>\n<p>The cache node adopts a fail-fast approach for configuration errors and a graceful degradation approach for runtime errors. This strategy provides clear feedback during development while maintaining availability during production operation.</p>\n<p><strong>Error Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Handling Strategy</th>\n<th>Example</th>\n<th>Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration</td>\n<td>Fail fast at startup</td>\n<td>Invalid memory limit</td>\n<td>Return error, refuse to start</td>\n</tr>\n<tr>\n<td>Memory allocation</td>\n<td>Graceful degradation</td>\n<td>Out of memory</td>\n<td>Log error, continue with eviction</td>\n</tr>\n<tr>\n<td>Concurrent access</td>\n<td>Panic on detection</td>\n<td>Data structure corruption</td>\n<td>Fail immediately to prevent data loss</td>\n</tr>\n<tr>\n<td>TTL calculation</td>\n<td>Graceful handling</td>\n<td>Clock skew/negative TTL</td>\n<td>Treat as immediate expiration</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Cache node implementation involves several subtle correctness and performance issues that frequently trip up implementers. Understanding these pitfalls helps avoid bugs that can cause data corruption, memory leaks, or performance degradation.</p>\n<h4 id=\"-pitfall-inconsistent-memory-accounting\">⚠️ Pitfall: Inconsistent Memory Accounting</h4>\n<p><strong>The Problem</strong>: Updating cache data structures without correspondingly updating the <code>used</code> memory field, or vice versa. This typically happens when implementing the Set operation - developers correctly add entries to the hash map and linked list but forget to increment memory usage, or they handle the happy path correctly but miss error conditions.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Inconsistent memory accounting leads to two serious problems. Under-counting memory usage allows the cache to exceed its configured limit, potentially causing out-of-memory conditions in the host application. Over-counting memory usage triggers premature evictions, reducing cache hit rates and performance.</p>\n<p><strong>How to Fix</strong>: Always update memory accounting within the same critical section as data structure modifications. Use defer statements or explicit cleanup blocks to ensure memory accounting is reverted if operations fail partway through.</p>\n<p><strong>Correct Pattern</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Acquire lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Calculate new memory requirement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Perform evictions if necessary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Update data structures AND memory accounting together</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Release lock</span></span></code></pre></div>\n\n<h4 id=\"-pitfall-lock-upgrade-race-conditions\">⚠️ Pitfall: Lock Upgrade Race Conditions</h4>\n<p><strong>The Problem</strong>: In the Get operation, upgrading from read lock to write lock creates a window where other goroutines can modify the cache state. Between releasing the read lock and acquiring the write lock, another goroutine might delete the entry or modify the linked list structure.</p>\n<p><strong>Why It&#39;s Wrong</strong>: The cache entry pointer retrieved under the read lock may become invalid after lock release, leading to use-after-free bugs or null pointer dereferences when attempting to move the entry to the front of the LRU list.</p>\n<p><strong>How to Fix</strong>: After upgrading to write lock, re-verify that the entry still exists in the cache before attempting to reorder it. Alternatively, use a write lock for the entire Get operation when a hit is detected.</p>\n<p><strong>Safe Pattern</strong>: Always re-lookup entries after lock upgrades and handle the case where entries disappear between lock acquisition phases.</p>\n<h4 id=\"-pitfall-ttl-cleanup-during-iteration\">⚠️ Pitfall: TTL Cleanup During Iteration</h4>\n<p><strong>The Problem</strong>: Modifying the hash map or linked list while iterating over it. This commonly occurs in the cleanup process where developers try to delete expired entries during a range loop over the items map.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Most programming languages, including Go, invalidate iterators when the underlying collection is modified during iteration. This can cause panics, infinite loops, or missed entries during cleanup.</p>\n<p><strong>How to Fix</strong>: Collect keys to delete in a separate slice during iteration, then delete them in a second pass. Never modify collections while iterating over them.</p>\n<p><strong>Correct Pattern</strong>:</p>\n<ol>\n<li>First pass: iterate and collect expired keys</li>\n<li>Second pass: delete collected keys from data structures</li>\n</ol>\n<h4 id=\"-pitfall-zero-time-comparison-errors\">⚠️ Pitfall: Zero Time Comparison Errors</h4>\n<p><strong>The Problem</strong>: Incorrectly handling the zero value of <code>time.Time</code> when checking expiration. Developers often write <code>if entry.ExpiresAt.Before(time.Now())</code> without checking for the zero value case, causing entries with no expiration to be immediately evicted.</p>\n<p><strong>Why It&#39;s Wrong</strong>: The zero value of <code>time.Time</code> represents January 1, year 1, 00:00:00 UTC, which is before any current time. This causes all entries with no TTL to appear expired during expiration checks.</p>\n<p><strong>How to Fix</strong>: Always check <code>entry.ExpiresAt.IsZero()</code> first to detect no-expiration entries, and only perform time comparison for entries with explicit TTLs.</p>\n<p><strong>Correct Pattern</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">entry.ExpiresAt.</span><span style=\"color:#B392F0\">IsZero</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> entry.ExpiresAt.</span><span style=\"color:#B392F0\">Before</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Entry is expired</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"-pitfall-linked-list-element-reuse\">⚠️ Pitfall: Linked List Element Reuse</h4>\n<p><strong>The Problem</strong>: Attempting to reuse linked list elements after they&#39;ve been removed from the list, or failing to create new elements when adding entries. This often happens when developers try to optimize memory allocations by reusing list elements.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Once a linked list element is removed from a list, its internal pointers may be corrupted or point to invalid memory locations. Reusing such elements can cause crashes or infinite loops when traversing the list.</p>\n<p><strong>How to Fix</strong>: Always create new list elements when adding entries to the cache. Never reuse elements that have been removed from the list. The Go garbage collector efficiently handles the allocation and cleanup of small objects like list elements.</p>\n<h4 id=\"-pitfall-partial-entry-updates\">⚠️ Pitfall: Partial Entry Updates</h4>\n<p><strong>The Problem</strong>: Updating some fields of a cache entry (like the value) without updating other related fields (like the size or expiration time). This creates inconsistency between the entry&#39;s metadata and actual data.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Stale size information causes memory accounting errors. Stale expiration times cause entries to expire too early or too late. These inconsistencies compound over time and can cause significant behavioral anomalies.</p>\n<p><strong>How to Fix</strong>: Always treat cache entries as immutable. When updating an entry, create a new <code>CacheEntry</code> with all correct fields rather than modifying an existing entry. This prevents partial update bugs and makes the code easier to reason about.</p>\n<h4 id=\"-pitfall-cleanup-process-blocking-operations\">⚠️ Pitfall: Cleanup Process Blocking Operations</h4>\n<p><strong>The Problem</strong>: Running the periodic cleanup process while holding the write lock for extended periods, blocking all cache operations during cleanup. This is especially problematic for large caches where cleanup might take significant time.</p>\n<p><strong>Why It&#39;s Wrong</strong>: Holding locks during long-running operations creates availability problems. All Get, Set, and Delete operations block until cleanup completes, causing user-visible latency spikes and potential timeout errors.</p>\n<p><strong>How to Fix</strong>: Batch cleanup work and periodically release the lock to allow normal operations to proceed. Process cleanup in smaller chunks with lock release between chunks, or use more sophisticated techniques like double-buffering for very large caches.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical implementation details to help translate the cache node design into working code. The guidance focuses on Go as the primary language while providing principles applicable to other languages.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash Function</td>\n<td><code>hash/fnv</code> (FNV-1a)</td>\n<td><code>crypto/sha256</code> for cryptographic properties</td>\n</tr>\n<tr>\n<td>Linked List</td>\n<td><code>container/list</code> (built-in)</td>\n<td>Custom implementation for memory optimization</td>\n</tr>\n<tr>\n<td>JSON Serialization</td>\n<td><code>encoding/json</code> (standard library)</td>\n<td><code>github.com/json-iterator/go</code> for performance</td>\n</tr>\n<tr>\n<td>Time Handling</td>\n<td><code>time</code> package (standard library)</td>\n<td><code>time</code> with monotonic clock considerations</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td><code>log</code> package (standard library)</td>\n<td><code>github.com/sirupsen/logrus</code> for structured logging</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td><code>testing</code> package (standard library)</td>\n<td><code>github.com/stretchr/testify</code> for assertions</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/cache/\n  cache.go              ← Core LRUCache implementation\n  cache_test.go         ← Unit tests for cache operations\n  entry.go              ← CacheEntry struct and methods\n  metrics.go            ← Memory usage and performance metrics\n  cleanup.go            ← TTL cleanup background process\n  \ninternal/config/\n  config.go             ← NodeConfig structure and validation\n  config_test.go        ← Configuration loading tests\n  \ncmd/cache-node/\n  main.go               ← Server entry point and initialization</code></pre></div>\n\n<p>This structure separates concerns while keeping related functionality together. The <code>internal</code> packages prevent external imports while the <code>cmd</code> package provides the executable entry point.</p>\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Configuration Management (config.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ListenAddress       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"listen_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AdvertiseAddr       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"advertise_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    JoinAddresses       []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"join_addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxMemoryMB         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"max_memory_mb\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirtualNodes        </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"virtual_nodes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicationFactor   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"replication_factor\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HealthCheckInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"health_check_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GossipInterval      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"gossip_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RequestTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"request_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"reading config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"parsing config JSON: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> config.</span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid configuration: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.NodeID </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node_id is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ListenAddress </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"listen_address is required\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.MaxMemoryMB </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"max_memory_mb must be positive, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.MaxMemoryMB)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"virtual_nodes must be positive, got </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.VirtualNodes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetDefaults fills in reasonable default values for optional fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetDefaults</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.HealthCheckInterval </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.HealthCheckInterval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.GossipInterval </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.GossipInterval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.RequestTimeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.RequestTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.ReplicationFactor </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.ReplicationFactor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.VirtualNodes </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.VirtualNodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 150</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Cache Entry Implementation (entry.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cache</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">    `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpiresAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"expires_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Size      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCacheEntry creates a cache entry with calculated size and optional TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCacheEntry</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> expiresAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ttl </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expiresAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(ttl)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Key:       key,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Value:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(value)), </span><span style=\"color:#6A737D\">// Copy to avoid external mutations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ExpiresAt: expiresAt,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Size:      </span><span style=\"color:#B392F0\">calculateEntrySize</span><span style=\"color:#E1E4E8\">(key, value),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(entry.Value, value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsExpired returns true if the entry has exceeded its TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IsExpired</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> e.ExpiresAt.</span><span style=\"color:#B392F0\">IsZero</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#6A737D\"> // No expiration set</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">After</span><span style=\"color:#E1E4E8\">(e.ExpiresAt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// calculateEntrySize estimates total memory usage including metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> calculateEntrySize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Key string + value slice + CacheEntry struct + list element + map overhead</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(key) </span><span style=\"color:#F97583\">+</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(value) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 96</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// 96 bytes estimated metadata overhead</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Metrics Collection (metrics.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cache</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheMetrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hits           </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    misses         </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sets           </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    deletes        </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    evictions      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expiredCleanup </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    totalMemory    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entryCount     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCacheMetrics</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        startTime: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordHit</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.hits, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordMiss</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.misses, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordSet</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.sets, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordDelete</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.deletes, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordEviction</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.evictions, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordExpiredCleanup</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.expiredCleanup, count)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateMemory</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">used</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">entries</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">StoreInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.totalMemory, used)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">StoreInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.entryCount, entries)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetStats</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hits </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.hits)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    misses </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.misses)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hits </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> misses</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> hitRate </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> total </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hitRate </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(hits) </span><span style=\"color:#F97583\">/</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(total)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"hit_rate\"</span><span style=\"color:#E1E4E8\">:         hitRate,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"total_requests\"</span><span style=\"color:#E1E4E8\">:   total,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"hits\"</span><span style=\"color:#E1E4E8\">:            hits,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"misses\"</span><span style=\"color:#E1E4E8\">:          misses,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"sets\"</span><span style=\"color:#E1E4E8\">:            atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.sets),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"deletes\"</span><span style=\"color:#E1E4E8\">:         atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.deletes),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"evictions\"</span><span style=\"color:#E1E4E8\">:       atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.evictions),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"expired_cleanup\"</span><span style=\"color:#E1E4E8\">: atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.expiredCleanup),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"memory_used\"</span><span style=\"color:#E1E4E8\">:     atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.totalMemory),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"entry_count\"</span><span style=\"color:#E1E4E8\">:     atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m.entryCount),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"uptime_seconds\"</span><span style=\"color:#E1E4E8\">:  time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(m.startTime).</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-cache-implementation-skeleton\">Core Cache Implementation Skeleton</h4>\n<p><strong>LRU Cache Structure (cache.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> cache</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">container/list</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LRUCache</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capacity </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    used     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    items    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    order    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">List</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLRUCache creates an LRU cache with the specified memory capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLRUCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">capacityBytes</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        capacity: capacityBytes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        used:     </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        items:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        order:    list.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics:  </span><span style=\"color:#B392F0\">NewCacheMetrics</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Get retrieves a value from the cache and updates its position in LRU order</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock to safely access cache structures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Look up the key in the items map to get list element</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If key not found, record miss and return false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Extract CacheEntry from list element value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check if entry is expired using IsExpired() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If expired, upgrade to write lock and remove entry completely</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: If valid, upgrade to write lock and move element to front</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Record hit in metrics and return value copy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Always release locks in defer statements for safety</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Set stores a value in the cache with optional TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create new CacheEntry with calculated size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Acquire write lock for exclusive access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If key already exists, remove old entry from accounting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check if new entry fits within capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: While over capacity, evict LRU entries from back of list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create new list element and add to front (MRU position)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Add key->element mapping to items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update memory usage accounting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Record set operation in metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Delete removes a key from the cache if it exists</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock for exclusive access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Look up key in items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If not found, record delete attempt and return false</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove element from linked list using list.Remove()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Delete key from items map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update memory usage by subtracting entry size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Record successful delete in metrics and return true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupExpired removes expired entries and returns count cleaned</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CleanupExpired</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock for exclusive cleanup access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create slice to collect expired entry keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Iterate through linked list from back (LRU) to front</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each entry, check if expired using IsExpired()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If expired, add key to collection slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If non-expired found, break early (optimization)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Remove all collected keys from map, list, and accounting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update metrics and return count of cleaned entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evictLRU removes the least recently used entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evictLRU</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if list has any elements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get back element (least recently used)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Extract CacheEntry and remove from all structures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update memory accounting and metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return true if eviction occurred</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMetrics returns current cache performance statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMetrics</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.metrics.</span><span style=\"color:#B392F0\">UpdateMemory</span><span style=\"color:#E1E4E8\">(c.used, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(c.items)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> c.metrics.</span><span style=\"color:#B392F0\">GetStats</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Go-Specific Optimizations:</strong></p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for reader-writer concurrency patterns</li>\n<li>Copy byte slices with <code>make()</code> and <code>copy()</code> to prevent external mutations</li>\n<li>Use <code>atomic</code> package for metrics counters to avoid lock contention</li>\n<li>Leverage <code>time.Time.IsZero()</code> for TTL handling edge cases</li>\n<li>Use <code>defer</code> statements for reliable lock cleanup</li>\n</ul>\n<p><strong>Memory Management Best Practices:</strong></p>\n<ul>\n<li>Pre-allocate slices with known capacity using <code>make([]string, 0, expectedSize)</code></li>\n<li>Avoid string concatenation in hot paths; use <code>strings.Builder</code> instead</li>\n<li>Consider using <code>sync.Pool</code> for frequently allocated temporary objects</li>\n<li>Profile memory usage with <code>go tool pprof</code> to identify allocation hotspots</li>\n</ul>\n<p><strong>Concurrency Patterns:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Lock upgrade pattern for Get operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">element, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.items[key]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Upgrade to write lock for modification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">defer</span><span style=\"color:#E1E4E8\"> c.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Re-check existence after lock upgrade</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> element, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.items[key]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.order.</span><span style=\"color:#B392F0\">MoveToFront</span><span style=\"color:#E1E4E8\">(element)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ... continue with operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the cache node, verify functionality with these checkpoints:</p>\n<p><strong>Unit Test Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> internal/cache</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#9ECBFF\"> ./...</span></span></code></pre></div>\n<p>Expected output should show all tests passing with race detection enabled.</p>\n<p><strong>Manual Testing Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start cache node</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/cache-node/main.go</span><span style=\"color:#79B8FF\"> -config=config.json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test basic operations (implement simple HTTP handlers)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/cache/test</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"value\":\"hello\",\"ttl\":\"60s\"}'</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> http://localhost:8080/cache/test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> DELETE</span><span style=\"color:#9ECBFF\"> http://localhost:8080/cache/test</span></span></code></pre></div>\n\n<p><strong>Performance Verification:</strong>\nRun benchmark tests to ensure O(1) operation performance:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -bench=.</span><span style=\"color:#79B8FF\"> -benchmem</span><span style=\"color:#9ECBFF\"> ./internal/cache/</span></span></code></pre></div>\n<p>Look for consistent operation times regardless of cache size and reasonable memory allocations per operation.</p>\n<p><strong>Memory Limit Testing:</strong>\nConfigure a small memory limit (e.g., 10MB) and verify eviction behavior:</p>\n<ol>\n<li>Insert entries until memory limit reached</li>\n<li>Verify oldest entries are evicted first</li>\n<li>Confirm memory usage stays within configured bounds</li>\n<li>Test TTL cleanup reduces memory usage over time</li>\n</ol>\n<p><strong>Signs of Correct Implementation:</strong></p>\n<ul>\n<li>Cache operations complete in constant time regardless of cache size</li>\n<li>Memory usage accurately tracks configured limits with eviction</li>\n<li>Expired entries are never returned and are cleaned up periodically</li>\n<li>Concurrent access works without race conditions (verified with <code>-race</code> flag)</li>\n<li>Metrics accurately reflect cache behavior and performance</li>\n</ul>\n<h2 id=\"cluster-communication-and-discovery\">Cluster Communication and Discovery</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section covers Milestone 3 (Cluster Communication), building on the hash ring foundation from Milestone 1 and cache node implementation from Milestone 2 to enable distributed coordination and fault tolerance.</p>\n</blockquote>\n<p>The third milestone transforms our distributed cache from individual nodes into a coordinated cluster. While the previous milestones established how to distribute keys across nodes and implement efficient storage, this milestone addresses the fundamental challenge of distributed systems: how do nodes discover each other, maintain awareness of cluster membership, detect failures, and coordinate their activities without a central authority?</p>\n<p>The core architectural principle driving this milestone is <strong>peer-to-peer cluster design</strong> with <strong>symmetric responsibility</strong>. Every node in the cluster has equal status and capability—there are no dedicated coordinator nodes, no single points of failure, and no hierarchical relationships. This design choice provides excellent fault tolerance and horizontal scalability, but introduces the complexity of achieving consensus about cluster state through distributed protocols.</p>\n<h3 id=\"mental-model-the-neighborhood-watch\">Mental Model: The Neighborhood Watch</h3>\n<p>Think of the cluster communication system as a <strong>neighborhood watch program</strong> where residents (nodes) keep track of who lives in the neighborhood, monitor each other&#39;s wellbeing, and share important information through a network of conversations.</p>\n<p>In this analogy, each house represents a cache node, and the residents need to maintain awareness of their neighborhood without relying on a central authority like a homeowners association office. Here&#39;s how the parallel works:</p>\n<p><strong>Node Discovery</strong> is like new residents introducing themselves to the neighborhood. When someone moves in, they walk around and introduce themselves to their immediate neighbors. Those neighbors then mention the newcomer during their regular conversations with other neighbors, and gradually everyone in the neighborhood learns about the new resident. No one needs to maintain a master directory—the information spreads naturally through the social network.</p>\n<p><strong>Health Checking</strong> resembles neighbors looking out for each other&#39;s wellbeing. If Mrs. Johnson usually takes her morning walk but hasn&#39;t been seen for several days, her immediate neighbors become concerned. They might knock on her door or peer over the fence to check if everything is okay. If she doesn&#39;t respond, they spread word through the neighborhood that something might be wrong with Mrs. Johnson.</p>\n<p><strong>Gossip Protocol</strong> mirrors the way information spreads through casual conversations. When neighbors chat over the fence or meet at the mailbox, they naturally share news: &quot;Did you hear the Smiths are moving out next month?&quot; or &quot;I haven&#39;t seen Bob from number 47 in days—hope he&#39;s okay.&quot; This information doesn&#39;t travel in a structured broadcast; instead, it spreads organically as neighbors talk to each other, with each person sharing the most interesting or concerning updates they&#39;ve heard.</p>\n<p><strong>Request Routing</strong> is like knowing which neighbor to ask for help with specific problems. If you need to borrow a power drill, you don&#39;t knock on every door—you know that Tom three houses down is the tool guy, so you go directly to him. Similarly, when a cache request arrives for a key, the receiving node needs to know which node in the cluster is responsible for that key and either handle it locally or forward it to the correct neighbor.</p>\n<p>The beauty of this neighborhood watch model is its resilience. If one neighbor goes on vacation (node failure), the others continue sharing information and looking out for each other. If several new families move in simultaneously (multiple nodes joining), the existing social network naturally expands to include them. There&#39;s no single point of failure because the system depends on the collective behavior of the community, not on any individual member.</p>\n<p>This mental model helps explain why distributed coordination is more complex than centralized approaches—achieving consensus through informal social networks requires more sophisticated protocols than simply checking with a central authority—but also why it&#39;s more robust and scalable in the long run.</p>\n<h3 id=\"node-discovery-mechanism\">Node Discovery Mechanism</h3>\n<p>Node discovery solves the bootstrap problem: how does a new node find and join an existing cluster, and how do existing cluster members learn about the newcomer? The challenge is particularly acute in dynamic environments where nodes can join and leave frequently, IP addresses might change, and there&#39;s no guarantee that any particular node will always be available to serve as an entry point.</p>\n<p>Our discovery mechanism uses a <strong>bootstrap-then-gossip</strong> approach that combines initial contact with organic information propagation. When a new node starts up, it needs at least one existing cluster member to contact—this is provided through the <code>JoinAddresses</code> configuration parameter. However, once the initial contact is made, all further discovery happens through the gossip protocol, making the system resilient to changes in the bootstrap nodes.</p>\n<p>The discovery process follows this sequence:</p>\n<ol>\n<li><p><strong>Bootstrap Contact Phase</strong>: The new node attempts to contact each address in its <code>JoinAddresses</code> list until it successfully reaches an active cluster member. This contact serves two purposes: it announces the new node&#39;s presence to the cluster, and it downloads the current cluster membership information.</p>\n</li>\n<li><p><strong>Membership Synchronization</strong>: The contacted node responds with its current view of cluster membership, including all known nodes, their addresses, current health status, and version information. This gives the new node an immediate snapshot of the cluster topology.</p>\n</li>\n<li><p><strong>Cluster Announcement</strong>: The contacted node updates its local membership view to include the new node and begins propagating this information through the gossip protocol. The announcement includes the new node&#39;s identity, address, capabilities, and initial health status.</p>\n</li>\n<li><p><strong>Gossip Integration</strong>: As gossip messages propagate through the cluster, all nodes learn about the newcomer within a few gossip cycles. The new node also begins participating in the gossip protocol, both sending and receiving membership updates.</p>\n</li>\n<li><p><strong>Hash Ring Integration</strong>: Once cluster members are aware of the new node, they update their consistent hash rings to include it. This triggers key redistribution as some keys now map to the new node instead of their previous owners.</p>\n</li>\n</ol>\n<p>The discovery mechanism must handle several challenging scenarios. <strong>Split-brain recovery</strong> occurs when network partitions heal and previously isolated sub-clusters need to merge their membership views. <strong>Concurrent joins</strong> happen when multiple nodes attempt to join simultaneously, potentially causing inconsistent membership states. <strong>Bootstrap node failures</strong> require the system to continue functioning even if some or all of the original bootstrap addresses become unavailable.</p>\n<p>To address these challenges, the discovery mechanism implements several sophisticated behaviors:</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Detection</th>\n<th>Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Split-brain merge</td>\n<td>Gossip messages contain unfamiliar node IDs</td>\n<td>Merge membership lists, resolve conflicts using node startup timestamps</td>\n</tr>\n<tr>\n<td>Concurrent joins</td>\n<td>Multiple membership updates with overlapping timestamps</td>\n<td>Apply updates in deterministic order based on node ID lexicographic sorting</td>\n</tr>\n<tr>\n<td>Bootstrap failure</td>\n<td>Connection timeouts during initial contact</td>\n<td>Try remaining bootstrap addresses, continue with partial cluster view if any succeed</td>\n</tr>\n<tr>\n<td>Duplicate joins</td>\n<td>Node ID appears twice with different addresses</td>\n<td>Keep entry with most recent timestamp, mark older entry as stale</td>\n</tr>\n<tr>\n<td>Address changes</td>\n<td>Node appears with new address but same ID</td>\n<td>Update address, increment version number, propagate change via gossip</td>\n</tr>\n</tbody></table>\n<p>The membership information maintained by each node includes comprehensive state about every cluster member:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NodeID</td>\n<td>string</td>\n<td>Unique identifier for the node, typically a UUID generated at startup</td>\n</tr>\n<tr>\n<td>Address</td>\n<td>string</td>\n<td>Network address (IP:port) where the node can be contacted</td>\n</tr>\n<tr>\n<td>Status</td>\n<td>string</td>\n<td>Current health state: &quot;active&quot;, &quot;suspected&quot;, &quot;failed&quot;, &quot;leaving&quot;</td>\n</tr>\n<tr>\n<td>LastSeen</td>\n<td>time.Time</td>\n<td>Timestamp of most recent successful communication with this node</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint64</td>\n<td>Monotonically increasing counter for detecting stale information</td>\n</tr>\n<tr>\n<td>Capabilities</td>\n<td>map[string]string</td>\n<td>Node-specific metadata like supported protocols or capacity limits</td>\n</tr>\n<tr>\n<td>JoinTime</td>\n<td>time.Time</td>\n<td>When this node first joined the cluster (used for split-brain resolution)</td>\n</tr>\n</tbody></table>\n<p>The discovery mechanism also implements <strong>lazy cleanup</strong> of stale membership information. Nodes that haven&#39;t been seen for extended periods are gradually demoted from &quot;suspected&quot; to &quot;failed&quot; status, and eventually removed from the membership list entirely. This prevents unbounded growth of the membership table in long-running clusters with high node turnover.</p>\n<h3 id=\"health-checking-and-failure-detection\">Health Checking and Failure Detection</h3>\n<p>Failure detection in distributed systems faces the fundamental challenge of distinguishing between node failures and network partitions. When node A cannot contact node B, it&#39;s impossible to determine with certainty whether B has crashed or if the network path between them has failed. Our health checking system addresses this ambiguity through a combination of <strong>direct probing</strong> and <strong>indirect confirmation</strong> that provides rapid failure detection while minimizing false positives.</p>\n<p>The health checking mechanism operates on multiple time scales to balance responsiveness with stability. <strong>Fast detection</strong> happens through direct health probes sent every <code>HealthCheckInterval</code> (typically 1-2 seconds). <strong>Slow confirmation</strong> occurs through gossip message correlation over longer time periods (typically 30-60 seconds). This dual-layer approach allows the system to quickly suspect failures while avoiding hasty decisions that could cause unnecessary churn.</p>\n<p>Each node maintains a <strong>failure detector state machine</strong> for every other cluster member:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Probe Result</th>\n<th>Gossip Evidence</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Active</td>\n<td>Success</td>\n<td>Recent sightings</td>\n<td>Active</td>\n<td>Reset failure counters, update LastSeen timestamp</td>\n</tr>\n<tr>\n<td>Active</td>\n<td>Timeout</td>\n<td>Recent sightings</td>\n<td>Active</td>\n<td>Increment timeout counter, suspect if threshold exceeded</td>\n</tr>\n<tr>\n<td>Active</td>\n<td>Timeout</td>\n<td>No recent sightings</td>\n<td>Suspected</td>\n<td>Mark as suspected, begin indirect confirmation attempts</td>\n</tr>\n<tr>\n<td>Suspected</td>\n<td>Success</td>\n<td>Recent sightings</td>\n<td>Active</td>\n<td>Clear suspicion, restore to active status</td>\n</tr>\n<tr>\n<td>Suspected</td>\n<td>Timeout</td>\n<td>No recent sightings</td>\n<td>Suspected</td>\n<td>Continue indirect probing, fail if timeout threshold exceeded</td>\n</tr>\n<tr>\n<td>Suspected</td>\n<td>Timeout</td>\n<td>Confirmed by others</td>\n<td>Failed</td>\n<td>Mark as failed, begin gossip propagation of failure</td>\n</tr>\n<tr>\n<td>Failed</td>\n<td>Success</td>\n<td>Recent sightings</td>\n<td>Active</td>\n<td>Node has recovered, restore to active status</td>\n</tr>\n<tr>\n<td>Failed</td>\n<td>Any</td>\n<td>No activity</td>\n<td>Failed</td>\n<td>Maintain failed status, consider removal from membership</td>\n</tr>\n</tbody></table>\n<p>The <strong>direct probing</strong> mechanism sends lightweight health check messages to other cluster members at regular intervals. These probes serve multiple purposes beyond simple liveness detection. They carry <strong>piggybacked information</strong> including recent gossip updates, local load metrics, and membership version numbers. This approach maximizes the value of each network round-trip and helps keep cluster state synchronized.</p>\n<p>The health check message format includes diagnostic information that helps with failure attribution:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ProbeID</td>\n<td>string</td>\n<td>Unique identifier for tracking probe round-trips</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When the probe was sent (for latency measurement)</td>\n</tr>\n<tr>\n<td>SenderLoad</td>\n<td>float64</td>\n<td>Current CPU/memory utilization of sending node</td>\n</tr>\n<tr>\n<td>MembershipVersion</td>\n<td>uint64</td>\n<td>Sender&#39;s current membership list version</td>\n</tr>\n<tr>\n<td>RecentFailures</td>\n<td>[]string</td>\n<td>Nodes the sender has recently marked as failed</td>\n</tr>\n<tr>\n<td>PiggybackGossip</td>\n<td>GossipMessage</td>\n<td>Recent gossip updates to synchronize cluster state</td>\n</tr>\n</tbody></table>\n<p><strong>Indirect confirmation</strong> activates when direct probes begin failing. Rather than immediately marking a node as failed, the detecting node asks several other cluster members to probe the suspected node independently. If multiple nodes report probe failures, confidence in the failure assessment increases. If some nodes can still reach the suspected node, this suggests a network partition rather than a node failure, and the detector remains in the &quot;suspected&quot; state rather than escalating to &quot;failed.&quot;</p>\n<p>The indirect confirmation process uses a <strong>witness selection algorithm</strong> that chooses probe witnesses based on network diversity. Witnesses are selected from different network segments when possible, reducing the likelihood that a localized network issue will cause false failure detection. The witness selection considers factors like network latency patterns, recent communication success rates, and physical topology hints when available.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: The failure detection system optimizes for <strong>completeness over accuracy</strong>—it&#39;s better to occasionally suspect a healthy node (false positive) than to miss the failure of a crashed node (false negative). False positives cause temporary performance degradation as the cluster works around a supposedly failed node, but false negatives can cause data loss or extended unavailability as the cluster continues trying to use a failed node.</p>\n</blockquote>\n<p>The health checking system implements <strong>adaptive timing</strong> that adjusts probe intervals based on cluster stability. During stable periods with no recent failures, probe intervals can be lengthened to reduce network overhead. During unstable periods with recent failures or frequent membership changes, probe intervals are shortened to improve detection speed. This adaptivity helps the system scale efficiently while maintaining responsiveness during critical periods.</p>\n<p>Failure detection also integrates with the consistent hash ring to minimize disruption during suspected failures. When a node enters the &quot;suspected&quot; state, the hash ring temporarily routes traffic away from it while leaving it in the ring structure. This provides <strong>graceful degradation</strong>—if the suspicion is false, the node can quickly resume normal operation without requiring expensive ring restructuring. Only when a node is confirmed as &quot;failed&quot; does the hash ring undergo the costly process of removing the node and redistributing its keys.</p>\n<h3 id=\"request-routing-logic\">Request Routing Logic</h3>\n<p>Request routing transforms the distributed cache from a collection of independent nodes into a unified system that appears as a single logical cache to clients. The routing system must seamlessly direct each cache operation to the appropriate node based on consistent hashing while handling the complexities of node failures, network partitions, and cluster membership changes.</p>\n<p>The routing architecture follows a <strong>symmetric proxy model</strong> where every node can accept requests for any key, regardless of which node actually stores the data. This design provides excellent load distribution and eliminates single points of failure, but requires sophisticated forwarding logic to ensure requests reach the correct destination with minimal latency and overhead.</p>\n<p>When a client request arrives at any node, the routing system follows this decision process:</p>\n<ol>\n<li><p><strong>Hash Ring Lookup</strong>: The receiving node computes the hash of the request key and queries its local hash ring to determine which node should handle this key. This lookup considers the current cluster membership and any ongoing rebalancing operations.</p>\n</li>\n<li><p><strong>Local vs. Remote Determination</strong>: If the hash ring indicates the current node is responsible for the key, the request is handled locally using the node&#39;s <code>LRUCache</code>. If another node is responsible, the request requires forwarding.</p>\n</li>\n<li><p><strong>Target Node Health Check</strong>: Before forwarding, the routing system verifies that the target node is currently healthy and reachable. If the target is suspected or failed, the router consults the hash ring for the next available replica node.</p>\n</li>\n<li><p><strong>Forward or Handle Locally</strong>: For remote requests, the router serializes the request and sends it to the target node using the <code>HTTPTransport</code>. For local requests, it directly invokes the appropriate cache operation.</p>\n</li>\n<li><p><strong>Response Processing</strong>: Remote responses are deserialized and returned to the client. Local responses are returned directly. Both paths include error handling for timeouts, serialization failures, and data consistency issues.</p>\n</li>\n</ol>\n<p>The routing system maintains <strong>request correlation state</strong> to handle timeouts, retries, and partial failures gracefully:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>RequestID</td>\n<td>string</td>\n<td>Unique identifier for tracking requests across nodes</td>\n</tr>\n<tr>\n<td>ClientAddress</td>\n<td>string</td>\n<td>Original client address for routing responses</td>\n</tr>\n<tr>\n<td>OriginalTimestamp</td>\n<td>time.Time</td>\n<td>When the request first entered the cluster</td>\n</tr>\n<tr>\n<td>ForwardPath</td>\n<td>[]string</td>\n<td>Nodes that have forwarded this request (prevents loops)</td>\n</tr>\n<tr>\n<td>TimeoutDeadline</td>\n<td>time.Time</td>\n<td>When to abandon the request and return an error</td>\n</tr>\n<tr>\n<td>RetryCount</td>\n<td>int</td>\n<td>Number of retry attempts made for this request</td>\n</tr>\n<tr>\n<td>ConsistencyLevel</td>\n<td>string</td>\n<td>Required consistency level for the operation</td>\n</tr>\n</tbody></table>\n<p>One of the most challenging aspects of request routing is handling <strong>cluster membership changes</strong> during active requests. Consider a scenario where a client sends a GET request for key &quot;user:12345&quot; to node A, which forwards it to node B based on its current hash ring view. However, while the request is in transit, node B fails and is removed from the hash ring, causing key &quot;user:12345&quot; to now map to node C. The routing system must detect this situation and adapt gracefully.</p>\n<p>To handle membership changes, the routing system implements several sophisticated strategies:</p>\n<p><strong>Versioned Ring Consistency</strong>: Each request carries the hash ring version number from the originating node. The target node compares this version with its current ring version. If the versions differ significantly, the target node can detect that a membership change has occurred and potentially invalidate the routing decision.</p>\n<p><strong>Graceful Redirect</strong>: When a node receives a request for a key it no longer owns (due to recent ring changes), it doesn&#39;t immediately return an error. Instead, it consults its current hash ring, determines the new correct node, and either forwards the request automatically or returns a redirect response to the client.</p>\n<p><strong>Temporal Overlap Handling</strong>: During node additions or removals, there&#39;s a brief period where multiple nodes might legitimately handle requests for the same keys. The routing system uses timestamp-based conflict resolution to ensure consistency during these transition periods.</p>\n<p>The request forwarding mechanism implements <strong>circuit breaker</strong> patterns to prevent cascading failures when target nodes become overloaded or unresponsive:</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Trigger Condition</th>\n<th>Forwarding Behavior</th>\n<th>Recovery Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>Normal operation</td>\n<td>Forward all requests normally</td>\n<td>N/A - healthy operation</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Some failures detected</td>\n<td>Forward limited test requests</td>\n<td>Several successful requests</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>High failure rate reached</td>\n<td>Return errors immediately</td>\n<td>Timeout period expires</td>\n</tr>\n</tbody></table>\n<p><strong>Loop Prevention</strong> is critical in the symmetric proxy model since requests can potentially bounce between nodes indefinitely if there are inconsistencies in hash ring views. The routing system prevents loops through several mechanisms:</p>\n<ul>\n<li><strong>Hop Count Limiting</strong>: Each request includes a maximum hop count that decreases with each forward. Requests are abandoned if the hop limit is reached.</li>\n<li><strong>Forward Path Tracking</strong>: The <code>ForwardPath</code> field records which nodes have handled the request, preventing cycles.</li>\n<li><strong>Ring Version Propagation</strong>: Nodes with stale ring views update their membership information when they receive requests with newer version numbers.</li>\n</ul>\n<p>The routing system also optimizes for <strong>request locality</strong> by implementing intelligent caching of routing decisions. Frequently accessed keys have their routing information cached locally, reducing the overhead of hash ring lookups for hot data. This cache is invalidated appropriately when membership changes occur, ensuring correctness while improving performance for stable workloads.</p>\n<p>For <strong>batch operations</strong> and <strong>multi-key requests</strong>, the routing system implements request splitting and parallel execution. A client request involving multiple keys is automatically divided into sub-requests based on the hash ring distribution, sent to the appropriate nodes in parallel, and the results are merged before returning to the client. This provides significant performance benefits for workloads with spatial locality in their key access patterns.</p>\n<h3 id=\"gossip-protocol-implementation\">Gossip Protocol Implementation</h3>\n<p>The gossip protocol serves as the nervous system of the distributed cache, propagating cluster membership information, failure notifications, and topology changes throughout the network without requiring centralized coordination. Like its biological namesake, gossip spreads information through periodic, random exchanges between cluster members, ensuring that important updates eventually reach all nodes while providing remarkable resilience to network failures and partitions.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fgossip-protocol-flow.svg\" alt=\"Gossip Protocol Flow\"></p>\n<p>The protocol operates on the principle of <strong>eventual consistency</strong>—there&#39;s no guarantee that all nodes have identical membership views at any given moment, but the system converges toward consensus over time. This relaxed consistency model enables high availability and partition tolerance while maintaining sufficient coordination for the cache to function correctly.</p>\n<p>Each node maintains a <strong>membership table</strong> that represents its current view of the cluster state. This table is continuously updated through gossip exchanges and serves as the source of truth for local routing decisions. The table includes not only basic membership information but also <strong>version vectors</strong> and <strong>conflict resolution metadata</strong> that enable deterministic reconciliation of divergent views.</p>\n<p>The gossip message structure carries comprehensive state information that enables recipient nodes to update their local views efficiently:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SenderID</td>\n<td>string</td>\n<td>Identity of the node sending this gossip message</td>\n</tr>\n<tr>\n<td>MessageID</td>\n<td>string</td>\n<td>Unique identifier for deduplication and tracking</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>When this gossip message was generated</td>\n</tr>\n<tr>\n<td>MembershipVersion</td>\n<td>uint64</td>\n<td>Sender&#39;s current version of the membership table</td>\n</tr>\n<tr>\n<td>NodeStates</td>\n<td>map[string]NodeState</td>\n<td>Complete or partial membership information</td>\n</tr>\n<tr>\n<td>RecentChanges</td>\n<td>[]MembershipEvent</td>\n<td>Recent join/leave/failure events with timestamps</td>\n</tr>\n<tr>\n<td>Checksum</td>\n<td>string</td>\n<td>Hash of the membership data for integrity verification</td>\n</tr>\n</tbody></table>\n<p>The <strong>gossip scheduling algorithm</strong> determines when to send gossip messages and which nodes to contact. The system uses a combination of <strong>periodic gossip</strong> (sent at regular intervals) and <strong>triggered gossip</strong> (sent immediately after important events like failures or joins). The target selection balances randomness with connectivity optimization—nodes prefer to gossip with peers they haven&#39;t communicated with recently, ensuring information spreads throughout the cluster rather than circulating within small subgroups.</p>\n<blockquote>\n<p><strong>Design Decision: Push vs. Pull Gossip</strong></p>\n<ul>\n<li><strong>Context</strong>: Gossip protocols can be push-based (nodes send updates proactively), pull-based (nodes request updates from others), or hybrid approaches.</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Pure push: Simple implementation, but may miss nodes or flood the network</li>\n<li>Pure pull: More bandwidth efficient, but slower propagation of urgent updates  </li>\n<li>Push-pull hybrid: Best of both approaches, but increased complexity</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Hybrid push-pull with adaptive behavior based on cluster stability</li>\n<li><strong>Rationale</strong>: Push provides rapid dissemination of critical events like failures, while pull ensures eventual consistency and handles network partitions gracefully. Adaptive behavior optimizes bandwidth usage during stable periods.</li>\n<li><strong>Consequences</strong>: More complex implementation but superior performance characteristics and partition resilience</li>\n</ul>\n</blockquote>\n<p>The gossip protocol implements <strong>anti-entropy</strong> mechanisms that detect and repair inconsistencies in membership views. When two nodes exchange gossip messages, they compare their membership versions and node state information. If discrepancies are discovered, the nodes engage in a <strong>state reconciliation process</strong> that merges their views using deterministic conflict resolution rules.</p>\n<p>State reconciliation follows these priority rules:</p>\n<ol>\n<li><strong>Timestamp Ordering</strong>: More recent updates override older ones, based on the timestamp associated with each membership change</li>\n<li><strong>Node Self-Authority</strong>: Nodes are authoritative about their own state—if node A claims to be active and node B claims A is failed, node A&#39;s claim takes precedence if A is reachable</li>\n<li><strong>Majority Consensus</strong>: For conflicting claims about third-party nodes, the view supported by more cluster members prevails</li>\n<li><strong>Lexicographic Tiebreaking</strong>: When timestamps are identical, node IDs are compared lexicographically to ensure deterministic outcomes</li>\n</ol>\n<p>The protocol handles <strong>network partitions</strong> through sophisticated partition detection and healing mechanisms. When nodes lose contact with portions of the cluster, they continue operating with their local membership view while marking unreachable nodes as &quot;suspected.&quot; The gossip protocol includes <strong>partition identifiers</strong> that help detect when network connectivity is restored and previously isolated sub-clusters need to merge their state.</p>\n<p><strong>Partition healing</strong> follows a careful process to prevent data inconsistencies and membership conflicts:</p>\n<ol>\n<li><strong>Reachability Testing</strong>: When a previously unreachable node becomes contactable again, the local node performs comprehensive reachability tests to verify connectivity is stable</li>\n<li><strong>State Exchange</strong>: The nodes exchange their complete membership tables and identify all discrepancies that accumulated during the partition</li>\n<li><strong>Conflict Resolution</strong>: Membership conflicts are resolved using the standard reconciliation rules, with special handling for nodes that may have failed and recovered during the partition</li>\n<li><strong>Ring Synchronization</strong>: Once membership agreement is reached, both nodes update their hash rings and begin redistributing keys as necessary</li>\n</ol>\n<p>The gossip protocol incorporates <strong>epidemic dissemination</strong> characteristics that provide mathematical guarantees about information propagation speed. Under normal conditions, gossip messages reach all cluster members within O(log N) gossip rounds, where N is the cluster size. This logarithmic propagation time means the protocol scales efficiently even to very large clusters.</p>\n<p><strong>Bandwidth optimization</strong> is critical for gossip protocols since naive implementations can generate excessive network traffic. Our implementation uses several techniques to minimize overhead:</p>\n<ul>\n<li><strong>Delta Compression</strong>: Instead of sending complete membership tables, nodes send only the changes since their last communication with each peer</li>\n<li><strong>Piggybacking</strong>: Gossip information is attached to other messages like health checks and client responses when possible</li>\n<li><strong>Adaptive Frequency</strong>: Gossip intervals are increased during stable periods and decreased during cluster changes</li>\n<li><strong>Message Batching</strong>: Multiple membership updates are combined into single gossip messages when they occur in rapid succession</li>\n</ul>\n<p>The protocol also implements <strong>failure amplification</strong> to ensure that critical events like node failures propagate rapidly throughout the cluster. When a node detects a failure, it immediately sends gossip messages to multiple peers rather than waiting for the next scheduled gossip round. These urgent messages are marked with high priority and trigger additional gossip rounds at the recipients, creating an epidemic of failure notification that reaches all cluster members quickly.</p>\n<p><strong>Gossip message ordering</strong> presents challenges since messages can arrive out of order due to network delays and routing variations. The protocol uses <strong>vector clocks</strong> to establish causal ordering of membership events, ensuring that updates are applied in the correct sequence even when messages arrive out of order. Each node maintains a vector clock that tracks the logical time of membership changes, and gossip messages include vector clock information that enables proper sequencing at recipients.</p>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The cluster communication system required numerous architectural decisions that significantly impact system behavior, performance, and operational characteristics. Each decision involved careful evaluation of alternatives and trade-offs, with the choices optimized for the specific requirements of a distributed cache workload.</p>\n<blockquote>\n<p><strong>Decision: Transport Protocol Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Nodes need reliable, efficient communication for gossip, health checks, and request forwarding across potentially unreliable networks</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>HTTP/1.1 with JSON: Universal compatibility, simple debugging, but higher overhead</li>\n<li>HTTP/2 with Protocol Buffers: Better performance, multiplexing, but increased complexity</li>\n<li>Custom TCP protocol: Maximum efficiency, full control, but significant implementation effort</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: HTTP/1.1 with JSON for initial implementation, with pluggable transport interface</li>\n<li><strong>Rationale</strong>: HTTP/1.1 provides excellent debugging capabilities, universal firewall compatibility, and rapid development velocity. JSON offers human-readable message formats that simplify troubleshooting. Performance overhead is acceptable for cache workloads where network latency is typically much higher than serialization costs.</li>\n<li><strong>Consequences</strong>: Slightly higher CPU and bandwidth usage compared to binary protocols, but significantly easier operational troubleshooting and broader deployment compatibility</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Transport Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP/1.1 + JSON</td>\n<td>Universal compatibility, easy debugging, simple implementation</td>\n<td>Higher overhead, limited multiplexing</td>\n<td>✓ Yes</td>\n</tr>\n<tr>\n<td>HTTP/2 + Protobuf</td>\n<td>Better performance, multiplexing, type safety</td>\n<td>Complex debugging, additional dependencies</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Custom TCP</td>\n<td>Maximum efficiency, full control over protocol</td>\n<td>High implementation cost, difficult debugging</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Discovery Mechanism Design</strong></p>\n<ul>\n<li><strong>Context</strong>: New nodes need to locate and join existing clusters without requiring centralized directory services or complex orchestration</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Static configuration with full node lists: Simple but brittle in dynamic environments</li>\n<li>Multicast discovery: Automatic but limited to single network segments</li>\n<li>Bootstrap nodes with gossip propagation: Hybrid approach balancing simplicity and flexibility</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Bootstrap node list with gossip-based propagation and automatic failover</li>\n<li><strong>Rationale</strong>: Bootstrap nodes provide reliable entry points for cluster joining while gossip propagation eliminates long-term dependencies on bootstrap nodes. This approach works across network segments and provides resilience against bootstrap node failures.</li>\n<li><strong>Consequences</strong>: Requires initial configuration of bootstrap addresses, but provides excellent long-term stability and supports dynamic cluster membership</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Failure Detection Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Distinguish between node failures and network partitions while balancing detection speed against false positive rates</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Simple timeout-based detection: Fast but prone to false positives during network congestion</li>\n<li>Consensus-based detection: Accurate but slow and complex to implement correctly</li>\n<li>Hybrid approach with suspicion states: Balances speed and accuracy with moderate complexity</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Multi-stage failure detection with direct probing, suspicion states, and indirect confirmation</li>\n<li><strong>Rationale</strong>: Suspicion states allow rapid response to potential failures while indirect confirmation reduces false positives. This approach provides the responsiveness needed for cache workloads while maintaining cluster stability.</li>\n<li><strong>Consequences</strong>: More complex state management but significantly better balance between detection speed and accuracy</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Gossip Protocol Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Propagate cluster membership changes efficiently while handling network partitions and maintaining eventual consistency</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Flooding-based propagation: Simple but generates excessive network traffic</li>\n<li>Tree-based propagation: Efficient but fragile to node failures</li>\n<li>Epidemic gossip with random peer selection: Robust and scalable but slower convergence</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Epidemic gossip with intelligent peer selection and adaptive timing</li>\n<li><strong>Rationale</strong>: Epidemic protocols provide excellent partition tolerance and scale logarithmically with cluster size. Intelligent peer selection improves convergence time while adaptive timing reduces bandwidth usage during stable periods.</li>\n<li><strong>Consequences</strong>: Moderate implementation complexity but excellent scalability and partition resilience</li>\n</ul>\n</blockquote>\n<p>The <strong>request routing architecture</strong> decision involved choosing between centralized routing through designated proxy nodes versus distributed routing where any node can handle any request. The distributed approach was selected because it eliminates single points of failure and provides better load distribution, despite the increased complexity of maintaining consistent routing tables.</p>\n<p><strong>Consistency model selection</strong> for cluster membership required balancing between strong consistency (which would require consensus protocols) and eventual consistency (which accepts temporary inconsistencies). Eventual consistency was chosen because cache workloads can tolerate brief periods of inconsistent routing, and the performance benefits of avoiding consensus protocols are substantial.</p>\n<p>The <strong>health check timing parameters</strong> required careful tuning to balance failure detection speed against network overhead and false positive rates. The selected parameters (1-second probe intervals, 5-second suspicion timeouts, 30-second failure confirmations) provide good responsiveness for typical cache workloads while maintaining stability under moderate network congestion.</p>\n<p><strong>Message serialization format</strong> decisions impact both performance and operational debugging capabilities. JSON was chosen over binary formats like Protocol Buffers because the human-readable format significantly simplifies troubleshooting cluster communication issues, and the performance overhead is acceptable for cache workloads where network latency typically dominates over serialization costs.</p>\n<p>The <strong>circuit breaker configuration</strong> for request forwarding protects against cascading failures when target nodes become overloaded. The parameters were chosen to detect failures within 10-15 seconds while allowing sufficient time for temporary network hiccups to resolve without triggering unnecessary circuit opening.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Distributed communication systems present numerous subtle failure modes that can cause mysterious outages, data inconsistencies, and performance degradations. These pitfalls often manifest only under specific timing conditions or failure scenarios, making them particularly challenging to diagnose and reproduce.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Message Ordering in Gossip Protocol</strong></p>\n<p>Many implementations assume that network messages arrive in the order they were sent, but this assumption breaks down in distributed systems. Consider a scenario where node A sends two gossip messages: first marking node C as &quot;suspected&quot;, then marking it as &quot;failed&quot;. If these messages arrive at node B in reverse order due to network routing variations, node B will first learn that C is failed, then receive the older &quot;suspected&quot; message and incorrectly downgrade C&#39;s status.</p>\n<p>This ordering problem causes <strong>membership state oscillation</strong> where cluster members continuously flip between different views of node health, preventing convergence and causing routing instability. The symptoms include inconsistent routing decisions, requests being sent to failed nodes, and excessive cluster membership churn in logs.</p>\n<p><strong>Solution</strong>: Implement vector clocks or logical timestamps in gossip messages. Each membership update includes version information that allows recipients to determine the correct ordering regardless of arrival sequence. Messages with older timestamps are discarded if newer information about the same node has already been processed.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Timeout Configuration</strong></p>\n<p>Choosing timeout values for health checks and failure detection requires balancing conflicting requirements: short timeouts enable rapid failure detection but increase false positive rates during network congestion, while long timeouts reduce false positives but slow down failure detection and recovery.</p>\n<p>A common mistake is using fixed timeout values that work well in test environments but fail in production networks with variable latency. For example, setting health check timeouts to 1 second might work perfectly on a local network but cause constant false failures when deployed across regions with 200-300ms baseline latency.</p>\n<p><strong>Solution</strong>: Implement adaptive timeout algorithms that adjust based on observed network conditions. Track the distribution of response times for successful health checks and set timeouts to 3-4 standard deviations above the mean. This approach automatically adapts to different network environments while maintaining appropriate failure detection sensitivity.</p>\n<p>⚠️ <strong>Pitfall: Split-Brain Membership Reconciliation Errors</strong></p>\n<p>When network partitions heal, previously isolated sub-clusters must merge their divergent membership views. A critical error occurs when the reconciliation process doesn&#39;t properly handle nodes that failed and recovered during the partition. Consider this scenario:</p>\n<ol>\n<li>Network partition separates nodes A,B from nodes C,D</li>\n<li>Node A fails and restarts (getting a new node ID) while partitioned</li>\n<li>Partition heals and nodes attempt to reconcile membership</li>\n<li>The reconciliation process sees the old node ID for A marked as failed and the new node ID as active, but doesn&#39;t realize they represent the same physical machine</li>\n</ol>\n<p>This leads to <strong>phantom node entries</strong> in the membership table and incorrect hash ring calculations that route some keys to non-existent nodes.</p>\n<p><strong>Solution</strong>: Implement proper node identity management that distinguishes between logical node IDs and physical node instances. Include additional identifying information like MAC addresses or installation UUIDs that persist across restarts. During reconciliation, detect and merge entries that represent the same physical node.</p>\n<p>⚠️ <strong>Pitfall: Request Forwarding Loops</strong></p>\n<p>In symmetric proxy architectures where any node can forward requests to any other node, subtle inconsistencies in hash ring views can create forwarding loops. For example:</p>\n<ol>\n<li>Client sends GET request for key &quot;user:123&quot; to node A</li>\n<li>Node A&#39;s hash ring shows key belongs to node B, forwards request</li>\n<li>Node B&#39;s hash ring shows key belongs to node C (due to recent membership change), forwards request  </li>\n<li>Node C&#39;s hash ring shows key belongs to node A, forwards back to A</li>\n<li>Request loops indefinitely until timeout</li>\n</ol>\n<p><strong>Solution</strong>: Include hop count limits and forward path tracking in request headers. Each forwarding operation decrements the hop count and adds the current node to the path. If a node sees its own ID in the forward path or the hop count reaches zero, it terminates the loop and returns an error. Additionally, propagate hash ring version numbers in forwarding requests to detect and resolve membership inconsistencies.</p>\n<p>⚠️ <strong>Pitfall: Gossip Message Amplification</strong></p>\n<p>Naive gossip implementations can create <strong>message storms</strong> during cluster instability. When multiple nodes detect the same failure simultaneously, they may all send urgent gossip messages to their peers. If recipients immediately forward these urgent messages to their peers, the message volume can grow exponentially and overwhelm the network.</p>\n<p>This amplification is particularly problematic during cascading failures where multiple nodes fail in rapid succession, causing each failure to trigger a separate message storm.</p>\n<p><strong>Solution</strong>: Implement <strong>gossip rate limiting</strong> and <strong>message deduplication</strong>. Each node maintains a recent message cache and discards duplicate gossip messages based on content hash and sender ID. Additionally, implement exponential backoff for urgent gossip messages to prevent amplification during failure scenarios.</p>\n<p>⚠️ <strong>Pitfall: Health Check False Positives During Load</strong></p>\n<p>Health check implementations often fail to account for node load conditions. A common pattern is for health check messages to be processed by the same thread pool or event loop that handles client requests. During high load periods, client request processing can consume all available resources, causing health check messages to be delayed or dropped even though the node is functioning correctly.</p>\n<p>This creates a <strong>load-induced failure detection</strong> problem where busy nodes are incorrectly marked as failed, causing traffic to be redirected to other nodes and potentially creating cascading overload conditions.</p>\n<p><strong>Solution</strong>: Implement <strong>dedicated health check processing</strong> that operates independently of client request handling. Use separate thread pools, network ports, or processing queues for health check messages. Additionally, include load metrics in health check responses so that peers can distinguish between node failures and temporary overload conditions.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Cluster State During Rolling Updates</strong></p>\n<p>During rolling updates where cluster nodes are upgraded sequentially, different nodes may have different versions of the gossip protocol or membership management logic. This version skew can cause persistent membership inconsistencies that don&#39;t resolve even after the update completes.</p>\n<p>For example, if the new version changes the format of gossip messages or the logic for conflict resolution, old and new nodes may be unable to synchronize their membership views properly.</p>\n<p><strong>Solution</strong>: Design gossip protocols with <strong>backward compatibility</strong> and <strong>protocol versioning</strong>. Include protocol version numbers in gossip messages and implement fallback logic that allows nodes with different versions to communicate. Plan rolling update sequences to minimize the time period where mixed versions are active.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The cluster communication system requires careful coordination between multiple subsystems including network transports, gossip protocols, failure detection, and request routing. This implementation guidance provides complete working infrastructure and detailed skeletons for the core learning components.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Transport</td>\n<td>net/http with JSON encoding (encoding/json)</td>\n<td>HTTP/2 with Protocol Buffers (google.golang.org/protobuf)</td>\n</tr>\n<tr>\n<td>Gossip Serialization</td>\n<td>JSON with gzip compression</td>\n<td>MessagePack or Protocol Buffers for efficiency</td>\n</tr>\n<tr>\n<td>Health Check Transport</td>\n<td>HTTP GET requests with JSON responses</td>\n<td>Custom UDP protocol with binary messages</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static configuration file</td>\n<td>Consul or etcd integration</td>\n</tr>\n<tr>\n<td>Message Queuing</td>\n<td>Go channels with buffering</td>\n<td>Apache Kafka or NATS for persistence</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard library log package</td>\n<td>Structured logging with logrus or zap</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The cluster communication components should be organized to separate concerns and enable independent testing:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/cluster/\n  transport/\n    http_transport.go        ← HTTPTransport implementation\n    transport.go            ← Transport interface definition  \n    transport_test.go       ← Transport unit tests\n  gossip/\n    gossip.go              ← Core gossip protocol logic\n    message.go             ← Message types and serialization\n    membership.go          ← Membership table management\n    gossip_test.go         ← Gossip protocol tests\n  discovery/\n    discovery.go           ← Node discovery and bootstrap logic\n    discovery_test.go      ← Discovery mechanism tests\n  health/\n    health_checker.go      ← Health check and failure detection\n    failure_detector.go    ← Failure detector state machine\n    health_test.go         ← Health check tests\n  router/\n    request_router.go      ← Request routing and forwarding\n    circuit_breaker.go     ← Circuit breaker for node failures\n    router_test.go         ← Request routing tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>HTTP Transport Implementation</strong> (complete, ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HTTPTransport provides HTTP-based communication between cluster nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HTTPTransport</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHTTPTransport creates an HTTP transport with specified timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHTTPTransport</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Transport: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Transport</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                MaxIdleConns:        </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                MaxIdleConnsPerHost: </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                IdleConnTimeout:     </span><span style=\"color:#79B8FF\">60</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendMessage sends a JSON message to the specified address and returns the response</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    jsonData, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to marshal message: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"http://\"</span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\">address</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">\"/cluster\"</span><span style=\"color:#E1E4E8\">, bytes.</span><span style=\"color:#B392F0\">NewBuffer</span><span style=\"color:#E1E4E8\">(jsonData))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to send request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"received error response: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    body, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">(resp.Body)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read response: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> body, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck performs a health check against the specified address</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"http://\"</span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\">address</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">\"/health\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create health check request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check returned status </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartServer starts the HTTP server for receiving cluster messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.server </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:         address,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler:      handler,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  t.timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: t.timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        IdleTimeout:  </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> t.timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> t.server.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully stops the HTTP server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> t.server </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> t.server.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Message Types and Serialization</strong> (complete, ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> gossip</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Message represents a general cluster communication message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Message</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Sender    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"sender\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">   `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data      </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GossipMessage contains cluster membership information for gossip protocol</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GossipMessage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeStates </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#9ECBFF\"> `json:\"node_states\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version    </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">               `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeState represents the current state of a cluster node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSeen </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"last_seen\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version  </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">    `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheckRequest is sent to verify node liveness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthCheckRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProbeID     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"probe_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SenderLoad  </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"sender_load\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheckResponse confirms node is alive and healthy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthCheckResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProbeID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"probe_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Load      </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"load\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// JoinRequest is sent when a node wants to join the cluster</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> JoinRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// JoinResponse contains current cluster membership for new nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> JoinResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">                     `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Membership  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#9ECBFF\">     `json:\"membership\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version     </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">                   `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SerializeMessage converts a message to JSON bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> SerializeMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(msg)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeserializeMessage converts JSON bytes to a message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> DeserializeMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, msg)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreateMessage wraps data in a Message envelope with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CreateMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msgType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">senderID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#B392F0\">Message</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> Message</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Type:      msgType,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Sender:    senderID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Data:      data,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeletons\">Core Logic Skeletons</h4>\n<p><strong>Node Discovery Implementation</strong> (signatures + detailed TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> discovery</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeDiscovery handles cluster membership discovery and bootstrapping</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeDiscovery</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transport     </span><span style=\"color:#B392F0\">Transport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    membership    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    membershipMu  </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version       </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    joinListeners []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewNodeDiscovery creates a new discovery service with the specified configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewNodeDiscovery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">transport</span><span style=\"color:#B392F0\"> Transport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:     config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        transport:  transport,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        membership: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        version:    </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// JoinCluster attempts to join an existing cluster using bootstrap addresses</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">JoinCluster</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Iterate through JoinAddresses from config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each address, attempt to send JoinRequest with local node info</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If request succeeds, process JoinResponse to update local membership</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update membership table with received node states</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Set local membership version to received version + 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If all bootstrap addresses fail, start as single-node cluster</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Notify join listeners about successful cluster join</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use transport.SendMessage with JoinRequest, handle timeouts gracefully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleJoinRequest processes join requests from new nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleJoinRequest</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> JoinRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">JoinResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate the join request (check node ID format, address reachability)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Acquire write lock on membership table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check if node ID already exists (handle duplicate joins)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add new node to membership table with \"active\" status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Increment membership version number</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create JoinResponse with current membership snapshot</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Release write lock and notify join listeners</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Trigger gossip message to propagate new node to cluster</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Return complete membership map, not just new node info</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMembership returns a snapshot of current cluster membership</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMembership</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock on membership table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create deep copy of membership map (don't return direct reference)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release read lock and return copied membership</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Make sure to copy NodeState structs, not just map references</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateNodeState updates the state of a specific node in the membership table</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateNodeState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">newState</span><span style=\"color:#B392F0\"> NodeState</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock on membership table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if node exists in current membership</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Compare version numbers to avoid applying stale updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update node state and increment membership version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Release write lock and trigger gossip propagation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Version comparison prevents race conditions during concurrent updates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RemoveNode removes a failed node from the cluster membership</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RemoveNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock on membership table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify node exists before attempting removal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Delete node from membership map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Increment membership version number</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Release write lock and notify removal listeners</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Trigger gossip to propagate removal throughout cluster</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Consider graceful vs forceful removal based on node state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Gossip Protocol Implementation</strong> (signatures + detailed TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> gossip</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GossipProtocol manages cluster state propagation using epidemic algorithms</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GossipProtocol</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transport    </span><span style=\"color:#B392F0\">Transport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    membership   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    running      </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runningMu    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh       </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GossipConfig contains tuning parameters for the gossip protocol</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GossipConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GossipInterval    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GossipFanout      </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxMessageSize    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompressionEnabled </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewGossipProtocol creates a new gossip service</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewGossipProtocol</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">transport</span><span style=\"color:#B392F0\"> Transport</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">membership</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">GossipConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipProtocol</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">GossipProtocol</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodeID:     nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        transport:  transport,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        membership: membership,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:     config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopCh:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins the gossip protocol background processes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipProtocol</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock and check if already running</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set running flag to true and start gossip timer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start goroutine for periodic gossip sending</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start goroutine for gossip message processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Register message handler with transport layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Ticker for periodic gossip, handle context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down the gossip protocol</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipProtocol</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock and check if currently running</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set running flag to false and close stop channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Wait for background goroutines to finish</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Unregister message handlers from transport</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.WaitGroup to coordinate graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendGossip initiates a gossip round with randomly selected peers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipProtocol</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendGossip</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get current membership snapshot from discovery service</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Select random subset of nodes for gossip (fanout algorithm)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Exclude self and recently contacted nodes from selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create GossipMessage with current membership state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send gossip messages to selected peers in parallel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle responses and update local state if newer info received</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update last-contact timestamps for successful communications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use goroutines with WaitGroup for parallel sending</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleGossipMessage processes incoming gossip from other nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipProtocol</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleGossipMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#B392F0\"> GossipMessage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">senderAddr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate gossip message structure and sender information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compare membership versions to detect newer information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each node in gossip message, compare with local state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply newer updates to local membership table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Resolve conflicts using timestamp and node ID ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update local membership version if changes were applied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Optionally trigger immediate gossip if critical updates received</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use vector clocks or logical timestamps for conflict resolution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SelectGossipTargets chooses random nodes for gossip communication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipProtocol</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SelectGossipTargets</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">membership</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Filter out self and failed nodes from potential targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Apply selection preferences (avoid recently contacted nodes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Randomly select up to GossipFanout nodes from candidates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Ensure selection includes diverse network segments if possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return list of selected node addresses for gossip</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use weighted random selection to prefer less-recently contacted nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Failure Detection Implementation</strong> (signatures + detailed TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> health</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FailureDetector implements sophisticated failure detection with suspicion states</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FailureDetector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transport     </span><span style=\"color:#B392F0\">Transport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    membership    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeDiscovery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeStates    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeHealthState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    statesMu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    running       </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh        </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeHealthState tracks health status and failure detection state for a node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeHealthState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastProbeTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastResponseTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailureCount     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SuspicionTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProbeHistory     []</span><span style=\"color:#B392F0\">ProbeResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProbeResult records the outcome of a health check probe</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ProbeResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success   </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Latency   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Error     </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthConfig contains parameters for failure detection behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProbeInterval     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ProbeTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SuspicionTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailureThreshold  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IndirectProbes    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewFailureDetector creates a failure detector with specified configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewFailureDetector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">transport</span><span style=\"color:#B392F0\"> Transport</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">membership</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NodeDiscovery</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HealthConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodeID:     nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        transport:  transport,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        membership: membership,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodeStates: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeHealthState</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:     config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopCh:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins health checking and failure detection processes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize health state for all known cluster members</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start goroutine for periodic direct health probing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start goroutine for indirect probe coordination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start goroutine for timeout processing and state transitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Register for membership change notifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use separate timers for different probe intervals</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProbeNode sends direct health check to specified node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProbeNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ProbeResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up node address from membership table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create HealthCheckRequest with probe ID and timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Send probe using transport with configured timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Measure round-trip latency for successful probes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Record probe result in node's health state history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update last probe time and response time timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return ProbeResult with success status and metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context.WithTimeout for probe timeout control</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessProbeResult updates node health state based on probe outcome</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessProbeResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ProbeResult</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock on node states map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Get or create NodeHealthState for the target node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add probe result to node's probe history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update failure count based on probe success/failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Determine if state transition is required (active->suspected->failed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Apply state transition and update timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Notify membership service of state changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Trigger gossip propagation for critical state changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sliding window for failure count to avoid permanent marking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PerformIndirectProbe coordinates indirect probing through other nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">PerformIndirectProbe</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">targetNodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Select subset of healthy nodes as indirect probe witnesses</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Send IndirectProbeRequest to witnesses in parallel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Collect responses from witnesses within timeout period</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Analyze witness responses to determine target node status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If majority confirms failure, mark target as failed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: If some witnesses succeed, maintain suspected status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update confidence metrics based on witness agreement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context with timeout for coordinating parallel probes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateNodeStatus transitions node to new health status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateNodeStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">newStatus</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock on node states map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate status transition is legal (active->suspected->failed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update node status and relevant timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Reset failure counters if transitioning to active</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update membership service with new node status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Log status transition with detailed context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Trigger immediate gossip for critical transitions (failed status)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement state machine validation for legal transitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing cluster communication, verify functionality with these specific tests:</p>\n<p><strong>Cluster Formation Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start first node (bootstrap)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> -config</span><span style=\"color:#9ECBFF\"> node1.json</span><span style=\"color:#79B8FF\"> -port</span><span style=\"color:#79B8FF\"> 8001</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Start second node (joins cluster)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> -config</span><span style=\"color:#9ECBFF\"> node2.json</span><span style=\"color:#79B8FF\"> -port</span><span style=\"color:#79B8FF\"> 8002</span><span style=\"color:#79B8FF\"> -join</span><span style=\"color:#9ECBFF\"> 127.0.0.1:8001</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify both nodes see each other in membership</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://127.0.0.1:8001/cluster/membership</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://127.0.0.1:8002/cluster/membership</span></span></code></pre></div>\n\n<p><strong>Expected Output</strong>: Both endpoints should return JSON with both nodes listed as &quot;active&quot; status.</p>\n<p><strong>Failure Detection Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start cluster with 3 nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Kill one node abruptly (SIGKILL)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">kill</span><span style=\"color:#79B8FF\"> -9</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#9ECBFF\">node_pi</span><span style=\"color:#E1E4E8\">d</span><span style=\"color:#F97583\">></span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Wait 10-15 seconds, check membership on remaining nodes</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://127.0.0.1:8001/cluster/membership</span></span></code></pre></div>\n\n<p><strong>Expected Output</strong>: Failed node should show &quot;failed&quot; status within 15 seconds.</p>\n<p><strong>Request Routing Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Send cache request to node that doesn't own the key</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> PUT</span><span style=\"color:#9ECBFF\"> http://127.0.0.1:8001/cache/test-key</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> \"test-value\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify request was routed to correct node based on hash ring</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://127.0.0.1:8002/cache/test-key</span></span></code></pre></div>\n\n<p><strong>Expected Output</strong>: Value should be retrievable from any node, indicating proper routing.</p>\n<p><strong>Troubleshooting Signs</strong>:</p>\n<ul>\n<li><strong>Nodes don&#39;t discover each other</strong>: Check network connectivity, bootstrap addresses, JSON parsing</li>\n<li><strong>False failure detection</strong>: Reduce health check timeouts, check network latency patterns  </li>\n<li><strong>Gossip not propagating</strong>: Verify gossip intervals, message serialization, fanout settings</li>\n<li><strong>Request routing loops</strong>: Check hash ring consistency, hop count limits, forward path tracking</li>\n</ul>\n<h2 id=\"replication-and-consistency\">Replication and Consistency</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section covers Milestone 4 (Replication &amp; Consistency), building on the distributed cache foundation from all previous milestones to add fault tolerance through data replication, configurable consistency levels, and conflict resolution mechanisms.</p>\n</blockquote>\n<h3 id=\"mental-model-the-document-copying-office\">Mental Model: The Document Copying Office</h3>\n<p>Think of data replication in a distributed cache like a government document copying office that serves multiple branches across a city. When an important document arrives, the office doesn&#39;t store just one copy in one location—that would be too risky if that building burned down or became inaccessible. Instead, they make multiple copies and store them in different branch offices across the city.</p>\n<p>The <strong>replication factor</strong> is like the office policy: &quot;Every document must have exactly 3 copies stored in 3 different branches.&quot; When someone submits a new document, the copying office identifies which branches should store copies based on their location and current workload, then sends copies to each designated branch.</p>\n<p><strong>Quorum-based operations</strong> work like approval processes. For a &quot;write quorum,&quot; imagine the policy says &quot;a document is officially filed when at least 2 out of 3 branches confirm they&#39;ve stored their copy.&quot; For a &quot;read quorum,&quot; the policy might say &quot;we can provide an official document when we receive the same version from at least 2 out of 3 branches.&quot;</p>\n<p><strong>Conflict resolution</strong> handles the case where branches have different versions of the same document. Maybe branch A received an updated version on Monday, but branch B was closed and still has the old version from last week. When someone requests that document, the copying office needs a policy: do they trust the newest timestamp, count votes from multiple branches, or use some other method to determine which version is authoritative?</p>\n<p><strong>Anti-entropy repair</strong> is like the weekly audit process. A supervisor visits all branches with a checklist, comparing document versions across locations. When they find inconsistencies—branch A has version 3 of a document but branch B only has version 2—they initiate a repair process to bring all copies back into sync.</p>\n<p>This analogy captures the core tension in distributed systems: we replicate for fault tolerance, but multiple copies introduce complexity around consistency. Just like the document office needs policies for handling disagreements between branches, our distributed cache needs algorithms for handling disagreements between replicas.</p>\n<h3 id=\"replication-factor-and-placement\">Replication Factor and Placement</h3>\n<p>The <strong>replication factor</strong> determines how many copies of each cache entry exist in the cluster. This is a fundamental configuration parameter that directly impacts both fault tolerance and storage overhead. A replication factor of 1 means no redundancy—if the node holding a key fails, that data is lost. A replication factor of 3 provides strong fault tolerance, allowing the system to survive the failure of any 2 nodes while still serving requests for all data.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Freplication-strategy.svg\" alt=\"Data Replication and Placement\"></p>\n<p><strong>Replica placement strategy</strong> determines which specific nodes store copies of each key. The most common approach in consistent hashing systems is <strong>successor-based placement</strong>: for any given key, find its position on the hash ring, then place replicas on the next N-1 successor nodes in clockwise order. This provides several advantages: replica placement is deterministic and calculable by any node, replicas are naturally distributed across different physical nodes, and the placement remains stable as nodes join and leave the cluster.</p>\n<blockquote>\n<p><strong>Decision: Successor-Based Replica Placement</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to determine which specific nodes store replicas for each key in a way that&#39;s deterministic, load-balanced, and resilient to node failures</li>\n<li><strong>Options Considered</strong>: Random placement, consistent successor placement, rack-aware placement</li>\n<li><strong>Decision</strong>: Use consistent successor placement on the hash ring</li>\n<li><strong>Rationale</strong>: Deterministic placement means any node can calculate replica locations without coordination. Successor placement naturally distributes load and maintains locality during ring changes. Implementation complexity is much lower than rack-aware placement.</li>\n<li><strong>Consequences</strong>: Provides good load distribution and fault tolerance. May place replicas on nodes that fail together in correlated failures, but rack-awareness can be added later as an enhancement.</li>\n</ul>\n</blockquote>\n<p>The replica placement algorithm works as follows:</p>\n<ol>\n<li><strong>Hash the key</strong> to determine its position on the consistent hash ring</li>\n<li><strong>Find the primary node</strong> responsible for that key position using the standard hash ring lookup</li>\n<li><strong>Select successor nodes</strong> by walking clockwise around the ring from the primary node&#39;s position</li>\n<li><strong>Skip duplicate physical nodes</strong> to ensure replicas are stored on different machines (if a physical node has multiple virtual nodes)</li>\n<li><strong>Return the list of replica nodes</strong> in ring order, with the primary node as the first element</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Replication Factor</th>\n<th>Fault Tolerance</th>\n<th>Storage Overhead</th>\n<th>Read/Write Overhead</th>\n<th>Recommended Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>None (single point of failure)</td>\n<td>1x</td>\n<td>Minimal</td>\n<td>Development, non-critical caching</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Survives 1 node failure</td>\n<td>2x</td>\n<td>Moderate</td>\n<td>Testing, low-risk applications</td>\n</tr>\n<tr>\n<td>3</td>\n<td>Survives 2 node failures</td>\n<td>3x</td>\n<td>Higher</td>\n<td>Production systems, standard recommendation</td>\n</tr>\n<tr>\n<td>5</td>\n<td>Survives 4 node failures</td>\n<td>5x</td>\n<td>Highest</td>\n<td>Mission-critical systems, geo-distributed clusters</td>\n</tr>\n</tbody></table>\n<p><strong>Replica node selection</strong> must handle several edge cases. When the cluster size is smaller than the replication factor, every available node stores a replica. During node failures, some keys may temporarily have fewer than the desired number of replicas until the cluster stabilizes. When nodes rejoin after a partition, their data may be stale and require synchronization with active replicas.</p>\n<p><strong>Virtual node considerations</strong> add complexity to replica placement. Since each physical node occupies multiple positions on the hash ring through virtual nodes, the placement algorithm must track physical node identity to avoid placing multiple replicas on the same machine. This requires maintaining a mapping from virtual node identifiers to physical node addresses, and skipping virtual nodes that belong to physical nodes already selected as replicas.</p>\n<p><strong>Load balancing</strong> across replicas becomes important under certain access patterns. If clients always read from the first replica in the list (the primary), that creates hotspots. Some systems rotate the replica list based on the client identifier or use randomized replica selection to distribute read load more evenly across all copies.</p>\n<h3 id=\"quorum-based-operations\">Quorum-Based Operations</h3>\n<p><strong>Quorum systems</strong> provide a mathematical foundation for achieving consistency in replicated systems. A quorum is the minimum number of replicas that must participate in an operation for it to be considered successful. The key insight is that if read quorums and write quorums overlap, the system guarantees that reads will see at least one up-to-date copy of the data.</p>\n<p>The <strong>fundamental quorum rule</strong> states that for a system with N replicas, if R is the read quorum size and W is the write quorum size, then R + W &gt; N ensures strong consistency. This mathematical constraint ensures that any read operation will contact at least one replica that participated in the most recent write operation.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration</th>\n<th>Read Quorum (R)</th>\n<th>Write Quorum (W)</th>\n<th>Consistency Level</th>\n<th>Performance Characteristics</th>\n<th>Failure Tolerance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Strong Consistency</td>\n<td>2</td>\n<td>2</td>\n<td>Strong (R+W &gt; N)</td>\n<td>Higher latency, lower availability</td>\n<td>Cannot serve requests if 2+ nodes down</td>\n</tr>\n<tr>\n<td>Read-Optimized</td>\n<td>1</td>\n<td>3</td>\n<td>Strong (R+W &gt; N)</td>\n<td>Fast reads, slow writes</td>\n<td>Cannot write if 2+ nodes down</td>\n</tr>\n<tr>\n<td>Write-Optimized</td>\n<td>3</td>\n<td>1</td>\n<td>Strong (R+W &gt; N)</td>\n<td>Fast writes, slow reads</td>\n<td>Cannot read if 2+ nodes down</td>\n</tr>\n<tr>\n<td>Eventually Consistent</td>\n<td>1</td>\n<td>1</td>\n<td>Eventual</td>\n<td>Fastest operations</td>\n<td>Highest availability, may read stale data</td>\n</tr>\n</tbody></table>\n<p><strong>Write quorum operations</strong> follow a specific sequence to ensure consistency:</p>\n<ol>\n<li><strong>Calculate target replicas</strong> for the key using the replica placement algorithm</li>\n<li><strong>Generate a write timestamp</strong> (often using vector clocks or logical timestamps)</li>\n<li><strong>Send write requests</strong> concurrently to all replica nodes with the timestamp and data</li>\n<li><strong>Wait for acknowledgments</strong> from at least W replica nodes before responding to the client</li>\n<li><strong>Handle partial failures</strong> by retrying failed writes or returning an error if insufficient replicas respond</li>\n<li><strong>Perform read repair</strong> on any replicas that failed to acknowledge the write during subsequent read operations</li>\n</ol>\n<p><strong>Read quorum operations</strong> must handle the possibility of retrieving different versions from different replicas:</p>\n<ol>\n<li><strong>Send read requests</strong> concurrently to all replica nodes (or a subset if using optimized strategies)</li>\n<li><strong>Collect responses</strong> from at least R replica nodes with their timestamps and data values</li>\n<li><strong>Resolve conflicts</strong> if replicas return different values by selecting the version with the highest timestamp</li>\n<li><strong>Trigger read repair</strong> if any replica has a stale version by asynchronously updating it with the latest value</li>\n<li><strong>Return the resolved value</strong> to the client along with success confirmation</li>\n</ol>\n<blockquote>\n<p><strong>Critical insight</strong>: The quorum intersection property (R + W &gt; N) is what provides linearizability in the face of node failures. Even if some replicas are temporarily unavailable, the mathematics ensure that reads will always see writes that have been acknowledged to clients.</p>\n</blockquote>\n<p><strong>Consistency level configuration</strong> allows applications to choose different trade-offs for different operations. Some cache entries might require strong consistency (user authentication tokens), while others can tolerate eventual consistency for better performance (recommendation engine results, metrics counters).</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Level</th>\n<th>Read Behavior</th>\n<th>Write Behavior</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STRONG</code></td>\n<td>Wait for read quorum</td>\n<td>Wait for write quorum</td>\n<td>Critical data, user sessions, financial data</td>\n</tr>\n<tr>\n<td><code>EVENTUAL</code></td>\n<td>Read from any replica</td>\n<td>Write to any replica</td>\n<td>Analytics data, recommendations, caching web content</td>\n</tr>\n<tr>\n<td><code>LOCAL</code></td>\n<td>Read from local replica only</td>\n<td>Write to local replica only</td>\n<td>Geographic locality, partition tolerance</td>\n</tr>\n<tr>\n<td><code>ONE</code></td>\n<td>Read from fastest replica</td>\n<td>Write to one replica</td>\n<td>Maximum performance, can tolerate data loss</td>\n</tr>\n</tbody></table>\n<p><strong>Sloppy quorums</strong> handle the case where some of the preferred replica nodes are unavailable. Instead of failing the operation, the system temporarily stores data on alternative nodes (called &quot;hinted handoff&quot; nodes) until the preferred replicas recover. This maintains availability during partial failures but requires additional complexity in the repair mechanisms.</p>\n<p><strong>Dynamic quorum adjustment</strong> can adapt to cluster membership changes. If a cluster normally has 5 nodes but 2 fail, the system might automatically adjust from a (3,3) quorum to a (2,2) quorum to maintain availability. However, this requires careful coordination to avoid split-brain scenarios during network partitions.</p>\n<h3 id=\"conflict-resolution-strategies\">Conflict Resolution Strategies</h3>\n<p><strong>Conflict resolution</strong> becomes necessary when multiple replicas hold different versions of the same key. This happens during network partitions, concurrent writes, or node failures followed by recovery. The distributed cache must have deterministic rules for deciding which version is authoritative when conflicts are discovered.</p>\n<p><strong>Timestamp-based resolution</strong> is the simplest approach, using either physical timestamps or logical clocks to determine ordering. Each write operation includes a timestamp, and during conflicts, the version with the highest timestamp wins. However, this approach has limitations: physical clocks can skew between nodes, and concurrent operations might receive identical timestamps.</p>\n<blockquote>\n<p><strong>Decision: Vector Clock Conflict Resolution</strong></p>\n<ul>\n<li><strong>Context</strong>: Need deterministic conflict resolution that can handle concurrent updates across multiple nodes and provide causality tracking</li>\n<li><strong>Options Considered</strong>: Physical timestamps, logical timestamps, vector clocks, last-write-wins with client timestamps</li>\n<li><strong>Decision</strong>: Use vector clocks with last-write-wins fallback for concurrent updates</li>\n<li><strong>Rationale</strong>: Vector clocks correctly capture causal relationships between updates. Can detect true concurrency vs. ordering. Physical timestamps are unreliable due to clock skew. Client timestamps are untrustworthy.</li>\n<li><strong>Consequences</strong>: More complex implementation and storage overhead, but provides correct conflict detection and resolution with strong semantic guarantees.</li>\n</ul>\n</blockquote>\n<p><strong>Vector clocks</strong> provide a more sophisticated approach to conflict resolution. Each replica maintains a vector timestamp that tracks the logical time from the perspective of every node in the cluster. When a write occurs, the writing node increments its entry in the vector clock. During conflict resolution, vector clocks can determine if one version causally precedes another, or if two versions are truly concurrent.</p>\n<table>\n<thead>\n<tr>\n<th>Vector Clock Scenario</th>\n<th>Node A Clock</th>\n<th>Node B Clock</th>\n<th>Relationship</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>A happens before B</td>\n<td>[A:1, B:0]</td>\n<td>[A:1, B:1]</td>\n<td>A → B</td>\n<td>B wins (supersedes A)</td>\n</tr>\n<tr>\n<td>B happens before A</td>\n<td>[A:2, B:1]</td>\n<td>[A:1, B:1]</td>\n<td>B → A</td>\n<td>A wins (supersedes B)</td>\n</tr>\n<tr>\n<td>Concurrent updates</td>\n<td>[A:2, B:1]</td>\n<td>[A:1, B:2]</td>\n<td>A ∥ B</td>\n<td>Apply conflict resolution policy</td>\n</tr>\n<tr>\n<td>Identical versions</td>\n<td>[A:1, B:1]</td>\n<td>[A:1, B:1]</td>\n<td>A = B</td>\n<td>No conflict, same data</td>\n</tr>\n</tbody></table>\n<p><strong>Concurrent update handling</strong> requires a policy for cases where vector clocks indicate true concurrency (neither version causally precedes the other). Common strategies include:</p>\n<ul>\n<li><strong>Last-write-wins (LWW)</strong>: Use physical timestamps as a tiebreaker, accepting that some updates may be lost</li>\n<li><strong>Multi-value resolution</strong>: Store both conflicting values and let the application decide which to use</li>\n<li><strong>Semantic resolution</strong>: Apply domain-specific rules (e.g., for counters, sum the concurrent updates)</li>\n<li><strong>Client-side resolution</strong>: Return the conflict to the client application with both versions</li>\n</ul>\n<p><strong>Version vector maintenance</strong> requires careful management as nodes join and leave the cluster. Vector clocks grow with cluster size, and tombstone entries must be kept for departed nodes until all references are cleaned up. Some systems use bounded vector clocks that compress old entries to control memory overhead.</p>\n<p>The conflict resolution algorithm operates during read operations when multiple replicas return different values:</p>\n<ol>\n<li><strong>Collect all replica responses</strong> with their associated vector clocks and values</li>\n<li><strong>Build a version graph</strong> showing causal relationships between all versions</li>\n<li><strong>Identify concurrent branches</strong> where no version causally dominates others</li>\n<li><strong>Apply resolution policy</strong> to select a winner among concurrent versions</li>\n<li><strong>Trigger repair operations</strong> to propagate the resolved version to all replicas</li>\n<li><strong>Return the resolved value</strong> to the client</li>\n</ol>\n<p><strong>Repair propagation</strong> ensures that once a conflict is resolved, all replicas converge to the same value. This typically involves asynchronously sending write operations to replicas that have stale or conflicting versions, using the resolved vector clock to prevent the repair from being seen as a new concurrent update.</p>\n<h3 id=\"anti-entropy-and-repair\">Anti-Entropy and Repair</h3>\n<p><strong>Anti-entropy processes</strong> run continuously in the background to detect and repair inconsistencies between replicas. Unlike read repair, which happens synchronously during client operations, anti-entropy proactively scans the data store to find divergent replicas and bring them back into sync.</p>\n<p><strong>Merkle tree comparison</strong> provides an efficient way to detect inconsistencies without transferring entire datasets. Each replica builds a Merkle tree over its key space, with leaves representing individual keys and internal nodes representing hashes of their children. By comparing Merkle tree roots and recursively drilling down into differing branches, nodes can identify exactly which keys have diverged.</p>\n<p>The anti-entropy process follows this general algorithm:</p>\n<ol>\n<li><strong>Select a peer node</strong> for comparison (often using a round-robin or random selection policy)</li>\n<li><strong>Exchange Merkle tree roots</strong> for corresponding key ranges</li>\n<li><strong>Recursively compare tree branches</strong> to identify divergent key ranges</li>\n<li><strong>Exchange detailed metadata</strong> (vector clocks, timestamps) for keys in divergent ranges</li>\n<li><strong>Apply conflict resolution</strong> to determine authoritative values for conflicting keys</li>\n<li><strong>Perform synchronization</strong> by transferring missing or updated values between nodes</li>\n<li><strong>Update local metadata</strong> to reflect the synchronized state</li>\n</ol>\n<p><strong>Repair scheduling</strong> balances thoroughness against system load. Continuous anti-entropy could consume significant network and CPU resources, while infrequent repairs allow inconsistencies to persist longer. Most systems use adaptive scheduling that increases repair frequency after detecting problems and reduces it during stable periods.</p>\n<table>\n<thead>\n<tr>\n<th>Repair Strategy</th>\n<th>Frequency</th>\n<th>Resource Usage</th>\n<th>Consistency Guarantee</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Continuous Scanning</td>\n<td>Seconds</td>\n<td>Very High</td>\n<td>Near-realtime repair</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Periodic Full Scan</td>\n<td>Hours</td>\n<td>High (during scan)</td>\n<td>Eventually consistent</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Adaptive Frequency</td>\n<td>Variable</td>\n<td>Moderate</td>\n<td>Responsive to problems</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Triggered Repair</td>\n<td>On-demand</td>\n<td>Low baseline</td>\n<td>Reactive only</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<p><strong>Hinted handoff recovery</strong> handles the special case where writes were temporarily stored on non-preferred replicas during failures. When preferred replica nodes recover, they need to retrieve any data that was written on their behalf to alternative nodes during their downtime.</p>\n<p>The hinted handoff process works as follows:</p>\n<ol>\n<li><strong>Detect node recovery</strong> through gossip protocol or direct health checks</li>\n<li><strong>Query hint storage</strong> on nodes that might have stored data on behalf of the recovered node</li>\n<li><strong>Transfer hinted data</strong> from temporary storage locations to the recovered node</li>\n<li><strong>Apply conflict resolution</strong> if the recovered node already has versions of transferred keys</li>\n<li><strong>Clean up hint storage</strong> after successful transfer to avoid accumulating stale hints</li>\n<li><strong>Resume normal operations</strong> with the recovered node participating in its assigned key ranges</li>\n</ol>\n<p><strong>Consistency monitoring</strong> tracks the health of the replication system by measuring metrics like replica divergence rates, repair operation frequency, and time-to-consistency for updates. High divergence rates might indicate network problems, overloaded nodes, or bugs in the synchronization logic.</p>\n<p><strong>Partition recovery</strong> requires special handling when network splits resolve. Nodes that were isolated during a partition may have accepted writes that conflict with writes accepted by the majority partition. The recovery process must identify these conflicts, apply resolution policies, and ensure that all nodes converge on the same final state.</p>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The replication and consistency subsystem requires several key architectural decisions that significantly impact system behavior, performance, and operational complexity.</p>\n<blockquote>\n<p><strong>Decision: Async Replication with Synchronous Quorum Acknowledgment</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance consistency guarantees with write performance and avoid blocking clients on the slowest replica</li>\n<li><strong>Options Considered</strong>: Fully synchronous replication, fully asynchronous replication, hybrid quorum-based approach</li>\n<li><strong>Decision</strong>: Send writes to all replicas concurrently but only wait for quorum acknowledgments before responding to client</li>\n<li><strong>Rationale</strong>: Synchronous replication to all replicas creates latency spikes due to tail latency. Fully async provides no consistency guarantees. Quorum approach provides tunable consistency with predictable latency.</li>\n<li><strong>Consequences</strong>: Enables strong consistency with better performance than full synchronous replication. Requires read repair to handle replicas that lag behind the quorum.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Per-Operation Consistency Level Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Different cache entries have different consistency requirements, and applications need flexibility to trade consistency for performance</li>\n<li><strong>Options Considered</strong>: Cluster-wide consistency level, per-key consistency levels, per-operation consistency levels</li>\n<li><strong>Decision</strong>: Allow clients to specify consistency level on each individual operation</li>\n<li><strong>Rationale</strong>: Maximum flexibility allows applications to use strong consistency for critical data and eventual consistency for performance-sensitive operations. Per-key levels require additional metadata storage and complexity.</li>\n<li><strong>Consequences</strong>: Enables optimal performance tuning by applications. Requires more complex client implementation and careful consideration of consistency semantics by application developers.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Vector Clock Metadata Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Vector clocks provide superior conflict resolution but require additional storage overhead and complexity</li>\n<li><strong>Options Considered</strong>: Physical timestamps only, logical timestamps, vector clocks, hybrid timestamp approach</li>\n<li><strong>Decision</strong>: Store vector clocks with each cache entry, with pruning for departed nodes</li>\n<li><strong>Rationale</strong>: Vector clocks correctly handle causality and concurrent updates. Physical timestamps are unreliable due to clock skew. The storage overhead is acceptable for the semantic correctness gained.</li>\n<li><strong>Consequences</strong>: Increases per-entry storage overhead by ~50-100 bytes depending on cluster size. Provides correct conflict resolution semantics and enables reliable replica synchronization.</li>\n</ul>\n</blockquote>\n<p><strong>Replication metadata storage</strong> extends the basic <code>CacheEntry</code> structure to include version information and replication status:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>VectorClock</code></td>\n<td><code>map[string]uint64</code></td>\n<td>Logical timestamp from each node that has written this key</td>\n</tr>\n<tr>\n<td><code>ReplicationStatus</code></td>\n<td><code>map[string]ReplicationState</code></td>\n<td>Status of this entry on each replica node</td>\n</tr>\n<tr>\n<td><code>LastModified</code></td>\n<td><code>time.Time</code></td>\n<td>Physical timestamp of the most recent write operation</td>\n</tr>\n<tr>\n<td><code>ConflictResolutionPolicy</code></td>\n<td><code>string</code></td>\n<td>Strategy to use for resolving concurrent updates</td>\n</tr>\n</tbody></table>\n<p><strong>Quorum configuration</strong> can be specified at multiple levels to provide flexibility:</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Level</th>\n<th>Priority</th>\n<th>Example</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-operation</td>\n<td>Highest</td>\n<td><code>GetRequest{ConsistencyLevel: &quot;STRONG&quot;}</code></td>\n<td>Critical individual operations</td>\n</tr>\n<tr>\n<td>Per-key pattern</td>\n<td>Medium</td>\n<td><code>users:*</code> requires strong consistency</td>\n<td>Different data types have different requirements</td>\n</tr>\n<tr>\n<td>Client default</td>\n<td>Low</td>\n<td>Client configured for eventual consistency</td>\n<td>Application-level consistency preferences</td>\n</tr>\n<tr>\n<td>Cluster default</td>\n<td>Lowest</td>\n<td>Cluster-wide fallback settings</td>\n<td>System-wide baseline behavior</td>\n</tr>\n</tbody></table>\n<p><strong>Failure handling policies</strong> determine how the system behaves when quorum requirements cannot be met:</p>\n<ul>\n<li><strong>Strict quorum</strong>: Fail operations immediately if insufficient replicas are available</li>\n<li><strong>Sloppy quorum</strong>: Accept writes to alternative nodes with hinted handoff</li>\n<li><strong>Best-effort</strong>: Succeed operations with whatever replicas are available, relying on repair to achieve consistency later</li>\n<li><strong>Client choice</strong>: Allow per-operation specification of failure handling behavior</li>\n</ul>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Ignoring Vector Clock Pruning</strong>\nVector clocks grow with cluster size and retain entries for all nodes that have ever written a key. Without proper pruning, vector clocks can consume significant memory and create performance problems. This is particularly dangerous in dynamic clusters where nodes frequently join and leave. The fix is to implement a pruning policy that removes entries for nodes that have been gone for a configurable time period, but only after ensuring all references have been cleaned up through anti-entropy processes.</p>\n<p>⚠️ <strong>Pitfall: Assuming Physical Clock Synchronization</strong>\nMany developers assume that server clocks are closely synchronized and use physical timestamps for conflict resolution. However, even small clock skew (milliseconds) can cause incorrect ordering decisions that lead to lost updates or inconsistent data. NTP synchronization is not sufficient for distributed system ordering. The solution is to use logical timestamps (Lamport clocks) or vector clocks that capture causal relationships without depending on physical time synchronization.</p>\n<p>⚠️ <strong>Pitfall: Blocking on the Slowest Replica</strong>\nWhen implementing quorum systems, it&#39;s tempting to send requests to all replicas and wait for them to complete. However, this creates tail latency problems where one slow replica delays the entire operation. The correct approach is to send requests to more replicas than the quorum size (typically all replicas) but return success as soon as the quorum threshold is met. This provides both fault tolerance and performance optimization.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Replica Placement During Ring Changes</strong>\nWhen nodes join or leave the cluster, the replica placement algorithm must ensure that all nodes agree on which replicas store each key. If different nodes calculate different replica sets during transitional periods, writes might go to different sets of nodes, creating permanent inconsistencies. The solution is to use deterministic replica placement based on the stable hash ring state and coordinate ring updates through the gossip protocol.</p>\n<p>⚠️ <strong>Pitfall: Read Repair Race Conditions</strong>\nRead repair attempts to fix inconsistencies by updating stale replicas during read operations. However, concurrent read repairs or writes can create race conditions where multiple nodes simultaneously try to &quot;fix&quot; the same replica. This can result in thrashing or lost updates. The fix is to use the same vector clock mechanisms for read repair operations as for regular writes, ensuring that repairs are properly ordered and don&#39;t overwrite newer updates.</p>\n<p>⚠️ <strong>Pitfall: Unbounded Hinted Handoff Storage</strong>\nHinted handoff stores writes for temporarily unavailable nodes, but if nodes stay down for extended periods, the hints can consume unbounded storage space. Additionally, stale hints can conflict with data that was written after node recovery. The solution is to implement hint expiration policies (typically 3-6 hours) and perform conflict resolution when delivering hints to recovered nodes.</p>\n<p>⚠️ <strong>Pitfall: Split-Brain During Network Partitions</strong>\nNetwork partitions can split the cluster into multiple components, each of which might continue accepting writes with reduced quorum sizes. When the partition heals, these divergent writes create conflicts that may not be resolvable without data loss. The solution is to require strict majority quorums (W &gt; N/2) for writes and potentially implement additional partition detection mechanisms.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The replication and consistency layer builds on top of the cache node and cluster communication components to provide fault tolerance and configurable consistency semantics. This guidance focuses on the core replication algorithms and conflict resolution mechanisms.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Vector Clocks</td>\n<td><code>map[string]uint64</code> with JSON serialization</td>\n<td>Custom binary encoding with compression</td>\n</tr>\n<tr>\n<td>Conflict Resolution</td>\n<td>Last-write-wins with physical timestamps</td>\n<td>Full vector clock causality detection</td>\n</tr>\n<tr>\n<td>Anti-Entropy</td>\n<td>Periodic full scan with direct comparison</td>\n<td>Merkle trees with incremental sync</td>\n</tr>\n<tr>\n<td>Metadata Storage</td>\n<td>In-memory maps with periodic snapshots</td>\n<td>Embedded database (BadgerDB/BoltDB)</td>\n</tr>\n</tbody></table>\n<p><strong>File Structure Extensions:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/replication/\n  replication.go           ← main replication manager\n  vector_clock.go          ← vector clock implementation\n  conflict_resolver.go     ← conflict resolution strategies\n  anti_entropy.go          ← background repair processes\n  quorum.go               ← quorum-based operation logic\n  hinted_handoff.go       ← temporary storage for unavailable nodes\n  replication_test.go     ← comprehensive test suite</code></pre></div>\n\n<p><strong>Core Data Structures:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ReplicationManager coordinates replica placement and consistency operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ReplicationManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashRing         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transport        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localStorage     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    vectorClocks     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">VectorClock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hintedHandoff    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HintEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config           </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    repairScheduler  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AntiEntropyScheduler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// VectorClock tracks logical time from each node's perspective</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> VectorClock</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Clock </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\"> `json:\"clock\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReplicatedEntry extends cache entries with replication metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ReplicatedEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    *</span><span style=\"color:#B392F0\">CacheEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VectorClock     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#9ECBFF\">           `json:\"vector_clock\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicationInfo </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">ReplicaInfo</span><span style=\"color:#9ECBFF\"> `json:\"replication_info\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ReplicationConfig defines system-wide replication parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ReplicationConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicationFactor   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"replication_factor\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DefaultReadQuorum   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"default_read_quorum\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DefaultWriteQuorum  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"default_write_quorum\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConflictResolution  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"conflict_resolution\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AntiEntropyInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"anti_entropy_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HintedHandoffTTL    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"hinted_handoff_ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Replication Manager Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// GetReplicas performs a quorum read operation with conflict resolution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetReplicas</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">GetRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GetResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate target replica nodes using hash ring and replication factor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine read quorum size from request or default configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Send concurrent read requests to all replica nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Wait for responses from at least read_quorum nodes (with timeout)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If multiple versions returned, apply conflict resolution using vector clocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Trigger read repair for any stale replicas (async)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return resolved value to client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use errgroup for concurrent requests with context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetReplicas performs a quorum write operation with vector clock update</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetReplicas</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">SetRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SetResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate new vector clock entry for this write operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate target replica nodes using consistent hashing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Determine write quorum size from request or configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Send concurrent write requests to all replicas with vector clock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Wait for acknowledgments from at least write_quorum nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle partial failures with hinted handoff for unavailable nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return success only after quorum acknowledgments received</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Store hints for failed replicas to enable later delivery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Vector Clock Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// NewVectorClock creates a new vector clock initialized for the local node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewVectorClock</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Clock: </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">{nodeID: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Update increments the clock for the specified node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">vc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Update</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock for thread safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Increment clock entry for specified nodeID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize entry to 1 if nodeID not present in clock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use vc.Clock[nodeID]++ but handle zero-value case</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Compare determines the relationship between two vector clocks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">vc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Compare</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">VectorClockRelation</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Compare all entries in both vector clocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return BEFORE if all entries in vc &#x3C;= other and at least one is strictly less</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return AFTER if all entries in vc >= other and at least one is strictly greater  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return EQUAL if all entries are identical</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return CONCURRENT if neither dominates (mixed greater/less relationships)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Need to handle missing entries as 0 values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Merge combines two vector clocks by taking maximum of each entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">vc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Merge</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">other</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">VectorClock</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock for thread safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through all entries in both clocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set each entry to max(vc.Clock[node], other.Clock[node])</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Include entries that exist in either clock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use math.Max() but convert uint64 appropriately</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Conflict Resolution Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ResolveConflicts determines the authoritative value among conflicting replicas</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ResolveConflicts</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">entries</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicatedEntry</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicatedEntry</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicatedEntry</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Build causality graph using vector clock comparisons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Identify entries that are not dominated by any other entry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If single winner, return it along with stale entries for repair</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If multiple concurrent entries, apply configured resolution policy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: For last-write-wins, use physical timestamps as tiebreaker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create merged entry with updated vector clock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return winner and list of entries that need repair</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: concurrent entries have vector clocks where neither dominates the other</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TriggerReadRepair asynchronously updates stale replicas with resolved value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TriggerReadRepair</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">winner</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ReplicatedEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">staleReplicas</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create repair request with resolved value and vector clock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Send async write requests to all nodes with stale replicas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use winner's vector clock to prevent repair from appearing as concurrent update</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle repair failures gracefully (log but don't block client)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update local metadata to track repair operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use goroutines for async repair but limit concurrency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Hinted Handoff Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// StoreHint saves a write operation for later delivery to an unavailable node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StoreHint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">targetNode</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">WriteOperation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create hint entry with target node, operation, and timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add hint to local storage with expiration time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure hint storage doesn't exceed memory limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Persist hints to survive local node restarts (optional)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use TTL to automatically expire old hints</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DeliverHints transfers stored hints to a recovered node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DeliverHints</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">recoveredNode</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Retrieve all hints stored for the recovered node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Group hints by key to handle multiple updates efficiently  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply conflict resolution if multiple hints exist for same key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Send resolved hints to recovered node with vector clocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Delete successfully delivered hints from local storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle delivery failures with exponential backoff retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Batch multiple hints in single request for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints:</strong></p>\n<p>After implementing vector clocks and basic replication:</p>\n<ul>\n<li>Run <code>go test ./internal/replication -v</code> to verify vector clock operations</li>\n<li>Test with 3-node cluster: set replication factor to 3, write a key, verify it appears on all nodes</li>\n<li>Simulate node failure: stop one node, write should still succeed with quorum</li>\n<li>Verify read repair: restart failed node, read operations should update its stale data</li>\n</ul>\n<p>After implementing quorum operations:</p>\n<ul>\n<li>Test different consistency levels: write with STRONG, read with EVENTUAL</li>\n<li>Verify quorum math: with RF=3, try R=2/W=2 (strong) vs R=1/W=1 (eventual)  </li>\n<li>Test partial failures: network partition 2 nodes from 1, verify availability behavior</li>\n<li>Measure performance impact: compare latencies with and without quorum requirements</li>\n</ul>\n<p>After implementing conflict resolution:</p>\n<ul>\n<li>Create artificial conflicts: partition cluster, write different values, heal partition</li>\n<li>Verify vector clock comparison correctly identifies concurrent vs ordered updates</li>\n<li>Test conflict resolution policies: last-write-wins vs application merge</li>\n<li>Verify read repair: conflicted keys should converge after resolution</li>\n</ul>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section synthesizes concepts from all milestones to show how the complete distributed cache system operates end-to-end, with particular relevance to Milestone 3 (Cluster Communication) and Milestone 4 (Replication &amp; Consistency).</p>\n</blockquote>\n<p>Understanding how components interact is crucial for building a cohesive distributed system. This section explores the communication patterns, message formats, and operational flows that bind together the hash ring, cache nodes, gossip protocol, and replication mechanisms into a functioning distributed cache.</p>\n<p>Think of the distributed cache as a <strong>well-orchestrated symphony</strong> where each musician (component) must know their part, when to play, and how to harmonize with others. The conductor&#39;s score (message protocols) ensures everyone plays in sync, while the sheet music (data formats) provides the common language. Just as a symphony creates beautiful music through coordinated individual performances, our distributed cache delivers high-performance caching through coordinated component interactions.</p>\n<h3 id=\"message-formats-and-protocols\">Message Formats and Protocols</h3>\n<p>The distributed cache communicates through a structured message protocol that enables reliable information exchange between nodes. Every interaction follows defined message formats that include routing information, operation parameters, and metadata required for consistency and replication.</p>\n<h4 id=\"transport-layer-design\">Transport Layer Design</h4>\n<p>The system uses HTTP as the primary transport protocol, providing simplicity, debugging visibility, and universal tooling support. Each node exposes REST-like endpoints for different operation types, while maintaining the flexibility to upgrade to more efficient protocols like gRPC in the future.</p>\n<blockquote>\n<p><strong>Decision: HTTP-Based Transport</strong></p>\n<ul>\n<li><strong>Context</strong>: Need reliable inter-node communication protocol that&#39;s debuggable and widely supported</li>\n<li><strong>Options Considered</strong>: Raw TCP sockets, HTTP/REST, gRPC with Protocol Buffers</li>\n<li><strong>Decision</strong>: HTTP/REST with JSON payloads for initial implementation</li>\n<li><strong>Rationale</strong>: HTTP provides excellent debugging capabilities, universal tooling, built-in error codes, and straightforward request/response semantics. JSON offers human-readable messages during development and testing</li>\n<li><strong>Consequences</strong>: Slightly higher overhead than binary protocols, but gains in development velocity and operational simplicity outweigh performance costs for most cache workloads</li>\n</ul>\n</blockquote>\n<p>The <code>HTTPTransport</code> component handles all network communication, providing a consistent interface for message exchange regardless of the underlying operation type. It manages connection pooling, timeout handling, and retry logic transparently to higher-level components.</p>\n<h4 id=\"core-message-structure\">Core Message Structure</h4>\n<p>All inter-node messages follow a consistent envelope format that includes routing metadata, operation identification, and payload data. This uniform structure simplifies message handling and enables features like request tracing and operation logging across the cluster.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Type</td>\n<td>string</td>\n<td>Message type identifier (get, set, delete, gossip, replicate)</td>\n</tr>\n<tr>\n<td>Sender</td>\n<td>string</td>\n<td>NodeID of the originating node for routing responses</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>Message creation time for ordering and timeout detection</td>\n</tr>\n<tr>\n<td>Data</td>\n<td>interface{}</td>\n<td>Operation-specific payload containing request/response details</td>\n</tr>\n</tbody></table>\n<p>The <code>Message</code> structure serves as the universal container for all inter-node communication. The Type field enables efficient routing to appropriate handlers, while Sender information supports response correlation and loop detection in gossip protocols.</p>\n<h4 id=\"cache-operation-messages\">Cache Operation Messages</h4>\n<p>Cache operations use strongly-typed request and response structures that include all necessary parameters for distributed execution. Each operation type has dedicated message formats optimized for its specific requirements while maintaining consistency in error handling and metadata.</p>\n<p><strong>Get Operation Messages:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key</td>\n<td>string</td>\n<td>Cache key to retrieve from distributed storage</td>\n</tr>\n<tr>\n<td>ConsistencyLevel</td>\n<td>string</td>\n<td>Required consistency level (eventual, strong, quorum)</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key</td>\n<td>string</td>\n<td>Echo of requested key for response correlation</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>[]byte</td>\n<td>Retrieved cache value or empty if key not found</td>\n</tr>\n<tr>\n<td>Found</td>\n<td>bool</td>\n<td>Indicates whether key exists in cache storage</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>Value timestamp for conflict resolution and consistency</td>\n</tr>\n</tbody></table>\n<p>The <code>GetRequest</code> and <code>GetResponse</code> messages handle distributed cache lookups. The ConsistencyLevel field allows clients to specify their consistency requirements, enabling the system to choose appropriate quorum sizes and conflict resolution strategies.</p>\n<p><strong>Set Operation Messages:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key</td>\n<td>string</td>\n<td>Cache key for storing the provided value</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>[]byte</td>\n<td>Raw bytes to store in distributed cache</td>\n</tr>\n<tr>\n<td>TTL</td>\n<td>time.Duration</td>\n<td>Time-to-live for automatic expiration, zero means no expiration</td>\n</tr>\n<tr>\n<td>ConsistencyLevel</td>\n<td>string</td>\n<td>Required write consistency level for replication</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Success</td>\n<td>bool</td>\n<td>Indicates whether write operation completed successfully</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>Timestamp of successful write for ordering and conflicts</td>\n</tr>\n</tbody></table>\n<p>The <code>SetRequest</code> and <code>SetResponse</code> messages handle distributed cache writes. The TTL field enables per-key expiration policies, while the ConsistencyLevel determines how many replicas must acknowledge the write before considering it successful.</p>\n<p><strong>Delete Operation Messages:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key</td>\n<td>string</td>\n<td>Cache key to remove from distributed storage</td>\n</tr>\n<tr>\n<td>ConsistencyLevel</td>\n<td>string</td>\n<td>Required consistency level for delete propagation</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Success</td>\n<td>bool</td>\n<td>Indicates whether delete operation completed successfully</td>\n</tr>\n<tr>\n<td>Found</td>\n<td>bool</td>\n<td>Whether key existed before deletion attempt</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>Deletion timestamp for tombstone tracking and conflicts</td>\n</tr>\n</tbody></table>\n<p>The <code>DeleteRequest</code> and <code>DeleteResponse</code> messages handle distributed cache deletions. The Found field helps distinguish between deleting non-existent keys and actual removal operations, which is important for conflict resolution and cache statistics.</p>\n<h4 id=\"cluster-membership-messages\">Cluster Membership Messages</h4>\n<p>The gossip protocol uses specialized message formats for propagating cluster state information efficiently across the network. These messages include incremental updates, full state synchronization, and membership change notifications.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NodeStates</td>\n<td>map[string]NodeState</td>\n<td>Current state of all known cluster nodes</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint64</td>\n<td>Monotonic version number for detecting stale information</td>\n</tr>\n</tbody></table>\n<p>The <code>GossipMessage</code> carries cluster membership information between nodes. The Version field enables efficient anti-entropy by allowing nodes to quickly determine whether received gossip contains newer information than their current state.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NodeID</td>\n<td>string</td>\n<td>Unique identifier for the cluster node</td>\n</tr>\n<tr>\n<td>Address</td>\n<td>string</td>\n<td>Network address for connecting to the node</td>\n</tr>\n<tr>\n<td>Status</td>\n<td>string</td>\n<td>Current node status (active, suspected, failed, leaving)</td>\n</tr>\n<tr>\n<td>LastSeen</td>\n<td>time.Time</td>\n<td>Timestamp of last successful communication with node</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>uint64</td>\n<td>Version number for this node&#39;s state information</td>\n</tr>\n</tbody></table>\n<p>The <code>NodeState</code> structure represents a single node&#39;s status within the cluster. The Status field drives failure detection and recovery processes, while LastSeen enables timeout-based failure detection when gossip messages stop arriving.</p>\n<h4 id=\"replication-protocol-messages\">Replication Protocol Messages</h4>\n<p>Replication operations require additional metadata for conflict resolution and consistency guarantees. These messages include vector clocks, replica coordination information, and conflict resolution data.</p>\n<p>Internal replication messages extend the basic cache operations with consistency metadata:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Operation</td>\n<td>string</td>\n<td>Type of replicated operation (get, set, delete)</td>\n</tr>\n<tr>\n<td>Key</td>\n<td>string</td>\n<td>Target key for the replication operation</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>[]byte</td>\n<td>Data payload for set operations, empty for get/delete</td>\n</tr>\n<tr>\n<td>VectorClock</td>\n<td>*VectorClock</td>\n<td>Logical timestamp for causality tracking</td>\n</tr>\n<tr>\n<td>QuorumSize</td>\n<td>int</td>\n<td>Required number of successful replicas for operation</td>\n</tr>\n<tr>\n<td>CoordinatorID</td>\n<td>string</td>\n<td>Node coordinating this replication operation</td>\n</tr>\n</tbody></table>\n<p>These replication messages ensure that write operations maintain consistency across replicas while providing the metadata necessary for conflict detection and resolution.</p>\n<h3 id=\"operation-sequence-flows\">Operation Sequence Flows</h3>\n<p>Understanding the step-by-step flow of cache operations reveals how the distributed system coordinates between components to deliver consistent, high-performance caching. Each operation type follows a specific sequence that balances performance, consistency, and fault tolerance requirements.</p>\n<h4 id=\"client-request-processing\">Client Request Processing</h4>\n<p>Every cache operation begins with a client request that must be routed to the appropriate cluster nodes. The routing process uses consistent hashing to determine target nodes while considering replication requirements and current cluster membership.</p>\n<p><strong>Initial Request Routing Flow:</strong></p>\n<ol>\n<li><strong>Client Connection</strong>: Client connects to any cluster node using HTTP and sends a cache operation request (GET, SET, or DELETE)</li>\n<li><strong>Request Parsing</strong>: Receiving node parses the HTTP request and extracts the cache key, operation parameters, and consistency requirements</li>\n<li><strong>Hash Ring Consultation</strong>: Node calls <code>GetNode(key)</code> on the local hash ring to identify the primary node responsible for the requested key</li>\n<li><strong>Local vs Remote Decision</strong>: If the current node is responsible for the key, it processes the request locally; otherwise, it forwards the request to the appropriate node</li>\n<li><strong>Request Forwarding</strong>: For remote keys, the node uses <code>HTTPTransport.SendMessage()</code> to forward the complete request to the responsible node</li>\n<li><strong>Response Handling</strong>: The forwarding node receives the response from the responsible node and relays it back to the original client</li>\n</ol>\n<p>This routing flow ensures that clients can connect to any cluster node while still reaching the correct data storage location through transparent request forwarding.</p>\n<h4 id=\"get-operation-flow\">Get Operation Flow</h4>\n<p>Read operations prioritize low latency while respecting consistency requirements. The flow varies based on the requested consistency level, from simple local reads to quorum-based operations with conflict resolution.</p>\n<p><strong>Single-Node Get Flow (Eventual Consistency):</strong></p>\n<ol>\n<li><strong>Key Ownership Verification</strong>: Target node confirms it&#39;s responsible for the requested key using its local hash ring state</li>\n<li><strong>Local Cache Lookup</strong>: Node calls <code>LRUCache.Get(key)</code> to retrieve the value from its local storage</li>\n<li><strong>Expiration Check</strong>: If the entry exists, node calls <code>CacheEntry.IsExpired()</code> to verify the TTL hasn&#39;t elapsed</li>\n<li><strong>Response Generation</strong>: Node creates a <code>GetResponse</code> with the value (if found and not expired) and current timestamp</li>\n<li><strong>Cache Statistics Update</strong>: Node increments hit or miss counters in <code>CacheMetrics</code> for monitoring and debugging</li>\n<li><strong>Client Response</strong>: Node sends the HTTP response back to the client with appropriate status codes and headers</li>\n</ol>\n<p>This simple flow provides the lowest possible latency for read operations when strong consistency isn&#39;t required, as it avoids network calls to replica nodes.</p>\n<p><strong>Quorum-Based Get Flow (Strong Consistency):</strong></p>\n<ol>\n<li><strong>Replica Set Identification</strong>: Coordinator node calls <code>HashRing.GetNodes(key, replicationFactor)</code> to identify all replica nodes for the key</li>\n<li><strong>Parallel Read Requests</strong>: Coordinator simultaneously sends <code>GetRequest</code> messages to all replica nodes using <code>HTTPTransport.SendMessage()</code></li>\n<li><strong>Response Collection</strong>: Coordinator waits for responses from replica nodes, timing out requests that exceed the configured timeout period</li>\n<li><strong>Quorum Verification</strong>: Coordinator checks if the number of successful responses meets the required read quorum size</li>\n<li><strong>Conflict Detection</strong>: If multiple replicas return different values, coordinator compares their vector clocks to detect conflicts</li>\n<li><strong>Conflict Resolution</strong>: For conflicting values, coordinator calls <code>ReplicationManager.ResolveConflicts()</code> to determine the authoritative value</li>\n<li><strong>Read Repair Trigger</strong>: If stale replicas are detected, coordinator asynchronously calls <code>TriggerReadRepair()</code> to update them</li>\n<li><strong>Client Response</strong>: Coordinator returns the resolved value to the client, ensuring the response reflects the most recent consistent state</li>\n</ol>\n<p>This flow trades higher latency for stronger consistency guarantees by consulting multiple replicas and resolving any conflicts detected during the read operation.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<h4 id=\"set-operation-flow\">Set Operation Flow</h4>\n<p>Write operations must balance write latency against durability and consistency requirements. The flow ensures that writes are properly replicated while avoiding unnecessary delays in acknowledging successful operations to clients.</p>\n<p><strong>Single-Node Set Flow (Eventual Consistency):</strong></p>\n<ol>\n<li><strong>Request Validation</strong>: Target node validates the set request parameters, checking key size limits and value constraints</li>\n<li><strong>Cache Entry Creation</strong>: Node calls <code>NewCacheEntry(key, value, ttl)</code> to create a properly sized cache entry with expiration information</li>\n<li><strong>Memory Check</strong>: Node verifies that storing the entry won&#39;t exceed its memory limits, potentially triggering LRU eviction</li>\n<li><strong>Local Storage</strong>: Node calls <code>LRUCache.Set(key, value, ttl)</code> to store the entry in its local cache</li>\n<li><strong>Response Generation</strong>: Node creates a <code>SetResponse</code> with success status and current timestamp</li>\n<li><strong>Asynchronous Replication</strong>: Node asynchronously replicates the write to other nodes without blocking the client response</li>\n<li><strong>Client Response</strong>: Node immediately responds to the client, acknowledging the write operation completion</li>\n</ol>\n<p>This flow provides low write latency by acknowledging writes after local storage while ensuring eventual consistency through background replication.</p>\n<p><strong>Quorum-Based Set Flow (Strong Consistency):</strong></p>\n<ol>\n<li><strong>Vector Clock Update</strong>: Coordinator node calls <code>VectorClock.Update(nodeID)</code> to increment its logical timestamp</li>\n<li><strong>Replica Set Identification</strong>: Coordinator identifies all replica nodes using <code>HashRing.GetNodes(key, replicationFactor)</code></li>\n<li><strong>Replicated Entry Creation</strong>: Coordinator creates a <code>ReplicatedEntry</code> containing the cache entry, vector clock, and replication metadata</li>\n<li><strong>Parallel Write Requests</strong>: Coordinator simultaneously sends write requests to all replica nodes with the complete replication information</li>\n<li><strong>Response Collection</strong>: Coordinator waits for acknowledgments from replica nodes, tracking success and failure responses</li>\n<li><strong>Write Quorum Verification</strong>: Coordinator verifies that the number of successful writes meets the required write quorum size</li>\n<li><strong>Failure Handling</strong>: For failed replicas, coordinator stores hinted handoff entries using <code>ReplicationManager.StoreHint()</code></li>\n<li><strong>Client Acknowledgment</strong>: Once write quorum is achieved, coordinator responds to the client confirming successful write completion</li>\n</ol>\n<p>This flow ensures that writes are durable across multiple nodes before acknowledging success to clients, providing strong consistency guarantees at the cost of higher write latency.</p>\n<h4 id=\"delete-operation-flow\">Delete Operation Flow</h4>\n<p>Delete operations require special handling to manage tombstones and ensure that deletions propagate correctly across replicas. The flow must distinguish between deleting non-existent keys and actual data removal.</p>\n<p><strong>Standard Delete Flow:</strong></p>\n<ol>\n<li><strong>Key Existence Check</strong>: Target node first attempts to retrieve the key using <code>LRUCache.Get(key)</code> to determine if it exists</li>\n<li><strong>Tombstone Creation</strong>: For existing keys, node creates a tombstone entry with a deletion timestamp instead of immediately removing the data</li>\n<li><strong>Local Deletion</strong>: Node calls <code>LRUCache.Delete(key)</code> to remove the entry from its local storage</li>\n<li><strong>Replication Coordination</strong>: Node identifies replica nodes and sends delete requests with tombstone information to each replica</li>\n<li><strong>Replica Acknowledgments</strong>: Node waits for acknowledgments from replicas, ensuring deletion propagates across the replica set</li>\n<li><strong>Response Generation</strong>: Node creates a <code>DeleteResponse</code> indicating success and whether the key existed before deletion</li>\n<li><strong>Cleanup Scheduling</strong>: Node schedules the tombstone for eventual garbage collection after a configured retention period</li>\n</ol>\n<p>This flow ensures that deletions are properly coordinated across replicas while maintaining the ability to distinguish between deleting existing versus non-existent keys.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fcache-operation-flow.svg\" alt=\"Cache Operation Sequence\"></p>\n<h3 id=\"replication-and-consistency-flows\">Replication and Consistency Flows</h3>\n<p>Replication flows handle the complex coordination required to maintain data consistency across multiple nodes while providing configurable consistency levels. These flows manage conflict resolution, vector clock maintenance, and repair operations that keep the distributed cache coherent.</p>\n<h4 id=\"write-replication-coordination\">Write Replication Coordination</h4>\n<p>Write operations with replication require careful coordination to ensure that the specified number of replicas successfully store the data before acknowledging the write to the client. The coordination process handles failures gracefully while maintaining consistency invariants.</p>\n<p><strong>Primary-Backup Write Flow:</strong></p>\n<ol>\n<li><strong>Coordinator Selection</strong>: The node receiving the client write request becomes the coordinator for this operation, regardless of whether it&#39;s a replica</li>\n<li><strong>Replica Set Determination</strong>: Coordinator calls <code>HashRing.GetNodes(key, config.ReplicationFactor)</code> to identify the ordered list of replica nodes</li>\n<li><strong>Vector Clock Increment</strong>: Coordinator increments its vector clock using <code>VectorClock.Update(coordinatorID)</code> to establish causality</li>\n<li><strong>Write Message Creation</strong>: Coordinator creates replication messages containing the key, value, vector clock, and coordination metadata</li>\n<li><strong>Parallel Replica Writes</strong>: Coordinator sends write messages to all replica nodes simultaneously using <code>HTTPTransport.SendMessage()</code></li>\n<li><strong>Success Tracking</strong>: Coordinator maintains a count of successful replica acknowledgments and any error responses received</li>\n<li><strong>Quorum Achievement</strong>: Once <code>config.DefaultWriteQuorum</code> successful responses are received, the coordinator can acknowledge success to the client</li>\n<li><strong>Hinted Handoff</strong>: For failed replica writes, coordinator stores hints using <code>ReplicationManager.StoreHint()</code> for later delivery</li>\n<li><strong>Client Acknowledgment</strong>: Coordinator responds to the client indicating successful completion once write quorum requirements are met</li>\n</ol>\n<p>This coordination flow ensures that writes achieve the required durability level while handling replica failures through hinted handoff mechanisms.</p>\n<h4 id=\"read-repair-and-consistency\">Read Repair and Consistency</h4>\n<p>Read operations with strong consistency requirements must detect and repair inconsistencies between replicas. The read repair process ensures that stale replicas are updated with the most recent values without impacting read operation latency.</p>\n<p><strong>Read-Time Consistency Flow:</strong></p>\n<ol>\n<li><strong>Multi-Replica Read</strong>: Coordinator sends read requests to all replicas in the replica set for the requested key</li>\n<li><strong>Response Collection</strong>: Coordinator collects responses from replicas, noting any timeouts or error responses from unavailable nodes</li>\n<li><strong>Vector Clock Comparison</strong>: For each successful response, coordinator compares vector clocks using <code>VectorClock.Compare()</code> to detect conflicts</li>\n<li><strong>Consistency Detection</strong>: Coordinator identifies whether all replicas returned the same value and vector clock, indicating consistency</li>\n<li><strong>Conflict Resolution</strong>: If conflicts exist, coordinator calls <code>ReplicationManager.ResolveConflicts()</code> to determine the authoritative value</li>\n<li><strong>Stale Replica Identification</strong>: Coordinator identifies replicas that returned stale values by comparing their vector clocks to the winning value</li>\n<li><strong>Client Response</strong>: Coordinator immediately returns the resolved value to the client without waiting for repairs</li>\n<li><strong>Asynchronous Repair</strong>: Coordinator triggers <code>TriggerReadRepair()</code> to asynchronously update stale replicas with the authoritative value</li>\n</ol>\n<p>This flow provides strong consistency for read operations while ensuring that detected inconsistencies are repaired without impacting client response times.</p>\n<h4 id=\"anti-entropy-and-background-repair\">Anti-Entropy and Background Repair</h4>\n<p>The anti-entropy process runs continuously in the background to detect and repair inconsistencies that might arise from network partitions, node failures, or other edge cases. This process ensures eventual consistency even when read repair doesn&#39;t catch all inconsistencies.</p>\n<p><strong>Periodic Anti-Entropy Flow:</strong></p>\n<ol>\n<li><strong>Repair Schedule Activation</strong>: The <code>AntiEntropyScheduler</code> periodically triggers repair operations based on <code>ReplicationConfig.AntiEntropyInterval</code></li>\n<li><strong>Peer Selection</strong>: Each node randomly selects peer nodes for comparison, avoiding excessive network traffic by limiting concurrent repairs</li>\n<li><strong>Key Range Exchange</strong>: Selected peers exchange information about their key ranges and the vector clocks of stored entries</li>\n<li><strong>Difference Detection</strong>: Each node compares received peer information with its local state to identify inconsistencies</li>\n<li><strong>Conflict Resolution</strong>: For detected conflicts, nodes use <code>ReplicationManager.ResolveConflicts()</code> to determine authoritative values</li>\n<li><strong>Repair Message Exchange</strong>: Nodes exchange repair messages containing the resolved values and updated vector clocks</li>\n<li><strong>Local State Update</strong>: Each node updates its local cache with any newer values discovered during the anti-entropy process</li>\n<li><strong>Repair Metrics Update</strong>: Nodes update repair statistics for monitoring the health of the anti-entropy process</li>\n</ol>\n<p>This background process ensures that the system maintains consistency even in the presence of failures or network issues that might prevent immediate consistency.</p>\n<h4 id=\"hinted-handoff-delivery\">Hinted Handoff Delivery</h4>\n<p>When replica nodes are unavailable during write operations, the coordinator stores hints that must be delivered when the failed nodes recover. The hinted handoff delivery process ensures that these temporary writes are properly applied.</p>\n<p><strong>Hint Delivery Flow:</strong></p>\n<ol>\n<li><strong>Node Recovery Detection</strong>: The gossip protocol notifies nodes when a previously failed node recovers and rejoins the cluster</li>\n<li><strong>Hint Queue Consultation</strong>: Nodes check their local hint storage for any pending writes destined for the recovered node</li>\n<li><strong>Hint Validation</strong>: Each hint is validated to ensure it hasn&#39;t exceeded the <code>ReplicationConfig.HintedHandoffTTL</code> expiration time</li>\n<li><strong>Delivery Attempt</strong>: Nodes call <code>ReplicationManager.DeliverHints()</code> to send accumulated writes to the recovered node</li>\n<li><strong>Recovery Acknowledgment</strong>: The recovered node processes delivered hints and acknowledges successful application</li>\n<li><strong>Hint Cleanup</strong>: Successfully delivered hints are removed from the local hint storage to prevent duplicate delivery</li>\n<li><strong>Failure Retry</strong>: If hint delivery fails, nodes reschedule delivery attempts with exponential backoff until TTL expiration</li>\n</ol>\n<p>This process ensures that writes accepted during node failures are eventually applied to all intended replicas, maintaining the system&#39;s durability guarantees.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The combination of synchronous quorum operations, asynchronous read repair, periodic anti-entropy, and hinted handoff provides multiple layers of consistency maintenance. This layered approach allows the system to provide immediate consistency for critical operations while ensuring eventual consistency through background processes.</p>\n</blockquote>\n<h4 id=\"consistency-level-implementation\">Consistency Level Implementation</h4>\n<p>The system supports multiple consistency levels that trade performance against consistency guarantees. Each consistency level uses different combinations of the above flows to achieve its specific guarantees.</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Level</th>\n<th>Read Behavior</th>\n<th>Write Behavior</th>\n<th>Guarantees</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Eventual</td>\n<td>Single replica read</td>\n<td>Local write + async replication</td>\n<td>Lowest latency, eventual consistency</td>\n</tr>\n<tr>\n<td>Quorum</td>\n<td>Read from R replicas</td>\n<td>Write to W replicas</td>\n<td>Configurable consistency vs. performance</td>\n</tr>\n<tr>\n<td>Strong</td>\n<td>Read from all replicas</td>\n<td>Write to all replicas</td>\n<td>Immediate consistency, highest latency</td>\n</tr>\n</tbody></table>\n<p>The implementation allows clients to specify consistency requirements per operation, enabling applications to choose appropriate consistency levels for different data types and use cases.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building the interaction and data flow components requires careful attention to message serialization, network error handling, and operation coordination. The implementation must handle concurrent operations, network failures, and partial replica failures gracefully.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Transport</td>\n<td>net/http with JSON encoding</td>\n<td>gRPC with Protocol Buffers</td>\n</tr>\n<tr>\n<td>Message Serialization</td>\n<td>encoding/json with struct tags</td>\n<td>Protocol Buffers with code generation</td>\n</tr>\n<tr>\n<td>Request Routing</td>\n<td>HTTP client with connection pooling</td>\n<td>Load balancer with health checking</td>\n</tr>\n<tr>\n<td>Operation Coordination</td>\n<td>Goroutines with channels</td>\n<td>Actor model with message passing</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Standard error types with wrapping</td>\n<td>Structured error codes with context</td>\n</tr>\n<tr>\n<td>Request Tracing</td>\n<td>Simple request ID propagation</td>\n<td>OpenTelemetry distributed tracing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/transport/\n  http_transport.go          ← HTTP-based inter-node communication\n  http_transport_test.go     ← Transport layer tests\n  message_types.go           ← Message format definitions\n  \ninternal/operations/\n  coordinator.go             ← Operation coordination logic\n  get_operation.go           ← GET operation implementation\n  set_operation.go           ← SET operation implementation  \n  delete_operation.go        ← DELETE operation implementation\n  replication_coordinator.go ← Replication coordination\n  \ninternal/consistency/\n  vector_clock.go           ← Vector clock implementation\n  conflict_resolver.go      ← Conflict resolution strategies\n  read_repair.go            ← Read repair coordination\n  anti_entropy.go           ← Background consistency maintenance</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>HTTP Transport Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HTTPTransport provides HTTP-based inter-node communication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HTTPTransport</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHTTPTransport</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Transport: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Transport</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                MaxIdleConns:        </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                MaxIdleConnsPerHost: </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                IdleConnTimeout:     </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to marshal message: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"http://</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/api/message\"</span><span style=\"color:#E1E4E8\">, address), bytes.</span><span style=\"color:#B392F0\">NewBuffer</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to send request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"request failed with status: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Buffer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result.</span><span style=\"color:#B392F0\">ReadFrom</span><span style=\"color:#E1E4E8\">(resp.Body)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read response: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result.</span><span style=\"color:#B392F0\">Bytes</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"http://</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/health\"</span><span style=\"color:#E1E4E8\">, address), </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create health check request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check failed with status: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Message Type Definitions:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> transport</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Message is the envelope for all inter-node communication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Message</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Sender    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"sender\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">   `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data      </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Cache operation request/response types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">    `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Found     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value            []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#9ECBFF\">        `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TTL              </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"consistency_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SetResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsistencyLevel </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"consistency_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DeleteResponse</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Found     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"found\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Cluster membership types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GossipMessage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeStates </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#9ECBFF\"> `json:\"node_states\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version    </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">               `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSeen </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"last_seen\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version  </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">    `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Operation Coordinator Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> operations</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// OperationCoordinator handles distributed cache operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> OperationCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashRing   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transport  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localCache </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheMetrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Get performs a distributed get operation with configurable consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OperationCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">GetRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GetResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use HashRing.GetNode(req.Key) to find the responsible node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If current node is responsible, perform local get from LRUCache</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If remote node responsible, forward request using HTTPTransport.SendMessage()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For quorum reads, call GetNodes() to get replica set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send parallel requests to all replicas using goroutines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Collect responses and check if read quorum is satisfied</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: If multiple responses, compare timestamps/vector clocks for conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Trigger read repair asynchronously if stale replicas detected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Update CacheMetrics hit/miss counters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Return GetResponse with resolved value and metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Set performs a distributed set operation with replication coordination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OperationCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">SetRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SetResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create CacheEntry with NewCacheEntry(key, value, ttl)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Use HashRing.GetNodes(req.Key, replicationFactor) to get replica set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update vector clock if using strong consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create replication messages with entry and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send parallel write requests to all replicas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Collect acknowledgments from replicas with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Check if write quorum satisfied (successful_writes >= WriteQuorum)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: For failed replicas, store hints using StoreHint()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Update CacheMetrics set counters and memory usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Return SetResponse indicating success/failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Delete performs a distributed delete operation with tombstone management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">OperationCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">DeleteRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DeleteResponse</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use HashRing.GetNodes(req.Key, replicationFactor) to get replica set  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create tombstone entry with deletion timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Send delete requests to all replica nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Track which replicas acknowledge the deletion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify delete quorum satisfied before returning success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Schedule tombstone cleanup after configured retention period</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update CacheMetrics delete counters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return DeleteResponse with success status and found indicator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<ul>\n<li>Use <code>context.Context</code> for request timeouts and cancellation throughout the operation flows</li>\n<li>Leverage <code>sync.WaitGroup</code> for coordinating parallel replica operations during quorum reads/writes  </li>\n<li>Use <code>time.After()</code> for implementing operation timeouts in select statements</li>\n<li>Consider using <code>sync.Map</code> for concurrent access to temporary operation state</li>\n<li>Use <code>encoding/json</code> with struct tags for message serialization - it&#39;s human-readable during debugging</li>\n<li>Implement exponential backoff for retry logic using <code>time.Sleep()</code> with increasing intervals</li>\n<li>Use <code>http.Client</code> with connection pooling configured for efficient inter-node communication</li>\n<li>Consider using buffered channels for collecting replica responses to avoid goroutine blocking</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the interaction and data flow components, verify the system with these checkpoints:</p>\n<p><strong>Basic Operation Flow Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start three nodes on different ports</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> -port=8001</span><span style=\"color:#79B8FF\"> -node-id=node1</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> -port=8002</span><span style=\"color:#79B8FF\"> -node-id=node2</span><span style=\"color:#79B8FF\"> -join=localhost:8001</span><span style=\"color:#E1E4E8\"> &#x26;  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> -port=8003</span><span style=\"color:#79B8FF\"> -node-id=node3</span><span style=\"color:#79B8FF\"> -join=localhost:8001</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test basic operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> localhost:8001/api/set</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"key\":\"test\", \"value\":\"hello\"}'</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> localhost:8002/api/get?key=test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> DELETE</span><span style=\"color:#9ECBFF\"> localhost:8003/api/delete?key=test</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n<li>Set operation should succeed and return <code>{&quot;success&quot;: true}</code></li>\n<li>Get operation should return the value even when requested from different nodes</li>\n<li>Delete operation should successfully remove the key from all replicas</li>\n<li>Operations should be forwarded correctly between nodes based on hash ring assignment</li>\n</ul>\n<p><strong>Consistency Level Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test eventual consistency</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> localhost:8001/api/set</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"key\":\"test\", \"value\":\"eventual\", \"consistency_level\":\"eventual\"}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test strong consistency  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> localhost:8001/api/set</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '{\"key\":\"test\", \"value\":\"strong\", \"consistency_level\":\"strong\"}'</span></span></code></pre></div>\n\n<p><strong>Signs of Problems:</strong></p>\n<ul>\n<li><strong>Operations timing out</strong>: Check network connectivity and node health endpoints</li>\n<li><strong>Inconsistent responses</strong>: Verify hash ring state is synchronized across nodes via gossip</li>\n<li><strong>Failed forwarding</strong>: Check HTTPTransport configuration and error logging</li>\n<li><strong>Missing replicas</strong>: Verify replication factor configuration and quorum settings</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Operations always timeout</td>\n<td>Network connectivity or wrong addresses</td>\n<td>Check node health endpoints with curl</td>\n<td>Verify AdvertiseAddr in node configuration</td>\n</tr>\n<tr>\n<td>Inconsistent read results</td>\n<td>Hash ring synchronization issues</td>\n<td>Compare ring state across nodes</td>\n<td>Ensure gossip protocol is working properly</td>\n</tr>\n<tr>\n<td>Write operations fail</td>\n<td>Insufficient healthy replicas</td>\n<td>Check cluster membership and node status</td>\n<td>Reduce write quorum or fix failed nodes</td>\n</tr>\n<tr>\n<td>High read latency</td>\n<td>Unnecessary replica consultation</td>\n<td>Check consistency level configuration</td>\n<td>Use eventual consistency for read-heavy workloads</td>\n</tr>\n<tr>\n<td>Memory usage keeps growing</td>\n<td>Hints and tombstones not being cleaned up</td>\n<td>Check hint delivery and cleanup logs</td>\n<td>Verify TTL settings and cleanup scheduling</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones but is particularly critical for Milestone 3 (Cluster Communication) and Milestone 4 (Replication &amp; Consistency), as distributed systems failure handling becomes essential for system reliability and data consistency.</p>\n</blockquote>\n<p>Building a robust distributed cache requires comprehensive error handling that goes far beyond simple try-catch blocks. Distributed systems face unique challenges where partial failures, network partitions, and timing anomalies create complex failure scenarios that single-node systems never encounter. This section provides a systematic approach to identifying, detecting, and recovering from failures in our distributed cache system.</p>\n<h3 id=\"mental-model-the-emergency-response-network\">Mental Model: The Emergency Response Network</h3>\n<p>Think of our distributed cache as a network of emergency response stations (nodes) that must coordinate during both normal operations and crisis situations. Just as emergency responders have protocols for communication failures, equipment breakdowns, and coordination challenges, our distributed cache needs well-defined procedures for handling node failures, network partitions, and data inconsistencies.</p>\n<p>When a fire station loses radio contact, other stations don&#39;t immediately assume it&#39;s destroyed—they follow escalation procedures to verify the situation and redistribute responsibilities. Similarly, when a cache node becomes unresponsive, the cluster follows systematic detection, verification, and recovery procedures before marking it as failed and redistributing its data.</p>\n<p>The emergency response analogy helps us understand that failure handling in distributed systems requires multiple detection mechanisms, graduated responses, and coordination protocols that prevent false alarms while ensuring rapid response to real emergencies.</p>\n<h3 id=\"failure-mode-analysis\">Failure Mode Analysis</h3>\n<p>Understanding the types of failures that can occur in our distributed cache system is the foundation for building effective detection and recovery mechanisms. Each failure mode has distinct characteristics, impact patterns, and appropriate response strategies.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Fnode-lifecycle.svg\" alt=\"Node Lifecycle State Machine\"></p>\n<p>The failure modes in our distributed cache fall into several categories based on their scope, duration, and impact on system operations:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Scope</th>\n<th>Duration</th>\n<th>Impact on Operations</th>\n<th>Impact on Data</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node Process Crash</td>\n<td>Single Node</td>\n<td>Until Restart</td>\n<td>Node unavailable for all operations</td>\n<td>Local cache lost, replicas intact</td>\n</tr>\n<tr>\n<td>Node Hardware Failure</td>\n<td>Single Node</td>\n<td>Hours to Days</td>\n<td>Node unavailable until replacement</td>\n<td>Local cache lost, replicas intact</td>\n</tr>\n<tr>\n<td>Network Partition</td>\n<td>Multiple Nodes</td>\n<td>Minutes to Hours</td>\n<td>Cluster splits, reduced availability</td>\n<td>Potential inconsistency between partitions</td>\n</tr>\n<tr>\n<td>Slow/Overloaded Node</td>\n<td>Single Node</td>\n<td>Variable</td>\n<td>High latency, timeouts</td>\n<td>No data loss but stale reads possible</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Single Node</td>\n<td>Variable</td>\n<td>Excessive eviction, reduced hit ratio</td>\n<td>Cache effectiveness degraded</td>\n</tr>\n<tr>\n<td>Cascading Failure</td>\n<td>Multiple Nodes</td>\n<td>Minutes</td>\n<td>Progressive node failures</td>\n<td>Risk of total unavailability</td>\n</tr>\n<tr>\n<td>Split Brain Scenario</td>\n<td>Cluster-wide</td>\n<td>Until Resolution</td>\n<td>Multiple active clusters</td>\n<td>High risk of data divergence</td>\n</tr>\n<tr>\n<td>Clock Skew</td>\n<td>Multiple Nodes</td>\n<td>Persistent</td>\n<td>TTL issues, ordering problems</td>\n<td>Premature expiration, conflict resolution errors</td>\n</tr>\n<tr>\n<td>Byzantine Behavior</td>\n<td>Single Node</td>\n<td>Variable</td>\n<td>Incorrect responses, data corruption</td>\n<td>Potential data corruption propagation</td>\n</tr>\n</tbody></table>\n<h4 id=\"node-level-failures\">Node-Level Failures</h4>\n<p>Node-level failures represent the most common failure mode in distributed systems. Our cache node can fail in several distinct ways, each requiring different detection and recovery strategies.</p>\n<p><strong>Process crash failures</strong> occur when the cache node process terminates unexpectedly due to bugs, resource exhaustion, or external signals. The node hardware remains functional, but the cache process is no longer running. This failure mode is characterized by immediate unavailability with no warning or graceful shutdown procedures.</p>\n<p><strong>Hardware failures</strong> affect the entire physical or virtual machine hosting the cache node. These failures may be complete (server won&#39;t boot) or partial (disk failure, memory corruption, network interface failure). Hardware failures typically have longer recovery times since they require physical intervention or virtual machine recreation.</p>\n<p><strong>Performance degradation failures</strong> occur when a node becomes extremely slow but doesn&#39;t completely fail. This might result from resource contention, garbage collection pauses, or external system dependencies becoming slow. These failures are particularly challenging because the node appears healthy but operates too slowly to meet system requirements.</p>\n<blockquote>\n<p><strong>Critical Insight:</strong> The distinction between fail-fast and fail-slow scenarios requires different detection mechanisms. Fail-fast scenarios (process crash) are easier to detect but create immediate unavailability. Fail-slow scenarios (performance degradation) are harder to detect but can cause subtle system-wide performance issues.</p>\n</blockquote>\n<h4 id=\"network-level-failures\">Network-Level Failures</h4>\n<p>Network failures in distributed systems create some of the most complex scenarios because they can cause partial connectivity where some nodes can communicate while others cannot.</p>\n<p><strong>Complete network isolation</strong> occurs when a node loses all network connectivity. From the node&#39;s perspective, all other nodes appear failed. From the cluster&#39;s perspective, the isolated node appears failed. This symmetric failure is relatively straightforward to handle.</p>\n<p><strong>Asymmetric network partitions</strong> create scenarios where node A can send messages to node B, but node B&#39;s responses don&#39;t reach node A. These failures can cause nodes to have inconsistent views of cluster membership and create complex debugging scenarios.</p>\n<p><strong>Network congestion and packet loss</strong> can cause intermittent communication failures where some messages succeed while others fail or experience high latency. This creates jitter in failure detection mechanisms and can cause nodes to oscillate between healthy and suspected states.</p>\n<h4 id=\"cluster-level-failures\">Cluster-Level Failures</h4>\n<p>Cluster-level failures affect the overall system&#39;s ability to maintain consistency and availability guarantees.</p>\n<p><strong>Split-brain scenarios</strong> occur when network partitions divide the cluster into multiple sub-clusters, each believing it represents the authoritative cluster state. Without careful design, both partitions might continue accepting writes, leading to data divergence that&#39;s difficult to reconcile.</p>\n<p><strong>Cascading failures</strong> happen when the failure of one or more nodes causes increased load on remaining nodes, potentially causing them to fail as well. This can lead to complete system unavailability even when the initial failure only affected a small portion of the cluster.</p>\n<p><strong>Quorum loss scenarios</strong> occur when so many nodes fail that the remaining nodes cannot achieve the required quorum for read or write operations. The system becomes unavailable for consistency-critical operations even though some nodes remain functional.</p>\n<h3 id=\"failure-detection-strategies\">Failure Detection Strategies</h3>\n<p>Effective failure detection in distributed systems requires multiple complementary mechanisms because no single detection method can reliably identify all failure modes while avoiding false positives.</p>\n<p><img src=\"/api/project/distributed-cache/architecture-doc/asset?path=diagrams%2Ffailure-detection-flow.svg\" alt=\"Failure Detection and Recovery Process\"></p>\n<p>Our distributed cache employs a layered approach to failure detection, combining direct probing, gossip-based information sharing, and operation-based detection to create a comprehensive failure detection system.</p>\n<h4 id=\"direct-health-probing\">Direct Health Probing</h4>\n<p>Direct health probing involves nodes actively sending health check requests to other nodes and interpreting the responses (or lack thereof) to assess node health.</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Probe Method</th>\n<th>Success Criteria</th>\n<th>Failure Threshold</th>\n<th>Detection Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Connectivity</td>\n<td>TCP Connect</td>\n<td>Connection established</td>\n<td>3 consecutive failures</td>\n<td>3 × HealthCheckInterval</td>\n</tr>\n<tr>\n<td>Application Health</td>\n<td>HTTP /health endpoint</td>\n<td>200 status + valid JSON</td>\n<td>3 consecutive failures</td>\n<td>3 × HealthCheckInterval</td>\n</tr>\n<tr>\n<td>Cache Functionality</td>\n<td>GET known test key</td>\n<td>Expected value returned</td>\n<td>3 consecutive failures</td>\n<td>3 × HealthCheckInterval</td>\n</tr>\n<tr>\n<td>Performance Health</td>\n<td>Operation latency</td>\n<td>&lt; 95th percentile + 2σ</td>\n<td>5 consecutive slow responses</td>\n<td>5 × HealthCheckInterval</td>\n</tr>\n</tbody></table>\n<p>The <code>ProbeNode</code> method implements our direct health checking mechanism with graduated levels of verification:</p>\n<ol>\n<li><strong>TCP Connectivity Check</strong>: Establishes a raw TCP connection to verify basic network reachability</li>\n<li><strong>HTTP Health Endpoint</strong>: Sends an HTTP GET request to the <code>/health</code> endpoint to verify the application is responding</li>\n<li><strong>Cache Operation Test</strong>: Performs a lightweight cache operation to verify the node can process cache requests</li>\n<li><strong>Performance Validation</strong>: Measures response time and compares against established baselines</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Multiple Health Check Levels</strong></p>\n<ul>\n<li><strong>Context</strong>: Single-level health checks can miss scenarios where connectivity exists but application functionality is impaired</li>\n<li><strong>Options Considered</strong>: TCP-only checks, HTTP-only checks, multi-level verification</li>\n<li><strong>Decision</strong>: Implement graduated health checks with escalating verification depth</li>\n<li><strong>Rationale</strong>: Provides early detection of network issues while ensuring application-level functionality</li>\n<li><strong>Consequences</strong>: More complex implementation but significantly more accurate failure detection</li>\n</ul>\n</blockquote>\n<p>Each health check level has different timeout values and failure thresholds to balance detection speed with false positive avoidance:</p>\n<table>\n<thead>\n<tr>\n<th>Check Level</th>\n<th>Timeout</th>\n<th>Failure Threshold</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TCP Connect</td>\n<td>1 second</td>\n<td>3 consecutive</td>\n<td>Network issues are typically persistent</td>\n</tr>\n<tr>\n<td>HTTP Health</td>\n<td>5 seconds</td>\n<td>3 consecutive</td>\n<td>Application startup or GC pauses may cause temporary delays</td>\n</tr>\n<tr>\n<td>Cache Operation</td>\n<td>10 seconds</td>\n<td>3 consecutive</td>\n<td>Cache operations may be slower during heavy load</td>\n</tr>\n<tr>\n<td>Performance Check</td>\n<td>Variable</td>\n<td>5 consecutive</td>\n<td>Performance degradation may be gradual</td>\n</tr>\n</tbody></table>\n<h4 id=\"gossip-based-failure-detection\">Gossip-Based Failure Detection</h4>\n<p>While direct probing provides immediate failure detection between specific node pairs, gossip-based detection leverages the collective knowledge of the entire cluster to improve detection accuracy and reduce false positives.</p>\n<p>The gossip protocol propagates failure suspicions throughout the cluster, allowing nodes to correlate their individual observations and build consensus about node health status. This distributed approach helps distinguish between actual node failures and local network issues affecting individual node pairs.</p>\n<table>\n<thead>\n<tr>\n<th>Gossip Message Field</th>\n<th>Purpose</th>\n<th>Update Trigger</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NodeStates</td>\n<td>Current health status of all known nodes</td>\n<td>Health check results, timeout detection</td>\n</tr>\n<tr>\n<td>LastSeen</td>\n<td>Timestamp of last successful communication</td>\n<td>Successful operations, health probes</td>\n</tr>\n<tr>\n<td>Version</td>\n<td>Logical timestamp for conflict resolution</td>\n<td>Any status change</td>\n</tr>\n<tr>\n<td>SuspicionLevel</td>\n<td>Confidence level in failure detection</td>\n<td>Accumulated evidence from multiple sources</td>\n</tr>\n</tbody></table>\n<p>The <code>HandleGossipMessage</code> method processes incoming cluster state information and updates local node status based on collective cluster knowledge:</p>\n<ol>\n<li><strong>Merge Remote State</strong>: Compare received node states with local knowledge</li>\n<li><strong>Resolve Conflicts</strong>: Use version numbers and timestamps to determine authoritative information</li>\n<li><strong>Update Suspicion Levels</strong>: Increase confidence in failure detection when multiple nodes report issues</li>\n<li><strong>Trigger Local Actions</strong>: Initiate additional health checks or status changes based on new information</li>\n</ol>\n<blockquote>\n<p><strong>Critical Principle</strong>: Gossip-based detection uses the &quot;phi accrual failure detector&quot; concept where suspicion levels gradually increase based on accumulated evidence rather than binary healthy/failed states.</p>\n</blockquote>\n<h4 id=\"operation-based-detection\">Operation-Based Detection</h4>\n<p>Operation-based detection monitors the success and failure patterns of actual cache operations to identify node health issues that might not be caught by explicit health checks.</p>\n<p>This approach is particularly valuable for detecting performance degradation, resource exhaustion, and subtle application-level issues that don&#39;t manifest as complete unavailability.</p>\n<table>\n<thead>\n<tr>\n<th>Detection Metric</th>\n<th>Normal Range</th>\n<th>Warning Threshold</th>\n<th>Failure Threshold</th>\n<th>Action Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Operation Success Rate</td>\n<td>&gt; 99%</td>\n<td>&lt; 95%</td>\n<td>&lt; 90%</td>\n<td>Increase health check frequency</td>\n</tr>\n<tr>\n<td>Average Latency</td>\n<td>Baseline ± 2σ</td>\n<td>Baseline + 3σ</td>\n<td>Baseline + 5σ</td>\n<td>Mark as slow, reduce traffic</td>\n</tr>\n<tr>\n<td>Timeout Rate</td>\n<td>&lt; 0.1%</td>\n<td>&gt; 1%</td>\n<td>&gt; 5%</td>\n<td>Begin failure suspicion process</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>&lt; 80% capacity</td>\n<td>&gt; 90% capacity</td>\n<td>&gt; 95% capacity</td>\n<td>Trigger aggressive eviction</td>\n</tr>\n</tbody></table>\n<p>The operation-based detection system maintains sliding windows of recent operation statistics and compares current performance against established baselines and peer node performance.</p>\n<h3 id=\"recovery-and-mitigation\">Recovery and Mitigation</h3>\n<p>When failures are detected, the distributed cache system must execute coordinated recovery procedures to maintain availability and data consistency while minimizing the impact on ongoing operations.</p>\n<p>Recovery procedures vary significantly based on the type and scope of the detected failure, requiring different strategies for single-node failures versus cluster-wide issues.</p>\n<h4 id=\"automated-recovery-procedures\">Automated Recovery Procedures</h4>\n<p>Automated recovery handles the most common failure scenarios without human intervention, focusing on rapid restoration of service availability and data accessibility.</p>\n<p><strong>Node Failure Recovery Process</strong>:</p>\n<ol>\n<li><strong>Failure Confirmation</strong>: Verify failure through multiple detection mechanisms to avoid false positives</li>\n<li><strong>Cluster State Update</strong>: Propagate failure information through gossip protocol to ensure cluster-wide awareness</li>\n<li><strong>Hash Ring Update</strong>: Remove failed node from consistent hash ring and recalculate key assignments</li>\n<li><strong>Replica Promotion</strong>: Promote replica nodes to primary status for affected key ranges</li>\n<li><strong>Load Redistribution</strong>: Rebalance traffic to remaining healthy nodes</li>\n<li><strong>Data Recovery</strong>: Restore replication factor by creating new replicas on available nodes</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Recovery Step</th>\n<th>Trigger Condition</th>\n<th>Timeout</th>\n<th>Rollback Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Failure Confirmation</td>\n<td>3 consecutive health check failures</td>\n<td>30 seconds</td>\n<td>Node responds during confirmation</td>\n</tr>\n<tr>\n<td>Ring Update</td>\n<td>Confirmed node failure</td>\n<td>5 seconds</td>\n<td>N/A (atomic operation)</td>\n</tr>\n<tr>\n<td>Replica Promotion</td>\n<td>Ring update complete</td>\n<td>10 seconds</td>\n<td>Insufficient healthy replicas</td>\n</tr>\n<tr>\n<td>Load Redistribution</td>\n<td>Replica promotion complete</td>\n<td>Variable</td>\n<td>Performance degradation detected</td>\n</tr>\n<tr>\n<td>Data Recovery</td>\n<td>Load redistribution stable</td>\n<td>Background</td>\n<td>Resource exhaustion</td>\n</tr>\n</tbody></table>\n<p><strong>Network Partition Recovery</strong>:</p>\n<p>Network partition recovery requires more sophisticated coordination because multiple sub-clusters may have continued operating independently during the partition.</p>\n<ol>\n<li><strong>Partition Detection</strong>: Identify when network connectivity is restored between previously partitioned sub-clusters</li>\n<li><strong>State Reconciliation</strong>: Compare cluster membership and data states between formerly isolated groups</li>\n<li><strong>Conflict Resolution</strong>: Resolve any data conflicts that occurred during partition using vector clocks</li>\n<li><strong>Ring Merge</strong>: Merge hash ring states and resolve any inconsistencies in key assignments</li>\n<li><strong>Anti-Entropy</strong>: Background process to identify and repair any data inconsistencies</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Majority Partition Continues Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: During network partitions, we must decide which partition(s) can continue accepting writes</li>\n<li><strong>Options Considered</strong>: All partitions continue, largest partition only, no writes during partition</li>\n<li><strong>Decision</strong>: Only partitions containing a majority of nodes can accept writes</li>\n<li><strong>Rationale</strong>: Prevents split-brain scenarios while maintaining availability when possible</li>\n<li><strong>Consequences</strong>: Reduces availability during partitions but ensures consistency when partitions merge</li>\n</ul>\n</blockquote>\n<h4 id=\"manual-intervention-procedures\">Manual Intervention Procedures</h4>\n<p>Some failure scenarios require human intervention, either because automated recovery is insufficient or because the risks of automated actions are too high.</p>\n<p><strong>Data Corruption Recovery</strong>:</p>\n<p>When data corruption is detected (through checksum verification or logical inconsistencies), manual intervention ensures that recovery actions don&#39;t propagate corruption to healthy replicas.</p>\n<ol>\n<li><strong>Isolation</strong>: Immediately isolate suspected corrupted nodes to prevent further corruption spread</li>\n<li><strong>Assessment</strong>: Manually verify the extent and nature of corruption</li>\n<li><strong>Recovery Source Selection</strong>: Identify clean replica sources for data restoration</li>\n<li><strong>Controlled Restoration</strong>: Manually trigger restoration from verified clean sources</li>\n<li><strong>Validation</strong>: Verify restored data integrity before returning node to service</li>\n</ol>\n<p><strong>Cluster Bootstrap Recovery</strong>:</p>\n<p>When all nodes in a cluster fail simultaneously (such as during a data center outage), manual intervention is required to safely restart the cluster and restore data consistency.</p>\n<table>\n<thead>\n<tr>\n<th>Bootstrap Step</th>\n<th>Verification Required</th>\n<th>Risk Level</th>\n<th>Rollback Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Select Seed Nodes</td>\n<td>Data integrity verification</td>\n<td>Low</td>\n<td>Shutdown and restart with different nodes</td>\n</tr>\n<tr>\n<td>Form Initial Ring</td>\n<td>Ring consistency check</td>\n<td>Medium</td>\n<td>Recalculate ring with verified nodes only</td>\n</tr>\n<tr>\n<td>Start Gossip</td>\n<td>Membership convergence</td>\n<td>Low</td>\n<td>Restart gossip with different intervals</td>\n</tr>\n<tr>\n<td>Enable Client Traffic</td>\n<td>End-to-end operation test</td>\n<td>High</td>\n<td>Disable traffic and investigate</td>\n</tr>\n<tr>\n<td>Full Cluster Recovery</td>\n<td>Complete data consistency check</td>\n<td>High</td>\n<td>Partial rollback to last known good state</td>\n</tr>\n</tbody></table>\n<h4 id=\"circuit-breaker-patterns\">Circuit Breaker Patterns</h4>\n<p>Circuit breakers protect the system from cascading failures by temporarily stopping operations that are likely to fail, allowing failed components time to recover.</p>\n<p>Our distributed cache implements circuit breakers at multiple levels to prevent localized failures from affecting system-wide availability.</p>\n<table>\n<thead>\n<tr>\n<th>Circuit Breaker Type</th>\n<th>Failure Threshold</th>\n<th>Reset Timeout</th>\n<th>Half-Open Test</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node Communication</td>\n<td>5 failures in 30 seconds</td>\n<td>60 seconds</td>\n<td>Single health check</td>\n</tr>\n<tr>\n<td>Cache Operations</td>\n<td>10 failures in 10 seconds</td>\n<td>30 seconds</td>\n<td>Single GET operation</td>\n</tr>\n<tr>\n<td>Replication</td>\n<td>3 failures in 5 seconds</td>\n<td>120 seconds</td>\n<td>Single replication write</td>\n</tr>\n<tr>\n<td>Gossip Protocol</td>\n<td>5 failures in 60 seconds</td>\n<td>300 seconds</td>\n<td>Single gossip exchange</td>\n</tr>\n</tbody></table>\n<p>Circuit breaker state transitions follow a three-state model:</p>\n<ol>\n<li><strong>Closed</strong>: Normal operation, requests pass through</li>\n<li><strong>Open</strong>: Failure threshold exceeded, requests fail fast without attempting operation</li>\n<li><strong>Half-Open</strong>: Timeout expired, limited requests allowed to test recovery</li>\n</ol>\n<h3 id=\"edge-cases-and-corner-scenarios\">Edge Cases and Corner Scenarios</h3>\n<p>Distributed systems create numerous edge cases that arise from the interaction of timing, partial failures, and concurrent operations. These scenarios often occur rarely in practice but can cause significant issues when they do occur.</p>\n<h4 id=\"timing-related-edge-cases\">Timing-Related Edge Cases</h4>\n<p><strong>Clock Skew Impact on TTL</strong>:</p>\n<p>When nodes have significantly different system clocks, TTL-based expiration can behave inconsistently across replicas. A key might expire on one replica but remain valid on another, leading to inconsistent read results.</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Clock Difference</th>\n<th>Impact</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Premature Expiration</td>\n<td>Replica clock fast by 5 minutes</td>\n<td>Key expires early on some replicas</td>\n<td>Use logical timestamps for TTL</td>\n</tr>\n<tr>\n<td>Delayed Expiration</td>\n<td>Replica clock slow by 5 minutes</td>\n<td>Key remains valid past intended TTL</td>\n<td>Regular clock synchronization monitoring</td>\n</tr>\n<tr>\n<td>Inconsistent Reads</td>\n<td>Mixed clock skew</td>\n<td>Different replicas return different results</td>\n<td>Vector clock-based consistency</td>\n</tr>\n</tbody></table>\n<p><strong>Race Conditions in Membership Changes</strong>:</p>\n<p>When nodes join or leave the cluster simultaneously, race conditions can occur in hash ring updates and key redistribution.</p>\n<ol>\n<li><strong>Concurrent Join Scenario</strong>: Two nodes attempt to join simultaneously, potentially creating inconsistent ring states</li>\n<li><strong>Join-Leave Race</strong>: A node joins while another leaves, creating complex key migration requirements</li>\n<li><strong>Rapid Membership Changes</strong>: Multiple membership changes occur faster than gossip can propagate</li>\n</ol>\n<h4 id=\"consistency-edge-cases\">Consistency Edge Cases</h4>\n<p><strong>Read-Your-Writes Violations</strong>:</p>\n<p>In distributed systems with eventual consistency, a client might not be able to read data it just wrote if the read request routes to a replica that hasn&#39;t yet received the update.</p>\n<table>\n<thead>\n<tr>\n<th>Read Type</th>\n<th>Consistency Guarantee</th>\n<th>Edge Case Risk</th>\n<th>Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Read from Any Replica</td>\n<td>Eventual</td>\n<td>High risk of stale reads</td>\n<td>Session affinity</td>\n</tr>\n<tr>\n<td>Read from Write Replica</td>\n<td>Read-your-writes</td>\n<td>Network routing changes</td>\n<td>Consistent hashing with preference list</td>\n</tr>\n<tr>\n<td>Quorum Reads</td>\n<td>Strong</td>\n<td>Split-brain scenarios</td>\n<td>Majority quorum requirement</td>\n</tr>\n</tbody></table>\n<p><strong>Vector Clock Conflicts</strong>:</p>\n<p>When using vector clocks for conflict resolution, certain scenarios can create ambiguous conflict resolution situations.</p>\n<ol>\n<li><strong>Concurrent Updates</strong>: Multiple clients update the same key simultaneously on different replicas</li>\n<li><strong>Clock Reset</strong>: Node restart causes vector clock reset, creating false conflicts</li>\n<li><strong>Deep Conflicts</strong>: Long-running partitions create complex conflict trees</li>\n</ol>\n<h4 id=\"resource-exhaustion-edge-cases\">Resource Exhaustion Edge Cases</h4>\n<p><strong>Memory Pressure During Recovery</strong>:</p>\n<p>Node failures can cause remaining nodes to experience sudden memory pressure as they take over responsibility for additional key ranges.</p>\n<p><strong>Network Bandwidth Saturation</strong>:</p>\n<p>During large-scale recovery operations, network bandwidth can become saturated, causing additional nodes to appear failed due to communication timeouts.</p>\n<p><strong>Disk Space Exhaustion</strong>:</p>\n<p>Logging and persistence mechanisms can consume excessive disk space during failure scenarios, potentially causing additional failures.</p>\n<h4 id=\"byzantine-failure-scenarios\">Byzantine Failure Scenarios</h4>\n<p>While our cache system doesn&#39;t implement full Byzantine fault tolerance, we must handle scenarios where nodes behave incorrectly due to bugs, corruption, or misconfiguration.</p>\n<table>\n<thead>\n<tr>\n<th>Byzantine Behavior</th>\n<th>Detection Method</th>\n<th>Response Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Incorrect Hash Ring</td>\n<td>Ring consistency verification</td>\n<td>Exclude from membership decisions</td>\n</tr>\n<tr>\n<td>Data Corruption</td>\n<td>Checksum verification</td>\n<td>Quarantine and rebuild from replicas</td>\n</tr>\n<tr>\n<td>Protocol Violations</td>\n<td>Message validation</td>\n<td>Temporary communication suspension</td>\n</tr>\n<tr>\n<td>Performance Attacks</td>\n<td>Rate limiting and monitoring</td>\n<td>Traffic throttling and investigation</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: False Positive Failure Detection</strong></p>\n<p>Many implementations use overly aggressive failure detection timeouts that cause healthy but temporarily slow nodes to be marked as failed. This creates unnecessary churn in cluster membership and can trigger cascading failures.</p>\n<p><strong>Why it&#39;s wrong</strong>: Temporary network congestion or garbage collection pauses are normal in distributed systems. Marking nodes as failed too quickly causes more harm than the original performance issue.</p>\n<p><strong>How to fix</strong>: Implement graduated suspicion levels instead of binary healthy/failed states. Use multiple detection mechanisms and require consensus before marking nodes as failed.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Split-Brain Scenarios</strong></p>\n<p>Some implementations assume network partitions won&#39;t occur or will be quickly resolved, failing to implement proper split-brain prevention mechanisms.</p>\n<p><strong>Why it&#39;s wrong</strong>: Network partitions are common in distributed systems, especially in cloud environments. Without split-brain prevention, data divergence can occur that&#39;s difficult or impossible to reconcile.</p>\n<p><strong>How to fix</strong>: Implement majority quorum requirements for membership changes and write operations. Ensure that only one partition can remain active during network splits.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Circuit Breaker Implementation</strong></p>\n<p>Circuit breakers that don&#39;t properly isolate failures can allow cascading failures to propagate throughout the system.</p>\n<p><strong>Why it&#39;s wrong</strong>: Without proper isolation, a single slow or failed node can cause the entire cluster to become unresponsive as healthy nodes wait for timeouts.</p>\n<p><strong>How to fix</strong>: Implement circuit breakers at multiple levels with appropriate timeout values. Ensure that circuit breakers fail fast and provide meaningful feedback about failure reasons.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Error Propagation</strong></p>\n<p>Different types of errors (network timeouts, node failures, data corruption) are often handled inconsistently, making debugging difficult and potentially masking serious issues.</p>\n<p><strong>Why it&#39;s wrong</strong>: Inconsistent error handling makes it difficult to understand system behavior and can hide critical issues that need immediate attention.</p>\n<p><strong>How to fix</strong>: Define clear error categories and handling strategies. Implement structured logging that provides sufficient context for debugging while maintaining performance.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building robust failure handling requires careful attention to both the detection mechanisms and the recovery procedures. The following implementation guidance provides concrete approaches for each component.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Health Checking</td>\n<td>HTTP endpoints with JSON status</td>\n<td>gRPC health checking protocol</td>\n</tr>\n<tr>\n<td>Failure Detection</td>\n<td>Timer-based probing</td>\n<td>Phi accrual failure detector</td>\n</tr>\n<tr>\n<td>Circuit Breakers</td>\n<td>Simple counter-based</td>\n<td>Hystrix-style with metrics</td>\n</tr>\n<tr>\n<td>Error Logging</td>\n<td>Standard library logging</td>\n<td>Structured logging with correlation IDs</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Basic counters</td>\n<td>Prometheus metrics with alerting</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Request ID propagation</td>\n<td>OpenTelemetry distributed tracing</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure\">File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/failuredetector/\n  detector.go              ← main failure detection logic\n  probe.go                 ← health checking implementation\n  circuit_breaker.go       ← circuit breaker patterns\n  phi_detector.go          ← phi accrual failure detector\n  detector_test.go         ← comprehensive failure scenario tests\n\ninternal/recovery/\n  coordinator.go           ← recovery coordination logic\n  ring_recovery.go         ← hash ring recovery procedures\n  data_recovery.go         ← data consistency recovery\n  recovery_test.go         ← recovery scenario tests\n\ninternal/gossip/\n  membership.go            ← cluster membership management\n  failure_propagation.go   ← failure information spreading\n  state_merge.go           ← cluster state reconciliation\n  gossip_test.go           ← membership change tests</code></pre></div>\n\n<h4 id=\"health-checking-infrastructure\">Health Checking Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// HealthChecker provides multi-level health verification for cluster nodes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implements graduated health checking with escalating verification depth.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthChecker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transport    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthMetrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    circuitBreakers </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthConfig defines timeouts and thresholds for health checking.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TCPTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"tcp_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HTTPTimeout     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"http_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CacheTimeout    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"cache_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailureThreshold </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">          `json:\"failure_threshold\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CheckInterval   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"check_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SlowThreshold   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"slow_threshold\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHealthChecker creates a health checker with configured timeouts and transport.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHealthChecker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">transport</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HealthConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        transport:       transport,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:         config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics:        </span><span style=\"color:#B392F0\">NewHealthMetrics</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        circuitBreakers: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:         log.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(os.Stderr, </span><span style=\"color:#9ECBFF\">\"[HEALTH] \"</span><span style=\"color:#E1E4E8\">, log.LstdFlags),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProbeNode performs graduated health checks on the specified node.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns detailed health status including performance metrics.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProbeNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check circuit breaker state - if open, return cached failure result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Perform TCP connectivity test with h.config.TCPTimeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If TCP succeeds, perform HTTP health check with h.config.HTTPTimeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If HTTP succeeds, perform cache operation test with h.config.CacheTimeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Measure response times and compare against h.config.SlowThreshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update circuit breaker state based on results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Record metrics and log results for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return comprehensive health result with all test outcomes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.WithTimeout for each test level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Record both success/failure and performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthResult contains detailed results from multi-level health checking.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TCPHealthy    </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">         `json:\"tcp_healthy\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HTTPHealthy   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">         `json:\"http_healthy\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CacheHealthy  </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">         `json:\"cache_healthy\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResponseTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"response_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorDetails  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"error_details,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    OverallHealth </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\"> `json:\"overall_health\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusHealthy</span><span style=\"color:#B392F0\">    HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusSlow</span><span style=\"color:#B392F0\">      HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"slow\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusSuspected</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"suspected\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusFailed</span><span style=\"color:#B392F0\">    HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"failure-detection-coordinator\">Failure Detection Coordinator</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// FailureDetector coordinates multiple detection mechanisms for comprehensive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// failure detection with minimal false positives.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FailureDetector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    healthChecker   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    phiDetector     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PhiAccrualDetector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gossipManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeStates      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeHealthState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    suspicionLevels </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    callbacks       []</span><span style=\"color:#B392F0\">FailureCallback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh          </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NodeHealthState tracks comprehensive health information for a cluster node.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NodeHealthState</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"node_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status           </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\">      `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSeen         </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"last_seen\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastHealthCheck  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"last_health_check\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConsecutiveFailures </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">            `json:\"consecutive_failures\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SuspicionLevel   </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">           `json:\"suspicion_level\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PerformanceMetrics </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#9ECBFF\"> `json:\"performance_metrics\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Version          </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">            `json:\"version\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartMonitoring begins continuous failure detection for all known nodes.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Coordinates health checking, gossip information, and suspicion level updates.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartMonitoring</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">knownNodes</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize node states for all known nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start health checking goroutine that probes nodes periodically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start gossip processing goroutine that handles membership updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start phi detector updates that calculate suspicion levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Start state reconciliation that merges local and remote information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Register cleanup handler for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use select with ctx.Done() for cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Space out health checks to avoid thundering herd</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ProcessGossipUpdate incorporates remote failure detection information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// into local node health state tracking.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FailureDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ProcessGossipUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">gossipMsg</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">GossipMessage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract node state information from gossip message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compare remote timestamps with local knowledge</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Merge state information using version numbers for conflict resolution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update local suspicion levels based on remote observations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Trigger additional health checks if suspicion levels increase</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Notify failure callbacks if node status changes significantly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use version numbers to determine which information is newer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Increase suspicion when multiple nodes report issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"recovery-coordinator\">Recovery Coordinator</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RecoveryCoordinator manages automated recovery procedures for various</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// failure scenarios while maintaining data consistency.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecoveryCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hashRing        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    replicationMgr  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gossipManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localStorage    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recoveryQueue   </span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> RecoveryTask</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    activeRecovery  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecoveryConfig defines parameters for automated recovery procedures.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RecoveryConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConfirmationTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"confirmation_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RingUpdateTimeout     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"ring_update_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicationTimeout    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"replication_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxConcurrentRecovery </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">          `json:\"max_concurrent_recovery\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataRepairEnabled     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">         `json:\"data_repair_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AutoPromoteReplicas   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">         `json:\"auto_promote_replicas\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleNodeFailure initiates comprehensive recovery procedures for failed node.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Coordinates ring updates, replica promotion, and data redistribution.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleNodeFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">failedNodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">confirmedBy</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify failure confirmation from multiple sources in confirmedBy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create recovery task with appropriate priority and dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove failed node from hash ring and recalculate key assignments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Identify all key ranges that need new primary nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Promote replica nodes to primary status for affected ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Initiate data replication to restore replication factor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update cluster membership and propagate through gossip</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Monitor recovery progress and handle any secondary failures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context with timeout for each recovery step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Rollback changes if critical steps fail</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleNetworkPartitionRecovery reconciles cluster state after partition healing.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Resolves conflicts between formerly isolated sub-clusters.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RecoveryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleNetworkPartitionRecovery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">mergeNodes</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Collect cluster state from all nodes in mergeNodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Identify conflicts in membership, ring topology, and data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Resolve membership conflicts using node priorities or timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Merge hash ring states and resolve key assignment conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Trigger anti-entropy process to repair data inconsistencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Propagate merged state through gossip to all nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use vector clocks to determine causality in state conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Prioritize larger partition's state for conflict resolution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Health Checking Verification</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test basic health checking functionality</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/failuredetector</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestHealthChecker</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify graduated health check levels</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/debug/health/nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: JSON showing health status for all cluster nodes</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test failure detection timing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Kill one node, verify detection within 3 * HealthCheckInterval</span></span></code></pre></div>\n\n<p><strong>Recovery Procedure Validation</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test automated node failure recovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/recovery</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestNodeFailureRecovery</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify ring rebalancing after node failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Start 3-node cluster, kill one node, verify key redistribution</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/debug/ring/topology</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Ring shows 2 nodes with evenly distributed keys</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test partition recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Simulate network partition, verify recovery when partition heals</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>False failure alerts</td>\n<td>Aggressive timeouts</td>\n<td>Check health check intervals and thresholds</td>\n<td>Increase timeouts, implement graduated suspicion</td>\n</tr>\n<tr>\n<td>Slow failure detection</td>\n<td>Conservative timeouts</td>\n<td>Monitor failure detection lag metrics</td>\n<td>Reduce health check intervals, implement phi detector</td>\n</tr>\n<tr>\n<td>Split-brain during partition</td>\n<td>Missing quorum enforcement</td>\n<td>Verify minority partitions stop accepting writes</td>\n<td>Implement majority quorum checks</td>\n</tr>\n<tr>\n<td>Cascading failures</td>\n<td>Missing circuit breakers</td>\n<td>Check error propagation and timeout patterns</td>\n<td>Add circuit breakers, implement graceful degradation</td>\n</tr>\n<tr>\n<td>Memory leaks during recovery</td>\n<td>Resource cleanup issues</td>\n<td>Monitor goroutine and memory usage during recovery</td>\n<td>Add proper resource cleanup and context cancellation</td>\n</tr>\n<tr>\n<td>Inconsistent cluster state</td>\n<td>Race conditions in gossip</td>\n<td>Check gossip convergence and conflict resolution</td>\n<td>Implement proper state merging with version numbers</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides testing methodologies and validation approaches for all milestones, with particular emphasis on verifying the correctness and resilience of distributed cache operations across Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication &amp; Consistency).</p>\n</blockquote>\n<h3 id=\"mental-model-the-quality-control-factory\">Mental Model: The Quality Control Factory</h3>\n<p>Think of testing a distributed cache like operating a quality control factory for a complex manufacturing process. Just as a factory has multiple quality checkpoints - individual part inspection, assembly line testing, final product validation, and stress testing under extreme conditions - our distributed cache requires layered testing approaches.</p>\n<p>At the component level (unit testing), we inspect each &quot;part&quot; in isolation - ensuring the hash ring calculates positions correctly, the LRU cache evicts entries properly, and the gossip protocol formats messages accurately. This is like checking individual screws and bolts before assembly.</p>\n<p>At the integration level, we test how these components work together - verifying that the hash ring correctly routes requests to cache nodes, that gossip messages propagate cluster state changes, and that replication maintains data consistency. This is like testing assembled subsystems before final integration.</p>\n<p>At the system level (milestone checkpoints), we validate complete end-to-end operations - ensuring clients can store and retrieve data reliably, that the cluster handles node failures gracefully, and that consistency guarantees hold under various scenarios. This is like testing the complete manufactured product.</p>\n<p>Finally, chaos testing is like subjecting our product to extreme environmental conditions - network partitions, cascading failures, and byzantine behavior - to ensure it remains functional when the real world throws unexpected challenges at it.</p>\n<h3 id=\"unit-testing-approach\">Unit Testing Approach</h3>\n<p>Unit testing forms the foundation of our testing strategy, focusing on individual components in complete isolation. Each component must demonstrate correct behavior independently before we can trust it in the larger distributed system.</p>\n<blockquote>\n<p><strong>Decision: Isolation-First Testing Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Distributed systems have complex interdependencies that can mask individual component bugs, making root cause analysis extremely difficult during integration testing.</li>\n<li><strong>Options Considered</strong>: Mock-heavy isolation, dependency injection with real components, hybrid approach with selective mocking</li>\n<li><strong>Decision</strong>: Comprehensive mocking of all external dependencies with dependency injection for testability</li>\n<li><strong>Rationale</strong>: Complete isolation ensures that test failures directly indicate the component under test, not its dependencies. This enables parallel development where teams can test components before their dependencies are complete.</li>\n<li><strong>Consequences</strong>: Requires more sophisticated test setup but provides faster feedback, easier debugging, and higher confidence in component correctness.</li>\n</ul>\n</blockquote>\n<h4 id=\"hash-ring-component-testing\">Hash Ring Component Testing</h4>\n<p>The <code>HashRing</code> component requires extensive testing to ensure consistent behavior across membership changes. Our test strategy validates both the mathematical correctness of consistent hashing and the implementation&#39;s handling of edge cases.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Cases</th>\n<th>Validation Method</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Operations</td>\n<td>AddNode, RemoveNode, GetNode</td>\n<td>Direct method calls with known inputs</td>\n<td>Deterministic node assignment for identical keys</td>\n</tr>\n<tr>\n<td>Virtual Node Distribution</td>\n<td>Node addition with different virtual node counts</td>\n<td>Statistical analysis of key distribution</td>\n<td>Even load distribution within 10% variance</td>\n</tr>\n<tr>\n<td>Key Redistribution</td>\n<td>Node removal and addition cycles</td>\n<td>Track which keys change assignment</td>\n<td>Minimal key movement (O(K/N) keys affected)</td>\n</tr>\n<tr>\n<td>Edge Cases</td>\n<td>Empty ring, single node, duplicate adds/removes</td>\n<td>Boundary condition testing</td>\n<td>Graceful handling without panics or corruption</td>\n</tr>\n<tr>\n<td>Consistency Verification</td>\n<td>Multiple identical rings with same operations</td>\n<td>Cross-ring comparison</td>\n<td>Identical results across independent instances</td>\n</tr>\n</tbody></table>\n<p>The hash ring testing focuses particularly on the mathematical properties of consistent hashing. We generate large sets of random keys and verify that load distribution remains balanced as nodes are added and removed. The critical insight is that a properly implemented consistent hash ring should maintain stable key assignments - when we add or remove a node, only keys in the affected ring segments should move to different nodes.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Hash Ring Test Scenarios:\n1. Create ring with nodes A, B, C and 100 virtual nodes each\n2. Generate 10,000 random keys and record their node assignments  \n3. Add node D and verify only ~25% of keys change assignment\n4. Remove node B and verify only ~33% of remaining keys move\n5. Statistical validation: no node should hold &gt;40% or &lt;10% of keys</code></pre></div>\n\n<p>Our virtual node testing uses statistical analysis to ensure load balancing effectiveness. We measure the coefficient of variation in key distribution - a properly tuned system with sufficient virtual nodes should maintain CV &lt; 0.1 under normal operation.</p>\n<h4 id=\"lru-cache-component-testing\">LRU Cache Component Testing</h4>\n<p>The <code>LRUCache</code> component testing emphasizes correctness under concurrent access and accurate memory accounting. Since cache nodes will face high concurrency, our tests must validate thread safety and performance characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Cases</th>\n<th>Concurrency Level</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Operations</td>\n<td>Get, Set, Delete with various data sizes</td>\n<td>Single-threaded</td>\n<td>Verify correct storage and retrieval</td>\n</tr>\n<tr>\n<td>LRU Eviction</td>\n<td>Fill cache beyond capacity, verify eviction order</td>\n<td>Single-threaded</td>\n<td>Track access patterns and eviction sequence</td>\n</tr>\n<tr>\n<td>TTL Expiration</td>\n<td>Set entries with various TTL values</td>\n<td>Single-threaded</td>\n<td>Verify expiration timing and cleanup</td>\n</tr>\n<tr>\n<td>Memory Accounting</td>\n<td>Operations with different value sizes</td>\n<td>Single-threaded</td>\n<td>Verify accurate memory usage tracking</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Mixed read/write operations</td>\n<td>100 goroutines</td>\n<td>Verify data integrity and no race conditions</td>\n</tr>\n<tr>\n<td>Performance Characteristics</td>\n<td>High-throughput operations</td>\n<td>Variable load</td>\n<td>Measure latency percentiles and throughput</td>\n</tr>\n</tbody></table>\n<p>Memory accounting testing requires particular attention to precision. We validate that the cache accurately tracks memory usage for both keys and values, including metadata overhead. The test creates entries with known sizes and verifies that reported memory usage matches expected calculations within a small tolerance for metadata overhead.</p>\n<p>TTL expiration testing uses controlled time manipulation to verify that entries expire correctly. We set entries with 1-second TTLs, advance time by 1.5 seconds, and verify that expired entries are removed during cleanup operations. The critical aspect is testing both lazy expiration (during access) and active cleanup (background processes).</p>\n<p>Concurrent access testing employs a sophisticated approach using multiple goroutines performing random operations while continuously validating cache invariants. We use race detection tools and verify that the cache maintains consistency under high contention.</p>\n<h4 id=\"gossip-protocol-component-testing\">Gossip Protocol Component Testing</h4>\n<p>Gossip protocol testing validates message propagation, convergence properties, and network partition handling. The challenge is simulating network conditions and timing scenarios that occur in distributed environments.</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Network Conditions</th>\n<th>Expected Outcome</th>\n<th>Verification Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Propagation</td>\n<td>Reliable network, 5 nodes</td>\n<td>Information reaches all nodes within 3 rounds</td>\n<td>Message tracking and convergence measurement</td>\n</tr>\n<tr>\n<td>Network Delays</td>\n<td>Variable latency 10-500ms</td>\n<td>Eventually consistent state across all nodes</td>\n<td>State comparison after stabilization period</td>\n</tr>\n<tr>\n<td>Message Loss</td>\n<td>10% packet loss rate</td>\n<td>Gossip resilience maintains convergence</td>\n<td>Probabilistic delivery verification</td>\n</tr>\n<tr>\n<td>Partition Recovery</td>\n<td>Split network, then heal</td>\n<td>Automatic state reconciliation</td>\n<td>Compare states before/after partition</td>\n</tr>\n<tr>\n<td>Membership Changes</td>\n<td>Nodes join/leave during gossip</td>\n<td>Accurate membership propagation</td>\n<td>Membership consistency validation</td>\n</tr>\n</tbody></table>\n<p>Gossip testing uses a sophisticated network simulator that can introduce controlled delays, packet loss, and partitions. This allows us to validate convergence properties under realistic network conditions without requiring actual network infrastructure.</p>\n<p>The convergence testing measures both speed and accuracy of information propagation. We inject a membership change at one node and measure how many gossip rounds it takes for all nodes to learn about the change. Properly tuned gossip should achieve convergence within O(log N) rounds.</p>\n<h4 id=\"replication-manager-component-testing\">Replication Manager Component Testing</h4>\n<p>Replication manager testing focuses on consistency guarantees, conflict resolution, and quorum behavior. These tests are particularly complex because they must validate distributed protocols while running in a single-process test environment.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Scenario</th>\n<th>Consistency Level</th>\n<th>Expected Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Replication</td>\n<td>Write to primary, read from replica</td>\n<td>Eventual consistency</td>\n<td>Value propagates within propagation delay</td>\n</tr>\n<tr>\n<td>Quorum Operations</td>\n<td>R=2, W=2, N=3 configuration</td>\n<td>Strong consistency</td>\n<td>Reads see all committed writes</td>\n</tr>\n<tr>\n<td>Conflict Resolution</td>\n<td>Concurrent writes to same key</td>\n<td>Vector clock ordering</td>\n<td>Deterministic conflict resolution</td>\n</tr>\n<tr>\n<td>Replica Failure</td>\n<td>One replica unavailable during write</td>\n<td>Sloppy quorum</td>\n<td>Write succeeds with hinted handoff</td>\n</tr>\n<tr>\n<td>Read Repair</td>\n<td>Stale replica detected during read</td>\n<td>Automatic repair</td>\n<td>Stale replica updated asynchronously</td>\n</tr>\n</tbody></table>\n<p>Vector clock testing requires careful validation of causality relationships. We create scenarios with concurrent updates and verify that vector clocks correctly identify concurrent vs. causally ordered events. The test creates two concurrent writes to the same key from different nodes and verifies that the conflict resolution algorithm handles them deterministically.</p>\n<p>Quorum testing simulates various replica availability scenarios and validates that the system maintains consistency guarantees. We use mock transport layers to simulate network partitions and node failures, then verify that quorum requirements are properly enforced.</p>\n<h3 id=\"integration-testing\">Integration Testing</h3>\n<p>Integration testing validates component interactions and emergent system behaviors that cannot be observed at the unit level. These tests use real components working together, with carefully controlled external dependencies.</p>\n<h4 id=\"cross-component-data-flow-testing\">Cross-Component Data Flow Testing</h4>\n<p>The most critical integration tests validate complete data flow paths through multiple components. These tests ensure that data transformations and state changes propagate correctly across component boundaries.</p>\n<table>\n<thead>\n<tr>\n<th>Data Flow Path</th>\n<th>Components Involved</th>\n<th>Test Scenario</th>\n<th>Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client Request Processing</td>\n<td>HTTPTransport, HashRing, LRUCache</td>\n<td>Client GET request for cached key</td>\n<td>Request routing, cache lookup, response formatting</td>\n</tr>\n<tr>\n<td>Ring Rebalancing</td>\n<td>HashRing, ReplicationManager, GossipManager</td>\n<td>Node addition triggers rebalancing</td>\n<td>Key redistribution, replica updates, gossip propagation</td>\n</tr>\n<tr>\n<td>Failure Detection</td>\n<td>HealthChecker, FailureDetector, GossipManager</td>\n<td>Node becomes unresponsive</td>\n<td>Health check failure, suspicion level increase, gossip notification</td>\n</tr>\n<tr>\n<td>Conflict Resolution</td>\n<td>ReplicationManager, VectorClock, LRUCache</td>\n<td>Concurrent writes during partition</td>\n<td>Conflict detection, vector clock comparison, resolution outcome</td>\n</tr>\n</tbody></table>\n<p>Client request processing tests trace complete request lifecycles from HTTP reception through hash ring lookup, cache node routing, local storage access, and response generation. We validate that each component correctly transforms and forwards data to the next stage.</p>\n<p>Ring rebalancing integration tests are particularly complex because they involve coordinated updates across multiple components. When a new node joins, the hash ring must recalculate key assignments, the replication manager must identify keys that need to move, and the gossip manager must propagate the membership change. Our tests validate that these operations complete successfully and leave the system in a consistent state.</p>\n<h4 id=\"cluster-state-consistency-testing\">Cluster State Consistency Testing</h4>\n<p>Cluster state consistency testing validates that all nodes maintain compatible views of cluster membership, ring topology, and data placement. These tests are essential because inconsistent cluster state can lead to data loss or availability problems.</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Scenario</th>\n<th>Initial State</th>\n<th>State Change</th>\n<th>Consistency Check</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Membership Convergence</td>\n<td>3-node cluster</td>\n<td>Add node D</td>\n<td>All nodes agree on 4-node membership within gossip interval</td>\n</tr>\n<tr>\n<td>Ring Position Agreement</td>\n<td>Identical hash rings</td>\n<td>Virtual node changes</td>\n<td>All nodes calculate identical key-to-node mappings</td>\n</tr>\n<tr>\n<td>Failure Detection Consensus</td>\n<td>Healthy cluster</td>\n<td>Node C stops responding</td>\n<td>All nodes mark C as failed within detection timeout</td>\n</tr>\n<tr>\n<td>Partition Recovery</td>\n<td>Split into two groups</td>\n<td>Network heals</td>\n<td>Consistent merged state across all nodes</td>\n</tr>\n</tbody></table>\n<p>Membership convergence testing uses controlled timing to ensure that gossip propagation completes within expected timeframes. We start with a stable cluster, introduce a membership change, and measure how long it takes for all nodes to converge on the new membership. The test fails if convergence takes longer than 3x the gossip interval or if nodes end up with inconsistent views.</p>\n<p>Ring position agreement testing validates that all nodes compute identical hash ring states given the same inputs. We create multiple independent <code>HashRing</code> instances, perform identical operations on each, and verify that they produce identical results for key lookups and node assignments.</p>\n<h4 id=\"end-to-end-operation-validation\">End-to-End Operation Validation</h4>\n<p>End-to-end operation validation tests complete client-visible operations across the entire distributed system. These tests simulate real client usage patterns and validate that the system provides expected semantics.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Test Scenario</th>\n<th>Fault Conditions</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Get/Set</td>\n<td>Store value, retrieve value</td>\n<td>None</td>\n<td>Retrieved value matches stored value</td>\n</tr>\n<tr>\n<td>Replicated Operations</td>\n<td>Write with R=2, W=2</td>\n<td>One replica fails during write</td>\n<td>Write succeeds, subsequent reads return correct value</td>\n</tr>\n<tr>\n<td>Consistent Reads</td>\n<td>Write to key, immediate read</td>\n<td>Write still replicating</td>\n<td>Read returns either old or new value, never partial state</td>\n</tr>\n<tr>\n<td>TTL Expiration</td>\n<td>Set key with 5-second TTL</td>\n<td>None</td>\n<td>Key unavailable after TTL expiration</td>\n</tr>\n<tr>\n<td>Node Failure Recovery</td>\n<td>Ongoing operations</td>\n<td>Primary node for key fails</td>\n<td>Operations continue using replica nodes</td>\n</tr>\n</tbody></table>\n<p>Simple get/set testing validates basic functionality but with realistic data sizes and access patterns. We test with various value sizes from small metadata (100 bytes) to large objects (1MB) to ensure the system handles different workloads correctly.</p>\n<p>Replicated operations testing introduces controlled failures during multi-node operations to validate fault tolerance. We use network simulation to disconnect nodes at specific points during replication and verify that the system handles these scenarios gracefully.</p>\n<p>Consistent reads testing validates that clients never observe partial or corrupted state, even when concurrent operations are modifying the same keys. This requires careful coordination between replication and consistency mechanisms.</p>\n<h3 id=\"milestone-checkpoints\">Milestone Checkpoints</h3>\n<p>Milestone checkpoints provide concrete validation criteria for each development phase. These checkpoints ensure that each milestone delivers working functionality before moving to the next phase.</p>\n<h4 id=\"milestone-1-consistent-hash-ring-validation\">Milestone 1: Consistent Hash Ring Validation</h4>\n<p>The consistent hash ring milestone focuses on correct key distribution and minimal redistribution during membership changes. The validation emphasizes mathematical correctness and performance characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Method</th>\n<th>Success Criteria</th>\n<th>Measurement Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Even Distribution</td>\n<td>Generate 50,000 random keys across 5-node ring</td>\n<td>No node holds &gt;25% of keys</td>\n<td>Statistical analysis of key distribution</td>\n</tr>\n<tr>\n<td>Virtual Node Effectiveness</td>\n<td>Compare distribution with 1, 10, 100, 1000 virtual nodes</td>\n<td>Distribution improves with more virtual nodes</td>\n<td>Coefficient of variation measurement</td>\n</tr>\n<tr>\n<td>Minimal Redistribution</td>\n<td>Add node to 4-node ring with 10,000 keys</td>\n<td>&lt;30% of keys change assignment</td>\n<td>Track key assignment changes</td>\n</tr>\n<tr>\n<td>Lookup Performance</td>\n<td>Time 100,000 key lookups</td>\n<td>&lt;1μs average lookup time</td>\n<td>Benchmark measurement</td>\n</tr>\n<tr>\n<td>Memory Efficiency</td>\n<td>Create ring with 100 nodes</td>\n<td>&lt;1MB memory usage for ring metadata</td>\n<td>Memory profiling</td>\n</tr>\n</tbody></table>\n<p>Even distribution validation uses statistical analysis to ensure load balancing effectiveness. We generate large numbers of random keys and verify that they distribute evenly across available nodes. The test calculates the coefficient of variation (CV) - the ratio of standard deviation to mean - for key distribution across nodes. A well-balanced system should achieve CV &lt; 0.15 with sufficient virtual nodes.</p>\n<p>Virtual node effectiveness testing demonstrates the improvement in load balancing as virtual node count increases. We start with 1 virtual node per physical node (equivalent to simple modulo hashing) and increase to 1000 virtual nodes, measuring distribution balance at each step. The test should show decreasing CV values as virtual node count increases.</p>\n<p>Minimal redistribution validation is critical for system performance because excessive key movement during membership changes creates significant network traffic and temporary unavailability. We record the initial assignment of 10,000 keys across a 4-node ring, add a fifth node, and count how many keys change assignment. Ideally, only ~20% of keys should move (since the new node should receive 1/5 of the total load).</p>\n<h4 id=\"milestone-2-cache-node-validation\">Milestone 2: Cache Node Validation</h4>\n<p>Cache node validation focuses on correct LRU behavior, accurate memory accounting, and proper TTL handling. These tests ensure that individual cache nodes provide reliable storage with expected eviction and expiration semantics.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Scenario</th>\n<th>Expected Behavior</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LRU Eviction Correctness</td>\n<td>Fill cache, access subset, add new entries</td>\n<td>Least recently accessed entries evicted first</td>\n<td>Track access order and eviction sequence</td>\n</tr>\n<tr>\n<td>Memory Limit Enforcement</td>\n<td>Add entries until memory limit reached</td>\n<td>Eviction begins before memory limit exceeded</td>\n<td>Monitor memory usage during operations</td>\n</tr>\n<tr>\n<td>TTL Expiration Accuracy</td>\n<td>Set entries with 1-second TTL</td>\n<td>Entries expire within 1.1 seconds</td>\n<td>Time-based validation with tolerance</td>\n</tr>\n<tr>\n<td>Concurrent Access Safety</td>\n<td>50 goroutines performing random operations</td>\n<td>No data races or corruption</td>\n<td>Race detector and data integrity checks</td>\n</tr>\n<tr>\n<td>Performance Under Load</td>\n<td>10,000 operations per second</td>\n<td>&lt;1ms p99 latency for cache operations</td>\n<td>Latency distribution measurement</td>\n</tr>\n</tbody></table>\n<p>LRU eviction correctness testing requires careful tracking of access patterns and eviction order. We create a cache with capacity for 10 entries, fill it completely, access entries 1, 3, 5, 7, 9, then add two new entries. The test should evict entries 2, 4 (least recently accessed) while preserving the others.</p>\n<p>Memory limit enforcement testing validates that the cache never significantly exceeds its configured memory limit. We add progressively larger entries and verify that eviction begins before the memory limit is reached. The cache should maintain memory usage within 5% of the configured limit.</p>\n<p>TTL expiration accuracy testing uses controlled time manipulation to verify precise expiration timing. We set entries with known TTL values and verify they expire at the expected time. The test accounts for cleanup delays and accepts expiration within a small tolerance window.</p>\n<h4 id=\"milestone-3-cluster-communication-validation\">Milestone 3: Cluster Communication Validation</h4>\n<p>Cluster communication validation ensures that nodes can discover each other, maintain consistent membership views, and route requests correctly. These tests validate the distributed aspects of the system.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Test Configuration</th>\n<th>Expected Outcome</th>\n<th>Verification Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node Discovery</td>\n<td>Start 3 nodes sequentially with join addresses</td>\n<td>All nodes discover each other within 10 seconds</td>\n<td>Membership list comparison</td>\n</tr>\n<tr>\n<td>Gossip Convergence</td>\n<td>Update node state, measure propagation</td>\n<td>State change reaches all nodes within 5 gossip intervals</td>\n<td>State consistency measurement</td>\n</tr>\n<tr>\n<td>Request Routing</td>\n<td>Send requests for specific keys</td>\n<td>Requests routed to correct hash ring nodes</td>\n<td>Request destination validation</td>\n</tr>\n<tr>\n<td>Health Check Accuracy</td>\n<td>Stop node processes</td>\n<td>Other nodes detect failure within 30 seconds</td>\n<td>Failure detection timing</td>\n</tr>\n<tr>\n<td>Partition Recovery</td>\n<td>Split cluster, then reconnect</td>\n<td>Cluster state merges correctly after recovery</td>\n<td>State reconciliation validation</td>\n</tr>\n</tbody></table>\n<p>Node discovery testing starts nodes with bootstrap addresses pointing to previously started nodes. Each new node should successfully join the cluster and receive complete membership information from existing nodes. The test verifies that all nodes end up with identical membership lists.</p>\n<p>Gossip convergence testing measures how quickly information propagates through the cluster. We update node metadata (such as load statistics) at one node and measure how long it takes for this information to reach all other nodes. With properly tuned gossip intervals, convergence should occur within 3-5 rounds.</p>\n<p>Request routing validation ensures that clients can send requests to any node and have them forwarded to the correct destination based on consistent hashing. We generate requests for specific keys with known hash ring assignments and verify that they reach the expected nodes.</p>\n<h4 id=\"milestone-4-replication-and-consistency-validation\">Milestone 4: Replication and Consistency Validation</h4>\n<p>Replication and consistency validation focuses on data durability, consistency guarantees, and conflict resolution. These tests ensure that the distributed cache maintains correctness under various failure scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint</th>\n<th>Scenario</th>\n<th>Consistency Configuration</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Replication</td>\n<td>Write key with RF=3</td>\n<td>W=1, R=1</td>\n<td>Value accessible from any replica node</td>\n</tr>\n<tr>\n<td>Quorum Consistency</td>\n<td>Concurrent read/write operations</td>\n<td>W=2, R=2, N=3</td>\n<td>Reads never return stale data</td>\n</tr>\n<tr>\n<td>Conflict Resolution</td>\n<td>Concurrent writes during partition</td>\n<td>Vector clock timestamps</td>\n<td>Deterministic conflict resolution</td>\n</tr>\n<tr>\n<td>Replica Recovery</td>\n<td>Node failure and restart</td>\n<td>Hinted handoff enabled</td>\n<td>Missed writes applied during recovery</td>\n</tr>\n<tr>\n<td>Read Repair</td>\n<td>Stale replica detected</td>\n<td>Automatic repair enabled</td>\n<td>Stale replicas updated transparently</td>\n</tr>\n</tbody></table>\n<p>Basic replication testing verifies that writes are successfully replicated to the configured number of nodes. We write a key with replication factor 3, then verify that the value can be retrieved by reading directly from each replica node. The test should succeed even if individual replica nodes are temporarily unavailable.</p>\n<p>Quorum consistency testing uses controlled timing to create race conditions between concurrent reads and writes. We configure quorum requirements (W=2, R=2 with N=3) and verify that reads never return stale or partially updated data, even when writes are still in progress.</p>\n<p>Conflict resolution testing creates intentional conflicts by performing concurrent writes to the same key from different nodes during a network partition. When the partition heals, the system should use vector clocks to deterministically resolve conflicts without losing data.</p>\n<h3 id=\"chaos-and-failure-testing\">Chaos and Failure Testing</h3>\n<p>Chaos and failure testing validates system resilience under adverse conditions that are difficult to predict but inevitable in production environments. These tests intentionally introduce failures and verify that the system continues to operate correctly.</p>\n<h4 id=\"network-partition-testing\">Network Partition Testing</h4>\n<p>Network partition testing simulates split-brain scenarios where the cluster becomes divided into multiple independent groups. These tests validate that the system maintains consistency and availability guarantees during partitions and recovers correctly when connectivity is restored.</p>\n<table>\n<thead>\n<tr>\n<th>Partition Scenario</th>\n<th>Cluster Configuration</th>\n<th>Partition Layout</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Majority/Minority Split</td>\n<td>5-node cluster</td>\n<td>3 nodes vs 2 nodes</td>\n<td>Majority side remains available, minority stops serving</td>\n</tr>\n<tr>\n<td>Even Split</td>\n<td>4-node cluster</td>\n<td>2 nodes vs 2 nodes</td>\n<td>Both sides reject operations requiring quorum</td>\n</tr>\n<tr>\n<td>Isolated Node</td>\n<td>5-node cluster</td>\n<td>1 node vs 4 nodes</td>\n<td>Isolated node stops serving, majority continues</td>\n</tr>\n<tr>\n<td>Progressive Partition</td>\n<td>6-node cluster</td>\n<td>Split into 3 groups of 2</td>\n<td>All groups stop serving for quorum operations</td>\n</tr>\n<tr>\n<td>Asymmetric Partition</td>\n<td>5-node cluster</td>\n<td>Full connectivity loss for one node pair</td>\n<td>Complex partition handling with partial connectivity</td>\n</tr>\n</tbody></table>\n<p>Majority/minority split testing validates that the system correctly implements quorum requirements during partitions. When a 5-node cluster splits into groups of 3 and 2 nodes, the majority group should continue serving requests while the minority group should reject operations that require quorum. This prevents split-brain scenarios where both sides accept writes that cannot be reconciled.</p>\n<p>Even split testing creates scenarios where neither partition has a clear majority. In these situations, the system should prioritize consistency over availability and reject operations that could lead to irreconcilable conflicts. Clients should receive clear error messages indicating that quorum cannot be achieved.</p>\n<p>Progressive partition testing simulates complex network failures where the cluster fractures into multiple small groups. This scenario tests the system&#39;s ability to handle cascading failures and maintain correctness even when normal operations become impossible.</p>\n<h4 id=\"cascading-failure-testing\">Cascading Failure Testing</h4>\n<p>Cascading failure testing validates that isolated failures don&#39;t trigger widespread system degradation. These tests are critical because distributed systems can exhibit complex failure propagation patterns that are difficult to predict.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Initial Trigger</th>\n<th>Expected Cascade</th>\n<th>Containment Mechanism</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Exhaustion</td>\n<td>One node runs out of memory</td>\n<td>Other nodes detect failure, rebalance load</td>\n<td>Circuit breakers prevent overload propagation</td>\n</tr>\n<tr>\n<td>Network Congestion</td>\n<td>High latency to one node</td>\n<td>Timeouts trigger suspicion, temporary exclusion</td>\n<td>Adaptive timeout and backoff mechanisms</td>\n</tr>\n<tr>\n<td>Processing Overload</td>\n<td>CPU saturation on cache node</td>\n<td>Request routing avoids overloaded node</td>\n<td>Load-aware request distribution</td>\n</tr>\n<tr>\n<td>Storage Failure</td>\n<td>Disk corruption on replica</td>\n<td>Read repair detects corruption, excludes bad replica</td>\n<td>Automated replica replacement</td>\n</tr>\n<tr>\n<td>Configuration Error</td>\n<td>Wrong replication factor setting</td>\n<td>Insufficient replicas for durability</td>\n<td>Validation prevents dangerous configurations</td>\n</tr>\n</tbody></table>\n<p>Memory exhaustion testing gradually increases memory pressure on individual nodes until they become unresponsive. The test verifies that other nodes detect the failure quickly and redistribute the load without becoming overloaded themselves. Circuit breakers should prevent failing nodes from receiving additional requests that would worsen their condition.</p>\n<p>Network congestion testing introduces artificial delays and packet loss to simulate overloaded network conditions. The system should adapt by adjusting timeouts, reducing gossip frequency, and temporarily excluding nodes that appear unresponsive due to network issues. The test verifies that temporary network problems don&#39;t cause permanent node exclusions.</p>\n<p>Processing overload testing creates CPU-intensive workloads on individual nodes to simulate performance degradation. The system should detect slow response times and redirect requests to healthier nodes while giving overloaded nodes time to recover.</p>\n<h4 id=\"byzantine-failure-testing\">Byzantine Failure Testing</h4>\n<p>Byzantine failure testing validates system behavior when nodes exhibit arbitrary or malicious behavior. While our cache system doesn&#39;t implement full Byzantine fault tolerance, it should handle common forms of byzantine behavior gracefully.</p>\n<table>\n<thead>\n<tr>\n<th>Byzantine Scenario</th>\n<th>Node Behavior</th>\n<th>System Response</th>\n<th>Recovery Mechanism</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Corrupted Responses</td>\n<td>Node returns garbage data for requests</td>\n<td>Other replicas provide correct data, bad node excluded</td>\n<td>Read repair corrects corrupted data</td>\n</tr>\n<tr>\n<td>Inconsistent Replies</td>\n<td>Node gives different answers to same question</td>\n<td>Majority consensus overrides inconsistent node</td>\n<td>Reputation system marks unreliable nodes</td>\n</tr>\n<tr>\n<td>Protocol Violations</td>\n<td>Node sends malformed gossip messages</td>\n<td>Message validation rejects bad messages</td>\n<td>Protocol parsing with strict validation</td>\n</tr>\n<tr>\n<td>Resource Hoarding</td>\n<td>Node accepts writes but never replicates</td>\n<td>Write timeouts detect replication failure</td>\n<td>Hinted handoff and alternative replicas</td>\n</tr>\n<tr>\n<td>Clock Skew</td>\n<td>Node has significantly wrong system time</td>\n<td>Vector clocks handle causality despite time skew</td>\n<td>Logical time prevents timestamp conflicts</td>\n</tr>\n</tbody></table>\n<p>Corrupted responses testing injects data corruption into individual nodes and verifies that the system detects and corrects the corruption using replicas. The test writes known values, corrupts them on one replica, then verifies that reads still return correct data through read repair mechanisms.</p>\n<p>Inconsistent replies testing creates nodes that provide different responses to identical requests, simulating flaky hardware or software bugs. The system should use majority consensus among replicas to determine the correct response and gradually exclude nodes that consistently provide inconsistent data.</p>\n<p>Protocol violations testing sends malformed messages to test the robustness of message parsing and validation. The system should gracefully handle invalid messages without crashing or corrupting state, while logging security events for investigation.</p>\n<h4 id=\"performance-degradation-testing\">Performance Degradation Testing</h4>\n<p>Performance degradation testing validates that the system maintains functionality even when performance characteristics degrade significantly from normal operation. These tests ensure that temporary performance problems don&#39;t cause permanent failures.</p>\n<table>\n<thead>\n<tr>\n<th>Degradation Type</th>\n<th>Induced Condition</th>\n<th>Performance Impact</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High Latency</td>\n<td>Network delays 1-5 seconds</td>\n<td>Operations timeout, retries increase</td>\n<td>Adaptive timeouts, exponential backoff</td>\n</tr>\n<tr>\n<td>Low Throughput</td>\n<td>Bandwidth limited to 1KB/s</td>\n<td>Gossip and replication slowed</td>\n<td>Priority queuing, compression</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Available RAM reduced to 10%</td>\n<td>Aggressive eviction, cache thrashing</td>\n<td>Emergency eviction, load shedding</td>\n</tr>\n<tr>\n<td>CPU Saturation</td>\n<td>Background CPU load at 90%</td>\n<td>Response times increase</td>\n<td>Request queuing, graceful degradation</td>\n</tr>\n<tr>\n<td>Disk I/O Limits</td>\n<td>Storage bandwidth capped</td>\n<td>Persistence operations delayed</td>\n<td>Async writes, batching</td>\n</tr>\n</tbody></table>\n<p>High latency testing introduces significant network delays and verifies that the system adapts its timeout values appropriately. Initial timeout values that work well under normal conditions may cause excessive failures when network latency increases. The system should detect increased latency and adjust timeouts dynamically.</p>\n<p>Low throughput testing simulates bandwidth-constrained environments and validates that essential operations continue to work despite reduced data transfer rates. Gossip protocols should reduce message frequency, and replication should prioritize critical data over less important metadata.</p>\n<p>Memory pressure testing reduces available memory and verifies that the cache system adapts by increasing eviction frequency and potentially reducing cache capacity. The system should maintain basic functionality even when memory pressure prevents optimal performance.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Testing distributed systems introduces numerous pitfalls that can lead to false confidence or missed bugs. Understanding these pitfalls helps create more effective tests that actually validate system correctness.</p>\n<p>⚠️ <strong>Pitfall: Timing-Dependent Test Assumptions</strong></p>\n<p>Many distributed cache tests make implicit assumptions about timing that don&#39;t hold under realistic conditions. For example, assuming that gossip messages will propagate within a fixed time window, or that TTL expiration will occur at precisely the specified time. These assumptions lead to flaky tests that pass in development but fail in production.</p>\n<p>The underlying problem is that distributed systems have inherently asynchronous behavior with variable timing. Network delays, garbage collection pauses, and CPU scheduling can all introduce timing variations that break tests with rigid timing assumptions.</p>\n<p>To fix this issue, use eventual consistency testing patterns with retries and reasonable timeout bounds. Instead of asserting that gossip propagation completes in exactly 5 seconds, check periodically for up to 30 seconds and fail only if propagation never completes. This approach tests the eventual outcome while accommodating realistic timing variations.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Concurrent Access Testing</strong></p>\n<p>Distributed caches experience high concurrency in production, but many tests only validate single-threaded scenarios. This leads to race conditions and data corruption that only appear under load. The problem is particularly severe for components like LRU caches that maintain complex internal state across multiple data structures.</p>\n<p>The root cause is that concurrency bugs often require specific timing and thread interleaving to manifest. Unit tests that don&#39;t stress the system with realistic concurrency patterns miss these critical issues.</p>\n<p>To address this, implement stress testing with hundreds of concurrent goroutines performing random operations. Use Go&#39;s race detector to catch data races, and run tests multiple times to increase the probability of triggering race conditions. Also validate that complex operations maintain consistency - for example, verifying that LRU eviction never corrupts the doubly-linked list structure.</p>\n<p>⚠️ <strong>Pitfall: Mock-Heavy Integration Tests</strong></p>\n<p>Some integration tests use so many mocks that they don&#39;t actually test component integration. While mocking is valuable for unit tests, overuse in integration tests can create false confidence by testing the mocks rather than real component interactions.</p>\n<p>This problem occurs because mocks may not accurately simulate the timing, error conditions, or state changes of real components. Tests pass because the mocks behave predictably, but real components may have different failure modes or performance characteristics.</p>\n<p>The solution is to use real components whenever possible in integration tests, with mocking reserved for external dependencies like network calls or file system operations. When mocks are necessary, ensure they accurately simulate the timing and failure modes of real components based on careful analysis of the actual implementations.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Failure Scenario Coverage</strong></p>\n<p>Many test suites focus heavily on happy path scenarios while providing insufficient coverage of failure modes. This is problematic because distributed systems spend significant time handling various types of failures, and correctness under failure is often more critical than optimal performance under normal conditions.</p>\n<p>The underlying issue is that failure scenarios are more complex to set up and validate than success scenarios. It&#39;s easier to test that a successful write returns the correct response than to test that a write during a network partition handles replica failures correctly while maintaining consistency guarantees.</p>\n<p>To improve failure coverage, systematically enumerate failure modes for each component and create specific tests for each scenario. Use fault injection techniques to simulate network failures, node crashes, and resource exhaustion. Validate not just that the system continues to function, but that it maintains correctness properties like consistency and durability.</p>\n<p>⚠️ <strong>Pitfall: Test Environment Differs from Production</strong></p>\n<p>Test environments often have characteristics that don&#39;t match production deployment, leading to bugs that only appear after deployment. Common differences include single-machine testing of distributed components, unrealistic network conditions, and different resource constraints.</p>\n<p>The problem is particularly severe for distributed cache systems because their behavior depends heavily on network latency, node failure patterns, and data distribution characteristics. A test that runs all nodes on localhost with microsecond latencies may not catch issues that appear with millisecond WAN latencies.</p>\n<p>To mitigate this issue, use test environments that simulate production characteristics as closely as possible. Run multi-node tests across separate processes or containers, introduce realistic network delays and packet loss, and test with data distributions that match expected production workloads. Consider using containerized test environments that can simulate various deployment scenarios.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing strategy requires sophisticated tooling and infrastructure to validate distributed system behavior effectively. The following implementation guidance provides concrete approaches for building comprehensive test suites.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Testing Layer</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>Go&#39;s built-in testing + testify assertions</td>\n<td>Ginkgo/Gomega BDD framework</td>\n</tr>\n<tr>\n<td>Mocking</td>\n<td>Manual interface mocking</td>\n<td>GoMock generated mocks</td>\n</tr>\n<tr>\n<td>Integration Testing</td>\n<td>Docker Compose multi-container</td>\n<td>Kubernetes test clusters</td>\n</tr>\n<tr>\n<td>Network Simulation</td>\n<td>Manual delays with time.Sleep</td>\n<td>Comcast network condition simulation</td>\n</tr>\n<tr>\n<td>Chaos Testing</td>\n<td>Manual failure injection</td>\n<td>Chaos Mesh orchestrated failures</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Simple goroutine loops</td>\n<td>Vegeta load testing tool</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Basic log validation</td>\n<td>Prometheus metrics + alerting</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-testing-structure\">Recommended Testing Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    hashring/\n      hashring.go\n      hashring_test.go          ← unit tests\n      integration_test.go       ← cross-component integration\n    cache/\n      lru.go\n      lru_test.go\n      benchmark_test.go         ← performance validation\n    cluster/\n      gossip.go\n      gossip_test.go\n      chaos_test.go            ← failure simulation\n  test/\n    fixtures/                   ← test data and configurations\n      cluster-configs/\n      test-keys.json\n    integration/                ← full system integration tests\n      cluster_test.go\n      replication_test.go\n    chaos/                      ← chaos engineering tests\n      partition_test.go\n      cascade_test.go\n    tools/                      ← testing utilities\n      network_simulator.go\n      cluster_manager.go\n  docker-compose.test.yml       ← multi-node test environment</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Complete network simulation utility for testing distributed behavior:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testtools</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NetworkSimulator provides controlled network conditions for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NetworkSimulator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes           </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SimulatedNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    partitions      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    globalLatency   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    packetLossRate  </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SimulatedNode represents a node in the test network</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SimulatedNode</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IsHealthy       </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CustomLatency   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MessageQueue    </span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> SimulatedMessage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DeliveryHandler </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">SimulatedMessage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SimulatedMessage represents a message in the test network</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SimulatedMessage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    From        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    To          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MessageType </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Payload     []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SentAt      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DeliverAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewNetworkSimulator creates a network simulator for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewNetworkSimulator</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodes:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SimulatedNode</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        partitions: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddNode registers a node in the simulated network</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">SimulatedMessage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ns.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SimulatedNode</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        NodeID:          nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Address:         address,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        IsHealthy:       </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MessageQueue:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> SimulatedMessage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DeliveryHandler: handler,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.nodes[nodeID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start message delivery goroutine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> ns.</span><span style=\"color:#B392F0\">processNodeMessages</span><span style=\"color:#E1E4E8\">(node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendMessage simulates sending a message with network conditions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">from</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">to</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">messageType</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">payload</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ns.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check if nodes are partitioned</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> ns.</span><span style=\"color:#B392F0\">arePartitioned</span><span style=\"color:#E1E4E8\">(from, to) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"network partition prevents delivery\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simulate packet loss</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> rand.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> ns.packetLossRate {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // Message lost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate delivery time with latency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    latency </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ns.globalLatency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> node, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ns.nodes[to]; exists </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> node.CustomLatency </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        latency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node.CustomLatency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> SimulatedMessage</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        From:        from,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        To:          to,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MessageType: messageType,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Payload:     payload,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SentAt:      time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        DeliverAt:   time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(latency),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Queue message for delivery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> node, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ns.nodes[to]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> node.MessageQueue </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> message:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"message queue full\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"destination node not found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CreatePartition simulates network partition between node groups</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CreatePartition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">group1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">group2</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ns.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    partitionID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"partition_</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UnixNano</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.partitions[partitionID] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(group1, group2</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealPartitions removes all network partitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealPartitions</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ns.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.partitions </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetGlobalLatency configures network latency for all connections</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetGlobalLatency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">latency</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ns.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.globalLatency </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> latency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetPacketLoss configures packet loss rate (0.0 to 1.0)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetPacketLoss</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rate</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ns.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ns.packetLossRate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// processNodeMessages delivers queued messages respecting timing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">processNodeMessages</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">node</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">SimulatedNode</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> message </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> node.MessageQueue {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Wait for delivery time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> delay </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Until</span><span style=\"color:#E1E4E8\">(message.DeliverAt); delay </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Deliver message if node is still healthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ns.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        isHealthy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> node.IsHealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ns.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> isHealthy {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            node.</span><span style=\"color:#B392F0\">DeliveryHandler</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// arePartitioned checks if two nodes are separated by partition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ns </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">arePartitioned</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">node1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">node2</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, partition </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ns.partitions {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        node1InPartition </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        node2InPartition </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> _, node </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> partition {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> node1 {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node1InPartition </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> node2 {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node2InPartition </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // If both nodes are in same partition, they can communicate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> node1InPartition </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> node2InPartition {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // If one is in partition and other isn't, they're separated</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> node1InPartition </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> node2InPartition {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Complete cluster test manager for integration testing:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testtools</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">distributed-cache/internal/cluster</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">distributed-cache/internal/config</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ClusterTestManager manages multi-node clusters for integration testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClusterTestManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes           []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">cluster</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    configs         []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">NodeConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    networkSim      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkSimulator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    basePort        </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataDir         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t               </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewClusterTestManager creates a test cluster manager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewClusterTestManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nodeCount</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodes:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">cluster</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Node</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, nodeCount),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        configs:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, nodeCount),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        networkSim: </span><span style=\"color:#B392F0\">NewNetworkSimulator</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        basePort:   </span><span style=\"color:#79B8FF\">9000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        dataDir:    t.</span><span style=\"color:#B392F0\">TempDir</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        t:          t,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartCluster creates and starts a test cluster with specified configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartCluster</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeCount</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">replicationFactor</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Generate node configurations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> joinAddresses []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> nodeCount; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        address </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"127.0.0.1:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, ctm.basePort</span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\">i)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodeConfig </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">NodeConfig</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            NodeID:              fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, i),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ListenAddress:       address,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            AdvertiseAddr:       address,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            JoinAddresses:       joinAddresses, </span><span style=\"color:#6A737D\">// Each node joins previous ones</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            MaxMemoryMB:         </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            VirtualNodes:        </span><span style=\"color:#79B8FF\">150</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReplicationFactor:   replicationFactor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            HealthCheckInterval: </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            GossipInterval:      </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            RequestTimeout:      </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctm.configs </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(ctm.configs, nodeConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        joinAddresses </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(joinAddresses, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start nodes sequentially to allow proper cluster formation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, config </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ctm.configs {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        node, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> cluster.</span><span style=\"color:#B392F0\">NewNode</span><span style=\"color:#E1E4E8\">(config)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to create node </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, i, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctm.nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(ctm.nodes, node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> node.</span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">()); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to start node </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, i, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Allow time for node to join cluster</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">500</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Millisecond)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Wait for cluster convergence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ctm.</span><span style=\"color:#B392F0\">WaitForConvergence</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WaitForConvergence waits until all nodes agree on cluster membership</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WaitForConvergence</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), timeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">500</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Millisecond)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"cluster convergence timeout after </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, timeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> ctm.</span><span style=\"color:#B392F0\">isClusterConverged</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ctm.t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Cluster converged with </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> nodes\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(ctm.nodes))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// isClusterConverged checks if all nodes have consistent membership views</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">isClusterConverged</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ctm.nodes) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expectedNodeCount </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ctm.nodes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, node </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ctm.nodes {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        membership </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> node.</span><span style=\"color:#B392F0\">GetMembership</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(membership) </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> expectedNodeCount {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ctm.t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Node </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> sees </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> nodes, expected </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, i, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(membership), expectedNodeCount)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Verify all expected nodes are present</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> j, expectedNode </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ctm.nodes {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            found </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> _, seenNodeID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> membership {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> seenNodeID </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> expectedNode.NodeID {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    found </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">found {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ctm.t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Node </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> doesn't see node </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">, i, j, expectedNode.NodeID)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StopCluster shuts down all nodes in the test cluster</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StopCluster</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> wg </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, node </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ctm.nodes {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">index</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">n</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">cluster</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Node</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            defer</span><span style=\"color:#E1E4E8\"> wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> n.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">(ctx); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ctm.t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Error stopping node </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, index, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }(i, node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctm.nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctm.configs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetNode returns a specific node from the cluster</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetNode</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">index</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">cluster</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Node</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ctm.nodes) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ctm.nodes[index]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetRandomNode returns a randomly selected node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetRandomNode</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">cluster</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Node</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ctm.nodes) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ctm.nodes[rand.</span><span style=\"color:#B392F0\">Intn</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(ctm.nodes))]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SimulateNodeFailure marks a node as failed and stops it</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SimulateNodeFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeIndex</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> nodeIndex </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ctm.nodes) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node index </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> out of range\"</span><span style=\"color:#E1E4E8\">, nodeIndex)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctm.nodes[nodeIndex]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctm.t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Simulating failure of node </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, node.NodeID)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> node.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SimulateNetworkPartition creates partition between specified node groups</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SimulateNetworkPartition</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">group1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">group2</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group1Nodes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(group1))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group2Nodes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(group2))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, nodeIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> group1 {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> nodeIndex </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ctm.nodes) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node index </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> out of range\"</span><span style=\"color:#E1E4E8\">, nodeIndex)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        group1Nodes[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ctm.nodes[nodeIndex].NodeID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i, nodeIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> group2 {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> nodeIndex </span><span style=\"color:#F97583\">>=</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(ctm.nodes) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"node index </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> out of range\"</span><span style=\"color:#E1E4E8\">, nodeIndex)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        group2Nodes[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ctm.nodes[nodeIndex].NodeID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctm.networkSim.</span><span style=\"color:#B392F0\">CreatePartition</span><span style=\"color:#E1E4E8\">(group1Nodes, group2Nodes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctm.t.</span><span style=\"color:#B392F0\">Logf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Created network partition: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\"> | </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, group1Nodes, group2Nodes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealNetworkPartitions removes all network partitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ctm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterTestManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealNetworkPartitions</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctm.networkSim.</span><span style=\"color:#B392F0\">HealPartitions</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctm.t.</span><span style=\"color:#B392F0\">Log</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Healed all network partitions\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>Hash ring testing skeleton with detailed validation steps:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// TestHashRingDistribution validates even key distribution across nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestHashRingDistribution</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create hash ring with 5 nodes and 150 virtual nodes each</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate 50,000 random keys using crypto/rand for true randomness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Map each key to its assigned node using GetNode()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Count keys per node and calculate distribution statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify no node has more than 25% of keys (perfect would be 20%)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Calculate coefficient of variation (CV = stddev/mean)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Assert CV &#x3C; 0.15 for acceptable load balancing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use t.Logf() to output distribution statistics for analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestHashRingRedistribution validates minimal key movement during membership changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestHashRingRedistribution</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create ring with 4 nodes, record initial key assignments for 10,000 keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add fifth node to the ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Recalculate key assignments and count how many keys moved</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify that approximately 20% of keys moved (optimal redistribution)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Repeat test with node removal, verify similar minimal movement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test with multiple membership changes, ensure cumulative effect is reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Store key->node mapping before change, compare after change</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>LRU cache concurrent testing skeleton:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// TestLRUCacheConcurrency validates thread safety under high concurrency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestLRUCacheConcurrency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create LRU cache with 1MB capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Launch 50 goroutines performing random Get/Set/Delete operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Each goroutine should perform 1000 operations with random keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Use sync.WaitGroup to coordinate goroutine completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate cache state consistency after all operations complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify no data races using go test -race</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Check that cache size never exceeds configured capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use shared counter for operation tracking, validate final state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestLRUEvictionOrder validates correct LRU eviction behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestLRUEvictionOrder</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create cache with capacity for exactly 5 entries of 100 bytes each</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Fill cache completely with keys A, B, C, D, E</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Access keys A, C, E to update their LRU positions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add new key F, verify that B is evicted (least recently used)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Add new key G, verify that D is evicted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify remaining keys A, C, E, F, G are all accessible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Track access order manually, verify eviction matches expectations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Checkpoint: Hash Ring Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run hash ring tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/hashring</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -race</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Distribution test: CV = 0.12 (acceptable)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Redistribution test: 22% keys moved (optimal)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Performance test: 0.8μs avg lookup time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ All tests pass without race conditions</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Create simple CLI tool to test hash ring manually</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/ring-test/main.go</span><span style=\"color:#79B8FF\"> --nodes</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#79B8FF\"> --keys</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#79B8FF\"> --virtual-nodes</span><span style=\"color:#79B8FF\"> 100</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint: Cache Node Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run cache node tests with race detection</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/cache</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#79B8FF\"> -timeout</span><span style=\"color:#9ECBFF\"> 30s</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ LRU eviction maintains correct order under concurrency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Memory accounting accurate within 1% tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ TTL expiration within 100ms of expected time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ No data races detected</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Start cache node and test via HTTP</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/cache-node/main.go</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 8080</span><span style=\"color:#79B8FF\"> --memory</span><span style=\"color:#9ECBFF\"> 100MB</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> PUT</span><span style=\"color:#9ECBFF\"> localhost:8080/cache/testkey</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> \"testvalue\"</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> localhost:8080/cache/testkey</span><span style=\"color:#6A737D\">  # Should return \"testvalue\"</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint: Cluster Communication Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run integration tests with multi-node cluster</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./test/integration</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -timeout</span><span style=\"color:#9ECBFF\"> 60s</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ 3-node cluster converges within 10 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Gossip propagates state changes within 5 intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Requests route to correct nodes based on hash ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Failed nodes detected within 30 seconds</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Start 3-node cluster manually</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/node/main.go</span><span style=\"color:#79B8FF\"> --node-id</span><span style=\"color:#9ECBFF\"> node1</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 9001</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/node/main.go</span><span style=\"color:#79B8FF\"> --node-id</span><span style=\"color:#9ECBFF\"> node2</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 9002</span><span style=\"color:#79B8FF\"> --join</span><span style=\"color:#9ECBFF\"> 127.0.0.1:9001</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/node/main.go</span><span style=\"color:#79B8FF\"> --node-id</span><span style=\"color:#9ECBFF\"> node3</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 9003</span><span style=\"color:#79B8FF\"> --join</span><span style=\"color:#9ECBFF\"> 127.0.0.1:9001</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test cluster membership via API</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> localhost:9001/cluster/members</span><span style=\"color:#6A737D\">  # Should show all 3 nodes</span></span></code></pre></div>\n\n<p><strong>Milestone 4 Checkpoint: Replication and Consistency Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run replication tests with fault injection</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./test/chaos</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> -timeout</span><span style=\"color:#9ECBFF\"> 120s</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Replication factor 3 stores data on 3 nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Quorum reads (R=2) never return stale data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Conflict resolution deterministic with vector clocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># ✓ Node recovery restores missed writes via hinted handoff</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Manual verification with network partition simulation:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Use tc (traffic control) to simulate network partition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># This requires sudo privileges and Linux environment</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sudo</span><span style=\"color:#9ECBFF\"> tc</span><span style=\"color:#9ECBFF\"> qdisc</span><span style=\"color:#9ECBFF\"> add</span><span style=\"color:#9ECBFF\"> dev</span><span style=\"color:#9ECBFF\"> lo</span><span style=\"color:#9ECBFF\"> root</span><span style=\"color:#9ECBFF\"> handle</span><span style=\"color:#9ECBFF\"> 1:</span><span style=\"color:#9ECBFF\"> prio</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sudo</span><span style=\"color:#9ECBFF\"> tc</span><span style=\"color:#9ECBFF\"> filter</span><span style=\"color:#9ECBFF\"> add</span><span style=\"color:#9ECBFF\"> dev</span><span style=\"color:#9ECBFF\"> lo</span><span style=\"color:#9ECBFF\"> parent</span><span style=\"color:#9ECBFF\"> 1:</span><span style=\"color:#9ECBFF\"> protocol</span><span style=\"color:#9ECBFF\"> ip</span><span style=\"color:#9ECBFF\"> pref</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#9ECBFF\"> u32</span><span style=\"color:#9ECBFF\"> match</span><span style=\"color:#9ECBFF\"> ip</span><span style=\"color:#9ECBFF\"> dport</span><span style=\"color:#79B8FF\"> 9001</span><span style=\"color:#79B8FF\"> 0xffff</span><span style=\"color:#9ECBFF\"> flowid</span><span style=\"color:#9ECBFF\"> 1:3</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sudo</span><span style=\"color:#9ECBFF\"> tc</span><span style=\"color:#9ECBFF\"> qdisc</span><span style=\"color:#9ECBFF\"> add</span><span style=\"color:#9ECBFF\"> dev</span><span style=\"color:#9ECBFF\"> lo</span><span style=\"color:#9ECBFF\"> parent</span><span style=\"color:#9ECBFF\"> 1:3</span><span style=\"color:#9ECBFF\"> handle</span><span style=\"color:#9ECBFF\"> 30:</span><span style=\"color:#9ECBFF\"> netem</span><span style=\"color:#9ECBFF\"> loss</span><span style=\"color:#9ECBFF\"> 100%</span><span style=\"color:#6A737D\">  # Block node1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test that cluster continues operating with 2/3 nodes available</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> localhost:9002/cache/testkey</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> PUT</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> \"partition-test\"</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> localhost:9003/cache/testkey</span><span style=\"color:#6A737D\">  # Should return \"partition-test\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Remove partition and verify recovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sudo</span><span style=\"color:#9ECBFF\"> tc</span><span style=\"color:#9ECBFF\"> qdisc</span><span style=\"color:#9ECBFF\"> del</span><span style=\"color:#9ECBFF\"> dev</span><span style=\"color:#9ECBFF\"> lo</span><span style=\"color:#9ECBFF\"> root</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test flakes randomly</td>\n<td>Race conditions or timing assumptions</td>\n<td>Run with <code>-race</code> flag, add logging to timing-sensitive code</td>\n<td>Use proper synchronization, avoid fixed timeouts</td>\n</tr>\n<tr>\n<td>Hash ring distribution uneven</td>\n<td>Poor hash function or insufficient virtual nodes</td>\n<td>Log key distribution statistics, visualize ring positions</td>\n<td>Increase virtual nodes, verify hash function quality</td>\n</tr>\n<tr>\n<td>Cache memory usage inaccurate</td>\n<td>Incorrect size calculations or metadata overhead</td>\n<td>Add detailed memory accounting logs</td>\n<td>Account for all metadata, verify size calculations</td>\n</tr>\n<tr>\n<td>Gossip convergence slow</td>\n<td>Network delays or too-large gossip payload</td>\n<td>Add gossip timing logs, measure message sizes</td>\n<td>Tune gossip intervals, compress large payloads</td>\n</tr>\n<tr>\n<td>Replication conflicts not resolved</td>\n<td>Vector clock bugs or incorrect conflict resolution</td>\n<td>Log vector clock states and comparison results</td>\n<td>Debug vector clock merge logic, verify timestamp handling</td>\n</tr>\n<tr>\n<td>Cluster partitions don&#39;t heal</td>\n<td>Stale membership information or failed node detection</td>\n<td>Add partition detection logs, verify failure detector</td>\n<td>Implement partition detection, improve failure detector accuracy</td>\n</tr>\n</tbody></table>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones but is particularly critical for troubleshooting issues that emerge during integration between components. It provides essential debugging techniques for Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication &amp; Consistency).</p>\n</blockquote>\n<h3 id=\"mental-model-the-detective39s-investigation-kit\">Mental Model: The Detective&#39;s Investigation Kit</h3>\n<p>Think of debugging a distributed cache like being a detective investigating a complex case with multiple witnesses (nodes) who may have different stories about what happened. Just as a detective needs multiple investigation techniques - interviewing witnesses, examining physical evidence, creating timelines, and testing theories - debugging distributed systems requires a systematic toolkit of diagnostic approaches.</p>\n<p>The key insight is that in distributed systems, no single node has the complete picture. Each node is like a witness who only saw part of the incident. Some witnesses might be lying (Byzantine behavior), some might be confused (clock skew), and some might not have been present when important events occurred (network partitions). Your job as the system detective is to piece together the truth from multiple partial and potentially contradictory sources of evidence.</p>\n<p>Unlike debugging single-threaded programs where you can step through line by line, distributed cache debugging is more like forensic investigation - you&#39;re analyzing artifacts left behind by asynchronous processes that may have run hours or days ago. The evidence is scattered across log files, metrics dashboards, and persistent state on different machines. Just as crime scene investigators have protocols for collecting and analyzing evidence without contaminating it, distributed systems debugging requires methodical approaches that don&#39;t inadvertently change the system state you&#39;re trying to observe.</p>\n<h3 id=\"common-bug-patterns\">Common Bug Patterns</h3>\n<p>Understanding the most frequently encountered issues in distributed cache implementations helps you quickly identify root causes when problems arise. These patterns emerge from the inherent complexity of coordinating multiple nodes, managing network communication, and maintaining data consistency across an unreliable infrastructure.</p>\n<h4 id=\"hash-ring-distribution-problems\">Hash Ring Distribution Problems</h4>\n<p><strong>Uneven Key Distribution Hotspots</strong></p>\n<p>The most common hash ring issue manifests as severe load imbalance where one or two nodes receive dramatically more traffic than others. This typically occurs when the hash function produces poor distribution or when insufficient virtual nodes allow natural clustering of hash values around certain ring positions.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Root Cause</th>\n<th>Detection Method</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>One node consistently at 90%+ CPU while others idle</td>\n<td>Poor hash function distribution or insufficient virtual nodes</td>\n<td>Monitor per-node request rates and memory usage</td>\n<td>Increase virtual nodes per physical node, verify hash function quality</td>\n</tr>\n<tr>\n<td>New keys consistently hash to same small subset of nodes</td>\n<td>Hash function has clustering bias for your key patterns</td>\n<td>Analyze hash output distribution across sample keys</td>\n<td>Switch hash functions (SHA1 to FNV or vice versa)</td>\n</tr>\n<tr>\n<td>Load becomes more uneven as cluster grows</td>\n<td>Virtual node count too low relative to cluster size</td>\n<td>Track load variance as nodes are added</td>\n<td>Use virtual nodes = 150-200 per physical node</td>\n</tr>\n<tr>\n<td>Cache hit rates vary wildly between nodes</td>\n<td>Popular keys concentrated on specific nodes</td>\n<td>Monitor hit rates and key access patterns per node</td>\n<td>Implement consistent hashing for client-side routing</td>\n</tr>\n</tbody></table>\n<p><strong>Incorrect Ring Rebalancing</strong></p>\n<p>When nodes join or leave the cluster, keys should only move between the affected node and its immediate neighbors on the hash ring. Incorrect rebalancing implementations often move far more keys than necessary, causing unnecessary cache misses and network traffic.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Typical Implementation Bug</th>\n<th>Fix Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>All keys move when single node added</td>\n<td>Recalculating all positions instead of incremental updates</td>\n<td>Only move keys between new node and its ring successor</td>\n</tr>\n<tr>\n<td>Keys move to wrong nodes during rebalancing</td>\n<td>Incorrect successor calculation or stale ring state</td>\n<td>Verify ring ordering and atomic ring updates</td>\n</tr>\n<tr>\n<td>Temporary key unavailability during rebalancing</td>\n<td>Removing old mapping before establishing new mapping</td>\n<td>Use two-phase rebalancing with overlap period</td>\n</tr>\n<tr>\n<td>Duplicate keys across multiple nodes</td>\n<td>Race condition during concurrent ring updates</td>\n<td>Serialize ring modifications with distributed locks</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Insight</strong>: The hash ring&#39;s primary value is minimizing key movement during membership changes. If you&#39;re moving more than 1/N keys when adding the Nth node, your rebalancing logic is incorrect.</p>\n</blockquote>\n<h4 id=\"lru-cache-implementation-bugs\">LRU Cache Implementation Bugs</h4>\n<p><strong>Memory Accounting Errors</strong></p>\n<p>Accurate memory accounting is essential for LRU eviction to work correctly. Small accounting errors compound over time and can lead to either premature eviction (wasting cache space) or memory exhaustion (allowing usage to exceed configured limits).</p>\n<p>⚠️ <strong>Pitfall: Double-Counting Memory During Updates</strong></p>\n<p>When updating an existing cache entry, naive implementations often add the new value size without subtracting the old value size, causing memory usage to grow unboundedly. The correct approach requires atomic size accounting where the delta (new size - old size) is applied as a single operation.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Accounting Error</th>\n<th>Manifestation</th>\n<th>Debugging Approach</th>\n<th>Correction</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Size never decreases on entry updates</td>\n<td>Memory usage only grows, never shrinks</td>\n<td>Log size changes for each operation</td>\n<td>Calculate size delta atomically</td>\n</tr>\n<tr>\n<td>Eviction triggers too early</td>\n<td>Cache stays well below configured limit</td>\n<td>Compare tracked usage vs actual entry sizes</td>\n<td>Audit size calculation for all data types</td>\n</tr>\n<tr>\n<td>Cache grows beyond memory limit</td>\n<td>Out of memory errors despite eviction</td>\n<td>Monitor actual vs tracked memory usage</td>\n<td>Add periodic memory reconciliation</td>\n</tr>\n<tr>\n<td>Negative memory usage values</td>\n<td>Arithmetic underflow in size tracking</td>\n<td>Check for unsigned integer wraparound</td>\n<td>Use signed integers and bounds checking</td>\n</tr>\n</tbody></table>\n<p><strong>TTL Cleanup Performance Issues</strong></p>\n<p>Expired entry cleanup must balance thoroughness with performance impact. Overly aggressive cleanup blocks normal cache operations, while insufficient cleanup allows expired entries to consume memory and skew LRU ordering.</p>\n<p>The lazy expiration approach checks TTL during normal <code>Get</code> operations, which works well for frequently accessed keys but allows rarely accessed expired entries to accumulate. Periodic background cleanup addresses this but must be carefully throttled to avoid interfering with cache performance.</p>\n<table>\n<thead>\n<tr>\n<th>TTL Issue Pattern</th>\n<th>Performance Impact</th>\n<th>Implementation Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Checking expiration on every cache access</td>\n<td>High CPU overhead on Get operations</td>\n<td>Batch expiration checks and use efficient time comparisons</td>\n</tr>\n<tr>\n<td>Full cache scan for expired entries</td>\n<td>Periodic operation stalls affecting all requests</td>\n<td>Implement incremental cleanup scanning fixed number of entries per interval</td>\n</tr>\n<tr>\n<td>Expired entries remain in LRU ordering</td>\n<td>Skewed eviction decisions favoring expired over live data</td>\n<td>Remove from LRU immediately on expiration detection</td>\n</tr>\n<tr>\n<td>TTL cleanup during high-traffic periods</td>\n<td>Cleanup competes with user requests for locks</td>\n<td>Schedule cleanup during low-traffic periods or use separate cleanup threads</td>\n</tr>\n</tbody></table>\n<h4 id=\"network-communication-failures\">Network Communication Failures</h4>\n<p><strong>Split-Brain Detection and Recovery</strong></p>\n<p>Network partitions create the most challenging debugging scenarios because different parts of the cluster have inconsistent views of membership and data. The key to diagnosis is recognizing the symptoms of partition behavior rather than simple node failures.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: In a true split-brain scenario, both sides of the partition believe they are the authoritative cluster and the other side has failed. This leads to divergent decisions that must be reconciled when the partition heals.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Split-Brain Symptom</th>\n<th>What It Indicates</th>\n<th>Diagnostic Steps</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Conflicting cluster membership from different nodes</td>\n<td>Network partition isolating node subsets</td>\n<td>Query membership from multiple nodes and compare</td>\n<td>Implement partition detection and read-only mode</td>\n</tr>\n<tr>\n<td>Same keys with different values on different nodes</td>\n<td>Writes continued during partition</td>\n<td>Compare key versions across suspected partition groups</td>\n<td>Trigger conflict resolution and anti-entropy repair</td>\n</tr>\n<tr>\n<td>Nodes report other healthy nodes as failed</td>\n<td>Asymmetric network failure</td>\n<td>Check bidirectional connectivity between reported failed nodes</td>\n<td>Adjust failure detection thresholds or implement external health checks</td>\n</tr>\n<tr>\n<td>Client gets different responses from different nodes</td>\n<td>Load balancer distributing across partitioned cluster</td>\n<td>Test connectivity matrix between all node pairs</td>\n<td>Update client routing to avoid partitioned nodes</td>\n</tr>\n</tbody></table>\n<p><strong>Gossip Protocol Convergence Issues</strong></p>\n<p>The gossip protocol should ensure all nodes eventually receive membership updates, but various implementation bugs can prevent convergence or cause excessive network traffic.</p>\n<p>⚠️ <strong>Pitfall: Gossip Message Size Growth</strong></p>\n<p>A common bug is including the entire cluster state in every gossip message rather than just recent changes. This works fine for small clusters but causes message sizes to grow quadratically with cluster size, eventually overwhelming network capacity or triggering packet fragmentation.</p>\n<table>\n<thead>\n<tr>\n<th>Convergence Problem</th>\n<th>Root Cause</th>\n<th>Detection Method</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Some nodes never learn about membership changes</td>\n<td>Gossip target selection bias or partition</td>\n<td>Compare final membership state across all nodes</td>\n<td>Implement random peer selection with partition detection</td>\n</tr>\n<tr>\n<td>Excessive gossip network traffic</td>\n<td>Including full state instead of deltas</td>\n<td>Monitor gossip message sizes and network bandwidth</td>\n<td>Send only recent state changes and implement anti-entropy</td>\n</tr>\n<tr>\n<td>Slow convergence even without failures</td>\n<td>Gossip interval too long or insufficient fan-out</td>\n<td>Measure time for membership change propagation</td>\n<td>Adjust gossip frequency and increase targets per round</td>\n</tr>\n<tr>\n<td>Gossip storms during cluster changes</td>\n<td>Every node immediately gossips when receiving updates</td>\n<td>Monitor gossip frequency spikes</td>\n<td>Implement jitter and backoff for gossip scheduling</td>\n</tr>\n</tbody></table>\n<h4 id=\"replication-consistency-bugs\">Replication Consistency Bugs</h4>\n<p><strong>Vector Clock Implementation Errors</strong></p>\n<p>Vector clocks track causality relationships between operations but are error-prone to implement correctly. The most common bugs involve incorrect clock advancement, comparison logic, or merging behavior.</p>\n<table>\n<thead>\n<tr>\n<th>Vector Clock Bug</th>\n<th>Incorrect Behavior</th>\n<th>Manifestation</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Not advancing local clock on writes</td>\n<td>Cannot detect which operations came first</td>\n<td>Conflicts between causally ordered operations</td>\n<td>Increment local node&#39;s clock entry on every write</td>\n</tr>\n<tr>\n<td>Incorrect causality comparison</td>\n<td>Wrong conflict resolution decisions</td>\n<td>Overwriting newer values with older ones</td>\n<td>Implement proper dominance checking (all entries ≤ with at least one &lt;)</td>\n</tr>\n<tr>\n<td>Clock merging overwrites instead of taking maximum</td>\n<td>Lost causality information</td>\n<td>False conflicts for concurrent operations</td>\n<td>Merge by taking max of each entry across both clocks</td>\n</tr>\n<tr>\n<td>Clock memory leaks from abandoned nodes</td>\n<td>Unbounded growth of clock size</td>\n<td>Performance degradation over time</td>\n<td>Implement clock pruning for nodes absent from recent operations</td>\n</tr>\n</tbody></table>\n<p><strong>Quorum Consistency Violations</strong></p>\n<p>Quorum-based consistency requires careful coordination between read and write operations to ensure the intersection property (R + W &gt; N). Implementation bugs often violate this property or handle partial failures incorrectly.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: The fundamental guarantee of quorum systems is that every read will see at least one replica that participated in the most recent successful write. Violating this guarantee leads to lost writes or stale reads.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Consistency Violation</th>\n<th>Implementation Bug</th>\n<th>Detection Approach</th>\n<th>Correction</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Reads return stale values after successful writes</td>\n<td>Read quorum doesn&#39;t overlap with write quorum</td>\n<td>Test R+W &gt; N and verify overlap</td>\n<td>Adjust quorum sizes or implement read repair</td>\n</tr>\n<tr>\n<td>Writes succeed but data is lost</td>\n<td>Write quorum not actually persistent</td>\n<td>Verify write durability across quorum replicas</td>\n<td>Ensure writes are flushed to disk before acknowledging</td>\n</tr>\n<tr>\n<td>Concurrent writes create permanent conflicts</td>\n<td>Insufficient conflict detection</td>\n<td>Monitor for unresolved conflicts over time</td>\n<td>Implement proper vector clock comparison and resolution</td>\n</tr>\n<tr>\n<td>Inconsistent conflict resolution</td>\n<td>Non-deterministic resolution across nodes</td>\n<td>Compare resolution decisions across replicas</td>\n<td>Use deterministic resolution (timestamp, node ID) as tiebreaker</td>\n</tr>\n</tbody></table>\n<h3 id=\"diagnostic-techniques\">Diagnostic Techniques</h3>\n<p>Effective distributed cache debugging requires systematic approaches that can isolate problems across multiple interacting components. The key is building comprehensive observability into your system and then using structured investigation techniques to narrow down root causes.</p>\n<h4 id=\"systematic-root-cause-analysis\">Systematic Root Cause Analysis</h4>\n<p><strong>The Distributed Systems Investigation Process</strong></p>\n<p>When facing a distributed cache problem, follow this structured approach to avoid jumping to conclusions or missing important evidence:</p>\n<ol>\n<li><p><strong>Establish the Timeline</strong>: Determine when the problem started and correlate with any system changes, deployments, or external events. Distributed systems problems often have delayed manifestations where the root cause occurred minutes or hours before symptoms appeared.</p>\n</li>\n<li><p><strong>Map the Failure Domain</strong>: Identify which nodes, operations, or key ranges are affected. This helps distinguish between localized issues (single node problems, network connectivity) and systemic issues (algorithm bugs, configuration errors).</p>\n</li>\n<li><p><strong>Collect Evidence Systematically</strong>: Gather logs, metrics, and state from ALL relevant nodes, not just the ones showing obvious symptoms. The root cause often lies in nodes that appear to be working normally.</p>\n</li>\n<li><p><strong>Reproduce in Isolation</strong>: Attempt to reproduce the problem in a controlled environment with known inputs. This is particularly important for race conditions and timing-dependent bugs.</p>\n</li>\n<li><p><strong>Test Hypotheses Incrementally</strong>: Form specific, testable hypotheses about the root cause and design experiments to validate or refute them. Avoid making multiple changes simultaneously.</p>\n</li>\n</ol>\n<p><strong>Correlation Analysis Techniques</strong></p>\n<p>Distributed cache problems often manifest as correlations between seemingly unrelated metrics or events. Learning to spot these patterns significantly accelerates diagnosis.</p>\n<table>\n<thead>\n<tr>\n<th>Correlation Pattern</th>\n<th>What It Suggests</th>\n<th>Investigation Steps</th>\n<th>Likely Root Causes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High CPU on specific nodes correlates with specific key patterns</td>\n<td>Hotspot or hash distribution problem</td>\n<td>Analyze key distribution and access patterns</td>\n<td>Insufficient virtual nodes, biased hash function, or popular key concentration</td>\n</tr>\n<tr>\n<td>Memory usage spikes correlate with TTL cleanup timing</td>\n<td>TTL cleanup implementation problem</td>\n<td>Monitor cleanup duration and memory accounting</td>\n<td>Inefficient cleanup algorithm or memory accounting bugs</td>\n</tr>\n<tr>\n<td>Network traffic spikes correlate with membership changes</td>\n<td>Rebalancing or gossip problem</td>\n<td>Analyze message sizes during membership changes</td>\n<td>Inefficient rebalancing or gossip storms</td>\n</tr>\n<tr>\n<td>Error rates correlate with time of day or traffic patterns</td>\n<td>Capacity or race condition problem</td>\n<td>Load test under similar conditions</td>\n<td>Resource exhaustion or concurrency bugs under load</td>\n</tr>\n</tbody></table>\n<h4 id=\"distributed-state-investigation\">Distributed State Investigation</h4>\n<p><strong>Cross-Node State Comparison</strong></p>\n<p>One of the most powerful diagnostic techniques is systematically comparing state across nodes to identify inconsistencies that point to root causes.</p>\n<p>The <code>HashRing</code> state comparison reveals whether nodes have consistent views of cluster membership and key distribution:</p>\n<table>\n<thead>\n<tr>\n<th>State Component</th>\n<th>Comparison Method</th>\n<th>Inconsistency Indicates</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ring topology (sortedKeys)</td>\n<td>Compare hash positions across nodes</td>\n<td>Membership disagreement or ring update bugs</td>\n</tr>\n<tr>\n<td>Node set (nodes map)</td>\n<td>Compare active node lists</td>\n<td>Gossip failures or split-brain scenarios</td>\n</tr>\n<tr>\n<td>Virtual node mapping</td>\n<td>Compare key-to-node assignments</td>\n<td>Hash ring implementation bugs or stale state</td>\n</tr>\n<tr>\n<td>Ring version numbers</td>\n<td>Compare update sequence across nodes</td>\n<td>Out-of-order updates or missing membership changes</td>\n</tr>\n</tbody></table>\n<p>The <code>LRUCache</code> state investigation focuses on cache consistency and memory accounting accuracy:</p>\n<table>\n<thead>\n<tr>\n<th>Cache State Element</th>\n<th>Diagnostic Value</th>\n<th>Investigation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Entry count vs capacity utilization</td>\n<td>Detects memory accounting errors</td>\n<td>Compare tracked memory usage with actual entry sizes</td>\n</tr>\n<tr>\n<td>LRU ordering consistency</td>\n<td>Identifies eviction algorithm bugs</td>\n<td>Verify most recently accessed entries are at front of order</td>\n</tr>\n<tr>\n<td>TTL expiration accuracy</td>\n<td>Finds cleanup implementation issues</td>\n<td>Check for expired entries still present in cache</td>\n</tr>\n<tr>\n<td>Key distribution per node</td>\n<td>Reveals hash ring problems</td>\n<td>Ensure keys are distributed according to hash ring assignments</td>\n</tr>\n</tbody></table>\n<p><strong>Cluster-Wide Consistency Audits</strong></p>\n<p>Implementing audit functions that verify system invariants across the entire cluster helps catch subtle bugs before they cause major problems.</p>\n<p>Critical invariants to verify periodically:</p>\n<ol>\n<li><strong>Hash Ring Consistency</strong>: Every node must have identical ring topology and key-to-node mappings</li>\n<li><strong>Replication Factor Compliance</strong>: Each key must have exactly ReplicationFactor copies across the cluster</li>\n<li><strong>Quorum Availability</strong>: For every key, at least ReadQuorum replicas must be accessible</li>\n<li><strong>Memory Limit Enforcement</strong>: No node should exceed its configured memory limit</li>\n<li><strong>TTL Correctness</strong>: No non-expired entries should be missing, no expired entries should be present</li>\n</ol>\n<h4 id=\"performance-bottleneck-identification\">Performance Bottleneck Identification</h4>\n<p><strong>Distributed Profiling Techniques</strong></p>\n<p>Performance problems in distributed caches often involve subtle interactions between components that only become apparent under specific load patterns or cluster configurations.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Symptom</th>\n<th>Investigation Approach</th>\n<th>Common Root Causes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High latency with low throughput</td>\n<td>Profile critical path operations across all nodes</td>\n<td>Lock contention, inefficient algorithms, or network serialization</td>\n</tr>\n<tr>\n<td>Uneven throughput across nodes</td>\n<td>Compare per-node performance metrics</td>\n<td>Load balancing issues, hardware differences, or hotspots</td>\n</tr>\n<tr>\n<td>Throughput degradation over time</td>\n<td>Monitor resource usage trends</td>\n<td>Memory leaks, unbounded data structures, or cleanup inefficiency</td>\n</tr>\n<tr>\n<td>Intermittent performance spikes</td>\n<td>Correlate spikes with system events</td>\n<td>Garbage collection, periodic cleanup, or rebalancing operations</td>\n</tr>\n</tbody></table>\n<p><strong>Critical Path Analysis</strong></p>\n<p>Identify the most expensive operations in your distributed cache and instrument them thoroughly. The critical paths typically include:</p>\n<ol>\n<li><strong>Key Lookup Path</strong>: Hash calculation → Ring lookup → Node resolution → Network routing</li>\n<li><strong>Cache Operation Path</strong>: Request parsing → Local cache access → Response serialization</li>\n<li><strong>Replication Path</strong>: Write operation → Replica identification → Parallel replication → Quorum waiting</li>\n<li><strong>Failure Recovery Path</strong>: Failure detection → Membership update → Ring rebalancing → Data migration</li>\n</ol>\n<p>For each critical path, measure:</p>\n<ul>\n<li>End-to-end latency distribution (not just averages)</li>\n<li>Resource utilization during operations</li>\n<li>Error rates and retry behavior</li>\n<li>Concurrency levels and lock contention</li>\n</ul>\n<h3 id=\"debugging-tools-and-instrumentation\">Debugging Tools and Instrumentation</h3>\n<p>Effective distributed cache debugging requires comprehensive instrumentation that provides visibility into both individual node behavior and cluster-wide interactions. The key is designing observability that helps you quickly isolate problems across the complexity of multiple interacting components.</p>\n<h4 id=\"logging-strategy-for-distributed-systems\">Logging Strategy for Distributed Systems</h4>\n<p><strong>Structured Logging with Correlation IDs</strong></p>\n<p>Every operation that spans multiple nodes must be trackable through correlation IDs that connect related log entries across the cluster. This enables you to follow a single cache operation from client request through hash ring lookup, network routing, replication, and response.</p>\n<table>\n<thead>\n<tr>\n<th>Log Level</th>\n<th>Use Case</th>\n<th>Example Events</th>\n<th>Required Fields</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ERROR</td>\n<td>Unrecoverable failures requiring immediate attention</td>\n<td>Node unreachable, quorum not achievable, data corruption</td>\n<td>CorrelationID, NodeID, Timestamp, ErrorCode, Context</td>\n</tr>\n<tr>\n<td>WARN</td>\n<td>Recoverable issues that may indicate problems</td>\n<td>Slow responses, retry attempts, partial failures</td>\n<td>CorrelationID, NodeID, Timestamp, Component, Details</td>\n</tr>\n<tr>\n<td>INFO</td>\n<td>Normal operations with business significance</td>\n<td>Cache hits/misses, node joins/leaves, replication events</td>\n<td>CorrelationID, NodeID, Timestamp, Operation, Target, Result</td>\n</tr>\n<tr>\n<td>DEBUG</td>\n<td>Detailed operation traces for troubleshooting</td>\n<td>Hash calculations, ring lookups, message routing</td>\n<td>CorrelationID, NodeID, Timestamp, Function, Parameters, Result</td>\n</tr>\n</tbody></table>\n<p><strong>Operation-Specific Logging Patterns</strong></p>\n<p>Each major distributed cache operation requires specific logging to enable effective debugging:</p>\n<p>Hash Ring Operations:</p>\n<ul>\n<li>Node addition/removal with before/after ring state</li>\n<li>Key lookup with hash calculation and node resolution</li>\n<li>Ring rebalancing with key migration details</li>\n</ul>\n<p>Cache Operations:</p>\n<ul>\n<li>Request received with key, operation type, and routing decision</li>\n<li>Local cache access with hit/miss result and LRU position changes</li>\n<li>TTL expiration events with cleanup statistics</li>\n</ul>\n<p>Replication Operations:</p>\n<ul>\n<li>Write initiation with target replicas and consistency level</li>\n<li>Individual replica responses with success/failure and timing</li>\n<li>Quorum achievement or failure with participating nodes</li>\n</ul>\n<p>Gossip Protocol:</p>\n<ul>\n<li>Gossip rounds with target selection and message content</li>\n<li>Membership state changes with version numbers and propagation</li>\n<li>Convergence detection with final membership views</li>\n</ul>\n<h4 id=\"metrics-and-observability\">Metrics and Observability</h4>\n<p><strong>Node-Level Metrics</strong></p>\n<p>Each cache node must expose metrics that provide insight into its individual performance and health status.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Key Measurements</th>\n<th>Aggregation Method</th>\n<th>Alerting Thresholds</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cache Performance</td>\n<td>Hit rate, miss rate, operation latency</td>\n<td>Per-node averages with cluster-wide distribution</td>\n<td>Hit rate &lt; 70%, p99 latency &gt; 100ms</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Used memory, available memory, eviction rate</td>\n<td>Current values with trend analysis</td>\n<td>Usage &gt; 90%, high eviction rate</td>\n</tr>\n<tr>\n<td>Network Activity</td>\n<td>Request rate, response time, error rate</td>\n<td>Per-node rates with peer-to-peer breakdowns</td>\n<td>Error rate &gt; 1%, response time &gt; 50ms</td>\n</tr>\n<tr>\n<td>Hash Ring Health</td>\n<td>Key distribution variance, rebalancing frequency</td>\n<td>Statistical distribution across nodes</td>\n<td>Load variance &gt; 2x mean, frequent rebalancing</td>\n</tr>\n</tbody></table>\n<p><strong>Cluster-Level Metrics</strong></p>\n<p>Distributed cache health requires metrics that capture cluster-wide behavior and consistency properties.</p>\n<table>\n<thead>\n<tr>\n<th>Cluster Metric</th>\n<th>Calculation Method</th>\n<th>Health Indicator</th>\n<th>Investigation Trigger</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Membership Consistency</td>\n<td>Compare node lists across cluster</td>\n<td>All nodes report identical membership</td>\n<td>Membership disagreement detected</td>\n</tr>\n<tr>\n<td>Data Consistency</td>\n<td>Sample key comparisons across replicas</td>\n<td>Replica values match for sampled keys</td>\n<td>Consistency violations found</td>\n</tr>\n<tr>\n<td>Load Balance Quality</td>\n<td>Standard deviation of per-node load</td>\n<td>Load evenly distributed across nodes</td>\n<td>High load variance</td>\n</tr>\n<tr>\n<td>Availability Percentage</td>\n<td>Successful operations / total operations</td>\n<td>High success rate for all operation types</td>\n<td>Success rate drops below SLA</td>\n</tr>\n</tbody></table>\n<h4 id=\"debug-specific-instrumentation\">Debug-Specific Instrumentation</h4>\n<p><strong>Circuit Breaker Integration</strong></p>\n<p>The <code>HealthChecker</code> component should integrate with circuit breaker patterns to provide debugging information about node health transitions and failure patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Logging Requirements</th>\n<th>Metric Tracking</th>\n<th>Debug Information</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed (Healthy)</td>\n<td>Successful health checks with response times</td>\n<td>Success rate and latency distribution</td>\n<td>Baseline performance characteristics</td>\n</tr>\n<tr>\n<td>Open (Failed)</td>\n<td>Failed health checks with error details</td>\n<td>Failure count and failure types</td>\n<td>Error patterns and recovery conditions</td>\n</tr>\n<tr>\n<td>Half-Open (Testing)</td>\n<td>Recovery attempt results</td>\n<td>Recovery success rate</td>\n<td>Transition timing and success criteria</td>\n</tr>\n</tbody></table>\n<p><strong>Distributed Tracing for Cross-Node Operations</strong></p>\n<p>Implement distributed tracing that follows cache operations across multiple nodes, providing end-to-end visibility into request processing.</p>\n<p>Trace spans should capture:</p>\n<ol>\n<li><strong>Client Request Span</strong>: Initial request parsing and validation</li>\n<li><strong>Hash Ring Lookup Span</strong>: Key hashing and node resolution</li>\n<li><strong>Network Routing Span</strong>: Request forwarding to target node</li>\n<li><strong>Local Cache Access Span</strong>: Cache operation execution</li>\n<li><strong>Replication Spans</strong>: Parallel replica operations (if applicable)</li>\n<li><strong>Response Assembly Span</strong>: Result aggregation and response formation</li>\n</ol>\n<p><strong>State Snapshot Capabilities</strong></p>\n<p>Implement functionality to capture complete system state snapshots for offline analysis and debugging.</p>\n<table>\n<thead>\n<tr>\n<th>Snapshot Component</th>\n<th>Captured Information</th>\n<th>Use Case</th>\n<th>Storage Format</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash Ring State</td>\n<td>Complete ring topology with virtual nodes</td>\n<td>Analyzing key distribution problems</td>\n<td>JSON with sorted ring positions</td>\n</tr>\n<tr>\n<td>Cache Contents</td>\n<td>All cache entries with metadata</td>\n<td>Investigating data consistency issues</td>\n<td>Compressed binary format with entry headers</td>\n</tr>\n<tr>\n<td>Cluster Membership</td>\n<td>Node states with health and version info</td>\n<td>Diagnosing split-brain scenarios</td>\n<td>JSON with timestamp and source node</td>\n</tr>\n<tr>\n<td>Vector Clock State</td>\n<td>All vector clocks for replicated entries</td>\n<td>Resolving consistency conflicts</td>\n<td>JSON with causality relationships</td>\n</tr>\n</tbody></table>\n<h4 id=\"automated-anomaly-detection\">Automated Anomaly Detection</h4>\n<p><strong>Statistical Anomaly Detection</strong></p>\n<p>Implement automated detection of unusual patterns that may indicate bugs or performance issues before they cause visible problems.</p>\n<table>\n<thead>\n<tr>\n<th>Anomaly Type</th>\n<th>Detection Method</th>\n<th>Alert Condition</th>\n<th>Likely Causes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Load Distribution Anomaly</td>\n<td>Monitor per-node load variance</td>\n<td>Variance exceeds 2x historical average</td>\n<td>Hash ring problems, hotspots, or failing nodes</td>\n</tr>\n<tr>\n<td>Performance Regression</td>\n<td>Track operation latency trends</td>\n<td>Latency increases beyond statistical thresholds</td>\n<td>Resource exhaustion, algorithm inefficiency, or external factors</td>\n</tr>\n<tr>\n<td>Consistency Drift</td>\n<td>Compare replica states periodically</td>\n<td>Increasing divergence between replicas</td>\n<td>Network issues, failed repairs, or implementation bugs</td>\n</tr>\n<tr>\n<td>Memory Usage Anomaly</td>\n<td>Monitor memory growth patterns</td>\n<td>Unexpected memory growth or fragmentation</td>\n<td>Memory leaks, accounting errors, or inefficient cleanup</td>\n</tr>\n</tbody></table>\n<p><strong>Proactive Health Monitoring</strong></p>\n<p>The <code>FailureDetector</code> component should implement proactive monitoring that identifies potential issues before they cause operational problems.</p>\n<p>Health monitoring categories:</p>\n<ol>\n<li><strong>Predictive Failure Detection</strong>: Monitor trends that often precede node failures</li>\n<li><strong>Performance Degradation Detection</strong>: Identify gradual performance decline</li>\n<li><strong>Consistency Violation Detection</strong>: Catch replica divergence early</li>\n<li><strong>Resource Exhaustion Prediction</strong>: Alert before resources are completely consumed</li>\n</ol>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical tools and techniques for debugging distributed cache systems, with complete code examples and diagnostic utilities that you can use immediately to identify and resolve common issues.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Debugging Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logging</td>\n<td>Go&#39;s log package with JSON formatting</td>\n<td>Structured logging with zerolog or logrus</td>\n</tr>\n<tr>\n<td>Metrics</td>\n<td>Prometheus client with basic metrics</td>\n<td>Full observability stack with Grafana dashboards</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Simple correlation ID propagation</td>\n<td>OpenTelemetry with Jaeger or Zipkin</td>\n</tr>\n<tr>\n<td>Health Checking</td>\n<td>HTTP ping endpoints</td>\n<td>Comprehensive health checks with circuit breakers</td>\n</tr>\n<tr>\n<td>State Inspection</td>\n<td>JSON dump endpoints</td>\n<td>Interactive debugging API with query capabilities</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Built-in pprof endpoints</td>\n<td>Continuous profiling with specialized tools</td>\n</tr>\n</tbody></table>\n<h4 id=\"debugging-infrastructure-code\">Debugging Infrastructure Code</h4>\n<p><strong>Complete Correlation ID Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debug</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/hex</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> contextKey</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#79B8FF\"> CorrelationIDKey</span><span style=\"color:#B392F0\"> contextKey</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"correlationID\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CorrelationID generates a new correlation ID for tracking operations across nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCorrelationID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bytes </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rand.</span><span style=\"color:#B392F0\">Read</span><span style=\"color:#E1E4E8\">(bytes)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hex.</span><span style=\"color:#B392F0\">EncodeToString</span><span style=\"color:#E1E4E8\">(bytes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WithCorrelationID adds a correlation ID to the context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> WithCorrelationID</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithValue</span><span style=\"color:#E1E4E8\">(ctx, CorrelationIDKey, id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetCorrelationID extracts the correlation ID from context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> GetCorrelationID</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> id, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> ctx.</span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">(CorrelationIDKey).(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StructuredLogger provides correlation-aware logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StructuredLogger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStructuredLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        nodeID: nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger: log.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(os.Stdout, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, log.LstdFlags),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">details</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"level\"</span><span style=\"color:#E1E4E8\">:         </span><span style=\"color:#9ECBFF\">\"ERROR\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"timestamp\"</span><span style=\"color:#E1E4E8\">:     time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Format</span><span style=\"color:#E1E4E8\">(time.RFC3339),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"correlationID\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#B392F0\">GetCorrelationID</span><span style=\"color:#E1E4E8\">(ctx),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"nodeID\"</span><span style=\"color:#E1E4E8\">:        sl.nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"component\"</span><span style=\"color:#E1E4E8\">:     component,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"message\"</span><span style=\"color:#E1E4E8\">:       message,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> details {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    jsonBytes, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sl.logger.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(jsonBytes))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sl </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">details</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"level\"</span><span style=\"color:#E1E4E8\">:         </span><span style=\"color:#9ECBFF\">\"INFO\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"timestamp\"</span><span style=\"color:#E1E4E8\">:     time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Format</span><span style=\"color:#E1E4E8\">(time.RFC3339),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"correlationID\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#B392F0\">GetCorrelationID</span><span style=\"color:#E1E4E8\">(ctx),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"nodeID\"</span><span style=\"color:#E1E4E8\">:        sl.nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"component\"</span><span style=\"color:#E1E4E8\">:     component,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"operation\"</span><span style=\"color:#E1E4E8\">:     operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> details {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entry[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    jsonBytes, _ </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sl.logger.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(jsonBytes))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Complete State Snapshot Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debug</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StateSnapshot captures complete system state for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StateSnapshot</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"nodeID\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HashRingState </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRingSnapshot</span><span style=\"color:#9ECBFF\">      `json:\"hashRing\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CacheState    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheSnapshot</span><span style=\"color:#9ECBFF\">         `json:\"cache\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ClusterState  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterSnapshot</span><span style=\"color:#9ECBFF\">       `json:\"cluster\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HealthState   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthSnapshot</span><span style=\"color:#9ECBFF\">        `json:\"health\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HashRingSnapshot</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    VirtualNodes </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">               `json:\"virtualNodes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ActiveNodes  []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `json:\"activeNodes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RingSize     </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">               `json:\"ringSize\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KeySample    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"keySample\"`</span><span style=\"color:#6A737D\"> // Sample keys with their assigned nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheSnapshot</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalEntries    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"totalEntries\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryUsed      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"memoryUsed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryLimit     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">             `json:\"memoryLimit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HitRate         </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">           `json:\"hitRate\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecentKeys      []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `json:\"recentKeys\"`</span><span style=\"color:#6A737D\">      // Recently accessed keys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExpiredCount    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">               `json:\"expiredCount\"`</span><span style=\"color:#6A737D\">    // Count of expired entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EvictionStats   </span><span style=\"color:#B392F0\">EvictionSnapshot</span><span style=\"color:#9ECBFF\">  `json:\"evictionStats\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> EvictionSnapshot</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalEvictions    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"totalEvictions\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecentEvictions   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">  `json:\"recentEvictions\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EvictionRate      </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"evictionRate\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastEvictionTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"lastEvictionTime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClusterSnapshot</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    KnownNodes       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#9ECBFF\"> `json:\"knownNodes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MembershipVersion </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">              `json:\"membershipVersion\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GossipStats      </span><span style=\"color:#B392F0\">GossipSnapshot</span><span style=\"color:#9ECBFF\">       `json:\"gossipStats\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GossipSnapshot</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MessagesSent     </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"messagesSent\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MessagesReceived </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"messagesReceived\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastGossipTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"lastGossipTime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PeerConnections  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">       `json:\"peerConnections\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthSnapshot</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeHealth      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\"> `json:\"nodeHealth\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastHealthCheck </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">               `json:\"lastHealthCheck\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailureCount    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">                     `json:\"failureCount\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RecoveryCount   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">                     `json:\"recoveryCount\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CaptureSnapshot creates a complete state snapshot for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> CaptureSnapshot</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">hashRing</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cache</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    clusterState</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NodeState</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">healthChecker</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StateSnapshot</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    snapshot </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">StateSnapshot</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        NodeID:    nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">UTC</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Capture hash ring state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    snapshot.HashRingState </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HashRingSnapshot</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        VirtualNodes: hashRing.virtualNodes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ActiveNodes:  </span><span style=\"color:#B392F0\">getActiveNodesList</span><span style=\"color:#E1E4E8\">(hashRing),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RingSize:     </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hashRing.sortedKeys),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        KeySample:    </span><span style=\"color:#B392F0\">captureKeySample</span><span style=\"color:#E1E4E8\">(hashRing),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Capture cache state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    snapshot.CacheState </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CacheSnapshot</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TotalEntries: </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(cache.items)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MemoryUsed:   cache.used,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MemoryLimit:  cache.capacity,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        HitRate:      </span><span style=\"color:#B392F0\">calculateHitRate</span><span style=\"color:#E1E4E8\">(cache),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        RecentKeys:   </span><span style=\"color:#B392F0\">getRecentlyAccessedKeys</span><span style=\"color:#E1E4E8\">(cache),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ExpiredCount: </span><span style=\"color:#B392F0\">countExpiredEntries</span><span style=\"color:#E1E4E8\">(cache),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Capture cluster state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    snapshot.ClusterState </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ClusterSnapshot</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        KnownNodes:       clusterState,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MembershipVersion: </span><span style=\"color:#B392F0\">getCurrentMembershipVersion</span><span style=\"color:#E1E4E8\">(clusterState),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> snapshot</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO: Implement helper functions for state capture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> getActiveNodesList</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">hashRing</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Extract list of active nodes from hash ring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return slice of node IDs currently in the ring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> captureKeySample</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">hashRing</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Generate sample of keys and their node assignments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Useful for verifying hash distribution correctness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> calculateHitRate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">cache</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate cache hit rate from metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Should track hits and misses over recent time window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Complete Health Check Debugging</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debug</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DiagnosticHealthChecker provides detailed health check with debugging info</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DiagnosticHealthChecker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transport </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPTransport</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StructuredLogger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDiagnosticHealthChecker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">transport</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HTTPTransport</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">HealthConfig</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                               logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiagnosticHealthChecker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DiagnosticHealthChecker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        transport: transport,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:    config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:    logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DetailedHealthCheck performs comprehensive health assessment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dhc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiagnosticHealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DetailedHealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                                       nodeID</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DetailedHealthResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlationID </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> GetCorrelationID</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DetailedHealthResult</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        NodeID:        nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Address:       address,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CorrelationID: correlationID,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StartTime:     startTime,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Checks:        </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TCP connectivity check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tcpResult </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> dhc.</span><span style=\"color:#B392F0\">checkTCPConnectivity</span><span style=\"color:#E1E4E8\">(ctx, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.Checks[</span><span style=\"color:#9ECBFF\">\"tcp\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tcpResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // HTTP endpoint check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpResult </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> dhc.</span><span style=\"color:#B392F0\">checkHTTPEndpoint</span><span style=\"color:#E1E4E8\">(ctx, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.Checks[</span><span style=\"color:#9ECBFF\">\"http\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> httpResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Cache functionality check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cacheResult </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> dhc.</span><span style=\"color:#B392F0\">checkCacheFunctionality</span><span style=\"color:#E1E4E8\">(ctx, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.Checks[</span><span style=\"color:#9ECBFF\">\"cache\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cacheResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ring consistency check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ringResult </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> dhc.</span><span style=\"color:#B392F0\">checkRingConsistency</span><span style=\"color:#E1E4E8\">(ctx, address)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.Checks[</span><span style=\"color:#9ECBFF\">\"ring\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ringResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Determine overall health</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.OverallHealth </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dhc.</span><span style=\"color:#B392F0\">determineOverallHealth</span><span style=\"color:#E1E4E8\">(result.Checks)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.CompletedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.TotalDuration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result.CompletedAt.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(startTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Log detailed results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dhc.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"HealthChecker\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"detailed_check_completed\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"nodeID\"</span><span style=\"color:#E1E4E8\">:        nodeID,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"address\"</span><span style=\"color:#E1E4E8\">:       address,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"overallHealth\"</span><span style=\"color:#E1E4E8\">: result.OverallHealth,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"duration\"</span><span style=\"color:#E1E4E8\">:      result.TotalDuration,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"checkResults\"</span><span style=\"color:#E1E4E8\">:  result.Checks,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DetailedHealthResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeID        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                   `json:\"nodeID\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Address       </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                   `json:\"address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CorrelationID </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                   `json:\"correlationID\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartTime     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                `json:\"startTime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompletedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                `json:\"completedAt\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalDuration </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">            `json:\"totalDuration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Checks        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#9ECBFF\">   `json:\"checks\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    OverallHealth </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\">            `json:\"overallHealth\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CheckResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success      </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorMessage </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"errorMessage,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Details      </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}   </span><span style=\"color:#9ECBFF\">`json:\"details,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dhc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiagnosticHealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkTCPConnectivity</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement TCP connectivity check with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Should attempt to establish TCP connection to node address</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Return success/failure with timing and error details</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dhc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiagnosticHealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkHTTPEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement HTTP health endpoint check</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Should call /health endpoint and verify response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Include response time and status code details</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dhc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiagnosticHealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkCacheFunctionality</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement cache operation test</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Should perform test SET/GET operations to verify cache works</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Include operation timing and correctness verification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dhc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiagnosticHealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">checkRingConsistency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement hash ring consistency check</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Should query node's ring state and compare with local state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Detect membership disagreements or stale ring information</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-debugging-skeletons\">Core Logic Debugging Skeletons</h4>\n<p><strong>Hash Ring State Validation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ValidateHashRing performs comprehensive validation of hash ring state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateHashRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">ValidationError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> errors []</span><span style=\"color:#B392F0\">ValidationError</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify ring positions are sorted correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Check that sortedKeys slice is in ascending order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate virtual node distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure each physical node has correct number of virtual nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check ring position to node mapping consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Verify ring map entries match sortedKeys positions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate node availability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Confirm all nodes in ring are marked as active</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Test key assignment consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sample keys should always map to same node for given ring state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"HashRing\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"validation_completed\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"errorCount\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(errors),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"ringSize\"</span><span style=\"color:#E1E4E8\">:   </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hr.sortedKeys),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"nodeCount\"</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hr.nodes),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"component\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorType   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"errorType\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Description </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"description\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Severity    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"severity\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Cache State Consistency Audit</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// AuditCacheConsistency checks LRU cache internal consistency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lru </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LRUCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AuditCacheConsistency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StructuredLogger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheAuditResult</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lru.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> lru.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CacheAuditResult</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        StartTime: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Issues:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">CacheIssue</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify memory accounting accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate actual memory usage and compare with tracked usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check LRU ordering correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure linked list ordering matches access patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate TTL expiration consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Find expired entries that should have been cleaned up</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify map-to-list consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure items map entries correspond to list elements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check capacity enforcement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Verify total usage doesn't exceed configured capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.CompletedAt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result.Duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result.CompletedAt.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(result.StartTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"LRUCache\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"consistency_audit_completed\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"duration\"</span><span style=\"color:#E1E4E8\">:   result.Duration,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"issueCount\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(result.Issues),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"entryCount\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(lru.items),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"memoryUsed\"</span><span style=\"color:#E1E4E8\">: lru.used,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheAuditResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">    `json:\"startTime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompletedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">    `json:\"completedAt\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Issues      []</span><span style=\"color:#B392F0\">CacheIssue</span><span style=\"color:#9ECBFF\"> `json:\"issues\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheIssue</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Severity    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"severity\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Description </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"description\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data        </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1: Hash Ring Debugging Validation</strong></p>\n<p>After implementing your consistent hash ring, run these debugging validations:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test hash ring state consistency</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/hashring</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestRingStateConsistency</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify key distribution quality</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> ring-distribution</span><span style=\"color:#79B8FF\"> --nodes=5</span><span style=\"color:#79B8FF\"> --keys=10000</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test ring rebalancing correctness</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> ring-rebalancing</span><span style=\"color:#79B8FF\"> --add-nodes=3</span><span style=\"color:#79B8FF\"> --remove-nodes=1</span></span></code></pre></div>\n\n<p>Expected output: Ring validation should show even key distribution (variance &lt; 20% of mean), and rebalancing should move minimal keys (approximately 1/N when adding Nth node).</p>\n<p><strong>Milestone 2: Cache Node Debugging Validation</strong></p>\n<p>After implementing your LRU cache node:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test cache consistency under load</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/cache</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestCacheConsistencyUnderLoad</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify memory accounting accuracy</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> cache-audit</span><span style=\"color:#79B8FF\"> --duration=60s</span><span style=\"color:#79B8FF\"> --operations=1000</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test TTL cleanup effectiveness</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> ttl-validation</span><span style=\"color:#79B8FF\"> --expire-ratio=0.3</span><span style=\"color:#79B8FF\"> --cleanup-interval=10s</span></span></code></pre></div>\n\n<p>Expected output: Memory accounting should be accurate within 1%, TTL cleanup should remove expired entries within 2x cleanup interval, and LRU ordering should remain consistent under concurrent access.</p>\n<p><strong>Milestone 3: Cluster Communication Debugging</strong></p>\n<p>After implementing gossip protocol and node discovery:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test membership convergence</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> gossip-convergence</span><span style=\"color:#79B8FF\"> --nodes=5</span><span style=\"color:#79B8FF\"> --partition-time=30s</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify health check accuracy</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/health</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestFailureDetectionAccuracy</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test network partition handling</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> partition-simulation</span><span style=\"color:#79B8FF\"> --groups=2</span><span style=\"color:#79B8FF\"> --duration=60s</span></span></code></pre></div>\n\n<p>Expected output: Membership should converge within 3-5 gossip rounds, health checks should detect failures within 2x check interval, and partitions should be detected without false positives.</p>\n<p><strong>Milestone 4: Replication Debugging Validation</strong></p>\n<p>After implementing replication and consistency:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test consistency under failures</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> consistency-test</span><span style=\"color:#79B8FF\"> --replicas=3</span><span style=\"color:#79B8FF\"> --failures=1</span><span style=\"color:#79B8FF\"> --operations=1000</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify conflict resolution correctness</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/replication</span><span style=\"color:#79B8FF\"> -run</span><span style=\"color:#9ECBFF\"> TestConflictResolution</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test anti-entropy effectiveness</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> ./cmd/debug-tools</span><span style=\"color:#9ECBFF\"> anti-entropy</span><span style=\"color:#79B8FF\"> --inconsistency-ratio=0.1</span><span style=\"color:#79B8FF\"> --repair-time=300s</span></span></code></pre></div>\n\n<p>Expected output: Quorum operations should maintain consistency despite single node failures, conflicts should resolve deterministically, and anti-entropy should repair inconsistencies within configured time window.</p>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section builds on the complete distributed cache system from all previous milestones to explore potential enhancements and how the current design accommodates future growth and feature additions.</p>\n</blockquote>\n<p>The distributed cache system we&#39;ve designed provides a solid foundation with consistent hashing, LRU eviction, cluster communication, and data replication. However, real-world production systems often require additional capabilities for performance, scalability, and advanced features. This section explores potential enhancements and demonstrates how our modular, peer-to-peer architecture naturally accommodates future extensions without requiring fundamental redesign.</p>\n<p>Think of our current distributed cache as a well-designed city infrastructure with roads, utilities, and zoning. Just as a city can expand by adding new neighborhoods, transit systems, and services while leveraging existing infrastructure, our cache can grow with new capabilities that build upon the consistent hash ring, gossip protocol, and replication mechanisms we&#39;ve established.</p>\n<h3 id=\"mental-model-the-evolving-city-infrastructure\">Mental Model: The Evolving City Infrastructure</h3>\n<p>Consider how a successful city grows over time. Initially, it has basic roads, power, and water systems that serve the core population. As the city expands, it adds express highways for faster transportation (performance optimizations), specialized districts like financial centers (advanced features), and regional connections to neighboring cities (scalability enhancements). Importantly, these additions leverage and extend the existing infrastructure rather than replacing it entirely.</p>\n<p>Our distributed cache follows the same pattern. The hash ring provides the &quot;road system&quot; for routing requests, gossip protocol serves as the &quot;communication network&quot; for coordination, and replication mechanisms act as the &quot;utility grid&quot; ensuring reliability. Future extensions build upon these foundations while adding new capabilities.</p>\n<p>The key insight is that we designed our system with extension points from the beginning. The <code>HashRing</code> interface can accommodate new hash functions and placement strategies. The <code>HTTPTransport</code> abstraction allows swapping in more efficient protocols. The <code>ReplicationManager</code> provides hooks for different consistency models. These design choices enable evolutionary growth rather than revolutionary rewrites.</p>\n<h3 id=\"performance-optimizations\">Performance Optimizations</h3>\n<p>Performance optimizations focus on reducing latency and increasing throughput while maintaining the system&#39;s correctness guarantees. These enhancements typically involve algorithmic improvements, better data structures, protocol optimizations, and resource management refinements.</p>\n<h4 id=\"connection-pooling-and-protocol-optimizations\">Connection Pooling and Protocol Optimizations</h4>\n<p>The current <code>HTTPTransport</code> creates new connections for each request, which introduces significant overhead in high-throughput scenarios. A production-ready system would implement sophisticated connection pooling with health monitoring and load balancing across multiple connections per node.</p>\n<p>Enhanced transport layer optimizations include persistent connections with keepalive, request pipelining to send multiple operations over a single connection, and protocol compression to reduce bandwidth usage. More advanced implementations might use HTTP/2 multiplexing or even custom binary protocols optimized for cache operations.</p>\n<p>The connection pool would maintain separate pools for different operation types - quick pools for health checks and gossip messages, and robust pools for data operations. Connection health monitoring would proactively replace failed connections before they impact operations, while circuit breakers would prevent cascading failures when remote nodes become overloaded.</p>\n<blockquote>\n<p><strong>Decision: Enhanced Transport Layer</strong></p>\n<ul>\n<li><strong>Context</strong>: HTTP request/response overhead becomes significant at high throughput, and connection establishment latency impacts operation latency</li>\n<li><strong>Options Considered</strong>: Keep current HTTP transport, upgrade to HTTP/2 with connection pooling, implement custom binary protocol</li>\n<li><strong>Decision</strong>: Implement HTTP/2 with connection pooling as an intermediate step, with hooks for future binary protocol support</li>\n<li><strong>Rationale</strong>: HTTP/2 provides immediate performance benefits while maintaining debugging capabilities and standard tooling support</li>\n<li><strong>Consequences</strong>: Reduces per-operation latency and increases throughput capacity, while maintaining protocol compatibility</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Transport Option</th>\n<th>Latency Impact</th>\n<th>Throughput Gain</th>\n<th>Implementation Complexity</th>\n<th>Debugging Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP/1.1 (current)</td>\n<td>Baseline</td>\n<td>Baseline</td>\n<td>Low</td>\n<td>Excellent</td>\n</tr>\n<tr>\n<td>HTTP/2 + pooling</td>\n<td>30-50% reduction</td>\n<td>200-400% increase</td>\n<td>Medium</td>\n<td>Good</td>\n</tr>\n<tr>\n<td>Custom binary</td>\n<td>50-70% reduction</td>\n<td>300-600% increase</td>\n<td>High</td>\n<td>Limited</td>\n</tr>\n</tbody></table>\n<h4 id=\"memory-layout-and-cache-friendly-data-structures\">Memory Layout and Cache-Friendly Data Structures</h4>\n<p>The current <code>LRUCache</code> uses Go&#39;s built-in map and doubly-linked list, which can suffer from poor cache locality due to pointer chasing. Performance-optimized versions would use arena allocation to keep related data structures in contiguous memory regions, reducing CPU cache misses during traversal operations.</p>\n<p>Advanced memory management includes object pooling to reduce garbage collection pressure, especially important for cache entries that are frequently allocated and deallocated. The system could implement a slab allocator that pre-allocates memory pools for different entry sizes, similar to kernel memory management.</p>\n<p>Memory-mapped files could replace in-memory storage for larger cache entries, providing automatic overflow to disk while maintaining fast access patterns. This hybrid approach allows the cache to exceed RAM capacity while keeping frequently accessed data in memory.</p>\n<p>Lock-free data structures using compare-and-swap operations could replace mutex-protected maps for certain hot paths, particularly for operations like incrementing statistics or updating cache metadata that don&#39;t require complex critical sections.</p>\n<h4 id=\"batch-operations-and-request-coalescing\">Batch Operations and Request Coalescing</h4>\n<p>Individual get/set operations create significant overhead when clients need to access multiple keys. Batch operations allow clients to retrieve or update multiple keys in a single network round-trip, dramatically improving throughput for bulk operations.</p>\n<p>Request coalescing automatically combines multiple concurrent requests for the same key into a single underlying operation, preventing duplicate work when many clients simultaneously request popular data. This is particularly valuable during cache warming or after cache invalidation events.</p>\n<p>The system would buffer outgoing replication requests and send them in batches to reduce network round-trips between nodes. Similarly, gossip messages could accumulate multiple state changes and transmit them together rather than sending individual updates.</p>\n<p>Pipelined operations allow clients to send multiple requests without waiting for responses, with the server processing them in sequence and returning results in order. This improves bandwidth utilization and reduces the impact of network latency on overall throughput.</p>\n<h4 id=\"smart-caching-and-predictive-loading\">Smart Caching and Predictive Loading</h4>\n<p>Machine learning-based access pattern analysis could identify frequently accessed key patterns and proactively load related data. For example, if accessing <code>user:123:profile</code> typically leads to accessing <code>user:123:preferences</code>, the system could speculatively load the preferences data.</p>\n<p>Bloom filters could track recently accessed keys at minimal memory cost, allowing nodes to make intelligent decisions about which data to retain during memory pressure or which keys to prefetch during recovery operations.</p>\n<p>Time-series analysis of access patterns could identify cyclical usage patterns (daily/weekly cycles) and adjust cache warming strategies accordingly. The system might proactively load data before predicted peak usage periods.</p>\n<p>Geographical awareness could optimize data placement based on client location patterns, storing popular regional data closer to the user base. This requires extending the consistent hash ring with location-aware placement policies.</p>\n<h3 id=\"advanced-features\">Advanced Features</h3>\n<p>Advanced features extend the system&#39;s capabilities beyond basic key-value storage, adding new functionality that leverages the distributed infrastructure we&#39;ve built. These features often require new data types, operation semantics, or consistency models.</p>\n<h4 id=\"multi-data-type-support\">Multi-Data Type Support</h4>\n<p>The current system stores arbitrary byte arrays, but production caches often need native support for structured data types like counters, sets, lists, and maps with atomic operations on these structures.</p>\n<p>Counter data types would support atomic increment/decrement operations across replicas, using techniques like PN-counters (increment/decrement counters) that can merge concurrent updates mathematically. This requires extending the conflict resolution system to understand counter semantics rather than treating them as opaque values.</p>\n<p>Set operations would provide atomic add/remove operations with set union for conflict resolution. Lists would support atomic append/prepend operations with position-based conflict resolution. Maps would support atomic field updates with per-field versioning.</p>\n<p>Each data type requires specialized replication logic that understands the semantic meaning of operations rather than treating all updates as simple value replacements. This extends the <code>VectorClock</code> system with operation-specific merge functions.</p>\n<table>\n<thead>\n<tr>\n<th>Data Type</th>\n<th>Atomic Operations</th>\n<th>Conflict Resolution Strategy</th>\n<th>Storage Format</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Counter</td>\n<td>Increment, Decrement, Reset</td>\n<td>PN-counter merge</td>\n<td>Integer + operation log</td>\n</tr>\n<tr>\n<td>Set</td>\n<td>Add, Remove, Contains</td>\n<td>Union of additions</td>\n<td>Bloom filter + tombstones</td>\n</tr>\n<tr>\n<td>List</td>\n<td>Append, Prepend, Insert</td>\n<td>Position-based ordering</td>\n<td>Segmented array</td>\n</tr>\n<tr>\n<td>Map</td>\n<td>Set field, Delete field</td>\n<td>Per-field vector clocks</td>\n<td>Nested hash map</td>\n</tr>\n</tbody></table>\n<h4 id=\"cache-hierarchies-and-multi-level-storage\">Cache Hierarchies and Multi-Level Storage</h4>\n<p>Production systems often implement cache hierarchies with multiple tiers of storage - L1 (in-memory), L2 (SSD), and L3 (distributed network storage). Our hash ring can be extended to support multiple storage tiers with automatic promotion and demotion based on access patterns.</p>\n<p>Hot data promotion would automatically move frequently accessed entries from slower tiers to faster tiers, while cold data demotion would move rarely accessed data to cheaper storage. The system tracks access frequency and recency to make intelligent tiering decisions.</p>\n<p>Write-through and write-behind policies would provide different consistency/performance trade-offs. Write-through ensures data reaches persistent storage before acknowledging the write, while write-behind batches writes for better performance at the cost of potential data loss during failures.</p>\n<p>Cross-datacenter replication could extend the replication system to maintain copies across geographical regions, with different consistency levels for local versus remote replicas. This requires careful handling of network partitions and split-brain scenarios across wide-area networks.</p>\n<h4 id=\"event-streaming-and-change-notifications\">Event Streaming and Change Notifications</h4>\n<p>Client applications often need real-time notifications when cached data changes. An event streaming system would allow clients to subscribe to key patterns and receive notifications when matching keys are modified.</p>\n<p>Change data capture would maintain a log of all cache modifications with timestamps and change types (insert/update/delete). Clients could replay this log to maintain synchronized local caches or trigger business logic based on data changes.</p>\n<p>Event filtering would allow clients to subscribe to specific types of changes or key patterns using regular expressions or prefix matching. The system would efficiently route events only to interested subscribers without flooding all clients.</p>\n<p>Guaranteed delivery would ensure critical events reach their destinations even if clients are temporarily disconnected. This requires persistent event queues with retry logic and dead letter handling for events that cannot be delivered.</p>\n<h4 id=\"advanced-consistency-models\">Advanced Consistency Models</h4>\n<p>The current system implements tunable consistency through read/write quorums, but production systems often need more sophisticated consistency models for different use cases.</p>\n<p>Causal consistency would ensure that operations that are causally related (one operation influenced another) are seen in the same order by all nodes, while allowing concurrent operations to be observed in different orders. This requires extending vector clocks to track causal relationships across client sessions.</p>\n<p>Read-your-writes consistency would guarantee that clients always see their own writes, even if they connect to different nodes. This requires session stickiness or read-through mechanisms that ensure client reads always include their own previous writes.</p>\n<p>Monotonic read consistency would ensure that clients never see older versions of data after seeing newer versions, preventing the confusing experience of data appearing to &quot;go backwards&quot; during network partitions or node failures.</p>\n<p>Timeline consistency would provide ordering guarantees within logical time windows while allowing relaxed consistency across window boundaries. This is useful for applications that need strong consistency for related operations but can tolerate eventual consistency for independent operations.</p>\n<h4 id=\"security-and-authentication\">Security and Authentication</h4>\n<p>Production cache systems require comprehensive security controls including client authentication, data encryption, and access control policies. The gossip protocol and replication mechanisms would need encryption to prevent eavesdropping and tampering.</p>\n<p>Role-based access control would allow different clients to have different permissions - some might only read certain key prefixes, while others can modify any data. This requires extending the protocol to include authentication tokens and authorization checks.</p>\n<p>Data encryption at rest would protect stored cache entries using configurable encryption algorithms. Key management becomes critical, with options for hardware security modules (HSMs) or key management services for enterprise deployments.</p>\n<p>Network security would include TLS for all inter-node communication, certificate-based node authentication to prevent rogue nodes from joining clusters, and audit logging for all security-relevant operations.</p>\n<h3 id=\"scalability-enhancements\">Scalability Enhancements</h3>\n<p>Scalability enhancements focus on supporting larger clusters, higher throughput, and more efficient resource utilization as the system grows. These modifications often require fundamental changes to algorithms and data structures to maintain performance at scale.</p>\n<h4 id=\"hierarchical-hash-rings-and-multi-level-routing\">Hierarchical Hash Rings and Multi-Level Routing</h4>\n<p>As clusters grow beyond hundreds of nodes, the flat hash ring model begins to show scalability limitations. Maintaining consistent view of all nodes becomes expensive, and gossip convergence time grows with cluster size. Hierarchical approaches organize nodes into regions or groups with separate hash rings at each level.</p>\n<p>Regional hash rings would partition nodes geographically or logically, with a top-level ring that routes between regions and regional rings that handle intra-region routing. This reduces gossip overhead since nodes primarily exchange information within their region, while region coordinators handle inter-region communication.</p>\n<p>Consistent hashing trees could replace the simple ring structure with a tree where each level provides increasingly fine-grained routing decisions. This reduces the number of nodes that need to be considered for each routing decision while maintaining the rebalancing properties of consistent hashing.</p>\n<p>Zone-aware replication would ensure replicas are placed across different failure domains (racks, datacenters, regions) to improve fault tolerance. The hash ring placement algorithm would consider node location metadata when selecting replica positions.</p>\n<blockquote>\n<p><strong>Decision: Hierarchical Cluster Organization</strong></p>\n<ul>\n<li><strong>Context</strong>: Single hash ring becomes inefficient for clusters exceeding 1000 nodes due to gossip convergence time and membership management overhead</li>\n<li><strong>Options Considered</strong>: Maintain flat ring with optimized gossip, implement regional rings with coordination layer, use distributed hash trees</li>\n<li><strong>Decision</strong>: Implement regional rings with gossip-based coordination between regions</li>\n<li><strong>Rationale</strong>: Provides linear scalability while preserving existing gossip protocol investments and maintaining operational simplicity</li>\n<li><strong>Consequences</strong>: Enables clusters of 10,000+ nodes while requiring careful region boundary management and cross-region consistency handling</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Scaling Approach</th>\n<th>Max Cluster Size</th>\n<th>Complexity</th>\n<th>Rebalancing Overhead</th>\n<th>Cross-Region Latency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Flat ring (current)</td>\n<td>~500 nodes</td>\n<td>Low</td>\n<td>O(N) gossip</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Regional rings</td>\n<td>~10,000 nodes</td>\n<td>Medium</td>\n<td>O(log N) regions</td>\n<td>1 extra hop</td>\n</tr>\n<tr>\n<td>Hash trees</td>\n<td>~50,000 nodes</td>\n<td>High</td>\n<td>O(log N) levels</td>\n<td>Variable</td>\n</tr>\n</tbody></table>\n<h4 id=\"adaptive-load-balancing\">Adaptive Load Balancing</h4>\n<p>The current consistent hashing with virtual nodes provides good load distribution, but it cannot adapt to non-uniform key popularity or node heterogeneity. Adaptive load balancing monitors actual load patterns and adjusts the hash ring to better balance work across nodes.</p>\n<p>Load-aware virtual node placement would dynamically add or remove virtual nodes based on actual traffic patterns. Overloaded nodes would receive fewer virtual nodes in future ring configurations, while underutilized nodes would receive more virtual nodes to improve load distribution.</p>\n<p>Weighted consistent hashing would assign different numbers of virtual nodes based on node capacity - more powerful nodes would handle proportionally more keys. This requires extending the <code>NodeConfig</code> to include capacity metadata and modifying ring construction algorithms.</p>\n<p>Hot key detection would identify keys that receive disproportionate traffic and potentially replicate them to multiple nodes or cache them at intermediate routing layers. This prevents individual hot keys from overwhelming single nodes.</p>\n<p>Load shedding mechanisms would temporarily reject requests when nodes become overloaded, using techniques like probabilistic rejection or priority-based queuing to maintain service quality for important operations while degrading gracefully under extreme load.</p>\n<h4 id=\"automated-operations-and-self-healing\">Automated Operations and Self-Healing</h4>\n<p>Large-scale systems require automated operations to manage complexity and reduce operational overhead. Self-healing capabilities would automatically detect and correct many common problems without human intervention.</p>\n<p>Automated failure recovery would detect node failures and automatically trigger data recovery processes without waiting for human operators. This includes promoting replicas, rebalancing data, and updating cluster membership in a coordinated fashion.</p>\n<p>Capacity management would monitor resource utilization trends and automatically trigger cluster expansion when approaching capacity limits. This might involve automatically provisioning new nodes in cloud environments or alerting operators when manual intervention is needed.</p>\n<p>Performance anomaly detection would identify nodes or keys that are performing significantly differently from baseline patterns. This could indicate hardware problems, configuration issues, or attack patterns that require attention.</p>\n<p>Self-tuning parameters would automatically adjust configuration values like TTL defaults, replication factors, and timeout values based on observed performance characteristics and failure patterns. This reduces the operational burden of manual tuning while maintaining optimal performance.</p>\n<h4 id=\"elastic-scaling-and-cloud-integration\">Elastic Scaling and Cloud Integration</h4>\n<p>Modern deployments often require elastic scaling capabilities that can quickly add or remove capacity based on demand patterns. This requires extending our architecture to work well with container orchestration systems and cloud auto-scaling groups.</p>\n<p>Rapid node onboarding would minimize the time required for new nodes to become fully functional cluster members. This includes optimized bootstrap processes, parallel data streaming, and incremental integration rather than bulk data transfers.</p>\n<p>Graceful node removal would coordinate the shutdown process to ensure data availability is maintained throughout the process. This requires careful ordering of operations - stop accepting new data, transfer existing data to other nodes, update hash ring, then shutdown.</p>\n<p>Container-aware deployment would optimize for containerized environments with features like health check endpoints for orchestration systems, graceful handling of container restarts, and configuration through environment variables rather than files.</p>\n<p>Auto-scaling integration would provide APIs and metrics that external auto-scaling systems can use to make intelligent scaling decisions. This includes predictive metrics that indicate when scaling events should be triggered before problems occur.</p>\n<h3 id=\"resource-efficiency-and-green-computing\">Resource Efficiency and Green Computing</h3>\n<p>As distributed systems scale, resource efficiency becomes increasingly important both for cost management and environmental responsibility. These optimizations focus on reducing power consumption, memory usage, and network bandwidth while maintaining performance.</p>\n<h4 id=\"energy-aware-scheduling\">Energy-Aware Scheduling</h4>\n<p>Power-efficient node selection would consider energy consumption when choosing which nodes to route requests to. During low-traffic periods, the system could concentrate load on fewer nodes and allow others to enter power-saving modes.</p>\n<p>Dynamic voltage and frequency scaling integration would coordinate with hardware power management to adjust CPU performance based on current load. Cache nodes could signal their current performance requirements to the operating system for optimal power/performance trade-offs.</p>\n<p>Temperature-aware load balancing would consider datacenter cooling costs when distributing load. During hot weather or cooling system maintenance, the system could shift load away from nodes in thermally constrained areas.</p>\n<h4 id=\"memory-and-storage-optimization\">Memory and Storage Optimization</h4>\n<p>Compressed storage would automatically compress cache entries using fast algorithms optimized for cache workloads. The system would balance compression ratio against CPU overhead, potentially using different compression levels for different data types or access patterns.</p>\n<p>Deduplication could identify and eliminate duplicate cache entries, particularly valuable for applications that store similar data with different keys. This requires careful handling to ensure that deduplication doesn&#39;t violate consistency guarantees.</p>\n<p>Memory-mapped storage could provide automatic overflow to disk for infrequently accessed data while keeping hot data in RAM. This hybrid approach allows cache sizes to exceed physical memory while maintaining performance for active data.</p>\n<p>Smart prefetching based on access patterns could reduce storage I/O by predicting which data will be needed and loading it proactively. This is particularly valuable for cache warming after restarts or during recovery operations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The future extensions described above build upon the architectural foundations we&#39;ve established throughout the previous milestones. This section provides concrete guidance for implementing these enhancements while maintaining system stability and correctness.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Enhancement Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transport Protocol</td>\n<td>HTTP/2 with connection pooling</td>\n<td>gRPC with streaming</td>\n</tr>\n<tr>\n<td>Data Types</td>\n<td>JSON encoding with type hints</td>\n<td>Protocol Buffers with schema evolution</td>\n</tr>\n<tr>\n<td>Storage Tiers</td>\n<td>File-based overflow storage</td>\n<td>RocksDB or similar LSM-tree</td>\n</tr>\n<tr>\n<td>Event Streaming</td>\n<td>HTTP Server-Sent Events</td>\n<td>Apache Kafka or NATS</td>\n</tr>\n<tr>\n<td>Security</td>\n<td>TLS + token authentication</td>\n<td>mTLS + RBAC with JWT</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus metrics</td>\n<td>OpenTelemetry with tracing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-extension-architecture\">Recommended Extension Architecture</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/server/main.go                    ← enhanced with plugin loading\n  internal/\n    extensions/                         ← extension interface definitions\n      transport/                        ← pluggable transport layer\n        http2_transport.go\n        grpc_transport.go\n      datatypes/                        ← advanced data type support\n        counter.go\n        set.go\n        list.go\n      storage/                          ← multi-tier storage\n        memory_tier.go\n        disk_tier.go\n        tiering_manager.go\n      security/                         ← authentication and authorization\n        auth_manager.go\n        rbac.go\n        encryption.go\n    optimization/                       ← performance enhancements\n      connection_pool.go\n      batch_processor.go\n      load_balancer.go\n    scaling/                           ← scalability enhancements\n      hierarchical_ring.go\n      regional_gossip.go\n      auto_scaler.go</code></pre></div>\n\n<h4 id=\"extension-interface-design\">Extension Interface Design</h4>\n<p>The key to successful extensibility is designing clean interfaces that new features can implement without modifying core system components. Here are the essential extension points:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// TransportLayer interface allows plugging in different communication protocols</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TransportLayer</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    SendMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">address</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    StartServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#B392F0\"> RequestHandler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DataType interface enables support for structured data with semantic operations  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DataType</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Type</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Serialize</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Deserialize</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ApplyOperation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">op</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">DataType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    MergeConflicts</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">others</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">DataType</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">DataType</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageTier interface allows implementing multi-level storage hierarchies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageTier</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">entry</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Delete</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    EvictToLowerTier</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">keys</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">lowerTier</span><span style=\"color:#B392F0\"> StorageTier</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    PromoteFromLowerTier</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">keys</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">lowerTier</span><span style=\"color:#B392F0\"> StorageTier</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Usage</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">StorageUsage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-extension-infrastructure\">Core Extension Infrastructure</h4>\n<p>This infrastructure starter code provides the foundation for loading and managing extensions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> extensions</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">plugin</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExtensionManager coordinates loading and lifecycle of system extensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ExtensionManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    transports    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">TransportLayer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataTypes     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">DataTypeFactory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageTiers  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">StorageTierFactory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    securityMgrs  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">SecurityManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewExtensionManager creates extension manager with built-in implementations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewExtensionManager</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExtensionManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ExtensionManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        transports:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">TransportLayer</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        dataTypes:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">DataTypeFactory</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storageTiers: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">StorageTierFactory</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        securityMgrs: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">SecurityManager</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadExtension dynamically loads extension from plugin file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">em </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExtensionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadExtension</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">pluginPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">extensionType</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load plugin using plugin.Open(pluginPath)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Look up extension factory function by type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call factory function to create extension instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Register extension in appropriate registry map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Initialize extension with current system configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetTransport returns transport implementation by name</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">em </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExtensionManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetTransport</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">TransportLayer</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock for thread safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Look up transport in registry map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return transport or error if not found</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"performance-optimization-skeleton\">Performance Optimization Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> optimization</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ConnectionPool manages reusable connections to remote nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ConnectionPool</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pools     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">nodePool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PoolConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PoolMetrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PoolConfig contains connection pool tuning parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PoolConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxConnectionsPerNode </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxIdleTime          </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    HealthCheckInterval  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ConnectTimeout       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewConnectionPool creates optimized connection pool for inter-node communication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewConnectionPool</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">PoolConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConnectionPool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ConnectionPool</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pools:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">nodePool</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:  config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">PoolMetrics</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetConnection returns healthy connection to specified node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ConnectionPool</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetConnection</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeAddress</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">Connection</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get or create pool for target node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Try to get healthy connection from pool</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If no healthy connections available, create new connection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test connection health before returning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update pool metrics and connection tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BatchProcessor aggregates operations for efficient processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BatchProcessor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batchSize     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flushInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batches       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">operationBatch</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processor     </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh        </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddOperation adds operation to appropriate batch for processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">bp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BatchProcessor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddOperation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">op</span><span style=\"color:#B392F0\"> Operation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get or create batch for target node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add operation to batch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check if batch is ready for processing (size or time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If ready, submit batch to processor asynchronously</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Reset batch for future operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"scalability-enhancement-framework\">Scalability Enhancement Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> scaling</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegionalRing extends hash ring with hierarchical organization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RegionalRing</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    region       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localRing    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    regionRings  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    coordinator  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RegionCoordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RegionalConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegionalConfig contains parameters for hierarchical organization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RegionalConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Region              </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CoordinatorNodes    []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CrossRegionLatency  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RegionHealthTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReplicationAcrossRegions </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRegionalRing creates hierarchical hash ring for large-scale deployments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRegionalRing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">region</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RegionalConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RegionalRing</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RegionalRing</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        region:      region,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        localRing:   </span><span style=\"color:#B392F0\">NewHashRing</span><span style=\"color:#E1E4E8\">(config.VirtualNodes),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        regionRings: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HashRing</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:      config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RouteOperation determines target node for operation, potentially cross-region</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">rr </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RegionalRing</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RouteOperation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">region</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Hash key to determine target region</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If target region is local, use local ring for node selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If target region is remote, select appropriate region coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return node ID, region, and any routing errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update routing metrics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AutoScaler monitors system metrics and triggers scaling operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AutoScaler</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricsCollector </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsCollector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scalingPolicy    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScalingPolicy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cloudProvider    </span><span style=\"color:#B392F0\">CloudProvider</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clusterManager   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scaleUpCooldown  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scaleDownCooldown </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScalingPolicy defines rules for when to scale up or down</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScalingPolicy</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TargetCPUPercent    </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TargetMemoryPercent </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MinNodes           </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxNodes           </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScaleUpThreshold   </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScaleDownThreshold </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MonitorAndScale continuously monitors metrics and triggers scaling when needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">as </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AutoScaler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">MonitorAndScale</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Collect current cluster metrics (CPU, memory, request rate)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Evaluate metrics against scaling policy thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If scale up needed and not in cooldown, provision new nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If scale down possible and not in cooldown, remove excess nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update cooldown timers and scaling history</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"extension-point-integration\">Extension Point Integration</h4>\n<p>The key to successful extension integration is providing clean hooks where new functionality can integrate with existing system components:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ExtensionHooks defines points where extensions can integrate with core system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ExtensionHooks</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PreOperation  []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PostOperation []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">Operation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PreReplication []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">ReplicationRequest</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PostReplication []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">ReplicationRequest</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeJoin      []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NodeLeave     []</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterHook adds extension callback at specified integration point</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">eh </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExtensionHooks</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterHook</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">hookType</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">callback</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate callback function signature matches hook requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add callback to appropriate hook slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Sort hooks by priority if priority system is implemented</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteHooks runs all registered callbacks for specified hook point</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">eh </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExtensionHooks</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteHooks</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">hookType</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">args</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get hooks for specified type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute each hook in order, passing provided arguments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle hook errors according to error policy (fail-fast vs continue)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Aggregate and return any errors encountered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints-for-extensions\">Milestone Checkpoints for Extensions</h4>\n<p>After implementing each category of extensions, validate the integration:</p>\n<p><strong>Performance Optimization Checkpoint:</strong></p>\n<ul>\n<li>Measure baseline performance with current HTTP transport</li>\n<li>Implement connection pooling and measure throughput improvement</li>\n<li>Add batch processing and verify reduced network round-trips</li>\n<li>Expected: 2-5x throughput increase with 30-50% latency reduction</li>\n</ul>\n<p><strong>Advanced Features Checkpoint:</strong></p>\n<ul>\n<li>Implement counter data type with increment operations</li>\n<li>Test concurrent increments with conflict resolution</li>\n<li>Verify counter values converge correctly across replicas</li>\n<li>Expected: Atomic counter operations work correctly under concurrent load</li>\n</ul>\n<p><strong>Scalability Enhancement Checkpoint:</strong></p>\n<ul>\n<li>Deploy hierarchical rings with multiple regions</li>\n<li>Test cross-region operations and replication</li>\n<li>Verify graceful handling of region failures</li>\n<li>Expected: System scales beyond 1000 nodes while maintaining sub-100ms latency</li>\n</ul>\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Go-Specific Optimizations:</strong></p>\n<ul>\n<li>Use <code>sync.Pool</code> for object pooling to reduce GC pressure</li>\n<li>Implement custom memory allocators using <code>unsafe</code> package for critical paths</li>\n<li>Use <code>go:linkname</code> to access internal runtime functions for advanced optimizations</li>\n<li>Consider <code>cgo</code> integration for high-performance compression libraries</li>\n</ul>\n<p><strong>Connection Management:</strong></p>\n<ul>\n<li>Use <code>net/http.Transport</code> with custom <code>DialContext</code> for connection pooling</li>\n<li>Implement connection health checks with <code>net.Conn.SetDeadline</code></li>\n<li>Use <code>context.WithTimeout</code> consistently for all network operations</li>\n<li>Monitor connection metrics with <code>http.Transport.RegisterProtocol</code></li>\n</ul>\n<p><strong>Memory Efficiency:</strong></p>\n<ul>\n<li>Use memory-mapped files with <code>golang.org/x/exp/mmap</code> for large cache entries</li>\n<li>Implement arena allocation patterns to reduce pointer chasing</li>\n<li>Use <code>sync.RWMutex</code> judiciously - prefer lock-free algorithms where possible</li>\n<li>Profile with <code>go tool pprof</code> to identify memory hotspots</li>\n</ul>\n<h4 id=\"common-extension-pitfalls\">Common Extension Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Breaking Core Functionality</strong>\nExtensions often inadvertently break core system guarantees by modifying shared state without proper coordination. Always use the provided extension hooks rather than directly modifying core components. Test extensions in isolation before integration.</p>\n<p>⚠️ <strong>Pitfall: Performance Regression</strong>\nPerformance optimizations can actually reduce performance if they introduce excessive overhead for small workloads. Always benchmark extensions against realistic workloads and provide configuration options to disable optimizations when they&#39;re not beneficial.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Extension State</strong>\nExtensions that maintain their own state must handle node failures and cluster membership changes correctly. Use the same replication and consistency mechanisms as core components, or ensure extension state can be reconstructed from authoritative sources.</p>\n<p>⚠️ <strong>Pitfall: Resource Leaks</strong>\nExtensions that create resources (connections, goroutines, file handles) must implement proper cleanup in their shutdown methods. Register cleanup functions with the extension manager and ensure they&#39;re called during system shutdown.</p>\n<p>⚠️ <strong>Pitfall: Configuration Complexity</strong>\nAdvanced features often introduce many configuration options, making the system difficult to operate. Provide sensible defaults, validate configuration combinations, and implement configuration templates for common deployment patterns.</p>\n<p>The extension architecture we&#39;ve designed enables the distributed cache to evolve and grow while maintaining the reliability and consistency guarantees established in the core milestones. Each extension builds upon the solid foundation of consistent hashing, replication, and cluster communication, demonstrating the value of designing for extensibility from the beginning.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides terminology and definitions supporting all milestones, serving as a comprehensive reference for technical terms, data structures, and concepts used throughout the distributed cache implementation.</p>\n</blockquote>\n<p>This glossary defines all technical terms, data structures, interfaces, and domain-specific vocabulary used throughout this distributed cache design document. Think of this as your technical dictionary - a reference you can return to whenever you encounter an unfamiliar term or need to clarify the exact meaning of a concept in the context of our distributed cache system.</p>\n<p>The terms are organized into logical categories to help you understand not just individual definitions, but also how concepts relate to each other within the broader system architecture. Each definition includes not only what the term means, but also why it matters in the context of building a distributed cache and how it connects to other concepts in the system.</p>\n<h3 id=\"core-architecture-terms\">Core Architecture Terms</h3>\n<p>The fundamental concepts that define how our distributed cache system is structured and operates.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>consistent hashing</strong></td>\n<td>A hash ring algorithm that minimizes key redistribution when cluster membership changes. Unlike simple modulo hashing, consistent hashing ensures that when a node joins or leaves, only a small fraction of keys need to be moved to different nodes, typically 1/N where N is the number of nodes.</td>\n</tr>\n<tr>\n<td><strong>hash ring</strong></td>\n<td>A circular key space where both cache keys and cluster nodes are positioned using hash values. The ring topology allows us to determine key ownership by finding the first node clockwise from a key&#39;s hash position.</td>\n</tr>\n<tr>\n<td><strong>virtual nodes</strong></td>\n<td>Multiple hash ring positions assigned to each physical node to achieve better load distribution. Each physical node appears at many positions on the ring, reducing the likelihood that one node receives disproportionate traffic from popular key ranges.</td>\n</tr>\n<tr>\n<td><strong>peer-to-peer cluster design</strong></td>\n<td>A distributed architecture where all nodes have equal responsibility and no single point of failure exists. Every node can handle client requests, participate in gossip communication, and store cache data without requiring dedicated coordinator nodes.</td>\n</tr>\n<tr>\n<td><strong>symmetric responsibility</strong></td>\n<td>A design principle where no nodes have special coordinator roles or elevated privileges. This eliminates single points of failure and simplifies cluster management since any node can perform any necessary cluster operation.</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-structures-and-types\">Data Structures and Types</h3>\n<p>The core data types that represent cache entries, cluster state, and system configuration throughout the distributed cache.</p>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th>Fields</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HashRing</code></td>\n<td>virtualNodes int, ring map[uint32]string, sortedKeys []uint32, nodes map[string]bool</td>\n<td>Implements consistent hashing for key distribution across cluster nodes</td>\n</tr>\n<tr>\n<td><code>LRUCache</code></td>\n<td>mutex sync.RWMutex, capacity int64, used int64, items map[string]*list.Element, order *list.List</td>\n<td>Thread-safe LRU cache with memory accounting and eviction</td>\n</tr>\n<tr>\n<td><code>CacheEntry</code></td>\n<td>Key string, Value []byte, ExpiresAt time.Time, Size int64</td>\n<td>Individual cache item with expiration and memory tracking</td>\n</tr>\n<tr>\n<td><code>NodeConfig</code></td>\n<td>NodeID string, ListenAddress string, AdvertiseAddr string, JoinAddresses []string, MaxMemoryMB int, VirtualNodes int, ReplicationFactor int, HealthCheckInterval time.Duration, GossipInterval time.Duration, RequestTimeout time.Duration</td>\n<td>Complete configuration for a cache node including networking, capacity, and timing parameters</td>\n</tr>\n<tr>\n<td><code>GossipMessage</code></td>\n<td>NodeStates map[string]NodeState, Version uint64</td>\n<td>Cluster membership information exchanged between nodes</td>\n</tr>\n<tr>\n<td><code>NodeState</code></td>\n<td>NodeID string, Address string, Status string, LastSeen time.Time, Version uint64</td>\n<td>Current status and metadata for a single cluster node</td>\n</tr>\n<tr>\n<td><code>ReplicatedEntry</code></td>\n<td>*CacheEntry, VectorClock *VectorClock, ReplicationInfo map[string]ReplicaInfo</td>\n<td>Cache entry extended with replication metadata and causality tracking</td>\n</tr>\n<tr>\n<td><code>VectorClock</code></td>\n<td>Clock map[string]uint64, mutex sync.RWMutex</td>\n<td>Logical timestamp tracking causality relationships between distributed operations</td>\n</tr>\n<tr>\n<td><code>HealthResult</code></td>\n<td>NodeID string, Timestamp time.Time, TCPHealthy bool, HTTPHealthy bool, CacheHealthy bool, ResponseTime time.Duration, ErrorDetails string, OverallHealth HealthStatus</td>\n<td>Comprehensive health assessment results for a cluster node</td>\n</tr>\n</tbody></table>\n<h3 id=\"cache-operations-and-memory-management\">Cache Operations and Memory Management</h3>\n<p>Terms related to cache functionality, memory management, and data lifecycle within individual nodes.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>LRU eviction</strong></td>\n<td>Memory management policy that removes least recently used entries when the cache reaches its memory limit. This policy assumes temporal locality - recently accessed data is more likely to be accessed again.</td>\n</tr>\n<tr>\n<td><strong>TTL expiration</strong></td>\n<td>Automatic removal of cache entries after their configured time-to-live expires. TTL provides a mechanism for cache invalidation based on data freshness requirements.</td>\n</tr>\n<tr>\n<td><strong>memory accounting</strong></td>\n<td>Precise tracking of memory usage by cache entries to enforce capacity limits. Includes not just the data size but also metadata overhead for keys, timestamps, and data structures.</td>\n</tr>\n<tr>\n<td><strong>lazy expiration</strong></td>\n<td>TTL checking strategy that verifies expiration during access operations rather than using background cleanup. This reduces CPU overhead but may allow expired entries to temporarily consume memory.</td>\n</tr>\n<tr>\n<td><strong>hotspot</strong></td>\n<td>A cluster node that receives disproportionately high load due to popular keys hashing to its ring positions. Virtual nodes help mitigate hotspots by distributing each node&#39;s key ranges.</td>\n</tr>\n<tr>\n<td><strong>temporal locality</strong></td>\n<td>The principle that recently accessed cache data is likely to be accessed again soon. LRU eviction policy leverages this principle to keep frequently used data in memory.</td>\n</tr>\n</tbody></table>\n<h3 id=\"distributed-systems-and-networking\">Distributed Systems and Networking</h3>\n<p>Concepts related to how nodes communicate, discover each other, and maintain cluster membership.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>gossip protocol</strong></td>\n<td>Peer-to-peer communication pattern where nodes periodically exchange cluster state information with random neighbors. Gossip ensures eventual convergence of cluster membership without requiring centralized coordination.</td>\n</tr>\n<tr>\n<td><strong>node discovery</strong></td>\n<td>The mechanism by which new nodes find and join an existing cluster. Typically involves bootstrap addresses that new nodes contact to learn about current cluster members.</td>\n</tr>\n<tr>\n<td><strong>failure mode analysis</strong></td>\n<td>Systematic categorization of potential system failures and their impacts on cluster operations. Helps design appropriate detection and recovery mechanisms for different types of failures.</td>\n</tr>\n<tr>\n<td><strong>graduated health checks</strong></td>\n<td>Multi-level health verification with escalating depth and frequency. Starts with lightweight checks like TCP connectivity and progresses to application-level cache operations for suspected failures.</td>\n</tr>\n<tr>\n<td><strong>split-brain scenario</strong></td>\n<td>A network partition where multiple isolated sub-clusters continue operating independently. Can lead to data inconsistency if not properly handled through quorum requirements.</td>\n</tr>\n<tr>\n<td><strong>cascading failure</strong></td>\n<td>Progressive failure spread where the failure of one component increases load on remaining components, potentially causing them to fail as well.</td>\n</tr>\n<tr>\n<td><strong>network partition</strong></td>\n<td>Communication failure that splits the cluster into isolated groups that cannot communicate with each other. Also called a &quot;network split&quot; or &quot;partition event&quot;.</td>\n</tr>\n</tbody></table>\n<h3 id=\"replication-and-consistency\">Replication and Consistency</h3>\n<p>Terms related to data replication, consistency guarantees, and conflict resolution in the distributed cache.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>replication factor</strong></td>\n<td>The number of copies stored for each cache entry across different cluster nodes. Higher replication factors provide better fault tolerance but consume more storage and network bandwidth.</td>\n</tr>\n<tr>\n<td><strong>quorum</strong></td>\n<td>The minimum number of replicas that must participate in a read or write operation to ensure consistency. Common configurations include majority quorums (N/2 + 1) for strong consistency.</td>\n</tr>\n<tr>\n<td><strong>vector clocks</strong></td>\n<td>Logical timestamps that track causality relationships between distributed operations. Each node maintains a clock that advances when it performs operations, enabling detection of concurrent vs. causally-related updates.</td>\n</tr>\n<tr>\n<td><strong>conflict resolution</strong></td>\n<td>Algorithms for handling divergent values between replicas when concurrent updates create inconsistencies. Strategies include last-writer-wins, vector clock comparison, and application-specific merge functions.</td>\n</tr>\n<tr>\n<td><strong>anti-entropy</strong></td>\n<td>Background processes that detect and repair data inconsistencies between replicas. Typically involves periodic comparison of replica contents and synchronization of differences.</td>\n</tr>\n<tr>\n<td><strong>hinted handoff</strong></td>\n<td>Temporary storage mechanism for writes to unavailable replica nodes. When a replica is down, hints are stored locally and delivered when the node recovers, ensuring eventual consistency.</td>\n</tr>\n<tr>\n<td><strong>read repair</strong></td>\n<td>Synchronous repair process that detects and fixes stale replicas during read operations. When replicas return different values, the most recent version is propagated to stale replicas.</td>\n</tr>\n<tr>\n<td><strong>eventual consistency</strong></td>\n<td>Consistency guarantee that all replicas will converge to the same value if no new updates occur. Provides high availability but allows temporary inconsistencies during network partitions or failures.</td>\n</tr>\n<tr>\n<td><strong>successor-based placement</strong></td>\n<td>Replica placement strategy that stores copies on the next N nodes clockwise from the primary node on the hash ring. Provides predictable replica locations and load distribution.</td>\n</tr>\n<tr>\n<td><strong>sloppy quorums</strong></td>\n<td>Quorum operations that can use alternative nodes when preferred replicas are unavailable. Maintains availability during failures but may require additional anti-entropy work.</td>\n</tr>\n</tbody></table>\n<h3 id=\"health-monitoring-and-failure-detection\">Health Monitoring and Failure Detection</h3>\n<p>Concepts related to monitoring node health, detecting failures, and maintaining cluster availability.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>phi accrual failure detector</strong></td>\n<td>Probabilistic failure detection algorithm that accumulates suspicion levels based on heartbeat patterns. Provides tunable sensitivity to network delays versus false failure detection.</td>\n</tr>\n<tr>\n<td><strong>circuit breaker</strong></td>\n<td>Protection mechanism that prevents operations likely to fail from being attempted. Opens when failure rate exceeds threshold, reducing load on struggling nodes and preventing cascading failures.</td>\n</tr>\n<tr>\n<td><strong>false positive detection</strong></td>\n<td>Incorrectly identifying healthy nodes as failed due to network delays or temporary overload. False positives can cause unnecessary cluster rebalancing and reduced availability.</td>\n</tr>\n<tr>\n<td><strong>byzantine behavior</strong></td>\n<td>Incorrect node behavior due to software bugs, data corruption, or misconfiguration. Distinguished from simple crash failures because the node continues responding but with incorrect behavior.</td>\n</tr>\n<tr>\n<td><strong>quorum loss</strong></td>\n<td>Situation where insufficient healthy replicas are available to meet consistency requirements. May require reducing consistency guarantees or rejecting operations until more nodes recover.</td>\n</tr>\n</tbody></table>\n<h3 id=\"request-processing-and-message-types\">Request Processing and Message Types</h3>\n<p>The different types of operations and messages that flow through the distributed cache system.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>MessageTypeGet</strong></td>\n<td>Cache retrieval operation message type used for inter-node communication when routing requests to the node responsible for a key.</td>\n</tr>\n<tr>\n<td><strong>MessageTypeSet</strong></td>\n<td>Cache storage operation message type used when storing or updating key-value pairs across replica nodes.</td>\n</tr>\n<tr>\n<td><strong>MessageTypeDelete</strong></td>\n<td>Cache removal operation message type used when removing keys from the cache across all replicas.</td>\n</tr>\n<tr>\n<td><strong>MessageTypeReplicate</strong></td>\n<td>Replication coordination message type used for managing replica consistency during write operations.</td>\n</tr>\n<tr>\n<td><strong>MessageTypeGossip</strong></td>\n<td>Cluster membership gossip message type used for propagating node state information throughout the cluster.</td>\n</tr>\n</tbody></table>\n<h3 id=\"health-status-enumeration\">Health Status Enumeration</h3>\n<p>The different states a cluster node can be in from a health monitoring perspective.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HealthStatusHealthy</strong></td>\n<td>Node is fully operational and responding to all types of health checks within acceptable time limits.</td>\n</tr>\n<tr>\n<td><strong>HealthStatusSlow</strong></td>\n<td>Node is responding but with degraded performance, possibly due to high load or resource contention.</td>\n</tr>\n<tr>\n<td><strong>HealthStatusSuspected</strong></td>\n<td>Node health is questionable based on accumulated evidence like missed heartbeats or failed health checks.</td>\n</tr>\n<tr>\n<td><strong>HealthStatusFailed</strong></td>\n<td>Node is confirmed failed and unavailable, requiring cluster rebalancing and replica recovery procedures.</td>\n</tr>\n</tbody></table>\n<h3 id=\"testing-and-debugging\">Testing and Debugging</h3>\n<p>Terms related to testing distributed cache behavior and diagnosing problems in the system.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>chaos testing</strong></td>\n<td>Testing methodology that intentionally introduces failures like network partitions, node crashes, and resource exhaustion to validate system resilience and recovery capabilities.</td>\n</tr>\n<tr>\n<td><strong>correlation ID</strong></td>\n<td>Unique identifier that tracks individual operations across multiple nodes and system components. Essential for debugging distributed operations and understanding request flows.</td>\n</tr>\n<tr>\n<td><strong>structured logging</strong></td>\n<td>Logging approach that uses consistent, machine-readable formats with searchable fields rather than free-form text. Enables automated log analysis and correlation across distributed components.</td>\n</tr>\n<tr>\n<td><strong>state snapshot</strong></td>\n<td>Complete capture of system state at a specific point in time for debugging purposes. Includes hash ring state, cache contents, node membership, and replication status.</td>\n</tr>\n<tr>\n<td><strong>anomaly detection</strong></td>\n<td>Automated identification of unusual patterns in system behavior that may indicate bugs, performance problems, or security issues.</td>\n</tr>\n<tr>\n<td><strong>distributed tracing</strong></td>\n<td>Technique for tracking requests as they flow through multiple system components, providing visibility into performance bottlenecks and failure points.</td>\n</tr>\n<tr>\n<td><strong>consistency audit</strong></td>\n<td>Systematic verification that system invariants hold across the entire cluster, such as proper replica placement and hash ring consistency.</td>\n</tr>\n<tr>\n<td><strong>failure domain</strong></td>\n<td>The scope of system components affected by a particular type of failure. Understanding failure domains helps design appropriate isolation and recovery mechanisms.</td>\n</tr>\n<tr>\n<td><strong>root cause analysis</strong></td>\n<td>Systematic investigation methodology to identify the underlying causes of system problems rather than just addressing symptoms.</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-and-optimization\">Performance and Optimization</h3>\n<p>Terms related to system performance, scalability, and optimization techniques.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>connection pooling</strong></td>\n<td>Technique for reusing network connections across multiple requests to reduce connection establishment overhead and improve throughput.</td>\n</tr>\n<tr>\n<td><strong>batch operations</strong></td>\n<td>Combining multiple individual cache operations into single network requests to reduce network overhead and improve efficiency.</td>\n</tr>\n<tr>\n<td><strong>hierarchical hash rings</strong></td>\n<td>Multi-level hash ring organization for managing very large clusters by organizing nodes into regions or data centers with local and global rings.</td>\n</tr>\n<tr>\n<td><strong>adaptive load balancing</strong></td>\n<td>Dynamic adjustment of load distribution based on actual traffic patterns and node performance rather than static hash-based assignment.</td>\n</tr>\n<tr>\n<td><strong>elastic scaling</strong></td>\n<td>Automatic addition or removal of cluster capacity based on demand, typically integrated with cloud infrastructure auto-scaling capabilities.</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-features-and-extensions\">Advanced Features and Extensions</h3>\n<p>Concepts related to extending the basic distributed cache with additional capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>multi-data type support</strong></td>\n<td>Native support for structured data types beyond simple byte arrays, such as lists, sets, and sorted sets with type-specific operations.</td>\n</tr>\n<tr>\n<td><strong>cache hierarchies</strong></td>\n<td>Multi-tier storage systems with different performance and cost characteristics, such as in-memory, SSD, and network-attached storage tiers.</td>\n</tr>\n<tr>\n<td><strong>extension hooks</strong></td>\n<td>Integration points where new functionality can be added to core system components without modifying the base implementation.</td>\n</tr>\n<tr>\n<td><strong>energy-aware scheduling</strong></td>\n<td>Optimization technique that considers power consumption when making load distribution and scaling decisions, important for large-scale deployments.</td>\n</tr>\n</tbody></table>\n<h3 id=\"interface-and-method-definitions\">Interface and Method Definitions</h3>\n<p>Key interfaces and their methods that define the contracts between system components.</p>\n<table>\n<thead>\n<tr>\n<th>Interface/Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HashFunction.Hash</code></td>\n<td>data string</td>\n<td>uint32</td>\n<td>Computes hash value for consistent ring positioning</td>\n</tr>\n<tr>\n<td><code>HashFunction.Name</code></td>\n<td>none</td>\n<td>string</td>\n<td>Returns identifier for the hash function algorithm</td>\n</tr>\n<tr>\n<td><code>TransportLayer.SendMessage</code></td>\n<td>ctx context.Context, address string, message interface{}</td>\n<td>[]byte, error</td>\n<td>Sends message to remote node and returns response</td>\n</tr>\n<tr>\n<td><code>TransportLayer.HealthCheck</code></td>\n<td>ctx context.Context, address string</td>\n<td>error</td>\n<td>Checks if remote node is responding</td>\n</tr>\n<tr>\n<td><code>DataType.Serialize</code></td>\n<td>none</td>\n<td>[]byte, error</td>\n<td>Converts data type instance to byte representation</td>\n</tr>\n<tr>\n<td><code>DataType.Deserialize</code></td>\n<td>data []byte</td>\n<td>error</td>\n<td>Reconstructs data type instance from bytes</td>\n</tr>\n<tr>\n<td><code>StorageTier.Get</code></td>\n<td>key string</td>\n<td>[]byte, bool, error</td>\n<td>Retrieves value from storage tier</td>\n</tr>\n<tr>\n<td><code>StorageTier.Set</code></td>\n<td>key string, value []byte</td>\n<td>error</td>\n<td>Stores value in storage tier</td>\n</tr>\n</tbody></table>\n<h3 id=\"algorithm-and-protocol-components\">Algorithm and Protocol Components</h3>\n<p>Specific algorithms and protocols used throughout the distributed cache implementation.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Purpose</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PhiAccrualDetector</code></td>\n<td>Probabilistic failure detection</td>\n<td>Accumulates suspicion based on heartbeat intervals, tunable sensitivity</td>\n</tr>\n<tr>\n<td><code>RegionCoordinator</code></td>\n<td>Multi-region cluster management</td>\n<td>Coordinates between regional hash rings for geographic distribution</td>\n</tr>\n<tr>\n<td><code>AntiEntropyScheduler</code></td>\n<td>Background consistency repair</td>\n<td>Schedules and coordinates replica synchronization processes</td>\n</tr>\n<tr>\n<td><code>CircuitBreaker</code></td>\n<td>Failure isolation and recovery</td>\n<td>Prevents cascading failures by stopping requests to failing nodes</td>\n</tr>\n<tr>\n<td><code>BatchProcessor</code></td>\n<td>Operation batching optimization</td>\n<td>Combines multiple operations for efficient network utilization</td>\n</tr>\n</tbody></table>\n<h3 id=\"configuration-and-operational-parameters\">Configuration and Operational Parameters</h3>\n<p>Important configuration options and their purposes within the system.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>VirtualNodes</code></td>\n<td>int</td>\n<td>Number of virtual positions per physical node on hash ring</td>\n</tr>\n<tr>\n<td><code>ReplicationFactor</code></td>\n<td>int</td>\n<td>Number of replica copies for each cache entry</td>\n</tr>\n<tr>\n<td><code>HealthCheckInterval</code></td>\n<td>time.Duration</td>\n<td>Frequency of node health verification checks</td>\n</tr>\n<tr>\n<td><code>GossipInterval</code></td>\n<td>time.Duration</td>\n<td>Frequency of cluster state propagation messages</td>\n</tr>\n<tr>\n<td><code>RequestTimeout</code></td>\n<td>time.Duration</td>\n<td>Maximum time to wait for inter-node operation responses</td>\n</tr>\n<tr>\n<td><code>MaxMemoryMB</code></td>\n<td>int</td>\n<td>Memory limit for cache storage on each node</td>\n</tr>\n<tr>\n<td><code>ConflictResolution</code></td>\n<td>string</td>\n<td>Strategy for resolving replica conflicts (vector-clock, last-write-wins)</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The following guidance helps bridge the gap between these conceptual definitions and actual implementation.</p>\n<h4 id=\"technology-stack-recommendations\">Technology Stack Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hash Function</td>\n<td>SHA-1 (crypto/sha1)</td>\n<td>Consistent hash with jump hash</td>\n</tr>\n<tr>\n<td>Transport</td>\n<td>HTTP REST + JSON (net/http)</td>\n<td>gRPC with Protocol Buffers</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON (encoding/json)</td>\n<td>MessagePack or Protocol Buffers</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard library (log)</td>\n<td>Structured logging (logrus, zap)</td>\n</tr>\n<tr>\n<td>Health Checks</td>\n<td>HTTP ping endpoints</td>\n<td>Multi-layer graduated checks</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON files</td>\n<td>YAML with environment variable override</td>\n</tr>\n</tbody></table>\n<h4 id=\"key-implementation-patterns\">Key Implementation Patterns</h4>\n<p>When implementing the distributed cache, several patterns emerge consistently across components:</p>\n<p><strong>State Machine Pattern</strong>: Many components (nodes, health status, recovery processes) follow state machine patterns. Always define states explicitly, document all valid transitions, and include timeout handling for stuck states.</p>\n<p><strong>Asynchronous Processing</strong>: Background tasks like gossip propagation, health checking, and anti-entropy repair should use separate goroutines with proper lifecycle management and cancellation support.</p>\n<p><strong>Graceful Degradation</strong>: Components should continue operating with reduced functionality rather than failing completely. For example, during network partitions, serve reads from available replicas even if writes must be rejected.</p>\n<p><strong>Idempotent Operations</strong>: Design operations to be safely retryable. Cache SET operations are naturally idempotent, but DELETE and replication operations need careful design to handle duplicate messages.</p>\n<h4 id=\"common-implementation-mistakes\">Common Implementation Mistakes</h4>\n<p>⚠️ <strong>Mistake: Inadequate Virtual Node Count</strong>\nUsing too few virtual nodes (&lt; 100 per physical node) results in uneven load distribution. This causes hotspots where some nodes handle much more traffic than others.</p>\n<p>⚠️ <strong>Mistake: Blocking TTL Cleanup</strong>\nRunning expiration cleanup synchronously during GET operations causes latency spikes. Always use background cleanup with lazy checking during access.</p>\n<p>⚠️ <strong>Mistake: Gossip Message Size Growth</strong>\nIncluding complete node state in every gossip message causes exponential growth in large clusters. Use incremental updates and version vectors to minimize message sizes.</p>\n<p>⚠️ <strong>Mistake: Synchronous Replication Blocking</strong>\nWaiting for all replicas to confirm writes causes availability problems during partial failures. Use asynchronous replication with configurable consistency levels.</p>\n<p>⚠️ <strong>Mistake: Memory Accounting Errors</strong>\nForgetting to account for metadata overhead (keys, timestamps, data structures) leads to memory limit violations. Always include complete memory footprint in calculations.</p>\n<h4 id=\"debugging-and-monitoring-essentials\">Debugging and Monitoring Essentials</h4>\n<p>Effective debugging of distributed cache systems requires comprehensive observability:</p>\n<p><strong>Correlation Tracking</strong>: Every operation should have a correlation ID that follows it across all nodes involved in processing. This enables end-to-end request tracing.</p>\n<p><strong>Structured Metrics</strong>: Collect metrics at multiple levels - per-operation, per-node, and cluster-wide. Key metrics include hit rates, latency distributions, memory usage, and replication lag.</p>\n<p><strong>State Introspection</strong>: Provide debugging endpoints that expose internal state like hash ring contents, replica placement, gossip view, and health check results.</p>\n<p><strong>Failure Simulation</strong>: Build chaos testing capabilities directly into the system for validating resilience under various failure conditions.</p>\n<p>The terminology and concepts defined in this glossary form the foundation for understanding and implementing a robust distributed cache system. Each term connects to broader system design principles and implementation patterns that ensure the cache operates reliably at scale.</p>\n","toc":[{"level":1,"text":"Distributed Cache: Design Document","id":"distributed-cache-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: The Library System Analogy","id":"mental-model-the-library-system-analogy"},{"level":3,"text":"Existing Approaches Comparison","id":"existing-approaches-comparison"},{"level":3,"text":"Why Distributed Caching is Hard","id":"why-distributed-caching-is-hard"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":3,"text":"Non-Functional Goals","id":"non-functional-goals"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"A. Technology Recommendations","id":"a-technology-recommendations"},{"level":4,"text":"B. Recommended File/Module Structure","id":"b-recommended-filemodule-structure"},{"level":4,"text":"C. Infrastructure Starter Code","id":"c-infrastructure-starter-code"},{"level":4,"text":"D. Core Logic Skeleton Code","id":"d-core-logic-skeleton-code"},{"level":4,"text":"E. Language-Specific Hints","id":"e-language-specific-hints"},{"level":4,"text":"F. Milestone Checkpoints","id":"f-milestone-checkpoints"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Overview","id":"component-overview"},{"level":4,"text":"Cache Node Component","id":"cache-node-component"},{"level":4,"text":"Hash Ring Component","id":"hash-ring-component"},{"level":4,"text":"Gossip Protocol Component","id":"gossip-protocol-component"},{"level":4,"text":"Transport Layer Component","id":"transport-layer-component"},{"level":4,"text":"Client Router Component","id":"client-router-component"},{"level":3,"text":"Recommended Module Structure","id":"recommended-module-structure"},{"level":3,"text":"Communication Patterns","id":"communication-patterns"},{"level":4,"text":"Client-Server Request-Response Pattern","id":"client-server-request-response-pattern"},{"level":4,"text":"Peer-to-Peer Gossip Pattern","id":"peer-to-peer-gossip-pattern"},{"level":4,"text":"Replication Coordination Pattern","id":"replication-coordination-pattern"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Starter Infrastructure Code","id":"starter-infrastructure-code"},{"level":4,"text":"Core Component Skeletons","id":"core-component-skeletons"},{"level":4,"text":"Development Checkpoints","id":"development-checkpoints"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Mental Model: The Digital Filing System","id":"mental-model-the-digital-filing-system"},{"level":3,"text":"Core Data Types","id":"core-data-types"},{"level":4,"text":"Configuration and Node Identity","id":"configuration-and-node-identity"},{"level":4,"text":"Hash Ring Structure","id":"hash-ring-structure"},{"level":4,"text":"Cache Storage Structure","id":"cache-storage-structure"},{"level":3,"text":"Cache Entry Structure","id":"cache-entry-structure"},{"level":4,"text":"Entry Data and Metadata","id":"entry-data-and-metadata"},{"level":4,"text":"TTL and Expiration Handling","id":"ttl-and-expiration-handling"},{"level":4,"text":"Memory Size Calculation","id":"memory-size-calculation"},{"level":3,"text":"Cluster State Information","id":"cluster-state-information"},{"level":4,"text":"Node State Tracking","id":"node-state-tracking"},{"level":4,"text":"Node Status State Machine","id":"node-status-state-machine"},{"level":4,"text":"Cluster Membership and Gossip","id":"cluster-membership-and-gossip"},{"level":4,"text":"Message Transport and Communication","id":"message-transport-and-communication"},{"level":4,"text":"Cache Operation Messages","id":"cache-operation-messages"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Consistent Hash Ring Design","id":"consistent-hash-ring-design"},{"level":3,"text":"Mental Model: The Circular Table","id":"mental-model-the-circular-table"},{"level":3,"text":"Virtual Nodes Strategy","id":"virtual-nodes-strategy"},{"level":3,"text":"Ring Operations and Algorithms","id":"ring-operations-and-algorithms"},{"level":4,"text":"Key Lookup Algorithm","id":"key-lookup-algorithm"},{"level":4,"text":"Node Addition Algorithm","id":"node-addition-algorithm"},{"level":4,"text":"Node Removal Algorithm","id":"node-removal-algorithm"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Cache Node Implementation","id":"cache-node-implementation"},{"level":3,"text":"Mental Model: The Smart Filing Cabinet","id":"mental-model-the-smart-filing-cabinet"},{"level":3,"text":"LRU Eviction Algorithm","id":"lru-eviction-algorithm"},{"level":4,"text":"LRU Data Structure Design","id":"lru-data-structure-design"},{"level":4,"text":"LRU Operation Algorithms","id":"lru-operation-algorithms"},{"level":4,"text":"Memory Accounting Strategy","id":"memory-accounting-strategy"},{"level":4,"text":"Concurrency and Lock Strategy","id":"concurrency-and-lock-strategy"},{"level":3,"text":"TTL and Expiration Handling","id":"ttl-and-expiration-handling"},{"level":4,"text":"TTL Storage and Representation","id":"ttl-storage-and-representation"},{"level":4,"text":"Expiration Checking During Access","id":"expiration-checking-during-access"},{"level":4,"text":"Background Expiration Cleanup","id":"background-expiration-cleanup"},{"level":4,"text":"TTL and Memory Management Interaction","id":"ttl-and-memory-management-interaction"},{"level":3,"text":"Memory Accounting and Limits","id":"memory-accounting-and-limits"},{"level":4,"text":"Memory Tracking Granularity","id":"memory-tracking-granularity"},{"level":4,"text":"Capacity Enforcement Strategy","id":"capacity-enforcement-strategy"},{"level":4,"text":"Memory Limit Configuration","id":"memory-limit-configuration"},{"level":4,"text":"Memory Usage Monitoring","id":"memory-usage-monitoring"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":4,"text":"Data Structure Selection","id":"data-structure-selection"},{"level":4,"text":"Concurrency Model","id":"concurrency-model"},{"level":4,"text":"Memory Accounting Precision","id":"memory-accounting-precision"},{"level":4,"text":"TTL Implementation Strategy","id":"ttl-implementation-strategy"},{"level":4,"text":"Error Handling Philosophy","id":"error-handling-philosophy"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":4,"text":"⚠️ Pitfall: Inconsistent Memory Accounting","id":"-pitfall-inconsistent-memory-accounting"},{"level":4,"text":"⚠️ Pitfall: Lock Upgrade Race Conditions","id":"-pitfall-lock-upgrade-race-conditions"},{"level":4,"text":"⚠️ Pitfall: TTL Cleanup During Iteration","id":"-pitfall-ttl-cleanup-during-iteration"},{"level":4,"text":"⚠️ Pitfall: Zero Time Comparison Errors","id":"-pitfall-zero-time-comparison-errors"},{"level":4,"text":"⚠️ Pitfall: Linked List Element Reuse","id":"-pitfall-linked-list-element-reuse"},{"level":4,"text":"⚠️ Pitfall: Partial Entry Updates","id":"-pitfall-partial-entry-updates"},{"level":4,"text":"⚠️ Pitfall: Cleanup Process Blocking Operations","id":"-pitfall-cleanup-process-blocking-operations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Cache Implementation Skeleton","id":"core-cache-implementation-skeleton"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Cluster Communication and Discovery","id":"cluster-communication-and-discovery"},{"level":3,"text":"Mental Model: The Neighborhood Watch","id":"mental-model-the-neighborhood-watch"},{"level":3,"text":"Node Discovery Mechanism","id":"node-discovery-mechanism"},{"level":3,"text":"Health Checking and Failure Detection","id":"health-checking-and-failure-detection"},{"level":3,"text":"Request Routing Logic","id":"request-routing-logic"},{"level":3,"text":"Gossip Protocol Implementation","id":"gossip-protocol-implementation"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeletons","id":"core-logic-skeletons"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Replication and Consistency","id":"replication-and-consistency"},{"level":3,"text":"Mental Model: The Document Copying Office","id":"mental-model-the-document-copying-office"},{"level":3,"text":"Replication Factor and Placement","id":"replication-factor-and-placement"},{"level":3,"text":"Quorum-Based Operations","id":"quorum-based-operations"},{"level":3,"text":"Conflict Resolution Strategies","id":"conflict-resolution-strategies"},{"level":3,"text":"Anti-Entropy and Repair","id":"anti-entropy-and-repair"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Message Formats and Protocols","id":"message-formats-and-protocols"},{"level":4,"text":"Transport Layer Design","id":"transport-layer-design"},{"level":4,"text":"Core Message Structure","id":"core-message-structure"},{"level":4,"text":"Cache Operation Messages","id":"cache-operation-messages"},{"level":4,"text":"Cluster Membership Messages","id":"cluster-membership-messages"},{"level":4,"text":"Replication Protocol Messages","id":"replication-protocol-messages"},{"level":3,"text":"Operation Sequence Flows","id":"operation-sequence-flows"},{"level":4,"text":"Client Request Processing","id":"client-request-processing"},{"level":4,"text":"Get Operation Flow","id":"get-operation-flow"},{"level":4,"text":"Set Operation Flow","id":"set-operation-flow"},{"level":4,"text":"Delete Operation Flow","id":"delete-operation-flow"},{"level":3,"text":"Replication and Consistency Flows","id":"replication-and-consistency-flows"},{"level":4,"text":"Write Replication Coordination","id":"write-replication-coordination"},{"level":4,"text":"Read Repair and Consistency","id":"read-repair-and-consistency"},{"level":4,"text":"Anti-Entropy and Background Repair","id":"anti-entropy-and-background-repair"},{"level":4,"text":"Hinted Handoff Delivery","id":"hinted-handoff-delivery"},{"level":4,"text":"Consistency Level Implementation","id":"consistency-level-implementation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: The Emergency Response Network","id":"mental-model-the-emergency-response-network"},{"level":3,"text":"Failure Mode Analysis","id":"failure-mode-analysis"},{"level":4,"text":"Node-Level Failures","id":"node-level-failures"},{"level":4,"text":"Network-Level Failures","id":"network-level-failures"},{"level":4,"text":"Cluster-Level Failures","id":"cluster-level-failures"},{"level":3,"text":"Failure Detection Strategies","id":"failure-detection-strategies"},{"level":4,"text":"Direct Health Probing","id":"direct-health-probing"},{"level":4,"text":"Gossip-Based Failure Detection","id":"gossip-based-failure-detection"},{"level":4,"text":"Operation-Based Detection","id":"operation-based-detection"},{"level":3,"text":"Recovery and Mitigation","id":"recovery-and-mitigation"},{"level":4,"text":"Automated Recovery Procedures","id":"automated-recovery-procedures"},{"level":4,"text":"Manual Intervention Procedures","id":"manual-intervention-procedures"},{"level":4,"text":"Circuit Breaker Patterns","id":"circuit-breaker-patterns"},{"level":3,"text":"Edge Cases and Corner Scenarios","id":"edge-cases-and-corner-scenarios"},{"level":4,"text":"Timing-Related Edge Cases","id":"timing-related-edge-cases"},{"level":4,"text":"Consistency Edge Cases","id":"consistency-edge-cases"},{"level":4,"text":"Resource Exhaustion Edge Cases","id":"resource-exhaustion-edge-cases"},{"level":4,"text":"Byzantine Failure Scenarios","id":"byzantine-failure-scenarios"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Health Checking Infrastructure","id":"health-checking-infrastructure"},{"level":4,"text":"Failure Detection Coordinator","id":"failure-detection-coordinator"},{"level":4,"text":"Recovery Coordinator","id":"recovery-coordinator"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: The Quality Control Factory","id":"mental-model-the-quality-control-factory"},{"level":3,"text":"Unit Testing Approach","id":"unit-testing-approach"},{"level":4,"text":"Hash Ring Component Testing","id":"hash-ring-component-testing"},{"level":4,"text":"LRU Cache Component Testing","id":"lru-cache-component-testing"},{"level":4,"text":"Gossip Protocol Component Testing","id":"gossip-protocol-component-testing"},{"level":4,"text":"Replication Manager Component Testing","id":"replication-manager-component-testing"},{"level":3,"text":"Integration Testing","id":"integration-testing"},{"level":4,"text":"Cross-Component Data Flow Testing","id":"cross-component-data-flow-testing"},{"level":4,"text":"Cluster State Consistency Testing","id":"cluster-state-consistency-testing"},{"level":4,"text":"End-to-End Operation Validation","id":"end-to-end-operation-validation"},{"level":3,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Milestone 1: Consistent Hash Ring Validation","id":"milestone-1-consistent-hash-ring-validation"},{"level":4,"text":"Milestone 2: Cache Node Validation","id":"milestone-2-cache-node-validation"},{"level":4,"text":"Milestone 3: Cluster Communication Validation","id":"milestone-3-cluster-communication-validation"},{"level":4,"text":"Milestone 4: Replication and Consistency Validation","id":"milestone-4-replication-and-consistency-validation"},{"level":3,"text":"Chaos and Failure Testing","id":"chaos-and-failure-testing"},{"level":4,"text":"Network Partition Testing","id":"network-partition-testing"},{"level":4,"text":"Cascading Failure Testing","id":"cascading-failure-testing"},{"level":4,"text":"Byzantine Failure Testing","id":"byzantine-failure-testing"},{"level":4,"text":"Performance Degradation Testing","id":"performance-degradation-testing"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Testing Structure","id":"recommended-testing-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: The Detective&#39;s Investigation Kit","id":"mental-model-the-detective39s-investigation-kit"},{"level":3,"text":"Common Bug Patterns","id":"common-bug-patterns"},{"level":4,"text":"Hash Ring Distribution Problems","id":"hash-ring-distribution-problems"},{"level":4,"text":"LRU Cache Implementation Bugs","id":"lru-cache-implementation-bugs"},{"level":4,"text":"Network Communication Failures","id":"network-communication-failures"},{"level":4,"text":"Replication Consistency Bugs","id":"replication-consistency-bugs"},{"level":3,"text":"Diagnostic Techniques","id":"diagnostic-techniques"},{"level":4,"text":"Systematic Root Cause Analysis","id":"systematic-root-cause-analysis"},{"level":4,"text":"Distributed State Investigation","id":"distributed-state-investigation"},{"level":4,"text":"Performance Bottleneck Identification","id":"performance-bottleneck-identification"},{"level":3,"text":"Debugging Tools and Instrumentation","id":"debugging-tools-and-instrumentation"},{"level":4,"text":"Logging Strategy for Distributed Systems","id":"logging-strategy-for-distributed-systems"},{"level":4,"text":"Metrics and Observability","id":"metrics-and-observability"},{"level":4,"text":"Debug-Specific Instrumentation","id":"debug-specific-instrumentation"},{"level":4,"text":"Automated Anomaly Detection","id":"automated-anomaly-detection"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Debugging Infrastructure Code","id":"debugging-infrastructure-code"},{"level":4,"text":"Core Logic Debugging Skeletons","id":"core-logic-debugging-skeletons"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Mental Model: The Evolving City Infrastructure","id":"mental-model-the-evolving-city-infrastructure"},{"level":3,"text":"Performance Optimizations","id":"performance-optimizations"},{"level":4,"text":"Connection Pooling and Protocol Optimizations","id":"connection-pooling-and-protocol-optimizations"},{"level":4,"text":"Memory Layout and Cache-Friendly Data Structures","id":"memory-layout-and-cache-friendly-data-structures"},{"level":4,"text":"Batch Operations and Request Coalescing","id":"batch-operations-and-request-coalescing"},{"level":4,"text":"Smart Caching and Predictive Loading","id":"smart-caching-and-predictive-loading"},{"level":3,"text":"Advanced Features","id":"advanced-features"},{"level":4,"text":"Multi-Data Type Support","id":"multi-data-type-support"},{"level":4,"text":"Cache Hierarchies and Multi-Level Storage","id":"cache-hierarchies-and-multi-level-storage"},{"level":4,"text":"Event Streaming and Change Notifications","id":"event-streaming-and-change-notifications"},{"level":4,"text":"Advanced Consistency Models","id":"advanced-consistency-models"},{"level":4,"text":"Security and Authentication","id":"security-and-authentication"},{"level":3,"text":"Scalability Enhancements","id":"scalability-enhancements"},{"level":4,"text":"Hierarchical Hash Rings and Multi-Level Routing","id":"hierarchical-hash-rings-and-multi-level-routing"},{"level":4,"text":"Adaptive Load Balancing","id":"adaptive-load-balancing"},{"level":4,"text":"Automated Operations and Self-Healing","id":"automated-operations-and-self-healing"},{"level":4,"text":"Elastic Scaling and Cloud Integration","id":"elastic-scaling-and-cloud-integration"},{"level":3,"text":"Resource Efficiency and Green Computing","id":"resource-efficiency-and-green-computing"},{"level":4,"text":"Energy-Aware Scheduling","id":"energy-aware-scheduling"},{"level":4,"text":"Memory and Storage Optimization","id":"memory-and-storage-optimization"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Extension Architecture","id":"recommended-extension-architecture"},{"level":4,"text":"Extension Interface Design","id":"extension-interface-design"},{"level":4,"text":"Core Extension Infrastructure","id":"core-extension-infrastructure"},{"level":4,"text":"Performance Optimization Skeleton","id":"performance-optimization-skeleton"},{"level":4,"text":"Scalability Enhancement Framework","id":"scalability-enhancement-framework"},{"level":4,"text":"Extension Point Integration","id":"extension-point-integration"},{"level":4,"text":"Milestone Checkpoints for Extensions","id":"milestone-checkpoints-for-extensions"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Common Extension Pitfalls","id":"common-extension-pitfalls"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Core Architecture Terms","id":"core-architecture-terms"},{"level":3,"text":"Data Structures and Types","id":"data-structures-and-types"},{"level":3,"text":"Cache Operations and Memory Management","id":"cache-operations-and-memory-management"},{"level":3,"text":"Distributed Systems and Networking","id":"distributed-systems-and-networking"},{"level":3,"text":"Replication and Consistency","id":"replication-and-consistency"},{"level":3,"text":"Health Monitoring and Failure Detection","id":"health-monitoring-and-failure-detection"},{"level":3,"text":"Request Processing and Message Types","id":"request-processing-and-message-types"},{"level":3,"text":"Health Status Enumeration","id":"health-status-enumeration"},{"level":3,"text":"Testing and Debugging","id":"testing-and-debugging"},{"level":3,"text":"Performance and Optimization","id":"performance-and-optimization"},{"level":3,"text":"Advanced Features and Extensions","id":"advanced-features-and-extensions"},{"level":3,"text":"Interface and Method Definitions","id":"interface-and-method-definitions"},{"level":3,"text":"Algorithm and Protocol Components","id":"algorithm-and-protocol-components"},{"level":3,"text":"Configuration and Operational Parameters","id":"configuration-and-operational-parameters"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Stack Recommendations","id":"technology-stack-recommendations"},{"level":4,"text":"Key Implementation Patterns","id":"key-implementation-patterns"},{"level":4,"text":"Common Implementation Mistakes","id":"common-implementation-mistakes"},{"level":4,"text":"Debugging and Monitoring Essentials","id":"debugging-and-monitoring-essentials"}],"title":"Distributed Cache: Design Document","markdown":"# Distributed Cache: Design Document\n\n\n## Overview\n\nA distributed cache system using consistent hashing to store and retrieve key-value data across multiple nodes, providing high availability and horizontal scalability. The key architectural challenge is maintaining data consistency and availability while handling node failures and dynamic cluster membership.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** This section provides foundational understanding for all milestones, with particular relevance to Milestone 1 (Consistent Hash Ring) and Milestone 3 (Cluster Communication).\n\nBuilding a distributed cache system presents fundamental challenges that don't exist in single-node caches. When data must be distributed across multiple machines, we face complex problems around data placement, consistency, fault tolerance, and dynamic cluster membership. This section explores why these challenges are inherent to distributed systems and compares different approaches to solving them.\n\nThe core tension in distributed caching lies between **scalability** and **complexity**. A single-node cache is simple and fast but limited by the memory and processing power of one machine. A distributed cache can scale horizontally by adding more nodes, but introduces coordination overhead, network latency, and partial failure scenarios that fundamentally change the system's behavior.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\nThe distributed cache we're building must handle several critical scenarios that don't exist in single-node systems. When a client requests data, the system must quickly determine which node should handle that request. When nodes are added or removed from the cluster, existing data must be redistributed efficiently. When nodes fail, the system must detect failures quickly and route around them while maintaining data availability. These requirements create a complex web of interdependent design decisions.\n\n### Mental Model: The Library System Analogy\n\nTo understand distributed caching intuitively, imagine a city library system that has grown from a single building to multiple branches across the city. This analogy helps explain the core concepts and challenges of distributed caching in familiar terms.\n\n**Single Library (Single-Node Cache)**: Initially, the city had one central library where all books were stored. Patrons knew exactly where to go, and librarians could quickly locate any book because they knew the entire collection. The system was simple and efficient, but as the city grew, the single library became overcrowded. Patrons faced long lines, parking became impossible, and the building couldn't physically hold more books.\n\n**Multiple Branches (Distributed Cache)**: To solve the capacity problem, the city built multiple library branches. Now patrons have several nearby locations, reducing travel time and crowding. However, this creates new challenges: Which branch should stock which books? How do patrons know which branch has the book they want? What happens when a branch temporarily closes for maintenance?\n\n**Book Distribution Strategy (Data Partitioning)**: The library system needs a strategy for deciding which books go to which branch. They could use simple rules like \"Branch 1 gets books A-F, Branch 2 gets books G-M, Branch 3 gets books N-Z\" (similar to simple hashing). This works well initially, but what happens when they open a new branch? Suddenly they need four branches: A-E, F-J, K-P, Q-Z. This requires moving books from every existing branch - a massive reorganization effort.\n\n**The Smart Distribution System (Consistent Hashing)**: A better approach is to imagine a circular arrangement where branches are positioned around a circle, and books are assigned to the \"next\" branch clockwise from their position on the circle. When a new branch opens, only books from its immediate neighbors need to be redistributed. This dramatically reduces the reorganization effort while maintaining balanced distribution.\n\n**Popular Books (Hot Keys)**: Some books are extremely popular and get requested frequently. If a popular book is only stored at one branch, that branch becomes overwhelmed while others remain underutilized. The library system can address this by storing multiple copies of popular books at different branches (replication) or by temporarily moving popular books to less busy branches (load balancing).\n\n**Branch Communication (Gossip Protocol)**: Branch librarians need to stay informed about the status of other branches. Is Branch 3 closed for renovations? Did Branch 5 just receive a large shipment of new books? Rather than having a central coordinator make phone calls to every branch (which would be a single point of failure), branches can share information with their neighbors, who then share with their neighbors. This gossip-like communication ensures all branches stay informed even if some communication lines are temporarily down.\n\n**Inter-Library Loans (Request Routing)**: When a patron visits Branch 1 looking for a book that's actually stored at Branch 4, the librarian doesn't just say \"sorry, we don't have it.\" Instead, they can check their catalog system, identify the correct branch, and either direct the patron there or arrange for the book to be transferred. This is similar to how cache nodes route requests to the correct node in the cluster.\n\n**Backup Copies (Replication)**: Important or popular books might have copies at multiple branches. If Branch 2 is damaged in a flood, patrons can still access those books from other branches that have copies. However, this creates new challenges: If someone returns a book with notes in the margins to Branch 1, how do other branches know about those changes? The library system needs policies for keeping copies synchronized and resolving conflicts.\n\nThis library analogy captures the essential trade-offs of distributed systems: improved capacity and availability through distribution, but increased complexity in coordination, consistency, and failure handling. Just as the library system must balance patron convenience with operational complexity, our distributed cache must balance performance and scalability with system complexity and consistency guarantees.\n\n### Existing Approaches Comparison\n\nDifferent strategies exist for distributing data across multiple cache nodes, each with distinct trade-offs in terms of implementation complexity, load distribution, and operational characteristics. Understanding these approaches helps explain why consistent hashing has become the preferred solution for many distributed systems.\n\n**Simple Modulo Hashing** represents the most straightforward approach to data distribution. In this strategy, keys are hashed using a standard hash function, and the resulting hash value is taken modulo the number of nodes to determine placement. For example, with 4 nodes, `hash(key) % 4` determines which node (0, 1, 2, or 3) should store the key.\n\nThe primary advantage of modulo hashing is its simplicity. The algorithm is trivial to implement, requires no complex data structures, and provides excellent load distribution when the number of nodes remains constant. Key lookup is extremely fast - just a single hash computation and modulo operation.\n\nHowever, modulo hashing has a fatal flaw for dynamic systems: **reshuffling on membership changes**. When nodes are added or removed, the modulo value changes, causing most keys to map to different nodes than before. Adding a 5th node to our example changes the modulo from 4 to 5, meaning a key that previously mapped to `hash(key) % 4 = 2` might now map to `hash(key) % 5 = 3`. This typically requires redistributing 60-80% of all stored data, creating massive network traffic and temporary inconsistency.\n\n**Range-Based Partitioning** divides the key space into contiguous ranges assigned to different nodes. For example, keys starting with A-F go to Node 1, G-M to Node 2, N-S to Node 3, and T-Z to Node 4. This approach is conceptually simple and works well when key access patterns are uniform across the alphabet.\n\nRange partitioning excels when queries often involve key ranges (though this is less common in caching scenarios). Adding new nodes requires splitting existing ranges rather than reshuffling all data, which seems like an improvement over modulo hashing.\n\nHowever, range partitioning suffers from **hotspot problems** when key distributions are non-uniform. If most cache keys start with \"user_\" or \"session_\", the corresponding node becomes overwhelmed while others remain idle. Additionally, determining optimal range boundaries requires knowledge of key distribution patterns that may change over time.\n\n**Directory-Based Approaches** maintain a separate mapping service that tracks which node stores each key or key range. Clients first query the directory service to determine the correct node, then send their cache request to that node.\n\nDirectory services provide maximum flexibility - keys can be placed on any node based on current load, geographic proximity, or other factors. Load balancing becomes straightforward since the directory can direct requests to less busy nodes. The mapping can be changed at any time without complex redistribution algorithms.\n\nThe downside is **additional complexity and failure modes**. Every cache operation now requires two network calls: one to the directory service and one to the actual cache node. The directory service becomes a potential bottleneck and single point of failure. Keeping directory information consistent across multiple directory replicas introduces the same consistency challenges we're trying to solve for the cache itself.\n\n**Consistent Hashing** strikes a balance between the simplicity of modulo hashing and the flexibility needed for dynamic cluster membership. The key insight is to arrange both nodes and keys on a circular hash ring, where each key is assigned to the first node encountered when moving clockwise around the ring.\n\nWhen nodes are added or removed, only keys stored on adjacent nodes need to be redistributed. For example, if Node B is added between existing nodes A and C, only keys that were previously assigned to C but now hash to a position between A and B need to move. This typically affects only 10-20% of keys for small membership changes.\n\nConsistent hashing also enables **virtual nodes** - each physical node appears at multiple positions on the ring. This provides better load distribution and reduces the impact of node failures. Instead of each node having exactly one position on the ring, Node 1 might appear at positions 100, 300, 750, and 900. This reduces variance in load distribution and makes the system more resilient to individual node characteristics.\n\nThe following table summarizes the key trade-offs between these approaches:\n\n| Approach | Load Distribution | Membership Change Impact | Implementation Complexity | Failure Characteristics |\n|----------|------------------|--------------------------|---------------------------|------------------------|\n| Modulo Hashing | Excellent | 60-80% data movement | Very Low | Simple - remaining nodes handle increased load |\n| Range Partitioning | Poor (hotspots) | Moderate - range splits | Low | Hotspot nodes create cascading failures |\n| Directory Service | Excellent | Low - directory updates only | High | Directory becomes single point of failure |\n| Consistent Hashing | Good | 10-20% data movement | Moderate | Gradual degradation with virtual nodes |\n\n> **Design Insight**: The choice of distribution strategy fundamentally determines the system's operational characteristics. Simple approaches like modulo hashing work well for static clusters but become operationally expensive when nodes are frequently added or removed. Consistent hashing provides the best balance of operational simplicity and dynamic behavior for most distributed cache scenarios.\n\n**Hybrid Approaches** combine multiple strategies to optimize for specific use cases. Some systems use consistent hashing for general distribution but maintain directory entries for frequently accessed keys to enable better load balancing. Others use range partitioning within each consistent hash partition to optimize for workloads that benefit from range queries.\n\nAmazon's DynamoDB, for example, uses consistent hashing for initial key distribution but employs additional techniques like virtual node splitting and heat balancing to address load imbalances. Cassandra uses consistent hashing with virtual nodes and adds a replication strategy that considers rack and datacenter topology for improved fault tolerance.\n\nThe key lesson is that data distribution strategy affects every aspect of the system: performance characteristics, operational procedures, failure modes, and implementation complexity. Choosing the right approach requires understanding both the workload characteristics and the operational environment where the system will be deployed.\n\n### Why Distributed Caching is Hard\n\nDistributed caching appears straightforward on the surface - just spread data across multiple machines and route requests appropriately. However, this simple description masks several fundamental challenges that emerge when transitioning from single-node to multi-node systems. These challenges stem from the **CAP theorem** and the inherent complexities of network communication in distributed systems.\n\n**The Network is Unreliable**: In a single-node cache, all operations happen within a single process, where function calls always succeed or fail immediately. In distributed systems, network communication introduces partial failures, message delays, and message reordering. A request to store a key-value pair might succeed on the target node but the response might be lost on the way back to the client. From the client's perspective, the operation failed, but from the system's perspective, it succeeded.\n\nThis ambiguity creates difficult decisions for system designers. Should the client retry the operation (risking duplicate writes) or assume it failed (potentially losing data)? Should the system wait for acknowledgments from all replica nodes (risking timeouts) or proceed with partial success (risking inconsistency)? These decisions require understanding the specific requirements of the application and the acceptable trade-offs between consistency, availability, and performance.\n\n**Distributed State is Hard to Coordinate**: A single-node cache has one authoritative view of what data exists and which operations have completed. In a distributed cache, multiple nodes must coordinate to maintain a consistent view of the system state. Consider a simple scenario where a client wants to increment a counter stored in the cache. In a single-node system, this is atomic: read current value, increment, store new value. In a distributed system with replicas, this operation might succeed on some replicas and fail on others, leaving the system in an inconsistent state where different replicas have different counter values.\n\nMaintaining consistency across replicas requires sophisticated protocols like **two-phase commit**, **Paxos**, or **Raft**. These protocols ensure all replicas agree on the order of operations and the final state, but they introduce significant complexity and performance overhead. The system must handle scenarios where some replicas are temporarily unavailable, network partitions split the cluster, and nodes recover after extended downtime with potentially stale data.\n\n**Cluster Membership is Dynamic**: Unlike single-node systems where the \"cluster\" consists of exactly one node, distributed caches must handle nodes joining and leaving the cluster dynamically. New nodes might be added to increase capacity, existing nodes might fail or be taken offline for maintenance, and network partitions might temporarily split the cluster into multiple sub-clusters.\n\nEach membership change potentially affects data placement, load distribution, and replication strategies. When a node fails, its data becomes unavailable unless replicated elsewhere. When a node joins, existing data must be redistributed to maintain load balance. When a network partition heals, nodes must reconcile potentially conflicting changes made during the partition.\n\nThe challenge is detecting membership changes reliably and quickly. A node might be slow to respond due to high load rather than actual failure. Network congestion might delay heartbeat messages, leading to false failure detections. Automated systems must distinguish between temporary slowdowns and permanent failures while avoiding unnecessary data movement and client disruption.\n\n**Failure Modes Multiply**: Single-node systems have simple failure modes: the process crashes, the machine loses power, or the disk fails. Distributed systems inherit all these failure modes and add many more: network partitions, cascading failures, split-brain scenarios, and partial failures where some operations succeed while others fail.\n\nConsider what happens when a cache node becomes temporarily unreachable due to network congestion. Should other nodes immediately mark it as failed and begin redistributing its data? If they do, and the node recovers quickly, the redistribution work was unnecessary and created temporary inconsistency. If they don't, and the node has actually failed, clients experience longer outages waiting for timeouts.\n\n**Split-brain scenarios** occur when network partitions divide the cluster into multiple sub-clusters, each believing it's the authoritative portion of the system. Both sub-clusters might accept writes for the same keys, creating conflicts that must be resolved when the partition heals. Prevention requires sophisticated quorum protocols that ensure at most one partition can accept writes, but this risks availability during partitions.\n\n**Cascading failures** happen when the failure of one node increases load on remaining nodes, potentially causing them to fail as well. If a 10-node cache cluster suddenly loses 2 nodes, the remaining 8 nodes must handle 25% more load. If they were already operating near capacity, this additional load might cause timeouts, which clients interpret as failures, leading them to retry operations and create even more load.\n\n**Data Consistency vs. Performance Trade-offs**: The CAP theorem formally proves that distributed systems cannot simultaneously provide consistency, availability, and partition tolerance. Cache systems must explicitly choose which guarantees to provide and under what circumstances to sacrifice others.\n\n**Strong consistency** ensures all replicas always return the same value for a given key, but requires coordination overhead that increases latency and reduces availability during network issues. **Eventual consistency** allows temporary divergence between replicas in exchange for better performance and availability, but complicates application logic since reads might return stale data.\n\n**Read-your-writes consistency** guarantees that clients immediately see their own updates, but may see stale data from other clients' updates. This requires sticky sessions or careful coordination between the client and replica nodes. **Monotonic read consistency** ensures clients never see older versions of data than they've previously seen, but allows different clients to see different versions simultaneously.\n\nEach consistency level has different implementation requirements and performance characteristics. Strong consistency typically requires all replicas to agree before completing operations, while eventual consistency can complete operations locally and propagate changes asynchronously. The choice affects not just performance, but also the complexity of client logic and operational procedures.\n\n**Load Distribution Challenges**: Even with perfect hashing algorithms, distributed caches face load imbalances due to variations in data size, access patterns, and node characteristics. Some keys are accessed much more frequently than others (**hot keys**), causing the nodes that store them to become bottlenecks. Some values are much larger than others, causing uneven memory usage across nodes.\n\n**Hot key problems** are particularly challenging because they violate the assumption that load distributes evenly across nodes. A single trending topic on social media might cause millions of cache requests for the same key, overwhelming whichever node happens to store it. Traditional solutions like replication help but don't eliminate the bottleneck, since all writes still go to the same primary replica.\n\n**Data size variations** can cause some nodes to reach memory limits while others remain half-empty. A few large cached objects (like serialized images or documents) might consume most of a node's memory, while other nodes store many small key-value pairs. Simple approaches like moving large objects to different nodes help temporarily but don't solve the fundamental problem of unpredictable data sizes.\n\n**Observability and Debugging Complexity**: Single-node systems can be debugged by examining logs, memory dumps, and profiler output from a single process. Distributed systems require correlating information across multiple nodes, networks, and time zones to understand system behavior.\n\n**Distributed tracing** becomes essential for following requests across multiple nodes and understanding where latency or failures occur. A single client request might touch 5-10 different cache nodes when considering replication, routing, and failure recovery. Understanding why a request was slow requires correlating timing information across all these nodes and accounting for network delays.\n\n**Monitoring and alerting** must account for the fact that partial failures are normal in distributed systems. Traditional monitoring approaches that alert on any node failure would create alert fatigue, since individual node failures should be handled automatically. Instead, monitoring must focus on cluster-level health: overall availability, consistency violations, and degraded performance rather than individual node status.\n\n> **Key Insight**: The fundamental challenge of distributed caching isn't technical complexity - it's the explosion of possible system states and failure modes. A single-node cache has predictable behavior and simple failure modes. A distributed cache must handle scenarios where some operations succeed while others fail, some nodes are available while others aren't, and some data is consistent while other data diverges. Managing this complexity requires careful design decisions about consistency guarantees, failure detection, and recovery procedures.\n\nThe distributed cache we're building addresses these challenges through several key design decisions: consistent hashing for minimal data movement during membership changes, gossip protocols for robust membership detection, configurable consistency levels for application-specific trade-offs, and comprehensive monitoring and observability for operational simplicity. Each of these decisions involves trade-offs, and understanding the alternatives helps inform better choices for specific deployment environments and application requirements.\n\n### Implementation Guidance\n\nBuilding a distributed cache requires careful technology choices and architectural decisions that will affect the entire implementation. This section provides concrete recommendations for getting started with a Go-based implementation that balances simplicity with the flexibility to add advanced features later.\n\n#### Technology Recommendations\n\nThe following table outlines technology choices for different system components, providing both simple options for initial implementation and advanced alternatives for production deployments:\n\n| Component | Simple Option | Advanced Option | Reasoning |\n|-----------|---------------|-----------------|-----------|\n| Transport Protocol | HTTP REST with JSON (net/http) | gRPC with Protocol Buffers | HTTP is easier to debug and test; gRPC provides better performance and type safety |\n| Serialization | JSON (encoding/json) | Protocol Buffers or MessagePack | JSON is human-readable and widely supported; binary formats are more efficient |\n| Node Discovery | Static configuration file | etcd, Consul, or mDNS | Static config is simple for development; service discovery scales better |\n| Health Checking | HTTP health endpoints | Custom UDP heartbeat protocol | HTTP reuses existing infrastructure; UDP reduces network overhead |\n| Consistent Hashing | SHA-256 with standard library | Jump Consistent Hash or MurmurHash3 | SHA-256 is built-in and secure; specialized hashes may perform better |\n| Logging | Standard log package | Structured logging (logrus, zap) | Standard library requires no dependencies; structured logs enable better analysis |\n| Metrics | Simple counters in memory | Prometheus metrics with HTTP endpoint | In-memory is zero-dependency; Prometheus integrates with monitoring systems |\n| Configuration | JSON config file | YAML with environment variable override | JSON is simple and built-in; YAML is more readable for complex configs |\n\n#### Recommended Project Structure\n\nOrganize the codebase to separate concerns clearly and make it easy to test individual components in isolation:\n\n```\ndistributed-cache/\n├── cmd/\n│   ├── cachenode/\n│   │   └── main.go                    ← Entry point for cache node process\n│   └── client/\n│       └── main.go                    ← Example client for testing\n├── internal/\n│   ├── cache/\n│   │   ├── lru.go                     ← LRU cache implementation (Milestone 2)\n│   │   ├── node.go                    ← Cache node with networking\n│   │   └── cache_test.go              ← Unit tests\n│   ├── ring/\n│   │   ├── consistent.go              ← Consistent hash ring (Milestone 1)\n│   │   ├── virtual.go                 ← Virtual node management\n│   │   └── ring_test.go               ← Hash ring tests\n│   ├── cluster/\n│   │   ├── membership.go              ← Node discovery and membership (Milestone 3)\n│   │   ├── gossip.go                  ← Gossip protocol implementation\n│   │   ├── health.go                  ← Health checking\n│   │   └── cluster_test.go            ← Integration tests\n│   ├── replication/\n│   │   ├── strategy.go                ← Replication factor and placement (Milestone 4)\n│   │   ├── quorum.go                  ← Read/write quorum handling\n│   │   ├── conflict.go                ← Conflict resolution\n│   │   └── replication_test.go        ← Replication tests\n│   ├── transport/\n│   │   ├── http.go                    ← HTTP transport layer\n│   │   ├── messages.go                ← Message type definitions\n│   │   └── transport_test.go          ← Transport tests\n│   └── config/\n│       ├── config.go                  ← Configuration loading and validation\n│       └── defaults.go                ← Default configuration values\n├── pkg/\n│   └── api/\n│       ├── client.go                  ← Client library for applications\n│       └── types.go                   ← Public API types\n├── configs/\n│   ├── node1.json                     ← Example node configurations\n│   ├── node2.json\n│   └── cluster.json                   ← Cluster-wide settings\n├── scripts/\n│   ├── start-cluster.sh               ← Development cluster startup\n│   └── test-integration.sh            ← Integration test runner\n├── go.mod\n├── go.sum\n├── README.md\n└── Makefile                           ← Build and test automation\n```\n\nThis structure separates the core algorithms (`internal/ring`, `internal/cache`) from the networking and coordination logic (`internal/cluster`, `internal/transport`). The `pkg/api` directory contains the public interface that client applications will use, while `internal/` contains implementation details that shouldn't be imported by external code.\n\n#### Infrastructure Starter Code\n\nThe following complete implementations handle networking and configuration concerns, allowing learners to focus on the core distributed caching algorithms:\n\n**Configuration Management** (`internal/config/config.go`):\n\n```go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n)\n\n// NodeConfig contains all settings for a single cache node\ntype NodeConfig struct {\n    NodeID          string        `json:\"node_id\"`\n    ListenAddress   string        `json:\"listen_address\"`\n    AdvertiseAddr   string        `json:\"advertise_address\"`\n    JoinAddresses   []string      `json:\"join_addresses\"`\n    MaxMemoryMB     int           `json:\"max_memory_mb\"`\n    VirtualNodes    int           `json:\"virtual_nodes\"`\n    ReplicationFactor int         `json:\"replication_factor\"`\n    HealthCheckInterval time.Duration `json:\"health_check_interval\"`\n    GossipInterval  time.Duration `json:\"gossip_interval\"`\n    RequestTimeout  time.Duration `json:\"request_timeout\"`\n}\n\n// LoadConfig reads configuration from file and environment variables\nfunc LoadConfig(filename string) (*NodeConfig, error) {\n    file, err := os.Open(filename)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to open config file: %w\", err)\n    }\n    defer file.Close()\n\n    var config NodeConfig\n    decoder := json.NewDecoder(file)\n    if err := decoder.Decode(&config); err != nil {\n        return nil, fmt.Errorf(\"failed to parse config: %w\", err)\n    }\n\n    // Apply defaults for missing values\n    if config.MaxMemoryMB == 0 {\n        config.MaxMemoryMB = 1024 // 1GB default\n    }\n    if config.VirtualNodes == 0 {\n        config.VirtualNodes = 150 // Good default for load distribution\n    }\n    if config.ReplicationFactor == 0 {\n        config.ReplicationFactor = 3\n    }\n    if config.HealthCheckInterval == 0 {\n        config.HealthCheckInterval = 5 * time.Second\n    }\n    if config.GossipInterval == 0 {\n        config.GossipInterval = 1 * time.Second\n    }\n    if config.RequestTimeout == 0 {\n        config.RequestTimeout = 10 * time.Second\n    }\n\n    return &config, nil\n}\n\n// Validate checks that configuration values are sensible\nfunc (c *NodeConfig) Validate() error {\n    if c.NodeID == \"\" {\n        return fmt.Errorf(\"node_id is required\")\n    }\n    if c.ListenAddress == \"\" {\n        return fmt.Errorf(\"listen_address is required\")\n    }\n    if c.MaxMemoryMB < 1 {\n        return fmt.Errorf(\"max_memory_mb must be at least 1\")\n    }\n    if c.VirtualNodes < 1 {\n        return fmt.Errorf(\"virtual_nodes must be at least 1\")\n    }\n    if c.ReplicationFactor < 1 {\n        return fmt.Errorf(\"replication_factor must be at least 1\")\n    }\n    return nil\n}\n```\n\n**HTTP Transport Layer** (`internal/transport/http.go`):\n\n```go\npackage transport\n\nimport (\n    \"bytes\"\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\n// HTTPTransport handles HTTP communication between cache nodes\ntype HTTPTransport struct {\n    client  *http.Client\n    timeout time.Duration\n}\n\n// NewHTTPTransport creates a transport with specified timeout\nfunc NewHTTPTransport(timeout time.Duration) *HTTPTransport {\n    return &HTTPTransport{\n        client: &http.Client{\n            Timeout: timeout,\n        },\n        timeout: timeout,\n    }\n}\n\n// SendMessage sends a message to a remote node and returns the response\nfunc (t *HTTPTransport) SendMessage(ctx context.Context, address string, message interface{}) ([]byte, error) {\n    // Serialize message to JSON\n    data, err := json.Marshal(message)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to marshal message: %w\", err)\n    }\n\n    // Create HTTP request\n    url := fmt.Sprintf(\"http://%s/api/message\", address)\n    req, err := http.NewRequestWithContext(ctx, \"POST\", url, bytes.NewReader(data))\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to create request: %w\", err)\n    }\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    // Send request\n    resp, err := t.client.Do(req)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to send request: %w\", err)\n    }\n    defer resp.Body.Close()\n\n    // Check response status\n    if resp.StatusCode != http.StatusOK {\n        return nil, fmt.Errorf(\"request failed with status %d\", resp.StatusCode)\n    }\n\n    // Read response body\n    var response bytes.Buffer\n    if _, err := response.ReadFrom(resp.Body); err != nil {\n        return nil, fmt.Errorf(\"failed to read response: %w\", err)\n    }\n\n    return response.Bytes(), nil\n}\n\n// HealthCheck sends a health check request to a node\nfunc (t *HTTPTransport) HealthCheck(ctx context.Context, address string) error {\n    url := fmt.Sprintf(\"http://%s/health\", address)\n    req, err := http.NewRequestWithContext(ctx, \"GET\", url, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to create health check request: %w\", err)\n    }\n\n    resp, err := t.client.Do(req)\n    if err != nil {\n        return fmt.Errorf(\"health check failed: %w\", err)\n    }\n    defer resp.Body.Close()\n\n    if resp.StatusCode != http.StatusOK {\n        return fmt.Errorf(\"health check returned status %d\", resp.StatusCode)\n    }\n\n    return nil\n}\n```\n\n**Message Type Definitions** (`internal/transport/messages.go`):\n\n```go\npackage transport\n\nimport \"time\"\n\n// MessageType identifies the type of message being sent\ntype MessageType string\n\nconst (\n    MessageTypeGossip     MessageType = \"gossip\"\n    MessageTypeGet        MessageType = \"get\"\n    MessageTypeSet        MessageType = \"set\"\n    MessageTypeDelete     MessageType = \"delete\"\n    MessageTypeReplicate  MessageType = \"replicate\"\n)\n\n// Message is the envelope for all inter-node communication\ntype Message struct {\n    Type      MessageType `json:\"type\"`\n    Sender    string      `json:\"sender\"`\n    Timestamp time.Time   `json:\"timestamp\"`\n    Data      interface{} `json:\"data\"`\n}\n\n// GetRequest represents a cache get operation\ntype GetRequest struct {\n    Key           string `json:\"key\"`\n    ConsistencyLevel string `json:\"consistency_level,omitempty\"`\n}\n\n// GetResponse contains the result of a get operation\ntype GetResponse struct {\n    Key       string    `json:\"key\"`\n    Value     []byte    `json:\"value,omitempty\"`\n    Found     bool      `json:\"found\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// SetRequest represents a cache set operation\ntype SetRequest struct {\n    Key       string        `json:\"key\"`\n    Value     []byte        `json:\"value\"`\n    TTL       time.Duration `json:\"ttl,omitempty\"`\n    ConsistencyLevel string `json:\"consistency_level,omitempty\"`\n}\n\n// SetResponse contains the result of a set operation\ntype SetResponse struct {\n    Success   bool      `json:\"success\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// DeleteRequest represents a cache delete operation\ntype DeleteRequest struct {\n    Key       string `json:\"key\"`\n    ConsistencyLevel string `json:\"consistency_level,omitempty\"`\n}\n\n// DeleteResponse contains the result of a delete operation\ntype DeleteResponse struct {\n    Success   bool      `json:\"success\"`\n    Found     bool      `json:\"found\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// GossipMessage carries cluster membership information\ntype GossipMessage struct {\n    NodeStates map[string]NodeState `json:\"node_states\"`\n    Version    uint64              `json:\"version\"`\n}\n\n// NodeState represents the state of a single node in the cluster\ntype NodeState struct {\n    NodeID      string    `json:\"node_id\"`\n    Address     string    `json:\"address\"`\n    Status      string    `json:\"status\"`      // \"active\", \"suspected\", \"failed\"\n    LastSeen    time.Time `json:\"last_seen\"`\n    Version     uint64    `json:\"version\"`\n}\n```\n\n#### Core Logic Skeleton Code\n\nThe following skeletons provide the method signatures and detailed TODO comments for the core components that learners should implement themselves:\n\n**Consistent Hash Ring Interface** (`internal/ring/consistent.go`):\n\n```go\npackage ring\n\nimport (\n    \"crypto/sha256\"\n    \"fmt\"\n    \"sort\"\n)\n\n// HashRing implements consistent hashing with virtual nodes\ntype HashRing struct {\n    virtualNodes int\n    ring        map[uint32]string  // hash position -> node ID\n    sortedKeys  []uint32           // sorted hash positions for binary search\n    nodes       map[string]bool    // set of active nodes\n}\n\n// NewHashRing creates a new consistent hash ring\nfunc NewHashRing(virtualNodes int) *HashRing {\n    return &HashRing{\n        virtualNodes: virtualNodes,\n        ring:        make(map[uint32]string),\n        sortedKeys:  make([]uint32, 0),\n        nodes:       make(map[string]bool),\n    }\n}\n\n// AddNode adds a node to the hash ring\n// This method should add virtual nodes for better load distribution\nfunc (h *HashRing) AddNode(nodeID string) {\n    // TODO 1: Check if node already exists, return early if it does\n    // TODO 2: Add node to the nodes set\n    // TODO 3: Create virtual nodes by hashing nodeID + virtual node index\n    // TODO 4: For each virtual node, calculate hash position and add to ring\n    // TODO 5: Add hash positions to sortedKeys slice\n    // TODO 6: Sort the sortedKeys slice for binary search\n    // Hint: Use fmt.Sprintf(\"%s:%d\", nodeID, i) for virtual node keys\n}\n\n// RemoveNode removes a node from the hash ring\nfunc (h *HashRing) RemoveNode(nodeID string) {\n    // TODO 1: Check if node exists, return early if it doesn't\n    // TODO 2: Remove node from nodes set\n    // TODO 3: Find all hash positions for this node's virtual nodes\n    // TODO 4: Remove hash positions from ring map and sortedKeys slice\n    // TODO 5: Sort the sortedKeys slice after removal\n    // Hint: You'll need to iterate through ring to find positions for this node\n}\n\n// GetNode returns the node responsible for storing the given key\nfunc (h *HashRing) GetNode(key string) string {\n    // TODO 1: Return empty string if ring has no nodes\n    // TODO 2: Calculate hash of the key using hashKey function\n    // TODO 3: Use binary search to find first position >= key hash\n    // TODO 4: If no position found, wrap around to first position (ring property)\n    // TODO 5: Return node ID at that position\n    // Hint: sort.Search() can help with binary search\n}\n\n// GetNodes returns the N nodes responsible for a key (for replication)\nfunc (h *HashRing) GetNodes(key string, count int) []string {\n    // TODO 1: Return empty slice if ring has no nodes or count <= 0\n    // TODO 2: Calculate hash of the key\n    // TODO 3: Find starting position using binary search\n    // TODO 4: Collect up to 'count' unique nodes by walking clockwise around ring\n    // TODO 5: Return slice of unique node IDs\n    // Hint: Use a map to track unique nodes since virtual nodes repeat\n}\n\n// GetNodeKeys returns all keys that should be stored on the given node\n// This is used during rebalancing to determine which keys to migrate\nfunc (h *HashRing) GetNodeKeys(nodeID string, allKeys []string) []string {\n    // TODO 1: Create slice to hold result keys\n    // TODO 2: For each key in allKeys, check if GetNode(key) == nodeID\n    // TODO 3: If yes, add to result slice\n    // TODO 4: Return result slice\n}\n\n// hashKey computes hash value for a key - you can use this helper\nfunc hashKey(key string) uint32 {\n    hasher := sha256.New()\n    hasher.Write([]byte(key))\n    hash := hasher.Sum(nil)\n    return uint32(hash[0])<<24 | uint32(hash[1])<<16 | uint32(hash[2])<<8 | uint32(hash[3])\n}\n```\n\n**LRU Cache Node** (`internal/cache/lru.go`):\n\n```go\npackage cache\n\nimport (\n    \"container/list\"\n    \"sync\"\n    \"time\"\n)\n\n// CacheEntry represents a single cached item\ntype CacheEntry struct {\n    Key       string\n    Value     []byte\n    ExpiresAt time.Time\n    Size      int64\n}\n\n// LRUCache implements an LRU cache with TTL support and memory limits\ntype LRUCache struct {\n    mutex      sync.RWMutex\n    capacity   int64                           // maximum memory in bytes\n    used       int64                           // current memory usage\n    items      map[string]*list.Element        // key -> list element\n    order      *list.List                      // LRU order (front = most recent)\n    // Note: list.Element.Value should be *CacheEntry\n}\n\n// NewLRUCache creates a new LRU cache with the specified capacity in bytes\nfunc NewLRUCache(capacityBytes int64) *LRUCache {\n    return &LRUCache{\n        capacity: capacityBytes,\n        items:    make(map[string]*list.Element),\n        order:    list.New(),\n    }\n}\n\n// Get retrieves a value from the cache\nfunc (c *LRUCache) Get(key string) ([]byte, bool) {\n    // TODO 1: Acquire read lock\n    // TODO 2: Check if key exists in items map\n    // TODO 3: If not found, return nil, false\n    // TODO 4: Get CacheEntry from list element\n    // TODO 5: Check if entry has expired (time.Now().After(ExpiresAt))\n    // TODO 6: If expired, remove entry and return nil, false\n    // TODO 7: Move element to front of list (most recently used)\n    // TODO 8: Return value copy, true\n    // Hint: Use c.removeElement() helper for expired entries\n}\n\n// Set stores a value in the cache\nfunc (c *LRUCache) Set(key string, value []byte, ttl time.Duration) {\n    // TODO 1: Acquire write lock\n    // TODO 2: Calculate entry size (len(key) + len(value) + overhead)\n    // TODO 3: Calculate expiration time (time.Now().Add(ttl) if ttl > 0)\n    // TODO 4: If key already exists, remove old entry first\n    // TODO 5: Ensure we have enough space by evicting LRU entries\n    // TODO 6: Create new CacheEntry\n    // TODO 7: Add entry to front of list and update items map\n    // TODO 8: Update used memory counter\n    // Hint: Use c.evictUntilCapacity() to make space\n}\n\n// Delete removes a key from the cache\nfunc (c *LRUCache) Delete(key string) bool {\n    // TODO 1: Acquire write lock\n    // TODO 2: Check if key exists in items map\n    // TODO 3: If found, remove element and return true\n    // TODO 4: If not found, return false\n}\n\n// evictUntilCapacity removes LRU entries until there's enough space\nfunc (c *LRUCache) evictUntilCapacity(neededSpace int64) {\n    // TODO 1: While (used + neededSpace > capacity) and cache not empty\n    // TODO 2: Get least recently used element (back of list)\n    // TODO 3: Remove that element\n    // TODO 4: Continue until enough space available\n}\n\n// removeElement removes a specific element from cache\nfunc (c *LRUCache) removeElement(element *list.Element) {\n    // TODO 1: Get CacheEntry from element.Value\n    // TODO 2: Remove element from list\n    // TODO 3: Remove key from items map  \n    // TODO 4: Update used memory counter\n}\n\n// CleanupExpired removes expired entries (call periodically)\nfunc (c *LRUCache) CleanupExpired() int {\n    // TODO 1: Acquire write lock\n    // TODO 2: Iterate through all entries\n    // TODO 3: Check expiration time for each entry\n    // TODO 4: Remove expired entries\n    // TODO 5: Return count of removed entries\n    // Hint: Iterate from back to front to avoid invalidating iterators\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Go-Specific Best Practices:**\n\n- Use `sync.RWMutex` for cache operations that are read-heavy. Multiple goroutines can hold read locks simultaneously, improving performance.\n- The `container/list` package provides an efficient doubly-linked list for LRU tracking. Elements can be moved to front in O(1) time.\n- Use `context.Context` for all network operations to enable timeouts and cancellation.\n- Prefer `time.Time` over Unix timestamps for expiration handling - it's more readable and handles timezone issues automatically.\n- Use `make(map[string]Type, estimatedSize)` when you know the approximate map size to avoid rehashing during growth.\n\n**Memory Management Tips:**\n\n```go\n// Calculate entry size including overhead\nfunc calculateEntrySize(key string, value []byte) int64 {\n    // Account for string header, slice header, and map entry overhead\n    const mapEntryOverhead = 32 // approximate\n    return int64(len(key) + len(value) + mapEntryOverhead)\n}\n\n// Create defensive copies of returned values\nfunc copyBytes(src []byte) []byte {\n    if src == nil {\n        return nil\n    }\n    dst := make([]byte, len(src))\n    copy(dst, src)\n    return dst\n}\n```\n\n**Networking Best Practices:**\n\n- Always set timeouts on HTTP clients: `http.Client{Timeout: 10 * time.Second}`\n- Use connection pooling by reusing HTTP clients rather than creating new ones for each request\n- Handle partial failures gracefully - a 500 status code from one node shouldn't crash the entire operation\n- Log request/response details at debug level for troubleshooting distributed issues\n\n**Testing Approach:**\n\nStart with unit tests for individual components before building integration tests:\n\n```bash\n# Test individual components\ngo test -v ./internal/ring/...\ngo test -v ./internal/cache/...\n\n# Test with race detector\ngo test -race ./internal/cache/...\n\n# Test with coverage\ngo test -cover ./internal/...\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 Checkpoint (Consistent Hash Ring):**\nAfter implementing the hash ring, verify it works correctly:\n\n```bash\n# Run hash ring tests\ngo test -v ./internal/ring/\n\n# Expected output: Tests should verify even key distribution,\n# minimal redistribution on node changes, and correct virtual node behavior\n```\n\nCreate a simple test program to verify hash ring behavior:\n\n```go\n// Test program in cmd/ring-test/main.go\nring := ring.NewHashRing(150)\nring.AddNode(\"node1\")\nring.AddNode(\"node2\")\nring.AddNode(\"node3\")\n\n// Test key distribution\nkeys := []string{\"user:1\", \"user:2\", \"user:3\", \"session:abc\", \"data:xyz\"}\nfor _, key := range keys {\n    node := ring.GetNode(key)\n    fmt.Printf(\"Key %s -> Node %s\\n\", key, node)\n}\n\n// Test node addition\nfmt.Println(\"\\nAdding node4...\")\nring.AddNode(\"node4\")\nfor _, key := range keys {\n    node := ring.GetNode(key)\n    fmt.Printf(\"Key %s -> Node %s\\n\", key, node)\n}\n```\n\n**Milestone 2 Checkpoint (Cache Node Implementation):**\nTest LRU cache behavior and memory limits:\n\n```bash\n# Run cache tests\ngo test -v ./internal/cache/\n\n# Test with race detector to check concurrency\ngo test -race ./internal/cache/\n```\n\nVerify cache eviction and TTL behavior manually:\n\n```go\ncache := cache.NewLRUCache(1024) // 1KB capacity\ncache.Set(\"key1\", make([]byte, 500), time.Hour)\ncache.Set(\"key2\", make([]byte, 500), time.Hour) \ncache.Set(\"key3\", make([]byte, 500), time.Hour) // Should evict key1\n\n_, found := cache.Get(\"key1\") // Should be false\nfmt.Printf(\"key1 found: %v\\n\", found)\n```\n\n**Signs of Problems and Debugging:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Uneven key distribution | Not enough virtual nodes | Check node assignment counts | Increase virtual node count to 150+ |\n| High memory usage | Memory not freed on delete | Add debug logging to removeElement | Ensure used counter decrements correctly |\n| Cache misses on existing data | Race condition in Get/Set | Run tests with `-race` flag | Add proper mutex locking |\n| Slow cache operations | Lock contention | Profile with `go tool pprof` | Use RWMutex for read-heavy workloads |\n| Test failures | Incorrect algorithm | Step through with debugger | Verify hash ring traversal logic |\n\nThe key to successful implementation is building incrementally and testing each component thoroughly before moving to the next milestone. Focus on getting the core algorithms correct before adding complexity like networking and replication.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** This section establishes the scope and success criteria for all milestones, with particular relevance to overall system design and architecture decisions.\n\nBuilding a distributed cache system requires careful balance between functionality, performance, and complexity. Think of this like planning a new public transportation system for a growing city. You need to define which neighborhoods you'll serve (functional goals), how fast and reliable the service must be (non-functional goals), and which advanced features like high-speed rail connections you'll defer to future phases (non-goals). Clear goal definition prevents scope creep and ensures every architectural decision serves a specific purpose.\n\nThis section establishes the boundaries of our distributed cache implementation, defining what success looks like at each milestone while explicitly excluding features that would complicate the core learning objectives. These goals directly influence every subsequent design decision, from the consistent hashing algorithm to the replication strategy.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n### Functional Goals\n\n**Functional goals** define the core capabilities our distributed cache must provide to users and applications. These represent the essential operations that make the system useful as a caching solution.\n\n| Goal Category | Specific Capability | Success Criteria | Milestone |\n|---------------|-------------------|------------------|-----------|\n| Key-Value Operations | GET operation | Retrieve cached values by key with sub-millisecond latency for cache hits | 2, 3 |\n| Key-Value Operations | SET operation | Store key-value pairs with optional TTL, supporting values up to 1MB | 2, 3 |\n| Key-Value Operations | DELETE operation | Remove specific keys from cache with immediate consistency | 2, 3 |\n| Data Distribution | Consistent hashing | Distribute keys evenly across cluster nodes with minimal reshuffling on membership changes | 1 |\n| Data Distribution | Virtual nodes | Support configurable virtual nodes (64-512 per physical node) for improved load balancing | 1 |\n| Cluster Management | Node discovery | New nodes automatically join cluster without manual configuration | 3 |\n| Cluster Management | Failure detection | Detect and handle node failures within 5 seconds using health checks | 3 |\n| Cluster Management | Dynamic membership | Support adding/removing nodes during runtime without service interruption | 1, 3 |\n| Data Persistence | Memory management | Enforce configurable memory limits per node with LRU eviction | 2 |\n| Data Persistence | TTL expiration | Automatically expire cached entries after configured time-to-live | 2 |\n| Fault Tolerance | Data replication | Store configurable number of copies (1-5) for each cached entry | 4 |\n| Fault Tolerance | Consistency levels | Support eventual consistency and strong consistency options for operations | 4 |\n\nThe **primary functional goal** is providing a reliable key-value store that distributes data across multiple nodes while maintaining reasonable performance characteristics. Unlike a single-node cache, our system must handle the complexity of data distribution, node failures, and network partitions while preserving the simple GET/SET/DELETE interface that applications expect.\n\n**Key-value operations** form the foundation of cache functionality. The `GET` operation must locate the correct node using consistent hashing, handle cases where data might be replicated across multiple nodes, and return results quickly. The `SET` operation needs to determine replica placement, coordinate writes across multiple nodes when using replication, and respect memory limits through eviction policies. The `DELETE` operation must remove all replicas consistently and handle scenarios where some replica nodes might be temporarily unavailable.\n\n**Data distribution** through consistent hashing solves the fundamental challenge of mapping keys to nodes in a way that minimizes data movement when cluster membership changes. Traditional modulo hashing would require redistributing most keys when adding or removing nodes, making it impractical for dynamic clusters. Our consistent hashing implementation must support virtual nodes to prevent hotspots when key distributions are uneven or when popular keys would otherwise concentrate on a single physical node.\n\n> **The critical insight** is that functional goals must work together harmoniously. For example, TTL expiration must interact correctly with replication (expired entries should be removed from all replicas), and LRU eviction must consider the replication factor when calculating memory usage.\n\n**Cluster management** capabilities enable the system to adapt to changing infrastructure conditions. Node discovery allows operators to add capacity by simply starting new cache nodes, which then automatically integrate into the existing cluster. Failure detection ensures that unresponsive nodes are quickly removed from the hash ring, preventing clients from being routed to dead nodes. Dynamic membership changes must preserve data availability by triggering appropriate rebalancing and replica migration.\n\n**Data persistence** in our context refers to keeping data available in memory while managing resource constraints. LRU eviction provides a reasonable policy for choosing which entries to remove when memory pressure occurs. TTL expiration handles the common caching pattern where data has a natural lifetime after which it should be automatically cleaned up.\n\n**Fault tolerance** through replication provides data availability when individual nodes fail. Supporting different consistency levels allows applications to choose their preferred trade-off between performance and data consistency. Eventual consistency enables faster operations with the possibility of temporary inconsistencies, while strong consistency guarantees that all replicas agree before operations complete.\n\n### Non-Functional Goals\n\n**Non-functional goals** specify the quality attributes and performance characteristics that make the distributed cache suitable for production use. These goals influence architectural decisions throughout the system design.\n\n| Goal Category | Requirement | Target Metric | Measurement Method |\n|---------------|-------------|---------------|-------------------|\n| Performance | GET latency | < 1ms for local cache hits, < 5ms for remote hits | P99 latency under normal load |\n| Performance | SET latency | < 2ms for single replica, < 10ms with replication factor 3 | P99 latency under normal load |\n| Performance | Throughput | 10,000+ operations/second per node | Sustained load testing |\n| Scalability | Cluster size | Support 10-100 nodes in single cluster | Linear scaling verification |\n| Scalability | Storage capacity | 1GB - 64GB memory per node | Configurable memory limits |\n| Scalability | Key space | Support millions of keys per cluster | Hash distribution testing |\n| Availability | Node failure tolerance | Survive failure of minority of nodes (< 50%) | Chaos testing scenarios |\n| Availability | Recovery time | Return to full service within 30 seconds of failure | Automated failure injection |\n| Availability | Uptime | 99.9% availability under normal conditions | Extended stability testing |\n| Consistency | Replication lag | Eventual consistency convergence within 1 second | Anti-entropy effectiveness |\n| Consistency | Conflict resolution | Deterministic resolution of concurrent writes | Vector clock or timestamp comparison |\n| Network Efficiency | Gossip overhead | < 5% of network bandwidth for membership gossip | Network monitoring |\n| Network Efficiency | Replication traffic | Minimal unnecessary duplicate transfers | Request routing optimization |\n| Resource Usage | Memory overhead | < 20% metadata overhead per cached entry | Memory profiling |\n| Resource Usage | CPU utilization | < 80% CPU under peak load per node | Performance monitoring |\n\n**Performance goals** establish the responsiveness expectations that make the cache useful for latency-sensitive applications. The `GET` operation latency targets distinguish between local hits (data stored on the receiving node) and remote hits (data retrieved from other cluster nodes). `SET` operation latency accounts for the additional work required when replicating data across multiple nodes. Throughput goals ensure that individual nodes can handle substantial request volumes without becoming bottlenecks.\n\n> **Design Principle:** Performance goals must remain achievable even as cluster size grows. This influences decisions like using efficient serialization formats, minimizing network round trips, and choosing algorithms with favorable complexity characteristics.\n\n**Scalability goals** define how the system should behave as load and cluster size increase. Supporting 10-100 nodes covers most practical deployment scenarios while avoiding the complexity required for massive clusters with thousands of nodes. Per-node storage capacity targets reflect typical cache deployment patterns where memory is the primary constraint. Key space scalability ensures that the consistent hashing algorithm and data structures can handle realistic cache workloads.\n\n**Availability goals** specify how the system should respond to various failure scenarios. Tolerating minority node failures means the system remains functional when less than half the nodes are unavailable, which covers most common failure patterns like single node crashes or small network partitions. Recovery time goals ensure that temporary failures don't cause extended service disruption. The 99.9% uptime target allows for planned maintenance windows while maintaining high service reliability.\n\n**Consistency goals** define how the system handles the inherent challenges of distributed data management. Replication lag targets ensure that eventual consistency doesn't create indefinitely inconsistent states that could confuse applications. Conflict resolution requirements address scenarios where network partitions or timing issues cause concurrent writes to the same key, requiring deterministic resolution to prevent data corruption.\n\n**Network efficiency goals** prevent the distributed coordination mechanisms from overwhelming the network infrastructure. Gossip protocol overhead must remain bounded even as cluster size increases. Replication traffic optimization ensures that replica updates don't create unnecessary network amplification, particularly important when replication factors are high or when clients frequently update the same keys.\n\n**Resource usage goals** ensure the system remains efficient and cost-effective to operate. Memory overhead limits prevent metadata from consuming excessive space relative to actual cached data. CPU utilization targets leave headroom for load spikes while ensuring efficient use of available processing capacity.\n\n> **Architecture Decision: Resource Efficiency vs. Feature Richness**\n> - **Context**: Distributed systems can implement sophisticated features like advanced conflict resolution, complex consistency models, or rich query capabilities, but these typically increase resource overhead\n> - **Options Considered**: \n>   1. Minimal feature set with optimal resource usage\n>   2. Rich feature set with higher overhead\n>   3. Configurable features allowing runtime trade-offs\n> - **Decision**: Minimal feature set with clear extension points\n> - **Rationale**: Learning-focused implementation benefits from understanding core concepts deeply rather than surface coverage of many features. Resource efficiency makes the system practical for development environments with limited resources.\n> - **Consequences**: Some advanced features must be implemented as future extensions, but core system remains understandable and efficient\n\n### Explicit Non-Goals\n\n**Explicit non-goals** define capabilities that are deliberately excluded from this implementation to maintain focus on the core learning objectives. These exclusions prevent scope creep and ensure the system remains comprehensible for educational purposes.\n\n| Category | Excluded Feature | Rationale for Exclusion | Future Consideration |\n|----------|------------------|-------------------------|---------------------|\n| Persistence | Disk-based storage | Adds complexity of durability, crash recovery, and disk I/O management without contributing to distributed systems learning | Could be added as storage engine abstraction |\n| Security | Authentication/authorization | Security implementation would obscure core distributed caching concepts | Essential for production deployment |\n| Security | Encryption (in-transit/at-rest) | Cryptographic concerns distract from cache algorithm focus | Required for production security |\n| Advanced Features | Complex queries (range, pattern matching) | Transforms system from cache to database, different problem domain | Would require query execution engine |\n| Advanced Features | Transactions across keys | ACID semantics add significant complexity without cache-specific learning value | Major architectural change required |\n| Advanced Features | Compression | Implementation detail that doesn't illustrate distributed systems principles | Straightforward optimization to add later |\n| Operational Features | Metrics/monitoring dashboard | Important for operations but orthogonal to core caching algorithms | Standard observability patterns apply |\n| Operational Features | Configuration hot-reloading | Operational convenience that adds state management complexity | Useful operational enhancement |\n| Operational Features | Rolling upgrades | Production deployment concern beyond scope of algorithm implementation | Requires versioning and compatibility strategy |\n| Performance Features | Memory-mapped files | OS-specific optimization that complicates memory management learning | Platform-specific enhancement |\n| Performance Features | NUMA-aware allocation | Hardware-specific optimization beyond algorithmic focus | Performance tuning for specific deployments |\n| Network Features | Protocol buffers/advanced serialization | Serialization format choice doesn't affect distributed algorithm design | Easy to substitute different formats |\n| Network Features | Connection pooling/multiplexing | Network optimization orthogonal to cache logic | Standard networking optimization |\n| Consistency Features | Causal consistency | Advanced consistency model adds significant complexity | Research-level feature |\n| Consistency Features | Byzantine fault tolerance | Addresses malicious failures rather than crash failures common in cache deployments | Different threat model entirely |\n\n**Persistence exclusions** keep the focus on distributed coordination rather than storage engine implementation. Disk-based storage would require implementing write-ahead logging, crash recovery, data file management, and disk I/O optimization - each substantial topics that don't contribute to understanding consistent hashing, replication, or cluster membership management. Memory-only storage is sufficient for cache use cases and eliminates entire categories of failure modes and recovery scenarios.\n\n**Security exclusions** prevent cryptographic and access control concerns from obscuring the core distributed systems algorithms. Authentication would require implementing user management, credential verification, and secure session handling. Encryption would add key management, certificate handling, and performance considerations around cryptographic operations. While essential for production systems, these features can be layered onto the core cache implementation without affecting its fundamental design.\n\n> **Important Clarification:** Excluding security features doesn't mean ignoring security entirely. The system design should avoid patterns that would make security difficult to add later, such as exposing internal state through APIs or using communication patterns incompatible with encryption.\n\n**Advanced feature exclusions** maintain the focus on caching rather than expanding into general-purpose data storage. Complex queries would require implementing query parsing, execution planning, and result aggregation - transforming the system from a cache into a distributed database with entirely different performance characteristics and consistency requirements. Transactions across multiple keys would require distributed locking, deadlock detection, and two-phase commit protocols that deserve their own dedicated implementation and learning focus.\n\n**Operational feature exclusions** recognize that production deployment concerns, while important, are separate from the core algorithmic challenges of distributed caching. Metrics and monitoring can be added using standard observability libraries without affecting the cache implementation. Configuration hot-reloading would require implementing safe configuration validation and state migration. Rolling upgrades would need version compatibility checking and gradual deployment orchestration.\n\n**Performance optimization exclusions** distinguish between algorithmic performance (which is essential) and platform-specific optimizations (which are implementation details). Memory-mapped files, NUMA-aware allocation, and advanced serialization formats can improve performance but don't change the fundamental algorithms for consistent hashing, replication, or failure detection. These optimizations can be added later without architectural changes.\n\n**Advanced consistency feature exclusions** focus the implementation on the consistency models most relevant to caching workloads. Causal consistency would require implementing vector clocks across all operations and maintaining causal ordering, adding significant complexity for guarantees that cache applications rarely need. Byzantine fault tolerance addresses malicious node behavior rather than the crash failures and network partitions common in cache deployments.\n\n> **Design Principle:** Non-goals should be revisitable. The system architecture should not preclude adding excluded features in future iterations, but should prioritize making the included features robust and well-designed.\n\nThe exclusion of these features doesn't mean they're unimportant, but rather that they represent distinct learning domains that would dilute focus from the core distributed caching challenges. Each excluded feature could form the basis for its own educational implementation project with different architectural requirements and trade-offs.\n\n### Implementation Guidance\n\nThis section provides concrete technical recommendations for implementing the goals defined above, with focus on practical decisions that support the learning objectives.\n\n#### A. Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Recommendation |\n|-----------|---------------|-----------------|----------------|\n| Network Transport | HTTP REST with JSON (net/http) | gRPC with Protocol Buffers | HTTP for learning clarity |\n| Serialization | JSON encoding/decoding | MessagePack or Protocol Buffers | JSON for debugging ease |\n| Hashing Function | SHA-256 via crypto/sha256 | xxHash or CityHash | SHA-256 for reliability |\n| Configuration | JSON files with encoding/json | YAML with structured validation | JSON for simplicity |\n| Logging | Standard log package | Structured logging (logrus, zap) | Standard log initially |\n| Testing Framework | Built-in testing package | Testify with assertions | Built-in testing |\n| HTTP Client | Default http.Client | Custom client with retries | Default with timeout |\n| Concurrency | sync.Mutex and sync.RWMutex | Channels and select statements | Mutexes for data protection |\n\nThe **simple options** are recommended for initial implementation because they reduce the learning curve and make debugging easier. JSON serialization allows easy inspection of messages and configuration files. HTTP transport provides familiar REST semantics that are easy to test with standard tools like curl. Standard library packages minimize external dependencies and compilation complexity.\n\n#### B. Recommended File/Module Structure\n\n```\ndistributed-cache/\n├── cmd/\n│   └── cache-server/\n│       └── main.go                 ← Server entry point, configuration loading\n├── internal/\n│   ├── config/\n│   │   ├── config.go              ← NodeConfig type and validation\n│   │   └── config_test.go         ← Configuration testing\n│   ├── hash/\n│   │   ├── ring.go                ← HashRing implementation\n│   │   └── ring_test.go           ← Consistent hashing tests\n│   ├── cache/\n│   │   ├── lru.go                 ← LRUCache implementation\n│   │   ├── entry.go               ← CacheEntry type definition\n│   │   └── cache_test.go          ← Cache operation tests\n│   ├── transport/\n│   │   ├── http.go                ← HTTPTransport implementation\n│   │   ├── messages.go            ← Request/Response types\n│   │   └── transport_test.go      ← Network communication tests\n│   ├── gossip/\n│   │   ├── protocol.go            ← Gossip protocol implementation\n│   │   ├── membership.go          ← NodeState management\n│   │   └── gossip_test.go         ← Cluster membership tests\n│   └── node/\n│       ├── node.go                ← Main cache node orchestration\n│       └── node_test.go           ← Integration tests\n├── pkg/\n│   └── client/\n│       ├── client.go              ← Client library for testing\n│       └── client_test.go         ← Client integration tests\n├── configs/\n│   ├── node1.json                 ← Example node configurations\n│   ├── node2.json\n│   └── node3.json\n├── scripts/\n│   ├── start-cluster.sh           ← Development cluster startup\n│   └── test-operations.sh         ← Manual testing scripts\n└── README.md                      ← Setup and usage instructions\n```\n\nThis structure separates concerns clearly while maintaining the internal/ convention for implementation details that shouldn't be imported by external packages. The pkg/client/ directory provides a reusable client library for testing and potential external use.\n\n#### C. Infrastructure Starter Code\n\n**Configuration Management** - Complete implementation for loading and validating node configuration:\n\n```go\n// internal/config/config.go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"errors\"\n    \"os\"\n    \"time\"\n)\n\n// NodeConfig represents complete configuration for a cache node.\n// All fields are required unless noted otherwise.\ntype NodeConfig struct {\n    NodeID              string        `json:\"node_id\"`              // Unique identifier for this node\n    ListenAddress       string        `json:\"listen_address\"`       // Address to bind HTTP server (e.g., \":8080\")\n    AdvertiseAddr       string        `json:\"advertise_address\"`    // Address other nodes use to contact this node\n    JoinAddresses       []string      `json:\"join_addresses\"`       // Bootstrap addresses for cluster discovery\n    MaxMemoryMB         int           `json:\"max_memory_mb\"`        // Maximum memory usage in megabytes\n    VirtualNodes        int           `json:\"virtual_nodes\"`        // Number of virtual nodes for consistent hashing\n    ReplicationFactor   int           `json:\"replication_factor\"`   // Number of replicas for each key\n    HealthCheckInterval time.Duration `json:\"health_check_interval\"` // Frequency of health checks\n    GossipInterval      time.Duration `json:\"gossip_interval\"`      // Frequency of gossip messages\n    RequestTimeout      time.Duration `json:\"request_timeout\"`      // Timeout for inter-node requests\n}\n\n// LoadConfig reads configuration from JSON file and validates all required fields.\nfunc LoadConfig(filename string) (*NodeConfig, error) {\n    data, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, err\n    }\n    \n    var config NodeConfig\n    if err := json.Unmarshal(data, &config); err != nil {\n        return nil, err\n    }\n    \n    if err := config.Validate(); err != nil {\n        return nil, err\n    }\n    \n    return &config, nil\n}\n\n// Validate ensures all configuration values are reasonable and required fields are present.\nfunc (c *NodeConfig) Validate() error {\n    if c.NodeID == \"\" {\n        return errors.New(\"node_id is required\")\n    }\n    if c.ListenAddress == \"\" {\n        return errors.New(\"listen_address is required\")\n    }\n    if c.AdvertiseAddr == \"\" {\n        return errors.New(\"advertise_address is required\")\n    }\n    if c.MaxMemoryMB < 1 || c.MaxMemoryMB > 65536 {\n        return errors.New(\"max_memory_mb must be between 1 and 65536\")\n    }\n    if c.VirtualNodes < 1 || c.VirtualNodes > 1000 {\n        return errors.New(\"virtual_nodes must be between 1 and 1000\")\n    }\n    if c.ReplicationFactor < 1 || c.ReplicationFactor > 10 {\n        return errors.New(\"replication_factor must be between 1 and 10\")\n    }\n    if c.HealthCheckInterval < time.Second {\n        return errors.New(\"health_check_interval must be at least 1 second\")\n    }\n    if c.GossipInterval < time.Second {\n        return errors.New(\"gossip_interval must be at least 1 second\")\n    }\n    if c.RequestTimeout < 100*time.Millisecond {\n        return errors.New(\"request_timeout must be at least 100ms\")\n    }\n    return nil\n}\n```\n\n**Message Type Definitions** - Complete types for inter-node communication:\n\n```go\n// internal/transport/messages.go\npackage transport\n\nimport \"time\"\n\n// MessageType constants for different inter-node message types\nconst (\n    MessageTypeGossip    = \"gossip\"\n    MessageTypeGet       = \"get\"\n    MessageTypeSet       = \"set\"\n    MessageTypeDelete    = \"delete\"\n    MessageTypeReplicate = \"replicate\"\n)\n\n// Message represents the envelope for all inter-node communication.\ntype Message struct {\n    Type      string      `json:\"type\"`      // MessageType constant\n    Sender    string      `json:\"sender\"`    // Node ID of sender\n    Timestamp time.Time   `json:\"timestamp\"` // When message was created\n    Data      interface{} `json:\"data\"`      // Payload specific to message type\n}\n\n// Cache operation request/response types\ntype GetRequest struct {\n    Key              string `json:\"key\"`               // Key to retrieve\n    ConsistencyLevel string `json:\"consistency_level\"` // \"eventual\" or \"strong\"\n}\n\ntype GetResponse struct {\n    Key       string    `json:\"key\"`       // Requested key\n    Value     []byte    `json:\"value\"`     // Retrieved value (nil if not found)\n    Found     bool      `json:\"found\"`     // Whether key exists\n    Timestamp time.Time `json:\"timestamp\"` // When value was last modified\n}\n\ntype SetRequest struct {\n    Key              string        `json:\"key\"`               // Key to store\n    Value            []byte        `json:\"value\"`             // Value to store\n    TTL              time.Duration `json:\"ttl\"`               // Time-to-live (0 for no expiration)\n    ConsistencyLevel string        `json:\"consistency_level\"` // \"eventual\" or \"strong\"\n}\n\ntype SetResponse struct {\n    Success   bool      `json:\"success\"`   // Whether operation succeeded\n    Timestamp time.Time `json:\"timestamp\"` // When value was stored\n}\n\ntype DeleteRequest struct {\n    Key              string `json:\"key\"`               // Key to delete\n    ConsistencyLevel string `json:\"consistency_level\"` // \"eventual\" or \"strong\"\n}\n\ntype DeleteResponse struct {\n    Success bool      `json:\"success\"`   // Whether operation succeeded\n    Found   bool      `json:\"found\"`     // Whether key existed before deletion\n    Timestamp time.Time `json:\"timestamp\"` // When deletion occurred\n}\n```\n\n#### D. Core Logic Skeleton Code\n\n**Hash Ring Implementation** - Method signatures with detailed TODOs:\n\n```go\n// internal/hash/ring.go\npackage hash\n\n// HashRing implements consistent hashing with virtual nodes for even key distribution.\ntype HashRing struct {\n    virtualNodes int                 // Number of virtual nodes per physical node\n    ring         map[uint32]string   // Hash position -> node ID mapping\n    sortedKeys   []uint32            // Sorted hash positions for binary search\n    nodes        map[string]bool     // Set of active nodes for membership tracking\n}\n\n// NewHashRing creates a new consistent hash ring with specified virtual nodes per physical node.\nfunc NewHashRing(virtualNodes int) *HashRing {\n    // TODO 1: Initialize ring with empty maps and slices\n    // TODO 2: Store virtualNodes parameter for later use in AddNode\n    // TODO 3: Return pointer to new HashRing instance\n}\n\n// AddNode adds a physical node to the ring by creating virtual nodes at hash positions.\nfunc (hr *HashRing) AddNode(nodeID string) {\n    // TODO 1: Check if node already exists in hr.nodes map - if yes, return early\n    // TODO 2: Add nodeID to hr.nodes set\n    // TODO 3: Loop hr.virtualNodes times to create virtual nodes\n    // TODO 4: For each virtual node, create unique string like \"nodeID:virtualIndex\"\n    // TODO 5: Hash the virtual node string to get uint32 position\n    // TODO 6: Add position -> nodeID mapping to hr.ring\n    // TODO 7: Add position to hr.sortedKeys slice\n    // TODO 8: Sort hr.sortedKeys to maintain binary search property\n    // Hint: Use sort.Slice(hr.sortedKeys, func(i, j int) bool { return hr.sortedKeys[i] < hr.sortedKeys[j] })\n}\n\n// GetNode returns the node responsible for storing the given key using consistent hashing.\nfunc (hr *HashRing) GetNode(key string) string {\n    // TODO 1: Handle empty ring case - return empty string if no nodes\n    // TODO 2: Hash the key to get uint32 position\n    // TODO 3: Use binary search to find first hash position >= key position\n    // TODO 4: If no position found (key hashes beyond last position), wrap to first position  \n    // TODO 5: Look up node ID from hr.ring using the found position\n    // TODO 6: Return the node ID\n    // Hint: Use sort.Search for binary search on hr.sortedKeys\n}\n```\n\n**LRU Cache Implementation** - Core eviction logic skeleton:\n\n```go\n// internal/cache/lru.go\npackage cache\n\nimport (\n    \"container/list\"\n    \"sync\"\n    \"time\"\n)\n\n// LRUCache implements thread-safe LRU cache with memory limits and TTL support.\ntype LRUCache struct {\n    mutex    sync.RWMutex           // Protects all fields from concurrent access\n    capacity int64                 // Maximum memory usage in bytes\n    used     int64                 // Current memory usage in bytes\n    items    map[string]*list.Element // Key -> list element mapping for O(1) lookup\n    order    *list.List            // Doubly-linked list for LRU order tracking\n}\n\n// NewLRUCache creates a new LRU cache with specified memory capacity in bytes.\nfunc NewLRUCache(capacityBytes int64) *LRUCache {\n    // TODO 1: Create LRUCache instance with capacity set\n    // TODO 2: Initialize items map and order list\n    // TODO 3: Set used memory to 0\n    // TODO 4: Return pointer to cache instance\n}\n\n// Set stores a key-value pair with optional TTL, evicting entries if necessary to stay under memory limit.\nfunc (c *LRUCache) Set(key string, value []byte, ttl time.Duration) {\n    // TODO 1: Acquire write lock\n    // TODO 2: Calculate entry size (key length + value length + CacheEntry overhead)\n    // TODO 3: Check if key already exists - if yes, remove old entry from used memory count\n    // TODO 4: Create CacheEntry with key, value, size, and expiration time (now + ttl, or zero if no TTL)\n    // TODO 5: While (used + entry size > capacity) and items exist, call evictOldest()\n    // TODO 6: Create list element containing the CacheEntry\n    // TODO 7: Add element to front of order list (most recently used position)\n    // TODO 8: Add key -> element mapping to items map\n    // TODO 9: Add entry size to used memory counter\n    // TODO 10: Release write lock\n    // Hint: Use time.Time{} for zero value when no TTL specified\n}\n\n// evictOldest removes the least recently used entry from cache to free memory.\nfunc (c *LRUCache) evictOldest() {\n    // TODO 1: Get last element from order list (least recently used)\n    // TODO 2: Remove element from order list\n    // TODO 3: Extract CacheEntry from element\n    // TODO 4: Remove key from items map\n    // TODO 5: Subtract entry size from used memory counter\n    // Hint: This method should only be called while holding write lock\n}\n```\n\n#### E. Language-Specific Hints\n\n**Go-Specific Implementation Tips:**\n\n- Use `crypto/sha256` package for consistent hashing: `sha256.Sum256([]byte(input))` returns [32]byte array\n- Convert hash to uint32 using binary encoding: `binary.BigEndian.Uint32(hash[:4])`\n- Use `sort.Search` for binary search on sorted slices: returns insertion index if not found\n- Handle JSON marshaling of time.Duration by implementing custom MarshalJSON/UnmarshalJSON methods\n- Use `sync.RWMutex` for cache operations: RLock() for reads, Lock() for writes\n- Use `container/list` for LRU ordering: PushFront() for new items, MoveToFront() for access\n- Use `http.Server` with custom handler functions for REST API endpoints\n- Use `context.Context` with timeout for inter-node requests: `context.WithTimeout(context.Background(), timeout)`\n- Use `time.Ticker` for periodic tasks like gossip and health checks\n- Use buffered channels for graceful shutdown: `make(chan struct{}, 1)` for shutdown signals\n\n**Memory Management Tips:**\n\n- Estimate CacheEntry overhead as ~100 bytes (struct fields, map overhead, list element)\n- Use `unsafe.Sizeof()` during development to measure actual struct sizes\n- Track memory usage incrementally rather than recalculating from scratch\n- Consider string interning for repeated keys to reduce memory usage\n- Use `runtime.GC()` and `runtime.ReadMemStats()` for memory pressure testing\n\n**Concurrency Patterns:**\n\n- Protect hash ring modifications with mutex during node additions/removals\n- Use read locks for cache lookups, write locks for modifications\n- Implement graceful shutdown by closing channels and waiting for goroutines\n- Use atomic operations (`sync/atomic`) for simple counters like request metrics\n- Avoid nested locks by carefully ordering lock acquisition\n\n#### F. Milestone Checkpoints\n\n**After implementing each milestone, verify these specific behaviors:**\n\n**Milestone 1 Checkpoint - Consistent Hash Ring:**\n```bash\ngo test ./internal/hash/... -v\n```\nExpected output should show:\n- Hash ring distributes 10,000 random keys evenly across 5 nodes (each node gets 15-25% of keys)\n- Adding 6th node redistributes only ~17% of existing keys (1/6 of total)\n- Removing 1 node redistributes only that node's keys to successors\n- Key lookup returns same node consistently for same key\n\nManual verification:\n```bash\ngo run ./cmd/hash-test/ -nodes=5 -keys=10000 -virtual-nodes=64\n```\nShould output distribution statistics showing balanced key assignment.\n\n**Milestone 2 Checkpoint - Cache Node:**\n```bash\ngo test ./internal/cache/... -v\n```\nExpected behaviors:\n- LRU eviction removes oldest entries when memory limit exceeded\n- TTL expiration automatically removes expired entries during cleanup\n- Concurrent access doesn't cause data races or panics\n- Memory accounting accurately tracks total usage\n\nManual verification:\n```bash\ncurl -X POST http://localhost:8080/set -d '{\"key\":\"test\",\"value\":\"aGVsbG8=\"}'  # Set key\ncurl http://localhost:8080/get/test  # Retrieve key\ncurl -X DELETE http://localhost:8080/delete/test  # Delete key\n```\n\n**Signs of problems and debugging:**\n- \"Uneven distribution\" → Check virtual node count (increase to 64-256)\n- \"Memory leaks\" → Verify evictOldest() actually removes from all data structures\n- \"Race conditions\" → Run with `go test -race` flag to detect data races\n- \"TTL not working\" → Check that CleanupExpired() runs periodically\n\nThis implementation guidance provides a complete foundation for building the distributed cache while ensuring learners focus on the core algorithmic challenges rather than infrastructure setup complexity.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** This section provides the architectural foundation for all milestones, with particular relevance to Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication & Consistency).\n\nThe high-level architecture of our distributed cache system follows a **peer-to-peer cluster design** where each node is both a cache server and a cluster participant. Think of this like a neighborhood watch system where every household both protects their own property and coordinates with neighbors to maintain security across the entire neighborhood. Each cache node stores data locally while participating in cluster-wide decisions about membership, routing, and replication.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\nThe architecture embraces the principle of **symmetric responsibility** - there are no special coordinator nodes or single points of failure. Every node can handle client requests, participate in gossip communication, and store cached data. This design choice provides inherent fault tolerance but requires sophisticated coordination mechanisms to maintain consistency and handle cluster membership changes.\n\n### Component Overview\n\nThe distributed cache system consists of five primary components that work together to provide a scalable, fault-tolerant caching service. Each component has distinct responsibilities but must coordinate closely with others to maintain system correctness.\n\n#### Cache Node Component\n\nThe **Cache Node** serves as the fundamental storage unit of the distributed cache, analogous to a smart librarian who not only manages their own collection of books but also knows exactly which other librarians have specific titles. Each cache node implements an LRU cache with TTL support, handling local storage operations while participating in the broader cluster ecosystem.\n\n| Responsibility | Description | Key Interactions |\n|---|---|---|\n| Local Data Storage | Maintains LRU cache with configurable memory limits | Interfaces with LRUCache for all storage operations |\n| TTL Management | Automatically expires entries after time-to-live | Runs background cleanup processes, coordinates with request handlers |\n| Request Processing | Handles GET/SET/DELETE operations from clients and peers | Routes requests through Hash Ring, coordinates with replication logic |\n| Memory Management | Enforces capacity limits through LRU eviction | Monitors memory usage, triggers eviction when necessary |\n| Health Monitoring | Exposes health status for cluster monitoring | Responds to health checks, reports metrics to gossip protocol |\n\nThe Cache Node must handle concurrent operations safely while maintaining LRU ordering and TTL accuracy. It serves as both the primary interface for client requests and a participant in inter-node replication protocols.\n\n#### Hash Ring Component\n\nThe **Hash Ring** implements consistent hashing to determine key placement across cluster nodes, functioning like a circular seating chart that automatically adjusts when guests arrive or leave while minimizing the number of people who need to change seats. This component is crucial for maintaining balanced load distribution and minimizing data movement during cluster membership changes.\n\n| Responsibility | Description | Key Operations |\n|---|---|---|\n| Key Distribution | Maps cache keys to responsible nodes using consistent hashing | `GetNode(key)` returns primary node, `GetNodes(key, count)` returns replica set |\n| Virtual Node Management | Maintains multiple hash positions per physical node | `AddNode(nodeID)` creates virtual nodes, `RemoveNode(nodeID)` cleans up positions |\n| Load Balancing | Distributes keys evenly across available nodes | Uses virtual nodes to reduce hotspots and improve distribution |\n| Membership Updates | Handles node addition and removal with minimal key redistribution | Rebalances ring while preserving existing key assignments where possible |\n| Routing Logic | Provides fast lookup for key-to-node mappings | Maintains sorted key list for O(log n) lookup performance |\n\nThe Hash Ring must handle concurrent membership changes while providing consistent routing decisions. It serves as the authoritative source for determining which nodes should store replicas of any given key.\n\n#### Gossip Protocol Component\n\nThe **Gossip Protocol** manages cluster membership and state propagation, operating like a neighborhood information network where residents periodically share news with their neighbors, ensuring that important information eventually reaches everyone even if some residents are temporarily unavailable. This component enables the cluster to maintain consistent membership views without requiring a central coordinator.\n\n| Responsibility | Description | Key Mechanisms |\n|---|---|---|\n| Node Discovery | Helps new nodes join the cluster and announce their presence | Exchanges node lists during gossip rounds, maintains bootstrap addresses |\n| Failure Detection | Identifies unresponsive nodes and propagates failure information | Combines direct health checks with gossip-based failure detection |\n| State Synchronization | Ensures all nodes have consistent views of cluster membership | Spreads node state updates using epidemic-style propagation |\n| Network Partition Handling | Maintains cluster coherence during network splits | Uses version vectors and conflict resolution for state merging |\n| Convergence Management | Ensures cluster state eventually converges across all nodes | Implements anti-entropy mechanisms to repair divergent state |\n\nThe Gossip Protocol must balance information freshness with network overhead, providing timely failure detection while avoiding excessive chattiness that could overwhelm the network.\n\n#### Transport Layer Component\n\nThe **Transport Layer** handles all inter-node communication, serving as the postal service of the distributed cache that reliably delivers messages between nodes while handling addressing, serialization, and connection management. This component abstracts network complexity from higher-level protocols.\n\n| Responsibility | Description | Implementation Details |\n|---|---|---|\n| Message Serialization | Converts protocol messages to wire format | Uses JSON for human readability, supports binary protocols for performance |\n| Connection Management | Maintains persistent connections between frequently communicating nodes | Implements connection pooling, handles reconnection after failures |\n| Request Routing | Delivers messages to correct destination nodes | Supports both point-to-point and broadcast communication patterns |\n| Error Handling | Manages network failures and timeout scenarios | Implements retry logic, circuit breakers for failing nodes |\n| Protocol Support | Provides unified interface for different message types | Supports gossip messages, cache operations, replication requests |\n\nThe Transport Layer must handle network partitions gracefully while providing reliable message delivery guarantees for critical cluster operations.\n\n#### Client Router Component\n\nThe **Client Router** serves as the primary interface between external clients and the distributed cache cluster, acting like a knowledgeable concierge who directs visitors to the right department while handling special requests and managing visitor experience. This component provides a unified API that abstracts cluster complexity from client applications.\n\n| Responsibility | Description | Client Benefits |\n|---|---|---|\n| Request Distribution | Routes client operations to appropriate cluster nodes | Clients see single cache interface regardless of cluster topology |\n| Load Balancing | Distributes client load across healthy cluster members | Prevents hotspots, improves overall cluster utilization |\n| Consistency Management | Enforces consistency levels for read and write operations | Supports configurable consistency from eventual to strong |\n| Failure Handling | Manages retries and fallbacks when nodes are unavailable | Provides high availability despite individual node failures |\n| Protocol Translation | Converts client API calls to internal cluster protocols | Maintains stable client API while allowing internal protocol evolution |\n\nThe Client Router must maintain high availability and low latency while providing consistent behavior even during cluster membership changes or partial failures.\n\n> **Key Architectural Insight**: The peer-to-peer design eliminates single points of failure but requires every component to handle partial information and eventual consistency. Each component must be designed with the assumption that other components might be temporarily unreachable or operating with stale information.\n\n### Recommended Module Structure\n\nThe codebase organization follows a **domain-driven structure** that groups related functionality while maintaining clear separation of concerns. This structure supports incremental development where each milestone builds upon previous components without requiring major refactoring.\n\n```\ndistributed-cache/\n├── cmd/\n│   ├── cache-node/\n│   │   └── main.go                    # Node entry point and CLI handling\n│   └── cache-client/\n│       └── main.go                    # Example client implementation\n├── internal/\n│   ├── hashring/\n│   │   ├── hashring.go               # HashRing implementation\n│   │   ├── virtual_nodes.go          # Virtual node management\n│   │   └── hashring_test.go          # Consistent hashing tests\n│   ├── cache/\n│   │   ├── lru_cache.go              # LRUCache implementation\n│   │   ├── cache_entry.go            # CacheEntry and TTL logic\n│   │   ├── memory_manager.go         # Memory accounting and limits\n│   │   └── cache_test.go             # Cache behavior tests\n│   ├── transport/\n│   │   ├── http_transport.go         # HTTPTransport implementation\n│   │   ├── messages.go               # Message types and serialization\n│   │   └── transport_test.go         # Network communication tests\n│   ├── gossip/\n│   │   ├── protocol.go               # Gossip protocol implementation\n│   │   ├── membership.go             # Node membership management\n│   │   ├── failure_detector.go       # Health checking and failure detection\n│   │   └── gossip_test.go            # Cluster membership tests\n│   ├── replication/\n│   │   ├── replicator.go             # Data replication logic\n│   │   ├── quorum.go                 # Quorum-based operations\n│   │   ├── conflict_resolver.go      # Conflict resolution strategies\n│   │   └── replication_test.go       # Replication and consistency tests\n│   └── node/\n│       ├── cache_node.go             # Main CacheNode orchestration\n│       ├── request_handler.go        # Client request processing\n│       ├── config.go                 # NodeConfig and configuration\n│       └── node_test.go              # Integration tests\n├── pkg/\n│   ├── api/\n│   │   ├── client.go                 # Public client library\n│   │   └── types.go                  # Public API types\n│   └── config/\n│       └── validation.go             # Configuration validation utilities\n├── configs/\n│   ├── node1.json                    # Example node configurations\n│   ├── node2.json\n│   └── cluster.json                  # Cluster-wide settings\n└── scripts/\n    ├── start-cluster.sh              # Development cluster startup\n    └── test-integration.sh           # Integration test runner\n```\n\n> **Decision: Internal vs Public Package Structure**\n> - **Context**: Go projects typically use `internal/` for private packages and `pkg/` for public APIs, but the boundaries affect how components can be tested and reused\n> - **Options Considered**: \n>   1. Everything in `pkg/` for maximum flexibility\n>   2. Everything in `internal/` for maximum encapsulation\n>   3. Core logic in `internal/`, stable APIs in `pkg/`\n> - **Decision**: Core implementation in `internal/`, public client APIs in `pkg/`\n> - **Rationale**: This prevents external dependencies on unstable internals while providing clean client libraries. The `internal/` boundary forces us to design proper interfaces between components.\n> - **Consequences**: Components can evolve independently, but inter-component communication must go through well-defined interfaces rather than direct struct access.\n\nThe module structure supports **dependency injection** patterns where each component receives its dependencies through constructor functions rather than creating them directly. This design enables comprehensive testing and supports different deployment configurations.\n\n| Package | Primary Types | Dependencies | Testing Strategy |\n|---|---|---|---|\n| `internal/hashring` | `HashRing` | None (pure algorithm) | Unit tests with synthetic node sets |\n| `internal/cache` | `LRUCache`, `CacheEntry` | None (pure storage) | Unit tests with memory pressure scenarios |\n| `internal/transport` | `HTTPTransport`, `Message` | `net/http`, `encoding/json` | Unit tests with mock servers |\n| `internal/gossip` | `GossipProtocol`, `NodeState` | `transport`, `time` | Integration tests with multiple nodes |\n| `internal/replication` | `Replicator`, `QuorumConfig` | `hashring`, `transport` | Integration tests with failure injection |\n| `internal/node` | `CacheNode`, `NodeConfig` | All other internal packages | End-to-end tests with real clusters |\n\n### Communication Patterns\n\nThe distributed cache employs three distinct communication patterns, each optimized for different types of interactions and consistency requirements. These patterns work together to provide both high performance for common operations and strong consistency for critical cluster management tasks.\n\n#### Client-Server Request-Response Pattern\n\n**Client requests** follow a synchronous request-response pattern where external clients communicate with any cluster node to perform cache operations. Think of this like visiting any branch of a bank - you can perform most transactions at any location, and the branch handles the complexity of accessing your account information regardless of where it's stored.\n\nThe request flow follows this sequence:\n\n1. **Client Connection**: Client establishes HTTP connection to any available cluster node\n2. **Request Validation**: Receiving node validates request format and authentication\n3. **Key Routing**: Node uses `HashRing.GetNode(key)` to determine responsible node(s)\n4. **Local vs Remote Handling**: If current node owns the key, handle locally; otherwise forward to correct node\n5. **Replication Coordination**: For write operations, coordinate with replica nodes based on replication factor\n6. **Response Assembly**: Collect responses from required replicas and return unified result to client\n\n| Message Type | Request Fields | Response Fields | Routing Logic |\n|---|---|---|---|\n| GET | `Key`, `ConsistencyLevel` | `Value`, `Found`, `Timestamp` | Route to primary, query replicas if strong consistency |\n| SET | `Key`, `Value`, `TTL`, `ConsistencyLevel` | `Success`, `Timestamp` | Route to primary and R replicas based on replication factor |\n| DELETE | `Key`, `ConsistencyLevel` | `Success`, `Found`, `Timestamp` | Route to all replicas, require quorum confirmation |\n\nThe request-response pattern provides **strong consistency guarantees** when required while allowing **eventual consistency** for performance-critical operations. Client libraries can specify consistency levels per operation, trading latency for consistency based on application requirements.\n\n#### Peer-to-Peer Gossip Pattern\n\n**Gossip communication** uses an asynchronous, epidemic-style protocol where nodes periodically exchange cluster state information with randomly selected peers. This pattern resembles how news spreads in a small town - residents occasionally chat with neighbors, sharing recent updates, and important information eventually reaches everyone even if not everyone talks to everyone else directly.\n\nThe gossip cycle operates on a fixed interval:\n\n1. **Peer Selection**: Each node randomly selects 1-3 peers for gossip exchange\n2. **State Preparation**: Node prepares `GossipMessage` containing local view of cluster membership\n3. **Message Exchange**: Nodes exchange gossip messages bidirectionally \n4. **State Merging**: Each node merges received state with local state, resolving conflicts\n5. **Change Detection**: Nodes detect membership changes and trigger appropriate actions\n6. **Convergence Check**: Periodic verification that cluster state has converged across all nodes\n\n| Gossip Phase | Information Exchanged | Conflict Resolution | Side Effects |\n|---|---|---|---|\n| Push Phase | Local node states, version vectors | Newer version wins, merge concurrent updates | Update local membership view |\n| Pull Phase | Request for specific node information | Compare timestamps and versions | Detect missing or stale information |\n| Anti-Entropy | Full state synchronization | Vector clock comparison | Repair divergent cluster views |\n\nThe gossip pattern provides **eventual consistency** for cluster membership while being highly resilient to network partitions and node failures. Information propagates logarithmically - even in a 1000-node cluster, updates typically reach all nodes within a few gossip rounds.\n\n> **Critical Design Insight**: Gossip intervals must be tuned carefully. Too frequent gossip creates network overhead; too infrequent gossip delays failure detection. The sweet spot is typically 1-5 seconds for most clusters, adjusted based on cluster size and network characteristics.\n\n#### Replication Coordination Pattern\n\n**Replication messages** use a coordinated protocol where one node acts as coordinator for a specific operation, ensuring consistency across multiple replicas. This resembles a meeting facilitator who ensures all participants have their say and records the group's decision - the coordinator doesn't dictate the outcome but ensures the process follows agreed-upon rules.\n\nThe coordination sequence varies by operation type:\n\n**Write Coordination (SET operations):**\n1. **Coordinator Selection**: Client request receiver becomes coordinator for this operation\n2. **Replica Identification**: Use `HashRing.GetNodes(key, replicationFactor)` to find replica nodes\n3. **Prepare Phase**: Send `SetRequest` to all replica nodes with operation details\n4. **Vote Collection**: Collect success/failure responses from replicas within timeout\n5. **Quorum Check**: Verify that sufficient replicas (W nodes) confirmed the operation\n6. **Client Response**: Return success if write quorum achieved, failure otherwise\n7. **Background Repair**: If some replicas failed, schedule repair operation\n\n**Read Coordination (GET operations):**\n1. **Coordinator Selection**: Request receiver coordinates read operation\n2. **Consistency Level Check**: Determine if eventual or strong consistency required\n3. **Replica Query**: For strong consistency, query R replicas; for eventual, query 1-2 replicas\n4. **Response Reconciliation**: If multiple responses differ, apply conflict resolution\n5. **Client Response**: Return most recent value based on timestamp or vector clock\n6. **Read Repair**: If inconsistencies detected, update stale replicas in background\n\n| Consistency Level | Read Quorum (R) | Write Quorum (W) | Guarantees | Trade-offs |\n|---|---|---|---|---|\n| Eventual | 1 | 1 | High availability, eventual consistency | Possible stale reads |\n| Strong | N/2 + 1 | N/2 + 1 | Linearizable consistency | Higher latency, lower availability |\n| Quorum | Configurable | Configurable | Tunable consistency-availability balance | Requires R + W > N for consistency |\n\nThe replication pattern enables **configurable consistency levels** while maintaining high availability during partial failures. Applications can choose appropriate consistency levels per operation based on their specific requirements.\n\n> **Architectural Trade-off**: The three communication patterns operate independently but must be carefully coordinated. For example, gossip protocol changes to cluster membership must be synchronized with hash ring updates to prevent inconsistent routing decisions during replication operations.\n\n**Common Pitfalls:**\n\n⚠️ **Pitfall: Blocking on Synchronous Operations During Gossip**\nMany implementations make the mistake of performing expensive operations like disk I/O or network calls during gossip message processing, causing gossip rounds to exceed their target intervals. This creates a cascading effect where delayed gossip leads to slower failure detection and potentially stale membership information. The fix is to process gossip messages asynchronously - update local state immediately but defer expensive operations like hash ring rebuilding to background tasks.\n\n⚠️ **Pitfall: Inconsistent Consistency Level Handling**\nA common error is assuming that consistency levels apply uniformly across all operations, leading to situations where a write at \"eventual\" consistency is immediately followed by a read at \"strong\" consistency, creating confusing behavior for clients. The solution is to clearly document consistency semantics and provide client libraries that make consistency choices explicit and prevent invalid combinations.\n\n⚠️ **Pitfall: Ignoring Message Ordering in Replication**\nDevelopers often forget that network messages can arrive out of order, leading to scenarios where a DELETE operation arrives before the corresponding SET operation, causing replicas to diverge. This is particularly problematic during network partitions or high load. The fix is to include vector clocks or Lamport timestamps in all replication messages and apply operations in causal order rather than arrival order.\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the distributed cache architecture, including technology choices, starter code, and development checkpoints.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Rationale |\n|---|---|---|---|\n| HTTP Transport | `net/http` with JSON | gRPC with Protocol Buffers | HTTP/JSON easier to debug, gRPC better performance |\n| Serialization | `encoding/json` | `encoding/gob` or Protocol Buffers | JSON human-readable, binary formats more efficient |\n| Hashing | `crypto/sha256` | `github.com/cespare/xxhash` | SHA-256 built-in and secure, xxhash faster for non-crypto use |\n| Configuration | JSON files | YAML with `gopkg.in/yaml.v3` | JSON simpler, YAML more readable for complex configs |\n| Logging | `log/slog` (Go 1.21+) | `github.com/rs/zerolog` | slog is standard library, zerolog has better performance |\n| Testing | `testing` package | `github.com/stretchr/testify` | Built-in package sufficient, testify adds convenience |\n\nFor this implementation, we recommend starting with the simple options to reduce external dependencies and complexity. Advanced options can be adopted later as performance requirements become clear.\n\n#### Starter Infrastructure Code\n\n**Configuration Management (`internal/node/config.go`):**\n\n```go\npackage node\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"time\"\n)\n\n// NodeConfig contains all configuration for a cache node\ntype NodeConfig struct {\n\tNodeID              string        `json:\"node_id\"`\n\tListenAddress       string        `json:\"listen_address\"`\n\tAdvertiseAddr       string        `json:\"advertise_address\"`\n\tJoinAddresses       []string      `json:\"join_addresses\"`\n\tMaxMemoryMB         int           `json:\"max_memory_mb\"`\n\tVirtualNodes        int           `json:\"virtual_nodes\"`\n\tReplicationFactor   int           `json:\"replication_factor\"`\n\tHealthCheckInterval time.Duration `json:\"health_check_interval\"`\n\tGossipInterval      time.Duration `json:\"gossip_interval\"`\n\tRequestTimeout      time.Duration `json:\"request_timeout\"`\n}\n\n// LoadConfig reads configuration from JSON file\nfunc LoadConfig(filename string) (*NodeConfig, error) {\n\tdata, err := os.ReadFile(filename)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read config file: %w\", err)\n\t}\n\n\tvar config NodeConfig\n\tif err := json.Unmarshal(data, &config); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to parse config JSON: %w\", err)\n\t}\n\n\tif err := config.Validate(); err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid configuration: %w\", err)\n\t}\n\n\treturn &config, nil\n}\n\n// Validate checks configuration values for correctness\nfunc (c *NodeConfig) Validate() error {\n\tif c.NodeID == \"\" {\n\t\treturn fmt.Errorf(\"node_id cannot be empty\")\n\t}\n\tif c.ListenAddress == \"\" {\n\t\treturn fmt.Errorf(\"listen_address cannot be empty\")\n\t}\n\tif c.MaxMemoryMB <= 0 {\n\t\treturn fmt.Errorf(\"max_memory_mb must be positive\")\n\t}\n\tif c.VirtualNodes <= 0 {\n\t\tc.VirtualNodes = 150 // reasonable default\n\t}\n\tif c.ReplicationFactor <= 0 {\n\t\tc.ReplicationFactor = 3 // reasonable default\n\t}\n\tif c.HealthCheckInterval <= 0 {\n\t\tc.HealthCheckInterval = 5 * time.Second\n\t}\n\tif c.GossipInterval <= 0 {\n\t\tc.GossipInterval = 2 * time.Second\n\t}\n\tif c.RequestTimeout <= 0 {\n\t\tc.RequestTimeout = 10 * time.Second\n\t}\n\treturn nil\n}\n```\n\n**HTTP Transport Layer (`internal/transport/http_transport.go`):**\n\n```go\npackage transport\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"time\"\n)\n\n// HTTPTransport handles HTTP communication between nodes\ntype HTTPTransport struct {\n\tclient  *http.Client\n\ttimeout time.Duration\n}\n\n// NewHTTPTransport creates HTTP transport with specified timeout\nfunc NewHTTPTransport(timeout time.Duration) *HTTPTransport {\n\treturn &HTTPTransport{\n\t\tclient: &http.Client{\n\t\t\tTimeout: timeout,\n\t\t},\n\t\ttimeout: timeout,\n\t}\n}\n\n// SendMessage sends JSON message to remote node and returns response\nfunc (t *HTTPTransport) SendMessage(ctx context.Context, address string, message interface{}) ([]byte, error) {\n\tjsonData, err := json.Marshal(message)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to marshal message: %w\", err)\n\t}\n\n\turl := fmt.Sprintf(\"http://%s/api/message\", address)\n\treq, err := http.NewRequestWithContext(ctx, \"POST\", url, bytes.NewBuffer(jsonData))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create request: %w\", err)\n\t}\n\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\n\tresp, err := t.client.Do(req)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to send request: %w\", err)\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, fmt.Errorf(\"request failed with status %d\", resp.StatusCode)\n\t}\n\n\tbody, err := io.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read response: %w\", err)\n\t}\n\n\treturn body, nil\n}\n\n// HealthCheck performs health check against remote node\nfunc (t *HTTPTransport) HealthCheck(ctx context.Context, address string) error {\n\turl := fmt.Sprintf(\"http://%s/health\", address)\n\treq, err := http.NewRequestWithContext(ctx, \"GET\", url, nil)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create health check request: %w\", err)\n\t}\n\n\tresp, err := t.client.Do(req)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"health check request failed: %w\", err)\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"node unhealthy: status %d\", resp.StatusCode)\n\t}\n\n\treturn nil\n}\n```\n\n**Message Types (`internal/transport/messages.go`):**\n\n```go\npackage transport\n\nimport \"time\"\n\n// Message types for inter-node communication\nconst (\n\tMessageTypeGossip    = \"gossip\"\n\tMessageTypeGet       = \"get\"\n\tMessageTypeSet       = \"set\"\n\tMessageTypeDelete    = \"delete\"\n\tMessageTypeReplicate = \"replicate\"\n)\n\n// Message is the base message structure for node communication\ntype Message struct {\n\tType      string      `json:\"type\"`\n\tSender    string      `json:\"sender\"`\n\tTimestamp time.Time   `json:\"timestamp\"`\n\tData      interface{} `json:\"data\"`\n}\n\n// Cache operation messages\ntype GetRequest struct {\n\tKey              string `json:\"key\"`\n\tConsistencyLevel string `json:\"consistency_level\"`\n}\n\ntype GetResponse struct {\n\tKey       string    `json:\"key\"`\n\tValue     []byte    `json:\"value\"`\n\tFound     bool      `json:\"found\"`\n\tTimestamp time.Time `json:\"timestamp\"`\n}\n\ntype SetRequest struct {\n\tKey              string        `json:\"key\"`\n\tValue            []byte        `json:\"value\"`\n\tTTL              time.Duration `json:\"ttl\"`\n\tConsistencyLevel string        `json:\"consistency_level\"`\n}\n\ntype SetResponse struct {\n\tSuccess   bool      `json:\"success\"`\n\tTimestamp time.Time `json:\"timestamp\"`\n}\n\ntype DeleteRequest struct {\n\tKey              string `json:\"key\"`\n\tConsistencyLevel string `json:\"consistency_level\"`\n}\n\ntype DeleteResponse struct {\n\tSuccess   bool      `json:\"success\"`\n\tFound     bool      `json:\"found\"`\n\tTimestamp time.Time `json:\"timestamp\"`\n}\n\n// Gossip protocol messages\ntype NodeState struct {\n\tNodeID   string    `json:\"node_id\"`\n\tAddress  string    `json:\"address\"`\n\tStatus   string    `json:\"status\"`\n\tLastSeen time.Time `json:\"last_seen\"`\n\tVersion  uint64    `json:\"version\"`\n}\n\ntype GossipMessage struct {\n\tNodeStates map[string]NodeState `json:\"node_states\"`\n\tVersion    uint64               `json:\"version\"`\n}\n```\n\n#### Core Component Skeletons\n\n**Hash Ring Core Logic (`internal/hashring/hashring.go`):**\n\n```go\npackage hashring\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"sort\"\n\t\"sync\"\n)\n\n// HashRing implements consistent hashing with virtual nodes\ntype HashRing struct {\n\tmutex        sync.RWMutex\n\tvirtualNodes int\n\tring         map[uint32]string // hash -> node ID\n\tsortedKeys   []uint32          // sorted hash values\n\tnodes        map[string]bool   // active nodes\n}\n\n// NewHashRing creates new consistent hash ring\nfunc NewHashRing(virtualNodes int) *HashRing {\n\t// TODO: Initialize HashRing struct with provided virtual node count\n\t// TODO: Create maps for ring, nodes, and slice for sortedKeys\n\t// TODO: Set mutex and virtualNodes field\n}\n\n// AddNode adds node with virtual nodes to ring\nfunc (hr *HashRing) AddNode(nodeID string) {\n\thr.mutex.Lock()\n\tdefer hr.mutex.Unlock()\n\t\n\t// TODO 1: Check if node already exists in hr.nodes map\n\t// TODO 2: Add nodeID to hr.nodes map\n\t// TODO 3: For i := 0 to hr.virtualNodes, create virtual node key\n\t// TODO 4: Hash virtual node key to get position on ring\n\t// TODO 5: Add hash->nodeID mapping to hr.ring\n\t// TODO 6: Rebuild hr.sortedKeys from hr.ring keys\n\t// TODO 7: Sort hr.sortedKeys for binary search\n\t// Hint: Use fmt.Sprintf(\"%s:%d\", nodeID, i) for virtual node keys\n\t// Hint: Use hashKey() helper function for consistent hashing\n}\n\n// RemoveNode removes node and virtual nodes from ring\nfunc (hr *HashRing) RemoveNode(nodeID string) {\n\thr.mutex.Lock()\n\tdefer hr.mutex.Unlock()\n\t\n\t// TODO 1: Check if node exists in hr.nodes map\n\t// TODO 2: Remove nodeID from hr.nodes map  \n\t// TODO 3: For i := 0 to hr.virtualNodes, create virtual node key\n\t// TODO 4: Hash virtual node key to get position\n\t// TODO 5: Delete hash from hr.ring map\n\t// TODO 6: Rebuild hr.sortedKeys from remaining hr.ring keys\n\t// TODO 7: Sort hr.sortedKeys\n}\n\n// GetNode returns node responsible for key\nfunc (hr *HashRing) GetNode(key string) string {\n\thr.mutex.RLock()\n\tdefer hr.mutex.RUnlock()\n\t\n\t// TODO 1: Check if ring is empty, return empty string\n\t// TODO 2: Hash the key to get position on ring\n\t// TODO 3: Use binary search on hr.sortedKeys to find first key >= hash\n\t// TODO 4: If no key found, wrap around to first key (ring property)\n\t// TODO 5: Return hr.ring[foundKey] to get node ID\n\t// Hint: Use sort.Search() for binary search\n\t// Hint: Handle wrap-around case when hash > all sortedKeys\n}\n\n// GetNodes returns N nodes for replication (starting with primary)\nfunc (hr *HashRing) GetNodes(key string, count int) []string {\n\thr.mutex.RLock()\n\tdefer hr.mutex.RUnlock()\n\t\n\t// TODO 1: Get primary node using GetNode(key)\n\t// TODO 2: Find starting position in hr.sortedKeys\n\t// TODO 3: Walk clockwise around ring collecting unique nodes\n\t// TODO 4: Handle wrap-around when reaching end of sortedKeys\n\t// TODO 5: Stop when collected 'count' unique nodes or exhausted all nodes\n\t// TODO 6: Return slice of collected node IDs\n\t// Hint: Use map[string]bool to track already-added nodes\n}\n\n// hashKey converts string to uint32 hash for ring positioning\nfunc hashKey(key string) uint32 {\n\th := sha256.Sum256([]byte(key))\n\treturn uint32(h[0])<<24 | uint32(h[1])<<16 | uint32(h[2])<<8 | uint32(h[3])\n}\n```\n\n**LRU Cache Core Logic (`internal/cache/lru_cache.go`):**\n\n```go\npackage cache\n\nimport (\n\t\"container/list\"\n\t\"sync\"\n\t\"time\"\n)\n\n// CacheEntry represents a cached key-value pair with metadata\ntype CacheEntry struct {\n\tKey       string    `json:\"key\"`\n\tValue     []byte    `json:\"value\"`\n\tExpiresAt time.Time `json:\"expires_at\"`\n\tSize      int64     `json:\"size\"`\n}\n\n// LRUCache implements LRU eviction with TTL support\ntype LRUCache struct {\n\tmutex    sync.RWMutex\n\tcapacity int64                    // max memory in bytes\n\tused     int64                    // current memory usage\n\titems    map[string]*list.Element // key -> list element\n\torder    *list.List               // LRU order (front = most recent)\n}\n\n// NewLRUCache creates LRU cache with memory limit\nfunc NewLRUCache(capacityBytes int64) *LRUCache {\n\t// TODO: Initialize LRUCache struct with provided capacity\n\t// TODO: Create items map and order list\n\t// TODO: Set used to 0 and mutex\n}\n\n// Get retrieves value and updates LRU order\nfunc (c *LRUCache) Get(key string) ([]byte, bool) {\n\tc.mutex.Lock()\n\tdefer c.mutex.Unlock()\n\t\n\t// TODO 1: Look up key in c.items map\n\t// TODO 2: If not found, return nil, false\n\t// TODO 3: Get CacheEntry from list element\n\t// TODO 4: Check if entry has expired using time.Now()\n\t// TODO 5: If expired, remove entry and return nil, false\n\t// TODO 6: If valid, move element to front of c.order list\n\t// TODO 7: Return entry.Value, true\n\t// Hint: Use c.order.MoveToFront() to update LRU order\n\t// Hint: Call c.removeElement() helper for cleanup\n}\n\n// Set stores value with optional TTL\nfunc (c *LRUCache) Set(key string, value []byte, ttl time.Duration) {\n\tc.mutex.Lock()\n\tdefer c.mutex.Unlock()\n\t\n\t// TODO 1: Calculate entry size: len(key) + len(value) + constant overhead\n\t// TODO 2: Create CacheEntry with key, value, size\n\t// TODO 3: Set ExpiresAt: time.Now().Add(ttl) if ttl > 0, else zero time\n\t// TODO 4: If key exists, update existing entry and move to front\n\t// TODO 5: If new key, create list element and add to front\n\t// TODO 6: Update c.items map and c.used memory counter\n\t// TODO 7: While c.used > c.capacity, evict from back of list\n\t// Hint: Use c.order.PushFront() for new entries\n\t// Hint: Call c.evictLRU() helper until under capacity\n}\n\n// Delete removes key from cache\nfunc (c *LRUCache) Delete(key string) bool {\n\tc.mutex.Lock()\n\tdefer c.mutex.Unlock()\n\t\n\t// TODO 1: Look up key in c.items map\n\t// TODO 2: If not found, return false\n\t// TODO 3: Remove element from c.order list\n\t// TODO 4: Delete key from c.items map\n\t// TODO 5: Update c.used counter by subtracting entry size\n\t// TODO 6: Return true\n}\n\n// CleanupExpired removes expired entries\nfunc (c *LRUCache) CleanupExpired() int {\n\tc.mutex.Lock()\n\tdefer c.mutex.Unlock()\n\t\n\t// TODO 1: Initialize counter for removed entries\n\t// TODO 2: Walk through c.items map\n\t// TODO 3: For each entry, check if ExpiresAt < time.Now()\n\t// TODO 4: If expired, remove from both c.items and c.order\n\t// TODO 5: Update c.used counter and increment removal counter\n\t// TODO 6: Return total number of entries removed\n\t// Hint: Collect keys to remove first, then delete to avoid map iteration issues\n}\n```\n\n#### Development Checkpoints\n\n**Milestone 1 Checkpoint - Hash Ring Implementation:**\n```bash\n# Run hash ring tests\ngo test ./internal/hashring/... -v\n\n# Expected output should show:\n# - Keys distribute evenly across nodes\n# - Virtual nodes improve distribution balance  \n# - Node addition/removal minimally affects existing keys\n# - Key lookup performs in O(log n) time\n\n# Manual verification:\ngo run cmd/test-hashring/main.go\n# Should show hash ring with virtual nodes and key distribution\n```\n\n**Milestone 2 Checkpoint - Cache Node:**\n```bash\n# Run cache tests\ngo test ./internal/cache/... -v\n\n# Expected behaviors:\n# - LRU eviction removes least recently used entries\n# - TTL cleanup removes expired entries\n# - Memory limits enforced through eviction\n# - Concurrent access handled safely\n\n# Load test:\ngo run cmd/cache-benchmark/main.go\n# Should handle 10K+ ops/sec without data corruption\n```\n\n**Language-Specific Implementation Hints:**\n\n- **Hashing**: Use `crypto/sha256` for consistent results across nodes. Convert hash to `uint32` by taking first 4 bytes for ring positioning.\n- **Concurrency**: Use `sync.RWMutex` for read-heavy workloads like hash ring lookups. Use `sync.Mutex` for write-heavy operations like LRU updates.\n- **Memory Accounting**: Calculate entry size as `len(key) + len(value) + 64` (64 bytes overhead for metadata and pointers).\n- **HTTP Timeouts**: Set both client timeout and server timeout. Use `context.WithTimeout()` for request-level timeouts.\n- **JSON Serialization**: Use `json:\",omitempty\"` tags to reduce message size. Consider `encoding/gob` for better performance in internal communication.\n- **Testing**: Use `go test -race` to detect race conditions. Use `testing.Short()` to skip slow tests during development.\n\n\n## Data Model\n\n> **Milestone(s):** This section provides the data structure foundation for all milestones, with particular relevance to Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication & Consistency).\n\n### Mental Model: The Digital Filing System\n\nThink of our distributed cache as a sophisticated digital filing system for a large organization with multiple offices. Each piece of information (cache entry) is like a document that needs to be filed, labeled, and easily retrievable. The filing system has several key components:\n\n- **Document Labels and Metadata**: Every document has a label (key), contents (value), expiration date (TTL), and storage requirements (size)\n- **Office Directory**: A master list showing which offices exist, their addresses, and current status\n- **Filing Rules**: Instructions for which office should store each document based on the label\n- **Office Coordination**: Each office maintains its own filing cabinet but coordinates with others to ensure documents are properly distributed and backed up\n\nJust as a physical filing system needs consistent rules for where documents go and how they're organized, our distributed cache needs well-defined data structures to represent cache entries, node information, and cluster state. These structures form the foundation that all other components build upon.\n\n### Core Data Types\n\nThe distributed cache system relies on several fundamental data structures that work together to provide distributed storage and retrieval capabilities. Each structure serves a specific purpose and contains carefully chosen fields to support the system's functionality.\n\n![Data Model and Type Relationships](./diagrams/data-model-relationships.svg)\n\nThe core data types form a hierarchy of responsibility, from individual cache entries up to cluster-wide coordination structures. Understanding these relationships is crucial for implementing the distributed cache correctly.\n\n#### Configuration and Node Identity\n\nThe `NodeConfig` structure contains all configuration parameters needed to initialize and run a cache node. This includes network settings, performance parameters, and operational timeouts that control the node's behavior within the cluster.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| NodeID | string | Unique identifier for this node in the cluster, typically a UUID or hostname |\n| ListenAddress | string | Address and port where this node accepts incoming requests (e.g., \":8080\") |\n| AdvertiseAddr | string | Address that other nodes should use to contact this node (for NAT/proxy scenarios) |\n| JoinAddresses | []string | List of existing cluster members to contact during bootstrap |\n| MaxMemoryMB | int | Maximum memory in megabytes this node can use for cache storage |\n| VirtualNodes | int | Number of virtual node positions to create on the hash ring |\n| ReplicationFactor | int | Number of replica copies to maintain for each cache entry |\n| HealthCheckInterval | time.Duration | How frequently to probe other nodes for health status |\n| GossipInterval | time.Duration | How often to exchange cluster state information with peers |\n| RequestTimeout | time.Duration | Maximum time to wait for responses from remote nodes |\n\nThe configuration design separates concerns between network identity, resource limits, and operational parameters. The `ListenAddress` and `AdvertiseAddr` distinction allows nodes to operate correctly behind load balancers or NAT devices, where the internal listening address differs from the external address other nodes should use.\n\n#### Hash Ring Structure\n\nThe `HashRing` structure implements consistent hashing for distributing keys across cluster nodes. It maintains the circular hash space and provides efficient lookups to determine which node should handle each key.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| virtualNodes | int | Number of virtual positions each physical node occupies on the ring |\n| ring | map[uint32]string | Maps hash positions to node IDs for key-to-node lookups |\n| sortedKeys | []uint32 | Sorted array of all hash positions for binary search during lookups |\n| nodes | map[string]bool | Set of active nodes currently participating in the ring |\n\nThe hash ring uses virtual nodes to improve load distribution. Instead of placing each physical node at a single position on the ring, each node occupies multiple virtual positions. This reduces the probability that a single node becomes a hotspot for popular keys and ensures more even distribution when nodes join or leave the cluster.\n\nThe `sortedKeys` array enables efficient O(log n) lookups using binary search to find the first hash position greater than or equal to a key's hash value. This position determines which node is responsible for storing that key.\n\n> **Design Insight**: Virtual nodes are critical for load balancing. With only physical nodes on the ring, key distribution can become severely skewed, especially in small clusters. By using 150-500 virtual nodes per physical node, the system achieves much more uniform key distribution and reduces the impact of hot spots.\n\n#### Cache Storage Structure\n\nThe `LRUCache` structure implements the in-memory storage for each node, providing least-recently-used eviction when memory limits are reached. This structure must handle concurrent access from multiple goroutines while maintaining correct LRU ordering.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| mutex | sync.RWMutex | Read-write mutex protecting concurrent access to cache state |\n| capacity | int64 | Maximum memory in bytes this cache can consume |\n| used | int64 | Current memory consumption in bytes across all stored entries |\n| items | map[string]*list.Element | Hash table mapping keys to doubly-linked list elements |\n| order | *list.List | Doubly-linked list maintaining LRU order (most recent at front) |\n\nThe LRU cache combines a hash table for O(1) key lookups with a doubly-linked list for O(1) LRU order maintenance. When an item is accessed, it moves to the front of the list. When eviction is needed, items are removed from the back of the list (least recently used).\n\nThe `sync.RWMutex` allows multiple concurrent readers while ensuring exclusive access for writers. This improves performance for read-heavy workloads, which are common in caching scenarios.\n\n### Cache Entry Structure\n\nEach piece of data stored in the distributed cache is represented by a `CacheEntry` structure. This structure contains not only the key-value pair but also metadata necessary for TTL expiration, memory management, and replication.\n\n#### Entry Data and Metadata\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Key | string | The cache key used to identify and retrieve this entry |\n| Value | []byte | The cached data as a byte array, allowing storage of any serializable data |\n| ExpiresAt | time.Time | Absolute timestamp when this entry should be considered expired |\n| Size | int64 | Total memory footprint of this entry in bytes (key + value + metadata) |\n\nThe `CacheEntry` design prioritizes simplicity and efficiency. Keys are strings to provide human-readable identifiers, while values are byte arrays to support any data type that can be serialized. The size field includes the complete memory footprint, not just the value size, enabling accurate memory accounting.\n\n#### TTL and Expiration Handling\n\nThe `ExpiresAt` field uses absolute timestamps rather than relative TTL values to avoid clock drift issues and simplify expiration checks. When a TTL is specified during a SET operation, it's converted to an absolute expiration time based on the local node's clock.\n\nA zero value for `ExpiresAt` (time.Time{}) indicates that the entry never expires. This allows the cache to support both TTL-based and permanent entries using the same data structure.\n\n> **Design Insight**: Using absolute timestamps for expiration avoids the complexity of tracking relative TTL values and updating them as time passes. However, it requires nodes to have reasonably synchronized clocks, which is generally acceptable in data center environments.\n\n#### Memory Size Calculation\n\nThe `Size` field represents the complete memory footprint of the cache entry, including:\n\n1. Key string memory (length in bytes plus string header overhead)\n2. Value byte array memory (slice length plus slice header overhead)\n3. Metadata structure memory (timestamps, integers, pointers)\n4. Any alignment padding required by the Go runtime\n\nAccurate size calculation is essential for enforcing memory limits and making informed eviction decisions. The size is calculated once when the entry is created and stored with the entry to avoid repeated calculations during memory pressure scenarios.\n\n### Cluster State Information\n\nThe distributed cache maintains comprehensive information about cluster membership, node health, and the current state of the hash ring. This information must be kept consistent across all nodes to ensure correct request routing and data placement.\n\n#### Node State Tracking\n\nThe `NodeState` structure represents the current status and metadata for a single node in the cluster. This information is exchanged through gossip protocol messages to maintain eventual consistency of cluster membership.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| NodeID | string | Unique identifier matching the node's configured NodeID |\n| Address | string | Network address where the node can be reached for cache requests |\n| Status | string | Current node status: \"joining\", \"active\", \"suspected\", \"failed\", or \"leaving\" |\n| LastSeen | time.Time | Timestamp when this node was last observed to be responsive |\n| Version | uint64 | Monotonic version number incremented when node state changes |\n\nNode states follow a specific lifecycle that helps the cluster distinguish between temporary network issues and permanent failures. The \"suspected\" state allows the system to continue operating while investigating potential failures, rather than immediately marking nodes as failed due to transient network problems.\n\nThe version number enables conflict resolution when different nodes have conflicting information about the same node. Higher version numbers always take precedence, ensuring that the most recent state information propagates through the cluster.\n\n#### Node Status State Machine\n\nThe node status field follows a well-defined state machine that governs how nodes transition through their lifecycle:\n\n| Current Status | Event | Next Status | Description |\n|---------------|-------|-------------|-------------|\n| - | Node starts and contacts cluster | joining | Node is attempting to join the cluster |\n| joining | Successfully joined cluster | active | Node is fully operational and serving requests |\n| active | Health check timeout | suspected | Node may have failed but is being investigated |\n| suspected | Health check succeeds | active | Node has recovered from temporary issue |\n| suspected | Failure confirmation timeout | failed | Node is confirmed failed and should be removed |\n| active | Graceful shutdown initiated | leaving | Node is cleanly exiting the cluster |\n| leaving | Shutdown complete | - | Node is removed from cluster state |\n\nThis state machine prevents flapping between active and failed states when nodes experience intermittent connectivity issues. The \"suspected\" state provides a grace period for recovery while allowing the cluster to continue operating.\n\n#### Cluster Membership and Gossip\n\nThe `GossipMessage` structure carries cluster state information between nodes. Each gossip message contains a complete snapshot of the sender's view of cluster membership, allowing recipients to update their local state and detect inconsistencies.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| NodeStates | map[string]NodeState | Complete mapping of node IDs to their current state information |\n| Version | uint64 | Overall cluster state version, incremented when any node state changes |\n\nGossip messages are exchanged periodically between randomly selected pairs of nodes. When a node receives a gossip message, it compares the contained node states with its local view and adopts any information with higher version numbers.\n\nThe gossip protocol ensures that cluster state changes eventually propagate to all nodes, even in the presence of network partitions or node failures. The probabilistic nature of gossip communication provides robustness against individual message failures while maintaining efficiency.\n\n#### Message Transport and Communication\n\nAll inter-node communication uses a standardized message format that supports multiple operation types and includes metadata for routing and debugging.\n\nThe `Message` structure provides a common envelope for all inter-node communications:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Type | string | Message type identifier (gossip, get, set, delete, replicate) |\n| Sender | string | Node ID of the message originator for routing responses |\n| Timestamp | time.Time | When the message was created, useful for timeout and debugging |\n| Data | interface{} | Message payload, type-specific to the message type |\n\nThe generic `Data` field allows the same message structure to carry different payload types without requiring separate message formats for each operation. The actual payload type is determined by the `Type` field and decoded accordingly.\n\n#### Cache Operation Messages\n\nSpecific message types handle cache operations between nodes. These structures define the format for requests and responses when cache operations need to be forwarded to the appropriate nodes or replicated to backup nodes.\n\n**Get Operation Messages:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| **GetRequest** | | |\n| Key | string | Cache key to retrieve |\n| ConsistencyLevel | string | Required consistency level (eventual, consistent, strong) |\n| **GetResponse** | | |\n| Key | string | Echo of the requested key for correlation |\n| Value | []byte | Retrieved value, nil if not found |\n| Found | bool | Whether the key exists in the cache |\n| Timestamp | time.Time | When the value was last modified, used for consistency |\n\n**Set Operation Messages:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| **SetRequest** | | |\n| Key | string | Cache key to store |\n| Value | []byte | Data to store in the cache |\n| TTL | time.Duration | Time-to-live for this entry, zero for no expiration |\n| ConsistencyLevel | string | Required consistency level for this write operation |\n| **SetResponse** | | |\n| Success | bool | Whether the operation completed successfully |\n| Timestamp | time.Time | When the value was stored, used for conflict resolution |\n\n**Delete Operation Messages:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| **DeleteRequest** | | |\n| Key | string | Cache key to remove |\n| ConsistencyLevel | string | Required consistency level for this delete operation |\n| **DeleteResponse** | | |\n| Success | bool | Whether the delete operation completed successfully |\n| Found | bool | Whether the key existed before deletion |\n| Timestamp | time.Time | When the deletion was processed |\n\nThese message structures support different consistency levels, allowing applications to choose between performance and consistency based on their requirements. The timestamp fields enable conflict resolution when multiple nodes have different versions of the same data.\n\n> **Architecture Decision: Message Format Design**\n> - **Context**: Inter-node communication requires standardized message formats that can handle different operation types while remaining extensible for future features.\n> - **Options Considered**: \n>   1. Operation-specific message formats with no common structure\n>   2. Single message type with operation-specific fields (many optional)\n>   3. Common message envelope with operation-specific payloads\n> - **Decision**: Common message envelope with type-specific payloads\n> - **Rationale**: This approach provides consistency across all operations while avoiding the complexity of a single message with many optional fields. The generic payload field allows strong typing for each operation while maintaining a common transport layer.\n> - **Consequences**: Enables consistent message handling, routing, and debugging while supporting future message types without breaking changes to the transport layer.\n\n### Common Pitfalls\n\nWhen implementing these data structures, several common mistakes can significantly impact system performance and correctness:\n\n⚠️ **Pitfall: Insufficient Memory Size Calculation**\nMany developers calculate only the value size when implementing the `Size` field in `CacheEntry`, ignoring the key string memory and metadata overhead. This leads to memory limit enforcement that allows 20-30% more memory usage than configured, potentially causing out-of-memory errors. Always calculate the complete memory footprint including string headers, slice headers, and struct overhead.\n\n⚠️ **Pitfall: Using Relative TTL Instead of Absolute Expiration**\nStoring relative TTL values (like \"expires in 5 minutes\") instead of absolute timestamps creates complexity when checking expiration and can lead to incorrect behavior if system time changes. The `ExpiresAt` field should always contain an absolute timestamp calculated when the entry is created, making expiration checks simple and reliable.\n\n⚠️ **Pitfall: Inadequate Virtual Node Distribution**\nUsing too few virtual nodes (less than 50 per physical node) leads to poor key distribution and hotspots, especially in small clusters. Using too many virtual nodes (more than 1000) increases memory overhead and hash ring operation costs without significant benefit. The recommended range of 150-500 virtual nodes provides good load balancing without excessive overhead.\n\n⚠️ **Pitfall: Race Conditions in LRU Cache State**\nForgetting to protect all cache state modifications with the mutex leads to race conditions that can corrupt the LRU order or cause panics when concurrent operations modify the linked list. Every operation that changes the `items` map, `order` list, or `used` counter must hold appropriate locks.\n\n⚠️ **Pitfall: Ignoring Clock Skew in Distributed Operations**\nUsing local timestamps for distributed operations without considering clock skew between nodes can cause incorrect conflict resolution and consistency issues. While the `Timestamp` fields in messages use local node time, conflict resolution algorithms must account for reasonable clock differences between nodes.\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the data model structures in Go, including complete working code for infrastructure components and skeleton code for core logic that learners should implement themselves.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Serialization | JSON with encoding/json | Protocol Buffers with protobuf |\n| Time Handling | time.Time with time.Now() | NTP-synchronized time with clock skew detection |\n| Memory Tracking | Manual calculation with unsafe.Sizeof | Memory profiling with runtime.MemStats |\n| Concurrent Access | sync.RWMutex | Lock-free data structures with atomic operations |\n\n#### Recommended File Structure\n\nThe data model components should be organized to separate concerns and enable easy testing:\n\n```\ninternal/\n  models/\n    cache_entry.go           ← CacheEntry and related utilities\n    cache_entry_test.go      ← Unit tests for cache entry operations\n    node_config.go           ← NodeConfig loading and validation\n    node_config_test.go      ← Configuration validation tests\n    messages.go              ← All message type definitions\n    cluster_state.go         ← NodeState and cluster membership types\n  ring/\n    hash_ring.go             ← HashRing implementation\n    hash_ring_test.go        ← Hash ring algorithm tests\n  cache/\n    lru_cache.go             ← LRUCache implementation\n    lru_cache_test.go        ← Cache behavior tests\n```\n\nThis structure separates data definitions from behavioral logic, making it easier to modify structures without affecting algorithms and enabling comprehensive unit testing of each component.\n\n#### Infrastructure Starter Code\n\n**Configuration Loading (complete implementation):**\n\n```go\npackage models\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n)\n\n// LoadConfig reads and validates configuration from a JSON file\nfunc LoadConfig(filename string) (*NodeConfig, error) {\n    data, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to read config file: %w\", err)\n    }\n    \n    var config NodeConfig\n    if err := json.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"failed to parse config JSON: %w\", err)\n    }\n    \n    if err := config.Validate(); err != nil {\n        return nil, fmt.Errorf(\"invalid configuration: %w\", err)\n    }\n    \n    return &config, nil\n}\n\n// Validate checks that all configuration values are reasonable\nfunc (c *NodeConfig) Validate() error {\n    if c.NodeID == \"\" {\n        return fmt.Errorf(\"NodeID cannot be empty\")\n    }\n    if c.ListenAddress == \"\" {\n        return fmt.Errorf(\"ListenAddress cannot be empty\")\n    }\n    if c.MaxMemoryMB <= 0 {\n        return fmt.Errorf(\"MaxMemoryMB must be positive, got %d\", c.MaxMemoryMB)\n    }\n    if c.VirtualNodes < 50 || c.VirtualNodes > 1000 {\n        return fmt.Errorf(\"VirtualNodes should be 50-1000, got %d\", c.VirtualNodes)\n    }\n    if c.ReplicationFactor <= 0 {\n        return fmt.Errorf(\"ReplicationFactor must be positive, got %d\", c.ReplicationFactor)\n    }\n    return nil\n}\n```\n\n**Memory Size Calculation Utilities (complete implementation):**\n\n```go\npackage models\n\nimport (\n    \"time\"\n    \"unsafe\"\n)\n\n// CalculateEntrySize computes the total memory footprint of a cache entry\nfunc CalculateEntrySize(key string, value []byte) int64 {\n    // String header + string data\n    keySize := int64(unsafe.Sizeof(key)) + int64(len(key))\n    \n    // Slice header + slice data  \n    valueSize := int64(unsafe.Sizeof(value)) + int64(len(value))\n    \n    // CacheEntry struct overhead\n    structSize := int64(unsafe.Sizeof(CacheEntry{}))\n    \n    return keySize + valueSize + structSize\n}\n\n// CreateCacheEntry creates a new cache entry with proper size calculation\nfunc CreateCacheEntry(key string, value []byte, ttl time.Duration) *CacheEntry {\n    var expiresAt time.Time\n    if ttl > 0 {\n        expiresAt = time.Now().Add(ttl)\n    }\n    \n    return &CacheEntry{\n        Key:       key,\n        Value:     value,\n        ExpiresAt: expiresAt,\n        Size:      CalculateEntrySize(key, value),\n    }\n}\n\n// IsExpired checks if a cache entry has exceeded its TTL\nfunc (e *CacheEntry) IsExpired() bool {\n    if e.ExpiresAt.IsZero() {\n        return false // Never expires\n    }\n    return time.Now().After(e.ExpiresAt)\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Hash Ring Implementation (skeleton with TODOs):**\n\n```go\npackage ring\n\nimport (\n    \"crypto/sha1\"\n    \"encoding/binary\"\n    \"sort\"\n)\n\n// NewHashRing creates a new consistent hash ring with the specified virtual nodes per physical node\nfunc NewHashRing(virtualNodes int) *HashRing {\n    return &HashRing{\n        virtualNodes: virtualNodes,\n        ring:         make(map[uint32]string),\n        sortedKeys:   make([]uint32, 0),\n        nodes:        make(map[string]bool),\n    }\n}\n\n// AddNode adds a physical node to the hash ring with its virtual nodes\nfunc (hr *HashRing) AddNode(nodeID string) {\n    // TODO 1: Check if node already exists in hr.nodes, return early if so\n    // TODO 2: Add nodeID to hr.nodes map\n    // TODO 3: For i := 0; i < hr.virtualNodes; i++, create virtual node positions:\n    //   - Generate virtual node key as nodeID + \":\" + i (convert i to string)\n    //   - Hash the virtual node key using hashKey() helper\n    //   - Add hash position to hr.ring mapping to nodeID\n    //   - Add hash position to hr.sortedKeys slice\n    // TODO 4: Sort hr.sortedKeys to maintain ordering for binary search\n    // Hint: Use sort.Slice(hr.sortedKeys, func(i, j int) bool { ... })\n}\n\n// RemoveNode removes a physical node and all its virtual nodes from the ring\nfunc (hr *HashRing) RemoveNode(nodeID string) {\n    // TODO 1: Check if node exists in hr.nodes, return early if not\n    // TODO 2: Remove nodeID from hr.nodes map  \n    // TODO 3: For i := 0; i < hr.virtualNodes; i++, remove virtual node positions:\n    //   - Generate same virtual node key as in AddNode\n    //   - Hash the virtual node key using hashKey() helper\n    //   - Remove hash position from hr.ring map\n    //   - Find and remove hash position from hr.sortedKeys slice\n    // TODO 4: Re-sort hr.sortedKeys after removals\n    // Hint: You'll need to rebuild hr.sortedKeys or carefully remove elements\n}\n\n// GetNode returns the node responsible for storing the given key\nfunc (hr *HashRing) GetNode(key string) string {\n    if len(hr.sortedKeys) == 0 {\n        return \"\"\n    }\n    \n    // TODO 1: Hash the key using hashKey() helper\n    // TODO 2: Use sort.Search to find first position >= key hash in hr.sortedKeys\n    // TODO 3: If search result >= len(hr.sortedKeys), wrap around to position 0\n    // TODO 4: Get the ring position at the found index\n    // TODO 5: Return the nodeID from hr.ring[position]\n    // Hint: sort.Search(len(hr.sortedKeys), func(i int) bool { return hr.sortedKeys[i] >= keyHash })\n}\n\n// hashKey generates a hash value for a string key\nfunc hashKey(key string) uint32 {\n    hash := sha1.Sum([]byte(key))\n    return binary.BigEndian.Uint32(hash[:4])\n}\n```\n\n**LRU Cache Implementation (skeleton with TODOs):**\n\n```go\npackage cache\n\nimport (\n    \"container/list\"\n    \"sync\"\n    \"time\"\n)\n\n// NewLRUCache creates an LRU cache with the specified memory capacity in bytes\nfunc NewLRUCache(capacityBytes int64) *LRUCache {\n    return &LRUCache{\n        capacity: capacityBytes,\n        used:     0,\n        items:    make(map[string]*list.Element),\n        order:    list.New(),\n    }\n}\n\n// Get retrieves a value from the cache and updates its LRU position\nfunc (lru *LRUCache) Get(key string) ([]byte, bool) {\n    lru.mutex.RLock()\n    \n    // TODO 1: Look up key in lru.items map\n    // TODO 2: If not found, release read lock and return nil, false\n    // TODO 3: Get the CacheEntry from element.Value\n    // TODO 4: Check if entry is expired using IsExpired() method\n    // TODO 5: If expired, release read lock, acquire write lock, remove entry, return nil, false\n    // TODO 6: If valid, release read lock, acquire write lock, move element to front of lru.order\n    // TODO 7: Release write lock and return value copy, true\n    // Hint: Use lru.order.MoveToFront(element) to update LRU position\n    // Hint: Make a copy of the value slice before returning to avoid races\n}\n\n// Set stores a value in the cache with optional TTL\nfunc (lru *LRUCache) Set(key string, value []byte, ttl time.Duration) {\n    lru.mutex.Lock()\n    defer lru.mutex.Unlock()\n    \n    // TODO 1: Create new CacheEntry using CreateCacheEntry helper\n    // TODO 2: Check if key already exists in lru.items\n    // TODO 3: If exists, subtract old entry size from lru.used, remove from lru.order\n    // TODO 4: Check if new entry size would exceed capacity (lru.used + entry.Size > lru.capacity)\n    // TODO 5: While over capacity, evict LRU entries using evictLRU() helper\n    // TODO 6: Add new entry to front of lru.order list\n    // TODO 7: Store list element in lru.items map\n    // TODO 8: Add entry size to lru.used counter\n    // Hint: list.Element.Value should be *CacheEntry\n}\n\n// evictLRU removes the least recently used entry (helper method)\nfunc (lru *LRUCache) evictLRU() bool {\n    // TODO 1: Get the back element from lru.order (least recently used)\n    // TODO 2: If list is empty, return false\n    // TODO 3: Get CacheEntry from element.Value\n    // TODO 4: Remove element from lru.order list\n    // TODO 5: Remove entry from lru.items map using entry.Key\n    // TODO 6: Subtract entry.Size from lru.used\n    // TODO 7: Return true\n}\n```\n\n#### Language-Specific Hints\n\n**Go-Specific Implementation Tips:**\n\n- Use `crypto/sha1` for consistent hashing as it provides good distribution and acceptable performance\n- The `unsafe.Sizeof()` function returns the size of the type structure, not the actual data (for slices/strings)\n- Use `sort.Search()` for efficient binary search in the sorted hash ring positions\n- `container/list` provides an efficient doubly-linked list implementation for LRU ordering\n- `sync.RWMutex` allows multiple concurrent readers, improving performance for read-heavy workloads\n- Always make copies of byte slices when returning from cache to prevent races with the underlying storage\n\n**Memory Management Best Practices:**\n\n- Include string header size (typically 16 bytes on 64-bit systems) in addition to string data\n- Include slice header size (typically 24 bytes on 64-bit systems) in addition to slice data\n- Account for struct padding and alignment when calculating sizes\n- Use `runtime.GC()` and `runtime.ReadMemStats()` for debugging memory accounting issues\n\n#### Milestone Checkpoint\n\nAfter implementing the data model structures, verify correct behavior with these checks:\n\n**Unit Test Commands:**\n```bash\ngo test ./internal/models/... -v\ngo test ./internal/ring/... -v  \ngo test ./internal/cache/... -v\n```\n\n**Expected Test Behavior:**\n- Configuration loading should accept valid JSON and reject invalid configurations\n- Hash ring should distribute keys evenly across virtual nodes and handle node additions/removals\n- LRU cache should correctly enforce memory limits and maintain LRU ordering under concurrent access\n- Memory size calculations should account for complete entry footprint\n\n**Manual Verification:**\n1. Create a hash ring with 3 nodes and 100 virtual nodes each\n2. Add 1000 random keys and verify distribution is reasonably even (no node has >40% of keys)\n3. Remove one node and verify that only ~33% of keys are remapped to different nodes\n4. Create an LRU cache with 1MB capacity and fill it with entries until eviction occurs\n5. Verify that the least recently used entries are evicted first\n\n**Warning Signs:**\n- Severe key distribution imbalance (one node getting >50% of keys)\n- Memory usage significantly exceeding configured limits  \n- LRU cache returning expired entries or incorrect eviction order\n- Panic or race condition errors under concurrent access\n\n\n## Consistent Hash Ring Design\n\n> **Milestone(s):** This section covers Milestone 1 (Consistent Hash Ring), providing the foundation for key distribution that enables Milestone 3 (Cluster Communication) and Milestone 4 (Replication & Consistency).\n\nThe consistent hash ring forms the backbone of our distributed cache, solving the fundamental challenge of distributing keys across nodes while minimizing data movement when cluster membership changes. This component determines which node stores each cache entry and ensures that adding or removing nodes only affects a small fraction of existing keys.\n\n### Mental Model: The Circular Table\n\nThink of consistent hashing as organizing a massive dinner party with a circular table that can magically expand and contract. Instead of assigning guests to numbered seats (which would require reshuffling everyone when the table size changes), we use a different approach.\n\nImagine the circular table has positions marked by compass-like coordinates from 0 to 360 degrees. Each guest (cache key) gets assigned a coordinate based on their name's hash value. Similarly, each waiter (cache node) is assigned multiple positions around the table. When a guest arrives, they sit at the first waiter position they encounter when walking clockwise around the table.\n\nThe magic happens when we add or remove waiters. If we remove waiter Alice (who was stationed at positions 45°, 135°, and 225°), only the guests between the previous waiter and Alice's positions need to move to the next waiter clockwise. All other guests stay exactly where they are. Similarly, adding a new waiter only affects guests in specific sections of the table.\n\nThis circular table model captures the essence of consistent hashing: locality of change. Unlike traditional modulo hashing (where changing from 3 to 4 servers would reassign roughly 75% of keys), consistent hashing ensures that membership changes only affect keys in the immediate vicinity of the change on the ring.\n\n![Consistent Hash Ring Structure](./diagrams/consistent-hash-ring.svg)\n\n### Virtual Nodes Strategy\n\nThe naive approach of placing each physical node at a single position on the hash ring creates significant problems. Consider a ring with three nodes positioned at hash values 100, 200, and 300. Node A (at position 100) would be responsible for the range from 301 to 100, which covers 160 degrees of the ring. Node B would cover 100 degrees, and Node C would cover only 100 degrees. This uneven distribution means Node A handles 60% more load than the other nodes.\n\nVirtual nodes solve this distribution problem by giving each physical node multiple positions on the ring. Instead of placing Node A at position 100, we might place it at positions 47, 156, 289, 334, and 401 (mod 360). Each physical node typically manages between 50 and 500 virtual nodes, creating a much more uniform distribution of key ranges.\n\n> **The fundamental insight is that virtual nodes act like statistical sampling. With only 3 physical nodes, we have 3 data points to distribute load. With 150 virtual nodes (50 per physical node), we have 150 data points, making the distribution much more likely to approach the theoretical ideal.**\n\nThe virtual nodes strategy provides several critical benefits beyond load balancing. When a physical node fails, its load gets redistributed among all remaining nodes rather than dumping everything on a single successor. If Node A fails and its virtual nodes were interleaved with those of nodes B and C, then approximately half of A's keys go to B and half go to C, preventing hotspots during failure scenarios.\n\nVirtual nodes also enable heterogeneous clusters where nodes have different capacities. A powerful node with 32GB of RAM might host 200 virtual nodes, while a smaller node with 8GB hosts only 50 virtual nodes. The ring naturally distributes proportionally more keys to the higher-capacity nodes.\n\n| Virtual Node Count | Distribution Quality | Memory Overhead | Rebalance Time | Recommended Use |\n|-------------------|---------------------|-----------------|----------------|-----------------|\n| 1-10 per node | Poor, many hotspots | Minimal | Fast | Never recommended |\n| 50-100 per node | Good for small clusters | Low | Fast | Development/testing |\n| 100-200 per node | Excellent | Moderate | Moderate | Production clusters |\n| 500+ per node | Diminishing returns | High | Slow | Very large clusters only |\n\n### Ring Operations and Algorithms\n\nThe hash ring implementation centers around three core operations: key lookup, node addition, and node removal. Each operation must maintain the ring's consistency while providing efficient access patterns for cache operations.\n\n#### Key Lookup Algorithm\n\nThe key lookup operation determines which node should handle a given cache key. This operation executes for every cache GET, SET, and DELETE request, making its performance critical to overall system throughput.\n\n1. **Hash the key**: Apply the chosen hash function (typically SHA-1 or MD5) to the cache key, producing a fixed-size hash value. Convert this hash to an unsigned 32-bit integer for ring positioning.\n\n2. **Find the position**: The hash value represents the key's position on the ring. This position falls somewhere between two virtual nodes.\n\n3. **Locate successor**: Search the sorted list of virtual node positions to find the first virtual node whose position is greater than or equal to the key's hash value. This virtual node is the key's \"successor\" on the ring.\n\n4. **Handle wraparound**: If no virtual node position is greater than the key's hash (meaning the key hashes to a position after the highest virtual node), wrap around to the first virtual node position. This maintains the ring's circular property.\n\n5. **Return physical node**: Map the responsible virtual node back to its corresponding physical node identifier. This physical node will handle the cache operation for this key.\n\nThe lookup operation's time complexity depends entirely on the search algorithm for step 3. A linear scan through virtual node positions would require O(V) time where V is the total number of virtual nodes across all physical nodes. However, since virtual node positions are stored in sorted order, binary search reduces this to O(log V), making lookups efficient even with hundreds of virtual nodes per physical node.\n\n#### Node Addition Algorithm\n\nAdding a new node to the cluster requires careful coordination to maintain data consistency while minimizing service disruption. The addition process involves ring updates, data migration, and membership synchronization.\n\n1. **Generate virtual node positions**: Create the specified number of virtual nodes for the new physical node. Hash the combination of node identifier and virtual node index (e.g., hash(\"node-5:vnode-23\")) to determine each virtual node's ring position.\n\n2. **Insert into ring structure**: Add each virtual node position to the sorted position list, maintaining sort order. Update the position-to-node mapping to include the new virtual nodes.\n\n3. **Identify affected ranges**: For each virtual node being added, determine the key range it will now own. This range extends from the previous virtual node position (clockwise) up to the new virtual node's position.\n\n4. **Update cluster membership**: Broadcast the membership change to all existing nodes through the gossip protocol, ensuring every node learns about the new member and updates its local ring representation.\n\n5. **Migrate affected keys**: The previous owner of each affected key range must transfer the relevant cache entries to the new node. This migration happens gradually to avoid overwhelming the new node or blocking cache operations.\n\n6. **Verify migration completion**: Confirm that all affected keys have been successfully transferred and that the new node is responding to cache operations for its assigned ranges.\n\nThe addition process requires careful ordering to prevent data loss. The new node must be added to the ring structure and membership before migration begins, ensuring that cache operations for affected keys get routed correctly during the transition period.\n\n#### Node Removal Algorithm\n\nRemoving a node (whether due to planned maintenance or failure detection) requires redistributing its cache entries while maintaining availability for active keys. The removal process differs significantly depending on whether it's a graceful shutdown or failure recovery.\n\n1. **Mark node as leaving**: Update the node's status in the cluster membership to indicate it's leaving the cluster. This prevents new keys from being assigned to the departing node.\n\n2. **Identify successor nodes**: For each virtual node belonging to the departing physical node, find the next virtual node position clockwise on the ring. The physical node owning that successor virtual node will inherit the departing node's key range.\n\n3. **Migrate keys to successors**: Transfer all cache entries owned by the departing node to their respective successor nodes. This migration should prioritize frequently accessed keys to minimize cache misses during the transition.\n\n4. **Update ring structure**: Remove all of the departing node's virtual nodes from the sorted position list and position-to-node mapping.\n\n5. **Broadcast membership change**: Notify all remaining cluster members about the node departure through gossip protocol messages, ensuring consistent ring views across the cluster.\n\n6. **Clean up resources**: Release any resources associated with the departed node, including connection pools, health check timers, and local state tracking.\n\nFor graceful departures, the leaving node can actively participate in migration, pushing its cache entries to successor nodes. For failure scenarios, successor nodes must pull the data from replicas or mark the keys as unavailable until anti-entropy processes restore the data.\n\n### Architecture Decisions\n\nThe consistent hash ring implementation requires several critical design decisions that affect performance, scalability, and operational complexity. Each decision involves trade-offs between different system qualities.\n\n> **Decision: Hash Function Selection**\n> - **Context**: The hash function must distribute keys uniformly across the ring space while providing consistent results across all nodes in the cluster.\n> - **Options Considered**: MD5, SHA-1, SHA-256, MurmurHash3, xxHash\n> - **Decision**: SHA-1 for production environments, with MD5 as an alternative for development\n> - **Rationale**: SHA-1 provides excellent distribution properties with reasonable computational cost. While cryptographic strength isn't required for this use case, the uniform distribution is critical. MD5 offers better performance for development/testing scenarios where security isn't a concern.\n> - **Consequences**: SHA-1 produces 160-bit hashes requiring truncation to 32-bit ring positions, but the distribution quality justifies this overhead. The choice can be made configurable to allow different environments to optimize differently.\n\n| Hash Function | Distribution Quality | Performance | Output Size | Security | Chosen? |\n|--------------|---------------------|-------------|-------------|----------|---------|\n| MD5 | Excellent | Very Fast | 128 bits | Weak | Development only |\n| SHA-1 | Excellent | Fast | 160 bits | Moderate | ✓ Production |\n| SHA-256 | Excellent | Slower | 256 bits | Strong | Overkill |\n| MurmurHash3 | Good | Very Fast | 32/128 bits | None | Too specialized |\n| xxHash | Good | Very Fast | 32/64 bits | None | Too specialized |\n\n> **Decision: Virtual Node Count Strategy**\n> - **Context**: The number of virtual nodes per physical node affects load distribution, memory overhead, and rebalancing performance.\n> - **Options Considered**: Fixed count (100 per node), proportional to capacity, adaptive based on cluster size\n> - **Decision**: Configurable with intelligent defaults: 150 for small clusters (<10 nodes), 100 for medium clusters (10-50 nodes), 50 for large clusters (50+ nodes)\n> - **Rationale**: Smaller clusters need more virtual nodes per physical node to achieve good distribution. Larger clusters achieve statistical balance with fewer virtual nodes per node, reducing memory overhead and rebalancing time.\n> - **Consequences**: Requires cluster-size-aware configuration, but provides optimal balance between distribution quality and performance across different deployment scales.\n\n> **Decision: Ring Position Storage Structure**\n> - **Context**: The ring must support efficient key lookups while allowing dynamic addition and removal of virtual nodes.\n> - **Options Considered**: Sorted slice with binary search, balanced binary tree (red-black), hash table with consistent hashing library\n> - **Decision**: Sorted slice of positions with binary search for lookups, rebuilding on membership changes\n> - **Rationale**: Membership changes are relatively infrequent compared to key lookups. The sorted slice provides O(log n) lookups with excellent cache locality and simple implementation. The rebuild cost on membership changes is acceptable given the infrequency.\n> - **Consequences**: Membership changes require O(n) time to rebuild the sorted slice, but the simplicity and lookup performance outweigh this cost. More complex structures would optimize for the uncommon case at the expense of the common case.\n\n> **Decision: Key Range Assignment Strategy**\n> - **Context**: When nodes join or leave, the system must determine which keys need migration and coordinate the transfer process.\n> - **Options Considered**: Push-based (departing node sends keys), pull-based (receiving node requests keys), hybrid approach\n> - **Decision**: Hybrid approach - push-based for graceful departures, pull-based for failure scenarios\n> - **Rationale**: Graceful departures can leverage the departing node's knowledge of its keys for efficient transfer. Failure scenarios require surviving nodes to take initiative and pull data from replicas or other sources.\n> - **Consequences**: Requires more complex implementation but optimizes for both planned and unplanned membership changes. The added complexity is justified by improved performance in each scenario.\n\n### Common Pitfalls\n\nImplementing consistent hashing appears straightforward in theory but contains several subtle traps that can cause severe problems in production environments.\n\n⚠️ **Pitfall: Insufficient Virtual Node Count**\n\nMany implementations start with too few virtual nodes per physical node, often using just 10-20 virtual nodes. This creates significant load imbalance, with some nodes receiving 2-3x more traffic than others. The problem worsens in small clusters where statistical effects have less opportunity to average out.\n\n*Why it's wrong*: With only 20 virtual nodes across a 3-node cluster (60 total), some nodes will own ranges covering 25% of the keyspace while others own only 10%. In a cache workload with uneven key popularity, this translates to proportionally uneven load.\n\n*How to fix it*: Use at least 50 virtual nodes per physical node for clusters with fewer than 20 nodes. Monitor actual key distribution in production and increase virtual node count if load balancing remains poor. Implement tooling to analyze key distribution across nodes and identify imbalance.\n\n⚠️ **Pitfall: Hash Function Consistency Across Nodes**\n\nDifferent nodes using different hash implementations or configurations will place keys at different ring positions, causing cache misses and data inconsistency. This often happens when nodes run different software versions or when hash function libraries change behavior between versions.\n\n*Why it's wrong*: If Node A thinks key \"user:123\" hashes to position 45 (owned by Node B) but Node B thinks the same key hashes to position 67 (owned by Node C), cache operations will fail unpredictably. SET operations might store data on one node while GET operations look for it on another.\n\n*How to fix it*: Standardize on a specific hash function implementation and version across all nodes. Include hash function choice and version in the cluster configuration. Implement hash compatibility tests in your deployment pipeline. Consider encoding the hash algorithm version in cache entry metadata to detect mismatches.\n\n⚠️ **Pitfall: Race Conditions During Ring Updates**\n\nConcurrent node additions or failures can create inconsistent ring states if multiple nodes attempt to update the ring simultaneously. This leads to split-brain scenarios where different nodes have different views of key ownership.\n\n*Why it's wrong*: If Node A thinks key \"session:456\" belongs to Node B while Node C thinks it belongs to Node D, cache coherency breaks down. Applications might see cache misses for recently stored data or inconsistent values across requests.\n\n*How to fix it*: Implement a gossip-based consensus mechanism for ring updates, ensuring all nodes converge to the same ring state. Use version numbers or vector clocks to detect and resolve conflicting ring updates. Add logging and monitoring to detect ring inconsistencies quickly.\n\n⚠️ **Pitfall: Blocking Operations During Rebalancing**\n\nSome implementations block all cache operations while rebalancing keys after membership changes. This can cause significant service disruption, especially for large datasets or slow migration processes.\n\n*Why it's wrong*: Blocking operations defeats the purpose of building a high-availability cache. Applications experience timeouts and degraded performance during routine cluster maintenance or failure recovery.\n\n*How to fix it*: Implement non-blocking rebalancing where cache operations continue during key migration. Use double-writing (storing keys on both old and new owners temporarily) during transitions. Implement graceful migration with rate limiting to avoid overwhelming nodes with migration traffic.\n\n⚠️ **Pitfall: Ignoring Hotspot Keys**\n\nConsistent hashing distributes keys uniformly across nodes, but it doesn't account for keys with vastly different access patterns. A single popular key can create a hotspot that overwhelms one node while others remain idle.\n\n*Why it's wrong*: Even perfect key distribution becomes meaningless if 90% of requests target 10% of keys that all happen to live on the same node. The cache becomes limited by the performance of individual nodes rather than scaling with cluster size.\n\n*How to fix it*: Implement hotspot detection by monitoring per-key access frequencies. For extremely popular keys, consider replicating them to multiple nodes or using a separate hot-key cache layer. Design your key naming scheme to avoid natural hotspots (like time-based prefixes that create temporal locality).\n\n### Implementation Guidance\n\nBuilding a production-ready consistent hash ring requires careful attention to data structures, concurrency, and performance characteristics. The following guidance provides both the foundational infrastructure and the core algorithms that form the heart of your distributed cache.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Hash Function | MD5 (crypto/md5) | SHA-1 (crypto/sha1) |\n| Data Structure | Sorted slice + binary search | Skip list or B-tree |\n| Serialization | JSON (encoding/json) | Protocol Buffers |\n| Concurrency | RWMutex (sync.RWMutex) | Lock-free with atomic operations |\n| Testing | Unit tests + table-driven tests | Property-based testing with rapid |\n\n#### Recommended File Structure\n\nThe hash ring implementation should be organized as a self-contained package with clear separation between core algorithms and supporting utilities:\n\n```\ninternal/hashring/\n  hashring.go              ← Core HashRing implementation\n  hashring_test.go         ← Unit tests and benchmarks\n  virtual_node.go          ← Virtual node management\n  hash_functions.go        ← Pluggable hash function support\n  rebalancer.go           ← Key migration coordination\n  ring_visualizer.go      ← Debugging and monitoring tools\n  \ninternal/hashring/testdata/\n  ring_scenarios.json     ← Test cases for complex scenarios\n  performance_baselines.json ← Performance regression tests\n```\n\n#### Infrastructure Starter Code\n\nThe following hash function abstraction provides a clean interface for different hashing strategies while maintaining consistency across the cluster:\n\n```go\npackage hashring\n\nimport (\n    \"crypto/md5\"\n    \"crypto/sha1\"\n    \"encoding/binary\"\n    \"fmt\"\n    \"hash\"\n)\n\n// HashFunction defines the interface for ring position calculation.\ntype HashFunction interface {\n    Hash(data string) uint32\n    Name() string\n}\n\n// SHA1Hash provides SHA-1 based hashing for production environments.\ntype SHA1Hash struct{}\n\nfunc (h SHA1Hash) Hash(data string) uint32 {\n    hasher := sha1.New()\n    hasher.Write([]byte(data))\n    hashBytes := hasher.Sum(nil)\n    \n    // Use first 4 bytes of SHA-1 output for ring position\n    return binary.BigEndian.Uint32(hashBytes[:4])\n}\n\nfunc (h SHA1Hash) Name() string {\n    return \"sha1\"\n}\n\n// MD5Hash provides MD5 based hashing for development environments.\ntype MD5Hash struct{}\n\nfunc (h MD5Hash) Hash(data string) uint32 {\n    hasher := md5.New()\n    hasher.Write([]byte(data))\n    hashBytes := hasher.Sum(nil)\n    \n    // Use first 4 bytes of MD5 output for ring position\n    return binary.BigEndian.Uint32(hashBytes[:4])\n}\n\nfunc (h MD5Hash) Name() string {\n    return \"md5\"\n}\n\n// NewHashFunction creates a hash function by name.\nfunc NewHashFunction(name string) (HashFunction, error) {\n    switch name {\n    case \"sha1\":\n        return SHA1Hash{}, nil\n    case \"md5\":\n        return MD5Hash{}, nil\n    default:\n        return nil, fmt.Errorf(\"unsupported hash function: %s\", name)\n    }\n}\n```\n\nThis ring position management utility handles the sorted array operations efficiently:\n\n```go\npackage hashring\n\nimport (\n    \"sort\"\n    \"sync\"\n)\n\n// RingPositions manages the sorted list of virtual node positions.\ntype RingPositions struct {\n    positions []uint32\n    nodes     map[uint32]string\n    mutex     sync.RWMutex\n}\n\n// NewRingPositions creates a new ring position manager.\nfunc NewRingPositions() *RingPositions {\n    return &RingPositions{\n        positions: make([]uint32, 0),\n        nodes:     make(map[uint32]string),\n    }\n}\n\n// Add inserts a new position and maintains sorted order.\nfunc (rp *RingPositions) Add(position uint32, nodeID string) {\n    rp.mutex.Lock()\n    defer rp.mutex.Unlock()\n    \n    // Insert position in sorted order\n    insertIndex := sort.Search(len(rp.positions), func(i int) bool {\n        return rp.positions[i] >= position\n    })\n    \n    // Expand slice and insert at correct position\n    rp.positions = append(rp.positions, 0)\n    copy(rp.positions[insertIndex+1:], rp.positions[insertIndex:])\n    rp.positions[insertIndex] = position\n    \n    // Map position to node\n    rp.nodes[position] = nodeID\n}\n\n// Remove deletes a position and updates the mapping.\nfunc (rp *RingPositions) Remove(position uint32) {\n    rp.mutex.Lock()\n    defer rp.mutex.Unlock()\n    \n    // Find position index\n    index := sort.Search(len(rp.positions), func(i int) bool {\n        return rp.positions[i] >= position\n    })\n    \n    if index < len(rp.positions) && rp.positions[index] == position {\n        // Remove from slice\n        copy(rp.positions[index:], rp.positions[index+1:])\n        rp.positions = rp.positions[:len(rp.positions)-1]\n        \n        // Remove from mapping\n        delete(rp.nodes, position)\n    }\n}\n\n// FindSuccessor returns the node responsible for the given position.\nfunc (rp *RingPositions) FindSuccessor(position uint32) (string, bool) {\n    rp.mutex.RLock()\n    defer rp.mutex.RUnlock()\n    \n    if len(rp.positions) == 0 {\n        return \"\", false\n    }\n    \n    // Find first position >= target\n    index := sort.Search(len(rp.positions), func(i int) bool {\n        return rp.positions[i] >= position\n    })\n    \n    // Handle wraparound case\n    if index >= len(rp.positions) {\n        index = 0\n    }\n    \n    successorPosition := rp.positions[index]\n    return rp.nodes[successorPosition], true\n}\n\n// GetPositions returns a copy of all positions for iteration.\nfunc (rp *RingPositions) GetPositions() []uint32 {\n    rp.mutex.RLock()\n    defer rp.mutex.RUnlock()\n    \n    positions := make([]uint32, len(rp.positions))\n    copy(positions, rp.positions)\n    return positions\n}\n\n// Size returns the number of positions in the ring.\nfunc (rp *RingPositions) Size() int {\n    rp.mutex.RLock()\n    defer rp.mutex.RUnlock()\n    return len(rp.positions)\n}\n```\n\n#### Core Logic Skeleton Code\n\nThe main `HashRing` implementation should focus on the consistent hashing algorithm while leveraging the supporting infrastructure:\n\n```go\npackage hashring\n\nimport (\n    \"fmt\"\n    \"sort\"\n    \"sync\"\n)\n\n// HashRing implements consistent hashing with virtual nodes.\ntype HashRing struct {\n    virtualNodes int\n    ring         map[uint32]string\n    sortedKeys   []uint32\n    nodes        map[string]bool\n    hashFunc     HashFunction\n    mutex        sync.RWMutex\n}\n\n// NewHashRing creates a new consistent hash ring with the specified number of virtual nodes.\nfunc NewHashRing(virtualNodes int) *HashRing {\n    // TODO 1: Validate that virtualNodes is positive (return error if not)\n    // TODO 2: Create HashRing struct with initialized maps and slices\n    // TODO 3: Set default hash function to SHA1Hash\n    // TODO 4: Return configured HashRing instance\n    // Hint: Use make() to initialize maps with reasonable capacity\n}\n\n// AddNode adds a physical node to the ring with its virtual nodes.\nfunc (hr *HashRing) AddNode(nodeID string) {\n    // TODO 1: Acquire write lock to prevent concurrent modifications\n    // TODO 2: Check if node already exists (skip if duplicate)\n    // TODO 3: For each virtual node index (0 to virtualNodes-1):\n    //   - Create virtual node key by combining nodeID and index\n    //   - Hash the virtual node key to get ring position\n    //   - Store position -> nodeID mapping in ring map\n    //   - Add position to sortedKeys slice\n    // TODO 4: Sort the sortedKeys slice to maintain order\n    // TODO 5: Mark node as active in nodes map\n    // TODO 6: Release write lock\n    // Hint: Virtual node key format could be \"nodeID:vnodeIndex\"\n}\n\n// RemoveNode removes a physical node and all its virtual nodes from the ring.\nfunc (hr *HashRing) RemoveNode(nodeID string) {\n    // TODO 1: Acquire write lock to prevent concurrent modifications\n    // TODO 2: Check if node exists (skip if not found)\n    // TODO 3: For each virtual node index (0 to virtualNodes-1):\n    //   - Create virtual node key by combining nodeID and index\n    //   - Hash the virtual node key to get ring position\n    //   - Remove position from ring map\n    //   - Find and remove position from sortedKeys slice\n    // TODO 4: Remove node from nodes map\n    // TODO 5: Rebuild sortedKeys slice to remove gaps\n    // TODO 6: Release write lock\n    // Hint: Consider using a temporary slice for efficient removal\n}\n\n// GetNode returns the node responsible for storing the given key.\nfunc (hr *HashRing) GetNode(key string) string {\n    // TODO 1: Acquire read lock for thread-safe access\n    // TODO 2: Check if ring is empty (return empty string if no nodes)\n    // TODO 3: Hash the key to get its ring position\n    // TODO 4: Use binary search on sortedKeys to find first position >= key position\n    // TODO 5: Handle wraparound case (if no position >= key position, use first position)\n    // TODO 6: Look up node ID using the found position in ring map\n    // TODO 7: Release read lock and return node ID\n    // Hint: sort.Search() provides binary search functionality\n}\n\n// GetNodes returns N nodes for replication, starting with the primary node.\nfunc (hr *HashRing) GetNodes(key string, count int) []string {\n    // TODO 1: Acquire read lock for thread-safe access\n    // TODO 2: Validate count parameter (should be positive and <= total nodes)\n    // TODO 3: Hash the key to get starting position\n    // TODO 4: Use binary search to find first virtual node >= key position\n    // TODO 5: Iterate through sortedKeys starting from found position:\n    //   - Add each unique physical node to result list\n    //   - Handle wraparound when reaching end of sortedKeys\n    //   - Stop when we have 'count' unique nodes\n    // TODO 6: Release read lock and return node list\n    // Hint: Use a map to track which nodes you've already added\n}\n\n// GetNodeKeys returns all keys that would be assigned to the specified node.\n// This method is used during rebalancing to identify keys for migration.\nfunc (hr *HashRing) GetNodeKeys(nodeID string, allKeys []string) []string {\n    // TODO 1: Acquire read lock for consistent ring state\n    // TODO 2: Create result slice to collect keys assigned to nodeID\n    // TODO 3: For each key in allKeys:\n    //   - Use GetNode() to find responsible node\n    //   - If responsible node equals nodeID, add to result\n    // TODO 4: Release read lock and return result slice\n    // TODO 5: Consider optimizing for large key sets using position ranges\n    // Hint: This method is primarily used during node migration\n}\n\n// GetRingState returns debugging information about the current ring state.\nfunc (hr *HashRing) GetRingState() map[string]interface{} {\n    // TODO 1: Acquire read lock for consistent snapshot\n    // TODO 2: Create map to hold ring statistics and state\n    // TODO 3: Add total virtual node count, physical node count\n    // TODO 4: Add list of active physical nodes\n    // TODO 5: Add ring position distribution for debugging\n    // TODO 6: Release read lock and return state map\n    // Hint: This information is valuable for monitoring and debugging\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Hash Function Performance**: Go's `crypto/sha1` and `crypto/md5` packages are optimized for performance. For maximum throughput, consider reusing hash instances with `Reset()` rather than creating new ones for each operation.\n\n**Binary Search Optimization**: The `sort.Search()` function is highly optimized in Go's standard library. Use it directly rather than implementing custom binary search logic.\n\n**Memory Pool Usage**: For high-throughput scenarios, consider using `sync.Pool` to reuse slice allocations in frequently called methods like `GetNodes()`.\n\n**Slice Growth Strategy**: When building the `sortedKeys` slice, pre-allocate capacity based on expected virtual node count: `make([]uint32, 0, virtualNodes*expectedNodes)`.\n\n**Lock Granularity**: The current design uses a single RWMutex for the entire ring. For extremely high-throughput scenarios, consider more granular locking or lock-free approaches using atomic operations.\n\n#### Milestone Checkpoint\n\nAfter implementing the consistent hash ring, verify correctness with these specific tests:\n\n**Basic Functionality Test**:\n```bash\ngo test ./internal/hashring/ -v -run TestHashRing\n```\n\nExpected behavior: All tests pass, showing correct key distribution, node addition/removal, and lookup operations.\n\n**Load Distribution Test**: Create a ring with 3 nodes, add 10,000 randomly generated keys, and verify that each node receives between 25-40% of keys (allowing for statistical variation with virtual nodes).\n\n**Rebalancing Test**: Add a fourth node to the ring and verify that only ~25% of existing keys change ownership. Remove a node and verify that its keys distribute evenly among remaining nodes.\n\n**Signs of Problems**:\n- **Uneven distribution**: Some nodes getting >50% of keys indicates insufficient virtual nodes\n- **High rebalancing cost**: >40% of keys moving when adding one node suggests poor hash function or virtual node strategy  \n- **Lookup inconsistencies**: Same key mapping to different nodes across calls indicates race conditions or sorting bugs\n\n**Performance Benchmark**: Run `go test -bench=BenchmarkGetNode` and verify lookup operations complete in <1 microsecond for rings with 100+ virtual nodes.\n\n\n## Cache Node Implementation\n\n> **Milestone(s):** This section covers Milestone 2 (Cache Node Implementation), building on the hash ring foundation from Milestone 1 and providing the storage foundation for Milestone 3 (Cluster Communication) and Milestone 4 (Replication & Consistency).\n\n### Mental Model: The Smart Filing Cabinet\n\nThink of each cache node as a smart filing cabinet in a busy office. Unlike a traditional filing cabinet, this one has several remarkable abilities:\n\nThe cabinet automatically reorganizes itself every time you access a file. When you pull out a document, the cabinet moves it to the front of the most accessible drawer. Files you haven't used in a while gradually migrate toward the back, and eventually get moved to storage or discarded entirely when the cabinet runs out of space. This is exactly how **LRU eviction** works - the \"least recently used\" items get pushed out when memory fills up.\n\nThe cabinet also has a built-in timer system. Each file folder has a sticky note with an expiration date. A helpful assistant periodically walks through and removes any files past their expiration - this represents **TTL expiration**. The assistant is careful not to interrupt your work; they clean expired files during quiet moments.\n\nFinally, the cabinet has a precise scale that weighs every document. It knows exactly how much storage capacity it has, and when new files would exceed the limit, it automatically removes the oldest, least-used files to make room. This is **memory accounting** - tracking the exact memory footprint and enforcing limits through eviction.\n\nThe key insight is that the filing cabinet must remain responsive to your requests even while reorganizing itself and cleaning expired files. This requires careful coordination - the cabinet uses a sophisticated locking system so multiple people can safely access it simultaneously without corrupting the organization system.\n\n### LRU Eviction Algorithm\n\nThe **Least Recently Used (LRU)** eviction algorithm maintains the invariant that when memory limits are exceeded, the cache removes entries that haven't been accessed for the longest time. This strategy assumes that recently accessed data is more likely to be accessed again soon - a principle called temporal locality that holds true for most caching workloads.\n\nThe core challenge is efficiently tracking access order while supporting concurrent operations. A naive approach might store timestamps and sort entries by last access time, but this would be prohibitively expensive for high-throughput caches. Instead, we use a combination of a hash map for O(1) lookups and a doubly-linked list for O(1) reordering.\n\n#### LRU Data Structure Design\n\nThe `LRUCache` combines two data structures to achieve efficient operations:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `mutex` | `sync.RWMutex` | Protects all cache operations from race conditions during concurrent access |\n| `capacity` | `int64` | Maximum memory in bytes before eviction begins |\n| `used` | `int64` | Current memory consumption in bytes across all stored entries |\n| `items` | `map[string]*list.Element` | Hash map providing O(1) key lookup to linked list nodes |\n| `order` | `*list.List` | Doubly-linked list maintaining LRU order, most recent at front |\n\nThe linked list elements contain `CacheEntry` pointers as their values. This design enables:\n- O(1) key lookup through the hash map\n- O(1) reordering when entries are accessed (move to front)\n- O(1) eviction of least recently used items (remove from back)\n- O(1) memory accounting updates\n\n#### LRU Operation Algorithms\n\n**Get Operation Algorithm:**\n\n1. Acquire read lock to prevent concurrent modifications during lookup\n2. Check if key exists in the items hash map\n3. If key not found, release lock and return miss\n4. If key found, upgrade to write lock for reordering\n5. Extract `CacheEntry` from the linked list element\n6. Check if entry has expired based on current time vs `ExpiresAt`\n7. If expired, remove from both hash map and linked list, update memory accounting\n8. If valid, move the linked list element to the front (most recently used position)\n9. Return the entry value and release write lock\n\n**Set Operation Algorithm:**\n\n1. Acquire write lock for exclusive access during modification\n2. Calculate memory size of new entry (key length + value length + metadata overhead)\n3. Check if key already exists in the cache\n4. If key exists, remove old entry from memory accounting and linked list\n5. While (current memory usage + new entry size) exceeds capacity, evict LRU entries\n6. For each eviction: remove element from back of linked list, delete from hash map, subtract from memory usage\n7. Create new `CacheEntry` with key, value, expiration time, and size\n8. Create new linked list element containing the cache entry\n9. Add element to front of linked list (most recently used position)\n10. Add key-to-element mapping in hash map\n11. Update memory usage accounting to include new entry size\n12. Release write lock\n\n**Delete Operation Algorithm:**\n\n1. Acquire write lock for exclusive access during modification\n2. Look up key in hash map to find corresponding linked list element\n3. If key not found, release lock and return false\n4. Remove element from linked list using list.Remove()\n5. Delete key from hash map\n6. Subtract entry size from current memory usage\n7. Release write lock and return true\n\n#### Memory Accounting Strategy\n\nAccurate memory accounting prevents the cache from exceeding its configured memory limit and enables precise eviction decisions. The cache tracks memory usage at the granularity of individual cache entries, including both data and metadata overhead.\n\n**Memory Calculation for Cache Entries:**\n\n| Component | Size Calculation | Notes |\n|-----------|------------------|-------|\n| Key | `len(key)` bytes | String storage overhead varies by runtime |\n| Value | `len(value)` bytes | Raw byte slice length |\n| Metadata | 64 bytes estimate | `CacheEntry` struct, linked list pointers, map overhead |\n| Expiration | 8 bytes | `time.Time` represents nanoseconds as int64 |\n\nThe 64-byte metadata estimate accounts for Go's runtime overhead including the `CacheEntry` struct (key string, value slice header, timestamp, size field), the linked list element (previous/next pointers, value interface), and hash map bucket overhead. This estimate provides a reasonable approximation without the complexity of precise runtime memory introspection.\n\n> **Critical Design Insight:** Memory accounting must be updated atomically with data structure changes. If memory tracking becomes inconsistent with actual usage, the cache may either exceed its memory limit (causing OOM conditions) or prematurely evict entries (reducing hit rates). All operations that modify cache contents must update the `used` field within the same critical section.\n\n#### Concurrency and Lock Strategy\n\nThe cache supports concurrent reads while ensuring data consistency during modifications. The lock strategy balances performance with safety:\n\n**Read Lock Usage (Get Operations):**\n- Multiple readers can proceed simultaneously for different keys\n- Prevents writers from modifying cache structure during reads\n- Upgraded to write lock only when reordering is needed (cache hit)\n\n**Write Lock Usage (Set/Delete Operations):**\n- Exclusive access during cache modifications\n- Protects both hash map and linked list consistency\n- Covers memory accounting updates\n\n**Lock Upgrade Pattern:**\nThe Get operation uses a lock upgrade pattern - starting with a read lock for the lookup, then upgrading to a write lock only if reordering is needed. This optimization allows multiple concurrent reads of different keys while still maintaining consistency.\n\n> **Design Trade-off:** Per-key locking could improve concurrency but adds complexity and memory overhead. The single-lock approach provides simpler correctness reasoning at the cost of some parallel access. For most caching workloads, the bottleneck is network I/O rather than lock contention.\n\n### TTL and Expiration Handling\n\n**Time-to-Live (TTL)** support allows cache entries to automatically expire after a configurable duration, preventing stale data from persisting indefinitely. The expiration system must handle three key requirements: accepting TTL specifications during writes, checking expiration during reads, and periodically cleaning expired entries to reclaim memory.\n\n#### TTL Storage and Representation\n\nEach `CacheEntry` contains an `ExpiresAt` field of type `time.Time` representing the absolute expiration timestamp. Using absolute timestamps rather than relative durations avoids complications with clock adjustments and provides consistent behavior across system events.\n\n**TTL Specification Patterns:**\n\n| TTL Value | ExpiresAt Calculation | Behavior |\n|-----------|----------------------|----------|\n| `0` (no TTL) | `time.Time{}` (zero value) | Entry never expires automatically |\n| Positive duration | `time.Now().Add(ttl)` | Entry expires after specified duration |\n| Negative duration | `time.Now()` (immediate expiration) | Entry expires immediately (effectively a no-op) |\n\nThe zero value of `time.Time` represents a special case indicating no expiration. This avoids the need for separate boolean flags or nullable timestamp types.\n\n#### Expiration Checking During Access\n\nEvery Get operation checks expiration before returning cached data. This **lazy expiration** approach ensures that expired entries are never returned to clients while distributing cleanup costs across normal operations.\n\n**Expiration Check Algorithm:**\n\n1. Retrieve `CacheEntry` from cache data structures\n2. Check if `ExpiresAt` is the zero value (no expiration configured)\n3. If expiration is set, compare `ExpiresAt` with `time.Now()`\n4. If current time exceeds expiration time, treat as cache miss\n5. Remove expired entry from hash map, linked list, and update memory accounting\n6. Return cache miss result to client\n\nThis approach guarantees that clients never receive expired data, even if background cleanup processes fall behind. The cost is minimal since the time comparison and potential cleanup occur only for cache hits.\n\n#### Background Expiration Cleanup\n\nWhile lazy expiration handles consistency during reads, expired entries can accumulate memory usage if they're never accessed again. A background cleanup process periodically scans for expired entries and removes them proactively.\n\n**Cleanup Process Design:**\n\n> **Decision: Periodic Scanning vs. Timer-Based Expiration**\n> - **Context**: Background cleanup needs to balance memory reclamation with performance overhead\n> - **Options Considered**: \n>   1. Periodic full scan of all entries\n>   2. Timer-based individual entry expiration\n>   3. Probabilistic sampling of entries\n> - **Decision**: Periodic full scan with configurable interval\n> - **Rationale**: Simple implementation, predictable resource usage, works well with LRU ordering since expired entries tend to migrate toward LRU end\n> - **Consequences**: Some memory overhead between cleanup intervals, but bounded and predictable\n\n| Cleanup Strategy | Pros | Cons | Chosen? |\n|-----------------|------|------|---------|\n| Periodic full scan | Simple, predictable overhead, thorough cleanup | May pause operations during scan | ✓ Yes |\n| Individual timers | Precise expiration timing, distributed overhead | Complex memory management, timer overhead | No |\n| Probabilistic sampling | Low overhead, scales with cache size | May miss expired entries, unpredictable cleanup | No |\n\n**Cleanup Implementation Algorithm:**\n\n1. Acquire write lock to prevent concurrent modifications during cleanup\n2. Create slice to collect keys of expired entries (avoids map modification during iteration)\n3. Iterate through all entries in the linked list from back to front (LRU order)\n4. For each entry, check if `ExpiresAt` is non-zero and less than current time\n5. If expired, append key to expiration slice and continue\n6. If non-expired entry found, break iteration (optimization: newer entries likely not expired)\n7. For each expired key, remove from hash map and linked list, update memory accounting\n8. Release write lock and return count of expired entries removed\n\nThe cleanup process iterates from the LRU end because expired entries are more likely to be found there - entries that haven't been accessed recently are both candidates for LRU eviction and more likely to have exceeded their TTL.\n\n#### TTL and Memory Management Interaction\n\nTTL expiration interacts with LRU eviction in important ways. Both mechanisms remove entries from the cache, but they operate on different criteria - time-based vs. access-based. The system must coordinate these mechanisms to maintain consistency.\n\n**Interaction Patterns:**\n\n| Scenario | LRU Behavior | TTL Behavior | Resolution |\n|----------|--------------|--------------|------------|\n| Memory pressure, no expired entries | Evict LRU entries until under capacity | Continue normal TTL checking | LRU eviction takes precedence |\n| Memory pressure, some expired entries | Would evict LRU entries | Expired entries available for cleanup | Remove expired entries first, then LRU if needed |\n| No memory pressure, expired entries exist | Normal LRU operation | Background cleanup runs periodically | TTL cleanup reclaims memory proactively |\n| Entry expires while being accessed | Entry would move to MRU position | Entry should be treated as expired | Expiration check prevents MRU promotion |\n\nThe key principle is that expiration checks occur before any cache operations, ensuring expired entries are never promoted to the MRU position or returned to clients.\n\n### Memory Accounting and Limits\n\nPrecise memory management ensures the cache operates within configured limits while providing predictable performance characteristics. The memory accounting system tracks actual memory usage, enforces capacity limits through eviction, and provides visibility into memory utilization patterns.\n\n#### Memory Tracking Granularity\n\nThe cache tracks memory usage at the entry level, accounting for both user data and system overhead. This granularity provides accurate enforcement of memory limits while remaining computationally efficient.\n\n**Memory Components per Entry:**\n\n| Component | Calculation Method | Example Size | Notes |\n|-----------|-------------------|--------------|-------|\n| Key string | `len(key)` | 20 bytes | UTF-8 encoding, varies by content |\n| Value bytes | `len(value)` | Variable | Raw payload size |\n| Entry struct | Fixed overhead | 56 bytes | Key, value slice header, timestamp, size |\n| List element | Fixed overhead | 24 bytes | Previous/next pointers, value interface |\n| Map overhead | Estimated | ~16 bytes | Hash bucket entry, varies with map load factor |\n| **Total** | Sum of components | 116 + data | Metadata overhead approximately 15% for 1KB values |\n\nThe fixed overhead calculation assumes 64-bit pointers and includes Go runtime structures. The map overhead is amortized across entries and varies with the hash map's load factor and bucket structure.\n\n#### Capacity Enforcement Strategy\n\nWhen adding new entries would exceed the configured memory capacity, the cache must decide which existing entries to remove. The eviction strategy coordinates LRU ordering with memory accounting to maintain both access patterns and memory limits.\n\n**Eviction Decision Algorithm:**\n\n1. Calculate total memory needed: current usage + new entry size\n2. If total is within capacity, proceed with insertion without eviction\n3. If total exceeds capacity, calculate excess memory: (current + new) - capacity\n4. Begin evicting entries from the LRU end of the linked list\n5. For each evicted entry, subtract its memory size from current usage\n6. Continue eviction until excess memory is eliminated\n7. Insert new entry and update memory accounting\n\n> **Design Principle**: The cache evicts slightly more entries than strictly necessary to create headroom for future insertions. This reduces the frequency of eviction operations and improves performance for bursty workloads.\n\n**Eviction Batch Size Strategy:**\n\n| Approach | Eviction Behavior | Pros | Cons | Chosen? |\n|----------|------------------|------|------|---------|\n| Exact eviction | Remove entries until exactly under limit | Maximizes memory utilization | Frequent eviction operations | No |\n| Headroom eviction | Remove extra 10% beyond requirement | Reduces eviction frequency | Slightly lower memory utilization | ✓ Yes |\n| Batch eviction | Remove fixed number of entries | Predictable operation cost | May over-evict or under-evict | No |\n\n#### Memory Limit Configuration\n\nThe memory limit configuration affects both performance and resource utilization. Setting appropriate limits requires understanding the trade-offs between memory usage, cache hit rates, and eviction overhead.\n\n**Configuration Guidelines:**\n\n| Memory Limit | Cache Behavior | Use Case | Trade-offs |\n|--------------|----------------|----------|------------|\n| < 100 MB | Frequent eviction, low hit rates | Development, testing | Low resource usage, poor performance |\n| 100 MB - 1 GB | Moderate eviction, good hit rates | Small production workloads | Balanced resource usage and performance |\n| 1 GB - 10 GB | Infrequent eviction, high hit rates | Large production workloads | High memory usage, excellent performance |\n| > 10 GB | Rare eviction, very high hit rates | Cache-heavy applications | Very high memory usage, optimal performance |\n\nThe optimal memory limit depends on available system memory, working set size, and access patterns. A general guideline is to allocate 50-70% of available memory to the cache, leaving headroom for the operating system and other processes.\n\n#### Memory Usage Monitoring\n\nThe cache provides real-time visibility into memory utilization patterns, enabling capacity planning and performance optimization. Monitoring includes both absolute usage and utilization rates.\n\n**Memory Metrics:**\n\n| Metric | Calculation | Purpose | Alert Threshold |\n|--------|-------------|---------|------------------|\n| Used Memory | Current `used` field value | Track absolute consumption | N/A (informational) |\n| Memory Utilization | `(used / capacity) * 100%` | Track capacity percentage | > 90% (approaching limit) |\n| Eviction Rate | Evictions per second | Monitor pressure | > 10/sec (high pressure) |\n| Entry Count | Length of items map | Track entry density | N/A (informational) |\n| Average Entry Size | `used / entry count` | Monitor size distribution | N/A (informational) |\n\nHigh memory utilization (>90%) indicates that the cache is operating near capacity and may experience frequent evictions. High eviction rates suggest that the working set size exceeds cache capacity and either the memory limit should be increased or the workload characteristics need optimization.\n\n### Architecture Decisions\n\nThe cache node implementation requires several critical design decisions that affect performance, correctness, and maintainability. Each decision involves trade-offs between competing concerns such as simplicity, performance, and resource usage.\n\n#### Data Structure Selection\n\n> **Decision: Hash Map + Doubly Linked List for LRU**\n> - **Context**: Need O(1) access and reordering for high-performance caching\n> - **Options Considered**:\n>   1. Single hash map with timestamps (simple but requires sorting)\n>   2. Hash map + doubly linked list (complex but efficient)\n>   3. Hash map + heap/priority queue (moderate complexity, log N reordering)\n> - **Decision**: Hash map + doubly linked list combination\n> - **Rationale**: Provides O(1) lookup, insertion, deletion, and reordering. Critical for maintaining consistent performance under load.\n> - **Consequences**: More complex implementation and memory overhead, but predictable performance characteristics essential for production caching.\n\n| Data Structure | Access Time | Reorder Time | Memory Overhead | Implementation Complexity |\n|---------------|-------------|--------------|-----------------|---------------------------|\n| Hash map + timestamps | O(1) | O(n log n) | Low | Low |\n| Hash map + linked list | O(1) | O(1) | Medium | High |\n| Hash map + heap | O(1) | O(log n) | Medium | Medium |\n\n#### Concurrency Model\n\n> **Decision: Single RWMutex for Cache-Wide Locking**\n> - **Context**: Need thread safety for concurrent access while maintaining data structure consistency\n> - **Options Considered**:\n>   1. No locking (unsafe for concurrent access)\n>   2. Single mutex for all operations (safe but limits concurrency)\n>   3. Single RWMutex allowing concurrent reads (balanced approach)\n>   4. Per-key locking with lock striping (complex but highly concurrent)\n> - **Decision**: Single RWMutex for the entire cache\n> - **Rationale**: Balances implementation simplicity with reasonable concurrency. Most cache workloads are network-bound rather than lock-bound.\n> - **Consequences**: Some lock contention under high concurrency, but simpler correctness reasoning and lower memory overhead per key.\n\n#### Memory Accounting Precision\n\n> **Decision: Estimated Metadata Overhead**\n> - **Context**: Need accurate memory accounting without runtime introspection overhead\n> - **Options Considered**:\n>   1. Track only user data size (inaccurate)\n>   2. Use runtime reflection for exact sizes (accurate but slow)\n>   3. Estimate metadata overhead with fixed constants (approximation)\n> - **Decision**: Estimate metadata overhead using fixed per-entry constants\n> - **Rationale**: Provides reasonable accuracy for memory enforcement while maintaining performance. The estimation error is bounded and consistent.\n> - **Consequences**: Memory usage may vary slightly from estimates, but behavior remains predictable and overhead is minimal.\n\n#### TTL Implementation Strategy\n\n> **Decision: Lazy Expiration with Periodic Cleanup**\n> - **Context**: Need to balance data freshness guarantees with performance overhead\n> - **Options Considered**:\n>   1. No expiration support (simple but limited functionality)\n>   2. Lazy expiration only (guarantees freshness but may waste memory)\n>   3. Active expiration with timers (complex implementation)\n>   4. Hybrid lazy + periodic cleanup (balanced approach)\n> - **Decision**: Lazy expiration during access plus periodic background cleanup\n> - **Rationale**: Guarantees that expired data is never served while providing bounded memory usage. Background cleanup prevents memory leaks from unaccessed expired entries.\n> - **Consequences**: Small memory overhead between cleanup intervals, but provides strong correctness guarantees with reasonable performance.\n\n#### Error Handling Philosophy\n\nThe cache node adopts a fail-fast approach for configuration errors and a graceful degradation approach for runtime errors. This strategy provides clear feedback during development while maintaining availability during production operation.\n\n**Error Categories:**\n\n| Error Type | Handling Strategy | Example | Behavior |\n|------------|------------------|---------|----------|\n| Configuration | Fail fast at startup | Invalid memory limit | Return error, refuse to start |\n| Memory allocation | Graceful degradation | Out of memory | Log error, continue with eviction |\n| Concurrent access | Panic on detection | Data structure corruption | Fail immediately to prevent data loss |\n| TTL calculation | Graceful handling | Clock skew/negative TTL | Treat as immediate expiration |\n\n### Common Pitfalls\n\nCache node implementation involves several subtle correctness and performance issues that frequently trip up implementers. Understanding these pitfalls helps avoid bugs that can cause data corruption, memory leaks, or performance degradation.\n\n#### ⚠️ Pitfall: Inconsistent Memory Accounting\n\n**The Problem**: Updating cache data structures without correspondingly updating the `used` memory field, or vice versa. This typically happens when implementing the Set operation - developers correctly add entries to the hash map and linked list but forget to increment memory usage, or they handle the happy path correctly but miss error conditions.\n\n**Why It's Wrong**: Inconsistent memory accounting leads to two serious problems. Under-counting memory usage allows the cache to exceed its configured limit, potentially causing out-of-memory conditions in the host application. Over-counting memory usage triggers premature evictions, reducing cache hit rates and performance.\n\n**How to Fix**: Always update memory accounting within the same critical section as data structure modifications. Use defer statements or explicit cleanup blocks to ensure memory accounting is reverted if operations fail partway through.\n\n**Correct Pattern**:\n```go\n// Acquire lock\n// Calculate new memory requirement\n// Perform evictions if necessary\n// Update data structures AND memory accounting together\n// Release lock\n```\n\n#### ⚠️ Pitfall: Lock Upgrade Race Conditions\n\n**The Problem**: In the Get operation, upgrading from read lock to write lock creates a window where other goroutines can modify the cache state. Between releasing the read lock and acquiring the write lock, another goroutine might delete the entry or modify the linked list structure.\n\n**Why It's Wrong**: The cache entry pointer retrieved under the read lock may become invalid after lock release, leading to use-after-free bugs or null pointer dereferences when attempting to move the entry to the front of the LRU list.\n\n**How to Fix**: After upgrading to write lock, re-verify that the entry still exists in the cache before attempting to reorder it. Alternatively, use a write lock for the entire Get operation when a hit is detected.\n\n**Safe Pattern**: Always re-lookup entries after lock upgrades and handle the case where entries disappear between lock acquisition phases.\n\n#### ⚠️ Pitfall: TTL Cleanup During Iteration\n\n**The Problem**: Modifying the hash map or linked list while iterating over it. This commonly occurs in the cleanup process where developers try to delete expired entries during a range loop over the items map.\n\n**Why It's Wrong**: Most programming languages, including Go, invalidate iterators when the underlying collection is modified during iteration. This can cause panics, infinite loops, or missed entries during cleanup.\n\n**How to Fix**: Collect keys to delete in a separate slice during iteration, then delete them in a second pass. Never modify collections while iterating over them.\n\n**Correct Pattern**:\n1. First pass: iterate and collect expired keys\n2. Second pass: delete collected keys from data structures\n\n#### ⚠️ Pitfall: Zero Time Comparison Errors\n\n**The Problem**: Incorrectly handling the zero value of `time.Time` when checking expiration. Developers often write `if entry.ExpiresAt.Before(time.Now())` without checking for the zero value case, causing entries with no expiration to be immediately evicted.\n\n**Why It's Wrong**: The zero value of `time.Time` represents January 1, year 1, 00:00:00 UTC, which is before any current time. This causes all entries with no TTL to appear expired during expiration checks.\n\n**How to Fix**: Always check `entry.ExpiresAt.IsZero()` first to detect no-expiration entries, and only perform time comparison for entries with explicit TTLs.\n\n**Correct Pattern**:\n```go\nif !entry.ExpiresAt.IsZero() && entry.ExpiresAt.Before(time.Now()) {\n    // Entry is expired\n}\n```\n\n#### ⚠️ Pitfall: Linked List Element Reuse\n\n**The Problem**: Attempting to reuse linked list elements after they've been removed from the list, or failing to create new elements when adding entries. This often happens when developers try to optimize memory allocations by reusing list elements.\n\n**Why It's Wrong**: Once a linked list element is removed from a list, its internal pointers may be corrupted or point to invalid memory locations. Reusing such elements can cause crashes or infinite loops when traversing the list.\n\n**How to Fix**: Always create new list elements when adding entries to the cache. Never reuse elements that have been removed from the list. The Go garbage collector efficiently handles the allocation and cleanup of small objects like list elements.\n\n#### ⚠️ Pitfall: Partial Entry Updates\n\n**The Problem**: Updating some fields of a cache entry (like the value) without updating other related fields (like the size or expiration time). This creates inconsistency between the entry's metadata and actual data.\n\n**Why It's Wrong**: Stale size information causes memory accounting errors. Stale expiration times cause entries to expire too early or too late. These inconsistencies compound over time and can cause significant behavioral anomalies.\n\n**How to Fix**: Always treat cache entries as immutable. When updating an entry, create a new `CacheEntry` with all correct fields rather than modifying an existing entry. This prevents partial update bugs and makes the code easier to reason about.\n\n#### ⚠️ Pitfall: Cleanup Process Blocking Operations\n\n**The Problem**: Running the periodic cleanup process while holding the write lock for extended periods, blocking all cache operations during cleanup. This is especially problematic for large caches where cleanup might take significant time.\n\n**Why It's Wrong**: Holding locks during long-running operations creates availability problems. All Get, Set, and Delete operations block until cleanup completes, causing user-visible latency spikes and potential timeout errors.\n\n**How to Fix**: Batch cleanup work and periodically release the lock to allow normal operations to proceed. Process cleanup in smaller chunks with lock release between chunks, or use more sophisticated techniques like double-buffering for very large caches.\n\n### Implementation Guidance\n\nThis section provides practical implementation details to help translate the cache node design into working code. The guidance focuses on Go as the primary language while providing principles applicable to other languages.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Hash Function | `hash/fnv` (FNV-1a) | `crypto/sha256` for cryptographic properties |\n| Linked List | `container/list` (built-in) | Custom implementation for memory optimization |\n| JSON Serialization | `encoding/json` (standard library) | `github.com/json-iterator/go` for performance |\n| Time Handling | `time` package (standard library) | `time` with monotonic clock considerations |\n| Logging | `log` package (standard library) | `github.com/sirupsen/logrus` for structured logging |\n| Testing | `testing` package (standard library) | `github.com/stretchr/testify` for assertions |\n\n#### Recommended File Structure\n\n```\ninternal/cache/\n  cache.go              ← Core LRUCache implementation\n  cache_test.go         ← Unit tests for cache operations\n  entry.go              ← CacheEntry struct and methods\n  metrics.go            ← Memory usage and performance metrics\n  cleanup.go            ← TTL cleanup background process\n  \ninternal/config/\n  config.go             ← NodeConfig structure and validation\n  config_test.go        ← Configuration loading tests\n  \ncmd/cache-node/\n  main.go               ← Server entry point and initialization\n```\n\nThis structure separates concerns while keeping related functionality together. The `internal` packages prevent external imports while the `cmd` package provides the executable entry point.\n\n#### Infrastructure Starter Code\n\n**Configuration Management (config.go):**\n\n```go\npackage config\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"time\"\n)\n\ntype NodeConfig struct {\n    NodeID              string        `json:\"node_id\"`\n    ListenAddress       string        `json:\"listen_address\"`\n    AdvertiseAddr       string        `json:\"advertise_address\"`\n    JoinAddresses       []string      `json:\"join_addresses\"`\n    MaxMemoryMB         int           `json:\"max_memory_mb\"`\n    VirtualNodes        int           `json:\"virtual_nodes\"`\n    ReplicationFactor   int           `json:\"replication_factor\"`\n    HealthCheckInterval time.Duration `json:\"health_check_interval\"`\n    GossipInterval      time.Duration `json:\"gossip_interval\"`\n    RequestTimeout      time.Duration `json:\"request_timeout\"`\n}\n\nfunc LoadConfig(filename string) (*NodeConfig, error) {\n    data, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, fmt.Errorf(\"reading config file: %w\", err)\n    }\n    \n    var config NodeConfig\n    if err := json.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"parsing config JSON: %w\", err)\n    }\n    \n    if err := config.Validate(); err != nil {\n        return nil, fmt.Errorf(\"invalid configuration: %w\", err)\n    }\n    \n    return &config, nil\n}\n\nfunc (c *NodeConfig) Validate() error {\n    if c.NodeID == \"\" {\n        return fmt.Errorf(\"node_id is required\")\n    }\n    if c.ListenAddress == \"\" {\n        return fmt.Errorf(\"listen_address is required\")\n    }\n    if c.MaxMemoryMB <= 0 {\n        return fmt.Errorf(\"max_memory_mb must be positive, got %d\", c.MaxMemoryMB)\n    }\n    if c.VirtualNodes <= 0 {\n        return fmt.Errorf(\"virtual_nodes must be positive, got %d\", c.VirtualNodes)\n    }\n    return nil\n}\n\n// SetDefaults fills in reasonable default values for optional fields\nfunc (c *NodeConfig) SetDefaults() {\n    if c.HealthCheckInterval == 0 {\n        c.HealthCheckInterval = 30 * time.Second\n    }\n    if c.GossipInterval == 0 {\n        c.GossipInterval = 5 * time.Second\n    }\n    if c.RequestTimeout == 0 {\n        c.RequestTimeout = 10 * time.Second\n    }\n    if c.ReplicationFactor == 0 {\n        c.ReplicationFactor = 1\n    }\n    if c.VirtualNodes == 0 {\n        c.VirtualNodes = 150\n    }\n}\n```\n\n**Cache Entry Implementation (entry.go):**\n\n```go\npackage cache\n\nimport (\n    \"time\"\n)\n\ntype CacheEntry struct {\n    Key       string    `json:\"key\"`\n    Value     []byte    `json:\"value\"`\n    ExpiresAt time.Time `json:\"expires_at\"`\n    Size      int64     `json:\"size\"`\n}\n\n// NewCacheEntry creates a cache entry with calculated size and optional TTL\nfunc NewCacheEntry(key string, value []byte, ttl time.Duration) *CacheEntry {\n    var expiresAt time.Time\n    if ttl > 0 {\n        expiresAt = time.Now().Add(ttl)\n    }\n    \n    entry := &CacheEntry{\n        Key:       key,\n        Value:     make([]byte, len(value)), // Copy to avoid external mutations\n        ExpiresAt: expiresAt,\n        Size:      calculateEntrySize(key, value),\n    }\n    \n    copy(entry.Value, value)\n    return entry\n}\n\n// IsExpired returns true if the entry has exceeded its TTL\nfunc (e *CacheEntry) IsExpired() bool {\n    if e.ExpiresAt.IsZero() {\n        return false // No expiration set\n    }\n    return time.Now().After(e.ExpiresAt)\n}\n\n// calculateEntrySize estimates total memory usage including metadata\nfunc calculateEntrySize(key string, value []byte) int64 {\n    // Key string + value slice + CacheEntry struct + list element + map overhead\n    return int64(len(key) + len(value) + 96) // 96 bytes estimated metadata overhead\n}\n```\n\n**Metrics Collection (metrics.go):**\n\n```go\npackage cache\n\nimport (\n    \"sync/atomic\"\n    \"time\"\n)\n\ntype CacheMetrics struct {\n    hits           int64\n    misses         int64\n    sets           int64\n    deletes        int64\n    evictions      int64\n    expiredCleanup int64\n    totalMemory    int64\n    entryCount     int64\n    startTime      time.Time\n}\n\nfunc NewCacheMetrics() *CacheMetrics {\n    return &CacheMetrics{\n        startTime: time.Now(),\n    }\n}\n\nfunc (m *CacheMetrics) RecordHit() {\n    atomic.AddInt64(&m.hits, 1)\n}\n\nfunc (m *CacheMetrics) RecordMiss() {\n    atomic.AddInt64(&m.misses, 1)\n}\n\nfunc (m *CacheMetrics) RecordSet() {\n    atomic.AddInt64(&m.sets, 1)\n}\n\nfunc (m *CacheMetrics) RecordDelete() {\n    atomic.AddInt64(&m.deletes, 1)\n}\n\nfunc (m *CacheMetrics) RecordEviction() {\n    atomic.AddInt64(&m.evictions, 1)\n}\n\nfunc (m *CacheMetrics) RecordExpiredCleanup(count int64) {\n    atomic.AddInt64(&m.expiredCleanup, count)\n}\n\nfunc (m *CacheMetrics) UpdateMemory(used, entries int64) {\n    atomic.StoreInt64(&m.totalMemory, used)\n    atomic.StoreInt64(&m.entryCount, entries)\n}\n\nfunc (m *CacheMetrics) GetStats() map[string]interface{} {\n    hits := atomic.LoadInt64(&m.hits)\n    misses := atomic.LoadInt64(&m.misses)\n    total := hits + misses\n    \n    var hitRate float64\n    if total > 0 {\n        hitRate = float64(hits) / float64(total)\n    }\n    \n    return map[string]interface{}{\n        \"hit_rate\":         hitRate,\n        \"total_requests\":   total,\n        \"hits\":            hits,\n        \"misses\":          misses,\n        \"sets\":            atomic.LoadInt64(&m.sets),\n        \"deletes\":         atomic.LoadInt64(&m.deletes),\n        \"evictions\":       atomic.LoadInt64(&m.evictions),\n        \"expired_cleanup\": atomic.LoadInt64(&m.expiredCleanup),\n        \"memory_used\":     atomic.LoadInt64(&m.totalMemory),\n        \"entry_count\":     atomic.LoadInt64(&m.entryCount),\n        \"uptime_seconds\":  time.Since(m.startTime).Seconds(),\n    }\n}\n```\n\n#### Core Cache Implementation Skeleton\n\n**LRU Cache Structure (cache.go):**\n\n```go\npackage cache\n\nimport (\n    \"container/list\"\n    \"sync\"\n    \"time\"\n)\n\ntype LRUCache struct {\n    mutex    sync.RWMutex\n    capacity int64\n    used     int64\n    items    map[string]*list.Element\n    order    *list.List\n    metrics  *CacheMetrics\n}\n\n// NewLRUCache creates an LRU cache with the specified memory capacity\nfunc NewLRUCache(capacityBytes int64) *LRUCache {\n    return &LRUCache{\n        capacity: capacityBytes,\n        used:     0,\n        items:    make(map[string]*list.Element),\n        order:    list.New(),\n        metrics:  NewCacheMetrics(),\n    }\n}\n\n// Get retrieves a value from the cache and updates its position in LRU order\nfunc (c *LRUCache) Get(key string) ([]byte, bool) {\n    // TODO 1: Acquire read lock to safely access cache structures\n    // TODO 2: Look up the key in the items map to get list element\n    // TODO 3: If key not found, record miss and return false\n    // TODO 4: Extract CacheEntry from list element value\n    // TODO 5: Check if entry is expired using IsExpired() method\n    // TODO 6: If expired, upgrade to write lock and remove entry completely\n    // TODO 7: If valid, upgrade to write lock and move element to front\n    // TODO 8: Record hit in metrics and return value copy\n    // TODO 9: Always release locks in defer statements for safety\n}\n\n// Set stores a value in the cache with optional TTL\nfunc (c *LRUCache) Set(key string, value []byte, ttl time.Duration) {\n    // TODO 1: Create new CacheEntry with calculated size\n    // TODO 2: Acquire write lock for exclusive access\n    // TODO 3: If key already exists, remove old entry from accounting\n    // TODO 4: Check if new entry fits within capacity\n    // TODO 5: While over capacity, evict LRU entries from back of list\n    // TODO 6: Create new list element and add to front (MRU position)\n    // TODO 7: Add key->element mapping to items map\n    // TODO 8: Update memory usage accounting\n    // TODO 9: Record set operation in metrics\n}\n\n// Delete removes a key from the cache if it exists\nfunc (c *LRUCache) Delete(key string) bool {\n    // TODO 1: Acquire write lock for exclusive access\n    // TODO 2: Look up key in items map\n    // TODO 3: If not found, record delete attempt and return false\n    // TODO 4: Remove element from linked list using list.Remove()\n    // TODO 5: Delete key from items map\n    // TODO 6: Update memory usage by subtracting entry size\n    // TODO 7: Record successful delete in metrics and return true\n}\n\n// CleanupExpired removes expired entries and returns count cleaned\nfunc (c *LRUCache) CleanupExpired() int {\n    // TODO 1: Acquire write lock for exclusive cleanup access\n    // TODO 2: Create slice to collect expired entry keys\n    // TODO 3: Iterate through linked list from back (LRU) to front\n    // TODO 4: For each entry, check if expired using IsExpired()\n    // TODO 5: If expired, add key to collection slice\n    // TODO 6: If non-expired found, break early (optimization)\n    // TODO 7: Remove all collected keys from map, list, and accounting\n    // TODO 8: Update metrics and return count of cleaned entries\n}\n\n// evictLRU removes the least recently used entry\nfunc (c *LRUCache) evictLRU() bool {\n    // TODO 1: Check if list has any elements\n    // TODO 2: Get back element (least recently used)\n    // TODO 3: Extract CacheEntry and remove from all structures\n    // TODO 4: Update memory accounting and metrics\n    // TODO 5: Return true if eviction occurred\n}\n\n// GetMetrics returns current cache performance statistics\nfunc (c *LRUCache) GetMetrics() map[string]interface{} {\n    c.mutex.RLock()\n    c.metrics.UpdateMemory(c.used, int64(len(c.items)))\n    c.mutex.RUnlock()\n    \n    return c.metrics.GetStats()\n}\n```\n\n#### Language-Specific Implementation Hints\n\n**Go-Specific Optimizations:**\n- Use `sync.RWMutex` for reader-writer concurrency patterns\n- Copy byte slices with `make()` and `copy()` to prevent external mutations\n- Use `atomic` package for metrics counters to avoid lock contention\n- Leverage `time.Time.IsZero()` for TTL handling edge cases\n- Use `defer` statements for reliable lock cleanup\n\n**Memory Management Best Practices:**\n- Pre-allocate slices with known capacity using `make([]string, 0, expectedSize)`\n- Avoid string concatenation in hot paths; use `strings.Builder` instead\n- Consider using `sync.Pool` for frequently allocated temporary objects\n- Profile memory usage with `go tool pprof` to identify allocation hotspots\n\n**Concurrency Patterns:**\n```go\n// Lock upgrade pattern for Get operations\nc.mutex.RLock()\nelement, exists := c.items[key]\nif !exists {\n    c.mutex.RUnlock()\n    return nil, false\n}\n\n// Upgrade to write lock for modification\nc.mutex.RUnlock()\nc.mutex.Lock()\ndefer c.mutex.Unlock()\n\n// Re-check existence after lock upgrade\nif element, exists := c.items[key]; exists {\n    c.order.MoveToFront(element)\n    // ... continue with operation\n}\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the cache node, verify functionality with these checkpoints:\n\n**Unit Test Verification:**\n```bash\ncd internal/cache\ngo test -v -race ./...\n```\nExpected output should show all tests passing with race detection enabled.\n\n**Manual Testing Commands:**\n```bash\n# Start cache node\ngo run cmd/cache-node/main.go -config=config.json\n\n# Test basic operations (implement simple HTTP handlers)\ncurl -X POST http://localhost:8080/cache/test -d '{\"value\":\"hello\",\"ttl\":\"60s\"}'\ncurl -X GET http://localhost:8080/cache/test\ncurl -X DELETE http://localhost:8080/cache/test\n```\n\n**Performance Verification:**\nRun benchmark tests to ensure O(1) operation performance:\n```bash\ngo test -bench=. -benchmem ./internal/cache/\n```\nLook for consistent operation times regardless of cache size and reasonable memory allocations per operation.\n\n**Memory Limit Testing:**\nConfigure a small memory limit (e.g., 10MB) and verify eviction behavior:\n1. Insert entries until memory limit reached\n2. Verify oldest entries are evicted first\n3. Confirm memory usage stays within configured bounds\n4. Test TTL cleanup reduces memory usage over time\n\n**Signs of Correct Implementation:**\n- Cache operations complete in constant time regardless of cache size\n- Memory usage accurately tracks configured limits with eviction\n- Expired entries are never returned and are cleaned up periodically\n- Concurrent access works without race conditions (verified with `-race` flag)\n- Metrics accurately reflect cache behavior and performance\n\n\n## Cluster Communication and Discovery\n\n> **Milestone(s):** This section covers Milestone 3 (Cluster Communication), building on the hash ring foundation from Milestone 1 and cache node implementation from Milestone 2 to enable distributed coordination and fault tolerance.\n\nThe third milestone transforms our distributed cache from individual nodes into a coordinated cluster. While the previous milestones established how to distribute keys across nodes and implement efficient storage, this milestone addresses the fundamental challenge of distributed systems: how do nodes discover each other, maintain awareness of cluster membership, detect failures, and coordinate their activities without a central authority?\n\nThe core architectural principle driving this milestone is **peer-to-peer cluster design** with **symmetric responsibility**. Every node in the cluster has equal status and capability—there are no dedicated coordinator nodes, no single points of failure, and no hierarchical relationships. This design choice provides excellent fault tolerance and horizontal scalability, but introduces the complexity of achieving consensus about cluster state through distributed protocols.\n\n### Mental Model: The Neighborhood Watch\n\nThink of the cluster communication system as a **neighborhood watch program** where residents (nodes) keep track of who lives in the neighborhood, monitor each other's wellbeing, and share important information through a network of conversations.\n\nIn this analogy, each house represents a cache node, and the residents need to maintain awareness of their neighborhood without relying on a central authority like a homeowners association office. Here's how the parallel works:\n\n**Node Discovery** is like new residents introducing themselves to the neighborhood. When someone moves in, they walk around and introduce themselves to their immediate neighbors. Those neighbors then mention the newcomer during their regular conversations with other neighbors, and gradually everyone in the neighborhood learns about the new resident. No one needs to maintain a master directory—the information spreads naturally through the social network.\n\n**Health Checking** resembles neighbors looking out for each other's wellbeing. If Mrs. Johnson usually takes her morning walk but hasn't been seen for several days, her immediate neighbors become concerned. They might knock on her door or peer over the fence to check if everything is okay. If she doesn't respond, they spread word through the neighborhood that something might be wrong with Mrs. Johnson.\n\n**Gossip Protocol** mirrors the way information spreads through casual conversations. When neighbors chat over the fence or meet at the mailbox, they naturally share news: \"Did you hear the Smiths are moving out next month?\" or \"I haven't seen Bob from number 47 in days—hope he's okay.\" This information doesn't travel in a structured broadcast; instead, it spreads organically as neighbors talk to each other, with each person sharing the most interesting or concerning updates they've heard.\n\n**Request Routing** is like knowing which neighbor to ask for help with specific problems. If you need to borrow a power drill, you don't knock on every door—you know that Tom three houses down is the tool guy, so you go directly to him. Similarly, when a cache request arrives for a key, the receiving node needs to know which node in the cluster is responsible for that key and either handle it locally or forward it to the correct neighbor.\n\nThe beauty of this neighborhood watch model is its resilience. If one neighbor goes on vacation (node failure), the others continue sharing information and looking out for each other. If several new families move in simultaneously (multiple nodes joining), the existing social network naturally expands to include them. There's no single point of failure because the system depends on the collective behavior of the community, not on any individual member.\n\nThis mental model helps explain why distributed coordination is more complex than centralized approaches—achieving consensus through informal social networks requires more sophisticated protocols than simply checking with a central authority—but also why it's more robust and scalable in the long run.\n\n### Node Discovery Mechanism\n\nNode discovery solves the bootstrap problem: how does a new node find and join an existing cluster, and how do existing cluster members learn about the newcomer? The challenge is particularly acute in dynamic environments where nodes can join and leave frequently, IP addresses might change, and there's no guarantee that any particular node will always be available to serve as an entry point.\n\nOur discovery mechanism uses a **bootstrap-then-gossip** approach that combines initial contact with organic information propagation. When a new node starts up, it needs at least one existing cluster member to contact—this is provided through the `JoinAddresses` configuration parameter. However, once the initial contact is made, all further discovery happens through the gossip protocol, making the system resilient to changes in the bootstrap nodes.\n\nThe discovery process follows this sequence:\n\n1. **Bootstrap Contact Phase**: The new node attempts to contact each address in its `JoinAddresses` list until it successfully reaches an active cluster member. This contact serves two purposes: it announces the new node's presence to the cluster, and it downloads the current cluster membership information.\n\n2. **Membership Synchronization**: The contacted node responds with its current view of cluster membership, including all known nodes, their addresses, current health status, and version information. This gives the new node an immediate snapshot of the cluster topology.\n\n3. **Cluster Announcement**: The contacted node updates its local membership view to include the new node and begins propagating this information through the gossip protocol. The announcement includes the new node's identity, address, capabilities, and initial health status.\n\n4. **Gossip Integration**: As gossip messages propagate through the cluster, all nodes learn about the newcomer within a few gossip cycles. The new node also begins participating in the gossip protocol, both sending and receiving membership updates.\n\n5. **Hash Ring Integration**: Once cluster members are aware of the new node, they update their consistent hash rings to include it. This triggers key redistribution as some keys now map to the new node instead of their previous owners.\n\nThe discovery mechanism must handle several challenging scenarios. **Split-brain recovery** occurs when network partitions heal and previously isolated sub-clusters need to merge their membership views. **Concurrent joins** happen when multiple nodes attempt to join simultaneously, potentially causing inconsistent membership states. **Bootstrap node failures** require the system to continue functioning even if some or all of the original bootstrap addresses become unavailable.\n\nTo address these challenges, the discovery mechanism implements several sophisticated behaviors:\n\n| Scenario | Detection | Response |\n|----------|-----------|----------|\n| Split-brain merge | Gossip messages contain unfamiliar node IDs | Merge membership lists, resolve conflicts using node startup timestamps |\n| Concurrent joins | Multiple membership updates with overlapping timestamps | Apply updates in deterministic order based on node ID lexicographic sorting |\n| Bootstrap failure | Connection timeouts during initial contact | Try remaining bootstrap addresses, continue with partial cluster view if any succeed |\n| Duplicate joins | Node ID appears twice with different addresses | Keep entry with most recent timestamp, mark older entry as stale |\n| Address changes | Node appears with new address but same ID | Update address, increment version number, propagate change via gossip |\n\nThe membership information maintained by each node includes comprehensive state about every cluster member:\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| NodeID | string | Unique identifier for the node, typically a UUID generated at startup |\n| Address | string | Network address (IP:port) where the node can be contacted |\n| Status | string | Current health state: \"active\", \"suspected\", \"failed\", \"leaving\" |\n| LastSeen | time.Time | Timestamp of most recent successful communication with this node |\n| Version | uint64 | Monotonically increasing counter for detecting stale information |\n| Capabilities | map[string]string | Node-specific metadata like supported protocols or capacity limits |\n| JoinTime | time.Time | When this node first joined the cluster (used for split-brain resolution) |\n\nThe discovery mechanism also implements **lazy cleanup** of stale membership information. Nodes that haven't been seen for extended periods are gradually demoted from \"suspected\" to \"failed\" status, and eventually removed from the membership list entirely. This prevents unbounded growth of the membership table in long-running clusters with high node turnover.\n\n### Health Checking and Failure Detection\n\nFailure detection in distributed systems faces the fundamental challenge of distinguishing between node failures and network partitions. When node A cannot contact node B, it's impossible to determine with certainty whether B has crashed or if the network path between them has failed. Our health checking system addresses this ambiguity through a combination of **direct probing** and **indirect confirmation** that provides rapid failure detection while minimizing false positives.\n\nThe health checking mechanism operates on multiple time scales to balance responsiveness with stability. **Fast detection** happens through direct health probes sent every `HealthCheckInterval` (typically 1-2 seconds). **Slow confirmation** occurs through gossip message correlation over longer time periods (typically 30-60 seconds). This dual-layer approach allows the system to quickly suspect failures while avoiding hasty decisions that could cause unnecessary churn.\n\nEach node maintains a **failure detector state machine** for every other cluster member:\n\n| Current State | Probe Result | Gossip Evidence | Next State | Actions Taken |\n|---------------|--------------|-----------------|------------|---------------|\n| Active | Success | Recent sightings | Active | Reset failure counters, update LastSeen timestamp |\n| Active | Timeout | Recent sightings | Active | Increment timeout counter, suspect if threshold exceeded |\n| Active | Timeout | No recent sightings | Suspected | Mark as suspected, begin indirect confirmation attempts |\n| Suspected | Success | Recent sightings | Active | Clear suspicion, restore to active status |\n| Suspected | Timeout | No recent sightings | Suspected | Continue indirect probing, fail if timeout threshold exceeded |\n| Suspected | Timeout | Confirmed by others | Failed | Mark as failed, begin gossip propagation of failure |\n| Failed | Success | Recent sightings | Active | Node has recovered, restore to active status |\n| Failed | Any | No activity | Failed | Maintain failed status, consider removal from membership |\n\nThe **direct probing** mechanism sends lightweight health check messages to other cluster members at regular intervals. These probes serve multiple purposes beyond simple liveness detection. They carry **piggybacked information** including recent gossip updates, local load metrics, and membership version numbers. This approach maximizes the value of each network round-trip and helps keep cluster state synchronized.\n\nThe health check message format includes diagnostic information that helps with failure attribution:\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| ProbeID | string | Unique identifier for tracking probe round-trips |\n| Timestamp | time.Time | When the probe was sent (for latency measurement) |\n| SenderLoad | float64 | Current CPU/memory utilization of sending node |\n| MembershipVersion | uint64 | Sender's current membership list version |\n| RecentFailures | []string | Nodes the sender has recently marked as failed |\n| PiggybackGossip | GossipMessage | Recent gossip updates to synchronize cluster state |\n\n**Indirect confirmation** activates when direct probes begin failing. Rather than immediately marking a node as failed, the detecting node asks several other cluster members to probe the suspected node independently. If multiple nodes report probe failures, confidence in the failure assessment increases. If some nodes can still reach the suspected node, this suggests a network partition rather than a node failure, and the detector remains in the \"suspected\" state rather than escalating to \"failed.\"\n\nThe indirect confirmation process uses a **witness selection algorithm** that chooses probe witnesses based on network diversity. Witnesses are selected from different network segments when possible, reducing the likelihood that a localized network issue will cause false failure detection. The witness selection considers factors like network latency patterns, recent communication success rates, and physical topology hints when available.\n\n> **Key Design Insight**: The failure detection system optimizes for **completeness over accuracy**—it's better to occasionally suspect a healthy node (false positive) than to miss the failure of a crashed node (false negative). False positives cause temporary performance degradation as the cluster works around a supposedly failed node, but false negatives can cause data loss or extended unavailability as the cluster continues trying to use a failed node.\n\nThe health checking system implements **adaptive timing** that adjusts probe intervals based on cluster stability. During stable periods with no recent failures, probe intervals can be lengthened to reduce network overhead. During unstable periods with recent failures or frequent membership changes, probe intervals are shortened to improve detection speed. This adaptivity helps the system scale efficiently while maintaining responsiveness during critical periods.\n\nFailure detection also integrates with the consistent hash ring to minimize disruption during suspected failures. When a node enters the \"suspected\" state, the hash ring temporarily routes traffic away from it while leaving it in the ring structure. This provides **graceful degradation**—if the suspicion is false, the node can quickly resume normal operation without requiring expensive ring restructuring. Only when a node is confirmed as \"failed\" does the hash ring undergo the costly process of removing the node and redistributing its keys.\n\n### Request Routing Logic\n\nRequest routing transforms the distributed cache from a collection of independent nodes into a unified system that appears as a single logical cache to clients. The routing system must seamlessly direct each cache operation to the appropriate node based on consistent hashing while handling the complexities of node failures, network partitions, and cluster membership changes.\n\nThe routing architecture follows a **symmetric proxy model** where every node can accept requests for any key, regardless of which node actually stores the data. This design provides excellent load distribution and eliminates single points of failure, but requires sophisticated forwarding logic to ensure requests reach the correct destination with minimal latency and overhead.\n\nWhen a client request arrives at any node, the routing system follows this decision process:\n\n1. **Hash Ring Lookup**: The receiving node computes the hash of the request key and queries its local hash ring to determine which node should handle this key. This lookup considers the current cluster membership and any ongoing rebalancing operations.\n\n2. **Local vs. Remote Determination**: If the hash ring indicates the current node is responsible for the key, the request is handled locally using the node's `LRUCache`. If another node is responsible, the request requires forwarding.\n\n3. **Target Node Health Check**: Before forwarding, the routing system verifies that the target node is currently healthy and reachable. If the target is suspected or failed, the router consults the hash ring for the next available replica node.\n\n4. **Forward or Handle Locally**: For remote requests, the router serializes the request and sends it to the target node using the `HTTPTransport`. For local requests, it directly invokes the appropriate cache operation.\n\n5. **Response Processing**: Remote responses are deserialized and returned to the client. Local responses are returned directly. Both paths include error handling for timeouts, serialization failures, and data consistency issues.\n\nThe routing system maintains **request correlation state** to handle timeouts, retries, and partial failures gracefully:\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| RequestID | string | Unique identifier for tracking requests across nodes |\n| ClientAddress | string | Original client address for routing responses |\n| OriginalTimestamp | time.Time | When the request first entered the cluster |\n| ForwardPath | []string | Nodes that have forwarded this request (prevents loops) |\n| TimeoutDeadline | time.Time | When to abandon the request and return an error |\n| RetryCount | int | Number of retry attempts made for this request |\n| ConsistencyLevel | string | Required consistency level for the operation |\n\nOne of the most challenging aspects of request routing is handling **cluster membership changes** during active requests. Consider a scenario where a client sends a GET request for key \"user:12345\" to node A, which forwards it to node B based on its current hash ring view. However, while the request is in transit, node B fails and is removed from the hash ring, causing key \"user:12345\" to now map to node C. The routing system must detect this situation and adapt gracefully.\n\nTo handle membership changes, the routing system implements several sophisticated strategies:\n\n**Versioned Ring Consistency**: Each request carries the hash ring version number from the originating node. The target node compares this version with its current ring version. If the versions differ significantly, the target node can detect that a membership change has occurred and potentially invalidate the routing decision.\n\n**Graceful Redirect**: When a node receives a request for a key it no longer owns (due to recent ring changes), it doesn't immediately return an error. Instead, it consults its current hash ring, determines the new correct node, and either forwards the request automatically or returns a redirect response to the client.\n\n**Temporal Overlap Handling**: During node additions or removals, there's a brief period where multiple nodes might legitimately handle requests for the same keys. The routing system uses timestamp-based conflict resolution to ensure consistency during these transition periods.\n\nThe request forwarding mechanism implements **circuit breaker** patterns to prevent cascading failures when target nodes become overloaded or unresponsive:\n\n| Circuit State | Trigger Condition | Forwarding Behavior | Recovery Condition |\n|---------------|-------------------|---------------------|--------------------|\n| Closed | Normal operation | Forward all requests normally | N/A - healthy operation |\n| Half-Open | Some failures detected | Forward limited test requests | Several successful requests |\n| Open | High failure rate reached | Return errors immediately | Timeout period expires |\n\n**Loop Prevention** is critical in the symmetric proxy model since requests can potentially bounce between nodes indefinitely if there are inconsistencies in hash ring views. The routing system prevents loops through several mechanisms:\n\n- **Hop Count Limiting**: Each request includes a maximum hop count that decreases with each forward. Requests are abandoned if the hop limit is reached.\n- **Forward Path Tracking**: The `ForwardPath` field records which nodes have handled the request, preventing cycles.\n- **Ring Version Propagation**: Nodes with stale ring views update their membership information when they receive requests with newer version numbers.\n\nThe routing system also optimizes for **request locality** by implementing intelligent caching of routing decisions. Frequently accessed keys have their routing information cached locally, reducing the overhead of hash ring lookups for hot data. This cache is invalidated appropriately when membership changes occur, ensuring correctness while improving performance for stable workloads.\n\nFor **batch operations** and **multi-key requests**, the routing system implements request splitting and parallel execution. A client request involving multiple keys is automatically divided into sub-requests based on the hash ring distribution, sent to the appropriate nodes in parallel, and the results are merged before returning to the client. This provides significant performance benefits for workloads with spatial locality in their key access patterns.\n\n### Gossip Protocol Implementation\n\nThe gossip protocol serves as the nervous system of the distributed cache, propagating cluster membership information, failure notifications, and topology changes throughout the network without requiring centralized coordination. Like its biological namesake, gossip spreads information through periodic, random exchanges between cluster members, ensuring that important updates eventually reach all nodes while providing remarkable resilience to network failures and partitions.\n\n![Gossip Protocol Flow](./diagrams/gossip-protocol-flow.svg)\n\nThe protocol operates on the principle of **eventual consistency**—there's no guarantee that all nodes have identical membership views at any given moment, but the system converges toward consensus over time. This relaxed consistency model enables high availability and partition tolerance while maintaining sufficient coordination for the cache to function correctly.\n\nEach node maintains a **membership table** that represents its current view of the cluster state. This table is continuously updated through gossip exchanges and serves as the source of truth for local routing decisions. The table includes not only basic membership information but also **version vectors** and **conflict resolution metadata** that enable deterministic reconciliation of divergent views.\n\nThe gossip message structure carries comprehensive state information that enables recipient nodes to update their local views efficiently:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| SenderID | string | Identity of the node sending this gossip message |\n| MessageID | string | Unique identifier for deduplication and tracking |\n| Timestamp | time.Time | When this gossip message was generated |\n| MembershipVersion | uint64 | Sender's current version of the membership table |\n| NodeStates | map[string]NodeState | Complete or partial membership information |\n| RecentChanges | []MembershipEvent | Recent join/leave/failure events with timestamps |\n| Checksum | string | Hash of the membership data for integrity verification |\n\nThe **gossip scheduling algorithm** determines when to send gossip messages and which nodes to contact. The system uses a combination of **periodic gossip** (sent at regular intervals) and **triggered gossip** (sent immediately after important events like failures or joins). The target selection balances randomness with connectivity optimization—nodes prefer to gossip with peers they haven't communicated with recently, ensuring information spreads throughout the cluster rather than circulating within small subgroups.\n\n> **Design Decision: Push vs. Pull Gossip**\n> - **Context**: Gossip protocols can be push-based (nodes send updates proactively), pull-based (nodes request updates from others), or hybrid approaches.\n> - **Options Considered**: \n>   - Pure push: Simple implementation, but may miss nodes or flood the network\n>   - Pure pull: More bandwidth efficient, but slower propagation of urgent updates  \n>   - Push-pull hybrid: Best of both approaches, but increased complexity\n> - **Decision**: Hybrid push-pull with adaptive behavior based on cluster stability\n> - **Rationale**: Push provides rapid dissemination of critical events like failures, while pull ensures eventual consistency and handles network partitions gracefully. Adaptive behavior optimizes bandwidth usage during stable periods.\n> - **Consequences**: More complex implementation but superior performance characteristics and partition resilience\n\nThe gossip protocol implements **anti-entropy** mechanisms that detect and repair inconsistencies in membership views. When two nodes exchange gossip messages, they compare their membership versions and node state information. If discrepancies are discovered, the nodes engage in a **state reconciliation process** that merges their views using deterministic conflict resolution rules.\n\nState reconciliation follows these priority rules:\n\n1. **Timestamp Ordering**: More recent updates override older ones, based on the timestamp associated with each membership change\n2. **Node Self-Authority**: Nodes are authoritative about their own state—if node A claims to be active and node B claims A is failed, node A's claim takes precedence if A is reachable\n3. **Majority Consensus**: For conflicting claims about third-party nodes, the view supported by more cluster members prevails\n4. **Lexicographic Tiebreaking**: When timestamps are identical, node IDs are compared lexicographically to ensure deterministic outcomes\n\nThe protocol handles **network partitions** through sophisticated partition detection and healing mechanisms. When nodes lose contact with portions of the cluster, they continue operating with their local membership view while marking unreachable nodes as \"suspected.\" The gossip protocol includes **partition identifiers** that help detect when network connectivity is restored and previously isolated sub-clusters need to merge their state.\n\n**Partition healing** follows a careful process to prevent data inconsistencies and membership conflicts:\n\n1. **Reachability Testing**: When a previously unreachable node becomes contactable again, the local node performs comprehensive reachability tests to verify connectivity is stable\n2. **State Exchange**: The nodes exchange their complete membership tables and identify all discrepancies that accumulated during the partition\n3. **Conflict Resolution**: Membership conflicts are resolved using the standard reconciliation rules, with special handling for nodes that may have failed and recovered during the partition\n4. **Ring Synchronization**: Once membership agreement is reached, both nodes update their hash rings and begin redistributing keys as necessary\n\nThe gossip protocol incorporates **epidemic dissemination** characteristics that provide mathematical guarantees about information propagation speed. Under normal conditions, gossip messages reach all cluster members within O(log N) gossip rounds, where N is the cluster size. This logarithmic propagation time means the protocol scales efficiently even to very large clusters.\n\n**Bandwidth optimization** is critical for gossip protocols since naive implementations can generate excessive network traffic. Our implementation uses several techniques to minimize overhead:\n\n- **Delta Compression**: Instead of sending complete membership tables, nodes send only the changes since their last communication with each peer\n- **Piggybacking**: Gossip information is attached to other messages like health checks and client responses when possible\n- **Adaptive Frequency**: Gossip intervals are increased during stable periods and decreased during cluster changes\n- **Message Batching**: Multiple membership updates are combined into single gossip messages when they occur in rapid succession\n\nThe protocol also implements **failure amplification** to ensure that critical events like node failures propagate rapidly throughout the cluster. When a node detects a failure, it immediately sends gossip messages to multiple peers rather than waiting for the next scheduled gossip round. These urgent messages are marked with high priority and trigger additional gossip rounds at the recipients, creating an epidemic of failure notification that reaches all cluster members quickly.\n\n**Gossip message ordering** presents challenges since messages can arrive out of order due to network delays and routing variations. The protocol uses **vector clocks** to establish causal ordering of membership events, ensuring that updates are applied in the correct sequence even when messages arrive out of order. Each node maintains a vector clock that tracks the logical time of membership changes, and gossip messages include vector clock information that enables proper sequencing at recipients.\n\n### Architecture Decisions\n\nThe cluster communication system required numerous architectural decisions that significantly impact system behavior, performance, and operational characteristics. Each decision involved careful evaluation of alternatives and trade-offs, with the choices optimized for the specific requirements of a distributed cache workload.\n\n> **Decision: Transport Protocol Selection**\n> - **Context**: Nodes need reliable, efficient communication for gossip, health checks, and request forwarding across potentially unreliable networks\n> - **Options Considered**: \n>   - HTTP/1.1 with JSON: Universal compatibility, simple debugging, but higher overhead\n>   - HTTP/2 with Protocol Buffers: Better performance, multiplexing, but increased complexity\n>   - Custom TCP protocol: Maximum efficiency, full control, but significant implementation effort\n> - **Decision**: HTTP/1.1 with JSON for initial implementation, with pluggable transport interface\n> - **Rationale**: HTTP/1.1 provides excellent debugging capabilities, universal firewall compatibility, and rapid development velocity. JSON offers human-readable message formats that simplify troubleshooting. Performance overhead is acceptable for cache workloads where network latency is typically much higher than serialization costs.\n> - **Consequences**: Slightly higher CPU and bandwidth usage compared to binary protocols, but significantly easier operational troubleshooting and broader deployment compatibility\n\n| Transport Option | Pros | Cons | Chosen? |\n|-----------------|------|------|---------|\n| HTTP/1.1 + JSON | Universal compatibility, easy debugging, simple implementation | Higher overhead, limited multiplexing | ✓ Yes |\n| HTTP/2 + Protobuf | Better performance, multiplexing, type safety | Complex debugging, additional dependencies | No |\n| Custom TCP | Maximum efficiency, full control over protocol | High implementation cost, difficult debugging | No |\n\n> **Decision: Discovery Mechanism Design**\n> - **Context**: New nodes need to locate and join existing clusters without requiring centralized directory services or complex orchestration\n> - **Options Considered**:\n>   - Static configuration with full node lists: Simple but brittle in dynamic environments\n>   - Multicast discovery: Automatic but limited to single network segments\n>   - Bootstrap nodes with gossip propagation: Hybrid approach balancing simplicity and flexibility\n> - **Decision**: Bootstrap node list with gossip-based propagation and automatic failover\n> - **Rationale**: Bootstrap nodes provide reliable entry points for cluster joining while gossip propagation eliminates long-term dependencies on bootstrap nodes. This approach works across network segments and provides resilience against bootstrap node failures.\n> - **Consequences**: Requires initial configuration of bootstrap addresses, but provides excellent long-term stability and supports dynamic cluster membership\n\n> **Decision: Failure Detection Strategy**\n> - **Context**: Distinguish between node failures and network partitions while balancing detection speed against false positive rates\n> - **Options Considered**:\n>   - Simple timeout-based detection: Fast but prone to false positives during network congestion\n>   - Consensus-based detection: Accurate but slow and complex to implement correctly\n>   - Hybrid approach with suspicion states: Balances speed and accuracy with moderate complexity\n> - **Decision**: Multi-stage failure detection with direct probing, suspicion states, and indirect confirmation\n> - **Rationale**: Suspicion states allow rapid response to potential failures while indirect confirmation reduces false positives. This approach provides the responsiveness needed for cache workloads while maintaining cluster stability.\n> - **Consequences**: More complex state management but significantly better balance between detection speed and accuracy\n\n> **Decision: Gossip Protocol Design**\n> - **Context**: Propagate cluster membership changes efficiently while handling network partitions and maintaining eventual consistency\n> - **Options Considered**:\n>   - Flooding-based propagation: Simple but generates excessive network traffic\n>   - Tree-based propagation: Efficient but fragile to node failures\n>   - Epidemic gossip with random peer selection: Robust and scalable but slower convergence\n> - **Decision**: Epidemic gossip with intelligent peer selection and adaptive timing\n> - **Rationale**: Epidemic protocols provide excellent partition tolerance and scale logarithmically with cluster size. Intelligent peer selection improves convergence time while adaptive timing reduces bandwidth usage during stable periods.\n> - **Consequences**: Moderate implementation complexity but excellent scalability and partition resilience\n\nThe **request routing architecture** decision involved choosing between centralized routing through designated proxy nodes versus distributed routing where any node can handle any request. The distributed approach was selected because it eliminates single points of failure and provides better load distribution, despite the increased complexity of maintaining consistent routing tables.\n\n**Consistency model selection** for cluster membership required balancing between strong consistency (which would require consensus protocols) and eventual consistency (which accepts temporary inconsistencies). Eventual consistency was chosen because cache workloads can tolerate brief periods of inconsistent routing, and the performance benefits of avoiding consensus protocols are substantial.\n\nThe **health check timing parameters** required careful tuning to balance failure detection speed against network overhead and false positive rates. The selected parameters (1-second probe intervals, 5-second suspicion timeouts, 30-second failure confirmations) provide good responsiveness for typical cache workloads while maintaining stability under moderate network congestion.\n\n**Message serialization format** decisions impact both performance and operational debugging capabilities. JSON was chosen over binary formats like Protocol Buffers because the human-readable format significantly simplifies troubleshooting cluster communication issues, and the performance overhead is acceptable for cache workloads where network latency typically dominates over serialization costs.\n\nThe **circuit breaker configuration** for request forwarding protects against cascading failures when target nodes become overloaded. The parameters were chosen to detect failures within 10-15 seconds while allowing sufficient time for temporary network hiccups to resolve without triggering unnecessary circuit opening.\n\n### Common Pitfalls\n\nDistributed communication systems present numerous subtle failure modes that can cause mysterious outages, data inconsistencies, and performance degradations. These pitfalls often manifest only under specific timing conditions or failure scenarios, making them particularly challenging to diagnose and reproduce.\n\n⚠️ **Pitfall: Ignoring Message Ordering in Gossip Protocol**\n\nMany implementations assume that network messages arrive in the order they were sent, but this assumption breaks down in distributed systems. Consider a scenario where node A sends two gossip messages: first marking node C as \"suspected\", then marking it as \"failed\". If these messages arrive at node B in reverse order due to network routing variations, node B will first learn that C is failed, then receive the older \"suspected\" message and incorrectly downgrade C's status.\n\nThis ordering problem causes **membership state oscillation** where cluster members continuously flip between different views of node health, preventing convergence and causing routing instability. The symptoms include inconsistent routing decisions, requests being sent to failed nodes, and excessive cluster membership churn in logs.\n\n**Solution**: Implement vector clocks or logical timestamps in gossip messages. Each membership update includes version information that allows recipients to determine the correct ordering regardless of arrival sequence. Messages with older timestamps are discarded if newer information about the same node has already been processed.\n\n⚠️ **Pitfall: Inadequate Timeout Configuration**\n\nChoosing timeout values for health checks and failure detection requires balancing conflicting requirements: short timeouts enable rapid failure detection but increase false positive rates during network congestion, while long timeouts reduce false positives but slow down failure detection and recovery.\n\nA common mistake is using fixed timeout values that work well in test environments but fail in production networks with variable latency. For example, setting health check timeouts to 1 second might work perfectly on a local network but cause constant false failures when deployed across regions with 200-300ms baseline latency.\n\n**Solution**: Implement adaptive timeout algorithms that adjust based on observed network conditions. Track the distribution of response times for successful health checks and set timeouts to 3-4 standard deviations above the mean. This approach automatically adapts to different network environments while maintaining appropriate failure detection sensitivity.\n\n⚠️ **Pitfall: Split-Brain Membership Reconciliation Errors**\n\nWhen network partitions heal, previously isolated sub-clusters must merge their divergent membership views. A critical error occurs when the reconciliation process doesn't properly handle nodes that failed and recovered during the partition. Consider this scenario:\n\n1. Network partition separates nodes A,B from nodes C,D\n2. Node A fails and restarts (getting a new node ID) while partitioned\n3. Partition heals and nodes attempt to reconcile membership\n4. The reconciliation process sees the old node ID for A marked as failed and the new node ID as active, but doesn't realize they represent the same physical machine\n\nThis leads to **phantom node entries** in the membership table and incorrect hash ring calculations that route some keys to non-existent nodes.\n\n**Solution**: Implement proper node identity management that distinguishes between logical node IDs and physical node instances. Include additional identifying information like MAC addresses or installation UUIDs that persist across restarts. During reconciliation, detect and merge entries that represent the same physical node.\n\n⚠️ **Pitfall: Request Forwarding Loops**\n\nIn symmetric proxy architectures where any node can forward requests to any other node, subtle inconsistencies in hash ring views can create forwarding loops. For example:\n\n1. Client sends GET request for key \"user:123\" to node A\n2. Node A's hash ring shows key belongs to node B, forwards request\n3. Node B's hash ring shows key belongs to node C (due to recent membership change), forwards request  \n4. Node C's hash ring shows key belongs to node A, forwards back to A\n5. Request loops indefinitely until timeout\n\n**Solution**: Include hop count limits and forward path tracking in request headers. Each forwarding operation decrements the hop count and adds the current node to the path. If a node sees its own ID in the forward path or the hop count reaches zero, it terminates the loop and returns an error. Additionally, propagate hash ring version numbers in forwarding requests to detect and resolve membership inconsistencies.\n\n⚠️ **Pitfall: Gossip Message Amplification**\n\nNaive gossip implementations can create **message storms** during cluster instability. When multiple nodes detect the same failure simultaneously, they may all send urgent gossip messages to their peers. If recipients immediately forward these urgent messages to their peers, the message volume can grow exponentially and overwhelm the network.\n\nThis amplification is particularly problematic during cascading failures where multiple nodes fail in rapid succession, causing each failure to trigger a separate message storm.\n\n**Solution**: Implement **gossip rate limiting** and **message deduplication**. Each node maintains a recent message cache and discards duplicate gossip messages based on content hash and sender ID. Additionally, implement exponential backoff for urgent gossip messages to prevent amplification during failure scenarios.\n\n⚠️ **Pitfall: Health Check False Positives During Load**\n\nHealth check implementations often fail to account for node load conditions. A common pattern is for health check messages to be processed by the same thread pool or event loop that handles client requests. During high load periods, client request processing can consume all available resources, causing health check messages to be delayed or dropped even though the node is functioning correctly.\n\nThis creates a **load-induced failure detection** problem where busy nodes are incorrectly marked as failed, causing traffic to be redirected to other nodes and potentially creating cascading overload conditions.\n\n**Solution**: Implement **dedicated health check processing** that operates independently of client request handling. Use separate thread pools, network ports, or processing queues for health check messages. Additionally, include load metrics in health check responses so that peers can distinguish between node failures and temporary overload conditions.\n\n⚠️ **Pitfall: Inconsistent Cluster State During Rolling Updates**\n\nDuring rolling updates where cluster nodes are upgraded sequentially, different nodes may have different versions of the gossip protocol or membership management logic. This version skew can cause persistent membership inconsistencies that don't resolve even after the update completes.\n\nFor example, if the new version changes the format of gossip messages or the logic for conflict resolution, old and new nodes may be unable to synchronize their membership views properly.\n\n**Solution**: Design gossip protocols with **backward compatibility** and **protocol versioning**. Include protocol version numbers in gossip messages and implement fallback logic that allows nodes with different versions to communicate. Plan rolling update sequences to minimize the time period where mixed versions are active.\n\n### Implementation Guidance\n\nThe cluster communication system requires careful coordination between multiple subsystems including network transports, gossip protocols, failure detection, and request routing. This implementation guidance provides complete working infrastructure and detailed skeletons for the core learning components.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Transport | net/http with JSON encoding (encoding/json) | HTTP/2 with Protocol Buffers (google.golang.org/protobuf) |\n| Gossip Serialization | JSON with gzip compression | MessagePack or Protocol Buffers for efficiency |\n| Health Check Transport | HTTP GET requests with JSON responses | Custom UDP protocol with binary messages |\n| Service Discovery | Static configuration file | Consul or etcd integration |\n| Message Queuing | Go channels with buffering | Apache Kafka or NATS for persistence |\n| Logging | Standard library log package | Structured logging with logrus or zap |\n\n#### Recommended File Structure\n\nThe cluster communication components should be organized to separate concerns and enable independent testing:\n\n```\ninternal/cluster/\n  transport/\n    http_transport.go        ← HTTPTransport implementation\n    transport.go            ← Transport interface definition  \n    transport_test.go       ← Transport unit tests\n  gossip/\n    gossip.go              ← Core gossip protocol logic\n    message.go             ← Message types and serialization\n    membership.go          ← Membership table management\n    gossip_test.go         ← Gossip protocol tests\n  discovery/\n    discovery.go           ← Node discovery and bootstrap logic\n    discovery_test.go      ← Discovery mechanism tests\n  health/\n    health_checker.go      ← Health check and failure detection\n    failure_detector.go    ← Failure detector state machine\n    health_test.go         ← Health check tests\n  router/\n    request_router.go      ← Request routing and forwarding\n    circuit_breaker.go     ← Circuit breaker for node failures\n    router_test.go         ← Request routing tests\n```\n\n#### Infrastructure Starter Code\n\n**HTTP Transport Implementation** (complete, ready to use):\n\n```go\npackage transport\n\nimport (\n    \"bytes\"\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n    \"time\"\n)\n\n// HTTPTransport provides HTTP-based communication between cluster nodes\ntype HTTPTransport struct {\n    client  *http.Client\n    timeout time.Duration\n    server  *http.Server\n}\n\n// NewHTTPTransport creates an HTTP transport with specified timeout\nfunc NewHTTPTransport(timeout time.Duration) *HTTPTransport {\n    return &HTTPTransport{\n        client: &http.Client{\n            Timeout: timeout,\n            Transport: &http.Transport{\n                MaxIdleConns:        100,\n                MaxIdleConnsPerHost: 10,\n                IdleConnTimeout:     60 * time.Second,\n            },\n        },\n        timeout: timeout,\n    }\n}\n\n// SendMessage sends a JSON message to the specified address and returns the response\nfunc (t *HTTPTransport) SendMessage(ctx context.Context, address string, message interface{}) ([]byte, error) {\n    jsonData, err := json.Marshal(message)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to marshal message: %w\", err)\n    }\n\n    req, err := http.NewRequestWithContext(ctx, \"POST\", \"http://\"+address+\"/cluster\", bytes.NewBuffer(jsonData))\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to create request: %w\", err)\n    }\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    resp, err := t.client.Do(req)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to send request: %w\", err)\n    }\n    defer resp.Body.Close()\n\n    if resp.StatusCode != http.StatusOK {\n        return nil, fmt.Errorf(\"received error response: %d\", resp.StatusCode)\n    }\n\n    body, err := io.ReadAll(resp.Body)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to read response: %w\", err)\n    }\n\n    return body, nil\n}\n\n// HealthCheck performs a health check against the specified address\nfunc (t *HTTPTransport) HealthCheck(ctx context.Context, address string) error {\n    req, err := http.NewRequestWithContext(ctx, \"GET\", \"http://\"+address+\"/health\", nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to create health check request: %w\", err)\n    }\n\n    resp, err := t.client.Do(req)\n    if err != nil {\n        return fmt.Errorf(\"health check failed: %w\", err)\n    }\n    defer resp.Body.Close()\n\n    if resp.StatusCode != http.StatusOK {\n        return fmt.Errorf(\"health check returned status %d\", resp.StatusCode)\n    }\n\n    return nil\n}\n\n// StartServer starts the HTTP server for receiving cluster messages\nfunc (t *HTTPTransport) StartServer(address string, handler http.Handler) error {\n    t.server = &http.Server{\n        Addr:         address,\n        Handler:      handler,\n        ReadTimeout:  t.timeout,\n        WriteTimeout: t.timeout,\n        IdleTimeout:  2 * t.timeout,\n    }\n\n    return t.server.ListenAndServe()\n}\n\n// Stop gracefully stops the HTTP server\nfunc (t *HTTPTransport) Stop(ctx context.Context) error {\n    if t.server != nil {\n        return t.server.Shutdown(ctx)\n    }\n    return nil\n}\n```\n\n**Message Types and Serialization** (complete, ready to use):\n\n```go\npackage gossip\n\nimport (\n    \"encoding/json\"\n    \"time\"\n)\n\n// Message represents a general cluster communication message\ntype Message struct {\n    Type      string      `json:\"type\"`\n    Sender    string      `json:\"sender\"`\n    Timestamp time.Time   `json:\"timestamp\"`\n    Data      interface{} `json:\"data\"`\n}\n\n// GossipMessage contains cluster membership information for gossip protocol\ntype GossipMessage struct {\n    NodeStates map[string]NodeState `json:\"node_states\"`\n    Version    uint64               `json:\"version\"`\n}\n\n// NodeState represents the current state of a cluster node\ntype NodeState struct {\n    NodeID   string    `json:\"node_id\"`\n    Address  string    `json:\"address\"`\n    Status   string    `json:\"status\"`\n    LastSeen time.Time `json:\"last_seen\"`\n    Version  uint64    `json:\"version\"`\n}\n\n// HealthCheckRequest is sent to verify node liveness\ntype HealthCheckRequest struct {\n    ProbeID     string    `json:\"probe_id\"`\n    Timestamp   time.Time `json:\"timestamp\"`\n    SenderLoad  float64   `json:\"sender_load\"`\n}\n\n// HealthCheckResponse confirms node is alive and healthy\ntype HealthCheckResponse struct {\n    ProbeID   string    `json:\"probe_id\"`\n    Timestamp time.Time `json:\"timestamp\"`\n    Load      float64   `json:\"load\"`\n    Status    string    `json:\"status\"`\n}\n\n// JoinRequest is sent when a node wants to join the cluster\ntype JoinRequest struct {\n    NodeID  string `json:\"node_id\"`\n    Address string `json:\"address\"`\n}\n\n// JoinResponse contains current cluster membership for new nodes\ntype JoinResponse struct {\n    Success     bool                     `json:\"success\"`\n    Membership  map[string]NodeState     `json:\"membership\"`\n    Version     uint64                   `json:\"version\"`\n}\n\n// SerializeMessage converts a message to JSON bytes\nfunc SerializeMessage(msg interface{}) ([]byte, error) {\n    return json.Marshal(msg)\n}\n\n// DeserializeMessage converts JSON bytes to a message\nfunc DeserializeMessage(data []byte, msg interface{}) error {\n    return json.Unmarshal(data, msg)\n}\n\n// CreateMessage wraps data in a Message envelope with metadata\nfunc CreateMessage(msgType, senderID string, data interface{}) Message {\n    return Message{\n        Type:      msgType,\n        Sender:    senderID,\n        Timestamp: time.Now(),\n        Data:      data,\n    }\n}\n```\n\n#### Core Logic Skeletons\n\n**Node Discovery Implementation** (signatures + detailed TODOs):\n\n```go\npackage discovery\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// NodeDiscovery handles cluster membership discovery and bootstrapping\ntype NodeDiscovery struct {\n    config        *NodeConfig\n    transport     Transport\n    membership    map[string]NodeState\n    membershipMu  sync.RWMutex\n    version       uint64\n    joinListeners []func(NodeState)\n}\n\n// NewNodeDiscovery creates a new discovery service with the specified configuration\nfunc NewNodeDiscovery(config *NodeConfig, transport Transport) *NodeDiscovery {\n    return &NodeDiscovery{\n        config:     config,\n        transport:  transport,\n        membership: make(map[string]NodeState),\n        version:    1,\n    }\n}\n\n// JoinCluster attempts to join an existing cluster using bootstrap addresses\nfunc (d *NodeDiscovery) JoinCluster(ctx context.Context) error {\n    // TODO 1: Iterate through JoinAddresses from config\n    // TODO 2: For each address, attempt to send JoinRequest with local node info\n    // TODO 3: If request succeeds, process JoinResponse to update local membership\n    // TODO 4: Update membership table with received node states\n    // TODO 5: Set local membership version to received version + 1\n    // TODO 6: If all bootstrap addresses fail, start as single-node cluster\n    // TODO 7: Notify join listeners about successful cluster join\n    // Hint: Use transport.SendMessage with JoinRequest, handle timeouts gracefully\n    return nil\n}\n\n// HandleJoinRequest processes join requests from new nodes\nfunc (d *NodeDiscovery) HandleJoinRequest(ctx context.Context, req JoinRequest) (*JoinResponse, error) {\n    // TODO 1: Validate the join request (check node ID format, address reachability)\n    // TODO 2: Acquire write lock on membership table\n    // TODO 3: Check if node ID already exists (handle duplicate joins)\n    // TODO 4: Add new node to membership table with \"active\" status\n    // TODO 5: Increment membership version number\n    // TODO 6: Create JoinResponse with current membership snapshot\n    // TODO 7: Release write lock and notify join listeners\n    // TODO 8: Trigger gossip message to propagate new node to cluster\n    // Hint: Return complete membership map, not just new node info\n    return nil, nil\n}\n\n// GetMembership returns a snapshot of current cluster membership\nfunc (d *NodeDiscovery) GetMembership() map[string]NodeState {\n    // TODO 1: Acquire read lock on membership table\n    // TODO 2: Create deep copy of membership map (don't return direct reference)\n    // TODO 3: Release read lock and return copied membership\n    // Hint: Make sure to copy NodeState structs, not just map references\n    return nil\n}\n\n// UpdateNodeState updates the state of a specific node in the membership table\nfunc (d *NodeDiscovery) UpdateNodeState(nodeID string, newState NodeState) {\n    // TODO 1: Acquire write lock on membership table\n    // TODO 2: Check if node exists in current membership\n    // TODO 3: Compare version numbers to avoid applying stale updates\n    // TODO 4: Update node state and increment membership version\n    // TODO 5: Release write lock and trigger gossip propagation\n    // Hint: Version comparison prevents race conditions during concurrent updates\n}\n\n// RemoveNode removes a failed node from the cluster membership\nfunc (d *NodeDiscovery) RemoveNode(nodeID string) {\n    // TODO 1: Acquire write lock on membership table\n    // TODO 2: Verify node exists before attempting removal\n    // TODO 3: Delete node from membership map\n    // TODO 4: Increment membership version number\n    // TODO 5: Release write lock and notify removal listeners\n    // TODO 6: Trigger gossip to propagate removal throughout cluster\n    // Hint: Consider graceful vs forceful removal based on node state\n}\n```\n\n**Gossip Protocol Implementation** (signatures + detailed TODOs):\n\n```go\npackage gossip\n\nimport (\n    \"context\"\n    \"math/rand\"\n    \"sync\"\n    \"time\"\n)\n\n// GossipProtocol manages cluster state propagation using epidemic algorithms\ntype GossipProtocol struct {\n    nodeID       string\n    transport    Transport\n    membership   *NodeDiscovery\n    config       *GossipConfig\n    running      bool\n    runningMu    sync.RWMutex\n    stopCh       chan struct{}\n}\n\n// GossipConfig contains tuning parameters for the gossip protocol\ntype GossipConfig struct {\n    GossipInterval    time.Duration\n    GossipFanout      int\n    MaxMessageSize    int\n    CompressionEnabled bool\n}\n\n// NewGossipProtocol creates a new gossip service\nfunc NewGossipProtocol(nodeID string, transport Transport, membership *NodeDiscovery, config *GossipConfig) *GossipProtocol {\n    return &GossipProtocol{\n        nodeID:     nodeID,\n        transport:  transport,\n        membership: membership,\n        config:     config,\n        stopCh:     make(chan struct{}),\n    }\n}\n\n// Start begins the gossip protocol background processes\nfunc (g *GossipProtocol) Start(ctx context.Context) error {\n    // TODO 1: Acquire write lock and check if already running\n    // TODO 2: Set running flag to true and start gossip timer\n    // TODO 3: Start goroutine for periodic gossip sending\n    // TODO 4: Start goroutine for gossip message processing\n    // TODO 5: Register message handler with transport layer\n    // Hint: Use time.Ticker for periodic gossip, handle context cancellation\n    return nil\n}\n\n// Stop gracefully shuts down the gossip protocol\nfunc (g *GossipProtocol) Stop() error {\n    // TODO 1: Acquire write lock and check if currently running\n    // TODO 2: Set running flag to false and close stop channel\n    // TODO 3: Wait for background goroutines to finish\n    // TODO 4: Unregister message handlers from transport\n    // Hint: Use sync.WaitGroup to coordinate graceful shutdown\n    return nil\n}\n\n// SendGossip initiates a gossip round with randomly selected peers\nfunc (g *GossipProtocol) SendGossip(ctx context.Context) error {\n    // TODO 1: Get current membership snapshot from discovery service\n    // TODO 2: Select random subset of nodes for gossip (fanout algorithm)\n    // TODO 3: Exclude self and recently contacted nodes from selection\n    // TODO 4: Create GossipMessage with current membership state\n    // TODO 5: Send gossip messages to selected peers in parallel\n    // TODO 6: Handle responses and update local state if newer info received\n    // TODO 7: Update last-contact timestamps for successful communications\n    // Hint: Use goroutines with WaitGroup for parallel sending\n    return nil\n}\n\n// HandleGossipMessage processes incoming gossip from other nodes\nfunc (g *GossipProtocol) HandleGossipMessage(ctx context.Context, msg GossipMessage, senderAddr string) error {\n    // TODO 1: Validate gossip message structure and sender information\n    // TODO 2: Compare membership versions to detect newer information\n    // TODO 3: For each node in gossip message, compare with local state\n    // TODO 4: Apply newer updates to local membership table\n    // TODO 5: Resolve conflicts using timestamp and node ID ordering\n    // TODO 6: Update local membership version if changes were applied\n    // TODO 7: Optionally trigger immediate gossip if critical updates received\n    // Hint: Use vector clocks or logical timestamps for conflict resolution\n    return nil\n}\n\n// SelectGossipTargets chooses random nodes for gossip communication\nfunc (g *GossipProtocol) SelectGossipTargets(membership map[string]NodeState) []string {\n    // TODO 1: Filter out self and failed nodes from potential targets\n    // TODO 2: Apply selection preferences (avoid recently contacted nodes)\n    // TODO 3: Randomly select up to GossipFanout nodes from candidates\n    // TODO 4: Ensure selection includes diverse network segments if possible\n    // TODO 5: Return list of selected node addresses for gossip\n    // Hint: Use weighted random selection to prefer less-recently contacted nodes\n    return nil\n}\n```\n\n**Failure Detection Implementation** (signatures + detailed TODOs):\n\n```go\npackage health\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// FailureDetector implements sophisticated failure detection with suspicion states\ntype FailureDetector struct {\n    nodeID        string\n    transport     Transport\n    membership    *NodeDiscovery\n    nodeStates    map[string]*NodeHealthState\n    statesMu      sync.RWMutex\n    config        *HealthConfig\n    running       bool\n    stopCh        chan struct{}\n}\n\n// NodeHealthState tracks health status and failure detection state for a node\ntype NodeHealthState struct {\n    NodeID           string\n    Address          string\n    Status           string\n    LastProbeTime    time.Time\n    LastResponseTime time.Time\n    FailureCount     int\n    SuspicionTime    time.Time\n    ProbeHistory     []ProbeResult\n}\n\n// ProbeResult records the outcome of a health check probe\ntype ProbeResult struct {\n    Timestamp time.Time\n    Success   bool\n    Latency   time.Duration\n    Error     error\n}\n\n// HealthConfig contains parameters for failure detection behavior\ntype HealthConfig struct {\n    ProbeInterval     time.Duration\n    ProbeTimeout      time.Duration\n    SuspicionTimeout  time.Duration\n    FailureThreshold  int\n    IndirectProbes    int\n}\n\n// NewFailureDetector creates a failure detector with specified configuration\nfunc NewFailureDetector(nodeID string, transport Transport, membership *NodeDiscovery, config *HealthConfig) *FailureDetector {\n    return &FailureDetector{\n        nodeID:     nodeID,\n        transport:  transport,\n        membership: membership,\n        nodeStates: make(map[string]*NodeHealthState),\n        config:     config,\n        stopCh:     make(chan struct{}),\n    }\n}\n\n// Start begins health checking and failure detection processes\nfunc (fd *FailureDetector) Start(ctx context.Context) error {\n    // TODO 1: Initialize health state for all known cluster members\n    // TODO 2: Start goroutine for periodic direct health probing\n    // TODO 3: Start goroutine for indirect probe coordination\n    // TODO 4: Start goroutine for timeout processing and state transitions\n    // TODO 5: Register for membership change notifications\n    // Hint: Use separate timers for different probe intervals\n    return nil\n}\n\n// ProbeNode sends direct health check to specified node\nfunc (fd *FailureDetector) ProbeNode(ctx context.Context, nodeID string) (*ProbeResult, error) {\n    // TODO 1: Look up node address from membership table\n    // TODO 2: Create HealthCheckRequest with probe ID and timestamp\n    // TODO 3: Send probe using transport with configured timeout\n    // TODO 4: Measure round-trip latency for successful probes\n    // TODO 5: Record probe result in node's health state history\n    // TODO 6: Update last probe time and response time timestamps\n    // TODO 7: Return ProbeResult with success status and metrics\n    // Hint: Use context.WithTimeout for probe timeout control\n    return nil, nil\n}\n\n// ProcessProbeResult updates node health state based on probe outcome\nfunc (fd *FailureDetector) ProcessProbeResult(nodeID string, result *ProbeResult) {\n    // TODO 1: Acquire write lock on node states map\n    // TODO 2: Get or create NodeHealthState for the target node\n    // TODO 3: Add probe result to node's probe history\n    // TODO 4: Update failure count based on probe success/failure\n    // TODO 5: Determine if state transition is required (active->suspected->failed)\n    // TODO 6: Apply state transition and update timestamps\n    // TODO 7: Notify membership service of state changes\n    // TODO 8: Trigger gossip propagation for critical state changes\n    // Hint: Use sliding window for failure count to avoid permanent marking\n}\n\n// PerformIndirectProbe coordinates indirect probing through other nodes\nfunc (fd *FailureDetector) PerformIndirectProbe(ctx context.Context, targetNodeID string) error {\n    // TODO 1: Select subset of healthy nodes as indirect probe witnesses\n    // TODO 2: Send IndirectProbeRequest to witnesses in parallel\n    // TODO 3: Collect responses from witnesses within timeout period\n    // TODO 4: Analyze witness responses to determine target node status\n    // TODO 5: If majority confirms failure, mark target as failed\n    // TODO 6: If some witnesses succeed, maintain suspected status\n    // TODO 7: Update confidence metrics based on witness agreement\n    // Hint: Use context with timeout for coordinating parallel probes\n    return nil\n}\n\n// UpdateNodeStatus transitions node to new health status\nfunc (fd *FailureDetector) UpdateNodeStatus(nodeID, newStatus string) {\n    // TODO 1: Acquire write lock on node states map\n    // TODO 2: Validate status transition is legal (active->suspected->failed)\n    // TODO 3: Update node status and relevant timestamps\n    // TODO 4: Reset failure counters if transitioning to active\n    // TODO 5: Update membership service with new node status\n    // TODO 6: Log status transition with detailed context\n    // TODO 7: Trigger immediate gossip for critical transitions (failed status)\n    // Hint: Implement state machine validation for legal transitions\n}\n```\n\n#### Milestone Checkpoint\n\nAfter implementing cluster communication, verify functionality with these specific tests:\n\n**Cluster Formation Test**:\n```bash\n# Start first node (bootstrap)\ngo run cmd/server/main.go -config node1.json -port 8001\n\n# Start second node (joins cluster)\ngo run cmd/server/main.go -config node2.json -port 8002 -join 127.0.0.1:8001\n\n# Verify both nodes see each other in membership\ncurl http://127.0.0.1:8001/cluster/membership\ncurl http://127.0.0.1:8002/cluster/membership\n```\n\n**Expected Output**: Both endpoints should return JSON with both nodes listed as \"active\" status.\n\n**Failure Detection Test**:\n```bash\n# Start cluster with 3 nodes\n# Kill one node abruptly (SIGKILL)\nkill -9 <node_pid>\n\n# Wait 10-15 seconds, check membership on remaining nodes\ncurl http://127.0.0.1:8001/cluster/membership\n```\n\n**Expected Output**: Failed node should show \"failed\" status within 15 seconds.\n\n**Request Routing Test**:\n```bash\n# Send cache request to node that doesn't own the key\ncurl -X PUT http://127.0.0.1:8001/cache/test-key -d \"test-value\"\n\n# Verify request was routed to correct node based on hash ring\ncurl http://127.0.0.1:8002/cache/test-key\n```\n\n**Expected Output**: Value should be retrievable from any node, indicating proper routing.\n\n**Troubleshooting Signs**:\n- **Nodes don't discover each other**: Check network connectivity, bootstrap addresses, JSON parsing\n- **False failure detection**: Reduce health check timeouts, check network latency patterns  \n- **Gossip not propagating**: Verify gossip intervals, message serialization, fanout settings\n- **Request routing loops**: Check hash ring consistency, hop count limits, forward path tracking\n\n\n## Replication and Consistency\n\n> **Milestone(s):** This section covers Milestone 4 (Replication & Consistency), building on the distributed cache foundation from all previous milestones to add fault tolerance through data replication, configurable consistency levels, and conflict resolution mechanisms.\n\n### Mental Model: The Document Copying Office\n\nThink of data replication in a distributed cache like a government document copying office that serves multiple branches across a city. When an important document arrives, the office doesn't store just one copy in one location—that would be too risky if that building burned down or became inaccessible. Instead, they make multiple copies and store them in different branch offices across the city.\n\nThe **replication factor** is like the office policy: \"Every document must have exactly 3 copies stored in 3 different branches.\" When someone submits a new document, the copying office identifies which branches should store copies based on their location and current workload, then sends copies to each designated branch.\n\n**Quorum-based operations** work like approval processes. For a \"write quorum,\" imagine the policy says \"a document is officially filed when at least 2 out of 3 branches confirm they've stored their copy.\" For a \"read quorum,\" the policy might say \"we can provide an official document when we receive the same version from at least 2 out of 3 branches.\"\n\n**Conflict resolution** handles the case where branches have different versions of the same document. Maybe branch A received an updated version on Monday, but branch B was closed and still has the old version from last week. When someone requests that document, the copying office needs a policy: do they trust the newest timestamp, count votes from multiple branches, or use some other method to determine which version is authoritative?\n\n**Anti-entropy repair** is like the weekly audit process. A supervisor visits all branches with a checklist, comparing document versions across locations. When they find inconsistencies—branch A has version 3 of a document but branch B only has version 2—they initiate a repair process to bring all copies back into sync.\n\nThis analogy captures the core tension in distributed systems: we replicate for fault tolerance, but multiple copies introduce complexity around consistency. Just like the document office needs policies for handling disagreements between branches, our distributed cache needs algorithms for handling disagreements between replicas.\n\n### Replication Factor and Placement\n\nThe **replication factor** determines how many copies of each cache entry exist in the cluster. This is a fundamental configuration parameter that directly impacts both fault tolerance and storage overhead. A replication factor of 1 means no redundancy—if the node holding a key fails, that data is lost. A replication factor of 3 provides strong fault tolerance, allowing the system to survive the failure of any 2 nodes while still serving requests for all data.\n\n![Data Replication and Placement](./diagrams/replication-strategy.svg)\n\n**Replica placement strategy** determines which specific nodes store copies of each key. The most common approach in consistent hashing systems is **successor-based placement**: for any given key, find its position on the hash ring, then place replicas on the next N-1 successor nodes in clockwise order. This provides several advantages: replica placement is deterministic and calculable by any node, replicas are naturally distributed across different physical nodes, and the placement remains stable as nodes join and leave the cluster.\n\n> **Decision: Successor-Based Replica Placement**\n> - **Context**: Need to determine which specific nodes store replicas for each key in a way that's deterministic, load-balanced, and resilient to node failures\n> - **Options Considered**: Random placement, consistent successor placement, rack-aware placement\n> - **Decision**: Use consistent successor placement on the hash ring\n> - **Rationale**: Deterministic placement means any node can calculate replica locations without coordination. Successor placement naturally distributes load and maintains locality during ring changes. Implementation complexity is much lower than rack-aware placement.\n> - **Consequences**: Provides good load distribution and fault tolerance. May place replicas on nodes that fail together in correlated failures, but rack-awareness can be added later as an enhancement.\n\nThe replica placement algorithm works as follows:\n\n1. **Hash the key** to determine its position on the consistent hash ring\n2. **Find the primary node** responsible for that key position using the standard hash ring lookup\n3. **Select successor nodes** by walking clockwise around the ring from the primary node's position\n4. **Skip duplicate physical nodes** to ensure replicas are stored on different machines (if a physical node has multiple virtual nodes)\n5. **Return the list of replica nodes** in ring order, with the primary node as the first element\n\n| Replication Factor | Fault Tolerance | Storage Overhead | Read/Write Overhead | Recommended Use Case |\n|-------------------|-----------------|------------------|---------------------|---------------------|\n| 1 | None (single point of failure) | 1x | Minimal | Development, non-critical caching |\n| 2 | Survives 1 node failure | 2x | Moderate | Testing, low-risk applications |\n| 3 | Survives 2 node failures | 3x | Higher | Production systems, standard recommendation |\n| 5 | Survives 4 node failures | 5x | Highest | Mission-critical systems, geo-distributed clusters |\n\n**Replica node selection** must handle several edge cases. When the cluster size is smaller than the replication factor, every available node stores a replica. During node failures, some keys may temporarily have fewer than the desired number of replicas until the cluster stabilizes. When nodes rejoin after a partition, their data may be stale and require synchronization with active replicas.\n\n**Virtual node considerations** add complexity to replica placement. Since each physical node occupies multiple positions on the hash ring through virtual nodes, the placement algorithm must track physical node identity to avoid placing multiple replicas on the same machine. This requires maintaining a mapping from virtual node identifiers to physical node addresses, and skipping virtual nodes that belong to physical nodes already selected as replicas.\n\n**Load balancing** across replicas becomes important under certain access patterns. If clients always read from the first replica in the list (the primary), that creates hotspots. Some systems rotate the replica list based on the client identifier or use randomized replica selection to distribute read load more evenly across all copies.\n\n### Quorum-Based Operations\n\n**Quorum systems** provide a mathematical foundation for achieving consistency in replicated systems. A quorum is the minimum number of replicas that must participate in an operation for it to be considered successful. The key insight is that if read quorums and write quorums overlap, the system guarantees that reads will see at least one up-to-date copy of the data.\n\nThe **fundamental quorum rule** states that for a system with N replicas, if R is the read quorum size and W is the write quorum size, then R + W > N ensures strong consistency. This mathematical constraint ensures that any read operation will contact at least one replica that participated in the most recent write operation.\n\n| Configuration | Read Quorum (R) | Write Quorum (W) | Consistency Level | Performance Characteristics | Failure Tolerance |\n|--------------|-----------------|------------------|-------------------|---------------------------|-------------------|\n| Strong Consistency | 2 | 2 | Strong (R+W > N) | Higher latency, lower availability | Cannot serve requests if 2+ nodes down |\n| Read-Optimized | 1 | 3 | Strong (R+W > N) | Fast reads, slow writes | Cannot write if 2+ nodes down |\n| Write-Optimized | 3 | 1 | Strong (R+W > N) | Fast writes, slow reads | Cannot read if 2+ nodes down |\n| Eventually Consistent | 1 | 1 | Eventual | Fastest operations | Highest availability, may read stale data |\n\n**Write quorum operations** follow a specific sequence to ensure consistency:\n\n1. **Calculate target replicas** for the key using the replica placement algorithm\n2. **Generate a write timestamp** (often using vector clocks or logical timestamps)\n3. **Send write requests** concurrently to all replica nodes with the timestamp and data\n4. **Wait for acknowledgments** from at least W replica nodes before responding to the client\n5. **Handle partial failures** by retrying failed writes or returning an error if insufficient replicas respond\n6. **Perform read repair** on any replicas that failed to acknowledge the write during subsequent read operations\n\n**Read quorum operations** must handle the possibility of retrieving different versions from different replicas:\n\n1. **Send read requests** concurrently to all replica nodes (or a subset if using optimized strategies)\n2. **Collect responses** from at least R replica nodes with their timestamps and data values\n3. **Resolve conflicts** if replicas return different values by selecting the version with the highest timestamp\n4. **Trigger read repair** if any replica has a stale version by asynchronously updating it with the latest value\n5. **Return the resolved value** to the client along with success confirmation\n\n> **Critical insight**: The quorum intersection property (R + W > N) is what provides linearizability in the face of node failures. Even if some replicas are temporarily unavailable, the mathematics ensure that reads will always see writes that have been acknowledged to clients.\n\n**Consistency level configuration** allows applications to choose different trade-offs for different operations. Some cache entries might require strong consistency (user authentication tokens), while others can tolerate eventual consistency for better performance (recommendation engine results, metrics counters).\n\n| Consistency Level | Read Behavior | Write Behavior | Use Cases |\n|------------------|---------------|----------------|-----------|\n| `STRONG` | Wait for read quorum | Wait for write quorum | Critical data, user sessions, financial data |\n| `EVENTUAL` | Read from any replica | Write to any replica | Analytics data, recommendations, caching web content |\n| `LOCAL` | Read from local replica only | Write to local replica only | Geographic locality, partition tolerance |\n| `ONE` | Read from fastest replica | Write to one replica | Maximum performance, can tolerate data loss |\n\n**Sloppy quorums** handle the case where some of the preferred replica nodes are unavailable. Instead of failing the operation, the system temporarily stores data on alternative nodes (called \"hinted handoff\" nodes) until the preferred replicas recover. This maintains availability during partial failures but requires additional complexity in the repair mechanisms.\n\n**Dynamic quorum adjustment** can adapt to cluster membership changes. If a cluster normally has 5 nodes but 2 fail, the system might automatically adjust from a (3,3) quorum to a (2,2) quorum to maintain availability. However, this requires careful coordination to avoid split-brain scenarios during network partitions.\n\n### Conflict Resolution Strategies\n\n**Conflict resolution** becomes necessary when multiple replicas hold different versions of the same key. This happens during network partitions, concurrent writes, or node failures followed by recovery. The distributed cache must have deterministic rules for deciding which version is authoritative when conflicts are discovered.\n\n**Timestamp-based resolution** is the simplest approach, using either physical timestamps or logical clocks to determine ordering. Each write operation includes a timestamp, and during conflicts, the version with the highest timestamp wins. However, this approach has limitations: physical clocks can skew between nodes, and concurrent operations might receive identical timestamps.\n\n> **Decision: Vector Clock Conflict Resolution**\n> - **Context**: Need deterministic conflict resolution that can handle concurrent updates across multiple nodes and provide causality tracking\n> - **Options Considered**: Physical timestamps, logical timestamps, vector clocks, last-write-wins with client timestamps\n> - **Decision**: Use vector clocks with last-write-wins fallback for concurrent updates\n> - **Rationale**: Vector clocks correctly capture causal relationships between updates. Can detect true concurrency vs. ordering. Physical timestamps are unreliable due to clock skew. Client timestamps are untrustworthy.\n> - **Consequences**: More complex implementation and storage overhead, but provides correct conflict detection and resolution with strong semantic guarantees.\n\n**Vector clocks** provide a more sophisticated approach to conflict resolution. Each replica maintains a vector timestamp that tracks the logical time from the perspective of every node in the cluster. When a write occurs, the writing node increments its entry in the vector clock. During conflict resolution, vector clocks can determine if one version causally precedes another, or if two versions are truly concurrent.\n\n| Vector Clock Scenario | Node A Clock | Node B Clock | Relationship | Resolution |\n|----------------------|--------------|--------------|--------------|------------|\n| A happens before B | [A:1, B:0] | [A:1, B:1] | A → B | B wins (supersedes A) |\n| B happens before A | [A:2, B:1] | [A:1, B:1] | B → A | A wins (supersedes B) |\n| Concurrent updates | [A:2, B:1] | [A:1, B:2] | A ∥ B | Apply conflict resolution policy |\n| Identical versions | [A:1, B:1] | [A:1, B:1] | A = B | No conflict, same data |\n\n**Concurrent update handling** requires a policy for cases where vector clocks indicate true concurrency (neither version causally precedes the other). Common strategies include:\n\n- **Last-write-wins (LWW)**: Use physical timestamps as a tiebreaker, accepting that some updates may be lost\n- **Multi-value resolution**: Store both conflicting values and let the application decide which to use\n- **Semantic resolution**: Apply domain-specific rules (e.g., for counters, sum the concurrent updates)\n- **Client-side resolution**: Return the conflict to the client application with both versions\n\n**Version vector maintenance** requires careful management as nodes join and leave the cluster. Vector clocks grow with cluster size, and tombstone entries must be kept for departed nodes until all references are cleaned up. Some systems use bounded vector clocks that compress old entries to control memory overhead.\n\nThe conflict resolution algorithm operates during read operations when multiple replicas return different values:\n\n1. **Collect all replica responses** with their associated vector clocks and values\n2. **Build a version graph** showing causal relationships between all versions\n3. **Identify concurrent branches** where no version causally dominates others\n4. **Apply resolution policy** to select a winner among concurrent versions\n5. **Trigger repair operations** to propagate the resolved version to all replicas\n6. **Return the resolved value** to the client\n\n**Repair propagation** ensures that once a conflict is resolved, all replicas converge to the same value. This typically involves asynchronously sending write operations to replicas that have stale or conflicting versions, using the resolved vector clock to prevent the repair from being seen as a new concurrent update.\n\n### Anti-Entropy and Repair\n\n**Anti-entropy processes** run continuously in the background to detect and repair inconsistencies between replicas. Unlike read repair, which happens synchronously during client operations, anti-entropy proactively scans the data store to find divergent replicas and bring them back into sync.\n\n**Merkle tree comparison** provides an efficient way to detect inconsistencies without transferring entire datasets. Each replica builds a Merkle tree over its key space, with leaves representing individual keys and internal nodes representing hashes of their children. By comparing Merkle tree roots and recursively drilling down into differing branches, nodes can identify exactly which keys have diverged.\n\nThe anti-entropy process follows this general algorithm:\n\n1. **Select a peer node** for comparison (often using a round-robin or random selection policy)\n2. **Exchange Merkle tree roots** for corresponding key ranges\n3. **Recursively compare tree branches** to identify divergent key ranges\n4. **Exchange detailed metadata** (vector clocks, timestamps) for keys in divergent ranges\n5. **Apply conflict resolution** to determine authoritative values for conflicting keys\n6. **Perform synchronization** by transferring missing or updated values between nodes\n7. **Update local metadata** to reflect the synchronized state\n\n**Repair scheduling** balances thoroughness against system load. Continuous anti-entropy could consume significant network and CPU resources, while infrequent repairs allow inconsistencies to persist longer. Most systems use adaptive scheduling that increases repair frequency after detecting problems and reduces it during stable periods.\n\n| Repair Strategy | Frequency | Resource Usage | Consistency Guarantee | Implementation Complexity |\n|----------------|-----------|----------------|-----------------------|-------------------------|\n| Continuous Scanning | Seconds | Very High | Near-realtime repair | Low |\n| Periodic Full Scan | Hours | High (during scan) | Eventually consistent | Medium |\n| Adaptive Frequency | Variable | Moderate | Responsive to problems | High |\n| Triggered Repair | On-demand | Low baseline | Reactive only | Low |\n\n**Hinted handoff recovery** handles the special case where writes were temporarily stored on non-preferred replicas during failures. When preferred replica nodes recover, they need to retrieve any data that was written on their behalf to alternative nodes during their downtime.\n\nThe hinted handoff process works as follows:\n\n1. **Detect node recovery** through gossip protocol or direct health checks\n2. **Query hint storage** on nodes that might have stored data on behalf of the recovered node\n3. **Transfer hinted data** from temporary storage locations to the recovered node\n4. **Apply conflict resolution** if the recovered node already has versions of transferred keys\n5. **Clean up hint storage** after successful transfer to avoid accumulating stale hints\n6. **Resume normal operations** with the recovered node participating in its assigned key ranges\n\n**Consistency monitoring** tracks the health of the replication system by measuring metrics like replica divergence rates, repair operation frequency, and time-to-consistency for updates. High divergence rates might indicate network problems, overloaded nodes, or bugs in the synchronization logic.\n\n**Partition recovery** requires special handling when network splits resolve. Nodes that were isolated during a partition may have accepted writes that conflict with writes accepted by the majority partition. The recovery process must identify these conflicts, apply resolution policies, and ensure that all nodes converge on the same final state.\n\n### Architecture Decisions\n\nThe replication and consistency subsystem requires several key architectural decisions that significantly impact system behavior, performance, and operational complexity.\n\n> **Decision: Async Replication with Synchronous Quorum Acknowledgment**\n> - **Context**: Need to balance consistency guarantees with write performance and avoid blocking clients on the slowest replica\n> - **Options Considered**: Fully synchronous replication, fully asynchronous replication, hybrid quorum-based approach\n> - **Decision**: Send writes to all replicas concurrently but only wait for quorum acknowledgments before responding to client\n> - **Rationale**: Synchronous replication to all replicas creates latency spikes due to tail latency. Fully async provides no consistency guarantees. Quorum approach provides tunable consistency with predictable latency.\n> - **Consequences**: Enables strong consistency with better performance than full synchronous replication. Requires read repair to handle replicas that lag behind the quorum.\n\n> **Decision: Per-Operation Consistency Level Selection**\n> - **Context**: Different cache entries have different consistency requirements, and applications need flexibility to trade consistency for performance\n> - **Options Considered**: Cluster-wide consistency level, per-key consistency levels, per-operation consistency levels\n> - **Decision**: Allow clients to specify consistency level on each individual operation\n> - **Rationale**: Maximum flexibility allows applications to use strong consistency for critical data and eventual consistency for performance-sensitive operations. Per-key levels require additional metadata storage and complexity.\n> - **Consequences**: Enables optimal performance tuning by applications. Requires more complex client implementation and careful consideration of consistency semantics by application developers.\n\n> **Decision: Vector Clock Metadata Storage**\n> - **Context**: Vector clocks provide superior conflict resolution but require additional storage overhead and complexity\n> - **Options Considered**: Physical timestamps only, logical timestamps, vector clocks, hybrid timestamp approach\n> - **Decision**: Store vector clocks with each cache entry, with pruning for departed nodes\n> - **Rationale**: Vector clocks correctly handle causality and concurrent updates. Physical timestamps are unreliable due to clock skew. The storage overhead is acceptable for the semantic correctness gained.\n> - **Consequences**: Increases per-entry storage overhead by ~50-100 bytes depending on cluster size. Provides correct conflict resolution semantics and enables reliable replica synchronization.\n\n**Replication metadata storage** extends the basic `CacheEntry` structure to include version information and replication status:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `VectorClock` | `map[string]uint64` | Logical timestamp from each node that has written this key |\n| `ReplicationStatus` | `map[string]ReplicationState` | Status of this entry on each replica node |\n| `LastModified` | `time.Time` | Physical timestamp of the most recent write operation |\n| `ConflictResolutionPolicy` | `string` | Strategy to use for resolving concurrent updates |\n\n**Quorum configuration** can be specified at multiple levels to provide flexibility:\n\n| Configuration Level | Priority | Example | Use Case |\n|--------------------|----------|---------|----------|\n| Per-operation | Highest | `GetRequest{ConsistencyLevel: \"STRONG\"}` | Critical individual operations |\n| Per-key pattern | Medium | `users:*` requires strong consistency | Different data types have different requirements |\n| Client default | Low | Client configured for eventual consistency | Application-level consistency preferences |\n| Cluster default | Lowest | Cluster-wide fallback settings | System-wide baseline behavior |\n\n**Failure handling policies** determine how the system behaves when quorum requirements cannot be met:\n\n- **Strict quorum**: Fail operations immediately if insufficient replicas are available\n- **Sloppy quorum**: Accept writes to alternative nodes with hinted handoff\n- **Best-effort**: Succeed operations with whatever replicas are available, relying on repair to achieve consistency later\n- **Client choice**: Allow per-operation specification of failure handling behavior\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Ignoring Vector Clock Pruning**\nVector clocks grow with cluster size and retain entries for all nodes that have ever written a key. Without proper pruning, vector clocks can consume significant memory and create performance problems. This is particularly dangerous in dynamic clusters where nodes frequently join and leave. The fix is to implement a pruning policy that removes entries for nodes that have been gone for a configurable time period, but only after ensuring all references have been cleaned up through anti-entropy processes.\n\n⚠️ **Pitfall: Assuming Physical Clock Synchronization**\nMany developers assume that server clocks are closely synchronized and use physical timestamps for conflict resolution. However, even small clock skew (milliseconds) can cause incorrect ordering decisions that lead to lost updates or inconsistent data. NTP synchronization is not sufficient for distributed system ordering. The solution is to use logical timestamps (Lamport clocks) or vector clocks that capture causal relationships without depending on physical time synchronization.\n\n⚠️ **Pitfall: Blocking on the Slowest Replica**\nWhen implementing quorum systems, it's tempting to send requests to all replicas and wait for them to complete. However, this creates tail latency problems where one slow replica delays the entire operation. The correct approach is to send requests to more replicas than the quorum size (typically all replicas) but return success as soon as the quorum threshold is met. This provides both fault tolerance and performance optimization.\n\n⚠️ **Pitfall: Inconsistent Replica Placement During Ring Changes**\nWhen nodes join or leave the cluster, the replica placement algorithm must ensure that all nodes agree on which replicas store each key. If different nodes calculate different replica sets during transitional periods, writes might go to different sets of nodes, creating permanent inconsistencies. The solution is to use deterministic replica placement based on the stable hash ring state and coordinate ring updates through the gossip protocol.\n\n⚠️ **Pitfall: Read Repair Race Conditions**\nRead repair attempts to fix inconsistencies by updating stale replicas during read operations. However, concurrent read repairs or writes can create race conditions where multiple nodes simultaneously try to \"fix\" the same replica. This can result in thrashing or lost updates. The fix is to use the same vector clock mechanisms for read repair operations as for regular writes, ensuring that repairs are properly ordered and don't overwrite newer updates.\n\n⚠️ **Pitfall: Unbounded Hinted Handoff Storage**\nHinted handoff stores writes for temporarily unavailable nodes, but if nodes stay down for extended periods, the hints can consume unbounded storage space. Additionally, stale hints can conflict with data that was written after node recovery. The solution is to implement hint expiration policies (typically 3-6 hours) and perform conflict resolution when delivering hints to recovered nodes.\n\n⚠️ **Pitfall: Split-Brain During Network Partitions**\nNetwork partitions can split the cluster into multiple components, each of which might continue accepting writes with reduced quorum sizes. When the partition heals, these divergent writes create conflicts that may not be resolvable without data loss. The solution is to require strict majority quorums (W > N/2) for writes and potentially implement additional partition detection mechanisms.\n\n### Implementation Guidance\n\nThe replication and consistency layer builds on top of the cache node and cluster communication components to provide fault tolerance and configurable consistency semantics. This guidance focuses on the core replication algorithms and conflict resolution mechanisms.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Vector Clocks | `map[string]uint64` with JSON serialization | Custom binary encoding with compression |\n| Conflict Resolution | Last-write-wins with physical timestamps | Full vector clock causality detection |\n| Anti-Entropy | Periodic full scan with direct comparison | Merkle trees with incremental sync |\n| Metadata Storage | In-memory maps with periodic snapshots | Embedded database (BadgerDB/BoltDB) |\n\n**File Structure Extensions:**\n```\ninternal/replication/\n  replication.go           ← main replication manager\n  vector_clock.go          ← vector clock implementation\n  conflict_resolver.go     ← conflict resolution strategies\n  anti_entropy.go          ← background repair processes\n  quorum.go               ← quorum-based operation logic\n  hinted_handoff.go       ← temporary storage for unavailable nodes\n  replication_test.go     ← comprehensive test suite\n```\n\n**Core Data Structures:**\n```go\n// ReplicationManager coordinates replica placement and consistency operations\ntype ReplicationManager struct {\n    nodeID           string\n    hashRing         *HashRing\n    transport        *HTTPTransport\n    localStorage     *LRUCache\n    vectorClocks     map[string]*VectorClock\n    hintedHandoff    map[string][]*HintEntry\n    config           *ReplicationConfig\n    repairScheduler  *AntiEntropyScheduler\n    mutex            sync.RWMutex\n}\n\n// VectorClock tracks logical time from each node's perspective\ntype VectorClock struct {\n    Clock map[string]uint64 `json:\"clock\"`\n    mutex sync.RWMutex\n}\n\n// ReplicatedEntry extends cache entries with replication metadata\ntype ReplicatedEntry struct {\n    *CacheEntry\n    VectorClock     *VectorClock           `json:\"vector_clock\"`\n    ReplicationInfo map[string]ReplicaInfo `json:\"replication_info\"`\n}\n\n// ReplicationConfig defines system-wide replication parameters\ntype ReplicationConfig struct {\n    ReplicationFactor   int           `json:\"replication_factor\"`\n    DefaultReadQuorum   int           `json:\"default_read_quorum\"`\n    DefaultWriteQuorum  int           `json:\"default_write_quorum\"`\n    ConflictResolution  string        `json:\"conflict_resolution\"`\n    AntiEntropyInterval time.Duration `json:\"anti_entropy_interval\"`\n    HintedHandoffTTL    time.Duration `json:\"hinted_handoff_ttl\"`\n}\n```\n\n**Replication Manager Implementation:**\n```go\n// GetReplicas performs a quorum read operation with conflict resolution\nfunc (rm *ReplicationManager) GetReplicas(ctx context.Context, req *GetRequest) (*GetResponse, error) {\n    // TODO 1: Calculate target replica nodes using hash ring and replication factor\n    // TODO 2: Determine read quorum size from request or default configuration\n    // TODO 3: Send concurrent read requests to all replica nodes\n    // TODO 4: Wait for responses from at least read_quorum nodes (with timeout)\n    // TODO 5: If multiple versions returned, apply conflict resolution using vector clocks\n    // TODO 6: Trigger read repair for any stale replicas (async)\n    // TODO 7: Return resolved value to client\n    // Hint: Use errgroup for concurrent requests with context cancellation\n}\n\n// SetReplicas performs a quorum write operation with vector clock update\nfunc (rm *ReplicationManager) SetReplicas(ctx context.Context, req *SetRequest) (*SetResponse, error) {\n    // TODO 1: Generate new vector clock entry for this write operation\n    // TODO 2: Calculate target replica nodes using consistent hashing\n    // TODO 3: Determine write quorum size from request or configuration\n    // TODO 4: Send concurrent write requests to all replicas with vector clock\n    // TODO 5: Wait for acknowledgments from at least write_quorum nodes\n    // TODO 6: Handle partial failures with hinted handoff for unavailable nodes\n    // TODO 7: Return success only after quorum acknowledgments received\n    // Hint: Store hints for failed replicas to enable later delivery\n}\n```\n\n**Vector Clock Implementation:**\n```go\n// NewVectorClock creates a new vector clock initialized for the local node\nfunc NewVectorClock(nodeID string) *VectorClock {\n    return &VectorClock{\n        Clock: map[string]uint64{nodeID: 0},\n    }\n}\n\n// Update increments the clock for the specified node\nfunc (vc *VectorClock) Update(nodeID string) {\n    // TODO 1: Acquire write lock for thread safety\n    // TODO 2: Increment clock entry for specified nodeID\n    // TODO 3: Initialize entry to 1 if nodeID not present in clock\n    // Hint: Use vc.Clock[nodeID]++ but handle zero-value case\n}\n\n// Compare determines the relationship between two vector clocks\nfunc (vc *VectorClock) Compare(other *VectorClock) VectorClockRelation {\n    // TODO 1: Compare all entries in both vector clocks\n    // TODO 2: Return BEFORE if all entries in vc <= other and at least one is strictly less\n    // TODO 3: Return AFTER if all entries in vc >= other and at least one is strictly greater  \n    // TODO 4: Return EQUAL if all entries are identical\n    // TODO 5: Return CONCURRENT if neither dominates (mixed greater/less relationships)\n    // Hint: Need to handle missing entries as 0 values\n}\n\n// Merge combines two vector clocks by taking maximum of each entry\nfunc (vc *VectorClock) Merge(other *VectorClock) {\n    // TODO 1: Acquire write lock for thread safety\n    // TODO 2: Iterate through all entries in both clocks\n    // TODO 3: Set each entry to max(vc.Clock[node], other.Clock[node])\n    // TODO 4: Include entries that exist in either clock\n    // Hint: Use math.Max() but convert uint64 appropriately\n}\n```\n\n**Conflict Resolution Implementation:**\n```go\n// ResolveConflicts determines the authoritative value among conflicting replicas\nfunc (rm *ReplicationManager) ResolveConflicts(entries []*ReplicatedEntry) (*ReplicatedEntry, []*ReplicatedEntry) {\n    // TODO 1: Build causality graph using vector clock comparisons\n    // TODO 2: Identify entries that are not dominated by any other entry\n    // TODO 3: If single winner, return it along with stale entries for repair\n    // TODO 4: If multiple concurrent entries, apply configured resolution policy\n    // TODO 5: For last-write-wins, use physical timestamps as tiebreaker\n    // TODO 6: Create merged entry with updated vector clock\n    // TODO 7: Return winner and list of entries that need repair\n    // Hint: concurrent entries have vector clocks where neither dominates the other\n}\n\n// TriggerReadRepair asynchronously updates stale replicas with resolved value\nfunc (rm *ReplicationManager) TriggerReadRepair(ctx context.Context, winner *ReplicatedEntry, staleReplicas []string) {\n    // TODO 1: Create repair request with resolved value and vector clock\n    // TODO 2: Send async write requests to all nodes with stale replicas\n    // TODO 3: Use winner's vector clock to prevent repair from appearing as concurrent update\n    // TODO 4: Handle repair failures gracefully (log but don't block client)\n    // TODO 5: Update local metadata to track repair operations\n    // Hint: Use goroutines for async repair but limit concurrency\n}\n```\n\n**Hinted Handoff Implementation:**\n```go\n// StoreHint saves a write operation for later delivery to an unavailable node\nfunc (rm *ReplicationManager) StoreHint(targetNode string, operation *WriteOperation) error {\n    // TODO 1: Create hint entry with target node, operation, and timestamp\n    // TODO 2: Add hint to local storage with expiration time\n    // TODO 3: Ensure hint storage doesn't exceed memory limits\n    // TODO 4: Persist hints to survive local node restarts (optional)\n    // Hint: Use TTL to automatically expire old hints\n}\n\n// DeliverHints transfers stored hints to a recovered node\nfunc (rm *ReplicationManager) DeliverHints(ctx context.Context, recoveredNode string) error {\n    // TODO 1: Retrieve all hints stored for the recovered node\n    // TODO 2: Group hints by key to handle multiple updates efficiently  \n    // TODO 3: Apply conflict resolution if multiple hints exist for same key\n    // TODO 4: Send resolved hints to recovered node with vector clocks\n    // TODO 5: Delete successfully delivered hints from local storage\n    // TODO 6: Handle delivery failures with exponential backoff retry\n    // Hint: Batch multiple hints in single request for efficiency\n}\n```\n\n**Milestone Checkpoints:**\n\nAfter implementing vector clocks and basic replication:\n- Run `go test ./internal/replication -v` to verify vector clock operations\n- Test with 3-node cluster: set replication factor to 3, write a key, verify it appears on all nodes\n- Simulate node failure: stop one node, write should still succeed with quorum\n- Verify read repair: restart failed node, read operations should update its stale data\n\nAfter implementing quorum operations:\n- Test different consistency levels: write with STRONG, read with EVENTUAL\n- Verify quorum math: with RF=3, try R=2/W=2 (strong) vs R=1/W=1 (eventual)  \n- Test partial failures: network partition 2 nodes from 1, verify availability behavior\n- Measure performance impact: compare latencies with and without quorum requirements\n\nAfter implementing conflict resolution:\n- Create artificial conflicts: partition cluster, write different values, heal partition\n- Verify vector clock comparison correctly identifies concurrent vs ordered updates\n- Test conflict resolution policies: last-write-wins vs application merge\n- Verify read repair: conflicted keys should converge after resolution\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** This section synthesizes concepts from all milestones to show how the complete distributed cache system operates end-to-end, with particular relevance to Milestone 3 (Cluster Communication) and Milestone 4 (Replication & Consistency).\n\nUnderstanding how components interact is crucial for building a cohesive distributed system. This section explores the communication patterns, message formats, and operational flows that bind together the hash ring, cache nodes, gossip protocol, and replication mechanisms into a functioning distributed cache.\n\nThink of the distributed cache as a **well-orchestrated symphony** where each musician (component) must know their part, when to play, and how to harmonize with others. The conductor's score (message protocols) ensures everyone plays in sync, while the sheet music (data formats) provides the common language. Just as a symphony creates beautiful music through coordinated individual performances, our distributed cache delivers high-performance caching through coordinated component interactions.\n\n### Message Formats and Protocols\n\nThe distributed cache communicates through a structured message protocol that enables reliable information exchange between nodes. Every interaction follows defined message formats that include routing information, operation parameters, and metadata required for consistency and replication.\n\n#### Transport Layer Design\n\nThe system uses HTTP as the primary transport protocol, providing simplicity, debugging visibility, and universal tooling support. Each node exposes REST-like endpoints for different operation types, while maintaining the flexibility to upgrade to more efficient protocols like gRPC in the future.\n\n> **Decision: HTTP-Based Transport**\n> - **Context**: Need reliable inter-node communication protocol that's debuggable and widely supported\n> - **Options Considered**: Raw TCP sockets, HTTP/REST, gRPC with Protocol Buffers\n> - **Decision**: HTTP/REST with JSON payloads for initial implementation\n> - **Rationale**: HTTP provides excellent debugging capabilities, universal tooling, built-in error codes, and straightforward request/response semantics. JSON offers human-readable messages during development and testing\n> - **Consequences**: Slightly higher overhead than binary protocols, but gains in development velocity and operational simplicity outweigh performance costs for most cache workloads\n\nThe `HTTPTransport` component handles all network communication, providing a consistent interface for message exchange regardless of the underlying operation type. It manages connection pooling, timeout handling, and retry logic transparently to higher-level components.\n\n#### Core Message Structure\n\nAll inter-node messages follow a consistent envelope format that includes routing metadata, operation identification, and payload data. This uniform structure simplifies message handling and enables features like request tracing and operation logging across the cluster.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Type | string | Message type identifier (get, set, delete, gossip, replicate) |\n| Sender | string | NodeID of the originating node for routing responses |\n| Timestamp | time.Time | Message creation time for ordering and timeout detection |\n| Data | interface{} | Operation-specific payload containing request/response details |\n\nThe `Message` structure serves as the universal container for all inter-node communication. The Type field enables efficient routing to appropriate handlers, while Sender information supports response correlation and loop detection in gossip protocols.\n\n#### Cache Operation Messages\n\nCache operations use strongly-typed request and response structures that include all necessary parameters for distributed execution. Each operation type has dedicated message formats optimized for its specific requirements while maintaining consistency in error handling and metadata.\n\n**Get Operation Messages:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Key | string | Cache key to retrieve from distributed storage |\n| ConsistencyLevel | string | Required consistency level (eventual, strong, quorum) |\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Key | string | Echo of requested key for response correlation |\n| Value | []byte | Retrieved cache value or empty if key not found |\n| Found | bool | Indicates whether key exists in cache storage |\n| Timestamp | time.Time | Value timestamp for conflict resolution and consistency |\n\nThe `GetRequest` and `GetResponse` messages handle distributed cache lookups. The ConsistencyLevel field allows clients to specify their consistency requirements, enabling the system to choose appropriate quorum sizes and conflict resolution strategies.\n\n**Set Operation Messages:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Key | string | Cache key for storing the provided value |\n| Value | []byte | Raw bytes to store in distributed cache |\n| TTL | time.Duration | Time-to-live for automatic expiration, zero means no expiration |\n| ConsistencyLevel | string | Required write consistency level for replication |\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Success | bool | Indicates whether write operation completed successfully |\n| Timestamp | time.Time | Timestamp of successful write for ordering and conflicts |\n\nThe `SetRequest` and `SetResponse` messages handle distributed cache writes. The TTL field enables per-key expiration policies, while the ConsistencyLevel determines how many replicas must acknowledge the write before considering it successful.\n\n**Delete Operation Messages:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Key | string | Cache key to remove from distributed storage |\n| ConsistencyLevel | string | Required consistency level for delete propagation |\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Success | bool | Indicates whether delete operation completed successfully |\n| Found | bool | Whether key existed before deletion attempt |\n| Timestamp | time.Time | Deletion timestamp for tombstone tracking and conflicts |\n\nThe `DeleteRequest` and `DeleteResponse` messages handle distributed cache deletions. The Found field helps distinguish between deleting non-existent keys and actual removal operations, which is important for conflict resolution and cache statistics.\n\n#### Cluster Membership Messages\n\nThe gossip protocol uses specialized message formats for propagating cluster state information efficiently across the network. These messages include incremental updates, full state synchronization, and membership change notifications.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| NodeStates | map[string]NodeState | Current state of all known cluster nodes |\n| Version | uint64 | Monotonic version number for detecting stale information |\n\nThe `GossipMessage` carries cluster membership information between nodes. The Version field enables efficient anti-entropy by allowing nodes to quickly determine whether received gossip contains newer information than their current state.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| NodeID | string | Unique identifier for the cluster node |\n| Address | string | Network address for connecting to the node |\n| Status | string | Current node status (active, suspected, failed, leaving) |\n| LastSeen | time.Time | Timestamp of last successful communication with node |\n| Version | uint64 | Version number for this node's state information |\n\nThe `NodeState` structure represents a single node's status within the cluster. The Status field drives failure detection and recovery processes, while LastSeen enables timeout-based failure detection when gossip messages stop arriving.\n\n#### Replication Protocol Messages\n\nReplication operations require additional metadata for conflict resolution and consistency guarantees. These messages include vector clocks, replica coordination information, and conflict resolution data.\n\nInternal replication messages extend the basic cache operations with consistency metadata:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Operation | string | Type of replicated operation (get, set, delete) |\n| Key | string | Target key for the replication operation |\n| Value | []byte | Data payload for set operations, empty for get/delete |\n| VectorClock | *VectorClock | Logical timestamp for causality tracking |\n| QuorumSize | int | Required number of successful replicas for operation |\n| CoordinatorID | string | Node coordinating this replication operation |\n\nThese replication messages ensure that write operations maintain consistency across replicas while providing the metadata necessary for conflict detection and resolution.\n\n### Operation Sequence Flows\n\nUnderstanding the step-by-step flow of cache operations reveals how the distributed system coordinates between components to deliver consistent, high-performance caching. Each operation type follows a specific sequence that balances performance, consistency, and fault tolerance requirements.\n\n#### Client Request Processing\n\nEvery cache operation begins with a client request that must be routed to the appropriate cluster nodes. The routing process uses consistent hashing to determine target nodes while considering replication requirements and current cluster membership.\n\n**Initial Request Routing Flow:**\n\n1. **Client Connection**: Client connects to any cluster node using HTTP and sends a cache operation request (GET, SET, or DELETE)\n2. **Request Parsing**: Receiving node parses the HTTP request and extracts the cache key, operation parameters, and consistency requirements\n3. **Hash Ring Consultation**: Node calls `GetNode(key)` on the local hash ring to identify the primary node responsible for the requested key\n4. **Local vs Remote Decision**: If the current node is responsible for the key, it processes the request locally; otherwise, it forwards the request to the appropriate node\n5. **Request Forwarding**: For remote keys, the node uses `HTTPTransport.SendMessage()` to forward the complete request to the responsible node\n6. **Response Handling**: The forwarding node receives the response from the responsible node and relays it back to the original client\n\nThis routing flow ensures that clients can connect to any cluster node while still reaching the correct data storage location through transparent request forwarding.\n\n#### Get Operation Flow\n\nRead operations prioritize low latency while respecting consistency requirements. The flow varies based on the requested consistency level, from simple local reads to quorum-based operations with conflict resolution.\n\n**Single-Node Get Flow (Eventual Consistency):**\n\n1. **Key Ownership Verification**: Target node confirms it's responsible for the requested key using its local hash ring state\n2. **Local Cache Lookup**: Node calls `LRUCache.Get(key)` to retrieve the value from its local storage\n3. **Expiration Check**: If the entry exists, node calls `CacheEntry.IsExpired()` to verify the TTL hasn't elapsed\n4. **Response Generation**: Node creates a `GetResponse` with the value (if found and not expired) and current timestamp\n5. **Cache Statistics Update**: Node increments hit or miss counters in `CacheMetrics` for monitoring and debugging\n6. **Client Response**: Node sends the HTTP response back to the client with appropriate status codes and headers\n\nThis simple flow provides the lowest possible latency for read operations when strong consistency isn't required, as it avoids network calls to replica nodes.\n\n**Quorum-Based Get Flow (Strong Consistency):**\n\n1. **Replica Set Identification**: Coordinator node calls `HashRing.GetNodes(key, replicationFactor)` to identify all replica nodes for the key\n2. **Parallel Read Requests**: Coordinator simultaneously sends `GetRequest` messages to all replica nodes using `HTTPTransport.SendMessage()`\n3. **Response Collection**: Coordinator waits for responses from replica nodes, timing out requests that exceed the configured timeout period\n4. **Quorum Verification**: Coordinator checks if the number of successful responses meets the required read quorum size\n5. **Conflict Detection**: If multiple replicas return different values, coordinator compares their vector clocks to detect conflicts\n6. **Conflict Resolution**: For conflicting values, coordinator calls `ReplicationManager.ResolveConflicts()` to determine the authoritative value\n7. **Read Repair Trigger**: If stale replicas are detected, coordinator asynchronously calls `TriggerReadRepair()` to update them\n8. **Client Response**: Coordinator returns the resolved value to the client, ensuring the response reflects the most recent consistent state\n\nThis flow trades higher latency for stronger consistency guarantees by consulting multiple replicas and resolving any conflicts detected during the read operation.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n#### Set Operation Flow\n\nWrite operations must balance write latency against durability and consistency requirements. The flow ensures that writes are properly replicated while avoiding unnecessary delays in acknowledging successful operations to clients.\n\n**Single-Node Set Flow (Eventual Consistency):**\n\n1. **Request Validation**: Target node validates the set request parameters, checking key size limits and value constraints\n2. **Cache Entry Creation**: Node calls `NewCacheEntry(key, value, ttl)` to create a properly sized cache entry with expiration information\n3. **Memory Check**: Node verifies that storing the entry won't exceed its memory limits, potentially triggering LRU eviction\n4. **Local Storage**: Node calls `LRUCache.Set(key, value, ttl)` to store the entry in its local cache\n5. **Response Generation**: Node creates a `SetResponse` with success status and current timestamp\n6. **Asynchronous Replication**: Node asynchronously replicates the write to other nodes without blocking the client response\n7. **Client Response**: Node immediately responds to the client, acknowledging the write operation completion\n\nThis flow provides low write latency by acknowledging writes after local storage while ensuring eventual consistency through background replication.\n\n**Quorum-Based Set Flow (Strong Consistency):**\n\n1. **Vector Clock Update**: Coordinator node calls `VectorClock.Update(nodeID)` to increment its logical timestamp\n2. **Replica Set Identification**: Coordinator identifies all replica nodes using `HashRing.GetNodes(key, replicationFactor)`\n3. **Replicated Entry Creation**: Coordinator creates a `ReplicatedEntry` containing the cache entry, vector clock, and replication metadata\n4. **Parallel Write Requests**: Coordinator simultaneously sends write requests to all replica nodes with the complete replication information\n5. **Response Collection**: Coordinator waits for acknowledgments from replica nodes, tracking success and failure responses\n6. **Write Quorum Verification**: Coordinator verifies that the number of successful writes meets the required write quorum size\n7. **Failure Handling**: For failed replicas, coordinator stores hinted handoff entries using `ReplicationManager.StoreHint()`\n8. **Client Acknowledgment**: Once write quorum is achieved, coordinator responds to the client confirming successful write completion\n\nThis flow ensures that writes are durable across multiple nodes before acknowledging success to clients, providing strong consistency guarantees at the cost of higher write latency.\n\n#### Delete Operation Flow\n\nDelete operations require special handling to manage tombstones and ensure that deletions propagate correctly across replicas. The flow must distinguish between deleting non-existent keys and actual data removal.\n\n**Standard Delete Flow:**\n\n1. **Key Existence Check**: Target node first attempts to retrieve the key using `LRUCache.Get(key)` to determine if it exists\n2. **Tombstone Creation**: For existing keys, node creates a tombstone entry with a deletion timestamp instead of immediately removing the data\n3. **Local Deletion**: Node calls `LRUCache.Delete(key)` to remove the entry from its local storage\n4. **Replication Coordination**: Node identifies replica nodes and sends delete requests with tombstone information to each replica\n5. **Replica Acknowledgments**: Node waits for acknowledgments from replicas, ensuring deletion propagates across the replica set\n6. **Response Generation**: Node creates a `DeleteResponse` indicating success and whether the key existed before deletion\n7. **Cleanup Scheduling**: Node schedules the tombstone for eventual garbage collection after a configured retention period\n\nThis flow ensures that deletions are properly coordinated across replicas while maintaining the ability to distinguish between deleting existing versus non-existent keys.\n\n![Cache Operation Sequence](./diagrams/cache-operation-flow.svg)\n\n### Replication and Consistency Flows\n\nReplication flows handle the complex coordination required to maintain data consistency across multiple nodes while providing configurable consistency levels. These flows manage conflict resolution, vector clock maintenance, and repair operations that keep the distributed cache coherent.\n\n#### Write Replication Coordination\n\nWrite operations with replication require careful coordination to ensure that the specified number of replicas successfully store the data before acknowledging the write to the client. The coordination process handles failures gracefully while maintaining consistency invariants.\n\n**Primary-Backup Write Flow:**\n\n1. **Coordinator Selection**: The node receiving the client write request becomes the coordinator for this operation, regardless of whether it's a replica\n2. **Replica Set Determination**: Coordinator calls `HashRing.GetNodes(key, config.ReplicationFactor)` to identify the ordered list of replica nodes\n3. **Vector Clock Increment**: Coordinator increments its vector clock using `VectorClock.Update(coordinatorID)` to establish causality\n4. **Write Message Creation**: Coordinator creates replication messages containing the key, value, vector clock, and coordination metadata\n5. **Parallel Replica Writes**: Coordinator sends write messages to all replica nodes simultaneously using `HTTPTransport.SendMessage()`\n6. **Success Tracking**: Coordinator maintains a count of successful replica acknowledgments and any error responses received\n7. **Quorum Achievement**: Once `config.DefaultWriteQuorum` successful responses are received, the coordinator can acknowledge success to the client\n8. **Hinted Handoff**: For failed replica writes, coordinator stores hints using `ReplicationManager.StoreHint()` for later delivery\n9. **Client Acknowledgment**: Coordinator responds to the client indicating successful completion once write quorum requirements are met\n\nThis coordination flow ensures that writes achieve the required durability level while handling replica failures through hinted handoff mechanisms.\n\n#### Read Repair and Consistency\n\nRead operations with strong consistency requirements must detect and repair inconsistencies between replicas. The read repair process ensures that stale replicas are updated with the most recent values without impacting read operation latency.\n\n**Read-Time Consistency Flow:**\n\n1. **Multi-Replica Read**: Coordinator sends read requests to all replicas in the replica set for the requested key\n2. **Response Collection**: Coordinator collects responses from replicas, noting any timeouts or error responses from unavailable nodes\n3. **Vector Clock Comparison**: For each successful response, coordinator compares vector clocks using `VectorClock.Compare()` to detect conflicts\n4. **Consistency Detection**: Coordinator identifies whether all replicas returned the same value and vector clock, indicating consistency\n5. **Conflict Resolution**: If conflicts exist, coordinator calls `ReplicationManager.ResolveConflicts()` to determine the authoritative value\n6. **Stale Replica Identification**: Coordinator identifies replicas that returned stale values by comparing their vector clocks to the winning value\n7. **Client Response**: Coordinator immediately returns the resolved value to the client without waiting for repairs\n8. **Asynchronous Repair**: Coordinator triggers `TriggerReadRepair()` to asynchronously update stale replicas with the authoritative value\n\nThis flow provides strong consistency for read operations while ensuring that detected inconsistencies are repaired without impacting client response times.\n\n#### Anti-Entropy and Background Repair\n\nThe anti-entropy process runs continuously in the background to detect and repair inconsistencies that might arise from network partitions, node failures, or other edge cases. This process ensures eventual consistency even when read repair doesn't catch all inconsistencies.\n\n**Periodic Anti-Entropy Flow:**\n\n1. **Repair Schedule Activation**: The `AntiEntropyScheduler` periodically triggers repair operations based on `ReplicationConfig.AntiEntropyInterval`\n2. **Peer Selection**: Each node randomly selects peer nodes for comparison, avoiding excessive network traffic by limiting concurrent repairs\n3. **Key Range Exchange**: Selected peers exchange information about their key ranges and the vector clocks of stored entries\n4. **Difference Detection**: Each node compares received peer information with its local state to identify inconsistencies\n5. **Conflict Resolution**: For detected conflicts, nodes use `ReplicationManager.ResolveConflicts()` to determine authoritative values\n6. **Repair Message Exchange**: Nodes exchange repair messages containing the resolved values and updated vector clocks\n7. **Local State Update**: Each node updates its local cache with any newer values discovered during the anti-entropy process\n8. **Repair Metrics Update**: Nodes update repair statistics for monitoring the health of the anti-entropy process\n\nThis background process ensures that the system maintains consistency even in the presence of failures or network issues that might prevent immediate consistency.\n\n#### Hinted Handoff Delivery\n\nWhen replica nodes are unavailable during write operations, the coordinator stores hints that must be delivered when the failed nodes recover. The hinted handoff delivery process ensures that these temporary writes are properly applied.\n\n**Hint Delivery Flow:**\n\n1. **Node Recovery Detection**: The gossip protocol notifies nodes when a previously failed node recovers and rejoins the cluster\n2. **Hint Queue Consultation**: Nodes check their local hint storage for any pending writes destined for the recovered node\n3. **Hint Validation**: Each hint is validated to ensure it hasn't exceeded the `ReplicationConfig.HintedHandoffTTL` expiration time\n4. **Delivery Attempt**: Nodes call `ReplicationManager.DeliverHints()` to send accumulated writes to the recovered node\n5. **Recovery Acknowledgment**: The recovered node processes delivered hints and acknowledges successful application\n6. **Hint Cleanup**: Successfully delivered hints are removed from the local hint storage to prevent duplicate delivery\n7. **Failure Retry**: If hint delivery fails, nodes reschedule delivery attempts with exponential backoff until TTL expiration\n\nThis process ensures that writes accepted during node failures are eventually applied to all intended replicas, maintaining the system's durability guarantees.\n\n> **Key Insight**: The combination of synchronous quorum operations, asynchronous read repair, periodic anti-entropy, and hinted handoff provides multiple layers of consistency maintenance. This layered approach allows the system to provide immediate consistency for critical operations while ensuring eventual consistency through background processes.\n\n#### Consistency Level Implementation\n\nThe system supports multiple consistency levels that trade performance against consistency guarantees. Each consistency level uses different combinations of the above flows to achieve its specific guarantees.\n\n| Consistency Level | Read Behavior | Write Behavior | Guarantees |\n|------------------|---------------|----------------|------------|\n| Eventual | Single replica read | Local write + async replication | Lowest latency, eventual consistency |\n| Quorum | Read from R replicas | Write to W replicas | Configurable consistency vs. performance |\n| Strong | Read from all replicas | Write to all replicas | Immediate consistency, highest latency |\n\nThe implementation allows clients to specify consistency requirements per operation, enabling applications to choose appropriate consistency levels for different data types and use cases.\n\n### Implementation Guidance\n\nBuilding the interaction and data flow components requires careful attention to message serialization, network error handling, and operation coordination. The implementation must handle concurrent operations, network failures, and partial replica failures gracefully.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Transport | net/http with JSON encoding | gRPC with Protocol Buffers |\n| Message Serialization | encoding/json with struct tags | Protocol Buffers with code generation |\n| Request Routing | HTTP client with connection pooling | Load balancer with health checking |\n| Operation Coordination | Goroutines with channels | Actor model with message passing |\n| Error Handling | Standard error types with wrapping | Structured error codes with context |\n| Request Tracing | Simple request ID propagation | OpenTelemetry distributed tracing |\n\n#### Recommended File Structure\n\n```\ninternal/transport/\n  http_transport.go          ← HTTP-based inter-node communication\n  http_transport_test.go     ← Transport layer tests\n  message_types.go           ← Message format definitions\n  \ninternal/operations/\n  coordinator.go             ← Operation coordination logic\n  get_operation.go           ← GET operation implementation\n  set_operation.go           ← SET operation implementation  \n  delete_operation.go        ← DELETE operation implementation\n  replication_coordinator.go ← Replication coordination\n  \ninternal/consistency/\n  vector_clock.go           ← Vector clock implementation\n  conflict_resolver.go      ← Conflict resolution strategies\n  read_repair.go            ← Read repair coordination\n  anti_entropy.go           ← Background consistency maintenance\n```\n\n#### Infrastructure Starter Code\n\n**HTTP Transport Implementation:**\n\n```go\npackage transport\n\nimport (\n    \"bytes\"\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\n// HTTPTransport provides HTTP-based inter-node communication\ntype HTTPTransport struct {\n    client  *http.Client\n    timeout time.Duration\n}\n\nfunc NewHTTPTransport(timeout time.Duration) *HTTPTransport {\n    return &HTTPTransport{\n        client: &http.Client{\n            Timeout: timeout,\n            Transport: &http.Transport{\n                MaxIdleConns:        100,\n                MaxIdleConnsPerHost: 10,\n                IdleConnTimeout:     30 * time.Second,\n            },\n        },\n        timeout: timeout,\n    }\n}\n\nfunc (t *HTTPTransport) SendMessage(ctx context.Context, address string, message interface{}) ([]byte, error) {\n    data, err := json.Marshal(message)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to marshal message: %w\", err)\n    }\n    \n    req, err := http.NewRequestWithContext(ctx, \"POST\", fmt.Sprintf(\"http://%s/api/message\", address), bytes.NewBuffer(data))\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to create request: %w\", err)\n    }\n    \n    req.Header.Set(\"Content-Type\", \"application/json\")\n    \n    resp, err := t.client.Do(req)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to send request: %w\", err)\n    }\n    defer resp.Body.Close()\n    \n    if resp.StatusCode != http.StatusOK {\n        return nil, fmt.Errorf(\"request failed with status: %d\", resp.StatusCode)\n    }\n    \n    var result bytes.Buffer\n    _, err = result.ReadFrom(resp.Body)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to read response: %w\", err)\n    }\n    \n    return result.Bytes(), nil\n}\n\nfunc (t *HTTPTransport) HealthCheck(ctx context.Context, address string) error {\n    req, err := http.NewRequestWithContext(ctx, \"GET\", fmt.Sprintf(\"http://%s/health\", address), nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to create health check request: %w\", err)\n    }\n    \n    resp, err := t.client.Do(req)\n    if err != nil {\n        return fmt.Errorf(\"health check failed: %w\", err)\n    }\n    defer resp.Body.Close()\n    \n    if resp.StatusCode != http.StatusOK {\n        return fmt.Errorf(\"health check failed with status: %d\", resp.StatusCode)\n    }\n    \n    return nil\n}\n```\n\n**Message Type Definitions:**\n\n```go\npackage transport\n\nimport \"time\"\n\n// Message is the envelope for all inter-node communication\ntype Message struct {\n    Type      string      `json:\"type\"`\n    Sender    string      `json:\"sender\"`\n    Timestamp time.Time   `json:\"timestamp\"`\n    Data      interface{} `json:\"data\"`\n}\n\n// Cache operation request/response types\ntype GetRequest struct {\n    Key              string `json:\"key\"`\n    ConsistencyLevel string `json:\"consistency_level\"`\n}\n\ntype GetResponse struct {\n    Key       string    `json:\"key\"`\n    Value     []byte    `json:\"value\"`\n    Found     bool      `json:\"found\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\ntype SetRequest struct {\n    Key              string        `json:\"key\"`\n    Value            []byte        `json:\"value\"`\n    TTL              time.Duration `json:\"ttl\"`\n    ConsistencyLevel string        `json:\"consistency_level\"`\n}\n\ntype SetResponse struct {\n    Success   bool      `json:\"success\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\ntype DeleteRequest struct {\n    Key              string `json:\"key\"`\n    ConsistencyLevel string `json:\"consistency_level\"`\n}\n\ntype DeleteResponse struct {\n    Success   bool      `json:\"success\"`\n    Found     bool      `json:\"found\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// Cluster membership types\ntype GossipMessage struct {\n    NodeStates map[string]NodeState `json:\"node_states\"`\n    Version    uint64               `json:\"version\"`\n}\n\ntype NodeState struct {\n    NodeID   string    `json:\"node_id\"`\n    Address  string    `json:\"address\"`\n    Status   string    `json:\"status\"`\n    LastSeen time.Time `json:\"last_seen\"`\n    Version  uint64    `json:\"version\"`\n}\n```\n\n#### Core Logic Skeleton\n\n**Operation Coordinator Implementation:**\n\n```go\npackage operations\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\n// OperationCoordinator handles distributed cache operations\ntype OperationCoordinator struct {\n    nodeID     string\n    hashRing   *HashRing\n    transport  *HTTPTransport\n    localCache *LRUCache\n    config     *NodeConfig\n    metrics    *CacheMetrics\n    mutex      sync.RWMutex\n}\n\n// Get performs a distributed get operation with configurable consistency\nfunc (c *OperationCoordinator) Get(ctx context.Context, req *GetRequest) (*GetResponse, error) {\n    // TODO 1: Use HashRing.GetNode(req.Key) to find the responsible node\n    // TODO 2: If current node is responsible, perform local get from LRUCache\n    // TODO 3: If remote node responsible, forward request using HTTPTransport.SendMessage()\n    // TODO 4: For quorum reads, call GetNodes() to get replica set\n    // TODO 5: Send parallel requests to all replicas using goroutines\n    // TODO 6: Collect responses and check if read quorum is satisfied\n    // TODO 7: If multiple responses, compare timestamps/vector clocks for conflicts\n    // TODO 8: Trigger read repair asynchronously if stale replicas detected\n    // TODO 9: Update CacheMetrics hit/miss counters\n    // TODO 10: Return GetResponse with resolved value and metadata\n}\n\n// Set performs a distributed set operation with replication coordination\nfunc (c *OperationCoordinator) Set(ctx context.Context, req *SetRequest) (*SetResponse, error) {\n    // TODO 1: Create CacheEntry with NewCacheEntry(key, value, ttl)\n    // TODO 2: Use HashRing.GetNodes(req.Key, replicationFactor) to get replica set\n    // TODO 3: Update vector clock if using strong consistency\n    // TODO 4: Create replication messages with entry and metadata\n    // TODO 5: Send parallel write requests to all replicas\n    // TODO 6: Collect acknowledgments from replicas with timeout\n    // TODO 7: Check if write quorum satisfied (successful_writes >= WriteQuorum)\n    // TODO 8: For failed replicas, store hints using StoreHint()\n    // TODO 9: Update CacheMetrics set counters and memory usage\n    // TODO 10: Return SetResponse indicating success/failure\n}\n\n// Delete performs a distributed delete operation with tombstone management\nfunc (c *OperationCoordinator) Delete(ctx context.Context, req *DeleteRequest) (*DeleteResponse, error) {\n    // TODO 1: Use HashRing.GetNodes(req.Key, replicationFactor) to get replica set  \n    // TODO 2: Create tombstone entry with deletion timestamp\n    // TODO 3: Send delete requests to all replica nodes\n    // TODO 4: Track which replicas acknowledge the deletion\n    // TODO 5: Verify delete quorum satisfied before returning success\n    // TODO 6: Schedule tombstone cleanup after configured retention period\n    // TODO 7: Update CacheMetrics delete counters\n    // TODO 8: Return DeleteResponse with success status and found indicator\n}\n```\n\n#### Language-Specific Hints\n\n- Use `context.Context` for request timeouts and cancellation throughout the operation flows\n- Leverage `sync.WaitGroup` for coordinating parallel replica operations during quorum reads/writes  \n- Use `time.After()` for implementing operation timeouts in select statements\n- Consider using `sync.Map` for concurrent access to temporary operation state\n- Use `encoding/json` with struct tags for message serialization - it's human-readable during debugging\n- Implement exponential backoff for retry logic using `time.Sleep()` with increasing intervals\n- Use `http.Client` with connection pooling configured for efficient inter-node communication\n- Consider using buffered channels for collecting replica responses to avoid goroutine blocking\n\n#### Milestone Checkpoint\n\nAfter implementing the interaction and data flow components, verify the system with these checkpoints:\n\n**Basic Operation Flow Test:**\n```bash\n# Start three nodes on different ports\ngo run cmd/server/main.go -port=8001 -node-id=node1 &\ngo run cmd/server/main.go -port=8002 -node-id=node2 -join=localhost:8001 &  \ngo run cmd/server/main.go -port=8003 -node-id=node3 -join=localhost:8001 &\n\n# Test basic operations\ncurl -X POST localhost:8001/api/set -d '{\"key\":\"test\", \"value\":\"hello\"}'\ncurl localhost:8002/api/get?key=test\ncurl -X DELETE localhost:8003/api/delete?key=test\n```\n\n**Expected Behavior:**\n- Set operation should succeed and return `{\"success\": true}`\n- Get operation should return the value even when requested from different nodes\n- Delete operation should successfully remove the key from all replicas\n- Operations should be forwarded correctly between nodes based on hash ring assignment\n\n**Consistency Level Testing:**\n```bash\n# Test eventual consistency\ncurl -X POST localhost:8001/api/set -d '{\"key\":\"test\", \"value\":\"eventual\", \"consistency_level\":\"eventual\"}'\n\n# Test strong consistency  \ncurl -X POST localhost:8001/api/set -d '{\"key\":\"test\", \"value\":\"strong\", \"consistency_level\":\"strong\"}'\n```\n\n**Signs of Problems:**\n- **Operations timing out**: Check network connectivity and node health endpoints\n- **Inconsistent responses**: Verify hash ring state is synchronized across nodes via gossip\n- **Failed forwarding**: Check HTTPTransport configuration and error logging\n- **Missing replicas**: Verify replication factor configuration and quorum settings\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|------------------|-----|\n| Operations always timeout | Network connectivity or wrong addresses | Check node health endpoints with curl | Verify AdvertiseAddr in node configuration |\n| Inconsistent read results | Hash ring synchronization issues | Compare ring state across nodes | Ensure gossip protocol is working properly |\n| Write operations fail | Insufficient healthy replicas | Check cluster membership and node status | Reduce write quorum or fix failed nodes |\n| High read latency | Unnecessary replica consultation | Check consistency level configuration | Use eventual consistency for read-heavy workloads |\n| Memory usage keeps growing | Hints and tombstones not being cleaned up | Check hint delivery and cleanup logs | Verify TTL settings and cleanup scheduling |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** This section applies to all milestones but is particularly critical for Milestone 3 (Cluster Communication) and Milestone 4 (Replication & Consistency), as distributed systems failure handling becomes essential for system reliability and data consistency.\n\nBuilding a robust distributed cache requires comprehensive error handling that goes far beyond simple try-catch blocks. Distributed systems face unique challenges where partial failures, network partitions, and timing anomalies create complex failure scenarios that single-node systems never encounter. This section provides a systematic approach to identifying, detecting, and recovering from failures in our distributed cache system.\n\n### Mental Model: The Emergency Response Network\n\nThink of our distributed cache as a network of emergency response stations (nodes) that must coordinate during both normal operations and crisis situations. Just as emergency responders have protocols for communication failures, equipment breakdowns, and coordination challenges, our distributed cache needs well-defined procedures for handling node failures, network partitions, and data inconsistencies.\n\nWhen a fire station loses radio contact, other stations don't immediately assume it's destroyed—they follow escalation procedures to verify the situation and redistribute responsibilities. Similarly, when a cache node becomes unresponsive, the cluster follows systematic detection, verification, and recovery procedures before marking it as failed and redistributing its data.\n\nThe emergency response analogy helps us understand that failure handling in distributed systems requires multiple detection mechanisms, graduated responses, and coordination protocols that prevent false alarms while ensuring rapid response to real emergencies.\n\n### Failure Mode Analysis\n\nUnderstanding the types of failures that can occur in our distributed cache system is the foundation for building effective detection and recovery mechanisms. Each failure mode has distinct characteristics, impact patterns, and appropriate response strategies.\n\n![Node Lifecycle State Machine](./diagrams/node-lifecycle.svg)\n\nThe failure modes in our distributed cache fall into several categories based on their scope, duration, and impact on system operations:\n\n| Failure Mode | Scope | Duration | Impact on Operations | Impact on Data |\n|--------------|-------|----------|---------------------|-----------------|\n| Node Process Crash | Single Node | Until Restart | Node unavailable for all operations | Local cache lost, replicas intact |\n| Node Hardware Failure | Single Node | Hours to Days | Node unavailable until replacement | Local cache lost, replicas intact |\n| Network Partition | Multiple Nodes | Minutes to Hours | Cluster splits, reduced availability | Potential inconsistency between partitions |\n| Slow/Overloaded Node | Single Node | Variable | High latency, timeouts | No data loss but stale reads possible |\n| Memory Pressure | Single Node | Variable | Excessive eviction, reduced hit ratio | Cache effectiveness degraded |\n| Cascading Failure | Multiple Nodes | Minutes | Progressive node failures | Risk of total unavailability |\n| Split Brain Scenario | Cluster-wide | Until Resolution | Multiple active clusters | High risk of data divergence |\n| Clock Skew | Multiple Nodes | Persistent | TTL issues, ordering problems | Premature expiration, conflict resolution errors |\n| Byzantine Behavior | Single Node | Variable | Incorrect responses, data corruption | Potential data corruption propagation |\n\n#### Node-Level Failures\n\nNode-level failures represent the most common failure mode in distributed systems. Our cache node can fail in several distinct ways, each requiring different detection and recovery strategies.\n\n**Process crash failures** occur when the cache node process terminates unexpectedly due to bugs, resource exhaustion, or external signals. The node hardware remains functional, but the cache process is no longer running. This failure mode is characterized by immediate unavailability with no warning or graceful shutdown procedures.\n\n**Hardware failures** affect the entire physical or virtual machine hosting the cache node. These failures may be complete (server won't boot) or partial (disk failure, memory corruption, network interface failure). Hardware failures typically have longer recovery times since they require physical intervention or virtual machine recreation.\n\n**Performance degradation failures** occur when a node becomes extremely slow but doesn't completely fail. This might result from resource contention, garbage collection pauses, or external system dependencies becoming slow. These failures are particularly challenging because the node appears healthy but operates too slowly to meet system requirements.\n\n> **Critical Insight:** The distinction between fail-fast and fail-slow scenarios requires different detection mechanisms. Fail-fast scenarios (process crash) are easier to detect but create immediate unavailability. Fail-slow scenarios (performance degradation) are harder to detect but can cause subtle system-wide performance issues.\n\n#### Network-Level Failures\n\nNetwork failures in distributed systems create some of the most complex scenarios because they can cause partial connectivity where some nodes can communicate while others cannot.\n\n**Complete network isolation** occurs when a node loses all network connectivity. From the node's perspective, all other nodes appear failed. From the cluster's perspective, the isolated node appears failed. This symmetric failure is relatively straightforward to handle.\n\n**Asymmetric network partitions** create scenarios where node A can send messages to node B, but node B's responses don't reach node A. These failures can cause nodes to have inconsistent views of cluster membership and create complex debugging scenarios.\n\n**Network congestion and packet loss** can cause intermittent communication failures where some messages succeed while others fail or experience high latency. This creates jitter in failure detection mechanisms and can cause nodes to oscillate between healthy and suspected states.\n\n#### Cluster-Level Failures\n\nCluster-level failures affect the overall system's ability to maintain consistency and availability guarantees.\n\n**Split-brain scenarios** occur when network partitions divide the cluster into multiple sub-clusters, each believing it represents the authoritative cluster state. Without careful design, both partitions might continue accepting writes, leading to data divergence that's difficult to reconcile.\n\n**Cascading failures** happen when the failure of one or more nodes causes increased load on remaining nodes, potentially causing them to fail as well. This can lead to complete system unavailability even when the initial failure only affected a small portion of the cluster.\n\n**Quorum loss scenarios** occur when so many nodes fail that the remaining nodes cannot achieve the required quorum for read or write operations. The system becomes unavailable for consistency-critical operations even though some nodes remain functional.\n\n### Failure Detection Strategies\n\nEffective failure detection in distributed systems requires multiple complementary mechanisms because no single detection method can reliably identify all failure modes while avoiding false positives.\n\n![Failure Detection and Recovery Process](./diagrams/failure-detection-flow.svg)\n\nOur distributed cache employs a layered approach to failure detection, combining direct probing, gossip-based information sharing, and operation-based detection to create a comprehensive failure detection system.\n\n#### Direct Health Probing\n\nDirect health probing involves nodes actively sending health check requests to other nodes and interpreting the responses (or lack thereof) to assess node health.\n\n| Health Check Type | Probe Method | Success Criteria | Failure Threshold | Detection Time |\n|-------------------|--------------|------------------|-------------------|----------------|\n| Basic Connectivity | TCP Connect | Connection established | 3 consecutive failures | 3 × HealthCheckInterval |\n| Application Health | HTTP /health endpoint | 200 status + valid JSON | 3 consecutive failures | 3 × HealthCheckInterval |\n| Cache Functionality | GET known test key | Expected value returned | 3 consecutive failures | 3 × HealthCheckInterval |\n| Performance Health | Operation latency | < 95th percentile + 2σ | 5 consecutive slow responses | 5 × HealthCheckInterval |\n\nThe `ProbeNode` method implements our direct health checking mechanism with graduated levels of verification:\n\n1. **TCP Connectivity Check**: Establishes a raw TCP connection to verify basic network reachability\n2. **HTTP Health Endpoint**: Sends an HTTP GET request to the `/health` endpoint to verify the application is responding\n3. **Cache Operation Test**: Performs a lightweight cache operation to verify the node can process cache requests\n4. **Performance Validation**: Measures response time and compares against established baselines\n\n> **Decision: Multiple Health Check Levels**\n> - **Context**: Single-level health checks can miss scenarios where connectivity exists but application functionality is impaired\n> - **Options Considered**: TCP-only checks, HTTP-only checks, multi-level verification\n> - **Decision**: Implement graduated health checks with escalating verification depth\n> - **Rationale**: Provides early detection of network issues while ensuring application-level functionality\n> - **Consequences**: More complex implementation but significantly more accurate failure detection\n\nEach health check level has different timeout values and failure thresholds to balance detection speed with false positive avoidance:\n\n| Check Level | Timeout | Failure Threshold | Rationale |\n|-------------|---------|-------------------|-----------|\n| TCP Connect | 1 second | 3 consecutive | Network issues are typically persistent |\n| HTTP Health | 5 seconds | 3 consecutive | Application startup or GC pauses may cause temporary delays |\n| Cache Operation | 10 seconds | 3 consecutive | Cache operations may be slower during heavy load |\n| Performance Check | Variable | 5 consecutive | Performance degradation may be gradual |\n\n#### Gossip-Based Failure Detection\n\nWhile direct probing provides immediate failure detection between specific node pairs, gossip-based detection leverages the collective knowledge of the entire cluster to improve detection accuracy and reduce false positives.\n\nThe gossip protocol propagates failure suspicions throughout the cluster, allowing nodes to correlate their individual observations and build consensus about node health status. This distributed approach helps distinguish between actual node failures and local network issues affecting individual node pairs.\n\n| Gossip Message Field | Purpose | Update Trigger |\n|---------------------|---------|----------------|\n| NodeStates | Current health status of all known nodes | Health check results, timeout detection |\n| LastSeen | Timestamp of last successful communication | Successful operations, health probes |\n| Version | Logical timestamp for conflict resolution | Any status change |\n| SuspicionLevel | Confidence level in failure detection | Accumulated evidence from multiple sources |\n\nThe `HandleGossipMessage` method processes incoming cluster state information and updates local node status based on collective cluster knowledge:\n\n1. **Merge Remote State**: Compare received node states with local knowledge\n2. **Resolve Conflicts**: Use version numbers and timestamps to determine authoritative information\n3. **Update Suspicion Levels**: Increase confidence in failure detection when multiple nodes report issues\n4. **Trigger Local Actions**: Initiate additional health checks or status changes based on new information\n\n> **Critical Principle**: Gossip-based detection uses the \"phi accrual failure detector\" concept where suspicion levels gradually increase based on accumulated evidence rather than binary healthy/failed states.\n\n#### Operation-Based Detection\n\nOperation-based detection monitors the success and failure patterns of actual cache operations to identify node health issues that might not be caught by explicit health checks.\n\nThis approach is particularly valuable for detecting performance degradation, resource exhaustion, and subtle application-level issues that don't manifest as complete unavailability.\n\n| Detection Metric | Normal Range | Warning Threshold | Failure Threshold | Action Taken |\n|------------------|--------------|-------------------|-------------------|--------------|\n| Operation Success Rate | > 99% | < 95% | < 90% | Increase health check frequency |\n| Average Latency | Baseline ± 2σ | Baseline + 3σ | Baseline + 5σ | Mark as slow, reduce traffic |\n| Timeout Rate | < 0.1% | > 1% | > 5% | Begin failure suspicion process |\n| Memory Pressure | < 80% capacity | > 90% capacity | > 95% capacity | Trigger aggressive eviction |\n\nThe operation-based detection system maintains sliding windows of recent operation statistics and compares current performance against established baselines and peer node performance.\n\n### Recovery and Mitigation\n\nWhen failures are detected, the distributed cache system must execute coordinated recovery procedures to maintain availability and data consistency while minimizing the impact on ongoing operations.\n\nRecovery procedures vary significantly based on the type and scope of the detected failure, requiring different strategies for single-node failures versus cluster-wide issues.\n\n#### Automated Recovery Procedures\n\nAutomated recovery handles the most common failure scenarios without human intervention, focusing on rapid restoration of service availability and data accessibility.\n\n**Node Failure Recovery Process**:\n\n1. **Failure Confirmation**: Verify failure through multiple detection mechanisms to avoid false positives\n2. **Cluster State Update**: Propagate failure information through gossip protocol to ensure cluster-wide awareness\n3. **Hash Ring Update**: Remove failed node from consistent hash ring and recalculate key assignments\n4. **Replica Promotion**: Promote replica nodes to primary status for affected key ranges\n5. **Load Redistribution**: Rebalance traffic to remaining healthy nodes\n6. **Data Recovery**: Restore replication factor by creating new replicas on available nodes\n\n| Recovery Step | Trigger Condition | Timeout | Rollback Condition |\n|---------------|-------------------|---------|-------------------|\n| Failure Confirmation | 3 consecutive health check failures | 30 seconds | Node responds during confirmation |\n| Ring Update | Confirmed node failure | 5 seconds | N/A (atomic operation) |\n| Replica Promotion | Ring update complete | 10 seconds | Insufficient healthy replicas |\n| Load Redistribution | Replica promotion complete | Variable | Performance degradation detected |\n| Data Recovery | Load redistribution stable | Background | Resource exhaustion |\n\n**Network Partition Recovery**:\n\nNetwork partition recovery requires more sophisticated coordination because multiple sub-clusters may have continued operating independently during the partition.\n\n1. **Partition Detection**: Identify when network connectivity is restored between previously partitioned sub-clusters\n2. **State Reconciliation**: Compare cluster membership and data states between formerly isolated groups\n3. **Conflict Resolution**: Resolve any data conflicts that occurred during partition using vector clocks\n4. **Ring Merge**: Merge hash ring states and resolve any inconsistencies in key assignments\n5. **Anti-Entropy**: Background process to identify and repair any data inconsistencies\n\n> **Decision: Majority Partition Continues Operations**\n> - **Context**: During network partitions, we must decide which partition(s) can continue accepting writes\n> - **Options Considered**: All partitions continue, largest partition only, no writes during partition\n> - **Decision**: Only partitions containing a majority of nodes can accept writes\n> - **Rationale**: Prevents split-brain scenarios while maintaining availability when possible\n> - **Consequences**: Reduces availability during partitions but ensures consistency when partitions merge\n\n#### Manual Intervention Procedures\n\nSome failure scenarios require human intervention, either because automated recovery is insufficient or because the risks of automated actions are too high.\n\n**Data Corruption Recovery**:\n\nWhen data corruption is detected (through checksum verification or logical inconsistencies), manual intervention ensures that recovery actions don't propagate corruption to healthy replicas.\n\n1. **Isolation**: Immediately isolate suspected corrupted nodes to prevent further corruption spread\n2. **Assessment**: Manually verify the extent and nature of corruption\n3. **Recovery Source Selection**: Identify clean replica sources for data restoration\n4. **Controlled Restoration**: Manually trigger restoration from verified clean sources\n5. **Validation**: Verify restored data integrity before returning node to service\n\n**Cluster Bootstrap Recovery**:\n\nWhen all nodes in a cluster fail simultaneously (such as during a data center outage), manual intervention is required to safely restart the cluster and restore data consistency.\n\n| Bootstrap Step | Verification Required | Risk Level | Rollback Strategy |\n|----------------|----------------------|------------|-------------------|\n| Select Seed Nodes | Data integrity verification | Low | Shutdown and restart with different nodes |\n| Form Initial Ring | Ring consistency check | Medium | Recalculate ring with verified nodes only |\n| Start Gossip | Membership convergence | Low | Restart gossip with different intervals |\n| Enable Client Traffic | End-to-end operation test | High | Disable traffic and investigate |\n| Full Cluster Recovery | Complete data consistency check | High | Partial rollback to last known good state |\n\n#### Circuit Breaker Patterns\n\nCircuit breakers protect the system from cascading failures by temporarily stopping operations that are likely to fail, allowing failed components time to recover.\n\nOur distributed cache implements circuit breakers at multiple levels to prevent localized failures from affecting system-wide availability.\n\n| Circuit Breaker Type | Failure Threshold | Reset Timeout | Half-Open Test |\n|---------------------|-------------------|---------------|----------------|\n| Node Communication | 5 failures in 30 seconds | 60 seconds | Single health check |\n| Cache Operations | 10 failures in 10 seconds | 30 seconds | Single GET operation |\n| Replication | 3 failures in 5 seconds | 120 seconds | Single replication write |\n| Gossip Protocol | 5 failures in 60 seconds | 300 seconds | Single gossip exchange |\n\nCircuit breaker state transitions follow a three-state model:\n\n1. **Closed**: Normal operation, requests pass through\n2. **Open**: Failure threshold exceeded, requests fail fast without attempting operation\n3. **Half-Open**: Timeout expired, limited requests allowed to test recovery\n\n### Edge Cases and Corner Scenarios\n\nDistributed systems create numerous edge cases that arise from the interaction of timing, partial failures, and concurrent operations. These scenarios often occur rarely in practice but can cause significant issues when they do occur.\n\n#### Timing-Related Edge Cases\n\n**Clock Skew Impact on TTL**:\n\nWhen nodes have significantly different system clocks, TTL-based expiration can behave inconsistently across replicas. A key might expire on one replica but remain valid on another, leading to inconsistent read results.\n\n| Scenario | Clock Difference | Impact | Mitigation Strategy |\n|----------|-----------------|--------|-------------------|\n| Premature Expiration | Replica clock fast by 5 minutes | Key expires early on some replicas | Use logical timestamps for TTL |\n| Delayed Expiration | Replica clock slow by 5 minutes | Key remains valid past intended TTL | Regular clock synchronization monitoring |\n| Inconsistent Reads | Mixed clock skew | Different replicas return different results | Vector clock-based consistency |\n\n**Race Conditions in Membership Changes**:\n\nWhen nodes join or leave the cluster simultaneously, race conditions can occur in hash ring updates and key redistribution.\n\n1. **Concurrent Join Scenario**: Two nodes attempt to join simultaneously, potentially creating inconsistent ring states\n2. **Join-Leave Race**: A node joins while another leaves, creating complex key migration requirements\n3. **Rapid Membership Changes**: Multiple membership changes occur faster than gossip can propagate\n\n#### Consistency Edge Cases\n\n**Read-Your-Writes Violations**:\n\nIn distributed systems with eventual consistency, a client might not be able to read data it just wrote if the read request routes to a replica that hasn't yet received the update.\n\n| Read Type | Consistency Guarantee | Edge Case Risk | Mitigation |\n|-----------|---------------------|---------------|------------|\n| Read from Any Replica | Eventual | High risk of stale reads | Session affinity |\n| Read from Write Replica | Read-your-writes | Network routing changes | Consistent hashing with preference list |\n| Quorum Reads | Strong | Split-brain scenarios | Majority quorum requirement |\n\n**Vector Clock Conflicts**:\n\nWhen using vector clocks for conflict resolution, certain scenarios can create ambiguous conflict resolution situations.\n\n1. **Concurrent Updates**: Multiple clients update the same key simultaneously on different replicas\n2. **Clock Reset**: Node restart causes vector clock reset, creating false conflicts\n3. **Deep Conflicts**: Long-running partitions create complex conflict trees\n\n#### Resource Exhaustion Edge Cases\n\n**Memory Pressure During Recovery**:\n\nNode failures can cause remaining nodes to experience sudden memory pressure as they take over responsibility for additional key ranges.\n\n**Network Bandwidth Saturation**:\n\nDuring large-scale recovery operations, network bandwidth can become saturated, causing additional nodes to appear failed due to communication timeouts.\n\n**Disk Space Exhaustion**:\n\nLogging and persistence mechanisms can consume excessive disk space during failure scenarios, potentially causing additional failures.\n\n#### Byzantine Failure Scenarios\n\nWhile our cache system doesn't implement full Byzantine fault tolerance, we must handle scenarios where nodes behave incorrectly due to bugs, corruption, or misconfiguration.\n\n| Byzantine Behavior | Detection Method | Response Strategy |\n|-------------------|------------------|-------------------|\n| Incorrect Hash Ring | Ring consistency verification | Exclude from membership decisions |\n| Data Corruption | Checksum verification | Quarantine and rebuild from replicas |\n| Protocol Violations | Message validation | Temporary communication suspension |\n| Performance Attacks | Rate limiting and monitoring | Traffic throttling and investigation |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: False Positive Failure Detection**\n\nMany implementations use overly aggressive failure detection timeouts that cause healthy but temporarily slow nodes to be marked as failed. This creates unnecessary churn in cluster membership and can trigger cascading failures.\n\n**Why it's wrong**: Temporary network congestion or garbage collection pauses are normal in distributed systems. Marking nodes as failed too quickly causes more harm than the original performance issue.\n\n**How to fix**: Implement graduated suspicion levels instead of binary healthy/failed states. Use multiple detection mechanisms and require consensus before marking nodes as failed.\n\n⚠️ **Pitfall: Ignoring Split-Brain Scenarios**\n\nSome implementations assume network partitions won't occur or will be quickly resolved, failing to implement proper split-brain prevention mechanisms.\n\n**Why it's wrong**: Network partitions are common in distributed systems, especially in cloud environments. Without split-brain prevention, data divergence can occur that's difficult or impossible to reconcile.\n\n**How to fix**: Implement majority quorum requirements for membership changes and write operations. Ensure that only one partition can remain active during network splits.\n\n⚠️ **Pitfall: Inadequate Circuit Breaker Implementation**\n\nCircuit breakers that don't properly isolate failures can allow cascading failures to propagate throughout the system.\n\n**Why it's wrong**: Without proper isolation, a single slow or failed node can cause the entire cluster to become unresponsive as healthy nodes wait for timeouts.\n\n**How to fix**: Implement circuit breakers at multiple levels with appropriate timeout values. Ensure that circuit breakers fail fast and provide meaningful feedback about failure reasons.\n\n⚠️ **Pitfall: Inconsistent Error Propagation**\n\nDifferent types of errors (network timeouts, node failures, data corruption) are often handled inconsistently, making debugging difficult and potentially masking serious issues.\n\n**Why it's wrong**: Inconsistent error handling makes it difficult to understand system behavior and can hide critical issues that need immediate attention.\n\n**How to fix**: Define clear error categories and handling strategies. Implement structured logging that provides sufficient context for debugging while maintaining performance.\n\n### Implementation Guidance\n\nBuilding robust failure handling requires careful attention to both the detection mechanisms and the recovery procedures. The following implementation guidance provides concrete approaches for each component.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Health Checking | HTTP endpoints with JSON status | gRPC health checking protocol |\n| Failure Detection | Timer-based probing | Phi accrual failure detector |\n| Circuit Breakers | Simple counter-based | Hystrix-style with metrics |\n| Error Logging | Standard library logging | Structured logging with correlation IDs |\n| Metrics Collection | Basic counters | Prometheus metrics with alerting |\n| Distributed Tracing | Request ID propagation | OpenTelemetry distributed tracing |\n\n#### File Structure\n\n```\ninternal/failuredetector/\n  detector.go              ← main failure detection logic\n  probe.go                 ← health checking implementation\n  circuit_breaker.go       ← circuit breaker patterns\n  phi_detector.go          ← phi accrual failure detector\n  detector_test.go         ← comprehensive failure scenario tests\n\ninternal/recovery/\n  coordinator.go           ← recovery coordination logic\n  ring_recovery.go         ← hash ring recovery procedures\n  data_recovery.go         ← data consistency recovery\n  recovery_test.go         ← recovery scenario tests\n\ninternal/gossip/\n  membership.go            ← cluster membership management\n  failure_propagation.go   ← failure information spreading\n  state_merge.go           ← cluster state reconciliation\n  gossip_test.go           ← membership change tests\n```\n\n#### Health Checking Infrastructure\n\n```go\n// HealthChecker provides multi-level health verification for cluster nodes.\n// Implements graduated health checking with escalating verification depth.\ntype HealthChecker struct {\n    transport    *HTTPTransport\n    config       *HealthConfig\n    metrics      *HealthMetrics\n    circuitBreakers map[string]*CircuitBreaker\n    logger       *log.Logger\n}\n\n// HealthConfig defines timeouts and thresholds for health checking.\ntype HealthConfig struct {\n    TCPTimeout      time.Duration `json:\"tcp_timeout\"`\n    HTTPTimeout     time.Duration `json:\"http_timeout\"`\n    CacheTimeout    time.Duration `json:\"cache_timeout\"`\n    FailureThreshold int          `json:\"failure_threshold\"`\n    CheckInterval   time.Duration `json:\"check_interval\"`\n    SlowThreshold   time.Duration `json:\"slow_threshold\"`\n}\n\n// NewHealthChecker creates a health checker with configured timeouts and transport.\nfunc NewHealthChecker(transport *HTTPTransport, config *HealthConfig) *HealthChecker {\n    return &HealthChecker{\n        transport:       transport,\n        config:         config,\n        metrics:        NewHealthMetrics(),\n        circuitBreakers: make(map[string]*CircuitBreaker),\n        logger:         log.New(os.Stderr, \"[HEALTH] \", log.LstdFlags),\n    }\n}\n\n// ProbeNode performs graduated health checks on the specified node.\n// Returns detailed health status including performance metrics.\nfunc (h *HealthChecker) ProbeNode(ctx context.Context, nodeID string, address string) (*HealthResult, error) {\n    // TODO 1: Check circuit breaker state - if open, return cached failure result\n    // TODO 2: Perform TCP connectivity test with h.config.TCPTimeout\n    // TODO 3: If TCP succeeds, perform HTTP health check with h.config.HTTPTimeout\n    // TODO 4: If HTTP succeeds, perform cache operation test with h.config.CacheTimeout\n    // TODO 5: Measure response times and compare against h.config.SlowThreshold\n    // TODO 6: Update circuit breaker state based on results\n    // TODO 7: Record metrics and log results for monitoring\n    // TODO 8: Return comprehensive health result with all test outcomes\n    \n    // Hint: Use time.WithTimeout for each test level\n    // Hint: Record both success/failure and performance metrics\n    return nil, nil\n}\n\n// HealthResult contains detailed results from multi-level health checking.\ntype HealthResult struct {\n    NodeID        string        `json:\"node_id\"`\n    Timestamp     time.Time     `json:\"timestamp\"`\n    TCPHealthy    bool         `json:\"tcp_healthy\"`\n    HTTPHealthy   bool         `json:\"http_healthy\"`\n    CacheHealthy  bool         `json:\"cache_healthy\"`\n    ResponseTime  time.Duration `json:\"response_time\"`\n    ErrorDetails  string       `json:\"error_details,omitempty\"`\n    OverallHealth HealthStatus `json:\"overall_health\"`\n}\n\ntype HealthStatus string\n\nconst (\n    HealthStatusHealthy    HealthStatus = \"healthy\"\n    HealthStatusSlow      HealthStatus = \"slow\"\n    HealthStatusSuspected HealthStatus = \"suspected\"\n    HealthStatusFailed    HealthStatus = \"failed\"\n)\n```\n\n#### Failure Detection Coordinator\n\n```go\n// FailureDetector coordinates multiple detection mechanisms for comprehensive\n// failure detection with minimal false positives.\ntype FailureDetector struct {\n    nodeID          string\n    healthChecker   *HealthChecker\n    phiDetector     *PhiAccrualDetector\n    gossipManager   *GossipManager\n    nodeStates      map[string]*NodeHealthState\n    suspicionLevels map[string]float64\n    callbacks       []FailureCallback\n    mutex           sync.RWMutex\n    stopCh          chan struct{}\n    logger          *log.Logger\n}\n\n// NodeHealthState tracks comprehensive health information for a cluster node.\ntype NodeHealthState struct {\n    NodeID           string            `json:\"node_id\"`\n    Address          string            `json:\"address\"`\n    Status           HealthStatus      `json:\"status\"`\n    LastSeen         time.Time         `json:\"last_seen\"`\n    LastHealthCheck  time.Time         `json:\"last_health_check\"`\n    ConsecutiveFailures int            `json:\"consecutive_failures\"`\n    SuspicionLevel   float64           `json:\"suspicion_level\"`\n    PerformanceMetrics *PerformanceMetrics `json:\"performance_metrics\"`\n    Version          uint64            `json:\"version\"`\n}\n\n// StartMonitoring begins continuous failure detection for all known nodes.\n// Coordinates health checking, gossip information, and suspicion level updates.\nfunc (fd *FailureDetector) StartMonitoring(ctx context.Context, knownNodes map[string]string) error {\n    // TODO 1: Initialize node states for all known nodes\n    // TODO 2: Start health checking goroutine that probes nodes periodically\n    // TODO 3: Start gossip processing goroutine that handles membership updates\n    // TODO 4: Start phi detector updates that calculate suspicion levels\n    // TODO 5: Start state reconciliation that merges local and remote information\n    // TODO 6: Register cleanup handler for graceful shutdown\n    \n    // Hint: Use select with ctx.Done() for cancellation\n    // Hint: Space out health checks to avoid thundering herd\n    return nil\n}\n\n// ProcessGossipUpdate incorporates remote failure detection information\n// into local node health state tracking.\nfunc (fd *FailureDetector) ProcessGossipUpdate(gossipMsg *GossipMessage) error {\n    // TODO 1: Extract node state information from gossip message\n    // TODO 2: Compare remote timestamps with local knowledge\n    // TODO 3: Merge state information using version numbers for conflict resolution\n    // TODO 4: Update local suspicion levels based on remote observations\n    // TODO 5: Trigger additional health checks if suspicion levels increase\n    // TODO 6: Notify failure callbacks if node status changes significantly\n    \n    // Hint: Use version numbers to determine which information is newer\n    // Hint: Increase suspicion when multiple nodes report issues\n    return nil\n}\n```\n\n#### Recovery Coordinator\n\n```go\n// RecoveryCoordinator manages automated recovery procedures for various\n// failure scenarios while maintaining data consistency.\ntype RecoveryCoordinator struct {\n    nodeID          string\n    hashRing        *HashRing\n    replicationMgr  *ReplicationManager\n    gossipManager   *GossipManager\n    localStorage    *LRUCache\n    recoveryQueue   chan RecoveryTask\n    activeRecovery  map[string]*RecoveryState\n    config          *RecoveryConfig\n    mutex           sync.RWMutex\n    logger          *log.Logger\n}\n\n// RecoveryConfig defines parameters for automated recovery procedures.\ntype RecoveryConfig struct {\n    ConfirmationTimeout   time.Duration `json:\"confirmation_timeout\"`\n    RingUpdateTimeout     time.Duration `json:\"ring_update_timeout\"`\n    ReplicationTimeout    time.Duration `json:\"replication_timeout\"`\n    MaxConcurrentRecovery int          `json:\"max_concurrent_recovery\"`\n    DataRepairEnabled     bool         `json:\"data_repair_enabled\"`\n    AutoPromoteReplicas   bool         `json:\"auto_promote_replicas\"`\n}\n\n// HandleNodeFailure initiates comprehensive recovery procedures for failed node.\n// Coordinates ring updates, replica promotion, and data redistribution.\nfunc (rc *RecoveryCoordinator) HandleNodeFailure(ctx context.Context, failedNodeID string, confirmedBy []string) error {\n    // TODO 1: Verify failure confirmation from multiple sources in confirmedBy\n    // TODO 2: Create recovery task with appropriate priority and dependencies\n    // TODO 3: Remove failed node from hash ring and recalculate key assignments\n    // TODO 4: Identify all key ranges that need new primary nodes\n    // TODO 5: Promote replica nodes to primary status for affected ranges\n    // TODO 6: Initiate data replication to restore replication factor\n    // TODO 7: Update cluster membership and propagate through gossip\n    // TODO 8: Monitor recovery progress and handle any secondary failures\n    \n    // Hint: Use context with timeout for each recovery step\n    // Hint: Rollback changes if critical steps fail\n    return nil\n}\n\n// HandleNetworkPartitionRecovery reconciles cluster state after partition healing.\n// Resolves conflicts between formerly isolated sub-clusters.\nfunc (rc *RecoveryCoordinator) HandleNetworkPartitionRecovery(ctx context.Context, mergeNodes []string) error {\n    // TODO 1: Collect cluster state from all nodes in mergeNodes\n    // TODO 2: Identify conflicts in membership, ring topology, and data\n    // TODO 3: Resolve membership conflicts using node priorities or timestamps\n    // TODO 4: Merge hash ring states and resolve key assignment conflicts\n    // TODO 5: Trigger anti-entropy process to repair data inconsistencies\n    // TODO 6: Propagate merged state through gossip to all nodes\n    \n    // Hint: Use vector clocks to determine causality in state conflicts\n    // Hint: Prioritize larger partition's state for conflict resolution\n    return nil\n}\n```\n\n#### Milestone Checkpoints\n\n**Health Checking Verification**:\n```bash\n# Test basic health checking functionality\ngo test ./internal/failuredetector -v -run TestHealthChecker\n\n# Verify graduated health check levels\ncurl http://localhost:8080/debug/health/nodes\n# Expected: JSON showing health status for all cluster nodes\n\n# Test failure detection timing\n# Kill one node, verify detection within 3 * HealthCheckInterval\n```\n\n**Recovery Procedure Validation**:\n```bash\n# Test automated node failure recovery\ngo test ./internal/recovery -v -run TestNodeFailureRecovery\n\n# Verify ring rebalancing after node failure\n# Start 3-node cluster, kill one node, verify key redistribution\ncurl http://localhost:8080/debug/ring/topology\n# Expected: Ring shows 2 nodes with evenly distributed keys\n\n# Test partition recovery\n# Simulate network partition, verify recovery when partition heals\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis Steps | Fix |\n|---------|--------------|-----------------|-----|\n| False failure alerts | Aggressive timeouts | Check health check intervals and thresholds | Increase timeouts, implement graduated suspicion |\n| Slow failure detection | Conservative timeouts | Monitor failure detection lag metrics | Reduce health check intervals, implement phi detector |\n| Split-brain during partition | Missing quorum enforcement | Verify minority partitions stop accepting writes | Implement majority quorum checks |\n| Cascading failures | Missing circuit breakers | Check error propagation and timeout patterns | Add circuit breakers, implement graceful degradation |\n| Memory leaks during recovery | Resource cleanup issues | Monitor goroutine and memory usage during recovery | Add proper resource cleanup and context cancellation |\n| Inconsistent cluster state | Race conditions in gossip | Check gossip convergence and conflict resolution | Implement proper state merging with version numbers |\n\n\n## Testing Strategy\n\n> **Milestone(s):** This section provides testing methodologies and validation approaches for all milestones, with particular emphasis on verifying the correctness and resilience of distributed cache operations across Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication & Consistency).\n\n### Mental Model: The Quality Control Factory\n\nThink of testing a distributed cache like operating a quality control factory for a complex manufacturing process. Just as a factory has multiple quality checkpoints - individual part inspection, assembly line testing, final product validation, and stress testing under extreme conditions - our distributed cache requires layered testing approaches.\n\nAt the component level (unit testing), we inspect each \"part\" in isolation - ensuring the hash ring calculates positions correctly, the LRU cache evicts entries properly, and the gossip protocol formats messages accurately. This is like checking individual screws and bolts before assembly.\n\nAt the integration level, we test how these components work together - verifying that the hash ring correctly routes requests to cache nodes, that gossip messages propagate cluster state changes, and that replication maintains data consistency. This is like testing assembled subsystems before final integration.\n\nAt the system level (milestone checkpoints), we validate complete end-to-end operations - ensuring clients can store and retrieve data reliably, that the cluster handles node failures gracefully, and that consistency guarantees hold under various scenarios. This is like testing the complete manufactured product.\n\nFinally, chaos testing is like subjecting our product to extreme environmental conditions - network partitions, cascading failures, and byzantine behavior - to ensure it remains functional when the real world throws unexpected challenges at it.\n\n### Unit Testing Approach\n\nUnit testing forms the foundation of our testing strategy, focusing on individual components in complete isolation. Each component must demonstrate correct behavior independently before we can trust it in the larger distributed system.\n\n> **Decision: Isolation-First Testing Strategy**\n> - **Context**: Distributed systems have complex interdependencies that can mask individual component bugs, making root cause analysis extremely difficult during integration testing.\n> - **Options Considered**: Mock-heavy isolation, dependency injection with real components, hybrid approach with selective mocking\n> - **Decision**: Comprehensive mocking of all external dependencies with dependency injection for testability\n> - **Rationale**: Complete isolation ensures that test failures directly indicate the component under test, not its dependencies. This enables parallel development where teams can test components before their dependencies are complete.\n> - **Consequences**: Requires more sophisticated test setup but provides faster feedback, easier debugging, and higher confidence in component correctness.\n\n#### Hash Ring Component Testing\n\nThe `HashRing` component requires extensive testing to ensure consistent behavior across membership changes. Our test strategy validates both the mathematical correctness of consistent hashing and the implementation's handling of edge cases.\n\n| Test Category | Test Cases | Validation Method | Expected Behavior |\n|--------------|------------|-------------------|-------------------|\n| Basic Operations | AddNode, RemoveNode, GetNode | Direct method calls with known inputs | Deterministic node assignment for identical keys |\n| Virtual Node Distribution | Node addition with different virtual node counts | Statistical analysis of key distribution | Even load distribution within 10% variance |\n| Key Redistribution | Node removal and addition cycles | Track which keys change assignment | Minimal key movement (O(K/N) keys affected) |\n| Edge Cases | Empty ring, single node, duplicate adds/removes | Boundary condition testing | Graceful handling without panics or corruption |\n| Consistency Verification | Multiple identical rings with same operations | Cross-ring comparison | Identical results across independent instances |\n\nThe hash ring testing focuses particularly on the mathematical properties of consistent hashing. We generate large sets of random keys and verify that load distribution remains balanced as nodes are added and removed. The critical insight is that a properly implemented consistent hash ring should maintain stable key assignments - when we add or remove a node, only keys in the affected ring segments should move to different nodes.\n\n```\nHash Ring Test Scenarios:\n1. Create ring with nodes A, B, C and 100 virtual nodes each\n2. Generate 10,000 random keys and record their node assignments  \n3. Add node D and verify only ~25% of keys change assignment\n4. Remove node B and verify only ~33% of remaining keys move\n5. Statistical validation: no node should hold >40% or <10% of keys\n```\n\nOur virtual node testing uses statistical analysis to ensure load balancing effectiveness. We measure the coefficient of variation in key distribution - a properly tuned system with sufficient virtual nodes should maintain CV < 0.1 under normal operation.\n\n#### LRU Cache Component Testing\n\nThe `LRUCache` component testing emphasizes correctness under concurrent access and accurate memory accounting. Since cache nodes will face high concurrency, our tests must validate thread safety and performance characteristics.\n\n| Test Category | Test Cases | Concurrency Level | Validation Method |\n|--------------|------------|------------------|-------------------|\n| Basic Operations | Get, Set, Delete with various data sizes | Single-threaded | Verify correct storage and retrieval |\n| LRU Eviction | Fill cache beyond capacity, verify eviction order | Single-threaded | Track access patterns and eviction sequence |\n| TTL Expiration | Set entries with various TTL values | Single-threaded | Verify expiration timing and cleanup |\n| Memory Accounting | Operations with different value sizes | Single-threaded | Verify accurate memory usage tracking |\n| Concurrent Access | Mixed read/write operations | 100 goroutines | Verify data integrity and no race conditions |\n| Performance Characteristics | High-throughput operations | Variable load | Measure latency percentiles and throughput |\n\nMemory accounting testing requires particular attention to precision. We validate that the cache accurately tracks memory usage for both keys and values, including metadata overhead. The test creates entries with known sizes and verifies that reported memory usage matches expected calculations within a small tolerance for metadata overhead.\n\nTTL expiration testing uses controlled time manipulation to verify that entries expire correctly. We set entries with 1-second TTLs, advance time by 1.5 seconds, and verify that expired entries are removed during cleanup operations. The critical aspect is testing both lazy expiration (during access) and active cleanup (background processes).\n\nConcurrent access testing employs a sophisticated approach using multiple goroutines performing random operations while continuously validating cache invariants. We use race detection tools and verify that the cache maintains consistency under high contention.\n\n#### Gossip Protocol Component Testing\n\nGossip protocol testing validates message propagation, convergence properties, and network partition handling. The challenge is simulating network conditions and timing scenarios that occur in distributed environments.\n\n| Test Scenario | Network Conditions | Expected Outcome | Verification Method |\n|--------------|-------------------|------------------|-------------------|\n| Basic Propagation | Reliable network, 5 nodes | Information reaches all nodes within 3 rounds | Message tracking and convergence measurement |\n| Network Delays | Variable latency 10-500ms | Eventually consistent state across all nodes | State comparison after stabilization period |\n| Message Loss | 10% packet loss rate | Gossip resilience maintains convergence | Probabilistic delivery verification |\n| Partition Recovery | Split network, then heal | Automatic state reconciliation | Compare states before/after partition |\n| Membership Changes | Nodes join/leave during gossip | Accurate membership propagation | Membership consistency validation |\n\nGossip testing uses a sophisticated network simulator that can introduce controlled delays, packet loss, and partitions. This allows us to validate convergence properties under realistic network conditions without requiring actual network infrastructure.\n\nThe convergence testing measures both speed and accuracy of information propagation. We inject a membership change at one node and measure how many gossip rounds it takes for all nodes to learn about the change. Properly tuned gossip should achieve convergence within O(log N) rounds.\n\n#### Replication Manager Component Testing\n\nReplication manager testing focuses on consistency guarantees, conflict resolution, and quorum behavior. These tests are particularly complex because they must validate distributed protocols while running in a single-process test environment.\n\n| Test Category | Scenario | Consistency Level | Expected Result |\n|--------------|----------|------------------|-----------------|\n| Basic Replication | Write to primary, read from replica | Eventual consistency | Value propagates within propagation delay |\n| Quorum Operations | R=2, W=2, N=3 configuration | Strong consistency | Reads see all committed writes |\n| Conflict Resolution | Concurrent writes to same key | Vector clock ordering | Deterministic conflict resolution |\n| Replica Failure | One replica unavailable during write | Sloppy quorum | Write succeeds with hinted handoff |\n| Read Repair | Stale replica detected during read | Automatic repair | Stale replica updated asynchronously |\n\nVector clock testing requires careful validation of causality relationships. We create scenarios with concurrent updates and verify that vector clocks correctly identify concurrent vs. causally ordered events. The test creates two concurrent writes to the same key from different nodes and verifies that the conflict resolution algorithm handles them deterministically.\n\nQuorum testing simulates various replica availability scenarios and validates that the system maintains consistency guarantees. We use mock transport layers to simulate network partitions and node failures, then verify that quorum requirements are properly enforced.\n\n### Integration Testing\n\nIntegration testing validates component interactions and emergent system behaviors that cannot be observed at the unit level. These tests use real components working together, with carefully controlled external dependencies.\n\n#### Cross-Component Data Flow Testing\n\nThe most critical integration tests validate complete data flow paths through multiple components. These tests ensure that data transformations and state changes propagate correctly across component boundaries.\n\n| Data Flow Path | Components Involved | Test Scenario | Validation Points |\n|----------------|-------------------|---------------|-------------------|\n| Client Request Processing | HTTPTransport, HashRing, LRUCache | Client GET request for cached key | Request routing, cache lookup, response formatting |\n| Ring Rebalancing | HashRing, ReplicationManager, GossipManager | Node addition triggers rebalancing | Key redistribution, replica updates, gossip propagation |\n| Failure Detection | HealthChecker, FailureDetector, GossipManager | Node becomes unresponsive | Health check failure, suspicion level increase, gossip notification |\n| Conflict Resolution | ReplicationManager, VectorClock, LRUCache | Concurrent writes during partition | Conflict detection, vector clock comparison, resolution outcome |\n\nClient request processing tests trace complete request lifecycles from HTTP reception through hash ring lookup, cache node routing, local storage access, and response generation. We validate that each component correctly transforms and forwards data to the next stage.\n\nRing rebalancing integration tests are particularly complex because they involve coordinated updates across multiple components. When a new node joins, the hash ring must recalculate key assignments, the replication manager must identify keys that need to move, and the gossip manager must propagate the membership change. Our tests validate that these operations complete successfully and leave the system in a consistent state.\n\n#### Cluster State Consistency Testing\n\nCluster state consistency testing validates that all nodes maintain compatible views of cluster membership, ring topology, and data placement. These tests are essential because inconsistent cluster state can lead to data loss or availability problems.\n\n| Consistency Scenario | Initial State | State Change | Consistency Check |\n|---------------------|---------------|--------------|-------------------|\n| Membership Convergence | 3-node cluster | Add node D | All nodes agree on 4-node membership within gossip interval |\n| Ring Position Agreement | Identical hash rings | Virtual node changes | All nodes calculate identical key-to-node mappings |\n| Failure Detection Consensus | Healthy cluster | Node C stops responding | All nodes mark C as failed within detection timeout |\n| Partition Recovery | Split into two groups | Network heals | Consistent merged state across all nodes |\n\nMembership convergence testing uses controlled timing to ensure that gossip propagation completes within expected timeframes. We start with a stable cluster, introduce a membership change, and measure how long it takes for all nodes to converge on the new membership. The test fails if convergence takes longer than 3x the gossip interval or if nodes end up with inconsistent views.\n\nRing position agreement testing validates that all nodes compute identical hash ring states given the same inputs. We create multiple independent `HashRing` instances, perform identical operations on each, and verify that they produce identical results for key lookups and node assignments.\n\n#### End-to-End Operation Validation\n\nEnd-to-end operation validation tests complete client-visible operations across the entire distributed system. These tests simulate real client usage patterns and validate that the system provides expected semantics.\n\n| Operation Type | Test Scenario | Fault Conditions | Success Criteria |\n|----------------|---------------|------------------|------------------|\n| Simple Get/Set | Store value, retrieve value | None | Retrieved value matches stored value |\n| Replicated Operations | Write with R=2, W=2 | One replica fails during write | Write succeeds, subsequent reads return correct value |\n| Consistent Reads | Write to key, immediate read | Write still replicating | Read returns either old or new value, never partial state |\n| TTL Expiration | Set key with 5-second TTL | None | Key unavailable after TTL expiration |\n| Node Failure Recovery | Ongoing operations | Primary node for key fails | Operations continue using replica nodes |\n\nSimple get/set testing validates basic functionality but with realistic data sizes and access patterns. We test with various value sizes from small metadata (100 bytes) to large objects (1MB) to ensure the system handles different workloads correctly.\n\nReplicated operations testing introduces controlled failures during multi-node operations to validate fault tolerance. We use network simulation to disconnect nodes at specific points during replication and verify that the system handles these scenarios gracefully.\n\nConsistent reads testing validates that clients never observe partial or corrupted state, even when concurrent operations are modifying the same keys. This requires careful coordination between replication and consistency mechanisms.\n\n### Milestone Checkpoints\n\nMilestone checkpoints provide concrete validation criteria for each development phase. These checkpoints ensure that each milestone delivers working functionality before moving to the next phase.\n\n#### Milestone 1: Consistent Hash Ring Validation\n\nThe consistent hash ring milestone focuses on correct key distribution and minimal redistribution during membership changes. The validation emphasizes mathematical correctness and performance characteristics.\n\n| Checkpoint | Test Method | Success Criteria | Measurement Approach |\n|------------|-------------|------------------|---------------------|\n| Even Distribution | Generate 50,000 random keys across 5-node ring | No node holds >25% of keys | Statistical analysis of key distribution |\n| Virtual Node Effectiveness | Compare distribution with 1, 10, 100, 1000 virtual nodes | Distribution improves with more virtual nodes | Coefficient of variation measurement |\n| Minimal Redistribution | Add node to 4-node ring with 10,000 keys | <30% of keys change assignment | Track key assignment changes |\n| Lookup Performance | Time 100,000 key lookups | <1μs average lookup time | Benchmark measurement |\n| Memory Efficiency | Create ring with 100 nodes | <1MB memory usage for ring metadata | Memory profiling |\n\nEven distribution validation uses statistical analysis to ensure load balancing effectiveness. We generate large numbers of random keys and verify that they distribute evenly across available nodes. The test calculates the coefficient of variation (CV) - the ratio of standard deviation to mean - for key distribution across nodes. A well-balanced system should achieve CV < 0.15 with sufficient virtual nodes.\n\nVirtual node effectiveness testing demonstrates the improvement in load balancing as virtual node count increases. We start with 1 virtual node per physical node (equivalent to simple modulo hashing) and increase to 1000 virtual nodes, measuring distribution balance at each step. The test should show decreasing CV values as virtual node count increases.\n\nMinimal redistribution validation is critical for system performance because excessive key movement during membership changes creates significant network traffic and temporary unavailability. We record the initial assignment of 10,000 keys across a 4-node ring, add a fifth node, and count how many keys change assignment. Ideally, only ~20% of keys should move (since the new node should receive 1/5 of the total load).\n\n#### Milestone 2: Cache Node Validation\n\nCache node validation focuses on correct LRU behavior, accurate memory accounting, and proper TTL handling. These tests ensure that individual cache nodes provide reliable storage with expected eviction and expiration semantics.\n\n| Checkpoint | Test Scenario | Expected Behavior | Validation Method |\n|------------|---------------|------------------|-------------------|\n| LRU Eviction Correctness | Fill cache, access subset, add new entries | Least recently accessed entries evicted first | Track access order and eviction sequence |\n| Memory Limit Enforcement | Add entries until memory limit reached | Eviction begins before memory limit exceeded | Monitor memory usage during operations |\n| TTL Expiration Accuracy | Set entries with 1-second TTL | Entries expire within 1.1 seconds | Time-based validation with tolerance |\n| Concurrent Access Safety | 50 goroutines performing random operations | No data races or corruption | Race detector and data integrity checks |\n| Performance Under Load | 10,000 operations per second | <1ms p99 latency for cache operations | Latency distribution measurement |\n\nLRU eviction correctness testing requires careful tracking of access patterns and eviction order. We create a cache with capacity for 10 entries, fill it completely, access entries 1, 3, 5, 7, 9, then add two new entries. The test should evict entries 2, 4 (least recently accessed) while preserving the others.\n\nMemory limit enforcement testing validates that the cache never significantly exceeds its configured memory limit. We add progressively larger entries and verify that eviction begins before the memory limit is reached. The cache should maintain memory usage within 5% of the configured limit.\n\nTTL expiration accuracy testing uses controlled time manipulation to verify precise expiration timing. We set entries with known TTL values and verify they expire at the expected time. The test accounts for cleanup delays and accepts expiration within a small tolerance window.\n\n#### Milestone 3: Cluster Communication Validation\n\nCluster communication validation ensures that nodes can discover each other, maintain consistent membership views, and route requests correctly. These tests validate the distributed aspects of the system.\n\n| Checkpoint | Test Configuration | Expected Outcome | Verification Method |\n|------------|-------------------|------------------|---------------------|\n| Node Discovery | Start 3 nodes sequentially with join addresses | All nodes discover each other within 10 seconds | Membership list comparison |\n| Gossip Convergence | Update node state, measure propagation | State change reaches all nodes within 5 gossip intervals | State consistency measurement |\n| Request Routing | Send requests for specific keys | Requests routed to correct hash ring nodes | Request destination validation |\n| Health Check Accuracy | Stop node processes | Other nodes detect failure within 30 seconds | Failure detection timing |\n| Partition Recovery | Split cluster, then reconnect | Cluster state merges correctly after recovery | State reconciliation validation |\n\nNode discovery testing starts nodes with bootstrap addresses pointing to previously started nodes. Each new node should successfully join the cluster and receive complete membership information from existing nodes. The test verifies that all nodes end up with identical membership lists.\n\nGossip convergence testing measures how quickly information propagates through the cluster. We update node metadata (such as load statistics) at one node and measure how long it takes for this information to reach all other nodes. With properly tuned gossip intervals, convergence should occur within 3-5 rounds.\n\nRequest routing validation ensures that clients can send requests to any node and have them forwarded to the correct destination based on consistent hashing. We generate requests for specific keys with known hash ring assignments and verify that they reach the expected nodes.\n\n#### Milestone 4: Replication and Consistency Validation\n\nReplication and consistency validation focuses on data durability, consistency guarantees, and conflict resolution. These tests ensure that the distributed cache maintains correctness under various failure scenarios.\n\n| Checkpoint | Scenario | Consistency Configuration | Success Criteria |\n|------------|----------|--------------------------|------------------|\n| Basic Replication | Write key with RF=3 | W=1, R=1 | Value accessible from any replica node |\n| Quorum Consistency | Concurrent read/write operations | W=2, R=2, N=3 | Reads never return stale data |\n| Conflict Resolution | Concurrent writes during partition | Vector clock timestamps | Deterministic conflict resolution |\n| Replica Recovery | Node failure and restart | Hinted handoff enabled | Missed writes applied during recovery |\n| Read Repair | Stale replica detected | Automatic repair enabled | Stale replicas updated transparently |\n\nBasic replication testing verifies that writes are successfully replicated to the configured number of nodes. We write a key with replication factor 3, then verify that the value can be retrieved by reading directly from each replica node. The test should succeed even if individual replica nodes are temporarily unavailable.\n\nQuorum consistency testing uses controlled timing to create race conditions between concurrent reads and writes. We configure quorum requirements (W=2, R=2 with N=3) and verify that reads never return stale or partially updated data, even when writes are still in progress.\n\nConflict resolution testing creates intentional conflicts by performing concurrent writes to the same key from different nodes during a network partition. When the partition heals, the system should use vector clocks to deterministically resolve conflicts without losing data.\n\n### Chaos and Failure Testing\n\nChaos and failure testing validates system resilience under adverse conditions that are difficult to predict but inevitable in production environments. These tests intentionally introduce failures and verify that the system continues to operate correctly.\n\n#### Network Partition Testing\n\nNetwork partition testing simulates split-brain scenarios where the cluster becomes divided into multiple independent groups. These tests validate that the system maintains consistency and availability guarantees during partitions and recovers correctly when connectivity is restored.\n\n| Partition Scenario | Cluster Configuration | Partition Layout | Expected Behavior |\n|-------------------|---------------------|------------------|-------------------|\n| Majority/Minority Split | 5-node cluster | 3 nodes vs 2 nodes | Majority side remains available, minority stops serving |\n| Even Split | 4-node cluster | 2 nodes vs 2 nodes | Both sides reject operations requiring quorum |\n| Isolated Node | 5-node cluster | 1 node vs 4 nodes | Isolated node stops serving, majority continues |\n| Progressive Partition | 6-node cluster | Split into 3 groups of 2 | All groups stop serving for quorum operations |\n| Asymmetric Partition | 5-node cluster | Full connectivity loss for one node pair | Complex partition handling with partial connectivity |\n\nMajority/minority split testing validates that the system correctly implements quorum requirements during partitions. When a 5-node cluster splits into groups of 3 and 2 nodes, the majority group should continue serving requests while the minority group should reject operations that require quorum. This prevents split-brain scenarios where both sides accept writes that cannot be reconciled.\n\nEven split testing creates scenarios where neither partition has a clear majority. In these situations, the system should prioritize consistency over availability and reject operations that could lead to irreconcilable conflicts. Clients should receive clear error messages indicating that quorum cannot be achieved.\n\nProgressive partition testing simulates complex network failures where the cluster fractures into multiple small groups. This scenario tests the system's ability to handle cascading failures and maintain correctness even when normal operations become impossible.\n\n#### Cascading Failure Testing\n\nCascading failure testing validates that isolated failures don't trigger widespread system degradation. These tests are critical because distributed systems can exhibit complex failure propagation patterns that are difficult to predict.\n\n| Failure Type | Initial Trigger | Expected Cascade | Containment Mechanism |\n|--------------|----------------|------------------|----------------------|\n| Memory Exhaustion | One node runs out of memory | Other nodes detect failure, rebalance load | Circuit breakers prevent overload propagation |\n| Network Congestion | High latency to one node | Timeouts trigger suspicion, temporary exclusion | Adaptive timeout and backoff mechanisms |\n| Processing Overload | CPU saturation on cache node | Request routing avoids overloaded node | Load-aware request distribution |\n| Storage Failure | Disk corruption on replica | Read repair detects corruption, excludes bad replica | Automated replica replacement |\n| Configuration Error | Wrong replication factor setting | Insufficient replicas for durability | Validation prevents dangerous configurations |\n\nMemory exhaustion testing gradually increases memory pressure on individual nodes until they become unresponsive. The test verifies that other nodes detect the failure quickly and redistribute the load without becoming overloaded themselves. Circuit breakers should prevent failing nodes from receiving additional requests that would worsen their condition.\n\nNetwork congestion testing introduces artificial delays and packet loss to simulate overloaded network conditions. The system should adapt by adjusting timeouts, reducing gossip frequency, and temporarily excluding nodes that appear unresponsive due to network issues. The test verifies that temporary network problems don't cause permanent node exclusions.\n\nProcessing overload testing creates CPU-intensive workloads on individual nodes to simulate performance degradation. The system should detect slow response times and redirect requests to healthier nodes while giving overloaded nodes time to recover.\n\n#### Byzantine Failure Testing\n\nByzantine failure testing validates system behavior when nodes exhibit arbitrary or malicious behavior. While our cache system doesn't implement full Byzantine fault tolerance, it should handle common forms of byzantine behavior gracefully.\n\n| Byzantine Scenario | Node Behavior | System Response | Recovery Mechanism |\n|-------------------|---------------|------------------|-------------------|\n| Corrupted Responses | Node returns garbage data for requests | Other replicas provide correct data, bad node excluded | Read repair corrects corrupted data |\n| Inconsistent Replies | Node gives different answers to same question | Majority consensus overrides inconsistent node | Reputation system marks unreliable nodes |\n| Protocol Violations | Node sends malformed gossip messages | Message validation rejects bad messages | Protocol parsing with strict validation |\n| Resource Hoarding | Node accepts writes but never replicates | Write timeouts detect replication failure | Hinted handoff and alternative replicas |\n| Clock Skew | Node has significantly wrong system time | Vector clocks handle causality despite time skew | Logical time prevents timestamp conflicts |\n\nCorrupted responses testing injects data corruption into individual nodes and verifies that the system detects and corrects the corruption using replicas. The test writes known values, corrupts them on one replica, then verifies that reads still return correct data through read repair mechanisms.\n\nInconsistent replies testing creates nodes that provide different responses to identical requests, simulating flaky hardware or software bugs. The system should use majority consensus among replicas to determine the correct response and gradually exclude nodes that consistently provide inconsistent data.\n\nProtocol violations testing sends malformed messages to test the robustness of message parsing and validation. The system should gracefully handle invalid messages without crashing or corrupting state, while logging security events for investigation.\n\n#### Performance Degradation Testing\n\nPerformance degradation testing validates that the system maintains functionality even when performance characteristics degrade significantly from normal operation. These tests ensure that temporary performance problems don't cause permanent failures.\n\n| Degradation Type | Induced Condition | Performance Impact | Mitigation Strategy |\n|------------------|------------------|-------------------|-------------------|\n| High Latency | Network delays 1-5 seconds | Operations timeout, retries increase | Adaptive timeouts, exponential backoff |\n| Low Throughput | Bandwidth limited to 1KB/s | Gossip and replication slowed | Priority queuing, compression |\n| Memory Pressure | Available RAM reduced to 10% | Aggressive eviction, cache thrashing | Emergency eviction, load shedding |\n| CPU Saturation | Background CPU load at 90% | Response times increase | Request queuing, graceful degradation |\n| Disk I/O Limits | Storage bandwidth capped | Persistence operations delayed | Async writes, batching |\n\nHigh latency testing introduces significant network delays and verifies that the system adapts its timeout values appropriately. Initial timeout values that work well under normal conditions may cause excessive failures when network latency increases. The system should detect increased latency and adjust timeouts dynamically.\n\nLow throughput testing simulates bandwidth-constrained environments and validates that essential operations continue to work despite reduced data transfer rates. Gossip protocols should reduce message frequency, and replication should prioritize critical data over less important metadata.\n\nMemory pressure testing reduces available memory and verifies that the cache system adapts by increasing eviction frequency and potentially reducing cache capacity. The system should maintain basic functionality even when memory pressure prevents optimal performance.\n\n### Common Pitfalls\n\nTesting distributed systems introduces numerous pitfalls that can lead to false confidence or missed bugs. Understanding these pitfalls helps create more effective tests that actually validate system correctness.\n\n⚠️ **Pitfall: Timing-Dependent Test Assumptions**\n\nMany distributed cache tests make implicit assumptions about timing that don't hold under realistic conditions. For example, assuming that gossip messages will propagate within a fixed time window, or that TTL expiration will occur at precisely the specified time. These assumptions lead to flaky tests that pass in development but fail in production.\n\nThe underlying problem is that distributed systems have inherently asynchronous behavior with variable timing. Network delays, garbage collection pauses, and CPU scheduling can all introduce timing variations that break tests with rigid timing assumptions.\n\nTo fix this issue, use eventual consistency testing patterns with retries and reasonable timeout bounds. Instead of asserting that gossip propagation completes in exactly 5 seconds, check periodically for up to 30 seconds and fail only if propagation never completes. This approach tests the eventual outcome while accommodating realistic timing variations.\n\n⚠️ **Pitfall: Insufficient Concurrent Access Testing**\n\nDistributed caches experience high concurrency in production, but many tests only validate single-threaded scenarios. This leads to race conditions and data corruption that only appear under load. The problem is particularly severe for components like LRU caches that maintain complex internal state across multiple data structures.\n\nThe root cause is that concurrency bugs often require specific timing and thread interleaving to manifest. Unit tests that don't stress the system with realistic concurrency patterns miss these critical issues.\n\nTo address this, implement stress testing with hundreds of concurrent goroutines performing random operations. Use Go's race detector to catch data races, and run tests multiple times to increase the probability of triggering race conditions. Also validate that complex operations maintain consistency - for example, verifying that LRU eviction never corrupts the doubly-linked list structure.\n\n⚠️ **Pitfall: Mock-Heavy Integration Tests**\n\nSome integration tests use so many mocks that they don't actually test component integration. While mocking is valuable for unit tests, overuse in integration tests can create false confidence by testing the mocks rather than real component interactions.\n\nThis problem occurs because mocks may not accurately simulate the timing, error conditions, or state changes of real components. Tests pass because the mocks behave predictably, but real components may have different failure modes or performance characteristics.\n\nThe solution is to use real components whenever possible in integration tests, with mocking reserved for external dependencies like network calls or file system operations. When mocks are necessary, ensure they accurately simulate the timing and failure modes of real components based on careful analysis of the actual implementations.\n\n⚠️ **Pitfall: Inadequate Failure Scenario Coverage**\n\nMany test suites focus heavily on happy path scenarios while providing insufficient coverage of failure modes. This is problematic because distributed systems spend significant time handling various types of failures, and correctness under failure is often more critical than optimal performance under normal conditions.\n\nThe underlying issue is that failure scenarios are more complex to set up and validate than success scenarios. It's easier to test that a successful write returns the correct response than to test that a write during a network partition handles replica failures correctly while maintaining consistency guarantees.\n\nTo improve failure coverage, systematically enumerate failure modes for each component and create specific tests for each scenario. Use fault injection techniques to simulate network failures, node crashes, and resource exhaustion. Validate not just that the system continues to function, but that it maintains correctness properties like consistency and durability.\n\n⚠️ **Pitfall: Test Environment Differs from Production**\n\nTest environments often have characteristics that don't match production deployment, leading to bugs that only appear after deployment. Common differences include single-machine testing of distributed components, unrealistic network conditions, and different resource constraints.\n\nThe problem is particularly severe for distributed cache systems because their behavior depends heavily on network latency, node failure patterns, and data distribution characteristics. A test that runs all nodes on localhost with microsecond latencies may not catch issues that appear with millisecond WAN latencies.\n\nTo mitigate this issue, use test environments that simulate production characteristics as closely as possible. Run multi-node tests across separate processes or containers, introduce realistic network delays and packet loss, and test with data distributions that match expected production workloads. Consider using containerized test environments that can simulate various deployment scenarios.\n\n### Implementation Guidance\n\nThe testing strategy requires sophisticated tooling and infrastructure to validate distributed system behavior effectively. The following implementation guidance provides concrete approaches for building comprehensive test suites.\n\n#### Technology Recommendations\n\n| Testing Layer | Simple Option | Advanced Option |\n|---------------|---------------|-----------------|\n| Unit Testing | Go's built-in testing + testify assertions | Ginkgo/Gomega BDD framework |\n| Mocking | Manual interface mocking | GoMock generated mocks |\n| Integration Testing | Docker Compose multi-container | Kubernetes test clusters |\n| Network Simulation | Manual delays with time.Sleep | Comcast network condition simulation |\n| Chaos Testing | Manual failure injection | Chaos Mesh orchestrated failures |\n| Load Testing | Simple goroutine loops | Vegeta load testing tool |\n| Monitoring | Basic log validation | Prometheus metrics + alerting |\n\n#### Recommended Testing Structure\n\n```\nproject-root/\n  internal/\n    hashring/\n      hashring.go\n      hashring_test.go          ← unit tests\n      integration_test.go       ← cross-component integration\n    cache/\n      lru.go\n      lru_test.go\n      benchmark_test.go         ← performance validation\n    cluster/\n      gossip.go\n      gossip_test.go\n      chaos_test.go            ← failure simulation\n  test/\n    fixtures/                   ← test data and configurations\n      cluster-configs/\n      test-keys.json\n    integration/                ← full system integration tests\n      cluster_test.go\n      replication_test.go\n    chaos/                      ← chaos engineering tests\n      partition_test.go\n      cascade_test.go\n    tools/                      ← testing utilities\n      network_simulator.go\n      cluster_manager.go\n  docker-compose.test.yml       ← multi-node test environment\n```\n\n#### Infrastructure Starter Code\n\nComplete network simulation utility for testing distributed behavior:\n\n```go\npackage testtools\n\nimport (\n    \"context\"\n    \"math/rand\"\n    \"sync\"\n    \"time\"\n)\n\n// NetworkSimulator provides controlled network conditions for testing\ntype NetworkSimulator struct {\n    nodes           map[string]*SimulatedNode\n    partitions      map[string][]string\n    globalLatency   time.Duration\n    packetLossRate  float64\n    mutex           sync.RWMutex\n}\n\n// SimulatedNode represents a node in the test network\ntype SimulatedNode struct {\n    NodeID          string\n    Address         string\n    IsHealthy       bool\n    CustomLatency   time.Duration\n    MessageQueue    chan SimulatedMessage\n    DeliveryHandler func(SimulatedMessage) error\n}\n\n// SimulatedMessage represents a message in the test network\ntype SimulatedMessage struct {\n    From        string\n    To          string\n    MessageType string\n    Payload     []byte\n    SentAt      time.Time\n    DeliverAt   time.Time\n}\n\n// NewNetworkSimulator creates a network simulator for testing\nfunc NewNetworkSimulator() *NetworkSimulator {\n    return &NetworkSimulator{\n        nodes:      make(map[string]*SimulatedNode),\n        partitions: make(map[string][]string),\n    }\n}\n\n// AddNode registers a node in the simulated network\nfunc (ns *NetworkSimulator) AddNode(nodeID string, address string, handler func(SimulatedMessage) error) {\n    ns.mutex.Lock()\n    defer ns.mutex.Unlock()\n    \n    node := &SimulatedNode{\n        NodeID:          nodeID,\n        Address:         address,\n        IsHealthy:       true,\n        MessageQueue:    make(chan SimulatedMessage, 1000),\n        DeliveryHandler: handler,\n    }\n    \n    ns.nodes[nodeID] = node\n    \n    // Start message delivery goroutine\n    go ns.processNodeMessages(node)\n}\n\n// SendMessage simulates sending a message with network conditions\nfunc (ns *NetworkSimulator) SendMessage(from, to string, messageType string, payload []byte) error {\n    ns.mutex.RLock()\n    defer ns.mutex.RUnlock()\n    \n    // Check if nodes are partitioned\n    if ns.arePartitioned(from, to) {\n        return errors.New(\"network partition prevents delivery\")\n    }\n    \n    // Simulate packet loss\n    if rand.Float64() < ns.packetLossRate {\n        return nil // Message lost\n    }\n    \n    // Calculate delivery time with latency\n    latency := ns.globalLatency\n    if node, exists := ns.nodes[to]; exists && node.CustomLatency > 0 {\n        latency = node.CustomLatency\n    }\n    \n    message := SimulatedMessage{\n        From:        from,\n        To:          to,\n        MessageType: messageType,\n        Payload:     payload,\n        SentAt:      time.Now(),\n        DeliverAt:   time.Now().Add(latency),\n    }\n    \n    // Queue message for delivery\n    if node, exists := ns.nodes[to]; exists {\n        select {\n        case node.MessageQueue <- message:\n            return nil\n        default:\n            return errors.New(\"message queue full\")\n        }\n    }\n    \n    return errors.New(\"destination node not found\")\n}\n\n// CreatePartition simulates network partition between node groups\nfunc (ns *NetworkSimulator) CreatePartition(group1, group2 []string) {\n    ns.mutex.Lock()\n    defer ns.mutex.Unlock()\n    \n    partitionID := fmt.Sprintf(\"partition_%d\", time.Now().UnixNano())\n    ns.partitions[partitionID] = append(group1, group2...)\n}\n\n// HealPartitions removes all network partitions\nfunc (ns *NetworkSimulator) HealPartitions() {\n    ns.mutex.Lock()\n    defer ns.mutex.Unlock()\n    ns.partitions = make(map[string][]string)\n}\n\n// SetGlobalLatency configures network latency for all connections\nfunc (ns *NetworkSimulator) SetGlobalLatency(latency time.Duration) {\n    ns.mutex.Lock()\n    defer ns.mutex.Unlock()\n    ns.globalLatency = latency\n}\n\n// SetPacketLoss configures packet loss rate (0.0 to 1.0)\nfunc (ns *NetworkSimulator) SetPacketLoss(rate float64) {\n    ns.mutex.Lock()\n    defer ns.mutex.Unlock()\n    ns.packetLossRate = rate\n}\n\n// processNodeMessages delivers queued messages respecting timing\nfunc (ns *NetworkSimulator) processNodeMessages(node *SimulatedNode) {\n    for message := range node.MessageQueue {\n        // Wait for delivery time\n        if delay := time.Until(message.DeliverAt); delay > 0 {\n            time.Sleep(delay)\n        }\n        \n        // Deliver message if node is still healthy\n        ns.mutex.RLock()\n        isHealthy := node.IsHealthy\n        ns.mutex.RUnlock()\n        \n        if isHealthy {\n            node.DeliveryHandler(message)\n        }\n    }\n}\n\n// arePartitioned checks if two nodes are separated by partition\nfunc (ns *NetworkSimulator) arePartitioned(node1, node2 string) bool {\n    for _, partition := range ns.partitions {\n        node1InPartition := false\n        node2InPartition := false\n        \n        for _, node := range partition {\n            if node == node1 {\n                node1InPartition = true\n            }\n            if node == node2 {\n                node2InPartition = true\n            }\n        }\n        \n        // If both nodes are in same partition, they can communicate\n        if node1InPartition && node2InPartition {\n            return false\n        }\n        // If one is in partition and other isn't, they're separated\n        if node1InPartition || node2InPartition {\n            return true\n        }\n    }\n    \n    return false\n}\n```\n\nComplete cluster test manager for integration testing:\n\n```go\npackage testtools\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"testing\"\n    \"time\"\n    \n    \"distributed-cache/internal/cluster\"\n    \"distributed-cache/internal/config\"\n)\n\n// ClusterTestManager manages multi-node clusters for integration testing\ntype ClusterTestManager struct {\n    nodes           []*cluster.Node\n    configs         []*config.NodeConfig\n    networkSim      *NetworkSimulator\n    basePort        int\n    dataDir         string\n    t               *testing.T\n}\n\n// NewClusterTestManager creates a test cluster manager\nfunc NewClusterTestManager(t *testing.T, nodeCount int) *ClusterTestManager {\n    return &ClusterTestManager{\n        nodes:      make([]*cluster.Node, 0, nodeCount),\n        configs:    make([]*config.NodeConfig, 0, nodeCount),\n        networkSim: NewNetworkSimulator(),\n        basePort:   9000,\n        dataDir:    t.TempDir(),\n        t:          t,\n    }\n}\n\n// StartCluster creates and starts a test cluster with specified configuration\nfunc (ctm *ClusterTestManager) StartCluster(nodeCount int, replicationFactor int) error {\n    // Generate node configurations\n    var joinAddresses []string\n    for i := 0; i < nodeCount; i++ {\n        address := fmt.Sprintf(\"127.0.0.1:%d\", ctm.basePort+i)\n        \n        nodeConfig := &config.NodeConfig{\n            NodeID:              fmt.Sprintf(\"node-%d\", i),\n            ListenAddress:       address,\n            AdvertiseAddr:       address,\n            JoinAddresses:       joinAddresses, // Each node joins previous ones\n            MaxMemoryMB:         100,\n            VirtualNodes:        150,\n            ReplicationFactor:   replicationFactor,\n            HealthCheckInterval: 2 * time.Second,\n            GossipInterval:      1 * time.Second,\n            RequestTimeout:      5 * time.Second,\n        }\n        \n        ctm.configs = append(ctm.configs, nodeConfig)\n        joinAddresses = append(joinAddresses, address)\n    }\n    \n    // Start nodes sequentially to allow proper cluster formation\n    for i, config := range ctm.configs {\n        node, err := cluster.NewNode(config)\n        if err != nil {\n            return fmt.Errorf(\"failed to create node %d: %w\", i, err)\n        }\n        \n        ctm.nodes = append(ctm.nodes, node)\n        \n        if err := node.Start(context.Background()); err != nil {\n            return fmt.Errorf(\"failed to start node %d: %w\", i, err)\n        }\n        \n        // Allow time for node to join cluster\n        time.Sleep(500 * time.Millisecond)\n    }\n    \n    // Wait for cluster convergence\n    return ctm.WaitForConvergence(30 * time.Second)\n}\n\n// WaitForConvergence waits until all nodes agree on cluster membership\nfunc (ctm *ClusterTestManager) WaitForConvergence(timeout time.Duration) error {\n    ctx, cancel := context.WithTimeout(context.Background(), timeout)\n    defer cancel()\n    \n    ticker := time.NewTicker(500 * time.Millisecond)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case <-ctx.Done():\n            return fmt.Errorf(\"cluster convergence timeout after %v\", timeout)\n        case <-ticker.C:\n            if ctm.isClusterConverged() {\n                ctm.t.Logf(\"Cluster converged with %d nodes\", len(ctm.nodes))\n                return nil\n            }\n        }\n    }\n}\n\n// isClusterConverged checks if all nodes have consistent membership views\nfunc (ctm *ClusterTestManager) isClusterConverged() bool {\n    if len(ctm.nodes) == 0 {\n        return false\n    }\n    \n    expectedNodeCount := len(ctm.nodes)\n    \n    for i, node := range ctm.nodes {\n        membership := node.GetMembership()\n        if len(membership) != expectedNodeCount {\n            ctm.t.Logf(\"Node %d sees %d nodes, expected %d\", i, len(membership), expectedNodeCount)\n            return false\n        }\n        \n        // Verify all expected nodes are present\n        for j, expectedNode := range ctm.nodes {\n            found := false\n            for _, seenNodeID := range membership {\n                if seenNodeID == expectedNode.NodeID {\n                    found = true\n                    break\n                }\n            }\n            if !found {\n                ctm.t.Logf(\"Node %d doesn't see node %d (%s)\", i, j, expectedNode.NodeID)\n                return false\n            }\n        }\n    }\n    \n    return true\n}\n\n// StopCluster shuts down all nodes in the test cluster\nfunc (ctm *ClusterTestManager) StopCluster() {\n    var wg sync.WaitGroup\n    \n    for i, node := range ctm.nodes {\n        wg.Add(1)\n        go func(index int, n *cluster.Node) {\n            defer wg.Done()\n            \n            ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n            defer cancel()\n            \n            if err := n.Stop(ctx); err != nil {\n                ctm.t.Logf(\"Error stopping node %d: %v\", index, err)\n            }\n        }(i, node)\n    }\n    \n    wg.Wait()\n    ctm.nodes = nil\n    ctm.configs = nil\n}\n\n// GetNode returns a specific node from the cluster\nfunc (ctm *ClusterTestManager) GetNode(index int) *cluster.Node {\n    if index >= 0 && index < len(ctm.nodes) {\n        return ctm.nodes[index]\n    }\n    return nil\n}\n\n// GetRandomNode returns a randomly selected node\nfunc (ctm *ClusterTestManager) GetRandomNode() *cluster.Node {\n    if len(ctm.nodes) == 0 {\n        return nil\n    }\n    return ctm.nodes[rand.Intn(len(ctm.nodes))]\n}\n\n// SimulateNodeFailure marks a node as failed and stops it\nfunc (ctm *ClusterTestManager) SimulateNodeFailure(nodeIndex int) error {\n    if nodeIndex >= len(ctm.nodes) {\n        return fmt.Errorf(\"node index %d out of range\", nodeIndex)\n    }\n    \n    node := ctm.nodes[nodeIndex]\n    ctm.t.Logf(\"Simulating failure of node %s\", node.NodeID)\n    \n    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n    defer cancel()\n    \n    return node.Stop(ctx)\n}\n\n// SimulateNetworkPartition creates partition between specified node groups\nfunc (ctm *ClusterTestManager) SimulateNetworkPartition(group1, group2 []int) error {\n    group1Nodes := make([]string, len(group1))\n    group2Nodes := make([]string, len(group2))\n    \n    for i, nodeIndex := range group1 {\n        if nodeIndex >= len(ctm.nodes) {\n            return fmt.Errorf(\"node index %d out of range\", nodeIndex)\n        }\n        group1Nodes[i] = ctm.nodes[nodeIndex].NodeID\n    }\n    \n    for i, nodeIndex := range group2 {\n        if nodeIndex >= len(ctm.nodes) {\n            return fmt.Errorf(\"node index %d out of range\", nodeIndex)\n        }\n        group2Nodes[i] = ctm.nodes[nodeIndex].NodeID\n    }\n    \n    ctm.networkSim.CreatePartition(group1Nodes, group2Nodes)\n    ctm.t.Logf(\"Created network partition: %v | %v\", group1Nodes, group2Nodes)\n    \n    return nil\n}\n\n// HealNetworkPartitions removes all network partitions\nfunc (ctm *ClusterTestManager) HealNetworkPartitions() {\n    ctm.networkSim.HealPartitions()\n    ctm.t.Log(\"Healed all network partitions\")\n}\n```\n\n#### Core Logic Skeleton Code\n\nHash ring testing skeleton with detailed validation steps:\n\n```go\n// TestHashRingDistribution validates even key distribution across nodes\nfunc TestHashRingDistribution(t *testing.T) {\n    // TODO 1: Create hash ring with 5 nodes and 150 virtual nodes each\n    // TODO 2: Generate 50,000 random keys using crypto/rand for true randomness\n    // TODO 3: Map each key to its assigned node using GetNode()\n    // TODO 4: Count keys per node and calculate distribution statistics\n    // TODO 5: Verify no node has more than 25% of keys (perfect would be 20%)\n    // TODO 6: Calculate coefficient of variation (CV = stddev/mean)\n    // TODO 7: Assert CV < 0.15 for acceptable load balancing\n    // Hint: Use t.Logf() to output distribution statistics for analysis\n}\n\n// TestHashRingRedistribution validates minimal key movement during membership changes\nfunc TestHashRingRedistribution(t *testing.T) {\n    // TODO 1: Create ring with 4 nodes, record initial key assignments for 10,000 keys\n    // TODO 2: Add fifth node to the ring\n    // TODO 3: Recalculate key assignments and count how many keys moved\n    // TODO 4: Verify that approximately 20% of keys moved (optimal redistribution)\n    // TODO 5: Repeat test with node removal, verify similar minimal movement\n    // TODO 6: Test with multiple membership changes, ensure cumulative effect is reasonable\n    // Hint: Store key->node mapping before change, compare after change\n}\n```\n\nLRU cache concurrent testing skeleton:\n\n```go\n// TestLRUCacheConcurrency validates thread safety under high concurrency\nfunc TestLRUCacheConcurrency(t *testing.T) {\n    // TODO 1: Create LRU cache with 1MB capacity\n    // TODO 2: Launch 50 goroutines performing random Get/Set/Delete operations\n    // TODO 3: Each goroutine should perform 1000 operations with random keys\n    // TODO 4: Use sync.WaitGroup to coordinate goroutine completion\n    // TODO 5: Validate cache state consistency after all operations complete\n    // TODO 6: Verify no data races using go test -race\n    // TODO 7: Check that cache size never exceeds configured capacity\n    // Hint: Use shared counter for operation tracking, validate final state\n}\n\n// TestLRUEvictionOrder validates correct LRU eviction behavior\nfunc TestLRUEvictionOrder(t *testing.T) {\n    // TODO 1: Create cache with capacity for exactly 5 entries of 100 bytes each\n    // TODO 2: Fill cache completely with keys A, B, C, D, E\n    // TODO 3: Access keys A, C, E to update their LRU positions\n    // TODO 4: Add new key F, verify that B is evicted (least recently used)\n    // TODO 5: Add new key G, verify that D is evicted\n    // TODO 6: Verify remaining keys A, C, E, F, G are all accessible\n    // Hint: Track access order manually, verify eviction matches expectations\n}\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 Checkpoint: Hash Ring Validation**\n```bash\n# Run hash ring tests\ngo test ./internal/hashring -v -race\n\n# Expected output should show:\n# ✓ Distribution test: CV = 0.12 (acceptable)\n# ✓ Redistribution test: 22% keys moved (optimal)  \n# ✓ Performance test: 0.8μs avg lookup time\n# ✓ All tests pass without race conditions\n\n# Manual verification:\n# Create simple CLI tool to test hash ring manually\ngo run cmd/ring-test/main.go --nodes 5 --keys 1000 --virtual-nodes 100\n```\n\n**Milestone 2 Checkpoint: Cache Node Validation**\n```bash\n# Run cache node tests with race detection\ngo test ./internal/cache -v -race -timeout 30s\n\n# Expected output should show:\n# ✓ LRU eviction maintains correct order under concurrency\n# ✓ Memory accounting accurate within 1% tolerance\n# ✓ TTL expiration within 100ms of expected time\n# ✓ No data races detected\n\n# Manual verification:\n# Start cache node and test via HTTP\ngo run cmd/cache-node/main.go --port 8080 --memory 100MB\ncurl -X PUT localhost:8080/cache/testkey -d \"testvalue\"\ncurl -X GET localhost:8080/cache/testkey  # Should return \"testvalue\"\n```\n\n**Milestone 3 Checkpoint: Cluster Communication Validation**\n```bash\n# Run integration tests with multi-node cluster\ngo test ./test/integration -v -timeout 60s\n\n# Expected output should show:\n# ✓ 3-node cluster converges within 10 seconds\n# ✓ Gossip propagates state changes within 5 intervals\n# ✓ Requests route to correct nodes based on hash ring\n# ✓ Failed nodes detected within 30 seconds\n\n# Manual verification:\n# Start 3-node cluster manually\ngo run cmd/node/main.go --node-id node1 --port 9001 &\ngo run cmd/node/main.go --node-id node2 --port 9002 --join 127.0.0.1:9001 &\ngo run cmd/node/main.go --node-id node3 --port 9003 --join 127.0.0.1:9001 &\n\n# Test cluster membership via API\ncurl localhost:9001/cluster/members  # Should show all 3 nodes\n```\n\n**Milestone 4 Checkpoint: Replication and Consistency Validation**\n```bash\n# Run replication tests with fault injection\ngo test ./test/chaos -v -timeout 120s\n\n# Expected output should show:\n# ✓ Replication factor 3 stores data on 3 nodes\n# ✓ Quorum reads (R=2) never return stale data\n# ✓ Conflict resolution deterministic with vector clocks\n# ✓ Node recovery restores missed writes via hinted handoff\n\n# Manual verification with network partition simulation:\n# Use tc (traffic control) to simulate network partition\n# This requires sudo privileges and Linux environment\nsudo tc qdisc add dev lo root handle 1: prio\nsudo tc filter add dev lo parent 1: protocol ip pref 1 u32 match ip dport 9001 0xffff flowid 1:3\nsudo tc qdisc add dev lo parent 1:3 handle 30: netem loss 100%  # Block node1\n\n# Test that cluster continues operating with 2/3 nodes available\ncurl localhost:9002/cache/testkey -X PUT -d \"partition-test\"\ncurl localhost:9003/cache/testkey  # Should return \"partition-test\"\n\n# Remove partition and verify recovery\nsudo tc qdisc del dev lo root\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Test flakes randomly | Race conditions or timing assumptions | Run with `-race` flag, add logging to timing-sensitive code | Use proper synchronization, avoid fixed timeouts |\n| Hash ring distribution uneven | Poor hash function or insufficient virtual nodes | Log key distribution statistics, visualize ring positions | Increase virtual nodes, verify hash function quality |\n| Cache memory usage inaccurate | Incorrect size calculations or metadata overhead | Add detailed memory accounting logs | Account for all metadata, verify size calculations |\n| Gossip convergence slow | Network delays or too-large gossip payload | Add gossip timing logs, measure message sizes | Tune gossip intervals, compress large payloads |\n| Replication conflicts not resolved | Vector clock bugs or incorrect conflict resolution | Log vector clock states and comparison results | Debug vector clock merge logic, verify timestamp handling |\n| Cluster partitions don't heal | Stale membership information or failed node detection | Add partition detection logs, verify failure detector | Implement partition detection, improve failure detector accuracy |\n\n\n## Debugging Guide\n\n> **Milestone(s):** This section applies to all milestones but is particularly critical for troubleshooting issues that emerge during integration between components. It provides essential debugging techniques for Milestone 1 (Consistent Hash Ring), Milestone 2 (Cache Node Implementation), Milestone 3 (Cluster Communication), and Milestone 4 (Replication & Consistency).\n\n### Mental Model: The Detective's Investigation Kit\n\nThink of debugging a distributed cache like being a detective investigating a complex case with multiple witnesses (nodes) who may have different stories about what happened. Just as a detective needs multiple investigation techniques - interviewing witnesses, examining physical evidence, creating timelines, and testing theories - debugging distributed systems requires a systematic toolkit of diagnostic approaches.\n\nThe key insight is that in distributed systems, no single node has the complete picture. Each node is like a witness who only saw part of the incident. Some witnesses might be lying (Byzantine behavior), some might be confused (clock skew), and some might not have been present when important events occurred (network partitions). Your job as the system detective is to piece together the truth from multiple partial and potentially contradictory sources of evidence.\n\nUnlike debugging single-threaded programs where you can step through line by line, distributed cache debugging is more like forensic investigation - you're analyzing artifacts left behind by asynchronous processes that may have run hours or days ago. The evidence is scattered across log files, metrics dashboards, and persistent state on different machines. Just as crime scene investigators have protocols for collecting and analyzing evidence without contaminating it, distributed systems debugging requires methodical approaches that don't inadvertently change the system state you're trying to observe.\n\n### Common Bug Patterns\n\nUnderstanding the most frequently encountered issues in distributed cache implementations helps you quickly identify root causes when problems arise. These patterns emerge from the inherent complexity of coordinating multiple nodes, managing network communication, and maintaining data consistency across an unreliable infrastructure.\n\n#### Hash Ring Distribution Problems\n\n**Uneven Key Distribution Hotspots**\n\nThe most common hash ring issue manifests as severe load imbalance where one or two nodes receive dramatically more traffic than others. This typically occurs when the hash function produces poor distribution or when insufficient virtual nodes allow natural clustering of hash values around certain ring positions.\n\n| Symptom | Root Cause | Detection Method | Resolution Strategy |\n|---------|------------|------------------|-------------------|\n| One node consistently at 90%+ CPU while others idle | Poor hash function distribution or insufficient virtual nodes | Monitor per-node request rates and memory usage | Increase virtual nodes per physical node, verify hash function quality |\n| New keys consistently hash to same small subset of nodes | Hash function has clustering bias for your key patterns | Analyze hash output distribution across sample keys | Switch hash functions (SHA1 to FNV or vice versa) |\n| Load becomes more uneven as cluster grows | Virtual node count too low relative to cluster size | Track load variance as nodes are added | Use virtual nodes = 150-200 per physical node |\n| Cache hit rates vary wildly between nodes | Popular keys concentrated on specific nodes | Monitor hit rates and key access patterns per node | Implement consistent hashing for client-side routing |\n\n**Incorrect Ring Rebalancing**\n\nWhen nodes join or leave the cluster, keys should only move between the affected node and its immediate neighbors on the hash ring. Incorrect rebalancing implementations often move far more keys than necessary, causing unnecessary cache misses and network traffic.\n\n| Failure Mode | Typical Implementation Bug | Fix Approach |\n|--------------|---------------------------|-------------|\n| All keys move when single node added | Recalculating all positions instead of incremental updates | Only move keys between new node and its ring successor |\n| Keys move to wrong nodes during rebalancing | Incorrect successor calculation or stale ring state | Verify ring ordering and atomic ring updates |\n| Temporary key unavailability during rebalancing | Removing old mapping before establishing new mapping | Use two-phase rebalancing with overlap period |\n| Duplicate keys across multiple nodes | Race condition during concurrent ring updates | Serialize ring modifications with distributed locks |\n\n> **Critical Insight**: The hash ring's primary value is minimizing key movement during membership changes. If you're moving more than 1/N keys when adding the Nth node, your rebalancing logic is incorrect.\n\n#### LRU Cache Implementation Bugs\n\n**Memory Accounting Errors**\n\nAccurate memory accounting is essential for LRU eviction to work correctly. Small accounting errors compound over time and can lead to either premature eviction (wasting cache space) or memory exhaustion (allowing usage to exceed configured limits).\n\n⚠️ **Pitfall: Double-Counting Memory During Updates**\n\nWhen updating an existing cache entry, naive implementations often add the new value size without subtracting the old value size, causing memory usage to grow unboundedly. The correct approach requires atomic size accounting where the delta (new size - old size) is applied as a single operation.\n\n| Memory Accounting Error | Manifestation | Debugging Approach | Correction |\n|------------------------|---------------|-------------------|------------|\n| Size never decreases on entry updates | Memory usage only grows, never shrinks | Log size changes for each operation | Calculate size delta atomically |\n| Eviction triggers too early | Cache stays well below configured limit | Compare tracked usage vs actual entry sizes | Audit size calculation for all data types |\n| Cache grows beyond memory limit | Out of memory errors despite eviction | Monitor actual vs tracked memory usage | Add periodic memory reconciliation |\n| Negative memory usage values | Arithmetic underflow in size tracking | Check for unsigned integer wraparound | Use signed integers and bounds checking |\n\n**TTL Cleanup Performance Issues**\n\nExpired entry cleanup must balance thoroughness with performance impact. Overly aggressive cleanup blocks normal cache operations, while insufficient cleanup allows expired entries to consume memory and skew LRU ordering.\n\nThe lazy expiration approach checks TTL during normal `Get` operations, which works well for frequently accessed keys but allows rarely accessed expired entries to accumulate. Periodic background cleanup addresses this but must be carefully throttled to avoid interfering with cache performance.\n\n| TTL Issue Pattern | Performance Impact | Implementation Fix |\n|------------------|-------------------|------------------|\n| Checking expiration on every cache access | High CPU overhead on Get operations | Batch expiration checks and use efficient time comparisons |\n| Full cache scan for expired entries | Periodic operation stalls affecting all requests | Implement incremental cleanup scanning fixed number of entries per interval |\n| Expired entries remain in LRU ordering | Skewed eviction decisions favoring expired over live data | Remove from LRU immediately on expiration detection |\n| TTL cleanup during high-traffic periods | Cleanup competes with user requests for locks | Schedule cleanup during low-traffic periods or use separate cleanup threads |\n\n#### Network Communication Failures\n\n**Split-Brain Detection and Recovery**\n\nNetwork partitions create the most challenging debugging scenarios because different parts of the cluster have inconsistent views of membership and data. The key to diagnosis is recognizing the symptoms of partition behavior rather than simple node failures.\n\n> **Design Principle**: In a true split-brain scenario, both sides of the partition believe they are the authoritative cluster and the other side has failed. This leads to divergent decisions that must be reconciled when the partition heals.\n\n| Split-Brain Symptom | What It Indicates | Diagnostic Steps | Recovery Action |\n|-------------------|------------------|-----------------|----------------|\n| Conflicting cluster membership from different nodes | Network partition isolating node subsets | Query membership from multiple nodes and compare | Implement partition detection and read-only mode |\n| Same keys with different values on different nodes | Writes continued during partition | Compare key versions across suspected partition groups | Trigger conflict resolution and anti-entropy repair |\n| Nodes report other healthy nodes as failed | Asymmetric network failure | Check bidirectional connectivity between reported failed nodes | Adjust failure detection thresholds or implement external health checks |\n| Client gets different responses from different nodes | Load balancer distributing across partitioned cluster | Test connectivity matrix between all node pairs | Update client routing to avoid partitioned nodes |\n\n**Gossip Protocol Convergence Issues**\n\nThe gossip protocol should ensure all nodes eventually receive membership updates, but various implementation bugs can prevent convergence or cause excessive network traffic.\n\n⚠️ **Pitfall: Gossip Message Size Growth**\n\nA common bug is including the entire cluster state in every gossip message rather than just recent changes. This works fine for small clusters but causes message sizes to grow quadratically with cluster size, eventually overwhelming network capacity or triggering packet fragmentation.\n\n| Convergence Problem | Root Cause | Detection Method | Solution |\n|--------------------|------------|------------------|----------|\n| Some nodes never learn about membership changes | Gossip target selection bias or partition | Compare final membership state across all nodes | Implement random peer selection with partition detection |\n| Excessive gossip network traffic | Including full state instead of deltas | Monitor gossip message sizes and network bandwidth | Send only recent state changes and implement anti-entropy |\n| Slow convergence even without failures | Gossip interval too long or insufficient fan-out | Measure time for membership change propagation | Adjust gossip frequency and increase targets per round |\n| Gossip storms during cluster changes | Every node immediately gossips when receiving updates | Monitor gossip frequency spikes | Implement jitter and backoff for gossip scheduling |\n\n#### Replication Consistency Bugs\n\n**Vector Clock Implementation Errors**\n\nVector clocks track causality relationships between operations but are error-prone to implement correctly. The most common bugs involve incorrect clock advancement, comparison logic, or merging behavior.\n\n| Vector Clock Bug | Incorrect Behavior | Manifestation | Fix |\n|------------------|-------------------|---------------|-----|\n| Not advancing local clock on writes | Cannot detect which operations came first | Conflicts between causally ordered operations | Increment local node's clock entry on every write |\n| Incorrect causality comparison | Wrong conflict resolution decisions | Overwriting newer values with older ones | Implement proper dominance checking (all entries ≤ with at least one <) |\n| Clock merging overwrites instead of taking maximum | Lost causality information | False conflicts for concurrent operations | Merge by taking max of each entry across both clocks |\n| Clock memory leaks from abandoned nodes | Unbounded growth of clock size | Performance degradation over time | Implement clock pruning for nodes absent from recent operations |\n\n**Quorum Consistency Violations**\n\nQuorum-based consistency requires careful coordination between read and write operations to ensure the intersection property (R + W > N). Implementation bugs often violate this property or handle partial failures incorrectly.\n\n> **Critical Insight**: The fundamental guarantee of quorum systems is that every read will see at least one replica that participated in the most recent successful write. Violating this guarantee leads to lost writes or stale reads.\n\n| Consistency Violation | Implementation Bug | Detection Approach | Correction |\n|-----------------------|-------------------|-------------------|------------|\n| Reads return stale values after successful writes | Read quorum doesn't overlap with write quorum | Test R+W > N and verify overlap | Adjust quorum sizes or implement read repair |\n| Writes succeed but data is lost | Write quorum not actually persistent | Verify write durability across quorum replicas | Ensure writes are flushed to disk before acknowledging |\n| Concurrent writes create permanent conflicts | Insufficient conflict detection | Monitor for unresolved conflicts over time | Implement proper vector clock comparison and resolution |\n| Inconsistent conflict resolution | Non-deterministic resolution across nodes | Compare resolution decisions across replicas | Use deterministic resolution (timestamp, node ID) as tiebreaker |\n\n### Diagnostic Techniques\n\nEffective distributed cache debugging requires systematic approaches that can isolate problems across multiple interacting components. The key is building comprehensive observability into your system and then using structured investigation techniques to narrow down root causes.\n\n#### Systematic Root Cause Analysis\n\n**The Distributed Systems Investigation Process**\n\nWhen facing a distributed cache problem, follow this structured approach to avoid jumping to conclusions or missing important evidence:\n\n1. **Establish the Timeline**: Determine when the problem started and correlate with any system changes, deployments, or external events. Distributed systems problems often have delayed manifestations where the root cause occurred minutes or hours before symptoms appeared.\n\n2. **Map the Failure Domain**: Identify which nodes, operations, or key ranges are affected. This helps distinguish between localized issues (single node problems, network connectivity) and systemic issues (algorithm bugs, configuration errors).\n\n3. **Collect Evidence Systematically**: Gather logs, metrics, and state from ALL relevant nodes, not just the ones showing obvious symptoms. The root cause often lies in nodes that appear to be working normally.\n\n4. **Reproduce in Isolation**: Attempt to reproduce the problem in a controlled environment with known inputs. This is particularly important for race conditions and timing-dependent bugs.\n\n5. **Test Hypotheses Incrementally**: Form specific, testable hypotheses about the root cause and design experiments to validate or refute them. Avoid making multiple changes simultaneously.\n\n**Correlation Analysis Techniques**\n\nDistributed cache problems often manifest as correlations between seemingly unrelated metrics or events. Learning to spot these patterns significantly accelerates diagnosis.\n\n| Correlation Pattern | What It Suggests | Investigation Steps | Likely Root Causes |\n|--------------------|------------------|-------------------|-------------------|\n| High CPU on specific nodes correlates with specific key patterns | Hotspot or hash distribution problem | Analyze key distribution and access patterns | Insufficient virtual nodes, biased hash function, or popular key concentration |\n| Memory usage spikes correlate with TTL cleanup timing | TTL cleanup implementation problem | Monitor cleanup duration and memory accounting | Inefficient cleanup algorithm or memory accounting bugs |\n| Network traffic spikes correlate with membership changes | Rebalancing or gossip problem | Analyze message sizes during membership changes | Inefficient rebalancing or gossip storms |\n| Error rates correlate with time of day or traffic patterns | Capacity or race condition problem | Load test under similar conditions | Resource exhaustion or concurrency bugs under load |\n\n#### Distributed State Investigation\n\n**Cross-Node State Comparison**\n\nOne of the most powerful diagnostic techniques is systematically comparing state across nodes to identify inconsistencies that point to root causes.\n\nThe `HashRing` state comparison reveals whether nodes have consistent views of cluster membership and key distribution:\n\n| State Component | Comparison Method | Inconsistency Indicates |\n|----------------|------------------|------------------------|\n| Ring topology (sortedKeys) | Compare hash positions across nodes | Membership disagreement or ring update bugs |\n| Node set (nodes map) | Compare active node lists | Gossip failures or split-brain scenarios |\n| Virtual node mapping | Compare key-to-node assignments | Hash ring implementation bugs or stale state |\n| Ring version numbers | Compare update sequence across nodes | Out-of-order updates or missing membership changes |\n\nThe `LRUCache` state investigation focuses on cache consistency and memory accounting accuracy:\n\n| Cache State Element | Diagnostic Value | Investigation Approach |\n|--------------------|------------------|----------------------|\n| Entry count vs capacity utilization | Detects memory accounting errors | Compare tracked memory usage with actual entry sizes |\n| LRU ordering consistency | Identifies eviction algorithm bugs | Verify most recently accessed entries are at front of order |\n| TTL expiration accuracy | Finds cleanup implementation issues | Check for expired entries still present in cache |\n| Key distribution per node | Reveals hash ring problems | Ensure keys are distributed according to hash ring assignments |\n\n**Cluster-Wide Consistency Audits**\n\nImplementing audit functions that verify system invariants across the entire cluster helps catch subtle bugs before they cause major problems.\n\nCritical invariants to verify periodically:\n\n1. **Hash Ring Consistency**: Every node must have identical ring topology and key-to-node mappings\n2. **Replication Factor Compliance**: Each key must have exactly ReplicationFactor copies across the cluster\n3. **Quorum Availability**: For every key, at least ReadQuorum replicas must be accessible\n4. **Memory Limit Enforcement**: No node should exceed its configured memory limit\n5. **TTL Correctness**: No non-expired entries should be missing, no expired entries should be present\n\n#### Performance Bottleneck Identification\n\n**Distributed Profiling Techniques**\n\nPerformance problems in distributed caches often involve subtle interactions between components that only become apparent under specific load patterns or cluster configurations.\n\n| Performance Symptom | Investigation Approach | Common Root Causes |\n|---------------------|----------------------|-------------------|\n| High latency with low throughput | Profile critical path operations across all nodes | Lock contention, inefficient algorithms, or network serialization |\n| Uneven throughput across nodes | Compare per-node performance metrics | Load balancing issues, hardware differences, or hotspots |\n| Throughput degradation over time | Monitor resource usage trends | Memory leaks, unbounded data structures, or cleanup inefficiency |\n| Intermittent performance spikes | Correlate spikes with system events | Garbage collection, periodic cleanup, or rebalancing operations |\n\n**Critical Path Analysis**\n\nIdentify the most expensive operations in your distributed cache and instrument them thoroughly. The critical paths typically include:\n\n1. **Key Lookup Path**: Hash calculation → Ring lookup → Node resolution → Network routing\n2. **Cache Operation Path**: Request parsing → Local cache access → Response serialization\n3. **Replication Path**: Write operation → Replica identification → Parallel replication → Quorum waiting\n4. **Failure Recovery Path**: Failure detection → Membership update → Ring rebalancing → Data migration\n\nFor each critical path, measure:\n- End-to-end latency distribution (not just averages)\n- Resource utilization during operations\n- Error rates and retry behavior\n- Concurrency levels and lock contention\n\n### Debugging Tools and Instrumentation\n\nEffective distributed cache debugging requires comprehensive instrumentation that provides visibility into both individual node behavior and cluster-wide interactions. The key is designing observability that helps you quickly isolate problems across the complexity of multiple interacting components.\n\n#### Logging Strategy for Distributed Systems\n\n**Structured Logging with Correlation IDs**\n\nEvery operation that spans multiple nodes must be trackable through correlation IDs that connect related log entries across the cluster. This enables you to follow a single cache operation from client request through hash ring lookup, network routing, replication, and response.\n\n| Log Level | Use Case | Example Events | Required Fields |\n|-----------|----------|----------------|----------------|\n| ERROR | Unrecoverable failures requiring immediate attention | Node unreachable, quorum not achievable, data corruption | CorrelationID, NodeID, Timestamp, ErrorCode, Context |\n| WARN | Recoverable issues that may indicate problems | Slow responses, retry attempts, partial failures | CorrelationID, NodeID, Timestamp, Component, Details |\n| INFO | Normal operations with business significance | Cache hits/misses, node joins/leaves, replication events | CorrelationID, NodeID, Timestamp, Operation, Target, Result |\n| DEBUG | Detailed operation traces for troubleshooting | Hash calculations, ring lookups, message routing | CorrelationID, NodeID, Timestamp, Function, Parameters, Result |\n\n**Operation-Specific Logging Patterns**\n\nEach major distributed cache operation requires specific logging to enable effective debugging:\n\nHash Ring Operations:\n- Node addition/removal with before/after ring state\n- Key lookup with hash calculation and node resolution\n- Ring rebalancing with key migration details\n\nCache Operations:\n- Request received with key, operation type, and routing decision\n- Local cache access with hit/miss result and LRU position changes\n- TTL expiration events with cleanup statistics\n\nReplication Operations:\n- Write initiation with target replicas and consistency level\n- Individual replica responses with success/failure and timing\n- Quorum achievement or failure with participating nodes\n\nGossip Protocol:\n- Gossip rounds with target selection and message content\n- Membership state changes with version numbers and propagation\n- Convergence detection with final membership views\n\n#### Metrics and Observability\n\n**Node-Level Metrics**\n\nEach cache node must expose metrics that provide insight into its individual performance and health status.\n\n| Metric Category | Key Measurements | Aggregation Method | Alerting Thresholds |\n|----------------|------------------|-------------------|-------------------|\n| Cache Performance | Hit rate, miss rate, operation latency | Per-node averages with cluster-wide distribution | Hit rate < 70%, p99 latency > 100ms |\n| Memory Management | Used memory, available memory, eviction rate | Current values with trend analysis | Usage > 90%, high eviction rate |\n| Network Activity | Request rate, response time, error rate | Per-node rates with peer-to-peer breakdowns | Error rate > 1%, response time > 50ms |\n| Hash Ring Health | Key distribution variance, rebalancing frequency | Statistical distribution across nodes | Load variance > 2x mean, frequent rebalancing |\n\n**Cluster-Level Metrics**\n\nDistributed cache health requires metrics that capture cluster-wide behavior and consistency properties.\n\n| Cluster Metric | Calculation Method | Health Indicator | Investigation Trigger |\n|----------------|-------------------|-----------------|---------------------|\n| Membership Consistency | Compare node lists across cluster | All nodes report identical membership | Membership disagreement detected |\n| Data Consistency | Sample key comparisons across replicas | Replica values match for sampled keys | Consistency violations found |\n| Load Balance Quality | Standard deviation of per-node load | Load evenly distributed across nodes | High load variance |\n| Availability Percentage | Successful operations / total operations | High success rate for all operation types | Success rate drops below SLA |\n\n#### Debug-Specific Instrumentation\n\n**Circuit Breaker Integration**\n\nThe `HealthChecker` component should integrate with circuit breaker patterns to provide debugging information about node health transitions and failure patterns.\n\n| Circuit State | Logging Requirements | Metric Tracking | Debug Information |\n|--------------|---------------------|-----------------|-------------------|\n| Closed (Healthy) | Successful health checks with response times | Success rate and latency distribution | Baseline performance characteristics |\n| Open (Failed) | Failed health checks with error details | Failure count and failure types | Error patterns and recovery conditions |\n| Half-Open (Testing) | Recovery attempt results | Recovery success rate | Transition timing and success criteria |\n\n**Distributed Tracing for Cross-Node Operations**\n\nImplement distributed tracing that follows cache operations across multiple nodes, providing end-to-end visibility into request processing.\n\nTrace spans should capture:\n1. **Client Request Span**: Initial request parsing and validation\n2. **Hash Ring Lookup Span**: Key hashing and node resolution\n3. **Network Routing Span**: Request forwarding to target node\n4. **Local Cache Access Span**: Cache operation execution\n5. **Replication Spans**: Parallel replica operations (if applicable)\n6. **Response Assembly Span**: Result aggregation and response formation\n\n**State Snapshot Capabilities**\n\nImplement functionality to capture complete system state snapshots for offline analysis and debugging.\n\n| Snapshot Component | Captured Information | Use Case | Storage Format |\n|-------------------|---------------------|----------|----------------|\n| Hash Ring State | Complete ring topology with virtual nodes | Analyzing key distribution problems | JSON with sorted ring positions |\n| Cache Contents | All cache entries with metadata | Investigating data consistency issues | Compressed binary format with entry headers |\n| Cluster Membership | Node states with health and version info | Diagnosing split-brain scenarios | JSON with timestamp and source node |\n| Vector Clock State | All vector clocks for replicated entries | Resolving consistency conflicts | JSON with causality relationships |\n\n#### Automated Anomaly Detection\n\n**Statistical Anomaly Detection**\n\nImplement automated detection of unusual patterns that may indicate bugs or performance issues before they cause visible problems.\n\n| Anomaly Type | Detection Method | Alert Condition | Likely Causes |\n|--------------|------------------|----------------|---------------|\n| Load Distribution Anomaly | Monitor per-node load variance | Variance exceeds 2x historical average | Hash ring problems, hotspots, or failing nodes |\n| Performance Regression | Track operation latency trends | Latency increases beyond statistical thresholds | Resource exhaustion, algorithm inefficiency, or external factors |\n| Consistency Drift | Compare replica states periodically | Increasing divergence between replicas | Network issues, failed repairs, or implementation bugs |\n| Memory Usage Anomaly | Monitor memory growth patterns | Unexpected memory growth or fragmentation | Memory leaks, accounting errors, or inefficient cleanup |\n\n**Proactive Health Monitoring**\n\nThe `FailureDetector` component should implement proactive monitoring that identifies potential issues before they cause operational problems.\n\nHealth monitoring categories:\n1. **Predictive Failure Detection**: Monitor trends that often precede node failures\n2. **Performance Degradation Detection**: Identify gradual performance decline\n3. **Consistency Violation Detection**: Catch replica divergence early\n4. **Resource Exhaustion Prediction**: Alert before resources are completely consumed\n\n### Implementation Guidance\n\nThis implementation guidance provides practical tools and techniques for debugging distributed cache systems, with complete code examples and diagnostic utilities that you can use immediately to identify and resolve common issues.\n\n#### Technology Recommendations\n\n| Debugging Component | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Logging | Go's log package with JSON formatting | Structured logging with zerolog or logrus |\n| Metrics | Prometheus client with basic metrics | Full observability stack with Grafana dashboards |\n| Distributed Tracing | Simple correlation ID propagation | OpenTelemetry with Jaeger or Zipkin |\n| Health Checking | HTTP ping endpoints | Comprehensive health checks with circuit breakers |\n| State Inspection | JSON dump endpoints | Interactive debugging API with query capabilities |\n| Performance Profiling | Built-in pprof endpoints | Continuous profiling with specialized tools |\n\n#### Debugging Infrastructure Code\n\n**Complete Correlation ID Implementation**\n\n```go\npackage debug\n\nimport (\n    \"context\"\n    \"crypto/rand\"\n    \"encoding/hex\"\n    \"log\"\n    \"time\"\n)\n\ntype contextKey string\n\nconst CorrelationIDKey contextKey = \"correlationID\"\n\n// CorrelationID generates a new correlation ID for tracking operations across nodes\nfunc NewCorrelationID() string {\n    bytes := make([]byte, 8)\n    rand.Read(bytes)\n    return hex.EncodeToString(bytes)\n}\n\n// WithCorrelationID adds a correlation ID to the context\nfunc WithCorrelationID(ctx context.Context, id string) context.Context {\n    return context.WithValue(ctx, CorrelationIDKey, id)\n}\n\n// GetCorrelationID extracts the correlation ID from context\nfunc GetCorrelationID(ctx context.Context) string {\n    if id, ok := ctx.Value(CorrelationIDKey).(string); ok {\n        return id\n    }\n    return \"unknown\"\n}\n\n// StructuredLogger provides correlation-aware logging\ntype StructuredLogger struct {\n    nodeID string\n    logger *log.Logger\n}\n\nfunc NewStructuredLogger(nodeID string) *StructuredLogger {\n    return &StructuredLogger{\n        nodeID: nodeID,\n        logger: log.New(os.Stdout, \"\", log.LstdFlags),\n    }\n}\n\nfunc (sl *StructuredLogger) Error(ctx context.Context, component, message string, details map[string]interface{}) {\n    entry := map[string]interface{}{\n        \"level\":         \"ERROR\",\n        \"timestamp\":     time.Now().UTC().Format(time.RFC3339),\n        \"correlationID\": GetCorrelationID(ctx),\n        \"nodeID\":        sl.nodeID,\n        \"component\":     component,\n        \"message\":       message,\n    }\n    for k, v := range details {\n        entry[k] = v\n    }\n    \n    jsonBytes, _ := json.Marshal(entry)\n    sl.logger.Println(string(jsonBytes))\n}\n\nfunc (sl *StructuredLogger) Info(ctx context.Context, component, operation string, details map[string]interface{}) {\n    entry := map[string]interface{}{\n        \"level\":         \"INFO\",\n        \"timestamp\":     time.Now().UTC().Format(time.RFC3339),\n        \"correlationID\": GetCorrelationID(ctx),\n        \"nodeID\":        sl.nodeID,\n        \"component\":     component,\n        \"operation\":     operation,\n    }\n    for k, v := range details {\n        entry[k] = v\n    }\n    \n    jsonBytes, _ := json.Marshal(entry)\n    sl.logger.Println(string(jsonBytes))\n}\n```\n\n**Complete State Snapshot Implementation**\n\n```go\npackage debug\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"time\"\n)\n\n// StateSnapshot captures complete system state for debugging\ntype StateSnapshot struct {\n    NodeID        string                 `json:\"nodeID\"`\n    Timestamp     time.Time              `json:\"timestamp\"`\n    HashRingState *HashRingSnapshot      `json:\"hashRing\"`\n    CacheState    *CacheSnapshot         `json:\"cache\"`\n    ClusterState  *ClusterSnapshot       `json:\"cluster\"`\n    HealthState   *HealthSnapshot        `json:\"health\"`\n}\n\ntype HashRingSnapshot struct {\n    VirtualNodes int               `json:\"virtualNodes\"`\n    ActiveNodes  []string          `json:\"activeNodes\"`\n    RingSize     int               `json:\"ringSize\"`\n    KeySample    map[string]string `json:\"keySample\"` // Sample keys with their assigned nodes\n}\n\ntype CacheSnapshot struct {\n    TotalEntries    int64             `json:\"totalEntries\"`\n    MemoryUsed      int64             `json:\"memoryUsed\"`\n    MemoryLimit     int64             `json:\"memoryLimit\"`\n    HitRate         float64           `json:\"hitRate\"`\n    RecentKeys      []string          `json:\"recentKeys\"`      // Recently accessed keys\n    ExpiredCount    int               `json:\"expiredCount\"`    // Count of expired entries\n    EvictionStats   EvictionSnapshot  `json:\"evictionStats\"`\n}\n\ntype EvictionSnapshot struct {\n    TotalEvictions    int64     `json:\"totalEvictions\"`\n    RecentEvictions   []string  `json:\"recentEvictions\"`\n    EvictionRate      float64   `json:\"evictionRate\"`\n    LastEvictionTime  time.Time `json:\"lastEvictionTime\"`\n}\n\ntype ClusterSnapshot struct {\n    KnownNodes       map[string]NodeState `json:\"knownNodes\"`\n    MembershipVersion uint64              `json:\"membershipVersion\"`\n    GossipStats      GossipSnapshot       `json:\"gossipStats\"`\n}\n\ntype GossipSnapshot struct {\n    MessagesSent     int64     `json:\"messagesSent\"`\n    MessagesReceived int64     `json:\"messagesReceived\"`\n    LastGossipTime   time.Time `json:\"lastGossipTime\"`\n    PeerConnections  int       `json:\"peerConnections\"`\n}\n\ntype HealthSnapshot struct {\n    NodeHealth      map[string]HealthStatus `json:\"nodeHealth\"`\n    LastHealthCheck time.Time               `json:\"lastHealthCheck\"`\n    FailureCount    int                     `json:\"failureCount\"`\n    RecoveryCount   int                     `json:\"recoveryCount\"`\n}\n\n// CaptureSnapshot creates a complete state snapshot for debugging\nfunc CaptureSnapshot(nodeID string, hashRing *HashRing, cache *LRUCache, \n                    clusterState map[string]NodeState, healthChecker *HealthChecker) *StateSnapshot {\n    \n    snapshot := &StateSnapshot{\n        NodeID:    nodeID,\n        Timestamp: time.Now().UTC(),\n    }\n    \n    // Capture hash ring state\n    snapshot.HashRingState = &HashRingSnapshot{\n        VirtualNodes: hashRing.virtualNodes,\n        ActiveNodes:  getActiveNodesList(hashRing),\n        RingSize:     len(hashRing.sortedKeys),\n        KeySample:    captureKeySample(hashRing),\n    }\n    \n    // Capture cache state\n    cache.mutex.RLock()\n    snapshot.CacheState = &CacheSnapshot{\n        TotalEntries: int64(len(cache.items)),\n        MemoryUsed:   cache.used,\n        MemoryLimit:  cache.capacity,\n        HitRate:      calculateHitRate(cache),\n        RecentKeys:   getRecentlyAccessedKeys(cache),\n        ExpiredCount: countExpiredEntries(cache),\n    }\n    cache.mutex.RUnlock()\n    \n    // Capture cluster state\n    snapshot.ClusterState = &ClusterSnapshot{\n        KnownNodes:       clusterState,\n        MembershipVersion: getCurrentMembershipVersion(clusterState),\n    }\n    \n    return snapshot\n}\n\n// TODO: Implement helper functions for state capture\nfunc getActiveNodesList(hashRing *HashRing) []string {\n    // TODO: Extract list of active nodes from hash ring\n    // Return slice of node IDs currently in the ring\n}\n\nfunc captureKeySample(hashRing *HashRing) map[string]string {\n    // TODO: Generate sample of keys and their node assignments\n    // Useful for verifying hash distribution correctness\n}\n\nfunc calculateHitRate(cache *LRUCache) float64 {\n    // TODO: Calculate cache hit rate from metrics\n    // Should track hits and misses over recent time window\n}\n```\n\n**Complete Health Check Debugging**\n\n```go\npackage debug\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\n// DiagnosticHealthChecker provides detailed health check with debugging info\ntype DiagnosticHealthChecker struct {\n    transport *HTTPTransport\n    config    *HealthConfig\n    logger    *StructuredLogger\n}\n\nfunc NewDiagnosticHealthChecker(transport *HTTPTransport, config *HealthConfig, \n                               logger *StructuredLogger) *DiagnosticHealthChecker {\n    return &DiagnosticHealthChecker{\n        transport: transport,\n        config:    config,\n        logger:    logger,\n    }\n}\n\n// DetailedHealthCheck performs comprehensive health assessment\nfunc (dhc *DiagnosticHealthChecker) DetailedHealthCheck(ctx context.Context, \n                                                       nodeID, address string) *DetailedHealthResult {\n    \n    correlationID := GetCorrelationID(ctx)\n    startTime := time.Now()\n    \n    result := &DetailedHealthResult{\n        NodeID:        nodeID,\n        Address:       address,\n        CorrelationID: correlationID,\n        StartTime:     startTime,\n        Checks:        make(map[string]CheckResult),\n    }\n    \n    // TCP connectivity check\n    tcpResult := dhc.checkTCPConnectivity(ctx, address)\n    result.Checks[\"tcp\"] = tcpResult\n    \n    // HTTP endpoint check\n    httpResult := dhc.checkHTTPEndpoint(ctx, address)\n    result.Checks[\"http\"] = httpResult\n    \n    // Cache functionality check\n    cacheResult := dhc.checkCacheFunctionality(ctx, address)\n    result.Checks[\"cache\"] = cacheResult\n    \n    // Ring consistency check\n    ringResult := dhc.checkRingConsistency(ctx, address)\n    result.Checks[\"ring\"] = ringResult\n    \n    // Determine overall health\n    result.OverallHealth = dhc.determineOverallHealth(result.Checks)\n    result.CompletedAt = time.Now()\n    result.TotalDuration = result.CompletedAt.Sub(startTime)\n    \n    // Log detailed results\n    dhc.logger.Info(ctx, \"HealthChecker\", \"detailed_check_completed\", map[string]interface{}{\n        \"nodeID\":        nodeID,\n        \"address\":       address,\n        \"overallHealth\": result.OverallHealth,\n        \"duration\":      result.TotalDuration,\n        \"checkResults\":  result.Checks,\n    })\n    \n    return result\n}\n\ntype DetailedHealthResult struct {\n    NodeID        string                   `json:\"nodeID\"`\n    Address       string                   `json:\"address\"`\n    CorrelationID string                   `json:\"correlationID\"`\n    StartTime     time.Time                `json:\"startTime\"`\n    CompletedAt   time.Time                `json:\"completedAt\"`\n    TotalDuration time.Duration            `json:\"totalDuration\"`\n    Checks        map[string]CheckResult   `json:\"checks\"`\n    OverallHealth HealthStatus            `json:\"overallHealth\"`\n}\n\ntype CheckResult struct {\n    Success      bool          `json:\"success\"`\n    Duration     time.Duration `json:\"duration\"`\n    ErrorMessage string        `json:\"errorMessage,omitempty\"`\n    Details      interface{}   `json:\"details,omitempty\"`\n}\n\nfunc (dhc *DiagnosticHealthChecker) checkTCPConnectivity(ctx context.Context, address string) CheckResult {\n    // TODO: Implement TCP connectivity check with timeout\n    // Should attempt to establish TCP connection to node address\n    // Return success/failure with timing and error details\n}\n\nfunc (dhc *DiagnosticHealthChecker) checkHTTPEndpoint(ctx context.Context, address string) CheckResult {\n    // TODO: Implement HTTP health endpoint check\n    // Should call /health endpoint and verify response\n    // Include response time and status code details\n}\n\nfunc (dhc *DiagnosticHealthChecker) checkCacheFunctionality(ctx context.Context, address string) CheckResult {\n    // TODO: Implement cache operation test\n    // Should perform test SET/GET operations to verify cache works\n    // Include operation timing and correctness verification\n}\n\nfunc (dhc *DiagnosticHealthChecker) checkRingConsistency(ctx context.Context, address string) CheckResult {\n    // TODO: Implement hash ring consistency check\n    // Should query node's ring state and compare with local state\n    // Detect membership disagreements or stale ring information\n}\n```\n\n#### Core Logic Debugging Skeletons\n\n**Hash Ring State Validation**\n\n```go\n// ValidateHashRing performs comprehensive validation of hash ring state\nfunc (hr *HashRing) ValidateHashRing(ctx context.Context, logger *StructuredLogger) []ValidationError {\n    var errors []ValidationError\n    \n    // TODO 1: Verify ring positions are sorted correctly\n    // Check that sortedKeys slice is in ascending order\n    \n    // TODO 2: Validate virtual node distribution\n    // Ensure each physical node has correct number of virtual nodes\n    \n    // TODO 3: Check ring position to node mapping consistency\n    // Verify ring map entries match sortedKeys positions\n    \n    // TODO 4: Validate node availability\n    // Confirm all nodes in ring are marked as active\n    \n    // TODO 5: Test key assignment consistency\n    // Sample keys should always map to same node for given ring state\n    \n    logger.Info(ctx, \"HashRing\", \"validation_completed\", map[string]interface{}{\n        \"errorCount\": len(errors),\n        \"ringSize\":   len(hr.sortedKeys),\n        \"nodeCount\":  len(hr.nodes),\n    })\n    \n    return errors\n}\n\ntype ValidationError struct {\n    Component   string `json:\"component\"`\n    ErrorType   string `json:\"errorType\"`\n    Description string `json:\"description\"`\n    Severity    string `json:\"severity\"`\n}\n```\n\n**Cache State Consistency Audit**\n\n```go\n// AuditCacheConsistency checks LRU cache internal consistency\nfunc (lru *LRUCache) AuditCacheConsistency(ctx context.Context, logger *StructuredLogger) *CacheAuditResult {\n    lru.mutex.RLock()\n    defer lru.mutex.RUnlock()\n    \n    result := &CacheAuditResult{\n        StartTime: time.Now(),\n        Issues:    make([]CacheIssue, 0),\n    }\n    \n    // TODO 1: Verify memory accounting accuracy\n    // Calculate actual memory usage and compare with tracked usage\n    \n    // TODO 2: Check LRU ordering correctness\n    // Ensure linked list ordering matches access patterns\n    \n    // TODO 3: Validate TTL expiration consistency\n    // Find expired entries that should have been cleaned up\n    \n    // TODO 4: Verify map-to-list consistency\n    // Ensure items map entries correspond to list elements\n    \n    // TODO 5: Check capacity enforcement\n    // Verify total usage doesn't exceed configured capacity\n    \n    result.CompletedAt = time.Now()\n    result.Duration = result.CompletedAt.Sub(result.StartTime)\n    \n    logger.Info(ctx, \"LRUCache\", \"consistency_audit_completed\", map[string]interface{}{\n        \"duration\":   result.Duration,\n        \"issueCount\": len(result.Issues),\n        \"entryCount\": len(lru.items),\n        \"memoryUsed\": lru.used,\n    })\n    \n    return result\n}\n\ntype CacheAuditResult struct {\n    StartTime   time.Time    `json:\"startTime\"`\n    CompletedAt time.Time    `json:\"completedAt\"`\n    Duration    time.Duration `json:\"duration\"`\n    Issues      []CacheIssue `json:\"issues\"`\n}\n\ntype CacheIssue struct {\n    Type        string      `json:\"type\"`\n    Severity    string      `json:\"severity\"`\n    Description string      `json:\"description\"`\n    Data        interface{} `json:\"data\"`\n}\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1: Hash Ring Debugging Validation**\n\nAfter implementing your consistent hash ring, run these debugging validations:\n\n```bash\n# Test hash ring state consistency\ngo test ./internal/hashring -run TestRingStateConsistency -v\n\n# Verify key distribution quality\ngo run ./cmd/debug-tools ring-distribution --nodes=5 --keys=10000\n\n# Test ring rebalancing correctness\ngo run ./cmd/debug-tools ring-rebalancing --add-nodes=3 --remove-nodes=1\n```\n\nExpected output: Ring validation should show even key distribution (variance < 20% of mean), and rebalancing should move minimal keys (approximately 1/N when adding Nth node).\n\n**Milestone 2: Cache Node Debugging Validation**\n\nAfter implementing your LRU cache node:\n\n```bash\n# Test cache consistency under load\ngo test ./internal/cache -run TestCacheConsistencyUnderLoad -v\n\n# Verify memory accounting accuracy\ngo run ./cmd/debug-tools cache-audit --duration=60s --operations=1000\n\n# Test TTL cleanup effectiveness\ngo run ./cmd/debug-tools ttl-validation --expire-ratio=0.3 --cleanup-interval=10s\n```\n\nExpected output: Memory accounting should be accurate within 1%, TTL cleanup should remove expired entries within 2x cleanup interval, and LRU ordering should remain consistent under concurrent access.\n\n**Milestone 3: Cluster Communication Debugging**\n\nAfter implementing gossip protocol and node discovery:\n\n```bash\n# Test membership convergence\ngo run ./cmd/debug-tools gossip-convergence --nodes=5 --partition-time=30s\n\n# Verify health check accuracy\ngo test ./internal/health -run TestFailureDetectionAccuracy -v\n\n# Test network partition handling\ngo run ./cmd/debug-tools partition-simulation --groups=2 --duration=60s\n```\n\nExpected output: Membership should converge within 3-5 gossip rounds, health checks should detect failures within 2x check interval, and partitions should be detected without false positives.\n\n**Milestone 4: Replication Debugging Validation**\n\nAfter implementing replication and consistency:\n\n```bash\n# Test consistency under failures\ngo run ./cmd/debug-tools consistency-test --replicas=3 --failures=1 --operations=1000\n\n# Verify conflict resolution correctness\ngo test ./internal/replication -run TestConflictResolution -v\n\n# Test anti-entropy effectiveness\ngo run ./cmd/debug-tools anti-entropy --inconsistency-ratio=0.1 --repair-time=300s\n```\n\nExpected output: Quorum operations should maintain consistency despite single node failures, conflicts should resolve deterministically, and anti-entropy should repair inconsistencies within configured time window.\n\n\n## Future Extensions\n\n> **Milestone(s):** This section builds on the complete distributed cache system from all previous milestones to explore potential enhancements and how the current design accommodates future growth and feature additions.\n\nThe distributed cache system we've designed provides a solid foundation with consistent hashing, LRU eviction, cluster communication, and data replication. However, real-world production systems often require additional capabilities for performance, scalability, and advanced features. This section explores potential enhancements and demonstrates how our modular, peer-to-peer architecture naturally accommodates future extensions without requiring fundamental redesign.\n\nThink of our current distributed cache as a well-designed city infrastructure with roads, utilities, and zoning. Just as a city can expand by adding new neighborhoods, transit systems, and services while leveraging existing infrastructure, our cache can grow with new capabilities that build upon the consistent hash ring, gossip protocol, and replication mechanisms we've established.\n\n### Mental Model: The Evolving City Infrastructure\n\nConsider how a successful city grows over time. Initially, it has basic roads, power, and water systems that serve the core population. As the city expands, it adds express highways for faster transportation (performance optimizations), specialized districts like financial centers (advanced features), and regional connections to neighboring cities (scalability enhancements). Importantly, these additions leverage and extend the existing infrastructure rather than replacing it entirely.\n\nOur distributed cache follows the same pattern. The hash ring provides the \"road system\" for routing requests, gossip protocol serves as the \"communication network\" for coordination, and replication mechanisms act as the \"utility grid\" ensuring reliability. Future extensions build upon these foundations while adding new capabilities.\n\nThe key insight is that we designed our system with extension points from the beginning. The `HashRing` interface can accommodate new hash functions and placement strategies. The `HTTPTransport` abstraction allows swapping in more efficient protocols. The `ReplicationManager` provides hooks for different consistency models. These design choices enable evolutionary growth rather than revolutionary rewrites.\n\n### Performance Optimizations\n\nPerformance optimizations focus on reducing latency and increasing throughput while maintaining the system's correctness guarantees. These enhancements typically involve algorithmic improvements, better data structures, protocol optimizations, and resource management refinements.\n\n#### Connection Pooling and Protocol Optimizations\n\nThe current `HTTPTransport` creates new connections for each request, which introduces significant overhead in high-throughput scenarios. A production-ready system would implement sophisticated connection pooling with health monitoring and load balancing across multiple connections per node.\n\nEnhanced transport layer optimizations include persistent connections with keepalive, request pipelining to send multiple operations over a single connection, and protocol compression to reduce bandwidth usage. More advanced implementations might use HTTP/2 multiplexing or even custom binary protocols optimized for cache operations.\n\nThe connection pool would maintain separate pools for different operation types - quick pools for health checks and gossip messages, and robust pools for data operations. Connection health monitoring would proactively replace failed connections before they impact operations, while circuit breakers would prevent cascading failures when remote nodes become overloaded.\n\n> **Decision: Enhanced Transport Layer**\n> - **Context**: HTTP request/response overhead becomes significant at high throughput, and connection establishment latency impacts operation latency\n> - **Options Considered**: Keep current HTTP transport, upgrade to HTTP/2 with connection pooling, implement custom binary protocol\n> - **Decision**: Implement HTTP/2 with connection pooling as an intermediate step, with hooks for future binary protocol support\n> - **Rationale**: HTTP/2 provides immediate performance benefits while maintaining debugging capabilities and standard tooling support\n> - **Consequences**: Reduces per-operation latency and increases throughput capacity, while maintaining protocol compatibility\n\n| Transport Option | Latency Impact | Throughput Gain | Implementation Complexity | Debugging Support |\n|-----------------|---------------|----------------|--------------------------|------------------|\n| HTTP/1.1 (current) | Baseline | Baseline | Low | Excellent |\n| HTTP/2 + pooling | 30-50% reduction | 200-400% increase | Medium | Good |\n| Custom binary | 50-70% reduction | 300-600% increase | High | Limited |\n\n#### Memory Layout and Cache-Friendly Data Structures\n\nThe current `LRUCache` uses Go's built-in map and doubly-linked list, which can suffer from poor cache locality due to pointer chasing. Performance-optimized versions would use arena allocation to keep related data structures in contiguous memory regions, reducing CPU cache misses during traversal operations.\n\nAdvanced memory management includes object pooling to reduce garbage collection pressure, especially important for cache entries that are frequently allocated and deallocated. The system could implement a slab allocator that pre-allocates memory pools for different entry sizes, similar to kernel memory management.\n\nMemory-mapped files could replace in-memory storage for larger cache entries, providing automatic overflow to disk while maintaining fast access patterns. This hybrid approach allows the cache to exceed RAM capacity while keeping frequently accessed data in memory.\n\nLock-free data structures using compare-and-swap operations could replace mutex-protected maps for certain hot paths, particularly for operations like incrementing statistics or updating cache metadata that don't require complex critical sections.\n\n#### Batch Operations and Request Coalescing\n\nIndividual get/set operations create significant overhead when clients need to access multiple keys. Batch operations allow clients to retrieve or update multiple keys in a single network round-trip, dramatically improving throughput for bulk operations.\n\nRequest coalescing automatically combines multiple concurrent requests for the same key into a single underlying operation, preventing duplicate work when many clients simultaneously request popular data. This is particularly valuable during cache warming or after cache invalidation events.\n\nThe system would buffer outgoing replication requests and send them in batches to reduce network round-trips between nodes. Similarly, gossip messages could accumulate multiple state changes and transmit them together rather than sending individual updates.\n\nPipelined operations allow clients to send multiple requests without waiting for responses, with the server processing them in sequence and returning results in order. This improves bandwidth utilization and reduces the impact of network latency on overall throughput.\n\n#### Smart Caching and Predictive Loading\n\nMachine learning-based access pattern analysis could identify frequently accessed key patterns and proactively load related data. For example, if accessing `user:123:profile` typically leads to accessing `user:123:preferences`, the system could speculatively load the preferences data.\n\nBloom filters could track recently accessed keys at minimal memory cost, allowing nodes to make intelligent decisions about which data to retain during memory pressure or which keys to prefetch during recovery operations.\n\nTime-series analysis of access patterns could identify cyclical usage patterns (daily/weekly cycles) and adjust cache warming strategies accordingly. The system might proactively load data before predicted peak usage periods.\n\nGeographical awareness could optimize data placement based on client location patterns, storing popular regional data closer to the user base. This requires extending the consistent hash ring with location-aware placement policies.\n\n### Advanced Features\n\nAdvanced features extend the system's capabilities beyond basic key-value storage, adding new functionality that leverages the distributed infrastructure we've built. These features often require new data types, operation semantics, or consistency models.\n\n#### Multi-Data Type Support\n\nThe current system stores arbitrary byte arrays, but production caches often need native support for structured data types like counters, sets, lists, and maps with atomic operations on these structures.\n\nCounter data types would support atomic increment/decrement operations across replicas, using techniques like PN-counters (increment/decrement counters) that can merge concurrent updates mathematically. This requires extending the conflict resolution system to understand counter semantics rather than treating them as opaque values.\n\nSet operations would provide atomic add/remove operations with set union for conflict resolution. Lists would support atomic append/prepend operations with position-based conflict resolution. Maps would support atomic field updates with per-field versioning.\n\nEach data type requires specialized replication logic that understands the semantic meaning of operations rather than treating all updates as simple value replacements. This extends the `VectorClock` system with operation-specific merge functions.\n\n| Data Type | Atomic Operations | Conflict Resolution Strategy | Storage Format |\n|-----------|------------------|------------------------------|---------------|\n| Counter | Increment, Decrement, Reset | PN-counter merge | Integer + operation log |\n| Set | Add, Remove, Contains | Union of additions | Bloom filter + tombstones |\n| List | Append, Prepend, Insert | Position-based ordering | Segmented array |\n| Map | Set field, Delete field | Per-field vector clocks | Nested hash map |\n\n#### Cache Hierarchies and Multi-Level Storage\n\nProduction systems often implement cache hierarchies with multiple tiers of storage - L1 (in-memory), L2 (SSD), and L3 (distributed network storage). Our hash ring can be extended to support multiple storage tiers with automatic promotion and demotion based on access patterns.\n\nHot data promotion would automatically move frequently accessed entries from slower tiers to faster tiers, while cold data demotion would move rarely accessed data to cheaper storage. The system tracks access frequency and recency to make intelligent tiering decisions.\n\nWrite-through and write-behind policies would provide different consistency/performance trade-offs. Write-through ensures data reaches persistent storage before acknowledging the write, while write-behind batches writes for better performance at the cost of potential data loss during failures.\n\nCross-datacenter replication could extend the replication system to maintain copies across geographical regions, with different consistency levels for local versus remote replicas. This requires careful handling of network partitions and split-brain scenarios across wide-area networks.\n\n#### Event Streaming and Change Notifications\n\nClient applications often need real-time notifications when cached data changes. An event streaming system would allow clients to subscribe to key patterns and receive notifications when matching keys are modified.\n\nChange data capture would maintain a log of all cache modifications with timestamps and change types (insert/update/delete). Clients could replay this log to maintain synchronized local caches or trigger business logic based on data changes.\n\nEvent filtering would allow clients to subscribe to specific types of changes or key patterns using regular expressions or prefix matching. The system would efficiently route events only to interested subscribers without flooding all clients.\n\nGuaranteed delivery would ensure critical events reach their destinations even if clients are temporarily disconnected. This requires persistent event queues with retry logic and dead letter handling for events that cannot be delivered.\n\n#### Advanced Consistency Models\n\nThe current system implements tunable consistency through read/write quorums, but production systems often need more sophisticated consistency models for different use cases.\n\nCausal consistency would ensure that operations that are causally related (one operation influenced another) are seen in the same order by all nodes, while allowing concurrent operations to be observed in different orders. This requires extending vector clocks to track causal relationships across client sessions.\n\nRead-your-writes consistency would guarantee that clients always see their own writes, even if they connect to different nodes. This requires session stickiness or read-through mechanisms that ensure client reads always include their own previous writes.\n\nMonotonic read consistency would ensure that clients never see older versions of data after seeing newer versions, preventing the confusing experience of data appearing to \"go backwards\" during network partitions or node failures.\n\nTimeline consistency would provide ordering guarantees within logical time windows while allowing relaxed consistency across window boundaries. This is useful for applications that need strong consistency for related operations but can tolerate eventual consistency for independent operations.\n\n#### Security and Authentication\n\nProduction cache systems require comprehensive security controls including client authentication, data encryption, and access control policies. The gossip protocol and replication mechanisms would need encryption to prevent eavesdropping and tampering.\n\nRole-based access control would allow different clients to have different permissions - some might only read certain key prefixes, while others can modify any data. This requires extending the protocol to include authentication tokens and authorization checks.\n\nData encryption at rest would protect stored cache entries using configurable encryption algorithms. Key management becomes critical, with options for hardware security modules (HSMs) or key management services for enterprise deployments.\n\nNetwork security would include TLS for all inter-node communication, certificate-based node authentication to prevent rogue nodes from joining clusters, and audit logging for all security-relevant operations.\n\n### Scalability Enhancements\n\nScalability enhancements focus on supporting larger clusters, higher throughput, and more efficient resource utilization as the system grows. These modifications often require fundamental changes to algorithms and data structures to maintain performance at scale.\n\n#### Hierarchical Hash Rings and Multi-Level Routing\n\nAs clusters grow beyond hundreds of nodes, the flat hash ring model begins to show scalability limitations. Maintaining consistent view of all nodes becomes expensive, and gossip convergence time grows with cluster size. Hierarchical approaches organize nodes into regions or groups with separate hash rings at each level.\n\nRegional hash rings would partition nodes geographically or logically, with a top-level ring that routes between regions and regional rings that handle intra-region routing. This reduces gossip overhead since nodes primarily exchange information within their region, while region coordinators handle inter-region communication.\n\nConsistent hashing trees could replace the simple ring structure with a tree where each level provides increasingly fine-grained routing decisions. This reduces the number of nodes that need to be considered for each routing decision while maintaining the rebalancing properties of consistent hashing.\n\nZone-aware replication would ensure replicas are placed across different failure domains (racks, datacenters, regions) to improve fault tolerance. The hash ring placement algorithm would consider node location metadata when selecting replica positions.\n\n> **Decision: Hierarchical Cluster Organization**\n> - **Context**: Single hash ring becomes inefficient for clusters exceeding 1000 nodes due to gossip convergence time and membership management overhead\n> - **Options Considered**: Maintain flat ring with optimized gossip, implement regional rings with coordination layer, use distributed hash trees\n> - **Decision**: Implement regional rings with gossip-based coordination between regions\n> - **Rationale**: Provides linear scalability while preserving existing gossip protocol investments and maintaining operational simplicity\n> - **Consequences**: Enables clusters of 10,000+ nodes while requiring careful region boundary management and cross-region consistency handling\n\n| Scaling Approach | Max Cluster Size | Complexity | Rebalancing Overhead | Cross-Region Latency |\n|------------------|------------------|------------|---------------------|---------------------|\n| Flat ring (current) | ~500 nodes | Low | O(N) gossip | N/A |\n| Regional rings | ~10,000 nodes | Medium | O(log N) regions | 1 extra hop |\n| Hash trees | ~50,000 nodes | High | O(log N) levels | Variable |\n\n#### Adaptive Load Balancing\n\nThe current consistent hashing with virtual nodes provides good load distribution, but it cannot adapt to non-uniform key popularity or node heterogeneity. Adaptive load balancing monitors actual load patterns and adjusts the hash ring to better balance work across nodes.\n\nLoad-aware virtual node placement would dynamically add or remove virtual nodes based on actual traffic patterns. Overloaded nodes would receive fewer virtual nodes in future ring configurations, while underutilized nodes would receive more virtual nodes to improve load distribution.\n\nWeighted consistent hashing would assign different numbers of virtual nodes based on node capacity - more powerful nodes would handle proportionally more keys. This requires extending the `NodeConfig` to include capacity metadata and modifying ring construction algorithms.\n\nHot key detection would identify keys that receive disproportionate traffic and potentially replicate them to multiple nodes or cache them at intermediate routing layers. This prevents individual hot keys from overwhelming single nodes.\n\nLoad shedding mechanisms would temporarily reject requests when nodes become overloaded, using techniques like probabilistic rejection or priority-based queuing to maintain service quality for important operations while degrading gracefully under extreme load.\n\n#### Automated Operations and Self-Healing\n\nLarge-scale systems require automated operations to manage complexity and reduce operational overhead. Self-healing capabilities would automatically detect and correct many common problems without human intervention.\n\nAutomated failure recovery would detect node failures and automatically trigger data recovery processes without waiting for human operators. This includes promoting replicas, rebalancing data, and updating cluster membership in a coordinated fashion.\n\nCapacity management would monitor resource utilization trends and automatically trigger cluster expansion when approaching capacity limits. This might involve automatically provisioning new nodes in cloud environments or alerting operators when manual intervention is needed.\n\nPerformance anomaly detection would identify nodes or keys that are performing significantly differently from baseline patterns. This could indicate hardware problems, configuration issues, or attack patterns that require attention.\n\nSelf-tuning parameters would automatically adjust configuration values like TTL defaults, replication factors, and timeout values based on observed performance characteristics and failure patterns. This reduces the operational burden of manual tuning while maintaining optimal performance.\n\n#### Elastic Scaling and Cloud Integration\n\nModern deployments often require elastic scaling capabilities that can quickly add or remove capacity based on demand patterns. This requires extending our architecture to work well with container orchestration systems and cloud auto-scaling groups.\n\nRapid node onboarding would minimize the time required for new nodes to become fully functional cluster members. This includes optimized bootstrap processes, parallel data streaming, and incremental integration rather than bulk data transfers.\n\nGraceful node removal would coordinate the shutdown process to ensure data availability is maintained throughout the process. This requires careful ordering of operations - stop accepting new data, transfer existing data to other nodes, update hash ring, then shutdown.\n\nContainer-aware deployment would optimize for containerized environments with features like health check endpoints for orchestration systems, graceful handling of container restarts, and configuration through environment variables rather than files.\n\nAuto-scaling integration would provide APIs and metrics that external auto-scaling systems can use to make intelligent scaling decisions. This includes predictive metrics that indicate when scaling events should be triggered before problems occur.\n\n### Resource Efficiency and Green Computing\n\nAs distributed systems scale, resource efficiency becomes increasingly important both for cost management and environmental responsibility. These optimizations focus on reducing power consumption, memory usage, and network bandwidth while maintaining performance.\n\n#### Energy-Aware Scheduling\n\nPower-efficient node selection would consider energy consumption when choosing which nodes to route requests to. During low-traffic periods, the system could concentrate load on fewer nodes and allow others to enter power-saving modes.\n\nDynamic voltage and frequency scaling integration would coordinate with hardware power management to adjust CPU performance based on current load. Cache nodes could signal their current performance requirements to the operating system for optimal power/performance trade-offs.\n\nTemperature-aware load balancing would consider datacenter cooling costs when distributing load. During hot weather or cooling system maintenance, the system could shift load away from nodes in thermally constrained areas.\n\n#### Memory and Storage Optimization\n\nCompressed storage would automatically compress cache entries using fast algorithms optimized for cache workloads. The system would balance compression ratio against CPU overhead, potentially using different compression levels for different data types or access patterns.\n\nDeduplication could identify and eliminate duplicate cache entries, particularly valuable for applications that store similar data with different keys. This requires careful handling to ensure that deduplication doesn't violate consistency guarantees.\n\nMemory-mapped storage could provide automatic overflow to disk for infrequently accessed data while keeping hot data in RAM. This hybrid approach allows cache sizes to exceed physical memory while maintaining performance for active data.\n\nSmart prefetching based on access patterns could reduce storage I/O by predicting which data will be needed and loading it proactively. This is particularly valuable for cache warming after restarts or during recovery operations.\n\n### Implementation Guidance\n\nThe future extensions described above build upon the architectural foundations we've established throughout the previous milestones. This section provides concrete guidance for implementing these enhancements while maintaining system stability and correctness.\n\n#### Technology Recommendations\n\n| Enhancement Category | Simple Option | Advanced Option |\n|---------------------|---------------|-----------------|\n| Transport Protocol | HTTP/2 with connection pooling | gRPC with streaming |\n| Data Types | JSON encoding with type hints | Protocol Buffers with schema evolution |\n| Storage Tiers | File-based overflow storage | RocksDB or similar LSM-tree |\n| Event Streaming | HTTP Server-Sent Events | Apache Kafka or NATS |\n| Security | TLS + token authentication | mTLS + RBAC with JWT |\n| Monitoring | Prometheus metrics | OpenTelemetry with tracing |\n\n#### Recommended Extension Architecture\n\n```\nproject-root/\n  cmd/server/main.go                    ← enhanced with plugin loading\n  internal/\n    extensions/                         ← extension interface definitions\n      transport/                        ← pluggable transport layer\n        http2_transport.go\n        grpc_transport.go\n      datatypes/                        ← advanced data type support\n        counter.go\n        set.go\n        list.go\n      storage/                          ← multi-tier storage\n        memory_tier.go\n        disk_tier.go\n        tiering_manager.go\n      security/                         ← authentication and authorization\n        auth_manager.go\n        rbac.go\n        encryption.go\n    optimization/                       ← performance enhancements\n      connection_pool.go\n      batch_processor.go\n      load_balancer.go\n    scaling/                           ← scalability enhancements\n      hierarchical_ring.go\n      regional_gossip.go\n      auto_scaler.go\n```\n\n#### Extension Interface Design\n\nThe key to successful extensibility is designing clean interfaces that new features can implement without modifying core system components. Here are the essential extension points:\n\n```go\n// TransportLayer interface allows plugging in different communication protocols\ntype TransportLayer interface {\n    SendMessage(ctx context.Context, address string, message interface{}) ([]byte, error)\n    HealthCheck(ctx context.Context, address string) error\n    StartServer(ctx context.Context, handler RequestHandler) error\n    Shutdown(ctx context.Context) error\n}\n\n// DataType interface enables support for structured data with semantic operations  \ntype DataType interface {\n    Type() string\n    Serialize() ([]byte, error)\n    Deserialize(data []byte) error\n    ApplyOperation(op Operation) (DataType, error)\n    MergeConflicts(others []DataType) (DataType, error)\n}\n\n// StorageTier interface allows implementing multi-level storage hierarchies\ntype StorageTier interface {\n    Get(key string) (*CacheEntry, error)\n    Set(key string, entry *CacheEntry) error\n    Delete(key string) error\n    EvictToLowerTier(keys []string, lowerTier StorageTier) error\n    PromoteFromLowerTier(keys []string, lowerTier StorageTier) error\n    Usage() StorageUsage\n}\n```\n\n#### Core Extension Infrastructure\n\nThis infrastructure starter code provides the foundation for loading and managing extensions:\n\n```go\npackage extensions\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"plugin\"\n    \"sync\"\n)\n\n// ExtensionManager coordinates loading and lifecycle of system extensions\ntype ExtensionManager struct {\n    transports    map[string]TransportLayer\n    dataTypes     map[string]DataTypeFactory\n    storageTiers  map[string]StorageTierFactory\n    securityMgrs  map[string]SecurityManager\n    mutex         sync.RWMutex\n}\n\n// NewExtensionManager creates extension manager with built-in implementations\nfunc NewExtensionManager() *ExtensionManager {\n    return &ExtensionManager{\n        transports:   make(map[string]TransportLayer),\n        dataTypes:    make(map[string]DataTypeFactory),\n        storageTiers: make(map[string]StorageTierFactory),\n        securityMgrs: make(map[string]SecurityManager),\n    }\n}\n\n// LoadExtension dynamically loads extension from plugin file\nfunc (em *ExtensionManager) LoadExtension(pluginPath string, extensionType string) error {\n    // TODO 1: Load plugin using plugin.Open(pluginPath)\n    // TODO 2: Look up extension factory function by type\n    // TODO 3: Call factory function to create extension instance\n    // TODO 4: Register extension in appropriate registry map\n    // TODO 5: Initialize extension with current system configuration\n}\n\n// GetTransport returns transport implementation by name\nfunc (em *ExtensionManager) GetTransport(name string) (TransportLayer, error) {\n    // TODO 1: Acquire read lock for thread safety\n    // TODO 2: Look up transport in registry map\n    // TODO 3: Return transport or error if not found\n}\n```\n\n#### Performance Optimization Skeleton\n\n```go\npackage optimization\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// ConnectionPool manages reusable connections to remote nodes\ntype ConnectionPool struct {\n    pools     map[string]*nodePool\n    config    *PoolConfig\n    metrics   *PoolMetrics\n    mutex     sync.RWMutex\n}\n\n// PoolConfig contains connection pool tuning parameters\ntype PoolConfig struct {\n    MaxConnectionsPerNode int\n    MaxIdleTime          time.Duration\n    HealthCheckInterval  time.Duration\n    ConnectTimeout       time.Duration\n}\n\n// NewConnectionPool creates optimized connection pool for inter-node communication\nfunc NewConnectionPool(config *PoolConfig) *ConnectionPool {\n    return &ConnectionPool{\n        pools:   make(map[string]*nodePool),\n        config:  config,\n        metrics: &PoolMetrics{},\n    }\n}\n\n// GetConnection returns healthy connection to specified node\nfunc (cp *ConnectionPool) GetConnection(nodeAddress string) (Connection, error) {\n    // TODO 1: Get or create pool for target node\n    // TODO 2: Try to get healthy connection from pool\n    // TODO 3: If no healthy connections available, create new connection\n    // TODO 4: Test connection health before returning\n    // TODO 5: Update pool metrics and connection tracking\n}\n\n// BatchProcessor aggregates operations for efficient processing\ntype BatchProcessor struct {\n    batchSize     int\n    flushInterval time.Duration\n    batches       map[string]*operationBatch\n    processor     func([]Operation) error\n    mutex         sync.Mutex\n    stopCh        chan struct{}\n}\n\n// AddOperation adds operation to appropriate batch for processing\nfunc (bp *BatchProcessor) AddOperation(nodeID string, op Operation) error {\n    // TODO 1: Get or create batch for target node\n    // TODO 2: Add operation to batch\n    // TODO 3: Check if batch is ready for processing (size or time)\n    // TODO 4: If ready, submit batch to processor asynchronously\n    // TODO 5: Reset batch for future operations\n}\n```\n\n#### Scalability Enhancement Framework\n\n```go\npackage scaling\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// RegionalRing extends hash ring with hierarchical organization\ntype RegionalRing struct {\n    region       string\n    localRing    *HashRing\n    regionRings  map[string]*HashRing\n    coordinator  *RegionCoordinator\n    config       *RegionalConfig\n}\n\n// RegionalConfig contains parameters for hierarchical organization\ntype RegionalConfig struct {\n    Region              string\n    CoordinatorNodes    []string\n    CrossRegionLatency  time.Duration\n    RegionHealthTimeout time.Duration\n    ReplicationAcrossRegions bool\n}\n\n// NewRegionalRing creates hierarchical hash ring for large-scale deployments\nfunc NewRegionalRing(region string, config *RegionalConfig) *RegionalRing {\n    return &RegionalRing{\n        region:      region,\n        localRing:   NewHashRing(config.VirtualNodes),\n        regionRings: make(map[string]*HashRing),\n        config:      config,\n    }\n}\n\n// RouteOperation determines target node for operation, potentially cross-region\nfunc (rr *RegionalRing) RouteOperation(key string) (nodeID string, region string, err error) {\n    // TODO 1: Hash key to determine target region\n    // TODO 2: If target region is local, use local ring for node selection\n    // TODO 3: If target region is remote, select appropriate region coordinator\n    // TODO 4: Return node ID, region, and any routing errors\n    // TODO 5: Update routing metrics for monitoring\n}\n\n// AutoScaler monitors system metrics and triggers scaling operations\ntype AutoScaler struct {\n    metricsCollector *MetricsCollector\n    scalingPolicy    *ScalingPolicy\n    cloudProvider    CloudProvider\n    clusterManager   *ClusterManager\n    scaleUpCooldown  time.Duration\n    scaleDownCooldown time.Duration\n}\n\n// ScalingPolicy defines rules for when to scale up or down\ntype ScalingPolicy struct {\n    TargetCPUPercent    float64\n    TargetMemoryPercent float64\n    MinNodes           int\n    MaxNodes           int\n    ScaleUpThreshold   float64\n    ScaleDownThreshold float64\n}\n\n// MonitorAndScale continuously monitors metrics and triggers scaling when needed\nfunc (as *AutoScaler) MonitorAndScale(ctx context.Context) error {\n    // TODO 1: Collect current cluster metrics (CPU, memory, request rate)\n    // TODO 2: Evaluate metrics against scaling policy thresholds\n    // TODO 3: If scale up needed and not in cooldown, provision new nodes\n    // TODO 4: If scale down possible and not in cooldown, remove excess nodes\n    // TODO 5: Update cooldown timers and scaling history\n}\n```\n\n#### Extension Point Integration\n\nThe key to successful extension integration is providing clean hooks where new functionality can integrate with existing system components:\n\n```go\n// ExtensionHooks defines points where extensions can integrate with core system\ntype ExtensionHooks struct {\n    PreOperation  []func(context.Context, Operation) error\n    PostOperation []func(context.Context, Operation, error)\n    PreReplication []func(context.Context, ReplicationRequest) error\n    PostReplication []func(context.Context, ReplicationRequest, error)\n    NodeJoin      []func(context.Context, string) error\n    NodeLeave     []func(context.Context, string) error\n}\n\n// RegisterHook adds extension callback at specified integration point\nfunc (eh *ExtensionHooks) RegisterHook(hookType string, callback interface{}) error {\n    // TODO 1: Validate callback function signature matches hook requirements\n    // TODO 2: Add callback to appropriate hook slice\n    // TODO 3: Sort hooks by priority if priority system is implemented\n}\n\n// ExecuteHooks runs all registered callbacks for specified hook point\nfunc (eh *ExtensionHooks) ExecuteHooks(ctx context.Context, hookType string, args ...interface{}) error {\n    // TODO 1: Get hooks for specified type\n    // TODO 2: Execute each hook in order, passing provided arguments\n    // TODO 3: Handle hook errors according to error policy (fail-fast vs continue)\n    // TODO 4: Aggregate and return any errors encountered\n}\n```\n\n#### Milestone Checkpoints for Extensions\n\nAfter implementing each category of extensions, validate the integration:\n\n**Performance Optimization Checkpoint:**\n- Measure baseline performance with current HTTP transport\n- Implement connection pooling and measure throughput improvement\n- Add batch processing and verify reduced network round-trips\n- Expected: 2-5x throughput increase with 30-50% latency reduction\n\n**Advanced Features Checkpoint:**\n- Implement counter data type with increment operations\n- Test concurrent increments with conflict resolution\n- Verify counter values converge correctly across replicas\n- Expected: Atomic counter operations work correctly under concurrent load\n\n**Scalability Enhancement Checkpoint:**\n- Deploy hierarchical rings with multiple regions\n- Test cross-region operations and replication\n- Verify graceful handling of region failures\n- Expected: System scales beyond 1000 nodes while maintaining sub-100ms latency\n\n#### Language-Specific Implementation Hints\n\n**Go-Specific Optimizations:**\n- Use `sync.Pool` for object pooling to reduce GC pressure\n- Implement custom memory allocators using `unsafe` package for critical paths\n- Use `go:linkname` to access internal runtime functions for advanced optimizations\n- Consider `cgo` integration for high-performance compression libraries\n\n**Connection Management:**\n- Use `net/http.Transport` with custom `DialContext` for connection pooling\n- Implement connection health checks with `net.Conn.SetDeadline`\n- Use `context.WithTimeout` consistently for all network operations\n- Monitor connection metrics with `http.Transport.RegisterProtocol`\n\n**Memory Efficiency:**\n- Use memory-mapped files with `golang.org/x/exp/mmap` for large cache entries\n- Implement arena allocation patterns to reduce pointer chasing\n- Use `sync.RWMutex` judiciously - prefer lock-free algorithms where possible\n- Profile with `go tool pprof` to identify memory hotspots\n\n#### Common Extension Pitfalls\n\n⚠️ **Pitfall: Breaking Core Functionality**\nExtensions often inadvertently break core system guarantees by modifying shared state without proper coordination. Always use the provided extension hooks rather than directly modifying core components. Test extensions in isolation before integration.\n\n⚠️ **Pitfall: Performance Regression**\nPerformance optimizations can actually reduce performance if they introduce excessive overhead for small workloads. Always benchmark extensions against realistic workloads and provide configuration options to disable optimizations when they're not beneficial.\n\n⚠️ **Pitfall: Inconsistent Extension State**\nExtensions that maintain their own state must handle node failures and cluster membership changes correctly. Use the same replication and consistency mechanisms as core components, or ensure extension state can be reconstructed from authoritative sources.\n\n⚠️ **Pitfall: Resource Leaks**\nExtensions that create resources (connections, goroutines, file handles) must implement proper cleanup in their shutdown methods. Register cleanup functions with the extension manager and ensure they're called during system shutdown.\n\n⚠️ **Pitfall: Configuration Complexity**\nAdvanced features often introduce many configuration options, making the system difficult to operate. Provide sensible defaults, validate configuration combinations, and implement configuration templates for common deployment patterns.\n\nThe extension architecture we've designed enables the distributed cache to evolve and grow while maintaining the reliability and consistency guarantees established in the core milestones. Each extension builds upon the solid foundation of consistent hashing, replication, and cluster communication, demonstrating the value of designing for extensibility from the beginning.\n\n\n## Glossary\n\n> **Milestone(s):** This section provides terminology and definitions supporting all milestones, serving as a comprehensive reference for technical terms, data structures, and concepts used throughout the distributed cache implementation.\n\nThis glossary defines all technical terms, data structures, interfaces, and domain-specific vocabulary used throughout this distributed cache design document. Think of this as your technical dictionary - a reference you can return to whenever you encounter an unfamiliar term or need to clarify the exact meaning of a concept in the context of our distributed cache system.\n\nThe terms are organized into logical categories to help you understand not just individual definitions, but also how concepts relate to each other within the broader system architecture. Each definition includes not only what the term means, but also why it matters in the context of building a distributed cache and how it connects to other concepts in the system.\n\n### Core Architecture Terms\n\nThe fundamental concepts that define how our distributed cache system is structured and operates.\n\n| Term | Definition |\n|------|------------|\n| **consistent hashing** | A hash ring algorithm that minimizes key redistribution when cluster membership changes. Unlike simple modulo hashing, consistent hashing ensures that when a node joins or leaves, only a small fraction of keys need to be moved to different nodes, typically 1/N where N is the number of nodes. |\n| **hash ring** | A circular key space where both cache keys and cluster nodes are positioned using hash values. The ring topology allows us to determine key ownership by finding the first node clockwise from a key's hash position. |\n| **virtual nodes** | Multiple hash ring positions assigned to each physical node to achieve better load distribution. Each physical node appears at many positions on the ring, reducing the likelihood that one node receives disproportionate traffic from popular key ranges. |\n| **peer-to-peer cluster design** | A distributed architecture where all nodes have equal responsibility and no single point of failure exists. Every node can handle client requests, participate in gossip communication, and store cache data without requiring dedicated coordinator nodes. |\n| **symmetric responsibility** | A design principle where no nodes have special coordinator roles or elevated privileges. This eliminates single points of failure and simplifies cluster management since any node can perform any necessary cluster operation. |\n\n### Data Structures and Types\n\nThe core data types that represent cache entries, cluster state, and system configuration throughout the distributed cache.\n\n| Type | Fields | Purpose |\n|------|--------|---------|\n| `HashRing` | virtualNodes int, ring map[uint32]string, sortedKeys []uint32, nodes map[string]bool | Implements consistent hashing for key distribution across cluster nodes |\n| `LRUCache` | mutex sync.RWMutex, capacity int64, used int64, items map[string]*list.Element, order *list.List | Thread-safe LRU cache with memory accounting and eviction |\n| `CacheEntry` | Key string, Value []byte, ExpiresAt time.Time, Size int64 | Individual cache item with expiration and memory tracking |\n| `NodeConfig` | NodeID string, ListenAddress string, AdvertiseAddr string, JoinAddresses []string, MaxMemoryMB int, VirtualNodes int, ReplicationFactor int, HealthCheckInterval time.Duration, GossipInterval time.Duration, RequestTimeout time.Duration | Complete configuration for a cache node including networking, capacity, and timing parameters |\n| `GossipMessage` | NodeStates map[string]NodeState, Version uint64 | Cluster membership information exchanged between nodes |\n| `NodeState` | NodeID string, Address string, Status string, LastSeen time.Time, Version uint64 | Current status and metadata for a single cluster node |\n| `ReplicatedEntry` | *CacheEntry, VectorClock *VectorClock, ReplicationInfo map[string]ReplicaInfo | Cache entry extended with replication metadata and causality tracking |\n| `VectorClock` | Clock map[string]uint64, mutex sync.RWMutex | Logical timestamp tracking causality relationships between distributed operations |\n| `HealthResult` | NodeID string, Timestamp time.Time, TCPHealthy bool, HTTPHealthy bool, CacheHealthy bool, ResponseTime time.Duration, ErrorDetails string, OverallHealth HealthStatus | Comprehensive health assessment results for a cluster node |\n\n### Cache Operations and Memory Management\n\nTerms related to cache functionality, memory management, and data lifecycle within individual nodes.\n\n| Term | Definition |\n|------|------------|\n| **LRU eviction** | Memory management policy that removes least recently used entries when the cache reaches its memory limit. This policy assumes temporal locality - recently accessed data is more likely to be accessed again. |\n| **TTL expiration** | Automatic removal of cache entries after their configured time-to-live expires. TTL provides a mechanism for cache invalidation based on data freshness requirements. |\n| **memory accounting** | Precise tracking of memory usage by cache entries to enforce capacity limits. Includes not just the data size but also metadata overhead for keys, timestamps, and data structures. |\n| **lazy expiration** | TTL checking strategy that verifies expiration during access operations rather than using background cleanup. This reduces CPU overhead but may allow expired entries to temporarily consume memory. |\n| **hotspot** | A cluster node that receives disproportionately high load due to popular keys hashing to its ring positions. Virtual nodes help mitigate hotspots by distributing each node's key ranges. |\n| **temporal locality** | The principle that recently accessed cache data is likely to be accessed again soon. LRU eviction policy leverages this principle to keep frequently used data in memory. |\n\n### Distributed Systems and Networking\n\nConcepts related to how nodes communicate, discover each other, and maintain cluster membership.\n\n| Term | Definition |\n|------|------------|\n| **gossip protocol** | Peer-to-peer communication pattern where nodes periodically exchange cluster state information with random neighbors. Gossip ensures eventual convergence of cluster membership without requiring centralized coordination. |\n| **node discovery** | The mechanism by which new nodes find and join an existing cluster. Typically involves bootstrap addresses that new nodes contact to learn about current cluster members. |\n| **failure mode analysis** | Systematic categorization of potential system failures and their impacts on cluster operations. Helps design appropriate detection and recovery mechanisms for different types of failures. |\n| **graduated health checks** | Multi-level health verification with escalating depth and frequency. Starts with lightweight checks like TCP connectivity and progresses to application-level cache operations for suspected failures. |\n| **split-brain scenario** | A network partition where multiple isolated sub-clusters continue operating independently. Can lead to data inconsistency if not properly handled through quorum requirements. |\n| **cascading failure** | Progressive failure spread where the failure of one component increases load on remaining components, potentially causing them to fail as well. |\n| **network partition** | Communication failure that splits the cluster into isolated groups that cannot communicate with each other. Also called a \"network split\" or \"partition event\". |\n\n### Replication and Consistency\n\nTerms related to data replication, consistency guarantees, and conflict resolution in the distributed cache.\n\n| Term | Definition |\n|------|------------|\n| **replication factor** | The number of copies stored for each cache entry across different cluster nodes. Higher replication factors provide better fault tolerance but consume more storage and network bandwidth. |\n| **quorum** | The minimum number of replicas that must participate in a read or write operation to ensure consistency. Common configurations include majority quorums (N/2 + 1) for strong consistency. |\n| **vector clocks** | Logical timestamps that track causality relationships between distributed operations. Each node maintains a clock that advances when it performs operations, enabling detection of concurrent vs. causally-related updates. |\n| **conflict resolution** | Algorithms for handling divergent values between replicas when concurrent updates create inconsistencies. Strategies include last-writer-wins, vector clock comparison, and application-specific merge functions. |\n| **anti-entropy** | Background processes that detect and repair data inconsistencies between replicas. Typically involves periodic comparison of replica contents and synchronization of differences. |\n| **hinted handoff** | Temporary storage mechanism for writes to unavailable replica nodes. When a replica is down, hints are stored locally and delivered when the node recovers, ensuring eventual consistency. |\n| **read repair** | Synchronous repair process that detects and fixes stale replicas during read operations. When replicas return different values, the most recent version is propagated to stale replicas. |\n| **eventual consistency** | Consistency guarantee that all replicas will converge to the same value if no new updates occur. Provides high availability but allows temporary inconsistencies during network partitions or failures. |\n| **successor-based placement** | Replica placement strategy that stores copies on the next N nodes clockwise from the primary node on the hash ring. Provides predictable replica locations and load distribution. |\n| **sloppy quorums** | Quorum operations that can use alternative nodes when preferred replicas are unavailable. Maintains availability during failures but may require additional anti-entropy work. |\n\n### Health Monitoring and Failure Detection\n\nConcepts related to monitoring node health, detecting failures, and maintaining cluster availability.\n\n| Term | Definition |\n|------|------------|\n| **phi accrual failure detector** | Probabilistic failure detection algorithm that accumulates suspicion levels based on heartbeat patterns. Provides tunable sensitivity to network delays versus false failure detection. |\n| **circuit breaker** | Protection mechanism that prevents operations likely to fail from being attempted. Opens when failure rate exceeds threshold, reducing load on struggling nodes and preventing cascading failures. |\n| **false positive detection** | Incorrectly identifying healthy nodes as failed due to network delays or temporary overload. False positives can cause unnecessary cluster rebalancing and reduced availability. |\n| **byzantine behavior** | Incorrect node behavior due to software bugs, data corruption, or misconfiguration. Distinguished from simple crash failures because the node continues responding but with incorrect behavior. |\n| **quorum loss** | Situation where insufficient healthy replicas are available to meet consistency requirements. May require reducing consistency guarantees or rejecting operations until more nodes recover. |\n\n### Request Processing and Message Types\n\nThe different types of operations and messages that flow through the distributed cache system.\n\n| Term | Definition |\n|------|------------|\n| **MessageTypeGet** | Cache retrieval operation message type used for inter-node communication when routing requests to the node responsible for a key. |\n| **MessageTypeSet** | Cache storage operation message type used when storing or updating key-value pairs across replica nodes. |\n| **MessageTypeDelete** | Cache removal operation message type used when removing keys from the cache across all replicas. |\n| **MessageTypeReplicate** | Replication coordination message type used for managing replica consistency during write operations. |\n| **MessageTypeGossip** | Cluster membership gossip message type used for propagating node state information throughout the cluster. |\n\n### Health Status Enumeration\n\nThe different states a cluster node can be in from a health monitoring perspective.\n\n| Term | Definition |\n|------|------------|\n| **HealthStatusHealthy** | Node is fully operational and responding to all types of health checks within acceptable time limits. |\n| **HealthStatusSlow** | Node is responding but with degraded performance, possibly due to high load or resource contention. |\n| **HealthStatusSuspected** | Node health is questionable based on accumulated evidence like missed heartbeats or failed health checks. |\n| **HealthStatusFailed** | Node is confirmed failed and unavailable, requiring cluster rebalancing and replica recovery procedures. |\n\n### Testing and Debugging\n\nTerms related to testing distributed cache behavior and diagnosing problems in the system.\n\n| Term | Definition |\n|------|------------|\n| **chaos testing** | Testing methodology that intentionally introduces failures like network partitions, node crashes, and resource exhaustion to validate system resilience and recovery capabilities. |\n| **correlation ID** | Unique identifier that tracks individual operations across multiple nodes and system components. Essential for debugging distributed operations and understanding request flows. |\n| **structured logging** | Logging approach that uses consistent, machine-readable formats with searchable fields rather than free-form text. Enables automated log analysis and correlation across distributed components. |\n| **state snapshot** | Complete capture of system state at a specific point in time for debugging purposes. Includes hash ring state, cache contents, node membership, and replication status. |\n| **anomaly detection** | Automated identification of unusual patterns in system behavior that may indicate bugs, performance problems, or security issues. |\n| **distributed tracing** | Technique for tracking requests as they flow through multiple system components, providing visibility into performance bottlenecks and failure points. |\n| **consistency audit** | Systematic verification that system invariants hold across the entire cluster, such as proper replica placement and hash ring consistency. |\n| **failure domain** | The scope of system components affected by a particular type of failure. Understanding failure domains helps design appropriate isolation and recovery mechanisms. |\n| **root cause analysis** | Systematic investigation methodology to identify the underlying causes of system problems rather than just addressing symptoms. |\n\n### Performance and Optimization\n\nTerms related to system performance, scalability, and optimization techniques.\n\n| Term | Definition |\n|------|------------|\n| **connection pooling** | Technique for reusing network connections across multiple requests to reduce connection establishment overhead and improve throughput. |\n| **batch operations** | Combining multiple individual cache operations into single network requests to reduce network overhead and improve efficiency. |\n| **hierarchical hash rings** | Multi-level hash ring organization for managing very large clusters by organizing nodes into regions or data centers with local and global rings. |\n| **adaptive load balancing** | Dynamic adjustment of load distribution based on actual traffic patterns and node performance rather than static hash-based assignment. |\n| **elastic scaling** | Automatic addition or removal of cluster capacity based on demand, typically integrated with cloud infrastructure auto-scaling capabilities. |\n\n### Advanced Features and Extensions\n\nConcepts related to extending the basic distributed cache with additional capabilities.\n\n| Term | Definition |\n|------|------------|\n| **multi-data type support** | Native support for structured data types beyond simple byte arrays, such as lists, sets, and sorted sets with type-specific operations. |\n| **cache hierarchies** | Multi-tier storage systems with different performance and cost characteristics, such as in-memory, SSD, and network-attached storage tiers. |\n| **extension hooks** | Integration points where new functionality can be added to core system components without modifying the base implementation. |\n| **energy-aware scheduling** | Optimization technique that considers power consumption when making load distribution and scaling decisions, important for large-scale deployments. |\n\n### Interface and Method Definitions\n\nKey interfaces and their methods that define the contracts between system components.\n\n| Interface/Method | Parameters | Returns | Description |\n|------------------|------------|---------|-------------|\n| `HashFunction.Hash` | data string | uint32 | Computes hash value for consistent ring positioning |\n| `HashFunction.Name` | none | string | Returns identifier for the hash function algorithm |\n| `TransportLayer.SendMessage` | ctx context.Context, address string, message interface{} | []byte, error | Sends message to remote node and returns response |\n| `TransportLayer.HealthCheck` | ctx context.Context, address string | error | Checks if remote node is responding |\n| `DataType.Serialize` | none | []byte, error | Converts data type instance to byte representation |\n| `DataType.Deserialize` | data []byte | error | Reconstructs data type instance from bytes |\n| `StorageTier.Get` | key string | []byte, bool, error | Retrieves value from storage tier |\n| `StorageTier.Set` | key string, value []byte | error | Stores value in storage tier |\n\n### Algorithm and Protocol Components\n\nSpecific algorithms and protocols used throughout the distributed cache implementation.\n\n| Component | Purpose | Key Characteristics |\n|-----------|---------|-------------------|\n| `PhiAccrualDetector` | Probabilistic failure detection | Accumulates suspicion based on heartbeat intervals, tunable sensitivity |\n| `RegionCoordinator` | Multi-region cluster management | Coordinates between regional hash rings for geographic distribution |\n| `AntiEntropyScheduler` | Background consistency repair | Schedules and coordinates replica synchronization processes |\n| `CircuitBreaker` | Failure isolation and recovery | Prevents cascading failures by stopping requests to failing nodes |\n| `BatchProcessor` | Operation batching optimization | Combines multiple operations for efficient network utilization |\n\n### Configuration and Operational Parameters\n\nImportant configuration options and their purposes within the system.\n\n| Configuration | Type | Purpose |\n|---------------|------|---------|\n| `VirtualNodes` | int | Number of virtual positions per physical node on hash ring |\n| `ReplicationFactor` | int | Number of replica copies for each cache entry |\n| `HealthCheckInterval` | time.Duration | Frequency of node health verification checks |\n| `GossipInterval` | time.Duration | Frequency of cluster state propagation messages |\n| `RequestTimeout` | time.Duration | Maximum time to wait for inter-node operation responses |\n| `MaxMemoryMB` | int | Memory limit for cache storage on each node |\n| `ConflictResolution` | string | Strategy for resolving replica conflicts (vector-clock, last-write-wins) |\n\n### Implementation Guidance\n\nThe following guidance helps bridge the gap between these conceptual definitions and actual implementation.\n\n#### Technology Stack Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| Hash Function | SHA-1 (crypto/sha1) | Consistent hash with jump hash |\n| Transport | HTTP REST + JSON (net/http) | gRPC with Protocol Buffers |\n| Serialization | JSON (encoding/json) | MessagePack or Protocol Buffers |\n| Logging | Standard library (log) | Structured logging (logrus, zap) |\n| Health Checks | HTTP ping endpoints | Multi-layer graduated checks |\n| Configuration | JSON files | YAML with environment variable override |\n\n#### Key Implementation Patterns\n\nWhen implementing the distributed cache, several patterns emerge consistently across components:\n\n**State Machine Pattern**: Many components (nodes, health status, recovery processes) follow state machine patterns. Always define states explicitly, document all valid transitions, and include timeout handling for stuck states.\n\n**Asynchronous Processing**: Background tasks like gossip propagation, health checking, and anti-entropy repair should use separate goroutines with proper lifecycle management and cancellation support.\n\n**Graceful Degradation**: Components should continue operating with reduced functionality rather than failing completely. For example, during network partitions, serve reads from available replicas even if writes must be rejected.\n\n**Idempotent Operations**: Design operations to be safely retryable. Cache SET operations are naturally idempotent, but DELETE and replication operations need careful design to handle duplicate messages.\n\n#### Common Implementation Mistakes\n\n⚠️ **Mistake: Inadequate Virtual Node Count**\nUsing too few virtual nodes (< 100 per physical node) results in uneven load distribution. This causes hotspots where some nodes handle much more traffic than others.\n\n⚠️ **Mistake: Blocking TTL Cleanup**\nRunning expiration cleanup synchronously during GET operations causes latency spikes. Always use background cleanup with lazy checking during access.\n\n⚠️ **Mistake: Gossip Message Size Growth**\nIncluding complete node state in every gossip message causes exponential growth in large clusters. Use incremental updates and version vectors to minimize message sizes.\n\n⚠️ **Mistake: Synchronous Replication Blocking**\nWaiting for all replicas to confirm writes causes availability problems during partial failures. Use asynchronous replication with configurable consistency levels.\n\n⚠️ **Mistake: Memory Accounting Errors**\nForgetting to account for metadata overhead (keys, timestamps, data structures) leads to memory limit violations. Always include complete memory footprint in calculations.\n\n#### Debugging and Monitoring Essentials\n\nEffective debugging of distributed cache systems requires comprehensive observability:\n\n**Correlation Tracking**: Every operation should have a correlation ID that follows it across all nodes involved in processing. This enables end-to-end request tracing.\n\n**Structured Metrics**: Collect metrics at multiple levels - per-operation, per-node, and cluster-wide. Key metrics include hit rates, latency distributions, memory usage, and replication lag.\n\n**State Introspection**: Provide debugging endpoints that expose internal state like hash ring contents, replica placement, gossip view, and health check results.\n\n**Failure Simulation**: Build chaos testing capabilities directly into the system for validating resilience under various failure conditions.\n\nThe terminology and concepts defined in this glossary form the foundation for understanding and implementing a robust distributed cache system. Each term connects to broader system design principles and implementation patterns that ensure the cache operates reliably at scale.\n"}