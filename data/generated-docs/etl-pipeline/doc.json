{"html":"<h1 id=\"data-pipeline-etl-system-design-document\">Data Pipeline / ETL System: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This system builds a scalable ETL pipeline framework that orchestrates data movement, transformation, and validation across heterogeneous sources. The key architectural challenge is managing complex task dependencies, handling failures gracefully, and ensuring data consistency while maintaining high throughput and observability.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - establishes the architectural context and requirements that guide the entire system design.</p>\n</blockquote>\n<h3 id=\"mental-model-factory-assembly-line\">Mental Model: Factory Assembly Line</h3>\n<p>Think of an ETL pipeline system as orchestrating a massive <strong>factory assembly line</strong> where raw materials flow through multiple processing stations to create finished products. Just as Henry Ford revolutionized manufacturing by breaking down car assembly into discrete, coordinated steps, modern data processing requires breaking down complex data workflows into manageable, interdependent tasks.</p>\n<p>In our factory analogy, <strong>raw data sources</strong> are like suppliers delivering different types of materials - some arrive by truck (batch files), others through conveyor belts (streaming APIs), and still others from warehouse inventory (databases). Each <strong>transformation step</strong> is a specialized workstation with specific tools and workers trained for particular operations - cutting, welding, painting, or quality inspection. The <strong>final destinations</strong> are shipping docks where finished products are loaded onto trucks bound for different customers.</p>\n<p>The critical insight from manufacturing applies directly to data processing: <strong>dependencies matter immensely</strong>. You cannot paint a car door before it&#39;s been cut and shaped. You cannot install wheels before the chassis is assembled. Similarly, you cannot aggregate sales data before cleaning customer records, and you cannot generate reports before all source systems have been synchronized. This dependency management becomes the central orchestration challenge.</p>\n<p>Just as a factory needs a <strong>production control system</strong> to track work orders, monitor station capacity, handle equipment failures, and ensure quality standards, our ETL system needs sophisticated orchestration to manage task scheduling, resource allocation, failure recovery, and data validation. The complexity multiplies when you consider that our &quot;factory&quot; operates 24/7 across distributed systems, with &quot;materials&quot; arriving from dozens of sources on different schedules, and &quot;customers&quot; expecting deliveries with strict SLA requirements.</p>\n<p>The assembly line analogy also reveals why simple script-based approaches fail at scale. A small workshop can operate with informal coordination - the craftsperson handles each order from start to finish. But when you need to process terabytes of data daily with sub-hour latency requirements, you need the equivalent of Toyota&#39;s production system: standardized processes, quality gates, just-in-time delivery, and sophisticated error recovery mechanisms.</p>\n<h3 id=\"existing-etl-solutions\">Existing ETL Solutions</h3>\n<p>The ETL landscape today offers several mature platforms, each optimizing for different aspects of the assembly line metaphor. Understanding their design philosophies and trade-offs illuminates the architectural decisions our system must make.</p>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Fsystem-overview.svg\" alt=\"System Component Overview\"></p>\n<h4 id=\"apache-airflow-the-enterprise-standard\">Apache Airflow: The Enterprise Standard</h4>\n<p>Apache Airflow dominates enterprise ETL orchestration by treating pipelines as <strong>Directed Acyclic Graphs (DAGs) defined in Python code</strong>. Airflow&#39;s strength lies in its comprehensive ecosystem and battle-tested scalability - it orchestrates workflows for companies processing petabytes daily across thousands of tasks.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Strength</th>\n<th>Limitation</th>\n<th>Impact on Design</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DAG Definition</td>\n<td>Python-native, familiar to data engineers</td>\n<td>Code-heavy configuration, version control complexity</td>\n<td>Need balance between code flexibility and declarative simplicity</td>\n</tr>\n<tr>\n<td>Task Execution</td>\n<td>Robust parallelization, resource management</td>\n<td>Heavy infrastructure requirements (Redis, Postgres)</td>\n<td>Must consider deployment complexity vs. feature richness</td>\n</tr>\n<tr>\n<td>Failure Handling</td>\n<td>Comprehensive retry policies, alerting integration</td>\n<td>Complex debugging when failures cascade through dependencies</td>\n<td>Error handling must be intuitive and provide clear diagnosis</td>\n</tr>\n<tr>\n<td>Extensibility</td>\n<td>Rich plugin ecosystem, custom operators</td>\n<td>Steep learning curve, opinionated architecture</td>\n<td>Plugin system should be simple but powerful</td>\n</tr>\n</tbody></table>\n<p>Airflow&#39;s <strong>scheduler-executor-webserver architecture</strong> separates concerns cleanly but requires significant operational overhead. The scheduler parses DAGs, determines task readiness, and queues work. Executors (LocalExecutor, CeleryExecutor, KubernetesExecutor) handle the actual task execution with different scalability and isolation characteristics. The webserver provides monitoring and manual intervention capabilities.</p>\n<p>A critical Airflow insight is that <strong>task definitions must be idempotent</strong> because the scheduler may retry tasks multiple times due to infrastructure failures. This idempotency requirement permeates every aspect of pipeline design and significantly influences our transformation engine architecture.</p>\n<h4 id=\"dagster-the-developer-experience-focus\">Dagster: The Developer Experience Focus</h4>\n<p>Dagster represents a newer generation of orchestration tools prioritizing <strong>developer experience and data quality</strong>. While Airflow focuses on workflow orchestration, Dagster emphasizes the data itself - assets, lineage, and validation.</p>\n<table>\n<thead>\n<tr>\n<th>Design Philosophy</th>\n<th>Dagster Approach</th>\n<th>Traditional ETL Approach</th>\n<th>Architectural Implication</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Mental Model</td>\n<td>Data assets with materialization logic</td>\n<td>Tasks that happen to process data</td>\n<td>Our system should be data-centric, not just task-centric</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>Built-in unit testing for individual ops</td>\n<td>Integration testing in staging environments</td>\n<td>Need comprehensive testing framework from the start</td>\n</tr>\n<tr>\n<td>Type System</td>\n<td>Strong typing with runtime validation</td>\n<td>Schema-on-read with runtime failures</td>\n<td>Type safety should be configurable, not mandatory</td>\n</tr>\n<tr>\n<td>Observability</td>\n<td>Asset lineage and data quality metrics built-in</td>\n<td>Monitoring added as afterthought</td>\n<td>Lineage tracking should be first-class, not bolted on</td>\n</tr>\n</tbody></table>\n<p>Dagster&#39;s <strong>software-defined assets</strong> concept treats data tables, files, and ML models as first-class citizens with explicit dependencies. Instead of thinking &quot;run this SQL query after that API call completes,&quot; you think &quot;materialize the customer_summary table when clean_customer_data and recent_orders assets are available.&quot; This abstraction level significantly improves reasoning about complex pipelines.</p>\n<p>The Dagster <strong>op (operation) and job</strong> model separates reusable logic (ops) from specific pipeline definitions (jobs). This separation enables better testing - you can unit test individual ops with mock data, then compose them into jobs for integration testing. Our system should adopt similar composability principles.</p>\n<h4 id=\"custom-solutions-the-build-vs-buy-decision\">Custom Solutions: The Build vs. Buy Decision</h4>\n<p>Many organizations build custom ETL solutions when existing tools don&#39;t match their specific requirements or constraints. Common drivers for custom development include:</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Why Existing Tools Fall Short</th>\n<th>Custom Solution Advantage</th>\n<th>Trade-off</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Extreme Performance</td>\n<td>General-purpose tools have abstraction overhead</td>\n<td>Optimize for specific data patterns and volumes</td>\n<td>Development and maintenance cost vs. performance gain</td>\n</tr>\n<tr>\n<td>Legacy Integration</td>\n<td>Existing tools don&#39;t support proprietary protocols</td>\n<td>Direct integration with legacy systems</td>\n<td>Technical debt vs. operational efficiency</td>\n</tr>\n<tr>\n<td>Regulatory Compliance</td>\n<td>Standard tools may not meet audit requirements</td>\n<td>Full control over security and logging</td>\n<td>Compliance assurance vs. feature velocity</td>\n</tr>\n<tr>\n<td>Cost Optimization</td>\n<td>Commercial tools expensive at scale</td>\n<td>No licensing fees, cloud-optimized architecture</td>\n<td>Development investment vs. operational savings</td>\n</tr>\n</tbody></table>\n<p>Custom solutions often excel in narrow domains but struggle with the <strong>feature breadth</strong> that mature platforms provide. A custom solution might perfectly handle your specific data transformation needs but lack comprehensive monitoring, alerting, user management, and operational tooling that enterprises require.</p>\n<blockquote>\n<p><strong>The Build vs. Buy Decision Framework</strong>: Choose custom development when the performance, integration, or cost advantages outweigh the opportunity cost of not investing in business logic development. The hidden costs of custom platforms include ongoing maintenance, feature development, security updates, and knowledge transfer as teams change.</p>\n</blockquote>\n<h3 id=\"core-technical-challenges\">Core Technical Challenges</h3>\n<p>Building a production-grade ETL system requires solving fundamental distributed systems challenges while maintaining the developer experience that makes complex data workflows manageable. These challenges are not independent - decisions in one area create constraints and opportunities in others.</p>\n<h4 id=\"dependency-management-the-coordination-problem\">Dependency Management: The Coordination Problem</h4>\n<p>Managing task dependencies in distributed systems involves more complexity than simple precedence constraints. In our assembly line analogy, this is equivalent to coordinating multiple production lines with shared resources, variable processing times, and occasional equipment failures.</p>\n<p><strong>The DAG Parsing Challenge</strong>: Unlike traditional job schedulers that work with simple precedence lists, ETL systems must parse potentially complex dependency graphs from various sources (YAML files, Python code, database configurations) and validate them for correctness before execution begins.</p>\n<table>\n<thead>\n<tr>\n<th>Parsing Requirement</th>\n<th>Technical Challenge</th>\n<th>Solution Approach</th>\n<th>Failure Mode</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cycle Detection</td>\n<td>Must identify circular dependencies in potentially large graphs</td>\n<td>Implement depth-first search with back-edge detection</td>\n<td>False positives when parsing incremental updates</td>\n</tr>\n<tr>\n<td>Dynamic Dependencies</td>\n<td>Task dependencies may depend on runtime data or external conditions</td>\n<td>Support conditional dependencies with runtime resolution</td>\n<td>Deadlocks when conditions create cycles</td>\n</tr>\n<tr>\n<td>Cross-Pipeline Dependencies</td>\n<td>Tasks in different pipelines may depend on each other</td>\n<td>Global dependency resolution with pipeline isolation</td>\n<td>Cascading failures across pipeline boundaries</td>\n</tr>\n<tr>\n<td>Parameterized Dependencies</td>\n<td>Template-based task definitions with variable substitution</td>\n<td>Parse dependencies after parameter resolution</td>\n<td>Invalid graphs when parameters create cycles</td>\n</tr>\n</tbody></table>\n<p><strong>The Topological Ordering Problem</strong>: Once dependencies are validated, the system must determine execution order while maximizing parallelism. This is a classic topological sort problem, but ETL systems add complications:</p>\n<ol>\n<li><strong>Resource Constraints</strong>: Not all ready tasks can execute simultaneously due to memory, CPU, or external system limits</li>\n<li><strong>Priority Scheduling</strong>: Critical path tasks should execute before non-critical tasks when resources are limited  </li>\n<li><strong>Dynamic Rescheduling</strong>: Task failures require recomputing execution order for remaining tasks</li>\n<li><strong>Partial Pipeline Execution</strong>: Users often want to run subsets of pipelines, requiring dependency subgraph extraction</li>\n</ol>\n<blockquote>\n<p><strong>Design Insight</strong>: The dependency resolution system is the architectural foundation that determines system scalability. A naive implementation that recomputes the entire topological order after each task completion will become the bottleneck as pipeline complexity grows. The system must maintain incremental data structures that support efficient updates.</p>\n</blockquote>\n<p><strong>Distributed State Consistency</strong>: In a multi-node deployment, task scheduling decisions must be coordinated across schedulers to prevent duplicate execution while maintaining high availability.</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Model</th>\n<th>Coordination Mechanism</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single Leader</td>\n<td>One scheduler node makes all decisions</td>\n<td>Simple, consistent, no conflicts</td>\n<td>Single point of failure, scalability bottleneck</td>\n</tr>\n<tr>\n<td>Consensus-Based</td>\n<td>Raft/Paxos for scheduling decisions</td>\n<td>High availability, consistency guarantees</td>\n<td>Complex implementation, network partition sensitivity</td>\n</tr>\n<tr>\n<td>Eventually Consistent</td>\n<td>Distributed schedulers with conflict resolution</td>\n<td>High availability, partition tolerance</td>\n<td>Potential duplicate execution, complex recovery</td>\n</tr>\n<tr>\n<td>External Coordination</td>\n<td>Use existing systems (etcd, Zookeeper)</td>\n<td>Proven reliability, operational familiarity</td>\n<td>External dependency, network latency overhead</td>\n</tr>\n</tbody></table>\n<h4 id=\"failure-recovery-building-resilient-data-workflows\">Failure Recovery: Building Resilient Data Workflows</h4>\n<p>Data processing failures occur at multiple levels - infrastructure, network, application logic, and data quality. Unlike web applications where failed requests can be retried immediately, ETL failures often require sophisticated recovery strategies because of data consistency requirements and resource costs.</p>\n<p><strong>Failure Classification and Response Strategies</strong>: Different failure types require different recovery approaches, and the system must automatically classify failures to apply appropriate remediation.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>Prevention Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient Network</td>\n<td>Connection timeout, DNS resolution failure</td>\n<td>Exponential backoff retry</td>\n<td>Circuit breaker pattern, connection pooling</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Out of memory, disk full, CPU throttling</td>\n<td>Queue task for later execution</td>\n<td>Resource monitoring, preemptive scaling</td>\n</tr>\n<tr>\n<td>Data Quality Issues</td>\n<td>Schema validation failure, constraint violations</td>\n<td>Skip bad records with logging</td>\n<td>Data profiling, upstream validation</td>\n</tr>\n<tr>\n<td>Logic Errors</td>\n<td>Application exceptions, assertion failures</td>\n<td>Human intervention required</td>\n<td>Comprehensive testing, gradual rollouts</td>\n</tr>\n<tr>\n<td>Upstream System Unavailable</td>\n<td>API rate limiting, database maintenance</td>\n<td>Defer execution, alert operators</td>\n<td>SLA monitoring, alternative data sources</td>\n</tr>\n</tbody></table>\n<p><strong>The Partial Failure Problem</strong>: ETL tasks often process large datasets where partial completion is valuable. A task that successfully processes 80% of records before encountering corrupt data should preserve its progress rather than starting from scratch.</p>\n<p>Consider a task that extracts customer records from a REST API with pagination. The task successfully processes pages 1-8 but encounters a malformed response on page 9. The recovery options are:</p>\n<ol>\n<li><strong>Full Restart</strong>: Discard all progress and restart from page 1 - simple but wasteful</li>\n<li><strong>Checkpoint Resume</strong>: Save pagination state after each page and resume from page 9 - complex but efficient  </li>\n<li><strong>Partial Success</strong>: Mark pages 1-8 as complete, fail page 9, and let dependent tasks proceed with available data - requires sophisticated dependency management</li>\n</ol>\n<blockquote>\n<p><strong>Architectural Decision</strong>: The system must support <strong>checkpoint-based recovery</strong> as a first-class concept. Tasks should be able to save intermediate state that enables resumption after failures. This requires careful API design to make checkpointing efficient without overwhelming task implementations with persistence concerns.</p>\n</blockquote>\n<p><strong>Cascading Failure Prevention</strong>: When upstream tasks fail, downstream tasks must decide whether to skip execution, execute with partial data, or wait for upstream recovery. This decision depends on business requirements and data freshness constraints.</p>\n<table>\n<thead>\n<tr>\n<th>Downstream Strategy</th>\n<th>When to Use</th>\n<th>Implementation Complexity</th>\n<th>Business Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Fail Fast</td>\n<td>Critical data dependencies, regulatory requirements</td>\n<td>Low - propagate failure status</td>\n<td>High - entire pipeline stops</td>\n</tr>\n<tr>\n<td>Partial Execution</td>\n<td>Analytics workloads, non-critical reporting</td>\n<td>Medium - handle missing data gracefully</td>\n<td>Medium - reduced data quality</td>\n</tr>\n<tr>\n<td>Wait and Retry</td>\n<td>Near real-time requirements, SLA commitments</td>\n<td>High - timeout management, resource allocation</td>\n<td>Low - maintains data quality</td>\n</tr>\n<tr>\n<td>Use Cached Data</td>\n<td>Dashboard updates, periodic reports</td>\n<td>Medium - cache invalidation logic</td>\n<td>Variable - depends on data staleness tolerance</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-consistency-managing-state-across-distributed-operations\">Data Consistency: Managing State Across Distributed Operations</h4>\n<p>ETL operations must maintain data consistency across multiple systems while providing reasonable performance. Unlike OLTP databases that handle small, frequent transactions, ETL systems deal with large bulk operations that may run for hours and interact with systems that don&#39;t support transactions.</p>\n<p><strong>The Two-Phase Load Problem</strong>: Many ETL workflows follow an &quot;extract-transform-load&quot; pattern where the final load step must be atomic despite potentially loading data to multiple destination systems. Consider a pipeline that loads both a data warehouse and a search index - both destinations must be updated or neither should be updated.</p>\n<p>Traditional database transactions don&#39;t apply because:</p>\n<ol>\n<li><strong>Cross-System Boundaries</strong>: Destinations may be different database types, file systems, or external APIs</li>\n<li><strong>Long-Running Operations</strong>: Bulk loads may take hours, exceeding reasonable transaction timeouts</li>\n<li><strong>Resource Constraints</strong>: Holding locks during large operations blocks other concurrent workflows</li>\n</ol>\n<p><strong>Idempotency Requirements</strong>: Since the system may retry failed operations, all transformations and loads must produce the same result when executed multiple times. This requirement significantly influences API design and data modeling.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Idempotency Challenge</th>\n<th>Solution Pattern</th>\n<th>Implementation Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Incremental Loads</td>\n<td>Duplicate records on retry</td>\n<td>Upsert with deterministic keys</td>\n<td>Requires unique key identification</td>\n</tr>\n<tr>\n<td>Aggregations</td>\n<td>Double-counting on retry</td>\n<td>Checkpoint aggregation state</td>\n<td>Complex with streaming data</td>\n</tr>\n<tr>\n<td>File Operations</td>\n<td>Partial writes on failure</td>\n<td>Atomic rename operations</td>\n<td>Requires staging area</td>\n</tr>\n<tr>\n<td>API Calls</td>\n<td>Side effects on retry</td>\n<td>Idempotency tokens</td>\n<td>Requires upstream system support</td>\n</tr>\n</tbody></table>\n<p><strong>Schema Evolution Management</strong>: Production ETL systems must handle schema changes in source systems without breaking downstream consumers. This is particularly challenging because schema changes often occur without coordination between teams.</p>\n<p>The schema evolution problem manifests in several ways:</p>\n<ul>\n<li><strong>Additive Changes</strong>: New columns in source tables may break transformations that use <code>SELECT *</code></li>\n<li><strong>Breaking Changes</strong>: Removed or renamed columns cause immediate pipeline failures  </li>\n<li><strong>Type Changes</strong>: Column type modifications may cause silent data truncation or conversion errors</li>\n<li><strong>Constraint Changes</strong>: New validation rules may reject previously acceptable data</li>\n</ul>\n<blockquote>\n<p><strong>Design Principle</strong>: The system should implement <strong>schema versioning</strong> with backward compatibility policies. Each data asset should declare its expected schema version, and the system should validate compatibility before execution and provide automatic migration paths for compatible changes.</p>\n</blockquote>\n<h4 id=\"scalability-challenges-performance-and-resource-management\">Scalability Challenges: Performance and Resource Management</h4>\n<p>ETL systems face unique scalability challenges because workloads are often <strong>bursty, resource-intensive, and deadline-driven</strong>. Unlike web applications with relatively predictable load patterns, ETL systems may need to scale from processing megabytes during quiet periods to terabytes during month-end batch jobs.</p>\n<p><strong>Resource Allocation and Queueing</strong>: The system must efficiently allocate compute, memory, and I/O resources across concurrent tasks while respecting priority constraints and resource limits.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Constraint Pattern</th>\n<th>Scaling Strategy</th>\n<th>Monitoring Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU</td>\n<td>Compute-intensive transformations</td>\n<td>Horizontal scaling, distributed processing</td>\n<td>CPU utilization per task type</td>\n</tr>\n<tr>\n<td>Memory</td>\n<td>Large dataset operations, caching</td>\n<td>Memory limits per task, spill-to-disk</td>\n<td>Memory high-water marks</td>\n</tr>\n<tr>\n<td>I/O</td>\n<td>Database connections, file system throughput</td>\n<td>Connection pooling, async operations</td>\n<td>Queue depths, latency percentiles</td>\n</tr>\n<tr>\n<td>Network</td>\n<td>Cross-region data movement</td>\n<td>Compression, regional caching</td>\n<td>Bandwidth utilization, transfer costs</td>\n</tr>\n</tbody></table>\n<p><strong>The Thundering Herd Problem</strong>: When upstream data becomes available, multiple downstream tasks may become eligible for execution simultaneously. Without careful resource management, this can overwhelm the execution infrastructure and cause cascading failures.</p>\n<p>Consider a nightly batch job that loads customer transaction data. When the load completes, dozens of downstream analytics tasks become ready - customer segmentation, fraud detection, reporting aggregations, and ML feature generation. If all these tasks start immediately, they may:</p>\n<ul>\n<li>Overwhelm the source database with concurrent queries</li>\n<li>Exhaust memory by loading overlapping datasets</li>\n<li>Saturate network bandwidth and slow each other down</li>\n<li>Cause resource contention that degrades overall throughput</li>\n</ul>\n<p><strong>Data Locality and Transfer Costs</strong>: Large-scale ETL systems must consider data gravity - the tendency for computation to move toward data rather than vice versa. Moving terabytes across network boundaries is expensive in both time and money.</p>\n<table>\n<thead>\n<tr>\n<th>Data Movement Pattern</th>\n<th>Cost Factor</th>\n<th>Optimization Strategy</th>\n<th>Trade-off</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cross-Region Transfer</td>\n<td>Network egress charges, latency</td>\n<td>Regional data replication</td>\n<td>Storage cost vs. transfer cost</td>\n</tr>\n<tr>\n<td>Cross-Cloud Transfer</td>\n<td>Bandwidth costs, vendor lock-in</td>\n<td>Multi-cloud data mesh</td>\n<td>Complexity vs. flexibility</td>\n</tr>\n<tr>\n<td>On-Premise to Cloud</td>\n<td>Internet bandwidth limits</td>\n<td>Incremental sync, compression</td>\n<td>Migration time vs. operational cost</td>\n</tr>\n<tr>\n<td>Database to Processing</td>\n<td>Query overhead, connection limits</td>\n<td>Result caching, read replicas</td>\n<td>Freshness vs. performance</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Scalability Design Principle</strong>: The system architecture must support <strong>elastic scaling</strong> that automatically adjusts resource allocation based on pipeline demand and data volume patterns. This requires sophisticated workload prediction and resource provisioning that considers both current demand and scheduled pipeline execution.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<p>The architecture described above can be implemented using various technology stacks. The choice depends on organizational constraints, performance requirements, and operational preferences.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Enterprise Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Definition</td>\n<td>YAML configuration files</td>\n<td>Python-based DSL (Airflow-style)</td>\n<td>Visual pipeline builder with code generation</td>\n</tr>\n<tr>\n<td>Task Execution</td>\n<td>Process-based execution</td>\n<td>Container orchestration (Docker)</td>\n<td>Kubernetes jobs with resource isolation</td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>SQLite with WAL mode</td>\n<td>PostgreSQL with connection pooling</td>\n<td>Distributed database (CockroachDB, Spanner)</td>\n</tr>\n<tr>\n<td>Message Queue</td>\n<td>In-memory queues</td>\n<td>Redis with persistence</td>\n<td>Apache Kafka or AWS SQS</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>File-based logging</td>\n<td>Structured logging (ELK stack)</td>\n<td>APM integration (Datadog, New Relic)</td>\n</tr>\n<tr>\n<td>Failure Recovery</td>\n<td>Simple retry with backoff</td>\n<td>Dead letter queues</td>\n<td>Circuit breakers with health checks</td>\n</tr>\n</tbody></table>\n<h4 id=\"core-architecture-patterns\">Core Architecture Patterns</h4>\n<p><strong>Repository Pattern for Pipeline Definitions</strong>: Abstracts pipeline storage and versioning to support multiple configuration sources.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Core pipeline definition with metadata and task specifications.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # cron expression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tasks: List[</span><span style=\"color:#9ECBFF\">'TaskDefinition'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters: </span><span style=\"color:#79B8FF\">dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual task within a pipeline with dependencies and configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # extract, transform, load</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config: </span><span style=\"color:#79B8FF\">dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># upstream task IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_policy: </span><span style=\"color:#9ECBFF\">'RetryPolicy'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryPolicy</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configurable retry behavior for task failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_seconds: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exponential_backoff: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_error_types: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineRepository</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for pipeline definition storage and retrieval.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[PipelineDefinition]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve pipeline definition by ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_pipelines</span><span style=\"color:#E1E4E8\">(self) -> List[PipelineDefinition]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List all available pipeline definitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Persist pipeline definition with version increment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate pipeline definition and return error messages.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> YamlPipelineRepository</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">PipelineRepository</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"File-based pipeline repository using YAML configuration files.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config_directory: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config_directory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config_directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize YAML parser and file system monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement configuration file validation schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up file watcher for automatic reload on changes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[PipelineDefinition]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load YAML file for specified pipeline ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse YAML content into PipelineDefinition object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate task dependency references within pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return None if file not found or parsing fails</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for circular dependencies in task graph</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate task configuration schemas by type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure all dependency references point to valid tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate cron schedule expression syntax</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for duplicate task IDs within pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>State Machine Pattern for Task Execution</strong>: Manages task lifecycle transitions with clear state boundaries and event handling.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Callable, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumeration of all possible task execution states.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WAITING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"waiting\"</span><span style=\"color:#6A737D\">      # waiting for dependencies</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    QUEUED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"queued\"</span><span style=\"color:#6A737D\">        # ready to execute</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"success\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRYING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retrying\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancelled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span><span style=\"color:#6A737D\">      # skipped due to upstream failure</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskEvent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Events that trigger task state transitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEPENDENCIES_MET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dependencies_met\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_STARTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_started\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retry_scheduled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MAX_RETRIES_EXCEEDED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"max_retries_exceeded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED_BY_USER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancelled_by_user\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPSTREAM_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"upstream_failed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskExecution</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Runtime state for individual task execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state: TaskState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logs: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskStateMachine</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages task state transitions and enforces valid state changes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Valid state transitions - current_state -> {event -> next_state}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TRANSITIONS</span><span style=\"color:#E1E4E8\">: Dict[TaskState, Dict[TaskEvent, TaskState]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskState.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">DEPENDENCIES_MET</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_COMPLETED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">RETRY_SCHEDULED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">MAX_RETRIES_EXCEEDED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize state transition hooks for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up metrics collection for state change events</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure audit logging for compliance requirements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transition</span><span style=\"color:#E1E4E8\">(self, execution: TaskExecution, event: TaskEvent, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Attempt to transition task to new state based on event.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if transition was successful, False otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate current state allows this event transition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update execution object with new state and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log state transition with timestamp and context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Trigger any registered state change hooks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update metrics counters for monitoring dashboard</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return False if transition is invalid for current state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_valid_events</span><span style=\"color:#E1E4E8\">(self, current_state: TaskState) -> List[TaskEvent]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return list of events that can be applied to current state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look up valid transitions from TRANSITIONS mapping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return empty list if state has no valid transitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Plugin System for Extensible Connectors</strong>: Enables adding new data sources and destinations without modifying core system.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataConnector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all data source and destination connectors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> connect</span><span style=\"color:#E1E4E8\">(self, config: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Connection'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Establish connection to data source using provided configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract</span><span style=\"color:#E1E4E8\">(self, connection: </span><span style=\"color:#9ECBFF\">'Connection'</span><span style=\"color:#E1E4E8\">, query: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'DataStream'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract data from source system based on query parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self, connection: </span><span style=\"color:#9ECBFF\">'Connection'</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#9ECBFF\">'DataStream'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">             target: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'LoadResult'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load data stream to target destination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_config</span><span style=\"color:#E1E4E8\">(self, config: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate connector configuration and return error messages.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DatabaseConnector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">DataConnector</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generic database connector supporting SQL-based sources.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, driver_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.driver_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> driver_name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize database driver and connection pooling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up query timeout and retry configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure connection health checking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract</span><span style=\"color:#E1E4E8\">(self, connection: </span><span style=\"color:#9ECBFF\">'Connection'</span><span style=\"color:#E1E4E8\">, query: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'DataStream'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Build SQL query from query parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute query with cursor-based pagination for large results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle connection failures with automatic retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Stream results to avoid memory exhaustion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Track extraction metrics (rows processed, query time)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load</span><span style=\"color:#E1E4E8\">(self, connection: </span><span style=\"color:#9ECBFF\">'Connection'</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#9ECBFF\">'DataStream'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">             target: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'LoadResult'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Begin database transaction for atomicity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Batch insert records for optimal performance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle constraint violations and data type errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Commit transaction only after all batches succeed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record load statistics and performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConnectorRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Registry for discovering and instantiating data connectors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._connectors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Scan for connector implementations in plugin directories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate connector implementations match interface</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set up hot-reload capability for development environments</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_connector</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, connector_class: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate connector_class implements DataConnector interface</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store connector in registry with name as key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log connector registration for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_connector</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">) -> DataConnector:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look up connector class by name in registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Instantiate connector with provided configuration  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate configuration before returning instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Raise descriptive error if connector not found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"development-environment-setup\">Development Environment Setup</h4>\n<p><strong>Recommended Project Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl-pipeline-system/\n src/\n    core/\n       __init__.py\n       pipeline.py          # PipelineDefinition, TaskDefinition models\n       repository.py        # PipelineRepository interface\n       state_machine.py     # TaskStateMachine implementation\n       executor.py          # Task execution engine\n    connectors/\n       __init__.py\n       base.py              # DataConnector abstract base class\n       database.py          # DatabaseConnector implementation\n       file_system.py       # File-based extraction/loading\n       rest_api.py          # HTTP REST API connector\n    scheduling/\n       __init__.py\n       scheduler.py         # Cron-based pipeline scheduling\n       dag_parser.py        # DAG validation and topological sorting\n       dependency_resolver.py  # Task dependency management\n    monitoring/\n       __init__.py\n       metrics.py           # Performance metrics collection\n       logging.py           # Structured logging configuration\n       alerting.py          # Failure notification system\n    web/\n        __init__.py\n        api.py               # REST API for pipeline management\n        dashboard.py         # Web dashboard for monitoring\n        templates/           # HTML templates for UI\n tests/\n    unit/                    # Component-specific unit tests\n    integration/             # End-to-end pipeline tests\n    fixtures/                # Test data and mock configurations\n config/\n    pipelines/               # Pipeline definition YAML files\n    connectors.yaml          # Data source connection configs\n    scheduler.yaml           # Scheduling and retry policies\n docs/\n    api/                     # API documentation\n    connectors/              # Connector development guides\n    deployment/              # Operational deployment guides\n requirements.txt             # Python dependencies\n setup.py                     # Package installation configuration\n README.md                    # Quick start and architecture overview</code></pre></div>\n\n<h4 id=\"language-specific-implementation-notes\">Language-Specific Implementation Notes</h4>\n<p><strong>Python Dependency Management</strong>: Use <code>requirements.txt</code> for development and <code>setup.py</code> for distribution. Pin exact versions for reproducible builds.</p>\n<p><strong>Concurrency Patterns</strong>: Leverage <code>asyncio</code> for I/O-bound operations (database queries, API calls) and <code>multiprocessing</code> for CPU-bound transformations. Use <code>concurrent.futures.ThreadPoolExecutor</code> for mixed workloads.</p>\n<p><strong>Configuration Management</strong>: Use <code>pydantic</code> for configuration validation with automatic type conversion and detailed error messages. Support both environment variables and configuration files.</p>\n<p><strong>Logging Best Practices</strong>: Configure structured logging with <code>python-json-logger</code> for machine-readable logs. Include correlation IDs to trace requests across distributed components.</p>\n<p><strong>Error Handling Patterns</strong>: Create custom exception hierarchies for different failure types (transient vs. permanent). Use <code>tenacity</code> library for declarative retry policies with exponential backoff.</p>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Context Validation Checkpoint</strong>: After reading this section, you should be able to:</p>\n<ol>\n<li><p><strong>Explain the Assembly Line Analogy</strong>: Describe how ETL pipelines resemble manufacturing processes and why dependency management is the central challenge.</p>\n</li>\n<li><p><strong>Compare ETL Platforms</strong>: Create a comparison table showing when to choose Airflow vs. Dagster vs. custom development based on specific requirements.</p>\n</li>\n<li><p><strong>Identify Technical Challenges</strong>: Given a data processing scenario, identify which of the four core challenges (dependency management, failure recovery, data consistency, scalability) apply and why.</p>\n</li>\n<li><p><strong>Design Trade-off Analysis</strong>: For any architectural decision, articulate the options considered, decision made, rationale, and consequences using the ADR format.</p>\n</li>\n</ol>\n<p><strong>Verification Steps</strong>:</p>\n<ul>\n<li>Draw a simple ETL pipeline as a DAG and identify potential failure points</li>\n<li>Write a brief architectural decision record for choosing between different state management approaches</li>\n<li>List the key differences between task-centric (Airflow) and data-centric (Dagster) pipeline models</li>\n<li>Explain why idempotency is crucial for ETL operations with specific examples</li>\n</ul>\n<p><strong>Common Issues at This Stage</strong>:</p>\n<ul>\n<li><p><strong>Symptom</strong>: Overwhelmed by the complexity of existing ETL tools</p>\n</li>\n<li><p><strong>Cause</strong>: Trying to understand implementation details before architectural concepts</p>\n</li>\n<li><p><strong>Fix</strong>: Focus on the mental models and analogies first, then gradually add technical detail</p>\n</li>\n<li><p><strong>Symptom</strong>: Unclear about build vs. buy decision criteria</p>\n</li>\n<li><p><strong>Cause</strong>: Not considering total cost of ownership including maintenance and feature development</p>\n</li>\n<li><p><strong>Fix</strong>: Create a decision matrix weighing development cost, operational overhead, and strategic flexibility</p>\n</li>\n</ul>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - defines system boundaries and requirements that guide implementation decisions across pipeline definition, data processing, and orchestration features.</p>\n</blockquote>\n<h3 id=\"mental-model-project-charter\">Mental Model: Project Charter</h3>\n<p>Think of this goals section as a <strong>project charter for a construction project</strong>. Before breaking ground on a skyscraper, architects and stakeholders must agree on fundamental questions: How many floors? What&#39;s the budget? Will it have a parking garage? What about a helicopter pad? The charter explicitly states &quot;we&#39;re building a 40-story office building with underground parking&quot; and equally importantly &quot;we&#39;re NOT building a shopping mall or residential units.&quot; This prevents scope creep and ensures everyone builds toward the same vision.</p>\n<p>Similarly, our ETL system goals define both what we&#39;re building and what we&#39;re deliberately not building. Without clear boundaries, an ETL system could expand infinitely - real-time streaming, machine learning pipelines, data lakes, visualization tools, user management systems. The goals act as guardrails that keep the architecture focused and implementable within reasonable complexity bounds.</p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p>The functional goals define the core capabilities our ETL system must deliver to users. These represent the essential features that justify the system&#39;s existence and differentiate it from simple scripts or manual processes.</p>\n<p><strong>Pipeline Definition and Management</strong></p>\n<p>Our system must enable users to define complex data workflows as directed acyclic graphs (DAGs) where tasks have dependencies on other tasks. Users should be able to express these dependencies declaratively using configuration files rather than writing procedural code. The system must support both YAML and Python configuration formats to accommodate different user preferences and complexity levels.</p>\n<p>The pipeline definition capability includes parameter substitution, allowing users to create reusable pipeline templates that can be instantiated with different configuration values. For example, a data ingestion pipeline template might accept database connection parameters and table names as runtime variables, enabling the same pipeline logic to process different data sources.</p>\n<blockquote>\n<p><strong>Decision: Declarative Pipeline Definition</strong></p>\n<ul>\n<li><strong>Context</strong>: Users need to define complex data workflows with task dependencies. Options include procedural code, visual designers, or declarative configuration.</li>\n<li><strong>Options Considered</strong>: Pure Python code, GUI-based visual editor, YAML/JSON configuration files</li>\n<li><strong>Decision</strong>: Declarative YAML/Python configuration with programmatic task definition support</li>\n<li><strong>Rationale</strong>: Declarative formats enable version control, code review, and automated validation. Visual editors create vendor lock-in and are difficult to version control. Pure code provides flexibility but requires programming expertise.</li>\n<li><strong>Consequences</strong>: Enables infrastructure-as-code practices and makes pipelines reviewable by non-programmers, but requires building a custom DSL and validation layer.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Description</th>\n<th>User Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DAG Definition</td>\n<td>Define task dependencies as directed acyclic graphs</td>\n<td>Ensures proper execution order and enables parallel processing</td>\n</tr>\n<tr>\n<td>Parameter Substitution</td>\n<td>Runtime variable replacement in configurations</td>\n<td>Creates reusable pipeline templates for different environments</td>\n</tr>\n<tr>\n<td>Multi-format Support</td>\n<td>Both YAML and Python configuration options</td>\n<td>Accommodates different user skill levels and complexity needs</td>\n</tr>\n<tr>\n<td>Dependency Validation</td>\n<td>Automatic cycle detection and dependency verification</td>\n<td>Prevents invalid pipeline definitions before execution</td>\n</tr>\n</tbody></table>\n<p><strong>Data Extraction and Loading</strong></p>\n<p>The system must provide robust connectors for extracting data from common enterprise data sources including relational databases, REST APIs, and file systems. These connectors must handle authentication, connection pooling, and error recovery automatically while exposing simple configuration interfaces to users.</p>\n<p>For databases, the extraction capability must support both full table dumps and incremental loading strategies. Incremental loading uses techniques like watermarking (tracking the maximum timestamp or ID processed) and change data capture to extract only new or modified records since the last pipeline run. This dramatically reduces processing time and resource usage for large datasets.</p>\n<p>Loading capabilities must support bulk insertion strategies that optimize throughput for target systems. The system should automatically batch records, use appropriate bulk loading APIs (like PostgreSQL&#39;s COPY command), and handle schema mapping between source and destination systems when column names or types differ.</p>\n<table>\n<thead>\n<tr>\n<th>Connector Type</th>\n<th>Extraction Features</th>\n<th>Loading Features</th>\n<th>Incremental Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database</td>\n<td>SQL query execution, connection pooling, authentication</td>\n<td>Bulk inserts, upserts, schema mapping</td>\n<td>Watermark-based, CDC integration</td>\n</tr>\n<tr>\n<td>REST API</td>\n<td>Pagination, rate limiting, retry logic, authentication</td>\n<td>HTTP POST/PUT with batching</td>\n<td>Cursor-based pagination, timestamp filtering</td>\n</tr>\n<tr>\n<td>File System</td>\n<td>Multiple formats (CSV, JSON, Parquet), compression</td>\n<td>Atomic writes, partitioning, compression</td>\n<td>File modification time, filename patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Data Transformation and Validation</strong></p>\n<p>The transformation engine must support both SQL-based transformations for users familiar with declarative data manipulation and Python user-defined functions (UDFs) for custom business logic that cannot be expressed in SQL. SQL transformations should support templating to inject runtime parameters and enable reusable transformation logic.</p>\n<p>Data validation capabilities must enforce schema constraints, business rules, and data quality checks during the transformation process. The system should support configurable validation strategies - some pipelines may reject invalid records entirely, while others may flag invalid records for manual review but continue processing valid data.</p>\n<p>Schema evolution handling is critical for production systems where source data structures change over time. The transformation engine must detect schema changes, apply configurable evolution strategies (like adding default values for new columns), and maintain backward compatibility with existing pipeline definitions.</p>\n<blockquote>\n<p><strong>Decision: Hybrid SQL and Python Transformation Support</strong></p>\n<ul>\n<li><strong>Context</strong>: Users have varying technical backgrounds and transformation complexity requirements. Some transformations are simple aggregations while others require complex business logic.</li>\n<li><strong>Options Considered</strong>: SQL-only, Python-only, or hybrid approach supporting both</li>\n<li><strong>Decision</strong>: Support both SQL and Python transformations with seamless integration</li>\n<li><strong>Rationale</strong>: SQL handles 80% of common transformations efficiently and is familiar to analysts. Python provides flexibility for complex logic and integration with external libraries.</li>\n<li><strong>Consequences</strong>: Increases implementation complexity but maximizes user adoption by supporting different skill sets and use cases.</li>\n</ul>\n</blockquote>\n<p><strong>Pipeline Orchestration and Scheduling</strong></p>\n<p>The orchestration engine must execute pipelines according to user-defined schedules using standard cron expressions for time-based triggers. Additionally, the system must support event-driven execution where pipelines trigger automatically when upstream data sources change or external events occur.</p>\n<p>Task execution must handle parallelism intelligently, running independent tasks concurrently while respecting dependency constraints. The system should provide configurable resource limits to prevent any single pipeline from overwhelming the execution environment.</p>\n<p>Failure handling capabilities must include configurable retry policies with exponential backoff, dead letter queues for persistently failing tasks, and alerting integration to notify operators when manual intervention is required. The system must maintain detailed execution history and provide log aggregation for debugging failed pipeline runs.</p>\n<h3 id=\"non-functional-goals\">Non-Functional Goals</h3>\n<p>Non-functional goals define the quality attributes and operational characteristics the system must exhibit in production environments. These requirements significantly influence architectural decisions and implementation approaches.</p>\n<p><strong>Performance and Scalability</strong></p>\n<p>The system must process datasets ranging from thousands to millions of records efficiently. Pipeline execution overhead should remain minimal - a simple two-task pipeline should complete within 30 seconds of trigger time, with most overhead attributed to actual data processing rather than orchestration.</p>\n<p>Task parallelism must scale to utilize available CPU cores effectively. On a 4-core development machine, four independent tasks should execute concurrently. The architecture must support horizontal scaling where additional worker machines can be added to increase overall processing capacity.</p>\n<p>Memory usage must remain bounded even for large datasets. The system should use streaming and batching strategies to process datasets larger than available RAM. A pipeline processing a 1GB dataset should not require more than 256MB of heap memory at peak usage.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Target</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Startup Overhead</td>\n<td>&lt; 30 seconds for simple pipelines</td>\n<td>Time from trigger to first task execution</td>\n</tr>\n<tr>\n<td>Task Parallelism</td>\n<td>Utilize all available CPU cores</td>\n<td>Concurrent task execution count</td>\n</tr>\n<tr>\n<td>Memory Efficiency</td>\n<td>Process 4x data size in available RAM</td>\n<td>Peak memory usage vs dataset size</td>\n</tr>\n<tr>\n<td>Throughput</td>\n<td>10,000 records/second for simple transformations</td>\n<td>Records processed per unit time</td>\n</tr>\n</tbody></table>\n<p><strong>Reliability and Availability</strong></p>\n<p>The system must handle infrastructure failures gracefully without losing data or leaving pipelines in inconsistent states. Task execution must be idempotent - running the same pipeline multiple times should produce identical results without negative side effects. This enables safe retries and recovery from partial failures.</p>\n<p>State persistence must survive process restarts and machine failures. Pipeline execution state, task progress, and metadata must be stored in durable storage with appropriate backup and recovery procedures. The system should resume interrupted pipelines from the last successful checkpoint rather than restarting from the beginning.</p>\n<p>Error recovery must be automatic for transient failures like network timeouts or temporary resource unavailability. Only persistent errors that require human intervention should halt pipeline execution and trigger alerts.</p>\n<blockquote>\n<p><strong>Decision: Checkpoint-Based Recovery Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Pipelines may fail partway through execution due to infrastructure issues, and restarting from the beginning wastes computational resources.</li>\n<li><strong>Options Considered</strong>: No recovery (restart from beginning), task-level checkpointing, operation-level checkpointing</li>\n<li><strong>Decision</strong>: Task-level checkpointing with idempotent operation design</li>\n<li><strong>Rationale</strong>: Task-level granularity provides good balance between implementation complexity and recovery efficiency. Operation-level checkpointing adds significant complexity for marginal benefit.</li>\n<li><strong>Consequences</strong>: Failed pipelines can resume from the last completed task, but individual tasks must be designed to be idempotent and handle partial state.</li>\n</ul>\n</blockquote>\n<p><strong>Monitoring and Observability</strong></p>\n<p>The system must provide comprehensive visibility into pipeline execution through metrics, logs, and tracing. Users should be able to answer questions like &quot;Why did my pipeline fail?&quot; and &quot;Which task is the bottleneck?&quot; without accessing raw log files or debugging tools.</p>\n<p>Real-time monitoring must track pipeline run status, task execution progress, and system resource utilization. Historical metrics should enable trend analysis to identify performance degradation or capacity planning needs. The monitoring system must integrate with standard observability tools like Prometheus, Grafana, or cloud-native monitoring services.</p>\n<p>Data lineage tracking must capture the provenance of every dataset - which source systems contributed data, what transformations were applied, and when the processing occurred. This lineage information supports compliance requirements, debugging, and impact analysis when upstream data sources change.</p>\n<table>\n<thead>\n<tr>\n<th>Observability Feature</th>\n<th>Information Provided</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Real-time Status</td>\n<td>Current pipeline and task execution state</td>\n<td>Operations monitoring and alerting</td>\n</tr>\n<tr>\n<td>Performance Metrics</td>\n<td>Execution time, throughput, resource usage</td>\n<td>Performance optimization and capacity planning</td>\n</tr>\n<tr>\n<td>Data Lineage</td>\n<td>Source-to-destination data flow tracking</td>\n<td>Compliance, debugging, impact analysis</td>\n</tr>\n<tr>\n<td>Audit Logs</td>\n<td>User actions and system changes</td>\n<td>Security auditing and change tracking</td>\n</tr>\n</tbody></table>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>Explicitly defining what the system will not do is equally important as defining what it will do. These non-goals prevent scope creep and help users understand the system&#39;s boundaries and integration requirements.</p>\n<p><strong>Real-Time Stream Processing</strong></p>\n<p>This ETL system focuses on batch processing workflows and does not attempt to provide real-time stream processing capabilities. Users requiring sub-second data processing latency should integrate with dedicated streaming platforms like Apache Kafka, Apache Flink, or cloud streaming services.</p>\n<p>The system&#39;s scheduling granularity targets minute-level intervals rather than millisecond-level responsiveness. While the system supports event-driven pipeline triggers, these events are expected to be coarse-grained notifications (like &quot;new file arrived&quot; or &quot;database sync completed&quot;) rather than individual record-level events.</p>\n<p>This architectural decision significantly simplifies the system design by avoiding the complexity of stream processing semantics, windowing, watermarking, and exactly-once processing guarantees that streaming systems require.</p>\n<p><strong>Data Storage and Serving</strong></p>\n<p>The ETL system acts as a data processing orchestrator rather than a data storage platform. Users must provide their own source and destination systems - the ETL system does not include embedded databases, data lakes, or serving layers.</p>\n<p>This boundary means the system does not handle data modeling, query optimization, or end-user data access patterns. Users requiring these capabilities must integrate with appropriate storage solutions like data warehouses, operational databases, or analytical platforms.</p>\n<p>The system focuses on the &quot;T&quot; (Transform) and orchestration aspects of ETL while delegating the &quot;E&quot; (Extract) and &quot;L&quot; (Load) to specialized connectors that interface with existing storage systems.</p>\n<p><strong>Visual Pipeline Designer</strong></p>\n<p>While the system provides pipeline visualization for monitoring and debugging purposes, it does not include a drag-and-drop visual pipeline designer or GUI-based pipeline authoring tools. Pipeline definitions must be created using configuration files or programmatic APIs.</p>\n<p>This decision prioritizes version control, code review, and infrastructure-as-code practices over visual ease-of-use. Visual pipeline designers often create vendor lock-in and make it difficult to apply software engineering best practices to pipeline definitions.</p>\n<p>Users requiring visual authoring capabilities should use external tools that can generate compatible pipeline configurations or integrate with the system&#39;s programmatic APIs.</p>\n<p><strong>Machine Learning and Advanced Analytics</strong></p>\n<p>The system provides general-purpose data transformation capabilities but does not include specialized machine learning features like model training, inference, or ML-specific data preprocessing operations. Users requiring these capabilities should integrate with dedicated ML platforms or data science tools.</p>\n<p>This boundary prevents the system from becoming an overly complex platform that tries to solve every data-related problem. By focusing on general ETL orchestration, the system can integrate effectively with specialized tools rather than competing with them.</p>\n<blockquote>\n<p><strong>Decision: Integration-First Architecture Over Platform Expansion</strong></p>\n<ul>\n<li><strong>Context</strong>: Data ecosystems require many specialized tools (streaming, ML, storage, visualization). The system could try to include these capabilities or focus on integration.</li>\n<li><strong>Options Considered</strong>: Build an all-in-one data platform, focus purely on ETL orchestration, hybrid approach with some built-in capabilities</li>\n<li><strong>Decision</strong>: Focus on ETL orchestration with excellent integration capabilities</li>\n<li><strong>Rationale</strong>: All-in-one platforms become bloated and cannot compete with specialized tools. Integration-first design enables users to choose best-of-breed tools for each use case.</li>\n<li><strong>Consequences</strong>: Users must manage multiple tools but can optimize each component. System remains focused and maintainable.</li>\n</ul>\n</blockquote>\n<p><strong>Multi-Tenancy and User Management</strong></p>\n<p>The system does not provide built-in user authentication, authorization, or multi-tenant isolation capabilities. Deployment security and access control must be handled by the surrounding infrastructure using standard practices like network isolation, service authentication, and infrastructure-level access controls.</p>\n<p>This decision reduces system complexity and allows organizations to integrate with their existing identity and access management solutions rather than learning yet another authentication system. The system assumes it operates in a trusted environment where access control is enforced at the infrastructure layer.</p>\n<p><strong>High-Frequency Trading or Financial Data</strong></p>\n<p>While the system can process financial datasets, it does not provide specialized features for high-frequency trading, market data processing, or real-time risk management that require microsecond-level latency and specialized financial protocols.</p>\n<p>The system&#39;s reliability and consistency guarantees target business intelligence and analytical use cases where eventual consistency and minute-level latency are acceptable, rather than trading systems where milliseconds matter and regulatory compliance requires specialized audit trails.</p>\n<table>\n<thead>\n<tr>\n<th>Non-Goal Category</th>\n<th>What We Don&#39;t Build</th>\n<th>Recommended Alternatives</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Stream Processing</td>\n<td>Real-time event processing, windowing, exactly-once semantics</td>\n<td>Apache Kafka, Flink, cloud streaming services</td>\n</tr>\n<tr>\n<td>Data Storage</td>\n<td>Embedded databases, data lakes, query engines</td>\n<td>PostgreSQL, Snowflake, BigQuery, S3/HDFS</td>\n</tr>\n<tr>\n<td>Visual Authoring</td>\n<td>Drag-and-drop pipeline designer, GUI configuration</td>\n<td>External tools that generate configurations</td>\n</tr>\n<tr>\n<td>Machine Learning</td>\n<td>Model training, inference, ML-specific preprocessing</td>\n<td>MLflow, Kubeflow, cloud ML platforms</td>\n</tr>\n<tr>\n<td>User Management</td>\n<td>Authentication, authorization, multi-tenancy</td>\n<td>Infrastructure-level access controls, identity providers</td>\n</tr>\n<tr>\n<td>Financial Systems</td>\n<td>High-frequency trading, microsecond latency, specialized compliance</td>\n<td>Dedicated financial data platforms</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance helps translate the functional and non-functional goals into concrete technical decisions and development practices.</p>\n<p><strong>Technology Recommendations</strong></p>\n<p>The following technology choices support the goals defined above while balancing simplicity for development with production-readiness for deployment:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Goal Alignment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Format</td>\n<td>YAML with JSON Schema validation</td>\n<td>Python-based DSL with type checking</td>\n<td>Supports declarative pipeline definition goal</td>\n</tr>\n<tr>\n<td>Task Execution</td>\n<td>Thread pool with concurrent.futures</td>\n<td>Distributed task queue (Celery, RQ)</td>\n<td>Enables parallelism and scalability goals</td>\n</tr>\n<tr>\n<td>State Storage</td>\n<td>SQLite for development</td>\n<td>PostgreSQL for production</td>\n<td>Provides reliability and state persistence</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python logging with file handlers</td>\n<td>Structured logging with centralized collection</td>\n<td>Supports observability and monitoring goals</td>\n</tr>\n<tr>\n<td>Scheduling</td>\n<td>APScheduler for cron support</td>\n<td>Integration with external schedulers (Airflow, etc.)</td>\n<td>Enables time-based and event-driven execution</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Simple metrics collection</td>\n<td>Prometheus + Grafana integration</td>\n<td>Provides production-ready observability</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Project Structure</strong></p>\n<p>Organize the codebase to support the modular architecture implied by our functional goals:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl-pipeline/\n src/\n    pipeline/\n       __init__.py\n       definition.py          # PipelineDefinition, TaskDefinition classes\n       validation.py          # Pipeline validation logic\n       serialization.py       # YAML/JSON config parsing\n    execution/\n       __init__.py\n       scheduler.py           # Schedule management and triggering\n       executor.py            # Task execution engine\n       state_machine.py       # TaskState transitions\n    connectors/\n       __init__.py\n       base.py               # Abstract connector interfaces\n       database.py           # Database extraction/loading\n       api.py                # REST API connectors\n       filesystem.py         # File-based connectors\n    transforms/\n       __init__.py\n       sql_transform.py      # SQL-based transformations\n       python_transform.py   # Python UDF support\n    monitoring/\n        __init__.py\n        metrics.py            # Performance and execution metrics\n        logging.py            # Log aggregation and formatting\n        lineage.py            # Data lineage tracking\n config/\n    pipeline_schemas/         # JSON schemas for pipeline validation\n    examples/                 # Example pipeline definitions\n tests/\n    unit/                    # Component-level unit tests\n    integration/             # End-to-end pipeline tests\n    fixtures/                # Test data and mock configurations\n docs/\n     user_guide.md            # Pipeline authoring documentation\n     api_reference.md         # Programmatic API documentation</code></pre></div>\n\n<p><strong>Core Data Structures Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryPolicy</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Defines retry behavior for failed task executions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_seconds: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exponential_backoff: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_error_types: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Defines a single task within a pipeline DAG.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # 'extract', 'transform', 'load'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Task IDs this task depends on</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_policy: RetryPolicy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete pipeline specification with metadata and task definitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # Cron expression or event trigger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tasks: List[TaskDefinition]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]  </span><span style=\"color:#6A737D\"># Runtime parameters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumeration of possible task execution states.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WAITING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"waiting\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    QUEUED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"queued\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"success\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRYING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retrying\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancelled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskEvent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Events that trigger task state transitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEPENDENCIES_MET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dependencies_met\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_STARTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_started\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retry_scheduled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MAX_RETRIES_EXCEEDED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"max_retries_exceeded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED_BY_USER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancelled_by_user\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPSTREAM_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"upstream_failed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskExecution</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Runtime state for a specific task execution instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state: TaskState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logs: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span></code></pre></div>\n\n<p><strong>Configuration Validation Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> jsonschema</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_pipeline</span><span style=\"color:#E1E4E8\">(pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[PipelineDefinition]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Retrieve pipeline definition by ID from configuration storage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        pipeline_id: Unique identifier for the pipeline</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        PipelineDefinition if found, None otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Implement configuration storage backend (file, database, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Add caching layer for frequently accessed pipelines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Handle configuration versioning and schema migration</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Implementation will load from config storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_pipeline</span><span style=\"color:#E1E4E8\">(pipeline: PipelineDefinition) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate pipeline definition and return list of error messages.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        pipeline: Pipeline definition to validate</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        List of validation error messages (empty if valid)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This function performs multiple validation checks:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Schema validation against JSON schema</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - DAG cycle detection</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Task dependency reference validation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    - Resource and timeout constraint validation</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate against JSON schema for structural correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for cycles in task dependency graph</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify all task dependencies reference existing tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate cron schedule expression syntax</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check resource constraints and timeout values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate connector configurations for each task type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> transition</span><span style=\"color:#E1E4E8\">(execution: TaskExecution, event: TaskEvent, error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Attempt state transition based on event and current state.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        execution: Current task execution instance</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        event: Event triggering the transition</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        error_message: Optional error details for failure events</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        True if transition was valid and applied, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Implement state machine transition logic based on TRANSITIONS mapping</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Update execution timestamps and attempt counts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Handle retry logic and backoff scheduling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Log state changes for audit and debugging</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Implementation will check TRANSITIONS table and update execution state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># State transition mapping - defines valid state changes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TRANSITIONS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">DEPENDENCIES_MET</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_COMPLETED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">RETRY_SCHEDULED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">MAX_RETRIES_EXCEEDED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Success, cancelled, and skipped are terminal states</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint Guidelines</strong></p>\n<p>After implementing the goals and boundaries defined in this section, verify the following checkpoint behaviors:</p>\n<ol>\n<li><p><strong>Goal Validation Checkpoint</strong>: Create a simple pipeline definition and verify it can be loaded and validated without errors. The validation should catch basic issues like missing required fields or invalid cron expressions.</p>\n</li>\n<li><p><strong>Non-Goal Boundary Checkpoint</strong>: Attempt to use features that are explicitly non-goals (like real-time streaming or visual authoring) and verify they are clearly unsupported with helpful error messages directing users to appropriate alternatives.</p>\n</li>\n<li><p><strong>Configuration Structure Checkpoint</strong>: The project structure should accommodate the modular architecture implied by the functional goals. Each major component (pipeline definition, execution, connectors, transforms, monitoring) should have its own module with clear interfaces.</p>\n</li>\n</ol>\n<p><strong>Common Implementation Pitfalls</strong></p>\n<p> <strong>Pitfall: Over-Engineering Goals</strong>\nEarly implementations often try to build every possible feature rather than focusing on the core goals. This leads to complex, unmaintainable systems that don&#39;t excel at their primary purpose. Stick to the defined functional goals and resist feature creep.</p>\n<p> <strong>Pitfall: Ignoring Non-Functional Requirements</strong>\nFunctional goals are visible to users, but non-functional goals (performance, reliability, observability) often determine production success. Design data structures and interfaces with performance and monitoring in mind from the beginning rather than retrofitting these concerns later.</p>\n<p> <strong>Pitfall: Unclear Goal Boundaries</strong>\nVague goal definitions lead to scope creep and architectural confusion. Each goal should be measurable and testable. If you can&#39;t write a test that verifies a goal is met, the goal needs to be more specific.</p>\n<p> <strong>Pitfall: Non-Goals Become Goals</strong>\nTeams often rationalize why a non-goal is actually essential and should be included. This dilutes the system&#39;s focus and increases complexity. When users request non-goal features, provide clear integration paths with external systems rather than expanding the system&#39;s scope.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - provides the architectural blueprint that guides implementation across pipeline definition (Milestone 1), data processing (Milestones 2-3), and orchestration/monitoring (Milestone 4).</p>\n</blockquote>\n<h3 id=\"mental-model-orchestra-conductor-system\">Mental Model: Orchestra Conductor System</h3>\n<p>Think of our ETL system like a symphony orchestra with multiple specialized sections working in harmony. The <strong>Conductor</strong> (Scheduler) reads the musical score (pipeline definition) and coordinates when each section plays. The <strong>Section Leaders</strong> (Task Executors) manage their musicians and ensure they play their parts correctly. The <strong>Stage Manager</strong> (DAG Engine) ensures all the sheet music is correct and tells the conductor the proper sequence. The <strong>Audio Engineers</strong> (Monitoring System) record the performance, adjust levels, and alert everyone if something goes wrong. Finally, the <strong>Musicians</strong> themselves (Connectors) are the specialists who actually produce the music by extracting, transforming, and loading data.</p>\n<p>Just as a conductor can&#39;t start the string section before the woodwinds finish their passage, our system ensures data dependencies are respected. When a musician makes a mistake, the section leader (executor) can have them retry their part without stopping the entire orchestra. The audio engineers continuously monitor the performance quality and can quickly identify which section needs attention.</p>\n<p>This orchestration analogy helps us understand why we need separate, specialized components rather than a monolithic system - each component has a distinct responsibility, and the coordination between them creates the beautiful symphony of data processing.</p>\n<h3 id=\"system-components\">System Components</h3>\n<p>Our ETL system consists of five core components, each with distinct responsibilities and clear boundaries. Understanding these components and their relationships is crucial for implementing a maintainable and scalable data pipeline system.</p>\n<h4 id=\"dag-definition-engine\">DAG Definition Engine</h4>\n<p>The <strong>DAG Definition Engine</strong> serves as the foundation of our pipeline system, responsible for parsing, validating, and maintaining pipeline definitions. This component acts as the &quot;compiler&quot; for our ETL pipelines, taking human-readable pipeline configurations and transforming them into executable task graphs.</p>\n<table>\n<thead>\n<tr>\n<th>Component Responsibility</th>\n<th>Description</th>\n<th>Key Data Structures</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Parsing</td>\n<td>Reads YAML/Python configuration files and converts them into <code>PipelineDefinition</code> objects</td>\n<td><code>PipelineDefinition</code>, <code>TaskDefinition</code></td>\n</tr>\n<tr>\n<td>Dependency Validation</td>\n<td>Performs cycle detection and validates task dependency relationships</td>\n<td>DAG validation results, dependency graphs</td>\n</tr>\n<tr>\n<td>Execution Graph Building</td>\n<td>Creates optimized task execution graphs using topological sorting</td>\n<td>Execution order lists, dependency matrices</td>\n</tr>\n<tr>\n<td>Schema Validation</td>\n<td>Ensures pipeline definitions conform to required schemas and constraints</td>\n<td>Validation error collections</td>\n</tr>\n</tbody></table>\n<p>The DAG Engine maintains a registry of all pipeline definitions and provides versioning capabilities. When a pipeline definition changes, it validates the new version against existing data schemas and dependency constraints before allowing the update.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The DAG Engine is intentionally stateless regarding pipeline executions. It only deals with pipeline <em>definitions</em>, not runtime state. This separation allows multiple scheduler instances to share the same pipeline definitions while maintaining independent execution state.</p>\n</blockquote>\n<h4 id=\"pipeline-scheduler\">Pipeline Scheduler</h4>\n<p>The <strong>Pipeline Scheduler</strong> orchestrates when pipelines execute based on time-based schedules, external triggers, or data availability conditions. Think of it as the &quot;air traffic control&quot; system that manages the flow of pipeline executions across the entire system.</p>\n<table>\n<thead>\n<tr>\n<th>Scheduling Capability</th>\n<th>Implementation Approach</th>\n<th>Configuration Options</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cron-based Scheduling</td>\n<td>Uses cron expressions parsed into trigger events</td>\n<td>Standard cron syntax with seconds precision</td>\n</tr>\n<tr>\n<td>Event-driven Triggers</td>\n<td>Listens for external events (file arrivals, API calls)</td>\n<td>HTTP webhooks, message queue listeners</td>\n</tr>\n<tr>\n<td>Data Dependency Scheduling</td>\n<td>Monitors upstream data sources for freshness</td>\n<td>Watermark-based triggers, file modification monitoring</td>\n</tr>\n<tr>\n<td>Manual Triggering</td>\n<td>Provides API endpoints for on-demand execution</td>\n<td>REST API with parameter overrides</td>\n</tr>\n</tbody></table>\n<p>The Scheduler maintains a priority queue of pending pipeline runs and coordinates with the Task Executor to ensure proper resource allocation. It implements backpressure mechanisms to prevent system overload when multiple pipelines are ready for execution.</p>\n<h4 id=\"task-executor\">Task Executor</h4>\n<p>The <strong>Task Executor</strong> is responsible for the actual execution of individual tasks within pipeline runs. It manages task lifecycle, resource allocation, failure handling, and state transitions. This component bridges the gap between high-level pipeline orchestration and low-level data processing operations.</p>\n<table>\n<thead>\n<tr>\n<th>Execution Capability</th>\n<th>Implementation Details</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parallel Task Execution</td>\n<td>Manages thread pools with configurable concurrency limits</td>\n<td>Task isolation prevents failures from affecting siblings</td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>Tracks <code>TaskExecution</code> objects through <code>TaskState</code> transitions</td>\n<td>Atomic state updates with event-driven notifications</td>\n</tr>\n<tr>\n<td>Resource Allocation</td>\n<td>Coordinates memory, CPU, and I/O resources across concurrent tasks</td>\n<td>Resource limits with graceful degradation</td>\n</tr>\n<tr>\n<td>Retry Logic</td>\n<td>Implements exponential backoff based on <code>RetryPolicy</code> configurations</td>\n<td>Configurable retry conditions and maximum attempt limits</td>\n</tr>\n</tbody></table>\n<p>The Task Executor maintains execution context for each task, including runtime parameters, connection pools, and intermediate results. It provides isolation between tasks to prevent resource conflicts and implements comprehensive logging for debugging purposes.</p>\n<h4 id=\"data-connectors\">Data Connectors</h4>\n<p><strong>Data Connectors</strong> are specialized components responsible for interfacing with external data systems. They provide a pluggable architecture that abstracts the complexity of different data sources and destinations behind a unified interface.</p>\n<table>\n<thead>\n<tr>\n<th>Connector Type</th>\n<th>Supported Systems</th>\n<th>Key Capabilities</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Source Connectors</td>\n<td>Databases (PostgreSQL, MySQL), APIs (REST, GraphQL), Files (S3, HDFS)</td>\n<td>Incremental extraction, schema discovery, pagination handling</td>\n</tr>\n<tr>\n<td>Destination Connectors</td>\n<td>Data warehouses (Snowflake, BigQuery), Databases, File systems</td>\n<td>Bulk loading, upsert operations, schema evolution</td>\n</tr>\n<tr>\n<td>Transform Connectors</td>\n<td>SQL engines (Spark, DuckDB), Python execution environments</td>\n<td>Custom logic execution, data validation, aggregations</td>\n</tr>\n</tbody></table>\n<p>Each connector implements standardized interfaces for connection management, data streaming, and error handling. The connector architecture supports plugin-style extensibility, allowing new data sources to be added without modifying core system components.</p>\n<h4 id=\"monitoring-and-observability-system\">Monitoring and Observability System</h4>\n<p>The <strong>Monitoring and Observability System</strong> provides comprehensive visibility into pipeline execution, performance metrics, and system health. It serves as the &quot;control tower&quot; that gives operators the information needed to maintain and optimize the ETL system.</p>\n<table>\n<thead>\n<tr>\n<th>Monitoring Aspect</th>\n<th>Data Collected</th>\n<th>Alerting Capabilities</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Execution</td>\n<td>Run duration, success/failure rates, task timing</td>\n<td>SLA violation alerts, failure notifications</td>\n</tr>\n<tr>\n<td>Resource Utilization</td>\n<td>Memory usage, CPU consumption, I/O throughput</td>\n<td>Resource exhaustion warnings, capacity planning metrics</td>\n</tr>\n<tr>\n<td>Data Quality</td>\n<td>Record counts, schema validation results, data profiling</td>\n<td>Data anomaly detection, quality threshold alerts</td>\n</tr>\n<tr>\n<td>System Health</td>\n<td>Component availability, error rates, dependency status</td>\n<td>Service degradation alerts, dependency failure notifications</td>\n</tr>\n</tbody></table>\n<p>The monitoring system implements configurable alerting rules that can escalate issues based on severity and duration. It maintains historical metrics for trend analysis and capacity planning.</p>\n<h3 id=\"component-interactions\">Component Interactions</h3>\n<p>Understanding how these components communicate and coordinate is essential for implementing a robust ETL system. Each interaction follows specific protocols and handles various failure scenarios gracefully.</p>\n<h4 id=\"scheduler-to-dag-engine-communication\">Scheduler to DAG Engine Communication</h4>\n<p>The Scheduler queries the DAG Engine to retrieve pipeline definitions and validate execution requests. This interaction is read-only from the Scheduler&#39;s perspective, ensuring clear separation of concerns.</p>\n<table>\n<thead>\n<tr>\n<th>Interaction Type</th>\n<th>Protocol</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Retrieval</td>\n<td>Synchronous API call using <code>get_pipeline(pipeline_id)</code></td>\n<td>Returns <code>None</code> for non-existent pipelines, triggering schedule cleanup</td>\n</tr>\n<tr>\n<td>Definition Validation</td>\n<td>Calls <code>validate_pipeline(pipeline)</code> before scheduling runs</td>\n<td>Validation errors prevent scheduling and generate operator alerts</td>\n</tr>\n<tr>\n<td>Dependency Resolution</td>\n<td>Requests task dependency graphs for execution planning</td>\n<td>Invalid dependencies cause immediate run failure with detailed error messages</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Pull-based Pipeline Retrieval</strong></p>\n<ul>\n<li><strong>Context</strong>: Scheduler needs access to current pipeline definitions for execution planning</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Push-based updates from DAG Engine to Scheduler</li>\n<li>Pull-based retrieval with caching</li>\n<li>Shared database access</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Pull-based retrieval with local caching</li>\n<li><strong>Rationale</strong>: Provides better fault isolation, simpler failure scenarios, and allows independent scaling of components</li>\n<li><strong>Consequences</strong>: Slight latency for pipeline definition updates, but improved system reliability and reduced coupling</li>\n</ul>\n</blockquote>\n<h4 id=\"scheduler-to-task-executor-coordination\">Scheduler to Task Executor Coordination</h4>\n<p>The Scheduler coordinates with Task Executors to manage pipeline run lifecycle and resource allocation. This relationship involves both command-style interactions and event-driven updates.</p>\n<table>\n<thead>\n<tr>\n<th>Coordination Activity</th>\n<th>Message Format</th>\n<th>Failure Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Run Initialization</td>\n<td><code>PipelineRun</code> object with execution context and parameters</td>\n<td>Executor startup failures trigger run cancellation and cleanup</td>\n</tr>\n<tr>\n<td>Task Assignment</td>\n<td>Individual <code>TaskExecution</code> assignments with dependency information</td>\n<td>Task assignment failures cause graceful task skipping with downstream impact analysis</td>\n</tr>\n<tr>\n<td>Progress Updates</td>\n<td>Periodic state updates using <code>TaskState</code> transitions</td>\n<td>Missing heartbeats trigger task timeout and potential retry scheduling</td>\n</tr>\n<tr>\n<td>Resource Management</td>\n<td>Resource allocation requests and availability notifications</td>\n<td>Resource exhaustion causes task queuing with backpressure to Scheduler</td>\n</tr>\n</tbody></table>\n<p>The Scheduler maintains a registry of available Task Executor instances and implements load balancing across them. When an Executor becomes unavailable, the Scheduler redistributes pending tasks to healthy instances.</p>\n<h4 id=\"task-executor-to-connector-interactions\">Task Executor to Connector Interactions</h4>\n<p>Task Executors invoke Data Connectors to perform actual data processing operations. These interactions involve streaming data transfers and require careful resource management.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Interface Method</th>\n<th>Resource Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Extraction</td>\n<td><code>extract(connection, query) -&gt; DataStream</code></td>\n<td>Memory usage for result buffering, connection pool management</td>\n</tr>\n<tr>\n<td>Data Loading</td>\n<td><code>load(connection, data, target) -&gt; LoadResult</code></td>\n<td>Batch size optimization, transaction management</td>\n</tr>\n<tr>\n<td>Schema Operations</td>\n<td><code>discover_schema(connection) -&gt; SchemaInfo</code></td>\n<td>Connection timeout handling, metadata caching</td>\n</tr>\n<tr>\n<td>Connection Management</td>\n<td><code>create_connection(config) -&gt; Connection</code></td>\n<td>Connection pooling, credential management, network retry logic</td>\n</tr>\n</tbody></table>\n<p>Connectors implement streaming interfaces to handle large datasets without exhausting system memory. They provide detailed error information that the Task Executor can use for intelligent retry decisions.</p>\n<h4 id=\"monitoring-system-event-collection\">Monitoring System Event Collection</h4>\n<p>The Monitoring System collects events and metrics from all other components through a combination of push-based events and pull-based metric collection.</p>\n<table>\n<thead>\n<tr>\n<th>Event Source</th>\n<th>Event Types</th>\n<th>Collection Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scheduler</td>\n<td>Pipeline started, completed, failed, cancelled</td>\n<td>Event streaming to monitoring message queue</td>\n</tr>\n<tr>\n<td>Task Executor</td>\n<td>Task state transitions, resource usage, performance metrics</td>\n<td>Periodic metric export via HTTP endpoints</td>\n</tr>\n<tr>\n<td>DAG Engine</td>\n<td>Pipeline definition changes, validation errors</td>\n<td>Audit log streaming with structured event format</td>\n</tr>\n<tr>\n<td>Connectors</td>\n<td>Data transfer volumes, connection health, schema changes</td>\n<td>Embedded metric collection within execution context</td>\n</tr>\n</tbody></table>\n<p>The monitoring system implements event correlation to connect related activities across components. For example, it can trace a pipeline failure back through task failures to specific connector errors.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Event correlation requires each pipeline run to carry a unique correlation ID that flows through all component interactions. This enables distributed tracing and comprehensive failure analysis.</p>\n</blockquote>\n<h4 id=\"error-propagation-and-recovery-coordination\">Error Propagation and Recovery Coordination</h4>\n<p>When failures occur, components coordinate recovery efforts while maintaining system consistency and preventing cascading failures.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Scenario</th>\n<th>Detection Method</th>\n<th>Recovery Coordination</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Task Execution Failure</td>\n<td>Executor reports failure via state transition</td>\n<td>Scheduler evaluates retry policy and coordinates with fresh Executor instance</td>\n</tr>\n<tr>\n<td>Connector Communication Loss</td>\n<td>Connection timeout or error response</td>\n<td>Executor implements circuit breaker pattern and reports degraded capability to Scheduler</td>\n</tr>\n<tr>\n<td>Scheduler Instance Failure</td>\n<td>Monitoring system detects missing heartbeats</td>\n<td>Standby Scheduler instances take over using persistent state from shared storage</td>\n</tr>\n<tr>\n<td>DAG Engine Unavailability</td>\n<td>Pipeline retrieval failures</td>\n<td>Scheduler operates with cached definitions and queues definition refresh requests</td>\n</tr>\n</tbody></table>\n<p>The system implements the <strong>Circuit Breaker Pattern</strong> for external dependencies. When a data source becomes unreliable, connectors temporarily stop attempting connections and return predictable errors rather than causing timeout delays across the system.</p>\n<h3 id=\"deployment-topology\">Deployment Topology</h3>\n<p>Understanding the recommended deployment architecture helps teams plan their infrastructure and avoid common deployment pitfalls. Our system supports both single-node development deployments and distributed production architectures.</p>\n<h4 id=\"development-environment-structure\">Development Environment Structure</h4>\n<p>For development and testing, all components can run within a single process or as separate processes on a single machine. This configuration simplifies debugging while maintaining the same interfaces used in production.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n cmd/\n    scheduler/           Scheduler service entry point\n    executor/            Task Executor service entry point  \n    single-node/         All-in-one development deployment\n internal/\n    dag/                 DAG Definition Engine\n       parser.go\n       validator.go\n       graph.go\n    scheduler/           Pipeline Scheduler\n       cron_scheduler.go\n       event_scheduler.go\n       run_manager.go\n    executor/            Task Executor\n       task_runner.go\n       state_machine.go\n       resource_manager.go\n    connectors/          Data Connectors\n       database/\n       api/\n       filesystem/\n    monitoring/          Monitoring System\n        metrics.go\n        events.go\n        alerting.go\n configs/\n    development.yaml     Single-node configuration\n    staging.yaml         Multi-node staging environment\n    production.yaml      Production deployment configuration\n pipelines/\n     examples/            Sample pipeline definitions\n     schemas/             Pipeline definition JSON schemas</code></pre></div>\n\n<p>The development configuration runs all components in a single process with shared in-memory state. This enables rapid iteration and simplified debugging with standard development tools.</p>\n<h4 id=\"production-deployment-architecture\">Production Deployment Architecture</h4>\n<p>Production deployments distribute components across multiple nodes for scalability, fault tolerance, and resource isolation. Each component type can scale independently based on workload characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Component Type</th>\n<th>Scaling Strategy</th>\n<th>Resource Requirements</th>\n<th>High Availability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scheduler</td>\n<td>Active/Standby with shared persistent state</td>\n<td>Low CPU, moderate memory for pipeline metadata</td>\n<td>Shared state store (PostgreSQL/etcd) enables quick failover</td>\n</tr>\n<tr>\n<td>Task Executor</td>\n<td>Horizontal scaling with auto-scaling based on queue depth</td>\n<td>High CPU/memory, varies by workload type</td>\n<td>Stateless design allows dynamic scaling without data loss</td>\n</tr>\n<tr>\n<td>DAG Engine</td>\n<td>Load balanced read replicas with single writer</td>\n<td>Low CPU, memory proportional to pipeline count</td>\n<td>Git-based pipeline storage provides natural backup and versioning</td>\n</tr>\n<tr>\n<td>Connectors</td>\n<td>Embedded within Task Executors</td>\n<td>Resource requirements vary by connector type</td>\n<td>Connection pooling and retry logic handle transient failures</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Clustered deployment with data replication</td>\n<td>Moderate CPU, high storage for metrics retention</td>\n<td>Time-series database clustering provides data durability</td>\n</tr>\n</tbody></table>\n<h4 id=\"network-communication-patterns\">Network Communication Patterns</h4>\n<p>Components communicate using well-defined protocols that support both local and distributed deployments. The communication patterns prioritize reliability and observability over raw performance.</p>\n<table>\n<thead>\n<tr>\n<th>Communication Path</th>\n<th>Protocol</th>\n<th>Failure Handling</th>\n<th>Security Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scheduler  DAG Engine</td>\n<td>HTTP REST with JSON</td>\n<td>Retry with exponential backoff, circuit breaker</td>\n<td>TLS encryption, API key authentication</td>\n</tr>\n<tr>\n<td>Scheduler  Task Executor</td>\n<td>Message queue (Redis/RabbitMQ)</td>\n<td>Message acknowledgment, dead letter queue</td>\n<td>Message-level encryption, queue access controls</td>\n</tr>\n<tr>\n<td>Task Executor  Connectors</td>\n<td>In-process function calls or HTTP</td>\n<td>Exception handling, connection pooling</td>\n<td>Credential injection, secret management</td>\n</tr>\n<tr>\n<td>All Components  Monitoring</td>\n<td>Event streaming + HTTP metrics</td>\n<td>Best-effort delivery with local buffering</td>\n<td>Metric anonymization, access logging</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Message Queue for Task Assignment</strong></p>\n<ul>\n<li><strong>Context</strong>: Scheduler needs reliable way to assign tasks to available executors</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Direct HTTP calls to executor instances</li>\n<li>Shared database with polling</li>\n<li>Message queue with work distribution</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Message queue with work distribution pattern</li>\n<li><strong>Rationale</strong>: Provides natural load balancing, handles executor failures gracefully, enables backpressure management</li>\n<li><strong>Consequences</strong>: Introduces message queue as additional infrastructure dependency, but significantly improves system resilience</li>\n</ul>\n</blockquote>\n<h4 id=\"persistent-state-management\">Persistent State Management</h4>\n<p>Each component manages different types of persistent state with appropriate storage technologies and consistency requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>State Type</th>\n<th>Storage Technology</th>\n<th>Consistency Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scheduler</td>\n<td>Run schedules, execution history</td>\n<td>PostgreSQL with ACID transactions</td>\n<td>Strong consistency for schedule integrity</td>\n</tr>\n<tr>\n<td>Task Executor</td>\n<td>Execution checkpoints, task logs</td>\n<td>Object storage (S3) + local caching</td>\n<td>Eventually consistent, focus on durability</td>\n</tr>\n<tr>\n<td>DAG Engine</td>\n<td>Pipeline definitions, validation cache</td>\n<td>Git repository + local file cache</td>\n<td>Strong consistency for definitions, eventual for cache</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Metrics, events, alert history</td>\n<td>Time-series database (Prometheus/InfluxDB)</td>\n<td>Eventually consistent, optimized for write throughput</td>\n</tr>\n</tbody></table>\n<p>The system implements <strong>checkpoint-based recovery</strong> for long-running tasks. Task Executors periodically save execution state to persistent storage, enabling recovery from partial failures without reprocessing all data.</p>\n<h4 id=\"security-and-access-control-deployment\">Security and Access Control Deployment</h4>\n<p>Production deployments implement defense-in-depth security with multiple layers of access control and data protection.</p>\n<table>\n<thead>\n<tr>\n<th>Security Layer</th>\n<th>Implementation</th>\n<th>Components Affected</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network Security</td>\n<td>VPC isolation, security groups, TLS encryption</td>\n<td>All inter-component communication</td>\n</tr>\n<tr>\n<td>Authentication</td>\n<td>Service-to-service authentication using certificates or tokens</td>\n<td>Scheduler, DAG Engine, Monitoring APIs</td>\n</tr>\n<tr>\n<td>Authorization</td>\n<td>Role-based access control for pipeline operations</td>\n<td>Pipeline definition access, execution permissions</td>\n</tr>\n<tr>\n<td>Secret Management</td>\n<td>External secret store integration (HashiCorp Vault, AWS Secrets Manager)</td>\n<td>Connector configurations, database credentials</td>\n</tr>\n<tr>\n<td>Audit Logging</td>\n<td>Comprehensive logging of all system actions with immutable audit trail</td>\n<td>All components with centralized log aggregation</td>\n</tr>\n</tbody></table>\n<p>Connector credentials are injected at runtime from the secret management system and never stored in pipeline definitions or system configuration files.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p> <strong>Pitfall: Shared State Between Components</strong></p>\n<p>Many developers initially try to share state directly between components (like using shared variables or files for coordination). This creates tight coupling and makes the system fragile to failures.</p>\n<p><strong>Why it&#39;s wrong</strong>: Shared state creates hidden dependencies that break when components are deployed separately or when failures occur. It also makes testing and development much more difficult.</p>\n<p><strong>How to fix it</strong>: Use message passing and well-defined APIs between components. Each component should own its state completely, and coordination should happen through explicit communication protocols.</p>\n<p> <strong>Pitfall: Synchronous Communication Everywhere</strong></p>\n<p>Using synchronous HTTP calls for all inter-component communication seems simple but creates cascading failure scenarios and tight coupling between component lifecycles.</p>\n<p><strong>Why it&#39;s wrong</strong>: When one component becomes slow or unavailable, synchronous calls cause the entire call chain to block or fail. This eliminates the benefits of component isolation.</p>\n<p><strong>How to fix it</strong>: Use asynchronous message queues for work distribution and events. Reserve synchronous calls only for queries where immediate responses are required (like pipeline definition retrieval).</p>\n<p> <strong>Pitfall: Insufficient Error Context</strong></p>\n<p>Developers often propagate generic errors between components without preserving context about what operation failed and why.</p>\n<p><strong>Why it&#39;s wrong</strong>: When debugging production issues, generic errors make it impossible to determine root causes. Operators need to understand which pipeline, task, and data source caused problems.</p>\n<p><strong>How to fix it</strong>: Implement structured error types that preserve operation context, correlation IDs, and relevant metadata. Each component should add its own context when propagating errors.</p>\n<p> <strong>Pitfall: No Resource Boundaries</strong></p>\n<p>Running all components with unlimited resource access causes resource contention and unpredictable performance as the system scales.</p>\n<p><strong>Why it&#39;s wrong</strong>: Without resource boundaries, one pipeline can consume all system memory or CPU, causing other pipelines to fail or perform poorly.</p>\n<p><strong>How to fix it</strong>: Implement resource limits at the Task Executor level, use connection pooling for external resources, and monitor resource usage to detect anomalies early.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inter-Component Communication</td>\n<td>HTTP REST with <code>requests</code> library</td>\n<td>Message queues (Redis/RabbitMQ) with <code>celery</code> or <code>rq</code></td>\n</tr>\n<tr>\n<td>State Storage</td>\n<td>SQLite for development, PostgreSQL for production</td>\n<td>Distributed databases (PostgreSQL + Redis cluster)</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>YAML files with <code>pyyaml</code> library</td>\n<td>Configuration service (Consul, etcd) with hot reloading</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Built-in Python <code>logging</code> + simple metrics</td>\n<td>Prometheus + Grafana with custom metrics</td>\n</tr>\n<tr>\n<td>Process Management</td>\n<td>Single Python process with threading</td>\n<td>Container orchestration (Docker + Kubernetes)</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl-system/\n src/\n    etl/\n       __init__.py\n       core/                     Shared data structures and interfaces\n          __init__.py\n          models.py             PipelineDefinition, TaskDefinition, TaskExecution\n          interfaces.py         Abstract base classes for connectors\n          events.py             TaskEvent, TaskState enums\n       dag/                      DAG Definition Engine (Milestone 1)\n          __init__.py\n          parser.py             YAML/Python pipeline parsing\n          validator.py          Cycle detection and validation\n          graph.py              Topological sorting and execution graphs\n       scheduler/                Pipeline Scheduler (Milestone 4)\n          __init__.py\n          cron_scheduler.py     Time-based scheduling\n          event_scheduler.py    Event-driven triggers\n          run_manager.py        Pipeline run lifecycle\n       executor/                 Task Executor (Milestone 4)\n          __init__.py\n          task_runner.py        Task execution engine\n          state_machine.py      TaskState transitions\n          resource_manager.py   Resource allocation and limits\n       connectors/               Data Connectors (Milestones 2-3)\n          __init__.py\n          base.py               Abstract connector interfaces\n          database/             Database source/destination connectors\n          api/                  REST API connectors\n          filesystem/           File-based connectors\n          transforms/           SQL and Python transformation engines\n       monitoring/               Monitoring System (Milestone 4)\n           __init__.py\n           metrics.py            Metrics collection and export\n           events.py             Event collection and correlation\n           alerting.py           Alert generation and notification\n    scripts/\n       start_scheduler.py        Scheduler service entry point\n       start_executor.py         Executor service entry point\n       single_node.py            Development single-process runner\n    tests/\n        unit/                     Component unit tests\n        integration/              Cross-component integration tests\n        e2e/                      End-to-end pipeline tests\n configs/\n    development.yaml              Development environment configuration\n    staging.yaml                  Staging environment configuration\n    production.yaml               Production deployment configuration\n pipelines/\n    examples/                     Sample pipeline definitions\n       simple_etl.yaml\n       complex_analytics.yaml\n    schemas/\n        pipeline_schema.json      JSON Schema for pipeline validation\n docker/\n    Dockerfile.scheduler          Container image for scheduler\n    Dockerfile.executor           Container image for executor\n    docker-compose.yml            Local development setup\n docs/\n     architecture.md               This design document\n     api/                          Component API documentation\n     examples/                     Usage examples and tutorials</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Core Data Models (complete implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/etl/core/models.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryPolicy</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for task retry behavior.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exponential_backoff: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_error_types: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"ConnectionError\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"TimeoutError\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"All possible states for a task execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WAITING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"waiting\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    QUEUED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"queued\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"success\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRYING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retrying\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancelled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskEvent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Events that trigger task state transitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEPENDENCIES_MET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dependencies_met\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_STARTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_started\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"execution_failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"retry_scheduled\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MAX_RETRIES_EXCEEDED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"max_retries_exceeded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED_BY_USER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancelled_by_user\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPSTREAM_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"upstream_failed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Definition of a single task within a pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # \"extract\", \"transform\", \"load\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dependencies: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_policy: RetryPolicy </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RetryPolicy)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3600</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete definition of an ETL pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # cron expression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tasks: List[TaskDefinition]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.now)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskExecution</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Runtime execution instance of a task.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state: TaskState </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TaskState.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logs: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'execution_id'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.execution_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4())</span></span></code></pre></div>\n\n<p><strong>State Machine Implementation (complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/etl/core/state_machine.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Set, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TaskState, TaskEvent, TaskExecution</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Valid state transitions - defines the complete state machine</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TRANSITIONS</span><span style=\"color:#E1E4E8\">: Dict[TaskState, Dict[TaskEvent, TaskState]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">DEPENDENCIES_MET</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">WAITING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">DEPENDENCIES_MET</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_COMPLETED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">RETRY_SCHEDULED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">MAX_RETRIES_EXCEEDED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># terminal state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Terminal states have no outgoing transitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> transition</span><span style=\"color:#E1E4E8\">(execution: TaskExecution, event: TaskEvent, error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Attempt to transition task execution to new state based on event.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns True if transition was valid and applied, False otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> execution.state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> current_state </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> TRANSITIONS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    valid_events </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> TRANSITIONS</span><span style=\"color:#E1E4E8\">[current_state]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> event </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> valid_events:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Apply the transition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    new_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> valid_events[event]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    execution.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Update execution metadata based on transition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> event </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        execution.started_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        execution.attempt_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> event </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_COMPLETED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        execution.completed_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    elif</span><span style=\"color:#E1E4E8\"> event </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_FAILED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        execution.completed_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> error_message:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            execution.error_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> error_message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_terminal_states</span><span style=\"color:#E1E4E8\">() -> Set[TaskState]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Return set of states that have no outgoing transitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {state </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> state, transitions </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> TRANSITIONS</span><span style=\"color:#E1E4E8\">.items() </span><span style=\"color:#F97583\">if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> transitions}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> is_terminal_state</span><span style=\"color:#E1E4E8\">(state: TaskState) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if a state is terminal (has no outgoing transitions).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> state </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> get_terminal_states()</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>DAG Engine Interface (skeleton for student implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/etl/dag/engine.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..core.models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineDefinition, TaskDefinition</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DAGEngine</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Core engine for pipeline definition management and validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, pipeline_storage_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize DAG engine with pipeline storage location.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pipeline_storage_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._pipeline_cache: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, PipelineDefinition] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[PipelineDefinition]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Retrieve pipeline definition by ID.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns None if pipeline doesn't exist.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check local cache first - return cached version if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load pipeline from storage (YAML file) if not in cache  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse YAML content into PipelineDefinition object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store in cache for future requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return PipelineDefinition or None if file doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.storage_path + pipeline_id + \".yaml\" as file path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Validate pipeline definition and return list of error messages.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns empty list if pipeline is valid.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that all task IDs are unique within pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that all task dependencies reference existing tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for circular dependencies using cycle detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate that each task has required config fields for its type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check that cron schedule expression is valid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self._detect_cycles() helper method for dependency validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _detect_cycles</span><span style=\"color:#E1E4E8\">(self, tasks: List[TaskDefinition]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Detect if task dependencies contain cycles using depth-first search.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if cycles are found, False otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build adjacency list from task dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Track visited nodes and recursion stack</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each unvisited task, start DFS traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If we encounter a node already in recursion stack, cycle detected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if any cycle found, False otherwise</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use three colors (white=unvisited, gray=visiting, black=done)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_execution_order</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> List[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Return task IDs grouped by execution level using topological sort.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Tasks in same inner list can execute in parallel.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build dependency graph with in-degree counting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start with tasks that have zero dependencies (in-degree = 0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Process tasks level by level, removing edges as we go</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add tasks to next level when their in-degree reaches 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of execution levels for parallel scheduling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use queue for breadth-first processing of each level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Python Specific Recommendations:</strong></p>\n<ul>\n<li>Use <code>dataclasses</code> with <code>frozen=True</code> for immutable data structures like <code>PipelineDefinition</code></li>\n<li>Implement <code>__post_init__</code> in dataclasses for validation and computed fields</li>\n<li>Use <code>typing.Protocol</code> for defining connector interfaces that can be implemented by third parties</li>\n<li>Use <code>concurrent.futures.ThreadPoolExecutor</code> for parallel task execution with configurable pool size</li>\n<li>Use <code>pathlib.Path</code> for all file system operations instead of string manipulation</li>\n<li>Use <code>logging</code> module with structured logging (JSON format) for production deployments</li>\n<li>Use <code>yaml.safe_load()</code> for parsing pipeline definitions to prevent code injection</li>\n<li>Implement connection pooling using <code>queue.Queue</code> for database connectors</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Custom exception hierarchy for better error handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ETLError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base exception for all ETL system errors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineValidationError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ETLError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when pipeline definition is invalid.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, errors: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pipeline_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pipeline_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Pipeline </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">pipeline_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> validation failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">errors</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskExecutionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ETLError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when task execution fails.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, attempt: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, cause: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.task_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> task_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.attempt </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> attempt</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cause </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cause</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Task </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">task_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> failed on attempt </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attempt</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cause</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing core data models and state machine:</strong></p>\n<ol>\n<li>Run unit tests: <code>python -m pytest tests/unit/test_models.py -v</code></li>\n<li>Expected output: All state transition tests pass, invalid transitions properly rejected</li>\n<li>Manual verification: Create a <code>TaskExecution</code> instance and call <code>transition()</code> with various events</li>\n<li>Check that terminal states (SUCCESS, FAILED, CANCELLED) don&#39;t accept any transitions</li>\n<li>Verify that attempt_count increments only on EXECUTION_STARTED events</li>\n</ol>\n<p><strong>After implementing DAG Engine basic functionality:</strong></p>\n<ol>\n<li>Create a sample pipeline YAML file with 3-4 tasks and dependencies</li>\n<li>Run: <code>python -c &quot;from etl.dag.engine import DAGEngine; engine = DAGEngine(&#39;pipelines/&#39;); print(engine.get_pipeline(&#39;test&#39;))&quot;</code></li>\n<li>Expected behavior: Pipeline loads successfully and prints PipelineDefinition object</li>\n<li>Test validation: Create pipeline with circular dependency, verify <code>validate_pipeline()</code> catches it</li>\n<li>Test execution order: Verify <code>get_execution_order()</code> returns correct parallel execution groups</li>\n</ol>\n<p><strong>After implementing basic component communication:</strong></p>\n<ol>\n<li>Start scheduler in one terminal: <code>python scripts/start_scheduler.py</code></li>\n<li>Start executor in another terminal: <code>python scripts/start_executor.py</code></li>\n<li>Submit a simple pipeline through REST API: <code>curl -X POST localhost:8080/pipelines/test/run</code></li>\n<li>Check that both components log the pipeline execution flow</li>\n<li>Verify that task state transitions are properly communicated between components</li>\n</ol>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline fails to load</td>\n<td>YAML syntax error or missing file</td>\n<td>Check file exists, validate YAML syntax</td>\n<td>Fix YAML formatting, verify file permissions</td>\n</tr>\n<tr>\n<td>Circular dependency error</td>\n<td>Invalid task dependencies in pipeline definition</td>\n<td>Draw dependency graph on paper</td>\n<td>Remove circular references, add proper task ordering</td>\n</tr>\n<tr>\n<td>Tasks stuck in PENDING state</td>\n<td>Dependency resolution failure</td>\n<td>Check task dependency configuration</td>\n<td>Verify all dependencies reference existing tasks</td>\n</tr>\n<tr>\n<td>High memory usage during execution</td>\n<td>Large dataset loading without streaming</td>\n<td>Monitor memory usage during connector operations</td>\n<td>Implement streaming in connectors, reduce batch sizes</td>\n</tr>\n<tr>\n<td>Components can&#39;t communicate</td>\n<td>Network configuration or service discovery issues</td>\n<td>Check port binding and firewall rules</td>\n<td>Verify component startup order and configuration</td>\n</tr>\n<tr>\n<td>State transitions fail</td>\n<td>Invalid state machine transitions</td>\n<td>Enable debug logging for state transitions</td>\n<td>Check that events match current state in TRANSITIONS table</td>\n</tr>\n</tbody></table>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - provides the data structures and state management that underpin pipeline definition (Milestone 1), data processing (Milestones 2-3), and orchestration (Milestone 4).</p>\n</blockquote>\n<h3 id=\"mental-model-digital-blueprint-and-construction-log\">Mental Model: Digital Blueprint and Construction Log</h3>\n<p>Think of the data model as the combination of <strong>architectural blueprints</strong> and a <strong>construction project log</strong>. The pipeline and task definitions are like detailed blueprints that specify exactly what needs to be built, in what order, and with what materials. The runtime state model is like the foreman&#39;s log that tracks which workers are doing what, when each task started and finished, and any problems encountered. The metadata and lineage system is like a comprehensive project history that documents every decision, every change order, and the complete chain of how raw materials became the finished building.</p>\n<p>Just as you wouldn&#39;t start construction without blueprints, and you wouldn&#39;t manage a complex construction project without tracking progress and maintaining detailed records, an ETL system needs these three layers of data organization to function reliably at scale.</p>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Data Model Relationships\"></p>\n<p>The data model serves as the foundation for all system operations, providing the structured representation of pipeline configurations, execution state, and historical metadata. This model must support concurrent access patterns, handle state transitions safely, and maintain data integrity across distributed components.</p>\n<h3 id=\"pipeline-and-task-definitions\">Pipeline and Task Definitions</h3>\n<p>The pipeline definition schema establishes the static blueprint for ETL workflows, capturing the declarative specification of what should happen without concern for runtime execution details. This separation between definition and execution enables versioning, validation, and reuse of pipeline logic across different environments.</p>\n<h4 id=\"pipelinedefinition-structure\">PipelineDefinition Structure</h4>\n<p>The <code>PipelineDefinition</code> serves as the top-level container for all pipeline metadata and task specifications. Each pipeline represents a complete ETL workflow with its own scheduling, parameters, and dependency graph.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>id</td>\n<td>str</td>\n<td>Unique identifier for the pipeline, used for referencing and scheduling</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable display name for the pipeline</td>\n</tr>\n<tr>\n<td>description</td>\n<td>str</td>\n<td>Detailed explanation of pipeline purpose and business logic</td>\n</tr>\n<tr>\n<td>schedule</td>\n<td>str</td>\n<td>Cron expression or event trigger specification for pipeline execution</td>\n</tr>\n<tr>\n<td>tasks</td>\n<td>List[TaskDefinition]</td>\n<td>Complete list of all tasks that comprise this pipeline</td>\n</tr>\n<tr>\n<td>parameters</td>\n<td>dict</td>\n<td>Default parameter values that can be overridden at runtime</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>datetime</td>\n<td>Timestamp when this pipeline version was first created</td>\n</tr>\n<tr>\n<td>version</td>\n<td>int</td>\n<td>Monotonically increasing version number for schema evolution</td>\n</tr>\n</tbody></table>\n<p>The pipeline identifier must be globally unique within the system and should follow a hierarchical naming convention that reflects organizational structure and purpose. The version field enables schema evolution and rollback capabilities, while the parameters dictionary provides a mechanism for runtime customization without modifying the core pipeline definition.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Pipeline definitions are immutable once created. Any modification creates a new version, ensuring that running pipelines complete with their original logic while new runs use updated definitions.</p>\n</blockquote>\n<h4 id=\"taskdefinition-structure\">TaskDefinition Structure</h4>\n<p>Individual tasks represent atomic units of work within the pipeline, each encapsulating a specific transformation, extraction, or loading operation. Tasks must be designed to be idempotent and independently executable to support retry and recovery scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>id</td>\n<td>str</td>\n<td>Unique task identifier within the pipeline scope</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable task name for monitoring and debugging</td>\n</tr>\n<tr>\n<td>type</td>\n<td>str</td>\n<td>Task type identifier that determines execution behavior</td>\n</tr>\n<tr>\n<td>config</td>\n<td>dict</td>\n<td>Task-specific configuration parameters and connection details</td>\n</tr>\n<tr>\n<td>dependencies</td>\n<td>List[str]</td>\n<td>List of upstream task IDs that must complete before this task</td>\n</tr>\n<tr>\n<td>retry_policy</td>\n<td>RetryPolicy</td>\n<td>Configuration for automatic retry behavior on failure</td>\n</tr>\n<tr>\n<td>timeout_seconds</td>\n<td>int</td>\n<td>Maximum execution time before task is considered failed</td>\n</tr>\n</tbody></table>\n<p>The task type field determines which executor will handle the task and defines the expected structure of the config dictionary. Common task types include <code>sql_transform</code>, <code>api_extract</code>, <code>file_load</code>, and <code>python_udf</code>. The dependencies list establishes the DAG structure and must be validated to ensure no cycles exist.</p>\n<blockquote>\n<p><strong>Architecture Decision: Task Configuration Flexibility</strong></p>\n<ul>\n<li><strong>Context</strong>: Tasks need varying configuration parameters based on their type, but we want type safety and validation</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Strongly typed task subclasses with specific fields</li>\n<li>Generic config dictionary with runtime validation</li>\n<li>JSON schema validation for each task type</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Generic config dictionary with pluggable validation per task type</li>\n<li><strong>Rationale</strong>: Provides maximum flexibility for custom task types while maintaining validation. Easier to extend than rigid inheritance hierarchies.</li>\n<li><strong>Consequences</strong>: Requires runtime validation and clear documentation of expected config schemas per task type</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Strongly typed subclasses</td>\n<td>Compile-time safety, clear API</td>\n<td>Rigid, hard to extend, complex inheritance</td>\n</tr>\n<tr>\n<td>Generic config dict</td>\n<td>Flexible, easy custom tasks</td>\n<td>Runtime validation only, potential config errors</td>\n</tr>\n<tr>\n<td>JSON schema validation</td>\n<td>Good balance, clear contracts</td>\n<td>Additional schema maintenance overhead</td>\n</tr>\n</tbody></table>\n<h4 id=\"retrypolicy-configuration\">RetryPolicy Configuration</h4>\n<p>Retry policies define how the system should respond to task failures, balancing between automatic recovery and avoiding infinite retry loops. The policy configuration must be expressive enough to handle different failure types with appropriate retry strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>max_attempts</td>\n<td>int</td>\n<td>Total number of execution attempts including the initial run</td>\n</tr>\n<tr>\n<td>backoff_seconds</td>\n<td>int</td>\n<td>Base delay between retry attempts</td>\n</tr>\n<tr>\n<td>exponential_backoff</td>\n<td>bool</td>\n<td>Whether to double the delay after each failure</td>\n</tr>\n<tr>\n<td>retry_on_error_types</td>\n<td>List[str]</td>\n<td>Specific error categories that should trigger retries</td>\n</tr>\n</tbody></table>\n<p>The retry policy enables sophisticated failure handling by distinguishing between transient errors (network timeouts, temporary resource unavailability) and permanent errors (authentication failures, malformed queries). The <code>retry_on_error_types</code> field allows fine-grained control over which failures justify retry attempts.</p>\n<h4 id=\"dependency-resolution-and-validation\">Dependency Resolution and Validation</h4>\n<p>Pipeline definitions undergo comprehensive validation during registration to catch configuration errors early and ensure reliable execution. The validation process examines both individual task configurations and the overall pipeline structure.</p>\n<p><strong>Pipeline Validation Steps:</strong></p>\n<ol>\n<li><strong>Task ID Uniqueness</strong>: Verify that all task IDs within a pipeline are unique and follow naming conventions</li>\n<li><strong>Dependency Reference Validation</strong>: Confirm that all task dependencies refer to valid task IDs within the same pipeline</li>\n<li><strong>Cycle Detection</strong>: Perform topological analysis to ensure the dependency graph forms a valid DAG with no cycles</li>\n<li><strong>Task Configuration Validation</strong>: Validate each task&#39;s config dictionary against its type-specific schema</li>\n<li><strong>Resource Requirement Analysis</strong>: Check that resource requirements don&#39;t exceed system capacity limits</li>\n<li><strong>Parameter Substitution Verification</strong>: Ensure all parameter references in task configs have corresponding defaults or required parameters</li>\n</ol>\n<p>The <code>validate_pipeline(pipeline) -&gt; List[str]</code> function performs this comprehensive validation and returns a list of human-readable error messages. An empty list indicates a valid pipeline ready for execution.</p>\n<blockquote>\n<p><strong>Common Pitfall</strong>: Circular Dependencies</p>\n<p> <strong>Pitfall: Hidden Circular Dependencies</strong>\nDevelopers often create circular dependencies through indirect chains (ABCA) that aren&#39;t obvious in large pipelines. The validation must detect these transitively, not just check immediate dependencies. Always run full cycle detection using depth-first search with a visited set to catch these subtle cycles.</p>\n</blockquote>\n<h3 id=\"runtime-state-model\">Runtime State Model</h3>\n<p>The runtime state model tracks the dynamic execution of pipeline definitions, maintaining detailed state information for active runs, completed executions, and failed attempts. This model must support concurrent access from multiple system components while ensuring consistency and auditability.</p>\n<h4 id=\"pipeline-run-lifecycle\">Pipeline Run Lifecycle</h4>\n<p>Each pipeline execution creates a <code>PipelineRun</code> instance that tracks the overall execution progress and coordinates individual task executions. Pipeline runs serve as the primary unit for monitoring, alerting, and historical analysis.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>run_id</td>\n<td>str</td>\n<td>Unique identifier for this specific pipeline execution</td>\n</tr>\n<tr>\n<td>pipeline_id</td>\n<td>str</td>\n<td>Reference to the pipeline definition being executed</td>\n</tr>\n<tr>\n<td>pipeline_version</td>\n<td>int</td>\n<td>Version of pipeline definition used for this run</td>\n</tr>\n<tr>\n<td>triggered_by</td>\n<td>str</td>\n<td>Identifier of user, scheduler, or event that initiated the run</td>\n</tr>\n<tr>\n<td>triggered_at</td>\n<td>datetime</td>\n<td>Timestamp when the run was initiated</td>\n</tr>\n<tr>\n<td>started_at</td>\n<td>datetime</td>\n<td>Timestamp when first task began execution</td>\n</tr>\n<tr>\n<td>completed_at</td>\n<td>datetime</td>\n<td>Timestamp when all tasks finished (success or failure)</td>\n</tr>\n<tr>\n<td>state</td>\n<td>PipelineRunState</td>\n<td>Overall run state (PENDING, RUNNING, SUCCESS, FAILED, CANCELLED)</td>\n</tr>\n<tr>\n<td>parameters</td>\n<td>dict</td>\n<td>Runtime parameters used for this execution</td>\n</tr>\n<tr>\n<td>task_executions</td>\n<td>List[TaskExecution]</td>\n<td>All task execution instances for this run</td>\n</tr>\n</tbody></table>\n<p>The pipeline run maintains the execution context and provides a consistent view of progress across all constituent tasks. The <code>pipeline_version</code> field ensures that runs can be analyzed against the correct pipeline definition even after newer versions are deployed.</p>\n<h4 id=\"task-execution-state-management\">Task Execution State Management</h4>\n<p>Individual task executions track the detailed progress of each task within a pipeline run. The state model must handle complex scenarios including retries, cancellations, and dependency failures while maintaining a clear audit trail.</p>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Ftask-state-machine.svg\" alt=\"Task State Transitions\"></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>task_id</td>\n<td>str</td>\n<td>Reference to task definition within the pipeline</td>\n</tr>\n<tr>\n<td>pipeline_run_id</td>\n<td>str</td>\n<td>Reference to parent pipeline run</td>\n</tr>\n<tr>\n<td>state</td>\n<td>TaskState</td>\n<td>Current execution state of this task</td>\n</tr>\n<tr>\n<td>attempt_count</td>\n<td>int</td>\n<td>Number of execution attempts including current attempt</td>\n</tr>\n<tr>\n<td>started_at</td>\n<td>datetime</td>\n<td>Timestamp when task execution began</td>\n</tr>\n<tr>\n<td>completed_at</td>\n<td>datetime</td>\n<td>Timestamp when task finished (success or failure)</td>\n</tr>\n<tr>\n<td>error_message</td>\n<td>str</td>\n<td>Detailed error description for failed executions</td>\n</tr>\n<tr>\n<td>logs</td>\n<td>List[str]</td>\n<td>Chronological log entries from task execution</td>\n</tr>\n<tr>\n<td>metrics</td>\n<td>Dict[str,float]</td>\n<td>Performance metrics (duration, records processed, etc.)</td>\n</tr>\n</tbody></table>\n<p>The <code>TaskExecution</code> model captures both the current state and the complete execution history for each task attempt. This granular tracking enables detailed debugging and performance analysis while supporting retry logic and failure recovery.</p>\n<h4 id=\"state-transition-logic\">State Transition Logic</h4>\n<p>Task state transitions follow a deterministic state machine that ensures consistent behavior across different execution scenarios. The transition function <code>transition(execution, event, error_message) -&gt; bool</code> attempts to move a task execution to the appropriate next state based on the triggering event.</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Action Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PENDING</td>\n<td>DEPENDENCIES_MET</td>\n<td>QUEUED</td>\n<td>Add to execution queue</td>\n</tr>\n<tr>\n<td>QUEUED</td>\n<td>EXECUTION_STARTED</td>\n<td>RUNNING</td>\n<td>Update started_at timestamp</td>\n</tr>\n<tr>\n<td>RUNNING</td>\n<td>EXECUTION_COMPLETED</td>\n<td>SUCCESS</td>\n<td>Update completed_at, record metrics</td>\n</tr>\n<tr>\n<td>RUNNING</td>\n<td>EXECUTION_FAILED</td>\n<td>FAILED or RETRYING</td>\n<td>Check retry policy, increment attempt_count</td>\n</tr>\n<tr>\n<td>RETRYING</td>\n<td>EXECUTION_STARTED</td>\n<td>RUNNING</td>\n<td>Begin new execution attempt</td>\n</tr>\n<tr>\n<td>RUNNING</td>\n<td>CANCELLED_BY_USER</td>\n<td>CANCELLED</td>\n<td>Terminate execution, cleanup resources</td>\n</tr>\n<tr>\n<td>WAITING</td>\n<td>UPSTREAM_FAILED</td>\n<td>SKIPPED</td>\n<td>Mark as skipped due to dependency failure</td>\n</tr>\n<tr>\n<td>FAILED</td>\n<td>MAX_RETRIES_EXCEEDED</td>\n<td>FAILED</td>\n<td>Final failure state, no more retries</td>\n</tr>\n</tbody></table>\n<p>The state machine enforces valid transitions and prevents invalid state changes that could compromise system integrity. Each transition is atomic and logged for audit purposes.</p>\n<blockquote>\n<p><strong>Architecture Decision: State Transition Atomicity</strong></p>\n<ul>\n<li><strong>Context</strong>: State transitions must be atomic to prevent race conditions in concurrent execution environments</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Database transactions for each state change</li>\n<li>In-memory locks with periodic persistence</li>\n<li>Event sourcing with append-only log</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Database transactions with optimistic locking for state changes</li>\n<li><strong>Rationale</strong>: Provides ACID guarantees while supporting distributed execution. Optimistic locking prevents contention in normal cases.</li>\n<li><strong>Consequences</strong>: Requires retry logic for concurrent updates, but ensures consistency and supports system recovery</li>\n</ul>\n</blockquote>\n<h4 id=\"execution-metrics-and-observability\">Execution Metrics and Observability</h4>\n<p>The runtime state model captures comprehensive metrics to enable monitoring, alerting, and performance optimization. Metrics are collected at both the task and pipeline level to provide different granularities of visibility.</p>\n<p><strong>Task-Level Metrics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>execution_duration_seconds</td>\n<td>float</td>\n<td>Wall-clock time from start to completion</td>\n</tr>\n<tr>\n<td>cpu_usage_percent</td>\n<td>float</td>\n<td>Average CPU utilization during execution</td>\n</tr>\n<tr>\n<td>memory_usage_mb</td>\n<td>float</td>\n<td>Peak memory consumption during execution</td>\n</tr>\n<tr>\n<td>records_processed</td>\n<td>float</td>\n<td>Number of data records processed by the task</td>\n</tr>\n<tr>\n<td>bytes_processed</td>\n<td>float</td>\n<td>Total bytes read and written by the task</td>\n</tr>\n<tr>\n<td>error_count</td>\n<td>float</td>\n<td>Number of recoverable errors encountered during execution</td>\n</tr>\n</tbody></table>\n<p><strong>Pipeline-Level Metrics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>total_duration_seconds</td>\n<td>float</td>\n<td>End-to-end pipeline execution time</td>\n</tr>\n<tr>\n<td>task_success_count</td>\n<td>float</td>\n<td>Number of tasks that completed successfully</td>\n</tr>\n<tr>\n<td>task_failure_count</td>\n<td>float</td>\n<td>Number of tasks that failed after all retries</td>\n</tr>\n<tr>\n<td>critical_path_duration</td>\n<td>float</td>\n<td>Duration of longest task dependency chain</td>\n</tr>\n<tr>\n<td>parallelism_achieved</td>\n<td>float</td>\n<td>Average number of tasks running concurrently</td>\n</tr>\n<tr>\n<td>data_freshness_hours</td>\n<td>float</td>\n<td>Age of the oldest input data processed</td>\n</tr>\n</tbody></table>\n<p>These metrics enable both real-time monitoring and historical analysis for capacity planning and performance optimization.</p>\n<h3 id=\"metadata-and-lineage\">Metadata and Lineage</h3>\n<p>The metadata and lineage system maintains comprehensive provenance information that tracks how data flows through the system, enabling compliance, debugging, and impact analysis. This system captures both technical lineage (what transformations were applied) and business lineage (what business processes were affected).</p>\n<h4 id=\"data-lineage-tracking\">Data Lineage Tracking</h4>\n<p>Data lineage captures the complete transformation history of data as it moves through ETL pipelines, creating an auditable trail that can answer questions like &quot;where did this data come from?&quot; and &quot;what processes will be affected if this source changes?&quot;</p>\n<p><strong>Dataset Registration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>dataset_id</td>\n<td>str</td>\n<td>Unique identifier for the logical dataset</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable dataset name</td>\n</tr>\n<tr>\n<td>schema_version</td>\n<td>int</td>\n<td>Current schema version number</td>\n</tr>\n<tr>\n<td>location</td>\n<td>str</td>\n<td>Physical location (table name, file path, etc.)</td>\n</tr>\n<tr>\n<td>owner</td>\n<td>str</td>\n<td>Business owner responsible for dataset quality</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>List[str]</td>\n<td>Metadata tags for discovery and classification</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>datetime</td>\n<td>When this dataset was first registered</td>\n</tr>\n<tr>\n<td>last_updated</td>\n<td>datetime</td>\n<td>Most recent data refresh timestamp</td>\n</tr>\n</tbody></table>\n<p><strong>Lineage Edge Tracking:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>source_dataset_id</td>\n<td>str</td>\n<td>Dataset that provides input data</td>\n</tr>\n<tr>\n<td>target_dataset_id</td>\n<td>str</td>\n<td>Dataset that receives transformed data</td>\n</tr>\n<tr>\n<td>pipeline_id</td>\n<td>str</td>\n<td>Pipeline that performs the transformation</td>\n</tr>\n<tr>\n<td>task_id</td>\n<td>str</td>\n<td>Specific task within pipeline that creates the relationship</td>\n</tr>\n<tr>\n<td>transformation_type</td>\n<td>str</td>\n<td>Type of transformation applied (JOIN, AGGREGATE, FILTER, etc.)</td>\n</tr>\n<tr>\n<td>column_mappings</td>\n<td>Dict[str,str]</td>\n<td>Mapping from source columns to target columns</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>datetime</td>\n<td>When this lineage relationship was established</td>\n</tr>\n</tbody></table>\n<p>The lineage system builds a directed graph of data dependencies that can be traversed in both directions to understand upstream sources and downstream consumers. This graph supports impact analysis when schema changes or data quality issues are detected.</p>\n<h4 id=\"schema-evolution-and-versioning\">Schema Evolution and Versioning</h4>\n<p>Schema evolution tracking ensures that changes to data structures are managed safely without breaking downstream consumers. The system maintains a complete history of schema changes and provides compatibility checking for pipeline modifications.</p>\n<p><strong>Schema Version History:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>dataset_id</td>\n<td>str</td>\n<td>Reference to the dataset being versioned</td>\n</tr>\n<tr>\n<td>version</td>\n<td>int</td>\n<td>Monotonically increasing version number</td>\n</tr>\n<tr>\n<td>schema_definition</td>\n<td>dict</td>\n<td>Complete schema specification (columns, types, constraints)</td>\n</tr>\n<tr>\n<td>change_type</td>\n<td>str</td>\n<td>Type of change (ADD_COLUMN, DROP_COLUMN, CHANGE_TYPE, etc.)</td>\n</tr>\n<tr>\n<td>change_description</td>\n<td>str</td>\n<td>Human-readable description of what changed</td>\n</tr>\n<tr>\n<td>compatibility</td>\n<td>str</td>\n<td>Backward compatibility assessment (COMPATIBLE, BREAKING, UNKNOWN)</td>\n</tr>\n<tr>\n<td>applied_by</td>\n<td>str</td>\n<td>User or system that applied this schema change</td>\n</tr>\n<tr>\n<td>applied_at</td>\n<td>datetime</td>\n<td>Timestamp when change was applied</td>\n</tr>\n</tbody></table>\n<p><strong>Schema Compatibility Rules:</strong></p>\n<ol>\n<li><strong>Backward Compatible Changes</strong>: Adding optional columns, relaxing constraints, adding enum values</li>\n<li><strong>Breaking Changes</strong>: Removing columns, changing data types, adding required columns, tightening constraints</li>\n<li><strong>Forward Compatible Changes</strong>: Changes that older readers can safely ignore</li>\n</ol>\n<p>The schema evolution system automatically analyzes proposed changes and identifies potentially affected downstream consumers, enabling proactive communication and migration planning.</p>\n<blockquote>\n<p><strong>Architecture Decision: Schema Evolution Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support schema changes without breaking existing pipelines while maintaining data quality</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Strict immutability - never change schemas</li>\n<li>Automatic migration with compatibility checking</li>\n<li>Explicit versioning with parallel schema support</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Explicit versioning with automated compatibility analysis and migration assistance</li>\n<li><strong>Rationale</strong>: Balances safety with flexibility. Provides clear upgrade paths while catching breaking changes early.</li>\n<li><strong>Consequences</strong>: Requires schema registry maintenance and coordination between teams for breaking changes</li>\n</ul>\n</blockquote>\n<h4 id=\"audit-trail-and-compliance\">Audit Trail and Compliance</h4>\n<p>The audit trail maintains a complete record of all system activities for compliance, debugging, and security analysis. This includes user actions, system events, and data access patterns that might be required for regulatory compliance.</p>\n<p><strong>Audit Event Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>event_id</td>\n<td>str</td>\n<td>Unique identifier for this audit event</td>\n</tr>\n<tr>\n<td>event_type</td>\n<td>str</td>\n<td>Category of event (USER_ACTION, SYSTEM_EVENT, DATA_ACCESS)</td>\n</tr>\n<tr>\n<td>actor</td>\n<td>str</td>\n<td>User, service, or system component that initiated the action</td>\n</tr>\n<tr>\n<td>resource_type</td>\n<td>str</td>\n<td>Type of resource affected (PIPELINE, TASK, DATASET)</td>\n</tr>\n<tr>\n<td>resource_id</td>\n<td>str</td>\n<td>Identifier of specific resource instance</td>\n</tr>\n<tr>\n<td>action</td>\n<td>str</td>\n<td>Specific action performed (CREATE, UPDATE, DELETE, EXECUTE, READ)</td>\n</tr>\n<tr>\n<td>timestamp</td>\n<td>datetime</td>\n<td>Precise timestamp when event occurred</td>\n</tr>\n<tr>\n<td>ip_address</td>\n<td>str</td>\n<td>Network address of request origin</td>\n</tr>\n<tr>\n<td>session_id</td>\n<td>str</td>\n<td>Session identifier for grouping related actions</td>\n</tr>\n<tr>\n<td>details</td>\n<td>dict</td>\n<td>Action-specific details and parameters</td>\n</tr>\n<tr>\n<td>result</td>\n<td>str</td>\n<td>Outcome of the action (SUCCESS, FAILURE, PARTIAL)</td>\n</tr>\n</tbody></table>\n<p><strong>Common Audit Event Types:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Actor</th>\n<th>Action</th>\n<th>Details Captured</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Execution</td>\n<td>System</td>\n<td>EXECUTE</td>\n<td>Run parameters, duration, success/failure</td>\n</tr>\n<tr>\n<td>Schema Change</td>\n<td>User</td>\n<td>UPDATE</td>\n<td>Old/new schemas, compatibility assessment</td>\n</tr>\n<tr>\n<td>Data Access</td>\n<td>Pipeline/User</td>\n<td>READ</td>\n<td>Query executed, rows returned, data volume</td>\n</tr>\n<tr>\n<td>Configuration Change</td>\n<td>User</td>\n<td>UPDATE</td>\n<td>Before/after configuration, change reason</td>\n</tr>\n<tr>\n<td>User Authentication</td>\n<td>User</td>\n<td>LOGIN/LOGOUT</td>\n<td>Authentication method, success/failure reason</td>\n</tr>\n</tbody></table>\n<p>The audit trail supports compliance with regulations like GDPR, SOX, and industry-specific requirements by maintaining detailed records of who accessed what data when, and what changes were made to processing logic.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Audit events are write-only and immutable once created. They use append-only storage with cryptographic integrity checking to prevent tampering and ensure compliance requirements are met.</p>\n</blockquote>\n<h4 id=\"data-quality-and-validation-history\">Data Quality and Validation History</h4>\n<p>The system tracks data quality metrics and validation results over time to identify trends, detect anomalies, and ensure data reliability for downstream consumers. This historical view enables proactive data quality management and root cause analysis.</p>\n<p><strong>Data Quality Metrics:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>dataset_id</td>\n<td>str</td>\n<td>Dataset being measured</td>\n</tr>\n<tr>\n<td>pipeline_run_id</td>\n<td>str</td>\n<td>Pipeline run that generated these metrics</td>\n</tr>\n<tr>\n<td>metric_name</td>\n<td>str</td>\n<td>Name of quality metric (COMPLETENESS, ACCURACY, CONSISTENCY)</td>\n</tr>\n<tr>\n<td>metric_value</td>\n<td>float</td>\n<td>Measured value for this metric</td>\n</tr>\n<tr>\n<td>threshold_min</td>\n<td>float</td>\n<td>Minimum acceptable value for this metric</td>\n</tr>\n<tr>\n<td>threshold_max</td>\n<td>float</td>\n<td>Maximum acceptable value for this metric</td>\n</tr>\n<tr>\n<td>status</td>\n<td>str</td>\n<td>Quality status (PASS, WARN, FAIL) based on thresholds</td>\n</tr>\n<tr>\n<td>measured_at</td>\n<td>datetime</td>\n<td>When this measurement was taken</td>\n</tr>\n<tr>\n<td>sample_size</td>\n<td>int</td>\n<td>Number of records included in measurement</td>\n</tr>\n</tbody></table>\n<p><strong>Validation Rule Results:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>rule_id</td>\n<td>str</td>\n<td>Identifier for the validation rule</td>\n</tr>\n<tr>\n<td>dataset_id</td>\n<td>str</td>\n<td>Dataset being validated</td>\n</tr>\n<tr>\n<td>rule_type</td>\n<td>str</td>\n<td>Type of validation (NOT_NULL, RANGE_CHECK, FORMAT_VALIDATION)</td>\n</tr>\n<tr>\n<td>rule_expression</td>\n<td>str</td>\n<td>Actual validation logic or SQL expression</td>\n</tr>\n<tr>\n<td>records_checked</td>\n<td>int</td>\n<td>Total number of records evaluated</td>\n</tr>\n<tr>\n<td>records_passed</td>\n<td>int</td>\n<td>Number of records that satisfied the rule</td>\n</tr>\n<tr>\n<td>records_failed</td>\n<td>int</td>\n<td>Number of records that violated the rule</td>\n</tr>\n<tr>\n<td>failure_examples</td>\n<td>List[dict]</td>\n<td>Sample records that failed validation</td>\n</tr>\n<tr>\n<td>execution_time_ms</td>\n<td>float</td>\n<td>Time taken to execute this validation rule</td>\n</tr>\n</tbody></table>\n<p>This quality tracking enables automated alerting when data quality degrades and provides historical context for understanding data reliability trends over time.</p>\n<p> <strong>Pitfall: Metadata Storage Performance</strong></p>\n<p>Lineage and audit systems generate high-volume write workloads that can impact operational system performance. Design these systems with separate storage infrastructure and asynchronous processing to avoid slowing down pipeline execution. Consider using time-series databases or append-only storage optimized for write-heavy workloads.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data model implementation requires careful consideration of storage technologies, access patterns, and consistency requirements. The following guidance provides practical recommendations for implementing a production-ready system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Definitions</td>\n<td>JSON files + file system</td>\n<td>PostgreSQL with versioning</td>\n</tr>\n<tr>\n<td>Runtime State</td>\n<td>SQLite with WAL mode</td>\n<td>PostgreSQL with connection pooling</td>\n</tr>\n<tr>\n<td>Metrics Storage</td>\n<td>In-memory + periodic dumps</td>\n<td>InfluxDB or TimescaleDB</td>\n</tr>\n<tr>\n<td>Lineage Graph</td>\n<td>PostgreSQL with recursive queries</td>\n<td>Neo4j graph database</td>\n</tr>\n<tr>\n<td>Audit Log</td>\n<td>Structured log files</td>\n<td>Elasticsearch with retention policies</td>\n</tr>\n<tr>\n<td>Schema Registry</td>\n<td>JSON Schema + file storage</td>\n<td>Confluent Schema Registry</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">etl_system</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\"> models</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline.py          </span><span style=\"color:#6A737D\"># PipelineDefinition, TaskDefinition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    execution.py         </span><span style=\"color:#6A737D\"># TaskExecution, PipelineRun</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lineage.py          </span><span style=\"color:#6A737D\"># Dataset, LineageEdge</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    audit.py            </span><span style=\"color:#6A737D\"># AuditEvent, QualityMetric</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\"> storage</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_store.py    </span><span style=\"color:#6A737D\"># Pipeline CRUD operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    execution_store.py   </span><span style=\"color:#6A737D\"># Runtime state management</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata_store.py    </span><span style=\"color:#6A737D\"># Lineage and audit storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\"> validation</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_validator.py </span><span style=\"color:#6A737D\"># Pipeline validation logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schema_validator.py  </span><span style=\"color:#6A737D\"># Schema compatibility checking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\"> migrations</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">     </span><span style=\"color:#FDAEB7;font-style:italic\">001_initial_schema</span><span style=\"color:#E1E4E8\">.sql</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">     </span><span style=\"color:#FDAEB7;font-style:italic\">002_add_lineage</span><span style=\"color:#E1E4E8\">.sql</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">     </span><span style=\"color:#FDAEB7;font-style:italic\">003_audit_indices</span><span style=\"color:#E1E4E8\">.sql</span></span></code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Database Schema Setup (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># storage/schema.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sqlalchemy </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> create_engine, Column, String, Integer, DateTime, Float, </span><span style=\"color:#79B8FF\">JSON</span><span style=\"color:#E1E4E8\">, Text, Boolean</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sqlalchemy.ext.declarative </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> declarative_base</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sqlalchemy.orm </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sessionmaker, relationship</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> sqlalchemy.dialects.postgresql </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> UUID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">Base </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> declarative_base()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineDefinitionModel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Base</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    __tablename__ </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'pipeline_definitions'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">primary_key</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(Text)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schedule </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tasks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(</span><span style=\"color:#79B8FF\">JSON</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Serialized TaskDefinition list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(</span><span style=\"color:#79B8FF\">JSON</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(DateTime, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(Integer, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Relationships</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_runs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> relationship(</span><span style=\"color:#9ECBFF\">\"PipelineRunModel\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">back_populates</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"pipeline\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineRunModel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Base</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    __tablename__ </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'pipeline_runs'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">primary_key</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_version </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(Integer, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    triggered_by </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    triggered_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(DateTime, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(DateTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(DateTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'PENDING'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(</span><span style=\"color:#79B8FF\">JSON</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Relationships</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> relationship(</span><span style=\"color:#9ECBFF\">\"PipelineDefinitionModel\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">back_populates</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"pipeline_runs\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_executions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> relationship(</span><span style=\"color:#9ECBFF\">\"TaskExecutionModel\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">back_populates</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"pipeline_run\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskExecutionModel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Base</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    __tablename__ </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'task_executions'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    execution_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">primary_key</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(String, </span><span style=\"color:#FFAB70\">nullable</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'PENDING'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(Integer, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(DateTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(DateTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(Text)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(</span><span style=\"color:#79B8FF\">JSON</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># List of log entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Column(</span><span style=\"color:#79B8FF\">JSON</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Relationships</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> relationship(</span><span style=\"color:#9ECBFF\">\"PipelineRunModel\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">back_populates</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"task_executions\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Database connection and session management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_database_engine</span><span style=\"color:#E1E4E8\">(connection_string: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create SQLAlchemy engine with connection pooling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    engine </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_engine(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connection_string,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        pool_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        max_overflow</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        pool_pre_ping</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Verify connections before use</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        echo</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#6A737D\">  # Set to True for SQL debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> engine</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_session_factory</span><span style=\"color:#E1E4E8\">(engine):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create session factory for database operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> sessionmaker(</span><span style=\"color:#FFAB70\">bind</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">engine, </span><span style=\"color:#FFAB70\">expire_on_commit</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> initialize_database</span><span style=\"color:#E1E4E8\">(engine):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create all tables and initial data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Base.metadata.create_all(engine)</span></span></code></pre></div>\n\n<p><strong>State Management Utilities (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># models/state.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Set, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"PENDING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WAITING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"WAITING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    QUEUED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"QUEUED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RUNNING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SUCCESS\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"FAILED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRYING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RETRYING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CANCELLED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SKIPPED\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskEvent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEPENDENCIES_MET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"DEPENDENCIES_MET\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_STARTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"EXECUTION_STARTED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"EXECUTION_COMPLETED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"EXECUTION_FAILED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RETRY_SCHEDULED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MAX_RETRIES_EXCEEDED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"MAX_RETRIES_EXCEEDED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED_BY_USER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CANCELLED_BY_USER\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPSTREAM_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"UPSTREAM_FAILED\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># State transition mapping - defines valid transitions</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TRANSITIONS</span><span style=\"color:#E1E4E8\">: Dict[TaskState, Dict[TaskEvent, TaskState]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">DEPENDENCIES_MET</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">WAITING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">DEPENDENCIES_MET</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">QUEUED</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_COMPLETED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Will be modified by retry logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">MAX_RETRIES_EXCEEDED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Terminal states have no outgoing transitions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">: {},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_terminal_states</span><span style=\"color:#E1E4E8\">() -> Set[TaskState]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Return states that represent completed execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> {TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">, TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">, TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">, TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> is_valid_transition</span><span style=\"color:#E1E4E8\">(current_state: TaskState, event: TaskEvent) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check if a state transition is valid.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> event </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> TRANSITIONS</span><span style=\"color:#E1E4E8\">.get(current_state, {})</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_next_state</span><span style=\"color:#E1E4E8\">(current_state: TaskState, event: TaskEvent) -> Optional[TaskState]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get the next state for a given current state and event.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> TRANSITIONS</span><span style=\"color:#E1E4E8\">.get(current_state, {}).get(event)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Pipeline Validation (Skeleton with TODOs):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># validation/pipeline_validator.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Set, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> models.pipeline </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineDefinition, TaskDefinition</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_pipeline</span><span style=\"color:#E1E4E8\">(pipeline: PipelineDefinition) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Validate pipeline definition and return list of error messages.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Empty list indicates valid pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate pipeline-level fields (id, name, schedule format)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use croniter library to validate cron expressions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract all task IDs and check for duplicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use set() to detect duplicates in task ID list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate each task definition individually</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Call validate_task_definition for each task</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Build dependency graph and check for cycles</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use depth-first search with visited/visiting sets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify all dependency references point to valid tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Check that every dependency ID exists in task_ids set</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check for unreachable tasks (tasks with no path from roots)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Find tasks with no dependencies, then traverse reachable tasks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_task_definition</span><span style=\"color:#E1E4E8\">(task: TaskDefinition, valid_task_ids: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate individual task definition.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check required fields are present and valid format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate task type is supported</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate retry policy parameters are reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check timeout_seconds is positive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate task-specific configuration based on task type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_cycles</span><span style=\"color:#E1E4E8\">(tasks: List[TaskDefinition]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Detect cycles in task dependency graph using DFS.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build adjacency list from task dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize visited and visiting sets for DFS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each unvisited task, start DFS traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Mark nodes as visiting when entering, visited when exiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If we encounter a visiting node, we found a cycle</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return detailed cycle information for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> find_unreachable_tasks</span><span style=\"color:#E1E4E8\">(tasks: List[TaskDefinition]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Find tasks that cannot be reached from root tasks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find root tasks (tasks with no dependencies)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Build adjacency list for forward traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Perform BFS/DFS from all root tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Mark all reachable tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of unreachable task IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span></code></pre></div>\n\n<p><strong>State Transition Logic (Skeleton with TODOs):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># models/execution.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> models.state </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TaskState, TaskEvent, </span><span style=\"color:#79B8FF\">TRANSITIONS</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> transition</span><span style=\"color:#E1E4E8\">(execution: </span><span style=\"color:#9ECBFF\">'TaskExecution'</span><span style=\"color:#E1E4E8\">, event: TaskEvent, error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Attempt state transition based on event. Returns True if transition succeeded.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Updates execution object in-place if successful.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TaskState(execution.state)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if transition is valid using TRANSITIONS mapping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use is_valid_transition helper function</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle retry logic for EXECUTION_FAILED event</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Check if task has remaining retry attempts and error type is retryable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Modify target state to RETRYING instead of FAILED if retry is appropriate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update execution object with new state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Set appropriate timestamps (started_at, completed_at)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Update error_message if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Increment attempt_count for retries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log the state transition for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Include old state, new state, event, and timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return success/failure based on whether transition was applied</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> should_retry</span><span style=\"color:#E1E4E8\">(execution: </span><span style=\"color:#9ECBFF\">'TaskExecution'</span><span style=\"color:#E1E4E8\">, error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Determine if task execution should be retried based on retry policy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if attempt_count &#x3C; retry_policy.max_attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if error type matches retry_on_error_types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return boolean indicating if retry should happen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_retry_delay</span><span style=\"color:#E1E4E8\">(execution: </span><span style=\"color:#9ECBFF\">'TaskExecution'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate delay in seconds before next retry attempt.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get base delay from retry_policy.backoff_seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply exponential backoff if retry_policy.exponential_backoff is True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add jitter to prevent thundering herd (random 20%)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return delay in seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span><span style=\"color:#6A737D\">  # Implementation goes here</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p><strong>Python-Specific Recommendations:</strong></p>\n<ul>\n<li>Use <code>SQLAlchemy</code> with <code>alembic</code> for database schema migrations and ORM mapping</li>\n<li>Use <code>pydantic</code> for data validation and serialization with automatic type checking</li>\n<li>Use <code>enum.Enum</code> for state definitions to ensure type safety and IDE support</li>\n<li>Use <code>dataclasses</code> or <code>pydantic.BaseModel</code> for data transfer objects between components</li>\n<li>Use <code>asyncio</code> and <code>asyncpg</code> for high-performance async database operations if needed</li>\n<li>Use <code>structlog</code> for structured logging with consistent field names and JSON output</li>\n<li>Use <code>pytest</code> with <code>pytest-asyncio</code> for comprehensive testing including async code paths</li>\n</ul>\n<p><strong>Database Optimization Tips:</strong></p>\n<ul>\n<li>Create indexes on frequently queried fields: <code>pipeline_run_id</code>, <code>state</code>, <code>started_at</code></li>\n<li>Use database constraints to enforce data integrity (foreign keys, check constraints)</li>\n<li>Consider partitioning large tables by time (<code>completed_at</code>) for better query performance</li>\n<li>Use connection pooling to handle concurrent database access efficiently</li>\n<li>Implement database health checks and circuit breakers for resilience</li>\n</ul>\n<p><strong>Concurrency Considerations:</strong></p>\n<ul>\n<li>Use optimistic locking for state transitions to handle concurrent updates safely</li>\n<li>Implement database transactions for operations that must be atomic</li>\n<li>Consider using database-level advisory locks for critical sections</li>\n<li>Design for idempotency - operations should be safe to retry</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1: Basic Data Model (After Pipeline Definition Implementation)</strong></p>\n<ul>\n<li><strong>Test Command</strong>: <code>python -m pytest tests/test_models.py -v</code></li>\n<li><strong>Expected Output</strong>: All basic model creation and validation tests pass</li>\n<li><strong>Manual Verification</strong>: <ol>\n<li>Create a simple pipeline definition with 3 tasks</li>\n<li>Verify cycle detection catches circular dependencies</li>\n<li>Confirm validation rejects invalid cron expressions</li>\n</ol>\n</li>\n<li><strong>Success Criteria</strong>: Pipeline definitions can be created, validated, and stored without errors</li>\n</ul>\n<p><strong>Checkpoint 2: State Management (After Runtime State Implementation)</strong></p>\n<ul>\n<li><strong>Test Command</strong>: <code>python -m pytest tests/test_state_machine.py -v</code></li>\n<li><strong>Expected Output</strong>: All state transition tests pass, invalid transitions are rejected</li>\n<li><strong>Manual Verification</strong>:<ol>\n<li>Create task execution and verify initial PENDING state</li>\n<li>Trigger valid transitions and confirm state changes</li>\n<li>Attempt invalid transitions and verify they&#39;re rejected</li>\n</ol>\n</li>\n<li><strong>Success Criteria</strong>: State machine enforces valid transitions and tracks execution history</li>\n</ul>\n<p><strong>Checkpoint 3: Metadata Integration (After Lineage Implementation)</strong></p>\n<ul>\n<li><strong>Test Command</strong>: <code>python -m pytest tests/test_lineage.py -v</code></li>\n<li><strong>Expected Output</strong>: Lineage tracking and schema evolution tests pass</li>\n<li><strong>Manual Verification</strong>:<ol>\n<li>Execute pipeline and verify lineage relationships are created</li>\n<li>Query upstream and downstream datasets for a given dataset</li>\n<li>Test schema evolution with compatible and breaking changes</li>\n</ol>\n</li>\n<li><strong>Success Criteria</strong>: Complete data provenance tracking with schema evolution support</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline validation hangs</td>\n<td>Circular dependency in large DAG</td>\n<td>Add logging to cycle detection algorithm, check task dependency chains</td>\n<td>Implement cycle detection with proper visited tracking</td>\n</tr>\n<tr>\n<td>State transitions fail silently</td>\n<td>Missing error handling in transition logic</td>\n<td>Add logging to transition function, check TRANSITIONS mapping</td>\n<td>Ensure all valid transitions are defined in state machine</td>\n</tr>\n<tr>\n<td>Database deadlocks during concurrent runs</td>\n<td>Multiple transactions updating same records</td>\n<td>Enable database deadlock logging, identify conflicting queries</td>\n<td>Use consistent lock ordering, shorter transactions</td>\n</tr>\n<tr>\n<td>Memory usage grows over time</td>\n<td>Task execution objects not garbage collected</td>\n<td>Profile memory usage, check for circular references</td>\n<td>Implement proper cleanup after task completion</td>\n</tr>\n<tr>\n<td>Lineage queries are slow</td>\n<td>Missing database indexes on relationship tables</td>\n<td>Use EXPLAIN ANALYZE on slow queries</td>\n<td>Add indexes on source_dataset_id, target_dataset_id</td>\n</tr>\n<tr>\n<td>Schema validation errors unclear</td>\n<td>Generic error messages in validation code</td>\n<td>Add detailed error context with field names and values</td>\n<td>Include specific field validation errors with suggested fixes</td>\n</tr>\n</tbody></table>\n<h2 id=\"dag-definition-and-validation-engine\">DAG Definition and Validation Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 - Pipeline DAG Definition: Implements pipeline definition with dependencies as directed acyclic graph, including DAG parsing, validation, cycle detection, and visualization capabilities.</p>\n</blockquote>\n<h3 id=\"mental-model-recipe-dependencies\">Mental Model: Recipe Dependencies</h3>\n<p>Think of an ETL pipeline like preparing a complex multi-course dinner where dishes have strict preparation dependencies. Just as you cannot plate the main course before cooking it, or start cooking before chopping ingredients, ETL tasks must execute in a precise order based on their dependencies.</p>\n<p>Consider making a Thanksgiving dinner: you must thaw the turkey before seasoning it, season it before cooking it, cook it before carving it, and carve it before serving. Meanwhile, side dishes like mashed potatoes can be prepared in parallel with the turkey cooking, but they depend on earlier steps like peeling and boiling potatoes. The cranberry sauce is completely independent and can be made at any time.</p>\n<p>This cooking analogy maps perfectly to ETL pipelines. Each recipe step is a <strong>task</strong> with specific inputs (dependencies) and outputs (results). The overall meal plan is the <strong>pipeline definition</strong>. Some tasks can run in parallel (like cooking turkey and preparing vegetables simultaneously), while others must wait for their dependencies (you cannot make gravy until the turkey drippings are ready). The critical insight is that we need to validate the entire recipe before we start cooking to ensure we never encounter impossible situations like circular dependencies (needing the gravy to cook the turkey, but needing the turkey to make the gravy).</p>\n<p>The <strong>DAG Definition and Validation Engine</strong> serves as both the head chef who designs the meal plan and the kitchen manager who ensures the recipe is logically sound before any cooking begins. It parses recipe definitions (YAML/Python configs), validates that all dependencies make sense (cycle detection), determines the optimal preparation order (topological sorting), and creates visual cooking schedules (DAG visualization) that kitchen staff can follow.</p>\n<h3 id=\"dag-parsing-and-validation\">DAG Parsing and Validation</h3>\n<p>The DAG parsing and validation system transforms human-readable pipeline definitions into validated execution graphs that the orchestration engine can safely execute. This process involves multiple stages of parsing, validation, and graph construction, each with specific responsibilities and error handling requirements.</p>\n<p><strong>Pipeline Definition Parsing</strong></p>\n<p>The system supports two primary formats for pipeline definitions: YAML configuration files and Python-based definitions. YAML provides a declarative approach suitable for data engineers who prefer configuration-over-code, while Python definitions offer programmatic flexibility for complex dynamic pipelines.</p>\n<p>The parsing engine first loads the raw configuration and performs structural validation to ensure all required fields are present and properly typed. The parser constructs <code>PipelineDefinition</code> objects that contain all necessary metadata and task specifications.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>id</td>\n<td>str</td>\n<td>Unique identifier for the pipeline across the entire system</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable display name for UI and logging purposes</td>\n</tr>\n<tr>\n<td>description</td>\n<td>str</td>\n<td>Detailed explanation of pipeline purpose and data flow</td>\n</tr>\n<tr>\n<td>schedule</td>\n<td>str</td>\n<td>Cron expression or event trigger specification for execution timing</td>\n</tr>\n<tr>\n<td>tasks</td>\n<td>List[TaskDefinition]</td>\n<td>Complete list of all tasks in the pipeline with their configurations</td>\n</tr>\n<tr>\n<td>parameters</td>\n<td>dict</td>\n<td>Global pipeline parameters that can be referenced by tasks at runtime</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>datetime</td>\n<td>Pipeline creation timestamp for versioning and audit purposes</td>\n</tr>\n<tr>\n<td>version</td>\n<td>int</td>\n<td>Pipeline version number for schema evolution and rollback support</td>\n</tr>\n</tbody></table>\n<p>Each <code>TaskDefinition</code> within the pipeline contains comprehensive metadata about individual task execution requirements and dependencies:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>id</td>\n<td>str</td>\n<td>Unique task identifier within the pipeline scope</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable task name for monitoring and debugging</td>\n</tr>\n<tr>\n<td>type</td>\n<td>str</td>\n<td>Task type indicating which executor should handle this task (extract, transform, load)</td>\n</tr>\n<tr>\n<td>config</td>\n<td>dict</td>\n<td>Task-specific configuration parameters including connection details and operations</td>\n</tr>\n<tr>\n<td>dependencies</td>\n<td>List[str]</td>\n<td>List of upstream task IDs that must complete successfully before this task runs</td>\n</tr>\n<tr>\n<td>retry_policy</td>\n<td>RetryPolicy</td>\n<td>Configuration for failure handling and automatic retry behavior</td>\n</tr>\n<tr>\n<td>timeout_seconds</td>\n<td>int</td>\n<td>Maximum execution time before the task is considered failed and terminated</td>\n</tr>\n</tbody></table>\n<p>The retry policy configuration provides fine-grained control over failure recovery behavior:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>max_attempts</td>\n<td>int</td>\n<td>Maximum number of execution attempts including the initial attempt</td>\n</tr>\n<tr>\n<td>backoff_seconds</td>\n<td>int</td>\n<td>Base delay between retry attempts in seconds</td>\n</tr>\n<tr>\n<td>exponential_backoff</td>\n<td>bool</td>\n<td>Whether to apply exponential backoff increasing delay between retries</td>\n</tr>\n<tr>\n<td>retry_on_error_types</td>\n<td>List[str]</td>\n<td>Specific error types that should trigger retries vs immediate failure</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Support Both YAML and Python Pipeline Definitions</strong></p>\n<ul>\n<li><strong>Context</strong>: Data engineers have varying preferences for configuration management - some prefer declarative YAML while others need programmatic Python flexibility</li>\n<li><strong>Options Considered</strong>: YAML-only (simple but limited), Python-only (flexible but complex), Both formats (flexible but more maintenance)</li>\n<li><strong>Decision</strong>: Support both YAML and Python definitions with a common internal representation</li>\n<li><strong>Rationale</strong>: YAML handles 80% of use cases with better readability, while Python enables complex dynamic pipelines and parameterization that configuration files cannot express</li>\n<li><strong>Consequences</strong>: Increases parser complexity but maximizes adoption across different user preferences and use cases</li>\n</ul>\n</blockquote>\n<p><strong>Dependency Graph Construction</strong></p>\n<p>After parsing individual task definitions, the engine constructs a directed graph representation where nodes represent tasks and edges represent dependencies. This graph construction process validates referential integrity by ensuring all declared dependencies reference existing tasks within the pipeline.</p>\n<p>The graph construction algorithm follows these steps:</p>\n<ol>\n<li>Create a node for each task definition using the task ID as the unique identifier</li>\n<li>Iterate through each task&#39;s dependency list and verify that each referenced task ID exists in the pipeline</li>\n<li>Create directed edges from dependency tasks to the current task (upstream  downstream)</li>\n<li>Build adjacency lists for efficient graph traversal during validation and execution planning</li>\n<li>Create reverse adjacency lists to support upstream impact analysis during failures</li>\n</ol>\n<p>The resulting graph structure supports multiple query patterns required by the orchestration engine:</p>\n<ul>\n<li><strong>Forward traversal</strong>: Finding all downstream tasks affected by a failure</li>\n<li><strong>Backward traversal</strong>: Identifying all upstream dependencies that must complete before a task can run</li>\n<li><strong>Parallel task identification</strong>: Finding tasks with satisfied dependencies that can execute simultaneously</li>\n<li><strong>Critical path analysis</strong>: Determining the longest dependency chain that affects overall pipeline completion time</li>\n</ul>\n<p><strong>Cycle Detection Algorithm</strong></p>\n<p>Cycle detection is the most critical validation step, as cyclical dependencies would cause the pipeline to deadlock indefinitely. The engine implements depth-first search (DFS) with coloring to detect cycles efficiently in O(V + E) time complexity where V is tasks and E is dependencies.</p>\n<p>The three-color DFS algorithm works as follows:</p>\n<ol>\n<li>Initialize all tasks as WHITE (unvisited)</li>\n<li>For each WHITE task, start a DFS traversal</li>\n<li>Mark the current task as GRAY (currently being processed) when entering its DFS branch</li>\n<li>Recursively visit all downstream tasks (dependencies of current task)</li>\n<li>If we encounter a GRAY task during traversal, we have found a cycle</li>\n<li>Mark the task as BLACK (completely processed) when finishing its DFS branch</li>\n<li>Report any cycles found with the specific tasks involved for debugging</li>\n</ol>\n<p>The coloring approach distinguishes between back edges (which indicate cycles) and forward/cross edges (which are harmless). This precision prevents false positives that simpler visited/unvisited algorithms might produce in complex graphs.</p>\n<blockquote>\n<p>The critical insight is that GRAY nodes represent the current DFS path stack. Finding a GRAY node means we have encountered a task that depends on itself through a chain of intermediate dependencies.</p>\n</blockquote>\n<p><strong>Validation Rule Engine</strong></p>\n<p>Beyond structural graph validation, the engine enforces semantic business rules that ensure pipeline correctness:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Description</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Task ID Uniqueness</td>\n<td>No duplicate task IDs within a pipeline</td>\n<td>Reject pipeline with specific duplicate IDs listed</td>\n</tr>\n<tr>\n<td>Dependency Existence</td>\n<td>All referenced dependencies must exist as tasks</td>\n<td>Report missing task IDs and suggest similar names</td>\n</tr>\n<tr>\n<td>Self-Dependency Prevention</td>\n<td>Tasks cannot depend on themselves directly</td>\n<td>Identify self-referential tasks and remove invalid dependencies</td>\n</tr>\n<tr>\n<td>Orphaned Task Detection</td>\n<td>All tasks except roots must have at least one path from a root</td>\n<td>Warn about unreachable tasks that will never execute</td>\n</tr>\n<tr>\n<td>Dangling Dependency Detection</td>\n<td>Dependencies must reference valid task IDs</td>\n<td>List invalid references with suggestions for correction</td>\n</tr>\n<tr>\n<td>Resource Constraint Validation</td>\n<td>Task resource requirements must not exceed system limits</td>\n<td>Warn about tasks that may fail due to resource constraints</td>\n</tr>\n</tbody></table>\n<p>The validation engine returns a structured list of errors, warnings, and suggestions rather than failing fast on the first issue. This comprehensive feedback helps pipeline developers fix multiple issues simultaneously rather than discovering them one at a time through trial and error.</p>\n<p><strong>Parameter Substitution and Templating</strong></p>\n<p>Pipeline definitions support parameterization through template variables that are resolved at parse time or runtime. The templating system enables environment-specific configurations and dynamic pipeline behavior without duplicating pipeline definitions.</p>\n<p>The parameter resolution process handles multiple scopes with clear precedence rules:</p>\n<ol>\n<li><strong>Global pipeline parameters</strong>: Defined at the pipeline level and available to all tasks</li>\n<li><strong>Task-specific parameters</strong>: Override global parameters for individual tasks</li>\n<li><strong>Runtime parameters</strong>: Provided when triggering pipeline execution to customize behavior</li>\n<li><strong>Environment variables</strong>: System environment variables available for configuration injection</li>\n</ol>\n<p>Template syntax follows standard conventions using double curly braces: <code>{{parameter_name}}</code> with support for default values: <code>{{parameter_name|default_value}}</code>. The templating engine validates that all referenced parameters are defined and substitutes values before final validation occurs.</p>\n<p><strong>Configuration Schema Validation</strong></p>\n<p>The parsing engine enforces strict schema validation to catch configuration errors early in the development cycle. JSON Schema definitions specify the exact structure, types, and constraints for pipeline and task configurations.</p>\n<p>Schema validation covers multiple levels:</p>\n<ul>\n<li><strong>Structural validation</strong>: Required fields, correct types, valid enumerations</li>\n<li><strong>Cross-field validation</strong>: Constraints that span multiple configuration fields</li>\n<li><strong>Business rule validation</strong>: Domain-specific rules like valid cron expressions and connection string formats</li>\n<li><strong>Version compatibility</strong>: Ensuring configurations are compatible with the current engine version</li>\n</ul>\n<p>The validation system provides detailed error messages with field paths, expected vs. actual values, and suggested corrections. This comprehensive feedback reduces the debugging time for pipeline developers and prevents runtime failures due to misconfiguration.</p>\n<h3 id=\"execution-order-resolution\">Execution Order Resolution</h3>\n<p>Once the DAG passes validation, the execution order resolution system determines the optimal sequence for task execution while maximizing parallelism and respecting all dependency constraints. This process involves sophisticated graph algorithms that balance execution efficiency with resource utilization.</p>\n<p><strong>Topological Sorting Algorithm</strong></p>\n<p>The foundation of execution order resolution is topological sorting, which produces a linear ordering of tasks such that for every dependency relationship, the upstream task appears before the downstream task in the sequence. The engine implements Kahn&#39;s algorithm for its stability and intuitive behavior.</p>\n<p>Kahn&#39;s algorithm operates through the following steps:</p>\n<ol>\n<li>Calculate the <strong>in-degree</strong> (number of incoming dependencies) for each task</li>\n<li>Initialize a queue with all tasks that have zero in-degree (no dependencies)</li>\n<li>While the queue is not empty, remove a task from the queue and add it to the result sequence</li>\n<li>For each downstream task dependent on the current task, decrement its in-degree</li>\n<li>If decrementing causes a downstream task&#39;s in-degree to reach zero, add it to the queue</li>\n<li>Continue until all tasks are processed or the queue becomes empty with tasks remaining</li>\n</ol>\n<p>The algorithm naturally handles parallel execution opportunities by identifying tasks with zero in-degree at each step. Tasks removed from the queue simultaneously can execute in parallel since they have no mutual dependencies.</p>\n<p><strong>Parallel Execution Planning</strong></p>\n<p>Beyond basic topological ordering, the execution planner groups tasks into <strong>execution levels</strong> that maximize parallelism while respecting resource constraints. Each execution level contains tasks that can run simultaneously without violating dependencies.</p>\n<p>The level-based execution planning algorithm works as follows:</p>\n<ol>\n<li>Start with level 0 containing all tasks with no dependencies</li>\n<li>For each subsequent level, include tasks whose dependencies are all satisfied by previous levels</li>\n<li>Continue creating levels until all tasks are assigned</li>\n<li>Within each level, apply resource-based scheduling to avoid oversubscription</li>\n<li>Generate execution plan with explicit parallelism annotations</li>\n</ol>\n<p>This approach provides several advantages over simple topological sorting:</p>\n<ul>\n<li><strong>Resource optimization</strong>: Tasks within a level can be scheduled based on available CPU, memory, and I/O capacity</li>\n<li><strong>Failure isolation</strong>: If a task fails, only subsequent levels containing dependent tasks need to be cancelled</li>\n<li><strong>Progress visualization</strong>: Users can see pipeline progress as completion percentages within each level</li>\n<li><strong>Dynamic rescheduling</strong>: Failed tasks can be retried without recalculating the entire execution plan</li>\n</ul>\n<p><strong>Critical Path Analysis</strong></p>\n<p>The execution planner performs critical path analysis to identify the longest dependency chain that determines minimum pipeline completion time. This analysis helps with resource allocation decisions and provides realistic completion time estimates.</p>\n<p>Critical path calculation involves:</p>\n<ol>\n<li>Calculate the <strong>longest path</strong> from each task to any terminal task (task with no downstream dependencies)</li>\n<li>Identify tasks that lie on the critical path - any delay in these tasks delays the entire pipeline</li>\n<li>Prioritize critical path tasks for resource allocation and monitoring</li>\n<li>Provide completion time estimates based on critical path length and historical task durations</li>\n</ol>\n<p>Tasks not on the critical path have <strong>slack time</strong> - the amount they can be delayed without affecting overall pipeline completion. The scheduler uses slack time information to optimize resource utilization and handle transient failures gracefully.</p>\n<p><strong>Dynamic Dependency Resolution</strong></p>\n<p>Some pipelines require dynamic dependency resolution where task dependencies are determined at runtime based on data characteristics or external conditions. The execution planner supports conditional dependencies through predicate evaluation.</p>\n<p>Dynamic dependency types include:</p>\n<table>\n<thead>\n<tr>\n<th>Dependency Type</th>\n<th>Description</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Conditional Dependencies</td>\n<td>Dependencies that apply only when specific conditions are met</td>\n<td>Evaluate predicates before adding edges to execution graph</td>\n</tr>\n<tr>\n<td>Data-Driven Dependencies</td>\n<td>Dependencies determined by upstream task output characteristics</td>\n<td>Re-evaluate dependency graph after each task completion</td>\n</tr>\n<tr>\n<td>External Dependencies</td>\n<td>Dependencies on external systems or scheduled events</td>\n<td>Poll external conditions before marking dependencies as satisfied</td>\n</tr>\n<tr>\n<td>Parameterized Dependencies</td>\n<td>Dependencies that vary based on pipeline parameters</td>\n<td>Resolve parameter values before constructing dependency graph</td>\n</tr>\n</tbody></table>\n<p>The dynamic resolution system maintains the execution graph as a mutable structure that can be safely modified during pipeline execution while preserving consistency and preventing cycles.</p>\n<p><strong>Resource-Aware Scheduling</strong></p>\n<p>The execution planner incorporates resource constraints to prevent system overload and ensure stable pipeline execution. Resource-aware scheduling considers multiple constraint types:</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Constraint</th>\n<th>Scheduling Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU Cores</td>\n<td>Maximum concurrent CPU-intensive tasks</td>\n<td>Queue CPU-bound tasks when core limit reached</td>\n</tr>\n<tr>\n<td>Memory</td>\n<td>Total memory allocation across running tasks</td>\n<td>Delay memory-intensive tasks until sufficient memory available</td>\n</tr>\n<tr>\n<td>Database Connections</td>\n<td>Connection pool limits per database</td>\n<td>Serialize tasks accessing the same database when pool exhausted</td>\n</tr>\n<tr>\n<td>API Rate Limits</td>\n<td>External API call quotas and rate limits</td>\n<td>Space API-dependent tasks to respect rate limiting windows</td>\n</tr>\n<tr>\n<td>File System I/O</td>\n<td>Concurrent file operations and disk bandwidth</td>\n<td>Balance file-intensive tasks across available storage devices</td>\n</tr>\n</tbody></table>\n<p>The scheduler maintains resource allocation tracking and updates availability as tasks start and complete. This real-time resource management prevents cascading failures due to resource exhaustion while maximizing system utilization.</p>\n<p><strong>Execution Plan Optimization</strong></p>\n<p>The final execution plan undergoes optimization to improve overall pipeline performance and resource efficiency. Optimization techniques include:</p>\n<p><strong>Task Batching</strong>: Small tasks with similar resource requirements are batched together to reduce orchestration overhead and improve resource locality.</p>\n<p><strong>Prefetching</strong>: The planner identifies opportunities to prefetch data or establish connections before tasks need them, reducing task startup latency.</p>\n<p><strong>Load Balancing</strong>: Tasks are distributed across available execution nodes to balance resource utilization and prevent hotspots.</p>\n<p><strong>Checkpoint Placement</strong>: Strategic checkpoints are inserted to enable efficient failure recovery without recomputing the entire execution plan.</p>\n<p>The optimized execution plan includes detailed scheduling metadata that the orchestration engine uses for efficient task execution and resource management.</p>\n<h3 id=\"dag-visualization\">DAG Visualization</h3>\n<p>DAG visualization transforms the abstract dependency graph into intuitive visual representations that help pipeline developers understand, debug, and communicate about complex data workflows. The visualization system provides multiple rendering modes optimized for different use cases and audiences.</p>\n<p><strong>Graph Rendering Engine</strong></p>\n<p>The visualization engine converts the internal DAG representation into multiple output formats suitable for different consumption scenarios. The core rendering pipeline processes the validated DAG structure and applies layout algorithms to create readable visual representations.</p>\n<p>The rendering process involves several stages:</p>\n<ol>\n<li><strong>Node positioning</strong>: Apply graph layout algorithms (hierarchical, force-directed, or circular) to determine optimal task placement</li>\n<li><strong>Edge routing</strong>: Calculate connection paths between tasks that minimize visual clutter and overlapping</li>\n<li><strong>Visual styling</strong>: Apply consistent colors, shapes, and annotations based on task types and states</li>\n<li><strong>Interactive elements</strong>: Add hover tooltips, click handlers, and zoom/pan capabilities for exploration</li>\n<li><strong>Export generation</strong>: Produce static images, interactive HTML, or embeddable SVG formats</li>\n</ol>\n<p><strong>Layout Algorithms</strong></p>\n<p>Different graph layout algorithms serve different visualization needs and user preferences:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Use Case</th>\n<th>Advantages</th>\n<th>Limitations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hierarchical (Layered)</td>\n<td>Sequential pipelines with clear stages</td>\n<td>Shows execution order clearly, minimal edge crossings</td>\n<td>Requires acyclic graphs, can be wide with many parallel tasks</td>\n</tr>\n<tr>\n<td>Force-Directed</td>\n<td>Complex interconnected pipelines</td>\n<td>Handles arbitrary graph structures, visually appealing</td>\n<td>May not show execution order clearly, can be unstable</td>\n</tr>\n<tr>\n<td>Circular</td>\n<td>Pipelines with hub-and-spoke patterns</td>\n<td>Compact representation, good for radial dependencies</td>\n<td>Difficult to read with many levels, edge crossings</td>\n</tr>\n<tr>\n<td>Grid-Based</td>\n<td>Pipelines with regular structure</td>\n<td>Predictable layout, easy to align with external systems</td>\n<td>Rigid, may waste space, doesn&#39;t adapt to graph structure</td>\n</tr>\n</tbody></table>\n<p>The visualization engine automatically selects the most appropriate algorithm based on graph characteristics but allows manual override for specific presentation needs.</p>\n<blockquote>\n<p><strong>Decision: Support Multiple Layout Algorithms with Automatic Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Different pipeline structures benefit from different visualization approaches, and users have varying preferences for graph layout</li>\n<li><strong>Options Considered</strong>: Single algorithm (simple but limited), User choice only (overwhelming for new users), Automatic with override (complexity but optimal UX)</li>\n<li><strong>Decision</strong>: Implement automatic algorithm selection with manual override capability</li>\n<li><strong>Rationale</strong>: Automatic selection provides good defaults for 90% of cases while expert users can choose specific algorithms for presentation or debugging needs</li>\n<li><strong>Consequences</strong>: Increases implementation complexity but dramatically improves user experience across different pipeline types</li>\n</ul>\n</blockquote>\n<p><strong>Interactive Features</strong></p>\n<p>Modern DAG visualization requires interactive capabilities that enable pipeline exploration and debugging workflows. The visualization system implements rich interactivity through web-based rendering with JavaScript integration.</p>\n<p>Core interactive features include:</p>\n<p><strong>Node Inspection</strong>: Clicking on task nodes displays detailed information including configuration parameters, execution history, dependency relationships, and current status. The inspection panel shows both design-time configuration and runtime execution metrics.</p>\n<p><strong>Dependency Tracing</strong>: Users can highlight dependency chains by selecting a task and visualizing all upstream dependencies (what must complete before this task) or downstream impacts (what depends on this task). This tracing capability is essential for impact analysis during debugging.</p>\n<p><strong>Execution Replay</strong>: The visualization can animate historical pipeline executions by showing task state changes over time. This temporal visualization helps understand execution bottlenecks, failure propagation, and resource utilization patterns.</p>\n<p><strong>Real-time Updates</strong>: During live pipeline execution, the visualization updates task states in real-time through WebSocket connections to the monitoring system. Color coding and progress indicators provide immediate feedback on pipeline health.</p>\n<p><strong>Filtering and Search</strong>: Large pipelines benefit from filtering capabilities that hide or highlight specific task types, execution states, or dependency patterns. Search functionality enables quick navigation to specific tasks by name or configuration attributes.</p>\n<p><strong>Graph Navigation</strong>: Zoom, pan, and minimap features enable exploration of large complex pipelines that don&#39;t fit entirely on screen. The navigation system maintains context and provides orientation cues for user spatial awareness.</p>\n<p><strong>Visual Design System</strong></p>\n<p>Consistent visual design creates intuitive understanding across different pipelines and reduces cognitive load for users switching between projects. The design system defines standardized visual elements and their meanings.</p>\n<p><strong>Node Styling Convention</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Task Type</th>\n<th>Shape</th>\n<th>Color</th>\n<th>Icon</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Extract</td>\n<td>Rectangle</td>\n<td>Blue</td>\n<td>Database/API symbol</td>\n</tr>\n<tr>\n<td>Transform</td>\n<td>Diamond</td>\n<td>Green</td>\n<td>Gear/Function symbol</td>\n</tr>\n<tr>\n<td>Load</td>\n<td>Rectangle</td>\n<td>Orange</td>\n<td>Target/Destination symbol</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td>Hexagon</td>\n<td>Purple</td>\n<td>Check/Warning symbol</td>\n</tr>\n<tr>\n<td>Notification</td>\n<td>Circle</td>\n<td>Yellow</td>\n<td>Bell/Message symbol</td>\n</tr>\n</tbody></table>\n<p><strong>Edge Styling Convention</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Dependency Type</th>\n<th>Line Style</th>\n<th>Color</th>\n<th>Arrow Style</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Dependency</td>\n<td>Solid</td>\n<td>Black</td>\n<td>Standard arrow</td>\n</tr>\n<tr>\n<td>Control Dependency</td>\n<td>Dashed</td>\n<td>Gray</td>\n<td>Hollow arrow</td>\n</tr>\n<tr>\n<td>Conditional Dependency</td>\n<td>Dotted</td>\n<td>Blue</td>\n<td>Diamond arrow</td>\n</tr>\n<tr>\n<td>External Dependency</td>\n<td>Dash-dot</td>\n<td>Red</td>\n<td>Square arrow</td>\n</tr>\n</tbody></table>\n<p><strong>State Indication System</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Task State</th>\n<th>Border Color</th>\n<th>Fill Pattern</th>\n<th>Animation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PENDING</td>\n<td>Gray</td>\n<td>None</td>\n<td>None</td>\n</tr>\n<tr>\n<td>RUNNING</td>\n<td>Blue</td>\n<td>Diagonal stripes</td>\n<td>Pulsing</td>\n</tr>\n<tr>\n<td>SUCCESS</td>\n<td>Green</td>\n<td>Solid</td>\n<td>None</td>\n</tr>\n<tr>\n<td>FAILED</td>\n<td>Red</td>\n<td>Cross-hatch</td>\n<td>None</td>\n</tr>\n<tr>\n<td>RETRYING</td>\n<td>Orange</td>\n<td>Dots</td>\n<td>Spinning</td>\n</tr>\n<tr>\n<td>SKIPPED</td>\n<td>Gray</td>\n<td>Faded</td>\n<td>None</td>\n</tr>\n</tbody></table>\n<p><strong>Export and Integration Capabilities</strong></p>\n<p>The visualization system supports multiple export formats to integrate with documentation systems, monitoring dashboards, and presentation tools.</p>\n<p><strong>Static Export Formats</strong>:</p>\n<ul>\n<li><strong>SVG</strong>: Vector graphics suitable for documentation and high-quality printing</li>\n<li><strong>PNG/JPEG</strong>: Raster images for embedding in presentations and reports</li>\n<li><strong>PDF</strong>: Multi-page layouts for large pipelines with detailed annotations</li>\n</ul>\n<p><strong>Interactive Export Formats</strong>:</p>\n<ul>\n<li><strong>HTML</strong>: Self-contained interactive visualizations that work without server connectivity</li>\n<li><strong>Embedding codes</strong>: JavaScript widgets for integration into custom dashboards and monitoring systems</li>\n<li><strong>API endpoints</strong>: Real-time data feeds for custom visualization tools</li>\n</ul>\n<p><strong>Integration with External Tools</strong>:</p>\n<p>The visualization engine provides REST API endpoints that external tools can query to retrieve graph data in standardized formats (GraphML, DOT, JSON). This integration capability enables pipeline visualization within existing workflow management tools and custom monitoring dashboards.</p>\n<p><strong>Performance Optimization for Large Pipelines</strong></p>\n<p>Large enterprise pipelines with hundreds or thousands of tasks require specialized rendering optimizations to maintain interactive performance. The visualization system implements several performance strategies:</p>\n<p><strong>Level-of-Detail Rendering</strong>: Distant or zoomed-out nodes are rendered with simplified graphics while detailed rendering applies only to visible nodes. This technique maintains smooth interaction even with complex graphs.</p>\n<p><strong>Virtual Scrolling</strong>: Only visible portions of large graphs are rendered in the DOM while off-screen elements exist as lightweight data structures. This approach prevents browser performance degradation with massive pipelines.</p>\n<p><strong>Clustering and Summarization</strong>: Related tasks can be visually grouped into clusters with expand/collapse functionality. This hierarchical approach enables users to understand high-level pipeline structure while drilling into specific areas of interest.</p>\n<p><strong>Incremental Updates</strong>: Real-time state updates use efficient differential rendering that updates only changed elements rather than re-rendering the entire graph. This optimization maintains responsiveness during active pipeline execution.</p>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Fpipeline-processing-flow.svg\" alt=\"End-to-End Pipeline Processing\"></p>\n<p><strong>Common Pitfalls in DAG Visualization</strong></p>\n<p> <strong>Pitfall: Over-complex Layout for Simple Pipelines</strong>\nMany visualization systems apply sophisticated force-directed algorithms to simple sequential pipelines, resulting in unnecessarily complex layouts that obscure the straightforward execution flow. Simple pipelines benefit from hierarchical left-to-right layouts that clearly show the execution sequence. The solution is to analyze graph structure first and select layout algorithms appropriate for the specific pipeline topology.</p>\n<p> <strong>Pitfall: Insufficient Visual Distinction Between Task Types</strong>\nUsing similar colors or shapes for different task types creates confusion and makes it difficult to understand pipeline structure at a glance. Users should be able to identify extract, transform, and load tasks immediately through visual cues. The solution is to establish clear visual conventions and apply them consistently across all pipeline visualizations.</p>\n<p> <strong>Pitfall: Missing Real-time State Updates</strong>\nStatic visualizations that don&#39;t reflect current execution state force users to switch between monitoring tools and pipeline diagrams, reducing debugging efficiency. The visualization should integrate with the monitoring system to show current task states, progress indicators, and error conditions directly on the graph. This integration requires WebSocket connections or polling mechanisms to fetch real-time state updates.</p>\n<p> <strong>Pitfall: Poor Performance with Large Graphs</strong>\nNaive rendering implementations become unusably slow with pipelines containing hundreds of tasks, forcing users to avoid the visualization tool when they need it most. The solution involves implementing level-of-detail rendering, virtual scrolling, and clustering techniques to maintain interactive performance regardless of pipeline size.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Graph Processing</td>\n<td>NetworkX (Python graphs)</td>\n<td>igraph (high-performance graph algorithms)</td>\n</tr>\n<tr>\n<td>YAML Parsing</td>\n<td>PyYAML (standard library)</td>\n<td>ruamel.yaml (preserves comments and formatting)</td>\n</tr>\n<tr>\n<td>Visualization Backend</td>\n<td>Matplotlib + NetworkX</td>\n<td>Graphviz + PyGraphviz</td>\n</tr>\n<tr>\n<td>Web Visualization</td>\n<td>D3.js + SVG</td>\n<td>Cytoscape.js (interactive graph library)</td>\n</tr>\n<tr>\n<td>Schema Validation</td>\n<td>jsonschema (Python)</td>\n<td>Cerberus (more expressive validation rules)</td>\n</tr>\n<tr>\n<td>Template Engine</td>\n<td>Jinja2 (widely adopted)</td>\n<td>Chevron (logic-less mustache templates)</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl-pipeline/\n  pipeline_engine/\n    dag/\n      __init__.py                     public API exports\n      parser.py                       YAML/Python config parsing\n      validator.py                    cycle detection and validation\n      executor_planner.py             topological sort and execution planning\n      visualizer.py                   graph rendering and layout\n      models.py                       PipelineDefinition, TaskDefinition classes\n      exceptions.py                   DAG-specific exception classes\n    schemas/\n      pipeline_schema.json            JSON schema for pipeline validation\n      task_schemas/                   task type-specific schemas\n        extract_schema.json\n        transform_schema.json\n        load_schema.json\n    templates/\n      visualization_template.html     HTML template for interactive graphs\n    tests/\n      test_parser.py\n      test_validator.py\n      test_execution_planner.py\n      fixtures/                       sample pipeline definitions\n        simple_pipeline.yaml\n        complex_pipeline.yaml\n        invalid_pipeline.yaml</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<p><strong>Pipeline Schema Validation (<code>schemas/pipeline_schema.json</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"$schema\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"http://json-schema.org/draft-07/schema#\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"required\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"tasks\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"properties\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"id\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"pattern\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"^[a-zA-Z0-9_-]+$\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Unique pipeline identifier\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"name\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"minLength\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Human-readable pipeline name\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"description\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Pipeline purpose and data flow description\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"schedule\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Cron expression or trigger specification\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"tasks\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"array\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"minItems\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"items\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"$ref\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"#/definitions/task\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"parameters\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Global pipeline parameters\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"definitions\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"task\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"required\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"config\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"properties\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"id\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"pattern\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"^[a-zA-Z0-9_-]+$\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"name\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"minLength\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"type\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"enum\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"extract\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"transform\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"load\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"validate\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"notify\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"config\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"object\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"dependencies\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"array\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"items\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"string\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        \"timeout_seconds\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"integer\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"minimum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">          \"maximum\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">86400</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Graph Utility Functions (<code>dag/graph_utils.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Set, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict, deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NodeColor</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WHITE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"white\"</span><span style=\"color:#6A737D\">  # Unvisited</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GRAY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"gray\"</span><span style=\"color:#6A737D\">    # Currently processing</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BLACK</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"black\"</span><span style=\"color:#6A737D\">  # Completely processed</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CycleDetectionResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, has_cycle: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, cycle_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.has_cycle </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> has_cycle</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cycle_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cycle_path </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> detect_cycles_dfs</span><span style=\"color:#E1E4E8\">(adjacency_list: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]) -> CycleDetectionResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Detect cycles in directed graph using DFS with three-color algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns CycleDetectionResult with cycle information if found.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    colors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {node: NodeColor.</span><span style=\"color:#79B8FF\">WHITE</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parent </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {node: </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> dfs_visit</span><span style=\"color:#E1E4E8\">(node: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        colors[node] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> NodeColor.</span><span style=\"color:#79B8FF\">GRAY</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> path </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> [node]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list.get(node, []):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> colors[neighbor] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> NodeColor.</span><span style=\"color:#79B8FF\">GRAY</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Found back edge - cycle detected</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cycle_start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_path.index(neighbor)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> current_path[cycle_start:] </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> [neighbor]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> colors[neighbor] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> NodeColor.</span><span style=\"color:#79B8FF\">WHITE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cycle </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dfs_visit(neighbor, current_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> cycle:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> cycle</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        colors[node] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> NodeColor.</span><span style=\"color:#79B8FF\">BLACK</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> colors[node] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> NodeColor.</span><span style=\"color:#79B8FF\">WHITE</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cycle </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dfs_visit(node, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> cycle:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> CycleDetectionResult(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, cycle)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> CycleDetectionResult(</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> topological_sort_kahns</span><span style=\"color:#E1E4E8\">(adjacency_list: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]) -> List[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Perform topological sort using Kahn's algorithm.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns list of lists where each inner list contains tasks that can run in parallel.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Calculate in-degrees</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    in_degree </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    all_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(adjacency_list.keys())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list[node]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_nodes.add(neighbor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            in_degree[neighbor] </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Initialize queue with nodes having zero in-degree</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> deque([node </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> all_nodes </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> in_degree[node] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result_levels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#E1E4E8\"> queue:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # All nodes in current queue can run in parallel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_level </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(queue)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result_levels.append(current_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queue.clear()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Process current level nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> current_level:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list.get(node, []):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                in_degree[neighbor] </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> in_degree[neighbor] </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    queue.append(neighbor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result_levels</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_critical_path</span><span style=\"color:#E1E4E8\">(adjacency_list: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          task_durations: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> Tuple[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Calculate the critical path (longest path) through the DAG.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns tuple of (critical_path_nodes, total_duration).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Build reverse adjacency list for backward traversal</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reverse_adj </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    all_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(adjacency_list.keys())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> adjacency_list[node]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            reverse_adj[neighbor].append(node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_nodes.add(neighbor)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Calculate longest path to each node using dynamic programming</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    longest_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path_predecessor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_longest_path_to_node</span><span style=\"color:#E1E4E8\">(node: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> longest_path:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> longest_path[node]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> reverse_adj[node]:  </span><span style=\"color:#6A737D\"># No predecessors - starting node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            longest_path[node] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> task_durations.get(node, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> longest_path[node]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        best_predecessor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> predecessor </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> reverse_adj[node]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pred_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> calculate_longest_path_to_node(predecessor)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> pred_path </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> max_path:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                max_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pred_path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                best_predecessor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> predecessor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        longest_path[node] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_path </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> task_durations.get(node, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        path_predecessor[node] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> best_predecessor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> longest_path[node]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Find the node with maximum longest path (end of critical path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    critical_end </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> all_nodes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> calculate_longest_path_to_node(node)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> duration </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> max_duration:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            max_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            critical_end </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Reconstruct critical path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    critical_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> critical_end</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    while</span><span style=\"color:#E1E4E8\"> current </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        critical_path.append(current)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> path_predecessor.get(current)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    critical_path.reverse()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> critical_path, max_duration</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<p><strong>Pipeline Parser (<code>dag/parser.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineDefinition, TaskDefinition, RetryPolicy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineParsingError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> jsonschema</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, schema_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize parser with JSON schema for validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(schema_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.schema </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_yaml_file</span><span style=\"color:#E1E4E8\">(self, file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> PipelineDefinition:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse pipeline definition from YAML file.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Validates schema and constructs PipelineDefinition object.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load YAML file and handle file reading errors gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate loaded YAML against JSON schema using jsonschema.validate()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract pipeline metadata (id, name, description, schedule, parameters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse tasks list and create TaskDefinition objects for each task</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle template parameter substitution if parameters are provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create and return PipelineDefinition object with all parsed data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Wrap any parsing errors in PipelineParsingError with helpful messages</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use yaml.safe_load() to avoid security issues with arbitrary Python execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: jsonschema.validate(data, schema) raises ValidationError with detailed messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_python_definition</span><span style=\"color:#E1E4E8\">(self, definition_func) -> PipelineDefinition:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Parse pipeline definition from Python function that returns pipeline dict.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Supports dynamic pipeline generation with parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Call definition_func() to get pipeline dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle any exceptions from dynamic pipeline generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate returned dictionary against JSON schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Convert dictionary to PipelineDefinition object (reuse logic from YAML parser)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return constructed PipelineDefinition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Python definitions enable dynamic task generation based on runtime conditions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_task_definition</span><span style=\"color:#E1E4E8\">(self, task_dict: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> TaskDefinition:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert task dictionary to TaskDefinition object with full validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract required fields (id, name, type, config) with validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse optional dependencies list, defaulting to empty list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse retry_policy dict and create RetryPolicy object, using defaults if not provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse timeout_seconds with reasonable default (e.g., 3600 seconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate that task type is one of supported types (extract, transform, load, validate, notify)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create and return TaskDefinition object</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use .get() method with defaults for optional fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Validate task_dict keys against expected schema before accessing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _substitute_parameters</span><span style=\"color:#E1E4E8\">(self, template_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Substitute template variables in configuration strings.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Supports {{variable}} and {{variable|default}} syntax.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find all template variables using regex pattern {{variable_name}} or {{variable_name|default}}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each template variable, check if parameter exists in parameters dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If parameter exists, substitute with actual value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If parameter doesn't exist but default is provided, use default value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If parameter doesn't exist and no default, raise PipelineParsingError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return string with all substitutions completed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use re.findall() to find template patterns, then re.sub() for substitution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>DAG Validator (<code>dag/validator.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineDefinition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .graph_utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> detect_cycles_dfs, CycleDetectionResult</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineValidationError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DAGValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Comprehensive pipeline validation returning list of error messages.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Empty list indicates valid pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate pipeline-level constraints (unique ID, valid schedule format)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Build task ID set and check for duplicates within pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate each task individually (call _validate_task for each)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Build dependency graph from task definitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate dependency references (all dependencies must exist as tasks)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Perform cycle detection on dependency graph</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Check for orphaned tasks (tasks with no path from root tasks)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Validate resource constraints if specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Return accumulated error list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Build adjacency list: {task_id: [list_of_dependent_task_ids]}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use set operations to find missing dependency references efficiently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_task</span><span style=\"color:#E1E4E8\">(self, task: TaskDefinition, all_task_ids: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate individual task definition and return any errors found.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate task ID format (alphanumeric, underscore, hyphen only)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate task name is non-empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate task type is supported (extract, transform, load, validate, notify)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check that dependencies list contains only valid task IDs from all_task_ids</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate retry policy values (max_attempts > 0, backoff_seconds >= 0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Validate timeout_seconds is reasonable (> 0, &#x3C; 86400)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Validate task-specific config based on task type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use regular expressions for ID format validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Task-specific config validation can be extended based on task types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _build_adjacency_list</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Build adjacency list representation of task dependency graph.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize adjacency list dictionary with all task IDs as keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each task, add its dependencies as outgoing edges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle tasks with no dependencies (empty adjacency list)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return complete adjacency list for graph algorithms</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Adjacency list maps each task to list of tasks that depend on it (downstream tasks)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_dependency_references</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check that all dependency references point to existing tasks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build set of all valid task IDs in pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each task, check that all its dependencies exist in task ID set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect any invalid dependency references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return list of error messages for invalid references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Suggest similar task names for typos using string similarity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use set difference to find invalid references efficiently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _detect_orphaned_tasks</span><span style=\"color:#E1E4E8\">(self, adjacency_list: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find tasks that have no path from any root task (tasks with no dependencies).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Identify root tasks (tasks with no incoming dependencies)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Perform BFS/DFS from all root tasks to find reachable tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare reachable tasks with all tasks to find orphaned ones</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return list of orphaned task IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Build reverse adjacency list to find tasks with no incoming edges</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Execution Planner (<code>dag/execution_planner.py</code>)</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineDefinition, TaskDefinition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .graph_utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> topological_sort_kahns, calculate_critical_path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionPlan</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.execution_levels: List[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []  </span><span style=\"color:#6A737D\"># Tasks grouped by execution level</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.critical_path: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []           </span><span style=\"color:#6A737D\"># Tasks on critical path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.estimated_duration: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#6A737D\">             # Total estimated runtime</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.resource_requirements: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># Resource needs per level</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionPlanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_parallel_tasks: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_parallel_tasks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_parallel_tasks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_execution_plan</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            task_durations: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Create optimized execution plan with parallelization and resource awareness.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns ExecutionPlan with detailed scheduling information.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build adjacency list from pipeline task dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Perform topological sort to get execution levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply resource constraints to limit parallelism within each level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate critical path if task durations provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Estimate total execution time based on critical path and parallelism</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Generate resource requirement estimates for each execution level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create and return ExecutionPlan object with all scheduling data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use topological_sort_kahns() from graph_utils for level-based execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Resource constraints may split execution levels into smaller batches</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _apply_resource_constraints</span><span style=\"color:#E1E4E8\">(self, execution_levels: List[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  pipeline: PipelineDefinition) -> List[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Split execution levels that exceed resource constraints into smaller batches.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: For each execution level, check if task count exceeds max_parallel_tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If level is too large, split into multiple sub-levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consider task resource requirements (CPU, memory) when splitting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Maintain dependency ordering when creating sub-levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return modified execution levels with resource constraints applied</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Simple approach is to split levels by max_parallel_tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Advanced: Consider actual resource requirements from task configs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_execution_time</span><span style=\"color:#E1E4E8\">(self, execution_levels: List[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               task_durations: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate estimated total execution time based on critical path and parallelism.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Sum the maximum task duration within each execution level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add estimated overhead for task startup and coordination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consider resource contention delays for large parallel levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return total estimated duration in seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Each level's duration is the maximum duration of tasks in that level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>NetworkX Integration</strong>: Use <code>nx.DiGraph()</code> for graph representation and <code>nx.is_directed_acyclic_graph()</code> for quick cycle checking, but implement custom algorithms for better error reporting</li>\n<li><strong>YAML Parsing</strong>: Use <code>yaml.safe_load()</code> instead of <code>yaml.load()</code> to prevent arbitrary code execution security vulnerabilities</li>\n<li><strong>JSON Schema Validation</strong>: Install <code>jsonschema</code> package and use detailed error messages from <code>ValidationError.message</code> for user-friendly feedback</li>\n<li><strong>Template Substitution</strong>: Use <code>re.findall(r&#39;\\{\\{([^}]+)\\}\\}&#39;, template_str)</code> to find template variables, then <code>str.replace()</code> for substitution</li>\n<li><strong>File Path Handling</strong>: Use <code>pathlib.Path</code> for cross-platform file operations and <code>Path.exists()</code> for file validation</li>\n<li><strong>Error Aggregation</strong>: Collect all validation errors before returning to provide comprehensive feedback rather than failing on first error</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing DAG parsing and validation:</p>\n<ol>\n<li><strong>Basic Functionality Test</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> pipeline_engine/dag/tests/test_parser.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> pipeline_engine/dag/tests/test_validator.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Integration Test</strong>:\nCreate a test pipeline YAML file and verify parsing:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pipeline_engine.dag.parser </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineParser</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pipeline_engine.dag.validator </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DAGValidator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PipelineParser(</span><span style=\"color:#9ECBFF\">'schemas/pipeline_schema.json'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">validator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DAGValidator()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should parse successfully</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">pipeline </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse_yaml_file(</span><span style=\"color:#9ECBFF\">'tests/fixtures/simple_pipeline.yaml'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validator.validate_pipeline(pipeline)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Validation errors: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">errors</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should detect cycle in invalid pipeline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">invalid_pipeline </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse_yaml_file(</span><span style=\"color:#9ECBFF\">'tests/fixtures/invalid_pipeline.yaml'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> validator.validate_pipeline(invalid_pipeline)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(errors) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Should have detected validation errors\"</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Expected Output</strong>:</li>\n</ol>\n<ul>\n<li>Valid pipelines parse without errors and produce <code>PipelineDefinition</code> objects</li>\n<li>Invalid pipelines return specific error messages mentioning cycle detection, missing dependencies, or schema violations</li>\n<li>Visualization generates SVG or HTML files showing task nodes and dependency edges</li>\n</ul>\n<ol start=\"4\">\n<li><strong>Signs of Issues</strong>:</li>\n</ol>\n<ul>\n<li><strong>Parsing fails silently</strong>: Check schema validation is working and exceptions are properly handled</li>\n<li><strong>Cycle detection gives false positives</strong>: Verify adjacency list construction - dependencies should point FROM upstream TO downstream</li>\n<li><strong>Topological sort produces wrong order</strong>: Check that in-degree calculation counts incoming dependencies correctly</li>\n</ul>\n<h2 id=\"data-extraction-and-loading\">Data Extraction and Loading</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 - Data Extraction &amp; Loading: Implements extractors and loaders for common data sources with incremental loading, schema mapping, and bulk transfer capabilities.</p>\n</blockquote>\n<h3 id=\"mental-model-universal-adapters\">Mental Model: Universal Adapters</h3>\n<p>Think of data connectors like universal power adapters when traveling internationally. Just as a universal adapter has a standard plug interface on one side and swappable country-specific plugs on the other, our ETL connectors have a standard internal data interface and pluggable source/destination-specific implementations.</p>\n<p>The universal adapter abstracts away the complexity of different electrical systems (voltage, frequency, plug shape) and provides a consistent interface to your device. Similarly, our data connectors abstract away the complexity of different data systems (authentication, pagination, data formats, network protocols) and provide a consistent <code>DataStream</code> interface to the transformation engine.</p>\n<p>When you plug your laptop into a universal adapter, you don&#39;t need to know whether you&#39;re connected to 120V or 240V power - the adapter handles that complexity. When your ETL pipeline calls <code>extract(connection, query)</code>, it doesn&#39;t need to know whether it&#39;s reading from PostgreSQL, a REST API, or a CSV file - the connector handles that complexity and returns a standardized data stream.</p>\n<p>Just as a good universal adapter is reliable (won&#39;t fry your electronics), efficient (minimal power loss), and resumable (maintains connection through brief outages), our data connectors must be reliable (handle network failures gracefully), efficient (stream data without loading everything into memory), and resumable (support incremental loading and checkpointing).</p>\n<h3 id=\"source-connectors\">Source Connectors</h3>\n<p>Source connectors implement the extraction logic for reading data from various systems. Each connector abstracts the complexity of its specific data source while providing a uniform interface to the ETL pipeline. The connector architecture uses a plugin pattern where each source type implements a common <code>SourceConnector</code> interface.</p>\n<p><strong>Database Connectors</strong> handle relational databases like PostgreSQL, MySQL, and SQL Server. These connectors manage database connections, execute SQL queries, and stream results efficiently. The database connector must handle connection pooling to avoid overwhelming the source system with too many concurrent connections. For large result sets, the connector implements cursor-based pagination to avoid loading millions of rows into memory at once.</p>\n<p>The database connector supports both full extraction (reading all data) and incremental extraction (reading only changed data since the last run). For incremental extraction, the connector uses watermarking strategies based on timestamp columns, auto-incrementing IDs, or change data capture (CDC) logs. The connector automatically detects the optimal incremental strategy by analyzing the table schema for suitable watermark columns.</p>\n<p><strong>API Connectors</strong> interface with REST APIs and handle the complexities of HTTP communication, authentication, rate limiting, and pagination. API responses often use different pagination patterns - some use offset/limit, others use cursor tokens, and some use page numbers. The API connector automatically detects the pagination pattern from the initial response and adapts its extraction strategy accordingly.</p>\n<p>Authentication is handled through pluggable authentication providers that support API keys, OAuth 2.0, JWT tokens, and basic authentication. The connector includes automatic token refresh logic for OAuth flows and respects rate limits by implementing exponential backoff when receiving 429 (Too Many Requests) responses.</p>\n<p><strong>File System Connectors</strong> read data from various file formats including CSV, JSON, Parquet, and Avro files. These connectors can read from local file systems, network shares, or cloud storage systems like S3, GCS, or Azure Blob Storage. The file connector automatically detects file formats based on extensions and content sniffing, then selects the appropriate parser.</p>\n<p>For large files, the connector implements streaming readers that process data in chunks rather than loading entire files into memory. This is particularly important for multi-gigabyte CSV files or large JSON arrays. The connector supports file globbing patterns to read multiple related files as a single logical dataset.</p>\n<blockquote>\n<p><strong>Decision: Pluggable Connector Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Different data sources have vastly different connection patterns, authentication schemes, and data formats, making a one-size-fits-all approach impractical</li>\n<li><strong>Options Considered</strong>: Monolithic connector with conditional logic, pluggable architecture with common interface, separate tools for each source type</li>\n<li><strong>Decision</strong>: Pluggable architecture with abstract base class and concrete implementations</li>\n<li><strong>Rationale</strong>: Enables independent development of connectors, easier testing and maintenance, and allows third-party connector development without modifying core system</li>\n<li><strong>Consequences</strong>: Requires well-designed interface abstraction but provides maximum flexibility and extensibility</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Connector Type</th>\n<th>Authentication Methods</th>\n<th>Pagination Support</th>\n<th>Incremental Loading</th>\n<th>Schema Detection</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database</td>\n<td>Username/password, connection strings, SSL certificates</td>\n<td>Cursor-based, offset/limit</td>\n<td>Timestamp, ID-based, CDC</td>\n<td>Information schema queries</td>\n</tr>\n<tr>\n<td>REST API</td>\n<td>API keys, OAuth 2.0, JWT, basic auth</td>\n<td>Cursor tokens, offset/limit, page numbers</td>\n<td>Modified-since headers, cursor bookmarks</td>\n<td>Content-Type headers, response introspection</td>\n</tr>\n<tr>\n<td>File System</td>\n<td>Access keys, service accounts, network credentials</td>\n<td>File chunking, directory scanning</td>\n<td>File modification time, manifest files</td>\n<td>File extension, content sniffing</td>\n</tr>\n</tbody></table>\n<p>The source connector interface defines a standard contract that all implementations must follow:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>connect()</code></td>\n<td>connection_config: Dict</td>\n<td>Connection</td>\n<td>Establishes connection to source system with retry logic</td>\n</tr>\n<tr>\n<td><code>extract()</code></td>\n<td>query: Query, options: ExtractOptions</td>\n<td>DataStream</td>\n<td>Extracts data matching query parameters as streaming iterator</td>\n</tr>\n<tr>\n<td><code>get_schema()</code></td>\n<td>table_name: str</td>\n<td>Schema</td>\n<td>Retrieves schema information including column names, types, constraints</td>\n</tr>\n<tr>\n<td><code>supports_incremental()</code></td>\n<td>table_name: str</td>\n<td>bool</td>\n<td>Indicates whether source supports incremental extraction</td>\n</tr>\n<tr>\n<td><code>get_watermark()</code></td>\n<td>table_name: str</td>\n<td>Optional[Any]</td>\n<td>Returns current high-water mark for incremental extraction</td>\n</tr>\n<tr>\n<td><code>test_connection()</code></td>\n<td>connection_config: Dict</td>\n<td>ConnectionTestResult</td>\n<td>Validates connection configuration without establishing full connection</td>\n</tr>\n</tbody></table>\n<p><strong>Change Data Capture (CDC) Integration</strong> represents an advanced incremental loading strategy where the source system publishes a stream of data changes (inserts, updates, deletes) rather than requiring the ETL system to poll for changes. Many modern databases support CDC through features like PostgreSQL&#39;s logical replication, MySQL&#39;s binlog, or SQL Server&#39;s Change Tracking.</p>\n<p>CDC connectors subscribe to these change streams and convert database-specific change events into standardized change records. Each change record includes the operation type (INSERT, UPDATE, DELETE), the affected row data, and metadata like transaction IDs and timestamps. This approach provides near real-time data synchronization with minimal impact on the source system.</p>\n<p>However, CDC comes with operational complexity. The connector must handle stream interruptions gracefully, maintain proper offset tracking to avoid missing or duplicating changes, and deal with schema evolution when source tables are altered. The CDC connector implements checkpointing to periodically save its position in the change stream, enabling recovery from exactly where it left off after failures.</p>\n<h3 id=\"destination-connectors\">Destination Connectors</h3>\n<p>Destination connectors implement the loading logic for writing data to target systems. Like source connectors, they follow a pluggable architecture but focus on optimizing write performance, handling schema mapping, and ensuring data consistency during loads.</p>\n<p><strong>Database Destination Connectors</strong> write data to relational databases using bulk loading techniques optimized for each database system. PostgreSQL destinations use <code>COPY</code> commands for maximum throughput, while MySQL destinations use <code>LOAD DATA INFILE</code> or batch <code>INSERT</code> statements. The connector automatically selects the optimal loading strategy based on the destination database type and data volume.</p>\n<p>Schema mapping is a critical function where the connector translates between source and destination data types. For example, a source API might return timestamps as ISO 8601 strings, but the destination PostgreSQL table expects <code>TIMESTAMP WITH TIME ZONE</code> columns. The connector maintains mapping rules that define how to convert between different data type systems.</p>\n<p>Upsert operations (insert or update) require careful handling of conflict resolution. The connector must identify which columns constitute the primary key or unique constraint, then generate appropriate <code>ON CONFLICT</code> (PostgreSQL) or <code>ON DUPLICATE KEY UPDATE</code> (MySQL) statements. For databases that don&#39;t support native upsert syntax, the connector implements upsert logic using separate insert and update operations with proper transaction boundaries.</p>\n<p><strong>Data Warehouse Connectors</strong> are specialized for analytical systems like Snowflake, BigQuery, or Redshift. These systems optimize for different usage patterns than transactional databases - they favor bulk loading over row-by-row operations and often require specific file formats for optimal performance.</p>\n<p>The data warehouse connector implements staging-based loading where data is first written to temporary staging tables, then merged into final destination tables using SQL <code>MERGE</code> statements. This pattern enables atomic updates where either the entire batch succeeds or fails together, preventing partial loads that could corrupt analytical queries.</p>\n<p>Column-oriented data warehouses often perform better with specific file formats. The connector can export data to Parquet files for systems like BigQuery, or generate CSV files with appropriate delimiters and escaping for Snowflake&#39;s <code>COPY INTO</code> command. The connector handles the complexity of file generation, upload to cloud storage, and triggering the warehouse&#39;s native bulk loading commands.</p>\n<p><strong>Stream Processing Connectors</strong> write data to real-time systems like Apache Kafka, Amazon Kinesis, or Google Pub/Sub. These connectors must handle message ordering, partitioning strategies, and delivery guarantees. Unlike batch-oriented database connectors, stream connectors must consider message size limits, serialization formats, and consumer scaling patterns.</p>\n<p>The stream connector implements configurable partitioning strategies to ensure related records are delivered to the same partition for ordered processing. For example, all changes for a specific customer might be routed to the same Kafka partition using a hash of the customer ID. This ensures downstream consumers can process changes in the correct order.</p>\n<blockquote>\n<p><strong>Decision: Staging-Based Loading for Data Warehouses</strong></p>\n<ul>\n<li><strong>Context</strong>: Data warehouses optimize for analytical queries and bulk operations, while row-by-row loading creates small files and poor query performance</li>\n<li><strong>Options Considered</strong>: Direct row-by-row inserts, streaming API calls, staging table with bulk merge operations</li>\n<li><strong>Decision</strong>: Staging-based loading with temporary tables and bulk merge operations</li>\n<li><strong>Rationale</strong>: Provides atomicity (all-or-nothing loading), optimal performance through bulk operations, and enables data validation before final commit</li>\n<li><strong>Consequences</strong>: Requires additional storage for staging tables but significantly improves loading performance and data consistency</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Destination Type</th>\n<th>Loading Strategy</th>\n<th>Conflict Resolution</th>\n<th>Transaction Support</th>\n<th>Optimal Batch Size</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PostgreSQL</td>\n<td>COPY commands, batch INSERT</td>\n<td>ON CONFLICT clauses</td>\n<td>Full ACID transactions</td>\n<td>10,000-50,000 rows</td>\n</tr>\n<tr>\n<td>MySQL</td>\n<td>LOAD DATA, batch INSERT</td>\n<td>ON DUPLICATE KEY UPDATE</td>\n<td>Full ACID transactions</td>\n<td>5,000-25,000 rows</td>\n</tr>\n<tr>\n<td>Snowflake</td>\n<td>COPY INTO from staged files</td>\n<td>MERGE statements</td>\n<td>Warehouse-level consistency</td>\n<td>1M+ rows per file</td>\n</tr>\n<tr>\n<td>BigQuery</td>\n<td>Streaming inserts, load jobs</td>\n<td>Table decorators, DML MERGE</td>\n<td>Eventually consistent</td>\n<td>10MB-1GB per load job</td>\n</tr>\n<tr>\n<td>Kafka</td>\n<td>Producer API with batching</td>\n<td>Idempotent producers</td>\n<td>At-least-once delivery</td>\n<td>100-1000 messages</td>\n</tr>\n</tbody></table>\n<p>The destination connector interface mirrors the source connector pattern with methods optimized for write operations:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>connect()</code></td>\n<td>connection_config: Dict</td>\n<td>Connection</td>\n<td>Establishes connection to destination system with write permissions</td>\n</tr>\n<tr>\n<td><code>load()</code></td>\n<td>data_stream: DataStream, target: str, options: LoadOptions</td>\n<td>LoadResult</td>\n<td>Writes data stream to target table/topic with specified options</td>\n</tr>\n<tr>\n<td><code>create_target()</code></td>\n<td>target: str, schema: Schema</td>\n<td>bool</td>\n<td>Creates target table/topic if it doesn&#39;t exist</td>\n</tr>\n<tr>\n<td><code>get_target_schema()</code></td>\n<td>target: str</td>\n<td>Schema</td>\n<td>Retrieves current schema of target table for validation</td>\n</tr>\n<tr>\n<td><code>supports_upsert()</code></td>\n<td>target: str</td>\n<td>bool</td>\n<td>Indicates whether destination supports upsert operations</td>\n</tr>\n<tr>\n<td><code>begin_transaction()</code></td>\n<td>None</td>\n<td>Transaction</td>\n<td>Starts transaction for atomic multi-table loading</td>\n</tr>\n<tr>\n<td><code>commit_transaction()</code></td>\n<td>transaction: Transaction</td>\n<td>bool</td>\n<td>Commits transaction and makes changes permanent</td>\n</tr>\n</tbody></table>\n<p><strong>Bulk Loading Optimization</strong> is crucial for achieving high throughput in ETL pipelines. The destination connector implements several optimization techniques based on the target system&#39;s capabilities. For databases, this includes disabling indexes during loading, using unlogged tables for temporary data, and parallelizing writes across multiple connections.</p>\n<p>The connector monitors loading performance and dynamically adjusts batch sizes based on throughput metrics. If small batches are causing excessive overhead, the connector increases batch size up to memory limits. If large batches are causing timeouts or memory issues, the connector reduces batch size to maintain stability.</p>\n<p>Connection pooling is essential for parallel loading scenarios where multiple tasks might write to the same destination simultaneously. The connector maintains a pool of database connections that can be shared across concurrent loading operations while ensuring proper isolation and transaction boundaries.</p>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Fconnector-architecture.svg\" alt=\"Connector Architecture\"></p>\n<h3 id=\"incremental-loading-strategies\">Incremental Loading Strategies</h3>\n<p>Incremental loading is the practice of extracting only data that has changed since the last successful pipeline run, rather than reprocessing the entire dataset. This dramatically improves pipeline performance and reduces load on source systems, but requires careful state management and change detection strategies.</p>\n<p><strong>Watermarking</strong> is the most common incremental loading pattern, where the pipeline tracks a &quot;high-water mark&quot; representing the latest processed data point. The watermark is typically based on a monotonically increasing column like a timestamp, auto-increment ID, or version number. On each pipeline run, the system extracts only records where the watermark column is greater than the stored high-water mark.</p>\n<p>Timestamp-based watermarking uses columns like <code>created_at</code> or <code>modified_at</code> to identify new or changed records. The pipeline stores the maximum timestamp from the previous run and extracts records with timestamps greater than this value. However, timestamp watermarking has subtleties around clock skew, transaction timing, and null values that must be handled carefully.</p>\n<p>Consider a scenario where records are inserted with <code>created_at = &#39;2024-01-15 14:30:00&#39;</code> but the previous pipeline run completed at <code>14:30:05</code>. If the pipeline uses <code>WHERE created_at &gt; &#39;2024-01-15 14:30:05&#39;</code> for the next extraction, it will miss records that were inserted during the brief overlap period. To handle this, the watermarking strategy typically includes a small lookback window (e.g., 5 minutes) to ensure no records are missed due to timing issues.</p>\n<p><strong>Cursor-Based Pagination</strong> provides a more robust alternative to timestamp watermarking by using opaque cursor tokens that represent positions in the data stream. Many APIs provide cursor tokens that encode the exact position of the last returned record, allowing the next request to continue from that exact point without gaps or duplicates.</p>\n<p>The cursor approach handles several edge cases that timestamp watermarking struggles with. If multiple records have identical timestamps, cursor-based pagination can still distinguish between them. If records are updated rather than inserted, cursors can track the update sequence. If the source system experiences clock changes or time zone transitions, cursors remain unaffected.</p>\n<p>However, cursor-based pagination requires the source system to maintain stable cursors across API calls. Some systems generate time-limited cursors that expire after a certain period, requiring the pipeline to fall back to full extraction if too much time passes between runs. The incremental loading logic must handle cursor expiration gracefully and maintain fallback strategies.</p>\n<p><strong>Change Data Capture (CDC) Watermarking</strong> represents the most sophisticated incremental loading strategy, where the source system publishes a stream of change events that the pipeline can consume incrementally. Each change event contains metadata like log sequence numbers (LSNs) or transaction IDs that serve as watermarks for tracking progress through the change stream.</p>\n<p>CDC watermarking provides several advantages over other strategies. It captures all types of changes (inserts, updates, deletes) rather than just new records. It provides near real-time latency since changes are available immediately rather than waiting for batch extraction windows. It eliminates the need to query the source system for changes since changes are pushed rather than pulled.</p>\n<p>The complexity of CDC watermarking lies in handling stream failures and recovery. If the pipeline crashes or loses connection to the change stream, it must be able to resume from exactly where it left off without missing or duplicating changes. This requires persistent storage of watermark positions and careful handling of duplicate detection during recovery periods.</p>\n<blockquote>\n<p><strong>Decision: Multi-Strategy Incremental Loading</strong></p>\n<ul>\n<li><strong>Context</strong>: Different source systems provide different mechanisms for change detection, and no single strategy works optimally for all sources</li>\n<li><strong>Options Considered</strong>: Single timestamp-based strategy, single cursor-based strategy, multi-strategy approach with automatic selection</li>\n<li><strong>Decision</strong>: Multi-strategy approach where connectors automatically select the best available strategy</li>\n<li><strong>Rationale</strong>: Maximizes compatibility with diverse source systems while optimizing performance for each system&#39;s capabilities</li>\n<li><strong>Consequences</strong>: Increases connector complexity but provides optimal incremental loading for each source type</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Best Use Cases</th>\n<th>Advantages</th>\n<th>Limitations</th>\n<th>Recovery Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Timestamp Watermarking</td>\n<td>Tables with reliable created_at/modified_at columns</td>\n<td>Simple implementation, works with any SQL database</td>\n<td>Clock skew issues, may miss concurrent inserts</td>\n<td>Low - just store last timestamp</td>\n</tr>\n<tr>\n<td>ID-Based Watermarking</td>\n<td>Tables with auto-increment primary keys</td>\n<td>No clock dependencies, handles concurrent inserts</td>\n<td>Only captures new records, not updates</td>\n<td>Low - store last processed ID</td>\n</tr>\n<tr>\n<td>Cursor-Based Pagination</td>\n<td>APIs with stable cursor support</td>\n<td>No gaps or duplicates, handles complex ordering</td>\n<td>Requires API cursor support, cursors may expire</td>\n<td>Medium - handle cursor expiration</td>\n</tr>\n<tr>\n<td>Change Data Capture</td>\n<td>Systems with CDC capabilities</td>\n<td>Real-time, captures all change types including deletes</td>\n<td>Complex setup, requires CDC infrastructure</td>\n<td>High - manage stream offsets and recovery</td>\n</tr>\n</tbody></table>\n<p><strong>Watermark Storage and Management</strong> requires careful consideration of where and how to store watermark values. The watermark must be stored transactionally with the loaded data to ensure consistency. If the data load succeeds but the watermark update fails, the next pipeline run will reprocess the same data, potentially causing duplicates. If the watermark update succeeds but the data load fails, the next pipeline run will skip data that was never actually loaded.</p>\n<p>The watermarking system stores watermarks in a dedicated metadata table with columns for pipeline ID, source table, watermark column, and watermark value. Each watermark entry includes timestamps for when it was created and last updated, enabling audit trails and debugging of incremental loading issues.</p>\n<p>For complex pipelines that extract from multiple source tables, the watermarking system maintains separate watermarks for each source table. This allows different tables to have different incremental strategies and prevents failures in one table from affecting watermark management for other tables.</p>\n<p><strong>Backfill and Historical Loading</strong> represents a special case of incremental loading where the pipeline needs to process historical data that predates the current watermark. This might be necessary when adding new data sources, recovering from extended outages, or reprocessing data after bug fixes.</p>\n<p>The backfill process creates temporary watermarks that start from a specified historical point and incrementally process data in chunks up to the current watermark. This allows historical processing to use the same incremental loading logic as regular pipeline runs while maintaining proper progress tracking.</p>\n<p>Backfill operations must coordinate with regular pipeline runs to avoid conflicts. The system implements backfill scheduling that ensures historical processing doesn&#39;t interfere with current data extraction, typically by running backfill jobs during low-traffic periods or using separate resource pools.</p>\n<p><strong>Schema Evolution Handling</strong> becomes critical in incremental loading scenarios where the source or destination schema changes between pipeline runs. The incremental loading system must detect schema changes and handle them gracefully without breaking the watermarking logic.</p>\n<p>When new columns are added to source tables, the incremental loading logic continues to work with existing watermark columns. However, the pipeline must handle cases where watermark columns themselves are modified or removed. The system maintains schema fingerprints alongside watermarks to detect when schema changes might affect incremental loading strategies.</p>\n<p>For schema changes that break existing watermarks (such as changing the watermark column data type), the system provides schema migration tools that can reset watermarks and optionally trigger backfill operations to ensure data consistency.</p>\n<p> <strong>Pitfall: Watermark Clock Skew Issues</strong></p>\n<p>A common mistake is using server timestamps as watermarks without accounting for clock differences between the source system and the ETL pipeline. If the source database server&#39;s clock is 5 minutes ahead of the ETL server&#39;s clock, timestamp-based watermarking may miss records that appear to be &quot;in the future&quot; from the ETL system&#39;s perspective. Always use the source system&#39;s clock for timestamp comparisons and include appropriate lookback windows to handle minor clock skew.</p>\n<p> <strong>Pitfall: Non-Atomic Watermark Updates</strong></p>\n<p>Another frequent error is updating the watermark in a separate transaction from the data loading operation. This creates a race condition where the system might crash after loading data but before updating the watermark, causing data duplication on the next run. Alternatively, updating the watermark before confirming successful data loading can cause data loss if the load fails. Always update watermarks atomically with data loading operations using database transactions or equivalent consistency mechanisms.</p>\n<p> <strong>Pitfall: Ignoring Null Watermark Values</strong></p>\n<p>Many developers forget to handle null values in watermark columns, which can cause incremental loading queries to behave unexpectedly. In SQL, comparisons with null values return unknown rather than true or false, potentially excluding records with null timestamps from incremental loads. Design incremental loading queries to explicitly handle null values in watermark columns and decide whether to include or exclude them based on business requirements.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data extraction and loading system requires careful balance between flexibility and performance. This section provides practical guidance for implementing connectors that can handle diverse data sources while maintaining high throughput and reliability.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database Connectivity</td>\n<td><code>psycopg2</code> for PostgreSQL, <code>mysql-connector-python</code> for MySQL</td>\n<td><code>SQLAlchemy</code> with connection pooling and dialect abstraction</td>\n</tr>\n<tr>\n<td>HTTP Client</td>\n<td><code>requests</code> library with session reuse</td>\n<td><code>aiohttp</code> for async operations or <code>httpx</code> for HTTP/2 support</td>\n</tr>\n<tr>\n<td>File Processing</td>\n<td><code>csv</code> module, <code>json</code> module for basic formats</td>\n<td><code>pandas</code> for complex transformations, <code>pyarrow</code> for Parquet</td>\n</tr>\n<tr>\n<td>Cloud Storage</td>\n<td><code>boto3</code> for AWS S3, individual cloud SDKs</td>\n<td><code>fsspec</code> for unified interface across cloud providers</td>\n</tr>\n<tr>\n<td>Streaming</td>\n<td><code>itertools</code> for basic iteration patterns</td>\n<td><code>apache-beam</code> or <code>dask</code> for distributed stream processing</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl-system/\n  src/connectors/\n    __init__.py                     connector registry and factory functions\n    base.py                        abstract base classes for source/destination connectors\n    source/\n      __init__.py                  source connector imports and registry\n      database.py                  database source connector implementation\n      api.py                       REST API source connector implementation  \n      filesystem.py                file system source connector implementation\n    destination/\n      __init__.py                  destination connector imports and registry\n      database.py                  database destination connector implementation\n      warehouse.py                 data warehouse connector implementation\n      streaming.py                 stream processing connector implementation\n    utils/\n      __init__.py\n      pagination.py                pagination strategy implementations\n      watermark.py                 watermark management utilities\n      schema.py                    schema mapping and validation utilities\n  tests/connectors/\n    test_source_database.py        database connector tests with test database\n    test_destination_warehouse.py  warehouse connector tests with mock services\n    integration/                   integration tests with real services\n  config/\n    connector-examples/            example connector configurations</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<p>Complete watermark management utility that handles persistent storage and atomic updates:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timezone</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Any, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sqlite3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WatermarkEntry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_table: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    watermark_column: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    watermark_value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    watermark_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    updated_at: datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WatermarkManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages watermark persistence and atomic updates for incremental loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, db_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"watermarks.db\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._init_database()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _init_database</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize watermark storage database with proper schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE TABLE IF NOT EXISTS watermarks (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    pipeline_id TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    source_table TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    watermark_column TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    watermark_value TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    watermark_type TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    created_at TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    updated_at TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    PRIMARY KEY (pipeline_id, source_table, watermark_column)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE INDEX IF NOT EXISTS idx_pipeline_table </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                ON watermarks(pipeline_id, source_table)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_watermark</span><span style=\"color:#E1E4E8\">(self, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     watermark_column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve current watermark value for specified source.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conn.execute(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"SELECT watermark_value, watermark_type FROM watermarks \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"WHERE pipeline_id = ? AND source_table = ? AND watermark_column = ?\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                (pipeline_id, source_table, watermark_column)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            row </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cursor.fetchone()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> row:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                value_str, value_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> row</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._deserialize_watermark(value_str, value_type)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_watermark</span><span style=\"color:#E1E4E8\">(self, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        watermark_column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, watermark_value: Any) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically update watermark value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        now </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now(timezone.utc).isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._serialize_watermark(watermark_value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> type</span><span style=\"color:#E1E4E8\">(watermark_value).</span><span style=\"color:#79B8FF\">__name__</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                INSERT OR REPLACE INTO watermarks </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                (pipeline_id, source_table, watermark_column, watermark_value, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                 watermark_type, created_at, updated_at)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                VALUES (?, ?, ?, ?, ?, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        COALESCE((SELECT created_at FROM watermarks </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                                 WHERE pipeline_id = ? AND source_table = ? </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                                 AND watermark_column = ?), ?), ?)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">, (pipeline_id, source_table, watermark_column, value_str, value_type,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  pipeline_id, source_table, watermark_column, now, now))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _serialize_watermark</span><span style=\"color:#E1E4E8\">(self, value: Any) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert watermark value to storable string format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(value, datetime):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> value.isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(value, (</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> json.dumps(value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _deserialize_watermark</span><span style=\"color:#E1E4E8\">(self, value_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert stored string back to original watermark type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> value_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"datetime\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> datetime.fromisoformat(value_str)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> value_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"int\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(value_str)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> value_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"float\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(value_str)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> value_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"str\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> value_str</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> json.loads(value_str)</span></span></code></pre></div>\n\n<p>Complete connection pooling utility for database connectors:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Generator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> queue </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Queue, Empty</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psycopg2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> psycopg2 </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pool</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConnectionPoolManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe connection pool manager for database connectors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, connection_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 min_connections: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, max_connections: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.min_connections </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> min_connections</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_connections </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_connections</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._initialize_pool()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _initialize_pool</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create database connection pool with specified parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psycopg2.pool.ThreadedConnectionPool(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                minconn</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.min_connections,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                maxconn</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_connections,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                host</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_config[</span><span style=\"color:#9ECBFF\">'host'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                port</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_config.get(</span><span style=\"color:#9ECBFF\">'port'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5432</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                database</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_config[</span><span style=\"color:#9ECBFF\">'database'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                user</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_config[</span><span style=\"color:#9ECBFF\">'user'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                password</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_config[</span><span style=\"color:#9ECBFF\">'password'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ConnectionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to initialize connection pool: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_connection</span><span style=\"color:#E1E4E8\">(self) -> Generator[Any, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get connection from pool with automatic cleanup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pool.getconn()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#E1E4E8\"> conn</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conn.rollback()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> e</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">._pool.putconn(conn)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> close_all_connections</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Close all connections in the pool.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pool:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._pool.closeall()</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code:</strong></p>\n<p>Database source connector implementation with detailed TODOs for learners:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Iterator, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pandas </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> pd</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SourceConnector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract base class for all source connectors.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract</span><span style=\"color:#E1E4E8\">(self, query: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, options: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract data from source system as iterator of records.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_schema</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get schema information for specified table.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DatabaseSourceConnector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">SourceConnector</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Source connector for relational databases with incremental loading support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, connection_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pool_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.watermark_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> WatermarkManager()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract</span><span style=\"color:#E1E4E8\">(self, query: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, options: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract data from database with optional incremental loading.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize connection pool if not already created</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if incremental loading is requested in options</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If incremental, modify query to include watermark WHERE clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute query using connection from pool</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Fetch results in batches to avoid memory issues (use fetchmany)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Yield each row as dictionary with column names as keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Track maximum watermark value seen during extraction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: After successful extraction, update stored watermark</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use pandas.read_sql with chunksize parameter for large result sets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Convert each row tuple to dict using column names from cursor.description</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_schema</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve column information from database system tables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Connect to database using connection pool</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Query information_schema.columns for table structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle different database types (PostgreSQL, MySQL, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Map database-specific types to standard type names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return dictionary mapping column names to standardized types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: PostgreSQL uses information_schema.columns, MySQL uses DESCRIBE</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Common type mapping: VARCHAR->str, INTEGER->int, TIMESTAMP->datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _build_incremental_query</span><span style=\"color:#E1E4E8\">(self, base_query: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, watermark_info: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Modify base query to include watermark filtering for incremental extraction.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse base query to identify WHERE clause location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add watermark condition (e.g., modified_at > last_watermark)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle case where base query already has WHERE clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Include small lookback window to handle clock skew</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return modified query string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use SQL parsing library or simple string manipulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Always use parameterized queries to prevent SQL injection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p>API source connector with pagination and rate limiting:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Iterator, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> requests.adapters </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> HTTPAdapter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> urllib3.util.retry </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Retry</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> APISourceConnector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">SourceConnector</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Source connector for REST APIs with automatic pagination and rate limiting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, connection_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_config[</span><span style=\"color:#9ECBFF\">'base_url'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.auth_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_config.get(</span><span style=\"color:#9ECBFF\">'auth'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.session </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._create_session()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract</span><span style=\"color:#E1E4E8\">(self, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, options: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract data from API endpoint with automatic pagination handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Authenticate with API using configured auth method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Make initial request to endpoint with base parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect pagination pattern from response (cursor, offset, page-based)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Extract records from response data based on configured data_path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Yield individual records from current page</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Check for next page indicator and prepare next request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Implement rate limiting with exponential backoff on 429 errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Continue pagination until no more pages available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Common pagination patterns - look for 'next', 'cursor', 'has_more' fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use time.sleep() between requests to respect rate limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_session</span><span style=\"color:#E1E4E8\">(self) -> requests.Session:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create HTTP session with retry strategy and timeout configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create requests.Session object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Configure retry strategy for transient failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set appropriate timeouts for connect and read operations  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add authentication headers/parameters based on auth_config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Mount HTTPAdapter with retry configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Retry on 500, 502, 503, 504 status codes but not 4xx errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use exponential backoff starting from 1 second</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_pagination</span><span style=\"color:#E1E4E8\">(self, response: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine next page parameters based on response pagination metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check response for pagination indicators (next_cursor, next_page, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract pagination parameters for next request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return None if no more pages available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle different pagination patterns (offset/limit, cursor-based, page numbers)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate pagination parameters to avoid infinite loops</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Store pagination state to detect when you've seen the same cursor twice</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the source and destination connectors, verify functionality with these tests:</p>\n<ol>\n<li><p><strong>Database Connector Test</strong>: Create a test PostgreSQL database with sample data. Run <code>python -m pytest tests/connectors/test_source_database.py</code> and verify:</p>\n<ul>\n<li>Full extraction returns all records</li>\n<li>Incremental extraction with timestamp watermark returns only new records</li>\n<li>Schema detection correctly identifies column types</li>\n<li>Connection pooling handles concurrent extractions</li>\n</ul>\n</li>\n<li><p><strong>API Connector Test</strong>: Use a public API like JSONPlaceholder (<a href=\"https://jsonplaceholder.typicode.com/\">https://jsonplaceholder.typicode.com/</a>). Run extraction and verify:</p>\n<ul>\n<li>Pagination automatically follows all pages</li>\n<li>Rate limiting respects API limits without errors</li>\n<li>Authentication headers are properly included</li>\n<li>Cursor-based pagination maintains position across requests</li>\n</ul>\n</li>\n<li><p><strong>Integration Test</strong>: Set up end-to-end pipeline from database source to database destination:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   python</span><span style=\"color:#9ECBFF\"> scripts/test_pipeline.py</span><span style=\"color:#79B8FF\"> --source</span><span style=\"color:#9ECBFF\"> postgres://test_db/users</span><span style=\"color:#79B8FF\"> --dest</span><span style=\"color:#9ECBFF\"> postgres://target_db/users_copy</span></span></code></pre></div>\n<p>   Expected output shows successful extraction, transformation, and loading with record counts.</p>\n<p><strong>Common Debugging Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Incremental extraction misses records</td>\n<td>Clock skew between systems</td>\n<td>Check timestamps on both source and ETL servers</td>\n<td>Add 5-10 minute lookback window to watermark queries</td>\n</tr>\n<tr>\n<td>API extraction fails with 429 errors</td>\n<td>Rate limiting too aggressive</td>\n<td>Check API rate limit headers in response</td>\n<td>Implement exponential backoff with longer delays</td>\n</tr>\n<tr>\n<td>Database connection timeout</td>\n<td>Connection pool exhausted</td>\n<td>Monitor active connection count</td>\n<td>Increase pool size or reduce query timeout</td>\n</tr>\n<tr>\n<td>Partial data loads in destination</td>\n<td>Non-atomic watermark updates</td>\n<td>Check if watermark is updated before load completion</td>\n<td>Use database transactions to update watermark with data</td>\n</tr>\n<tr>\n<td>Memory errors during large extractions</td>\n<td>Loading entire result set into memory</td>\n<td>Check if using streaming vs batch loading</td>\n<td>Implement cursor-based iteration with smaller batch sizes</td>\n</tr>\n</tbody></table>\n<h2 id=\"data-transformation-engine\">Data Transformation Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 - Data Transformations: Implements transformation operations with schema validation, supporting SQL-based transformations, Python UDFs, null handling, and data validation rules.</p>\n</blockquote>\n<h3 id=\"mental-model-data-refinery\">Mental Model: Data Refinery</h3>\n<p>Think of the data transformation engine as an <strong>oil refinery</strong> that processes raw crude oil into various refined products. Just as a refinery has multiple processing units (distillation towers, crackers, reformers) that each perform specific transformations on the petroleum, our data transformation engine has multiple transformation stages that clean, reshape, and enrich raw data.</p>\n<p>In an oil refinery, crude oil enters the facility and flows through a series of specialized processing units. Each unit has specific operating parameters (temperature, pressure, catalysts) and produces intermediate products that feed into downstream units. Quality control labs continuously test the products at each stage, rejecting batches that don&#39;t meet specifications and sending them back for reprocessing.</p>\n<p>Similarly, our data transformation engine receives raw extracted data and passes it through a pipeline of transformation stages. Each stage has configurable parameters (SQL queries, Python functions, validation rules) and produces intermediate datasets. Schema validation acts like quality control, checking data types, null constraints, and business rules at each stage, flagging or rejecting records that don&#39;t meet specifications.</p>\n<p>The key insight from this analogy is that <strong>transformations are composable and order-dependent</strong>. Just as you can&#39;t crack heavy oil before distilling it, you often need to clean and standardize data before applying complex business logic transformations. The refinery analogy also highlights that <strong>quality control must be continuous</strong> - checking only the final output misses problems that compound through multiple stages.</p>\n<p>This mental model guides our design decisions: transformation stages should be pluggable and reusable, intermediate results should be inspectable for debugging, and validation should happen at stage boundaries to catch problems early rather than at the end of a long transformation pipeline.</p>\n<h3 id=\"sql-based-transformations\">SQL-Based Transformations</h3>\n<p>SQL-based transformations form the backbone of most ETL operations because SQL provides a declarative, widely-understood language for data manipulation. The transformation engine treats SQL as a <strong>templating language</strong> where queries can contain parameter substitutions, enabling reusable transformation logic across different pipeline runs and datasets.</p>\n<blockquote>\n<p><strong>Decision: Templated SQL with Runtime Parameter Substitution</strong></p>\n<ul>\n<li><strong>Context</strong>: ETL pipelines need to apply similar transformations across different time periods, datasets, or environments, requiring dynamic query generation</li>\n<li><strong>Options Considered</strong>: Static SQL files, string concatenation, template engine (Jinja2), prepared statement parameters</li>\n<li><strong>Decision</strong>: Jinja2 template engine for SQL with runtime parameter substitution</li>\n<li><strong>Rationale</strong>: Template engines provide safe parameter substitution preventing SQL injection, support complex logic (loops, conditionals), and are familiar to data engineers. Prepared statements only handle value parameters, not structural changes like table names or column lists.</li>\n<li><strong>Consequences</strong>: Enables parameterized queries but requires template validation and introduces dependency on Jinja2. Complex templates can become hard to debug.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Template Feature</th>\n<th>Purpose</th>\n<th>Example Usage</th>\n<th>Validation Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Variable Substitution</td>\n<td>Dynamic table/column names</td>\n<td><code>SELECT * FROM {{ source_table }}</code></td>\n<td>Table existence check</td>\n</tr>\n<tr>\n<td>Date Range Filters</td>\n<td>Incremental processing</td>\n<td><code>WHERE created_at &gt;= &#39;{{ start_date }}&#39;</code></td>\n<td>Date format validation</td>\n</tr>\n<tr>\n<td>Conditional Logic</td>\n<td>Environment-specific queries</td>\n<td><code>{% if env == &#39;prod&#39; %} AND status = &#39;active&#39; {% endif %}</code></td>\n<td>Branch coverage testing</td>\n</tr>\n<tr>\n<td>Loop Constructs</td>\n<td>Dynamic column generation</td>\n<td><code>{% for col in numeric_cols %} SUM({{ col }}) {% endfor %}</code></td>\n<td>Column existence validation</td>\n</tr>\n<tr>\n<td>Macro Functions</td>\n<td>Reusable query fragments</td>\n<td><code>{{ standardize_phone_number(&#39;phone_col&#39;) }}</code></td>\n<td>Macro parameter validation</td>\n</tr>\n</tbody></table>\n<p>The SQL transformation executor follows a multi-phase execution model that separates template rendering from SQL execution, enabling better error handling and debugging:</p>\n<ol>\n<li><strong>Template Validation Phase</strong>: Parse the SQL template using Jinja2 parser to check for syntax errors, undefined variables, and template structure issues before runtime</li>\n<li><strong>Parameter Injection Phase</strong>: Merge runtime parameters from pipeline configuration, upstream task outputs, and system variables into template context</li>\n<li><strong>Template Rendering Phase</strong>: Generate the final SQL query by applying parameters to template, producing executable SQL with all variables resolved</li>\n<li><strong>Query Validation Phase</strong>: Validate the rendered SQL for syntax correctness, check referenced tables/columns exist, and verify the user has required permissions</li>\n<li><strong>Execution Phase</strong>: Execute the SQL against the target database, handling connection pooling, transaction management, and result set streaming</li>\n<li><strong>Result Processing Phase</strong>: Convert database result sets into standardized data structures, apply any post-processing transformations, and pass results to downstream tasks</li>\n</ol>\n<blockquote>\n<p>The critical insight here is that template rendering and SQL execution are separate phases - this allows us to generate and inspect the final SQL before execution, enabling better debugging and dry-run capabilities.</p>\n</blockquote>\n<p><strong>Parameterization Strategy</strong></p>\n<p>The transformation engine supports multiple parameter sources with a defined precedence order to handle conflicts:</p>\n<table>\n<thead>\n<tr>\n<th>Parameter Source</th>\n<th>Precedence</th>\n<th>Example Usage</th>\n<th>When to Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Task Configuration</td>\n<td>1 (highest)</td>\n<td>Task-specific table names, custom business logic</td>\n<td>Task-specific overrides</td>\n</tr>\n<tr>\n<td>Pipeline Parameters</td>\n<td>2</td>\n<td>Environment settings, global date ranges</td>\n<td>Pipeline-wide configuration</td>\n</tr>\n<tr>\n<td>System Variables</td>\n<td>3</td>\n<td>Current timestamp, pipeline run ID, execution date</td>\n<td>Built-in system context</td>\n</tr>\n<tr>\n<td>Environment Variables</td>\n<td>4 (lowest)</td>\n<td>Database connection strings, API keys</td>\n<td>Infrastructure configuration</td>\n</tr>\n</tbody></table>\n<p>The parameter resolution engine merges these sources into a single context dictionary, with higher precedence sources overriding lower ones. This enables flexible configuration inheritance where global settings provide defaults that specific tasks can override as needed.</p>\n<p><strong>Query Result Handling</strong></p>\n<p>SQL transformations produce tabular results that must be converted into the standard <code>DataStream</code> format for pipeline interoperability. The transformation engine handles this conversion while managing memory efficiently for large result sets:</p>\n<table>\n<thead>\n<tr>\n<th>Result Processing Strategy</th>\n<th>Memory Usage</th>\n<th>Throughput</th>\n<th>When to Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Streaming Row-by-Row</td>\n<td>Low</td>\n<td>Medium</td>\n<td>Large result sets, memory constraints</td>\n</tr>\n<tr>\n<td>Batched Processing</td>\n<td>Medium</td>\n<td>High</td>\n<td>Moderate result sets, balanced performance</td>\n</tr>\n<tr>\n<td>Full Materialization</td>\n<td>High</td>\n<td>Highest</td>\n<td>Small result sets, need random access</td>\n</tr>\n</tbody></table>\n<p>The executor automatically chooses the appropriate strategy based on result set size estimates from query plan analysis. For result sets under 10MB, it uses full materialization for maximum performance. For larger sets, it switches to batched processing with configurable batch sizes, typically 1000-10000 rows per batch depending on row width.</p>\n<p><strong>Error Handling and Rollback</strong></p>\n<p>SQL transformations run within database transactions to ensure consistency and enable rollback on failures:</p>\n<ol>\n<li><strong>Connection Acquisition</strong>: Obtain database connection from connection pool, handling pool exhaustion with exponential backoff retry</li>\n<li><strong>Transaction Begin</strong>: Start explicit transaction to ensure atomicity of all SQL operations within the transformation</li>\n<li><strong>Query Execution</strong>: Execute the transformed SQL, capturing both result data and execution metadata (row counts, execution time)</li>\n<li><strong>Validation Checks</strong>: Verify result data meets expected schema and business rule constraints before committing</li>\n<li><strong>Transaction Commit</strong>: Commit the transaction if all validations pass, making changes permanent</li>\n<li><strong>Connection Release</strong>: Return connection to pool for reuse, ensuring proper cleanup regardless of success or failure</li>\n</ol>\n<p>If any step fails, the transaction automatically rolls back, leaving the database in its original state. This transactional approach is crucial for data consistency, especially when transformations modify multiple tables or when downstream tasks depend on complete, consistent datasets.</p>\n<p><strong>Common Pitfalls in SQL Transformations</strong></p>\n<p> <strong>Pitfall: Template Injection Vulnerabilities</strong>\nUsing string concatenation or unsafe templating can create SQL injection vulnerabilities. For example, <code>SELECT * FROM users WHERE name = &#39;{{ user_input }}&#39;</code> allows malicious input to break out of the string context. Always use Jinja2&#39;s auto-escaping features and validate parameter values against expected patterns before template rendering.</p>\n<p> <strong>Pitfall: Resource Exhaustion from Large Result Sets</strong>\nMaterializing large query results in memory can exhaust available RAM and crash the pipeline executor. Monitor query result size estimates and automatically switch to streaming processing for results over configurable thresholds. Implement query result limits as a safety mechanism.</p>\n<p> <strong>Pitfall: Transaction Timeout in Long-Running Queries</strong>\nDatabase transactions held open for extended periods can block other operations and may be automatically rolled back by database timeout settings. For transformations that take more than a few minutes, consider breaking them into smaller chunks or using separate transactions for independent operations.</p>\n<h3 id=\"python-user-defined-functions\">Python User-Defined Functions</h3>\n<p>Python User-Defined Functions (UDFs) enable custom transformation logic that goes beyond SQL capabilities, such as complex string processing, machine learning inference, external API calls, or specialized business logic. The UDF execution engine provides a safe, performant environment for running Python code within the transformation pipeline while maintaining data type safety and error isolation.</p>\n<blockquote>\n<p><strong>Decision: Isolated Python Execution with Resource Limits</strong></p>\n<ul>\n<li><strong>Context</strong>: Python UDFs need to execute arbitrary user code safely without compromising pipeline stability or security</li>\n<li><strong>Options Considered</strong>: Same-process execution, subprocess isolation, Docker containers, serverless functions</li>\n<li><strong>Decision</strong>: Subprocess isolation with resource limits and timeout enforcement</li>\n<li><strong>Rationale</strong>: Subprocess isolation prevents memory leaks and crashes from affecting the main pipeline process, while resource limits prevent runaway UDFs from consuming excessive CPU/memory. Simpler than containers but provides adequate isolation.</li>\n<li><strong>Consequences</strong>: Enables safe execution of arbitrary Python code but adds process overhead and inter-process communication complexity. Subprocess startup time affects performance for small datasets.</li>\n</ul>\n</blockquote>\n<p><strong>UDF Definition and Registration</strong></p>\n<p>Python UDFs are defined as regular Python functions with type annotations that specify input and output schemas. The transformation engine uses these annotations for automatic data type conversion and validation:</p>\n<table>\n<thead>\n<tr>\n<th>UDF Component</th>\n<th>Purpose</th>\n<th>Example</th>\n<th>Validation Applied</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Function Signature</td>\n<td>Define input parameters and types</td>\n<td><code>def clean_phone(phone: str) -&gt; str:</code></td>\n<td>Parameter count and types</td>\n</tr>\n<tr>\n<td>Type Annotations</td>\n<td>Specify expected data types</td>\n<td><code>phone: Optional[str]</code> for nullable columns</td>\n<td>Null handling requirements</td>\n</tr>\n<tr>\n<td>Docstring Schema</td>\n<td>Document expected behavior</td>\n<td><code>&quot;&quot;&quot;Standardizes phone numbers to E.164 format&quot;&quot;&quot;</code></td>\n<td>Human-readable documentation</td>\n</tr>\n<tr>\n<td>Return Type</td>\n<td>Specify output data type</td>\n<td><code>-&gt; Optional[str]</code></td>\n<td>Output type validation</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Manage exceptions</td>\n<td><code>try/except</code> blocks with specific error types</td>\n<td>Exception classification</td>\n</tr>\n</tbody></table>\n<p>The UDF registry maintains a catalog of available functions with their signatures, enabling the pipeline definition parser to validate UDF usage at pipeline definition time rather than runtime. This early validation prevents many common errors like type mismatches or missing function definitions.</p>\n<p><strong>Data Type Mapping and Conversion</strong></p>\n<p>The transformation engine automatically converts between the pipeline&#39;s internal data representation and Python native types, handling null values, type coercion, and precision requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Pipeline Type</th>\n<th>Python Type</th>\n<th>Null Handling</th>\n<th>Precision Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>String</code></td>\n<td><code>str</code></td>\n<td><code>None</code> for null</td>\n<td>Unicode encoding preserved</td>\n</tr>\n<tr>\n<td><code>Integer</code></td>\n<td><code>int</code></td>\n<td><code>None</code> for null</td>\n<td>Unlimited precision in Python</td>\n</tr>\n<tr>\n<td><code>Float</code></td>\n<td><code>float</code></td>\n<td><code>None</code> for null, <code>NaN</code> for invalid</td>\n<td>IEEE 754 double precision</td>\n</tr>\n<tr>\n<td><code>Decimal</code></td>\n<td><code>decimal.Decimal</code></td>\n<td><code>None</code> for null</td>\n<td>Arbitrary precision maintained</td>\n</tr>\n<tr>\n<td><code>Boolean</code></td>\n<td><code>bool</code></td>\n<td><code>None</code> for null</td>\n<td>Strict true/false, no truthy conversion</td>\n</tr>\n<tr>\n<td><code>Date</code></td>\n<td><code>datetime.date</code></td>\n<td><code>None</code> for null</td>\n<td>ISO date format</td>\n</tr>\n<tr>\n<td><code>Timestamp</code></td>\n<td><code>datetime.datetime</code></td>\n<td><code>None</code> for null</td>\n<td>Timezone-aware UTC</td>\n</tr>\n<tr>\n<td><code>JSON</code></td>\n<td><code>dict</code> or <code>list</code></td>\n<td><code>None</code> for null</td>\n<td>Nested structures supported</td>\n</tr>\n</tbody></table>\n<p>The type conversion engine handles edge cases like integer overflow, floating-point precision loss, and timezone conversion automatically. For cases where automatic conversion might lose information (like converting <code>Decimal</code> to <code>float</code>), it logs warnings and provides configuration options to make the conversion explicit or reject the operation.</p>\n<p><strong>UDF Execution Modes</strong></p>\n<p>The transformation engine supports different execution modes for Python UDFs based on performance requirements and data characteristics:</p>\n<table>\n<thead>\n<tr>\n<th>Execution Mode</th>\n<th>Use Case</th>\n<th>Performance Characteristics</th>\n<th>Memory Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Row-by-Row</td>\n<td>Complex logic per record</td>\n<td>Low throughput, high latency</td>\n<td>Constant</td>\n</tr>\n<tr>\n<td>Vectorized</td>\n<td>Pandas/NumPy operations</td>\n<td>High throughput, moderate latency</td>\n<td>Proportional to batch size</td>\n</tr>\n<tr>\n<td>Streaming</td>\n<td>Large datasets</td>\n<td>Moderate throughput, low latency</td>\n<td>Constant</td>\n</tr>\n<tr>\n<td>Batch</td>\n<td>Bulk operations</td>\n<td>Highest throughput, high latency</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<p><strong>Row-by-Row Mode</strong> executes the UDF once per input record, suitable for complex transformations that require full context of each record. This mode has the lowest performance but provides the most flexibility and is easiest to debug.</p>\n<p><strong>Vectorized Mode</strong> passes entire columns as pandas Series or NumPy arrays to the UDF, enabling efficient operations on numerical data. UDFs in this mode must be written to handle array inputs and produce array outputs with the same length.</p>\n<p><strong>Streaming Mode</strong> processes data in small batches (typically 100-1000 records) while maintaining constant memory usage. This balances performance with resource consumption for medium-sized datasets.</p>\n<p><strong>Batch Mode</strong> materializes the entire input dataset before passing it to the UDF, enabling operations that require full dataset context like statistical analysis or machine learning training.</p>\n<p><strong>Resource Management and Safety</strong></p>\n<p>Python UDF execution includes comprehensive resource management to prevent runaway processes and ensure predictable performance:</p>\n<ol>\n<li><strong>Memory Limits</strong>: Each UDF subprocess has a configurable memory limit (default 1GB) enforced through OS process limits, preventing individual functions from exhausting system memory</li>\n<li><strong>CPU Limits</strong>: CPU time limits prevent infinite loops or computationally intensive operations from blocking pipeline progress indefinitely</li>\n<li><strong>Timeout Enforcement</strong>: Wall-clock timeout ensures UDFs complete within reasonable time bounds, even if waiting on I/O operations</li>\n<li><strong>Subprocess Isolation</strong>: Each UDF runs in a separate Python subprocess, preventing crashes or memory leaks from affecting the main pipeline process</li>\n<li><strong>Import Restrictions</strong>: UDF execution environments restrict imports to approved libraries, preventing access to dangerous modules like <code>os</code>, <code>subprocess</code>, or <code>sys</code></li>\n<li><strong>Network Isolation</strong>: By default, UDF processes cannot make outbound network connections, though this can be configured for specific use cases</li>\n</ol>\n<p>The resource management system monitors UDF execution continuously and terminates processes that exceed limits, logging detailed information about resource usage for performance tuning.</p>\n<p><strong>Error Handling and Debugging</strong></p>\n<p>UDF error handling follows a structured approach that maximizes debugging information while maintaining pipeline reliability:</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Handling Strategy</th>\n<th>Information Captured</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Import Error</td>\n<td>Fail fast at registration</td>\n<td>Missing module, Python version</td>\n<td>Pipeline validation failure</td>\n</tr>\n<tr>\n<td>Type Error</td>\n<td>Record-level handling</td>\n<td>Expected vs actual types, record ID</td>\n<td>Skip record or fail task</td>\n</tr>\n<tr>\n<td>Value Error</td>\n<td>Configurable handling</td>\n<td>Invalid input value, validation rules</td>\n<td>Apply default or fail record</td>\n</tr>\n<tr>\n<td>Runtime Exception</td>\n<td>Isolation and logging</td>\n<td>Full stack trace, input data</td>\n<td>Fail record, continue batch</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Process termination</td>\n<td>Resource usage stats, partial results</td>\n<td>Retry with smaller batch</td>\n</tr>\n<tr>\n<td>Timeout</td>\n<td>Graceful shutdown</td>\n<td>Execution time, processed records</td>\n<td>Retry or skip based on config</td>\n</tr>\n</tbody></table>\n<p>The error handling system provides multiple recovery strategies:</p>\n<ul>\n<li><strong>Fail Fast</strong>: Stop pipeline execution immediately on first error, suitable for critical transformations where partial results are unacceptable</li>\n<li><strong>Skip Invalid Records</strong>: Continue processing valid records while logging invalid ones for later analysis</li>\n<li><strong>Apply Default Values</strong>: Replace failed transformations with configured default values, useful for optional enrichment operations</li>\n<li><strong>Retry with Backoff</strong>: Retry failed UDF calls with exponential backoff for transient errors like temporary resource constraints</li>\n</ul>\n<p><strong>Common Pitfalls in Python UDFs</strong></p>\n<p> <strong>Pitfall: Memory Leaks from Global Variables</strong>\nPython UDFs that use global variables or class-level state can accumulate memory across multiple invocations since subprocess environments are reused. Always use function-local variables and explicitly clean up any resources like file handles or database connections within the UDF.</p>\n<p> <strong>Pitfall: Inconsistent Null Handling</strong>\nDifferent Python libraries handle <code>None</code> values differently - pandas uses <code>NaN</code>, NumPy has multiple null representations, and standard library functions may raise exceptions. Always explicitly check for <code>None</code> in UDF inputs and decide how to handle null values consistently with your data model.</p>\n<p> <strong>Pitfall: Side Effects in Pure Transformation Functions</strong>\nUDFs should be pure functions that don&#39;t modify external state, write files, or make network calls unless explicitly designed for those purposes. Side effects make UDFs non-idempotent and can cause inconsistent results when pipelines retry or run in parallel.</p>\n<h3 id=\"schema-validation-and-evolution\">Schema Validation and Evolution</h3>\n<p>Schema validation ensures data consistency throughout the transformation pipeline by checking data types, constraints, and business rules at transformation boundaries. The validation engine must balance thoroughness with performance while handling the inevitable evolution of data schemas over time.</p>\n<p>Schema evolution presents one of the most challenging aspects of ETL system design because it requires maintaining backward compatibility while allowing data models to grow and change. The transformation engine treats schema as a <strong>first-class citizen</strong> with explicit versioning, migration strategies, and compatibility checking.</p>\n<blockquote>\n<p><strong>Decision: Schema Registry with Backward Compatibility Checking</strong></p>\n<ul>\n<li><strong>Context</strong>: Data schemas evolve over time as business requirements change, requiring systematic management of schema versions and compatibility</li>\n<li><strong>Options Considered</strong>: No schema management, inline schema definitions, external schema registry, database-driven schema evolution</li>\n<li><strong>Decision</strong>: Centralized schema registry with version management and compatibility checking</li>\n<li><strong>Rationale</strong>: Centralized registry enables schema reuse across pipelines, version management tracks evolution over time, and compatibility checking prevents breaking changes from propagating. External systems like Confluent Schema Registry provide proven solutions.</li>\n<li><strong>Consequences</strong>: Enables systematic schema evolution but adds operational complexity and external dependency. Schema registry becomes critical infrastructure component.</li>\n</ul>\n</blockquote>\n<p><strong>Schema Definition and Versioning</strong></p>\n<p>The schema registry maintains versioned schema definitions that specify data structure, constraints, and evolution rules:</p>\n<table>\n<thead>\n<tr>\n<th>Schema Component</th>\n<th>Purpose</th>\n<th>Example</th>\n<th>Validation Applied</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Field Definitions</td>\n<td>Column names, types, nullability</td>\n<td><code>{&quot;name&quot;: &quot;user_id&quot;, &quot;type&quot;: &quot;integer&quot;, &quot;nullable&quot;: false}</code></td>\n<td>Type compatibility, null constraints</td>\n</tr>\n<tr>\n<td>Primary Keys</td>\n<td>Unique identification</td>\n<td><code>{&quot;primary_key&quot;: [&quot;user_id&quot;, &quot;timestamp&quot;]}</code></td>\n<td>Uniqueness validation</td>\n</tr>\n<tr>\n<td>Foreign Keys</td>\n<td>Referential integrity</td>\n<td><code>{&quot;foreign_key&quot;: {&quot;field&quot;: &quot;user_id&quot;, &quot;references&quot;: &quot;users.id&quot;}}</code></td>\n<td>Reference validation</td>\n</tr>\n<tr>\n<td>Check Constraints</td>\n<td>Business rules</td>\n<td><code>{&quot;check&quot;: &quot;age &gt;= 0 AND age &lt;= 150&quot;}</code></td>\n<td>Value range validation</td>\n</tr>\n<tr>\n<td>Default Values</td>\n<td>Missing data handling</td>\n<td><code>{&quot;field&quot;: &quot;status&quot;, &quot;default&quot;: &quot;active&quot;}</code></td>\n<td>Type-compatible defaults</td>\n</tr>\n<tr>\n<td>Evolution Rules</td>\n<td>Schema change policies</td>\n<td><code>{&quot;allow_new_fields&quot;: true, &quot;allow_field_deletion&quot;: false}</code></td>\n<td>Compatibility enforcement</td>\n</tr>\n</tbody></table>\n<p>Each schema version includes a compatibility level that determines what changes are allowed:</p>\n<ul>\n<li><strong>Full Compatibility</strong>: New schema can read data written with any previous schema version, and previous versions can read data written with new schema</li>\n<li><strong>Backward Compatibility</strong>: New schema can read data written with previous schema versions, but not vice versa</li>\n<li><strong>Forward Compatibility</strong>: Previous schema versions can read data written with new schema, but new schema may not read old data</li>\n<li><strong>No Compatibility</strong>: Breaking changes allowed, requiring explicit data migration</li>\n</ul>\n<p><strong>Validation Pipeline Architecture</strong></p>\n<p>The validation engine operates as a series of validation stages that data passes through during transformation:</p>\n<ol>\n<li><strong>Input Validation Stage</strong>: Validate data entering transformation against source schema, checking basic type compatibility and required field presence</li>\n<li><strong>Constraint Validation Stage</strong>: Apply business rule constraints like range checks, format validation, and cross-field dependencies</li>\n<li><strong>Type Coercion Stage</strong>: Attempt automatic type conversions for compatible types, logging all coercions for audit purposes</li>\n<li><strong>Output Validation Stage</strong>: Validate transformation results against target schema before passing to next pipeline stage</li>\n<li><strong>Schema Evolution Check</strong>: Compare current data schema against registry to detect schema drift and compatibility issues</li>\n</ol>\n<p>Each stage can be configured with different error handling policies:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Policy</th>\n<th>Behavior</th>\n<th>Use Cases</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Strict Validation</td>\n<td>Fail pipeline on first validation error</td>\n<td>Critical financial data, regulatory compliance</td>\n<td>Lowest throughput</td>\n</tr>\n<tr>\n<td>Best Effort</td>\n<td>Log errors but continue processing valid records</td>\n<td>Data exploration, non-critical analytics</td>\n<td>Balanced performance</td>\n</tr>\n<tr>\n<td>Schema Inference</td>\n<td>Automatically adapt schema based on observed data</td>\n<td>Prototype pipelines, exploratory analysis</td>\n<td>Highest throughput</td>\n</tr>\n<tr>\n<td>Sampling Validation</td>\n<td>Validate random sample of records</td>\n<td>Large datasets with consistent structure</td>\n<td>Minimal impact</td>\n</tr>\n</tbody></table>\n<p><strong>Type System and Coercion Rules</strong></p>\n<p>The transformation engine uses a rich type system that captures both logical data types and physical representation requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Logical Type</th>\n<th>Physical Types</th>\n<th>Coercion Rules</th>\n<th>Precision Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Numeric</td>\n<td><code>int32</code>, <code>int64</code>, <code>float32</code>, <code>float64</code>, <code>decimal</code></td>\n<td>Widening conversions only</td>\n<td>Maintain maximum precision</td>\n</tr>\n<tr>\n<td>Text</td>\n<td><code>varchar(n)</code>, <code>text</code>, <code>char(n)</code></td>\n<td>Truncate with warning</td>\n<td>Preserve UTF-8 encoding</td>\n</tr>\n<tr>\n<td>Temporal</td>\n<td><code>date</code>, <code>timestamp</code>, <code>timestamptz</code></td>\n<td>Parse standard formats</td>\n<td>Maintain timezone info</td>\n</tr>\n<tr>\n<td>Boolean</td>\n<td><code>bool</code>, <code>bit</code>, <code>tinyint</code></td>\n<td>Standard truthy conversion</td>\n<td>Map to true/false</td>\n</tr>\n<tr>\n<td>JSON</td>\n<td><code>json</code>, <code>jsonb</code>, <code>text</code></td>\n<td>Parse and validate JSON syntax</td>\n<td>Preserve nested structure</td>\n</tr>\n<tr>\n<td>Binary</td>\n<td><code>blob</code>, <code>bytea</code>, <code>varbinary</code></td>\n<td>Base64 encoding for text transport</td>\n<td>Preserve exact bytes</td>\n</tr>\n</tbody></table>\n<p>Automatic type coercion follows safe conversion rules that never lose information without explicit user configuration. For example, <code>int32</code> to <code>int64</code> conversion is automatic, but <code>int64</code> to <code>int32</code> requires explicit truncation handling since it may lose data.</p>\n<p><strong>Null Value Semantics</strong></p>\n<p>Different data systems have varying concepts of null values, requiring careful handling during transformations:</p>\n<table>\n<thead>\n<tr>\n<th>System</th>\n<th>Null Representation</th>\n<th>Semantics</th>\n<th>Transformation Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SQL Databases</td>\n<td><code>NULL</code></td>\n<td>Unknown value, not equal to anything including itself</td>\n<td>Preserve SQL null semantics</td>\n</tr>\n<tr>\n<td>JSON</td>\n<td><code>null</code> literal</td>\n<td>Explicit null value</td>\n<td>Map to SQL NULL</td>\n</tr>\n<tr>\n<td>CSV Files</td>\n<td>Empty string or missing field</td>\n<td>Ambiguous - could be null or empty</td>\n<td>Configurable interpretation</td>\n</tr>\n<tr>\n<td>Python</td>\n<td><code>None</code> object</td>\n<td>Absence of value</td>\n<td>Direct mapping to SQL NULL</td>\n</tr>\n<tr>\n<td>Pandas</td>\n<td><code>NaN</code> (Not a Number)</td>\n<td>Missing numerical value</td>\n<td>Convert to SQL NULL</td>\n</tr>\n<tr>\n<td>Apache Parquet</td>\n<td>Null bitmap</td>\n<td>Efficient null encoding</td>\n<td>Direct null preservation</td>\n</tr>\n</tbody></table>\n<p>The transformation engine provides configurable null handling policies to manage these semantic differences:</p>\n<ul>\n<li><strong>Strict Null Preservation</strong>: Maintain exact null semantics from source system</li>\n<li><strong>Null Unification</strong>: Convert all null representations to standard SQL NULL</li>\n<li><strong>Null Replacement</strong>: Replace nulls with configurable default values based on data type</li>\n<li><strong>Null Rejection</strong>: Treat null values as validation errors for non-nullable fields</li>\n</ul>\n<p><strong>Schema Evolution Strategies</strong></p>\n<p>The schema evolution engine supports multiple strategies for handling schema changes over time:</p>\n<table>\n<thead>\n<tr>\n<th>Evolution Strategy</th>\n<th>Approach</th>\n<th>Benefits</th>\n<th>Limitations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Append-Only</td>\n<td>Only add new optional fields</td>\n<td>Maintains full compatibility</td>\n<td>Cannot remove obsolete fields</td>\n</tr>\n<tr>\n<td>Versioned Schemas</td>\n<td>Maintain multiple schema versions simultaneously</td>\n<td>Supports breaking changes</td>\n<td>Complexity increases with versions</td>\n</tr>\n<tr>\n<td>Schema Migration</td>\n<td>Transform data during schema updates</td>\n<td>Clean schema evolution</td>\n<td>Requires migration downtime</td>\n</tr>\n<tr>\n<td>Schema Union</td>\n<td>Merge schemas using union types</td>\n<td>Handles diverse data sources</td>\n<td>Complex type system</td>\n</tr>\n</tbody></table>\n<p><strong>Append-Only Evolution</strong> is the simplest approach where new fields can be added but existing fields cannot be removed or have their types changed. This maintains backward compatibility but can lead to schema bloat over time.</p>\n<p><strong>Versioned Schemas</strong> allow breaking changes by maintaining multiple schema versions and routing data through appropriate transformation pipelines. Each pipeline version handles a specific schema version, enabling gradual migration.</p>\n<p><strong>Schema Migration</strong> performs bulk transformation of existing data when schemas change incompatibly. This requires coordinated downtime but results in clean, consistent schemas.</p>\n<p><strong>Schema Union</strong> approaches treat schemas as unions of possible field sets, enabling pipelines to handle multiple schema versions simultaneously. This works well for diverse data sources but requires complex type resolution logic.</p>\n<p><strong>Performance Optimization for Validation</strong></p>\n<p>Schema validation can become a performance bottleneck for high-throughput pipelines, requiring optimization strategies:</p>\n<ol>\n<li><strong>Validation Caching</strong>: Cache validation results for identical records to avoid repeated validation work</li>\n<li><strong>Parallel Validation</strong>: Distribute validation across multiple threads or processes for CPU-bound operations</li>\n<li><strong>Lazy Validation</strong>: Defer expensive validation operations until data is actually accessed downstream</li>\n<li><strong>Sampling Strategies</strong>: Validate statistical samples rather than every record for consistent data sources</li>\n<li><strong>Early Exit Optimization</strong>: Stop validation on first error for fail-fast policies</li>\n<li><strong>Columnar Validation</strong>: Validate entire columns at once using vectorized operations when possible</li>\n</ol>\n<p>The validation engine monitors its own performance and can automatically adjust validation strategies based on observed data patterns and performance requirements.</p>\n<p><strong>Common Pitfalls in Schema Validation</strong></p>\n<p> <strong>Pitfall: Over-Strict Validation Causing Brittleness</strong>\nExtremely strict validation rules make pipelines brittle to minor schema changes that don&#39;t affect downstream consumers. For example, rejecting records because a rarely-used optional field changes type, even though most consumers ignore that field. Design validation rules to focus on fields actually used by downstream systems.</p>\n<p> <strong>Pitfall: Validation Performance Degradation</strong>\nValidating every field of every record can become extremely expensive for wide tables with complex constraints. Profile validation performance and use sampling or lazy validation for non-critical checks. Monitor validation overhead as a percentage of total pipeline runtime.</p>\n<p> <strong>Pitfall: Inconsistent Error Handling Across Schema Changes</strong>\nDifferent types of schema validation failures may require different handling strategies, but implementing inconsistent policies makes debugging difficult. Establish clear error handling hierarchies: syntax errors always fail, type errors configurable, constraint violations logged but allowed to pass.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SQL Templating</td>\n<td>Python string formatting</td>\n<td>Jinja2 template engine</td>\n</tr>\n<tr>\n<td>Python UDF Execution</td>\n<td>Same-process function calls</td>\n<td>Subprocess isolation with resource limits</td>\n</tr>\n<tr>\n<td>Schema Registry</td>\n<td>JSON files in version control</td>\n<td>Confluent Schema Registry or custom REST API</td>\n</tr>\n<tr>\n<td>Type Validation</td>\n<td>Manual isinstance() checks</td>\n<td>Pydantic models with automatic validation</td>\n</tr>\n<tr>\n<td>Database Connectivity</td>\n<td>Direct database drivers</td>\n<td>SQLAlchemy with connection pooling</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl-pipeline/\n  src/transformation/\n    __init__.py\n    engine.py                     Main transformation engine\n    sql_transformer.py            SQL template processing\n    python_udf.py                Python UDF execution\n    schema_validator.py          Schema validation logic\n    type_system.py               Type definitions and coercion\n    tests/\n      test_sql_transformer.py    SQL transformation tests\n      test_python_udf.py         UDF execution tests  \n      test_schema_validator.py   Schema validation tests\n  schemas/\n    registry/\n      user_events_v1.json        Schema definitions\n      user_events_v2.json\n    migrations/\n      v1_to_v2_migration.sql     Schema evolution scripts\n  transformations/\n    sql/\n      user_cleanup.sql           SQL transformation templates\n      daily_aggregation.sql\n    udfs/\n      phone_standardization.py   Python UDF definitions\n      address_geocoding.py</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<p>Here&#39;s a complete type system foundation for the transformation engine:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># type_system.py - Complete type system implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, List, Optional, Union, Type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, date</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> decimal </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Decimal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DataType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Supported data types in the transformation pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STRING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"string\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTEGER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"integer\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FLOAT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"float\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DECIMAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"decimal\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BOOLEAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"boolean\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"date\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TIMESTAMP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"timestamp\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JSON</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"json\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BINARY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"binary\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValidationResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of data validation operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, is_valid: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, errors: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, warnings: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_valid </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> is_valid</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.warnings </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> warnings </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_error</span><span style=\"color:#E1E4E8\">(self, error: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.errors.append(error)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_valid </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_warning</span><span style=\"color:#E1E4E8\">(self, warning: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.warnings.append(warning)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TypeConverter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles type conversion between different representations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.conversion_rules </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">INTEGER</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">FLOAT</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._int_to_float,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">INTEGER</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._int_to_string,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">FLOAT</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._float_to_string,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">INTEGER</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._string_to_int,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">FLOAT</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._string_to_float,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">BOOLEAN</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._string_to_bool,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">DATE</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._string_to_date,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (DataType.</span><span style=\"color:#79B8FF\">STRING</span><span style=\"color:#E1E4E8\">, DataType.</span><span style=\"color:#79B8FF\">TIMESTAMP</span><span style=\"color:#E1E4E8\">): </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._string_to_timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> convert</span><span style=\"color:#E1E4E8\">(self, value: Any, from_type: DataType, to_type: DataType) -> tuple[Any, ValidationResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert value from one type to another.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, ValidationResult(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> from_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> to_type:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> value, ValidationResult(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conversion_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (from_type, to_type)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> conversion_key </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.conversion_rules:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ValidationResult(</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result.add_error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"No conversion rule from </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">from_type.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> to </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">to_type.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> value, result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            converted_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.conversion_rules[conversion_key](value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> converted_value, ValidationResult(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ValidationResult(</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result.add_error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Conversion failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> value, result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _int_to_float</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _int_to_string</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _float_to_string</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _string_to_int</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(value.strip())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _string_to_float</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> float</span><span style=\"color:#E1E4E8\">(value.strip())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _string_to_bool</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value_lower </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value.strip().lower()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> value_lower </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">'true'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'1'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'yes'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'on'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> value_lower </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">'false'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'0'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'no'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'off'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot convert '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' to boolean\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _string_to_date</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> date:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Support common date formats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> fmt </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">'%Y-%m-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'%m/</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">/%Y'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">/%m/%Y'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> datetime.strptime(value.strip(), fmt).date()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot parse date from '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _string_to_timestamp</span><span style=\"color:#E1E4E8\">(self, value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> datetime:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Support common timestamp formats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> fmt </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">'%Y-%m-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> %H:%M:%S'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'%Y-%m-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">T%H:%M:%S'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'%Y-%m-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> %H:%M:%S.</span><span style=\"color:#79B8FF\">%f</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> datetime.strptime(value.strip(), fmt)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot parse timestamp from '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Schema management utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FieldDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Definition of a single field in a schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data_type: DataType, nullable: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 default_value: Any </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, constraints: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.data_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data_type</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.nullable </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> nullable</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.default_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> default_value</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.constraints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> constraints </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SchemaDefinition</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete schema definition for a dataset.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, fields: List[FieldDefinition]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.version </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> version</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.fields </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {field.name: field </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> field </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> fields}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_field</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[FieldDefinition]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fields.get(name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_record</span><span style=\"color:#E1E4E8\">(self, record: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> ValidationResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate a single record against this schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ValidationResult(</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for required fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> field_name, field_def </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fields.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> field_def.nullable </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> field_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> record:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result.add_error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Required field '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">field_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' is missing\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> field_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> record:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                record[field_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field_def.default_value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for unexpected fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> field_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> record:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> field_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.fields:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result.add_warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unexpected field '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">field_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">' found\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># sql_transformer.py - SQL transformation engine skeleton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Iterator, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> jinja2 </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Environment, FileSystemLoader, TemplateSyntaxError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SQLTransformer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Executes SQL-based transformations with parameter templating.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, template_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, connection_manager):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.template_env </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Environment(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            loader</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">FileSystemLoader(template_dir),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            autoescape</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#6A737D\">  # SQL doesn't need HTML escaping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_manager</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_transformation</span><span style=\"color:#E1E4E8\">(self, template_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             connection_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a SQL transformation using a template with parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load and validate the SQL template</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Use self.template_env.get_template(template_name)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle TemplateNotFound exception</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Return empty iterator on template errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Render the template with parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Call template.render(**parameters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle TemplateSyntaxError and UndefinedError</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Log the rendered SQL for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Acquire database connection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Use self.connection_manager.get_connection(connection_name)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle connection failures with appropriate retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Ensure connection is returned to pool on completion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute the rendered SQL query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Begin transaction for consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Execute query and get cursor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle SQL syntax errors and database exceptions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Stream results back to caller</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Fetch results in batches to manage memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Convert database rows to dictionaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Yield each record to maintain streaming behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Commit transaction on success, rollback on failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use try/finally to ensure connection cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Log execution metrics (row count, duration)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_template</span><span style=\"color:#E1E4E8\">(self, template_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> ValidationResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate template syntax and parameter completeness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Attempt to load template</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Try rendering with provided parameters  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for undefined variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return validation result with specific errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># python_udf.py - Python UDF execution engine skeleton  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> subprocess</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pickle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, List, Any, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> concurrent.futures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ProcessPoolExecutor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> resource</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PythonUDFExecutor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Executes Python User-Defined Functions with isolation and resource limits.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">, memory_limit_mb: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_workers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_workers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_limit_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> memory_limit_mb</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.function_registry: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_function</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, func: Callable):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a UDF for execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate function signature has type annotations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store function in registry with metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate function doesn't use restricted imports</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_function</span><span style=\"color:#E1E4E8\">(self, function_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        execution_mode: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"row\"</span><span style=\"color:#E1E4E8\">) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute a registered UDF on input data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Look up function in registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Validate function exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Get function metadata and signature</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Choose execution strategy based on mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - \"row\": execute once per record</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - \"batch\": execute on entire dataset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - \"stream\": execute on chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Prepare subprocess execution environment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Set memory limits using resource module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Prepare data serialization (pickle or json)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Set up timeout and CPU limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute UDF in isolated subprocess</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Use ProcessPoolExecutor for isolation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle subprocess timeouts and errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Capture both results and any error messages</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Process and validate results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Deserialize results from subprocess</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Validate output types match function signature</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Yield results maintaining input order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use pickle for complex Python objects, JSON for simple data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Monitor subprocess resource usage and terminate if exceeded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _execute_in_subprocess</span><span style=\"color:#E1E4E8\">(self, func_code: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Any, limits: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute function code in isolated subprocess with resource limits.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set resource limits (memory, CPU time)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute function code safely</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return results or error information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># schema_validator.py - Schema validation engine skeleton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SchemaValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates data against schema definitions with evolution support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, schema_registry):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.schema_registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schema_registry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.type_converter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TypeConverter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_data_stream</span><span style=\"color:#E1E4E8\">(self, data_stream: Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           schema_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema_version: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Iterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate and optionally transform data stream against schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Resolve schema from registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Get schema by name and version (latest if version is None)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle schema not found errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Log schema information for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up validation statistics tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Count total records processed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Count validation errors and warnings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Track performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Process each record in the stream</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Validate record structure against schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Apply type conversions where needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Handle validation errors based on policy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply business rule validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Check constraint rules (range checks, format validation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Validate cross-field dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Apply custom validation functions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Yield validated/transformed records</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Apply any schema transformations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Include validation metadata if requested</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - Log summary statistics periodically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use yield to maintain streaming behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Batch validation operations for better performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_schema_compatibility</span><span style=\"color:#E1E4E8\">(self, old_schema: SchemaDefinition, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 new_schema: SchemaDefinition) -> ValidationResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check compatibility between two schema versions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compare field definitions between schemas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for breaking changes (removed fields, type changes)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify safe changes (new optional fields)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return compatibility result with specific issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints</strong></p>\n<ul>\n<li><strong>Template Security</strong>: Use Jinja2&#39;s <code>select_autoescape()</code> with <code>autoescape=False</code> for SQL templates to avoid escaping SQL syntax, but validate all parameters before rendering</li>\n<li><strong>Subprocess Communication</strong>: Use <code>pickle</code> for serializing complex Python objects to subprocesses, but fallback to JSON for simple data types that need to be debuggable</li>\n<li><strong>Resource Limits</strong>: Use the <code>resource</code> module to set <code>RLIMIT_AS</code> (memory) and <code>RLIMIT_CPU</code> (CPU time) limits in subprocess before executing UDF code</li>\n<li><strong>Connection Pooling</strong>: Use SQLAlchemy&#39;s <code>pool_pre_ping=True</code> to validate connections before use, preventing stale connection errors</li>\n<li><strong>Type Validation</strong>: Use <code>typing.get_type_hints()</code> to extract type annotations from UDF functions for automatic validation</li>\n</ul>\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the data transformation engine, verify functionality with these steps:</p>\n<ol>\n<li><strong>SQL Transformation Test</strong>: Create a simple template <code>SELECT * FROM {{ table_name }} WHERE date &gt;= &#39;{{ start_date }}&#39;</code> and verify it renders correctly with parameters</li>\n<li><strong>UDF Registration Test</strong>: Register a simple function like <code>def upper_case(text: str) -&gt; str: return text.upper()</code> and verify it appears in the registry</li>\n<li><strong>Schema Validation Test</strong>: Define a simple schema with required and optional fields, then validate both compliant and non-compliant records</li>\n<li><strong>Integration Test</strong>: Run a complete transformation pipeline that extracts data, applies SQL transformations, executes a Python UDF, and validates against output schema</li>\n</ol>\n<p>Expected behavior:</p>\n<ul>\n<li>Templates should render without errors and produce valid SQL</li>\n<li>UDFs should execute in isolation and return expected results</li>\n<li>Schema validation should catch type mismatches and constraint violations</li>\n<li>The complete pipeline should process sample data end-to-end</li>\n</ul>\n<p>Signs something is wrong:</p>\n<ul>\n<li><strong>Template rendering fails</strong>: Check Jinja2 template syntax and parameter names</li>\n<li><strong>UDF execution hangs</strong>: Verify subprocess limits are set correctly and functions don&#39;t have infinite loops</li>\n<li><strong>Schema validation is slow</strong>: Profile validation logic and consider sampling or lazy validation</li>\n<li><strong>Memory usage grows</strong>: Check for memory leaks in UDF processes or connection pooling</li>\n</ul>\n<h2 id=\"pipeline-orchestration-and-monitoring\">Pipeline Orchestration and Monitoring</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 - Pipeline Orchestration &amp; Monitoring: Implements pipeline execution with monitoring, alerting, and lineage tracking</p>\n</blockquote>\n<h3 id=\"mental-model-air-traffic-control\">Mental Model: Air Traffic Control</h3>\n<p>Think of pipeline orchestration like an air traffic control (ATC) system at a busy airport. Just as ATC coordinates hundreds of flights with complex schedules, dependencies, and safety requirements, our orchestration system manages hundreds of data pipeline tasks with their own schedules, dependencies, and error handling needs.</p>\n<p>In this analogy, each <strong>pipeline</strong> is like a flight route with multiple legs (tasks). The <strong>scheduler</strong> acts as the control tower, deciding when each flight can take off based on weather conditions (resource availability), runway capacity (system load), and air traffic patterns (other running pipelines). The <strong>task execution engine</strong> is like the ground crew and pilots executing each flight leg, reporting status back to the control tower.</p>\n<p>Just as ATC must handle flight delays, cancellations, and emergency landings gracefully while keeping passengers informed, our orchestration system must handle task failures, retries, and cascading dependencies while providing clear visibility into what&#39;s happening. The <strong>monitoring and alerting system</strong> functions like the airport&#39;s information displays and announcement system, keeping stakeholders informed of delays, gate changes, and arrivals.</p>\n<p>The critical insight from this analogy is that orchestration is fundamentally about <strong>coordinating resources, managing dependencies, and maintaining visibility</strong> across a complex distributed system. Like ATC, our system must be reliable, observable, and capable of graceful degradation when things go wrong.</p>\n<h3 id=\"scheduler-integration\">Scheduler Integration</h3>\n<p>The scheduler integration component serves as the entry point for pipeline execution, responsible for determining when pipelines should run based on time-based schedules or external events. This component bridges the gap between pipeline definitions and actual execution, translating abstract scheduling requirements into concrete execution triggers.</p>\n<h4 id=\"cron-based-scheduling\">Cron-Based Scheduling</h4>\n<p>Cron-based scheduling provides time-triggered pipeline execution using familiar cron expression syntax. The scheduler maintains an internal registry of active pipelines and their schedules, continuously evaluating upcoming execution windows.</p>\n<p>The core scheduling algorithm operates on a polling model with configurable intervals (typically 30-60 seconds). During each poll cycle, the scheduler evaluates all registered pipelines to determine if any are due for execution. This evaluation considers the pipeline&#39;s cron expression, timezone settings, and any configured execution windows or blackout periods.</p>\n<blockquote>\n<p><strong>Decision: Polling vs Event-Driven Scheduling</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support both cron-based and event-driven pipeline triggers while maintaining simple operational semantics</li>\n<li><strong>Options Considered</strong>: Pure event-driven system with cron events, hybrid polling for cron with events for external triggers, pure polling for all trigger types</li>\n<li><strong>Decision</strong>: Hybrid approach with polling for cron-based schedules and event queues for external triggers</li>\n<li><strong>Rationale</strong>: Polling provides predictable resource usage and simple failure recovery for time-based schedules, while event queues enable low-latency response to external triggers without the complexity of distributed cron</li>\n<li><strong>Consequences</strong>: Slight delay (up to polling interval) for cron-triggered pipelines, but simplified operational model and better handling of system restarts</li>\n</ul>\n</blockquote>\n<p>The scheduler maintains pipeline execution state to prevent duplicate runs and handle overlapping schedules. Each pipeline definition includes execution policies that control behavior when previous runs are still active or when schedules overlap.</p>\n<table>\n<thead>\n<tr>\n<th>Execution Policy</th>\n<th>Behavior</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ALLOW_CONCURRENT</code></td>\n<td>Start new run regardless of existing runs</td>\n<td>Independent data processing</td>\n</tr>\n<tr>\n<td><code>SKIP_ON_RUNNING</code></td>\n<td>Skip execution if previous run still active</td>\n<td>Long-running ETL processes</td>\n</tr>\n<tr>\n<td><code>CANCEL_PREVIOUS</code></td>\n<td>Terminate previous run and start new one</td>\n<td>Real-time data updates</td>\n</tr>\n<tr>\n<td><code>QUEUE_SEQUENTIAL</code></td>\n<td>Queue execution to start after current completes</td>\n<td>Dependent processing chains</td>\n</tr>\n</tbody></table>\n<p>The scheduler implements <strong>catchup behavior</strong> for pipelines that miss scheduled executions due to system downtime. When the scheduler restarts, it evaluates missed execution windows and can optionally trigger historical runs to maintain data consistency.</p>\n<h4 id=\"event-driven-triggers\">Event-Driven Triggers</h4>\n<p>Event-driven pipeline triggers enable reactive execution based on external system events such as file arrivals, database changes, or API notifications. The scheduler maintains event subscription queues for different trigger types, processing incoming events asynchronously from the cron-based scheduling loop.</p>\n<p>Event triggers support <strong>payload-based parameterization</strong>, allowing external events to provide runtime parameters for pipeline execution. For example, a file arrival event can specify the file path as a pipeline parameter, enabling the same pipeline definition to process different files dynamically.</p>\n<p>The event processing system implements <strong>deduplication and idempotency</strong> to handle duplicate events gracefully. Each event includes a unique identifier and optional deduplication window, preventing redundant pipeline executions for repeated notifications.</p>\n<table>\n<thead>\n<tr>\n<th>Event Source Type</th>\n<th>Configuration</th>\n<th>Deduplication Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>FILE_ARRIVAL</code></td>\n<td>Directory path, file pattern, polling interval</td>\n<td>File path + modification time</td>\n</tr>\n<tr>\n<td><code>DATABASE_CHANGE</code></td>\n<td>Connection, table, CDC log position</td>\n<td>Transaction ID + sequence number</td>\n</tr>\n<tr>\n<td><code>API_WEBHOOK</code></td>\n<td>Endpoint URL, authentication, payload schema</td>\n<td>Event ID from payload</td>\n</tr>\n<tr>\n<td><code>MESSAGE_QUEUE</code></td>\n<td>Queue name, consumer group, message format</td>\n<td>Message ID + partition offset</td>\n</tr>\n</tbody></table>\n<h4 id=\"schedule-management-interface\">Schedule Management Interface</h4>\n<p>The scheduler exposes a management interface for registering, updating, and monitoring pipeline schedules. This interface supports dynamic schedule updates without requiring system restarts, enabling operational flexibility for changing business requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>register_schedule</code></td>\n<td><code>pipeline_id: str, schedule_config: ScheduleConfig</code></td>\n<td><code>bool</code></td>\n<td>Register new pipeline schedule</td>\n</tr>\n<tr>\n<td><code>update_schedule</code></td>\n<td><code>pipeline_id: str, schedule_config: ScheduleConfig</code></td>\n<td><code>bool</code></td>\n<td>Modify existing pipeline schedule</td>\n</tr>\n<tr>\n<td><code>pause_schedule</code></td>\n<td><code>pipeline_id: str, reason: str</code></td>\n<td><code>bool</code></td>\n<td>Temporarily disable pipeline execution</td>\n</tr>\n<tr>\n<td><code>resume_schedule</code></td>\n<td><code>pipeline_id: str</code></td>\n<td><code>bool</code></td>\n<td>Re-enable paused pipeline schedule</td>\n</tr>\n<tr>\n<td><code>get_next_run_time</code></td>\n<td><code>pipeline_id: str</code></td>\n<td><code>Optional[datetime]</code></td>\n<td>Calculate next scheduled execution</td>\n</tr>\n<tr>\n<td><code>get_schedule_status</code></td>\n<td><code>pipeline_id: str</code></td>\n<td><code>ScheduleStatus</code></td>\n<td>Retrieve current schedule state</td>\n</tr>\n</tbody></table>\n<p>The scheduler maintains <strong>schedule metadata</strong> including creation timestamps, modification history, and execution statistics to support operational monitoring and debugging.</p>\n<h3 id=\"task-execution-engine\">Task Execution Engine</h3>\n<p>The task execution engine orchestrates the actual execution of pipeline tasks, managing parallel execution, resource allocation, and state transitions. This component translates the abstract DAG structure into concrete task executions while maintaining dependency relationships and handling failures gracefully.</p>\n<h4 id=\"parallel-execution-management\">Parallel Execution Management</h4>\n<p>The execution engine implements <strong>level-based parallelization</strong>, executing all tasks at the same DAG level simultaneously while respecting resource constraints. This approach maximizes throughput while ensuring dependency correctness.</p>\n<p>The engine maintains an <strong>execution queue</strong> for each parallelization level, populated by the topological sort results from the DAG validation engine. As tasks complete successfully, the engine evaluates downstream tasks to determine when their dependencies are satisfied and they can be queued for execution.</p>\n<blockquote>\n<p><strong>Decision: Thread Pool vs Process Pool for Task Execution</strong></p>\n<ul>\n<li><strong>Context</strong>: Tasks may include CPU-intensive transformations, I/O-bound data loading, and potentially untrusted user-defined functions</li>\n<li><strong>Options Considered</strong>: Single-threaded sequential execution, thread pool with shared memory, process pool with isolation, hybrid approach based on task type</li>\n<li><strong>Decision</strong>: Configurable hybrid approach with thread pool for I/O-bound tasks and process pool for CPU-intensive or untrusted tasks</li>\n<li><strong>Rationale</strong>: Thread pool provides efficient resource sharing for database connections and network I/O, while process pool ensures isolation for UDFs and prevents memory leaks from affecting other tasks</li>\n<li><strong>Consequences</strong>: Requires task type classification in pipeline definitions, but provides optimal performance and safety characteristics</li>\n</ul>\n</blockquote>\n<p>The execution engine implements <strong>resource-aware scheduling</strong> to prevent system overload. Each task definition includes resource requirements (CPU cores, memory, network connections), and the engine tracks available system resources to determine execution capacity.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Measurement Unit</th>\n<th>Default Limit</th>\n<th>Overflow Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CPU_CORES</code></td>\n<td>Number of cores</td>\n<td>System CPU count</td>\n<td>Queue task until resources available</td>\n</tr>\n<tr>\n<td><code>MEMORY_MB</code></td>\n<td>Megabytes</td>\n<td>75% of system memory</td>\n<td>Queue task with memory pressure warning</td>\n</tr>\n<tr>\n<td><code>DB_CONNECTIONS</code></td>\n<td>Connection count</td>\n<td>Connection pool size</td>\n<td>Queue task until connection available</td>\n</tr>\n<tr>\n<td><code>NETWORK_BANDWIDTH</code></td>\n<td>Mbps</td>\n<td>Unlimited (monitoring only)</td>\n<td>Log warning but continue execution</td>\n</tr>\n</tbody></table>\n<h4 id=\"state-management-and-persistence\">State Management and Persistence</h4>\n<p>The execution engine maintains comprehensive state information for all active task executions, persisting state changes to enable recovery after system failures. The state model supports complex execution patterns including retries, cancellations, and dependency failures.</p>\n<p>Task execution state follows a well-defined state machine with specific transition rules. The engine validates all state transitions to ensure consistency and prevent invalid operations.</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Valid Events</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PENDING</code></td>\n<td><code>DEPENDENCIES_MET</code></td>\n<td><code>QUEUED</code></td>\n<td>Add to execution queue</td>\n</tr>\n<tr>\n<td><code>QUEUED</code></td>\n<td><code>EXECUTION_STARTED</code></td>\n<td><code>RUNNING</code></td>\n<td>Allocate resources, start task</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>EXECUTION_COMPLETED</code></td>\n<td><code>SUCCESS</code></td>\n<td>Release resources, update metrics</td>\n</tr>\n<tr>\n<td><code>RUNNING</code></td>\n<td><code>EXECUTION_FAILED</code></td>\n<td><code>FAILED</code> or <code>RETRYING</code></td>\n<td>Check retry policy, possibly reschedule</td>\n</tr>\n<tr>\n<td><code>FAILED</code></td>\n<td><code>RETRY_SCHEDULED</code></td>\n<td><code>QUEUED</code></td>\n<td>Increment attempt count, apply backoff</td>\n</tr>\n<tr>\n<td><code>RETRYING</code></td>\n<td><code>EXECUTION_STARTED</code></td>\n<td><code>RUNNING</code></td>\n<td>Start retry attempt</td>\n</tr>\n<tr>\n<td><code>SUCCESS</code></td>\n<td><code>UPSTREAM_FAILED</code></td>\n<td><code>SKIPPED</code></td>\n<td>Mark downstream tasks as skipped</td>\n</tr>\n</tbody></table>\n<p>The engine implements <strong>optimistic concurrency control</strong> for state updates, using version numbers to detect concurrent modifications. This ensures state consistency when multiple components (scheduler, execution threads, monitoring) update task state simultaneously.</p>\n<h4 id=\"resource-allocation-and-cleanup\">Resource Allocation and Cleanup</h4>\n<p>The execution engine manages system resources including database connections, temporary files, and memory allocations. Resource management follows a <strong>lease-based model</strong> where each task execution receives a time-bounded lease on required resources.</p>\n<p>Database connections are managed through a <strong>connection pooling strategy</strong> that maintains separate pools for different database types and connection configurations. The engine pre-allocates connections based on upcoming task requirements and implements connection health checking to ensure reliability.</p>\n<table>\n<thead>\n<tr>\n<th>Connection Pool</th>\n<th>Max Size</th>\n<th>Idle Timeout</th>\n<th>Health Check Interval</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PRIMARY_DB</code></td>\n<td>20 connections</td>\n<td>300 seconds</td>\n<td>60 seconds</td>\n</tr>\n<tr>\n<td><code>WAREHOUSE_DB</code></td>\n<td>10 connections</td>\n<td>600 seconds</td>\n<td>120 seconds</td>\n</tr>\n<tr>\n<td><code>STAGING_DB</code></td>\n<td>15 connections</td>\n<td>180 seconds</td>\n<td>30 seconds</td>\n</tr>\n</tbody></table>\n<p>Temporary file management implements <strong>automatic cleanup</strong> with configurable retention policies. The engine creates isolated temporary directories for each task execution and schedules cleanup based on task completion status and retention requirements.</p>\n<p> <strong>Pitfall: Resource Leak in Failed Tasks</strong>\nMany implementations fail to properly clean up resources when tasks fail unexpectedly. This leads to connection pool exhaustion, temporary disk space consumption, and memory leaks. Always implement resource cleanup in finally blocks or using context managers, and include explicit resource leak detection in monitoring dashboards. Track resource allocation per task execution and alert when cleanup doesn&#39;t occur within expected timeframes.</p>\n<h3 id=\"monitoring-and-alerting\">Monitoring and Alerting</h3>\n<p>The monitoring and alerting system provides comprehensive observability into pipeline execution, collecting metrics, logs, and lineage information to support operational management and debugging. This system operates as a separate subsystem that observes execution without interfering with performance.</p>\n<h4 id=\"metrics-collection-and-aggregation\">Metrics Collection and Aggregation</h4>\n<p>The monitoring system collects <strong>multi-dimensional metrics</strong> across different aspects of pipeline execution including performance, reliability, and resource utilization. Metrics collection uses an asynchronous publishing model to minimize impact on task execution performance.</p>\n<p>The system implements <strong>hierarchical metric aggregation</strong>, collecting detailed per-task metrics while rolling up summary statistics at the pipeline and system levels. This approach enables both detailed debugging and high-level operational dashboards.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Examples</th>\n<th>Collection Frequency</th>\n<th>Retention Period</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PERFORMANCE</code></td>\n<td>Task duration, queue wait time, throughput</td>\n<td>Per task execution</td>\n<td>90 days detailed, 1 year aggregated</td>\n</tr>\n<tr>\n<td><code>RELIABILITY</code></td>\n<td>Success rate, failure count, retry attempts</td>\n<td>Per task execution</td>\n<td>180 days detailed, 2 years aggregated</td>\n</tr>\n<tr>\n<td><code>RESOURCE</code></td>\n<td>CPU usage, memory consumption, I/O wait</td>\n<td>Every 30 seconds during execution</td>\n<td>30 days detailed, 6 months aggregated</td>\n</tr>\n<tr>\n<td><code>BUSINESS</code></td>\n<td>Records processed, data quality scores</td>\n<td>Per task execution</td>\n<td>1 year detailed, 5 years aggregated</td>\n</tr>\n</tbody></table>\n<p>The metrics system supports <strong>custom business metrics</strong> defined in pipeline configurations, enabling domain-specific monitoring. Tasks can emit custom metrics using a simple API that automatically handles aggregation and persistence.</p>\n<h4 id=\"real-time-dashboard-integration\">Real-Time Dashboard Integration</h4>\n<p>The monitoring system provides <strong>real-time dashboard integration</strong> through standardized APIs that support popular monitoring platforms. The dashboard data model emphasizes actionable information over raw metrics, presenting derived insights that guide operational decisions.</p>\n<p>The system implements <strong>adaptive alerting thresholds</strong> that automatically adjust based on historical performance patterns and seasonal variations. This reduces false positive alerts while maintaining sensitivity to genuine issues.</p>\n<table>\n<thead>\n<tr>\n<th>Dashboard View</th>\n<th>Update Frequency</th>\n<th>Key Metrics</th>\n<th>Drill-Down Capability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SYSTEM_OVERVIEW</code></td>\n<td>30 seconds</td>\n<td>Active pipelines, success rate, resource utilization</td>\n<td>Pipeline-level details</td>\n</tr>\n<tr>\n<td><code>PIPELINE_DETAIL</code></td>\n<td>15 seconds</td>\n<td>Task status, execution timeline, dependency graph</td>\n<td>Individual task logs</td>\n</tr>\n<tr>\n<td><code>RESOURCE_MONITORING</code></td>\n<td>10 seconds</td>\n<td>CPU, memory, network, storage I/O</td>\n<td>Historical trending</td>\n</tr>\n<tr>\n<td><code>DATA_QUALITY</code></td>\n<td>Per pipeline run</td>\n<td>Validation failures, schema changes, row counts</td>\n<td>Quality rule details</td>\n</tr>\n</tbody></table>\n<h4 id=\"failure-notification-and-escalation\">Failure Notification and Escalation</h4>\n<p>The alerting system implements <strong>intelligent failure classification</strong> that categorizes failures by severity, impact, and required response. This classification drives different notification channels and escalation timelines.</p>\n<p>The system maintains <strong>alert routing rules</strong> that direct notifications to appropriate teams based on failure characteristics, time of day, and team availability. Integration with on-call scheduling systems ensures critical failures reach available personnel quickly.</p>\n<blockquote>\n<p><strong>Decision: Push vs Pull Alerting Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance timely notifications with alert fatigue while supporting multiple notification channels</li>\n<li><strong>Options Considered</strong>: Pure push model with immediate notifications, pure pull model with polling-based alerts, hybrid model with intelligent routing</li>\n<li><strong>Decision</strong>: Hybrid model with immediate push for critical failures and batched notifications for non-critical issues</li>\n<li><strong>Rationale</strong>: Critical failures (data corruption, security issues) require immediate attention, while transient failures (temporary network issues) benefit from aggregation to reduce noise</li>\n<li><strong>Consequences</strong>: Requires failure severity classification but significantly improves alert signal-to-noise ratio and reduces operational fatigue</li>\n</ul>\n</blockquote>\n<p>The notification system supports <strong>alert suppression and correlation</strong> to prevent alert storms during widespread system issues. When multiple related failures occur simultaneously, the system identifies root cause relationships and suppresses downstream alerts.</p>\n<table>\n<thead>\n<tr>\n<th>Alert Severity</th>\n<th>Notification Channel</th>\n<th>Response Time SLA</th>\n<th>Escalation Timeline</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CRITICAL</code></td>\n<td>Phone, SMS, Slack</td>\n<td>Immediate</td>\n<td>15 minutes to secondary on-call</td>\n</tr>\n<tr>\n<td><code>HIGH</code></td>\n<td>Email, Slack</td>\n<td>5 minutes</td>\n<td>1 hour to team lead</td>\n</tr>\n<tr>\n<td><code>MEDIUM</code></td>\n<td>Email</td>\n<td>15 minutes</td>\n<td>4 hours to product owner</td>\n</tr>\n<tr>\n<td><code>LOW</code></td>\n<td>Dashboard only</td>\n<td>Best effort</td>\n<td>Weekly summary report</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-lineage-and-audit-trail\">Data Lineage and Audit Trail</h4>\n<p>The monitoring system captures comprehensive <strong>data lineage information</strong> that tracks data flow through transformation steps and across pipeline boundaries. This lineage information supports impact analysis, debugging, and compliance requirements.</p>\n<p>Lineage tracking operates at the <strong>dataset and column level</strong>, recording not just which tables were read and written, but which specific columns influenced which outputs. This granular tracking enables precise impact analysis when upstream data changes or issues occur.</p>\n<p>The system maintains an <strong>immutable audit trail</strong> of all pipeline executions, configuration changes, and manual interventions. This audit trail supports compliance requirements and provides a complete historical record for debugging complex issues.</p>\n<table>\n<thead>\n<tr>\n<th>Lineage Event Type</th>\n<th>Captured Information</th>\n<th>Retention Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>DATA_READ</code></td>\n<td>Source table, query, row count, timestamp</td>\n<td>2 years</td>\n</tr>\n<tr>\n<td><code>DATA_WRITE</code></td>\n<td>Target table, operation type, row count, schema</td>\n<td>5 years</td>\n</tr>\n<tr>\n<td><code>TRANSFORMATION</code></td>\n<td>Function name, input columns, output columns, parameters</td>\n<td>2 years</td>\n</tr>\n<tr>\n<td><code>SCHEMA_CHANGE</code></td>\n<td>Old schema, new schema, compatibility check results</td>\n<td>10 years</td>\n</tr>\n</tbody></table>\n<p> <strong>Pitfall: Lineage Collection Performance Impact</strong>\nDetailed lineage collection can significantly impact pipeline performance if not implemented carefully. Avoid synchronous lineage writes during task execution - instead, buffer lineage events in memory and flush asynchronously. Use sampling for high-throughput pipelines and implement circuit breakers to disable lineage collection if it begins affecting SLA compliance. Monitor lineage collection overhead and tune based on business requirements.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Task Queue</td>\n<td>Redis with <code>rq</code> library</td>\n<td>Apache Airflow or Celery with RabbitMQ</td>\n</tr>\n<tr>\n<td>Metrics Storage</td>\n<td>SQLite with custom tables</td>\n<td>InfluxDB or Prometheus</td>\n</tr>\n<tr>\n<td>Log Aggregation</td>\n<td>File-based logging with logrotate</td>\n<td>ELK stack (Elasticsearch, Logstash, Kibana)</td>\n</tr>\n<tr>\n<td>Alerting</td>\n<td>Email via <code>smtplib</code></td>\n<td>PagerDuty API integration</td>\n</tr>\n<tr>\n<td>Dashboard</td>\n<td>Flask web UI with simple HTML/CSS</td>\n<td>Grafana with custom dashboards</td>\n</tr>\n<tr>\n<td>State Persistence</td>\n<td>SQLite database</td>\n<td>PostgreSQL with connection pooling</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>pipeline_orchestration/\n __init__.py\n scheduler/\n    __init__.py\n    cron_scheduler.py      # Time-based scheduling logic\n    event_scheduler.py     # Event-driven trigger handling\n    schedule_manager.py    # Schedule registration and management\n executor/\n    __init__.py\n    task_executor.py       # Core task execution engine\n    resource_manager.py    # Resource allocation and cleanup\n    state_manager.py       # Task state persistence and transitions\n monitoring/\n    __init__.py\n    metrics_collector.py   # Metrics collection and aggregation\n    alerting.py           # Alert routing and notification\n    lineage_tracker.py    # Data lineage capture and storage\n web/\n     __init__.py\n     dashboard.py          # Web dashboard for monitoring\n     api.py               # REST API for external integrations</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Basic Metrics Collection System:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict, deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sqlite3</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricsCollector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe metrics collection with automatic aggregation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, db_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"metrics.db\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics_buffer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(deque)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._init_db()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._start_background_flush()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _init_db</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize metrics database schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE TABLE IF NOT EXISTS task_metrics (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    id INTEGER PRIMARY KEY AUTOINCREMENT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    task_id TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    pipeline_run_id TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    metric_name TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    metric_value REAL NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    labels TEXT,  -- JSON string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    timestamp DATETIME NOT NULL</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE INDEX IF NOT EXISTS idx_metrics_task_time </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                ON task_metrics(task_id, timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_metric</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     metric_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     labels: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record a metric value for a specific task.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metric_record </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'task_id'</span><span style=\"color:#E1E4E8\">: task_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'pipeline_run_id'</span><span style=\"color:#E1E4E8\">: pipeline_run_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'metric_name'</span><span style=\"color:#E1E4E8\">: metric_name,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'value'</span><span style=\"color:#E1E4E8\">: value,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'labels'</span><span style=\"color:#E1E4E8\">: json.dumps(labels </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.metrics_buffer[task_id].append(metric_record)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _flush_metrics</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Flush buffered metrics to database.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.metrics_buffer:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> task_metrics </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.metrics_buffer.values():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                all_metrics.extend(task_metrics)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.metrics_buffer.clear()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> all_metrics:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conn.executemany(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    INSERT INTO task_metrics </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    (task_id, pipeline_run_id, metric_name, metric_value, labels, timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    VALUES (?, ?, ?, ?, ?, ?)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">, [(m[</span><span style=\"color:#9ECBFF\">'task_id'</span><span style=\"color:#E1E4E8\">], m[</span><span style=\"color:#9ECBFF\">'pipeline_run_id'</span><span style=\"color:#E1E4E8\">], m[</span><span style=\"color:#9ECBFF\">'metric_name'</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      m[</span><span style=\"color:#9ECBFF\">'value'</span><span style=\"color:#E1E4E8\">], m[</span><span style=\"color:#9ECBFF\">'labels'</span><span style=\"color:#E1E4E8\">], m[</span><span style=\"color:#9ECBFF\">'timestamp'</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> m </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> all_metrics])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleStateManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe task state management with persistence.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, db_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"task_state.db\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._init_db()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _init_db</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize state database schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE TABLE IF NOT EXISTS task_executions (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    task_id TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    pipeline_run_id TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    state TEXT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    attempt_count INTEGER NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    started_at DATETIME,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    completed_at DATETIME,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    error_message TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    version INTEGER NOT NULL DEFAULT 1,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    PRIMARY KEY (task_id, pipeline_run_id)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_execution</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> TaskExecution:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new task execution in PENDING state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        execution </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TaskExecution(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            task_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">task_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            pipeline_run_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pipeline_run_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            state</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">TaskState.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            attempt_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            started_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            completed_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            error_message</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            logs</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            metrics</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                INSERT OR REPLACE INTO task_executions </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                (task_id, pipeline_run_id, state, attempt_count)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                VALUES (?, ?, ?, ?)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">, (task_id, pipeline_run_id, execution.state.value, execution.attempt_count))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> execution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transition_state</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        event: TaskEvent, error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Attempt state transition based on event.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Implementation details in skeleton below</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BasicAlertManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple alerting system with email and webhook support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.alert_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Keep recent alerts for suppression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> send_alert</span><span style=\"color:#E1E4E8\">(self, severity: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, title: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send alert through configured channels based on severity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        alert </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'severity'</span><span style=\"color:#E1E4E8\">: severity,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'title'</span><span style=\"color:#E1E4E8\">: title,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'message'</span><span style=\"color:#E1E4E8\">: message,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'task_id'</span><span style=\"color:#E1E4E8\">: task_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'pipeline_id'</span><span style=\"color:#E1E4E8\">: pipeline_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for suppression (simplified)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._should_suppress_alert(alert):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.alert_history.append(alert)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Route based on severity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        channels </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.get(</span><span style=\"color:#9ECBFF\">'alert_channels'</span><span style=\"color:#E1E4E8\">, {}).get(severity, [</span><span style=\"color:#9ECBFF\">'email'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> channel </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> channels:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._send_to_channel(channel, alert)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Task Execution Engine Core:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskExecutionEngine</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates parallel execution of pipeline tasks with resource management.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#E1E4E8\">, resource_limits: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_workers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_workers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.resource_limits </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> resource_limits </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_executions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TaskExecution] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.execution_queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue.Queue()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MetricsCollector()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SimpleStateManager()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute complete pipeline with dependency management and monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create execution plan using topological sort from DAG engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use create_execution_plan(pipeline, estimated_durations)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize task executions in PENDING state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Create TaskExecution for each task using state_manager.create_execution()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start execution levels in dependency order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Process execution_plan.execution_levels sequentially, tasks within level in parallel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Monitor running tasks and handle state transitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use worker threads to poll task status and call _handle_task_completion()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle failures and determine if pipeline should continue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check retry policies, update dependent task states, decide on pipeline failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Clean up resources and persist final state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Release all allocated resources, flush metrics, update pipeline run status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _execute_task_level</span><span style=\"color:#E1E4E8\">(self, task_ids: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all tasks in a dependency level in parallel.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check resource availability for all tasks in level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Sum resource requirements and compare to limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Allocate resources and transition tasks to QUEUED state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Reserve DB connections, temp directories, update task state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Submit tasks to thread pool for parallel execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use ThreadPoolExecutor or ProcessPoolExecutor based on task type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Wait for all tasks to complete with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use concurrent.futures.wait() with appropriate timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Collect results and handle any exceptions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check Future.exception() for each completed task</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># task_id -> success mapping</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _execute_single_task</span><span style=\"color:#E1E4E8\">(self, task_def: TaskDefinition, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute individual task with monitoring and error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        task_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> task_def.id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Transition task to RUNNING state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use state_manager.transition_state() with EXECUTION_STARTED event</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load appropriate connector/transformer based on task type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use factory pattern to instantiate correct handler for task_def.type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute task with timeout and resource monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Wrap execution in try/except, measure resource usage, enforce timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record execution metrics (duration, rows processed, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use metrics.record_metric() for performance and business metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle success/failure and update task state appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Call state_manager.transition_state() with appropriate event</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Clean up any task-specific resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Close connections, delete temp files, release memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#6A737D\">  # Return success/failure</span></span></code></pre></div>\n\n<p><strong>Scheduler Integration Core:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CronScheduler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handles time-based pipeline scheduling with catchup and overlap handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, execution_engine: TaskExecutionEngine):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.execution_engine </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> execution_engine</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.registered_pipelines: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, PipelineDefinition] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_runs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># pipeline_id -> run_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_scheduler</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the scheduler polling loop.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._scheduler_loop, </span><span style=\"color:#FFAB70\">daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">).start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline: PipelineDefinition) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register pipeline for scheduled execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate pipeline definition and schedule expression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use validate_pipeline() and parse cron expression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for schedule conflicts with existing pipelines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Evaluate if overlapping schedules might cause resource conflicts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store pipeline in registry with schedule metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Include next run time calculation and execution policy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log registration and trigger initial schedule calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Calculate next execution time and log for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _scheduler_loop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main scheduler polling loop that evaluates and triggers pipelines.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate current time and evaluation window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Use timezone-aware datetime, consider scheduling tolerance window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Evaluate each registered pipeline for execution readiness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Check if current time matches cron expression, handle missed executions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply execution policies for ready pipelines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Check ALLOW_CONCURRENT, SKIP_ON_RUNNING, etc. policies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Trigger pipeline executions and track active runs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Generate run_id, call execution_engine.execute_pipeline(), update tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Clean up completed runs and update metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Remove from active_runs, record scheduling metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Sleep until next evaluation cycle</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Use configurable poll interval, typically 30-60 seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Default poll interval</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the orchestration and monitoring components, verify the system with these checkpoints:</p>\n<p><strong>Test Command:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_orchestration.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/run_sample_pipeline.py</span><span style=\"color:#79B8FF\"> --pipeline</span><span style=\"color:#9ECBFF\"> simple_etl</span><span style=\"color:#79B8FF\"> --monitor</span><span style=\"color:#79B8FF\"> true</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ol>\n<li><strong>Scheduler Registration</strong>: Register a simple pipeline with 5-minute cron schedule, verify next execution time calculation</li>\n<li><strong>Task Execution</strong>: Execute pipeline manually, observe parallel task execution within dependency levels</li>\n<li><strong>State Transitions</strong>: Monitor task state changes from PENDING  QUEUED  RUNNING  SUCCESS</li>\n<li><strong>Metrics Collection</strong>: Verify metrics are collected for task duration, success rate, and resource usage</li>\n<li><strong>Alert Generation</strong>: Trigger a task failure and verify alert is generated with appropriate severity</li>\n<li><strong>Resource Management</strong>: Execute multiple pipelines simultaneously and verify resource limits are respected</li>\n</ol>\n<p><strong>Verification Steps:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check scheduler registration</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/schedules</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> jq</span><span style=\"color:#9ECBFF\"> .</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Monitor pipeline execution</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/runs/active</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> jq</span><span style=\"color:#9ECBFF\"> .</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># View collected metrics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">sqlite3</span><span style=\"color:#9ECBFF\"> metrics.db</span><span style=\"color:#9ECBFF\"> \"SELECT * FROM task_metrics ORDER BY timestamp DESC LIMIT 10;\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test alerting</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/test/trigger-failure</span></span></code></pre></div>\n\n<p><strong>Signs of Issues:</strong></p>\n<ul>\n<li>Tasks stuck in QUEUED state: Check resource availability and thread pool configuration</li>\n<li>Missing metrics: Verify metrics collector background flush is running</li>\n<li>Alert flooding: Check alert suppression logic and severity classification</li>\n<li>Memory leaks during long runs: Verify resource cleanup in finally blocks</li>\n</ul>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - describes how components from pipeline definition (Milestone 1), data processing (Milestones 2-3), and orchestration (Milestone 4) interact through well-defined interfaces and protocols.</p>\n</blockquote>\n<h3 id=\"mental-model-orchestra-conductor\">Mental Model: Orchestra Conductor</h3>\n<p>Think of the ETL system as a symphony orchestra performing a complex musical piece. The <strong>DAG Definition Engine</strong> is like the sheet music - it defines what needs to be played, when, and in what order. The <strong>Pipeline Orchestration Engine</strong> acts as the conductor, coordinating timing, cueing different sections, and managing the overall flow. Individual <strong>task executors</strong> are like musicians - each with specialized skills (violinists, trumpeters, percussionists) who execute their parts when signaled. The <strong>monitoring system</strong> is like the audience and recording equipment - observing the performance, capturing what happened, and providing feedback. Just as musicians must follow precise timing and hand-off cues between sections, ETL components communicate through well-defined message protocols and data contracts.</p>\n<p>The beauty of this orchestration lies in the coordination - when the conductor raises their baton to start the performance, a cascade of precisely-timed interactions begins. First movement starts with strings (extraction tasks), then woodwinds join in (transformation tasks), finally brass completes the harmony (loading tasks). Each musician knows exactly when to play their part based on visual cues from the conductor and audio cues from other sections. Similarly, each ETL component knows when to execute based on state transitions and dependency signals from other components.</p>\n<h3 id=\"component-communication\">Component Communication</h3>\n<p>The ETL system components communicate through a combination of <strong>synchronous API calls</strong> for control operations and <strong>asynchronous message passing</strong> for data processing events. This hybrid approach balances the need for immediate feedback on critical operations with the scalability requirements of high-throughput data processing.</p>\n<blockquote>\n<p><strong>Decision: Hybrid Synchronous/Asynchronous Communication</strong></p>\n<ul>\n<li><strong>Context</strong>: Components need both immediate control feedback and scalable event processing</li>\n<li><strong>Options Considered</strong>: Pure synchronous APIs, pure message queues, hybrid approach</li>\n<li><strong>Decision</strong>: Hybrid synchronous APIs for control, asynchronous messages for events</li>\n<li><strong>Rationale</strong>: Control operations need immediate success/failure feedback, but data events need decoupling for scalability</li>\n<li><strong>Consequences</strong>: Enables responsive control plane while maintaining scalable data plane, but requires managing two communication patterns</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Communication Type</th>\n<th>Use Cases</th>\n<th>Pattern</th>\n<th>Benefits</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Synchronous APIs</td>\n<td>Pipeline registration, schedule updates, manual triggers</td>\n<td>Request/Response</td>\n<td>Immediate feedback, simple error handling</td>\n<td>Blocking operations, limited scalability</td>\n</tr>\n<tr>\n<td>Asynchronous Messages</td>\n<td>Task state changes, metrics collection, lineage events</td>\n<td>Pub/Sub</td>\n<td>High throughput, loose coupling</td>\n<td>Eventual consistency, complex error handling</td>\n</tr>\n<tr>\n<td>Shared State</td>\n<td>Task execution status, pipeline run metadata</td>\n<td>Database/Cache</td>\n<td>Persistent state, complex queries</td>\n<td>Potential bottleneck, consistency complexity</td>\n</tr>\n</tbody></table>\n<h4 id=\"control-plane-apis\">Control Plane APIs</h4>\n<p>The <strong>control plane</strong> handles pipeline management operations through RESTful APIs. These operations require immediate feedback and typically have lower frequency but higher reliability requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Request Format</th>\n<th>Response Format</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>register_pipeline</code></td>\n<td>POST /api/v1/pipelines</td>\n<td><code>PipelineDefinition</code></td>\n<td><code>{&quot;pipeline_id&quot;: str, &quot;version&quot;: int}</code></td>\n<td>Register new pipeline definition</td>\n</tr>\n<tr>\n<td><code>update_pipeline</code></td>\n<td>PUT /api/v1/pipelines/{id}</td>\n<td><code>PipelineDefinition</code></td>\n<td><code>{&quot;pipeline_id&quot;: str, &quot;version&quot;: int}</code></td>\n<td>Update existing pipeline</td>\n</tr>\n<tr>\n<td><code>trigger_pipeline</code></td>\n<td>POST /api/v1/pipelines/{id}/runs</td>\n<td><code>{&quot;parameters&quot;: dict, &quot;priority&quot;: int}</code></td>\n<td><code>{&quot;run_id&quot;: str, &quot;status&quot;: str}</code></td>\n<td>Manually trigger pipeline execution</td>\n</tr>\n<tr>\n<td><code>get_pipeline_status</code></td>\n<td>GET /api/v1/pipelines/{id}/status</td>\n<td>None</td>\n<td><code>{&quot;status&quot;: str, &quot;last_run&quot;: datetime, &quot;next_run&quot;: datetime}</code></td>\n<td>Retrieve pipeline execution status</td>\n</tr>\n<tr>\n<td><code>pause_pipeline</code></td>\n<td>POST /api/v1/pipelines/{id}/pause</td>\n<td><code>{&quot;reason&quot;: str}</code></td>\n<td><code>{&quot;success&quot;: bool}</code></td>\n<td>Temporarily disable pipeline</td>\n</tr>\n<tr>\n<td><code>get_task_logs</code></td>\n<td>GET /api/v1/runs/{run_id}/tasks/{task_id}/logs</td>\n<td>None</td>\n<td><code>{&quot;logs&quot;: List[str], &quot;metrics&quot;: dict}</code></td>\n<td>Retrieve task execution logs</td>\n</tr>\n</tbody></table>\n<p>Each API endpoint follows a standard request/response pattern with consistent error handling. All endpoints return HTTP status codes that map directly to operation outcomes: 200 for success, 400 for validation errors, 404 for missing resources, 500 for system errors. Error responses include structured error objects with error codes, human-readable messages, and context information for debugging.</p>\n<h4 id=\"data-plane-messaging\">Data Plane Messaging</h4>\n<p>The <strong>data plane</strong> handles high-frequency operational events through asynchronous messaging. This system uses a publish/subscribe pattern where components publish events to named topics and subscribe to events they need to process.</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Topic</th>\n<th>Message Format</th>\n<th>Publisher</th>\n<th>Subscribers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Task State Change</td>\n<td><code>task.state.{pipeline_id}</code></td>\n<td><code>TaskStateEvent</code></td>\n<td>Task Executor</td>\n<td>Orchestrator, Monitoring</td>\n</tr>\n<tr>\n<td>Data Lineage</td>\n<td><code>lineage.{pipeline_id}</code></td>\n<td><code>LineageEvent</code></td>\n<td>Connectors, Transformers</td>\n<td>Lineage Tracker</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td><code>metrics.{component}</code></td>\n<td><code>MetricsEvent</code></td>\n<td>All Components</td>\n<td>Monitoring Dashboard</td>\n</tr>\n<tr>\n<td>Pipeline Progress</td>\n<td><code>progress.{run_id}</code></td>\n<td><code>ProgressEvent</code></td>\n<td>Orchestrator</td>\n<td>UI, Alerting</td>\n</tr>\n<tr>\n<td>Resource Usage</td>\n<td><code>resources.{executor_id}</code></td>\n<td><code>ResourceEvent</code></td>\n<td>Task Executors</td>\n<td>Resource Manager</td>\n</tr>\n</tbody></table>\n<p><strong>Message Format Specifications:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Required</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>event_id</code></td>\n<td>str</td>\n<td>Unique event identifier</td>\n<td>Yes</td>\n<td>&quot;evt_123e4567-e89b-12d3&quot;</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>datetime</td>\n<td>Event occurrence time</td>\n<td>Yes</td>\n<td>&quot;2024-01-15T14:30:00Z&quot;</td>\n</tr>\n<tr>\n<td><code>source_component</code></td>\n<td>str</td>\n<td>Component that generated event</td>\n<td>Yes</td>\n<td>&quot;task_executor_3&quot;</td>\n</tr>\n<tr>\n<td><code>event_type</code></td>\n<td>str</td>\n<td>Specific event classification</td>\n<td>Yes</td>\n<td>&quot;TASK_STATE_CHANGED&quot;</td>\n</tr>\n<tr>\n<td><code>pipeline_id</code></td>\n<td>str</td>\n<td>Associated pipeline identifier</td>\n<td>Yes</td>\n<td>&quot;customer_data_pipeline&quot;</td>\n</tr>\n<tr>\n<td><code>run_id</code></td>\n<td>str</td>\n<td>Associated pipeline run</td>\n<td>Yes</td>\n<td>&quot;run_20240115_143000&quot;</td>\n</tr>\n<tr>\n<td><code>task_id</code></td>\n<td>str</td>\n<td>Associated task identifier</td>\n<td>No</td>\n<td>&quot;extract_customer_data&quot;</td>\n</tr>\n<tr>\n<td><code>payload</code></td>\n<td>dict</td>\n<td>Event-specific data</td>\n<td>Yes</td>\n<td>{&quot;old_state&quot;: &quot;RUNNING&quot;, &quot;new_state&quot;: &quot;SUCCESS&quot;}</td>\n</tr>\n<tr>\n<td><code>correlation_id</code></td>\n<td>str</td>\n<td>Request correlation identifier</td>\n<td>No</td>\n<td>&quot;req_987fcdeb-51a2-34b5&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"inter-component-state-synchronization\">Inter-Component State Synchronization</h4>\n<p>Components maintain consistency through a combination of <strong>authoritative state ownership</strong> and <strong>event-driven synchronization</strong>. Each component owns specific state domains and publishes changes as events that other components can consume to maintain their derived views.</p>\n<blockquote>\n<p><strong>Key Design Principle</strong>: State ownership is clearly partitioned - the Orchestrator owns pipeline run state, Task Executors own task execution state, and the Monitoring system owns aggregated metrics. No component directly modifies another component&#39;s authoritative state.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Owned State</th>\n<th>Published Events</th>\n<th>Subscribed Events</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DAG Engine</td>\n<td>Pipeline definitions, validation results</td>\n<td><code>pipeline.registered</code>, <code>dag.validated</code></td>\n<td>None</td>\n</tr>\n<tr>\n<td>Orchestrator</td>\n<td>Pipeline runs, execution plans</td>\n<td><code>run.started</code>, <code>run.completed</code>, <code>task.scheduled</code></td>\n<td><code>task.state.*</code>, <code>executor.heartbeat</code></td>\n</tr>\n<tr>\n<td>Task Executor</td>\n<td>Task executions, resource usage</td>\n<td><code>task.state.*</code>, <code>metrics.executor</code></td>\n<td><code>task.scheduled</code>, <code>run.cancelled</code></td>\n</tr>\n<tr>\n<td>Scheduler</td>\n<td>Schedule configurations, next run times</td>\n<td><code>schedule.triggered</code>, <code>schedule.updated</code></td>\n<td><code>run.completed</code>, <code>pipeline.updated</code></td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Aggregated metrics, alerts</td>\n<td><code>alert.triggered</code>, <code>metrics.aggregated</code></td>\n<td><code>task.state.*</code>, <code>metrics.*</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"pipeline-execution-flow\">Pipeline Execution Flow</h3>\n<p>A complete pipeline execution involves a carefully orchestrated sequence of interactions between components. Understanding this flow is crucial for debugging issues and optimizing performance.</p>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Fdag-execution-flow.svg\" alt=\"DAG Execution Sequence\"></p>\n<h4 id=\"phase-1-pipeline-trigger-and-validation\">Phase 1: Pipeline Trigger and Validation</h4>\n<p>The execution flow begins when the <strong>Scheduler</strong> determines that a pipeline should run based on its schedule configuration or when a manual trigger is received through the control API.</p>\n<ol>\n<li><p><strong>Schedule Evaluation</strong>: The Scheduler evaluates all active pipeline schedules using their cron expressions and determines which pipelines are ready to execute. It checks the current time against the <code>get_next_run_time()</code> calculation for each pipeline.</p>\n</li>\n<li><p><strong>Trigger Generation</strong>: When a pipeline&#39;s execution time arrives, the Scheduler generates a trigger event containing the pipeline ID, scheduled execution time, and any default parameters defined in the schedule configuration.</p>\n</li>\n<li><p><strong>Trigger Validation</strong>: The Orchestrator receives the trigger event and performs initial validation checks. It verifies that the pipeline definition exists, is not paused, and that the execution policy allows a new run (checking concurrent execution rules).</p>\n</li>\n<li><p><strong>Run Instance Creation</strong>: The Orchestrator creates a new pipeline run instance with a unique <code>run_id</code>, initial state of <code>INITIALIZING</code>, and timestamps for tracking. It also resolves any runtime parameters by merging scheduled parameters with pipeline defaults.</p>\n</li>\n<li><p><strong>DAG Retrieval and Validation</strong>: The Orchestrator calls <code>get_pipeline(pipeline_id)</code> to retrieve the current pipeline definition, then invokes <code>validate_pipeline(pipeline)</code> to ensure the DAG is still valid (no cycles, all task definitions complete).</p>\n</li>\n<li><p><strong>Execution Plan Generation</strong>: The Orchestrator calls <code>create_execution_plan(pipeline, task_durations)</code> to determine the optimal execution order, parallel execution levels, and resource requirements. This plan guides the entire execution process.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Critical Insight</strong>: The execution plan is generated fresh for each run, allowing the system to incorporate updated task duration estimates and current resource availability. This dynamic planning enables better resource utilization than static execution orders.</p>\n</blockquote>\n<h4 id=\"phase-2-task-scheduling-and-dependency-resolution\">Phase 2: Task Scheduling and Dependency Resolution</h4>\n<p>Once the execution plan is ready, the Orchestrator begins scheduling individual tasks based on their dependency relationships and resource requirements.</p>\n<ol start=\"7\">\n<li><p><strong>Initial Task Identification</strong>: The Orchestrator examines the execution plan&#39;s first level (tasks with no dependencies) and marks them as eligible for scheduling. These tasks transition from <code>PENDING</code> to <code>WAITING</code> state.</p>\n</li>\n<li><p><strong>Resource Availability Check</strong>: For each eligible task, the Orchestrator queries available Task Executors to find those with sufficient resources (CPU, memory, storage) to handle the task based on its resource requirements specification.</p>\n</li>\n<li><p><strong>Task Assignment</strong>: The Orchestrator assigns tasks to available executors using a load balancing algorithm that considers current executor load, task resource requirements, and data locality hints. Each assignment creates a <code>TaskExecution</code> record with initial state <code>QUEUED</code>.</p>\n</li>\n<li><p><strong>Execution Message Dispatch</strong>: The Orchestrator publishes task execution messages to the <code>task.scheduled</code> topic, containing task definitions, runtime parameters, and execution context. Task Executors subscribe to these messages and begin processing assigned tasks.</p>\n</li>\n<li><p><strong>Dependency Tracking</strong>: The Orchestrator maintains a dependency tracking matrix that monitors which tasks are running, completed, or failed. This matrix enables efficient determination of when downstream tasks become eligible for execution.</p>\n</li>\n</ol>\n<h4 id=\"phase-3-task-execution-and-state-management\">Phase 3: Task Execution and State Management</h4>\n<p>Task Executors receive task assignments and manage the actual execution of extraction, transformation, and loading operations.</p>\n<ol start=\"12\">\n<li><p><strong>Task Initialization</strong>: When a Task Executor receives a task assignment, it transitions the task state to <code>RUNNING</code> and publishes a state change event. It also allocates local resources and establishes any required connections to data sources.</p>\n</li>\n<li><p><strong>Data Processing Execution</strong>: The executor invokes the appropriate connector or transformer based on the task type. For extraction tasks, it calls <code>extract(query, options)</code> to retrieve data. For transformation tasks, it calls transformation functions. For loading tasks, it calls <code>load(data_stream, target, options)</code>.</p>\n</li>\n<li><p><strong>Progress Monitoring</strong>: During execution, the Task Executor periodically publishes progress events containing metrics like records processed, bytes transferred, and execution time. These events enable real-time monitoring and early detection of performance issues.</p>\n</li>\n<li><p><strong>Error Handling and Retries</strong>: If a task encounters an error, the executor evaluates the task&#39;s <code>RetryPolicy</code> to determine whether to retry immediately, schedule a delayed retry with exponential backoff, or mark the task as failed. Each decision triggers appropriate state transitions.</p>\n</li>\n<li><p><strong>Task Completion</strong>: Upon successful completion, the executor publishes a task completion event with final metrics and output metadata. It also performs cleanup of local resources and temporary files.</p>\n</li>\n</ol>\n<h4 id=\"phase-4-dependency-propagation-and-pipeline-completion\">Phase 4: Dependency Propagation and Pipeline Completion</h4>\n<p>As tasks complete, the Orchestrator updates its dependency tracking and schedules downstream tasks that become eligible for execution.</p>\n<ol start=\"17\">\n<li><p><strong>Dependency Update</strong>: When the Orchestrator receives a task completion event, it updates its dependency matrix and identifies downstream tasks whose dependencies are now satisfied. These tasks transition from <code>PENDING</code> to <code>WAITING</code> state.</p>\n</li>\n<li><p><strong>Next Level Scheduling</strong>: The Orchestrator repeats the task scheduling process (steps 8-11) for newly eligible tasks, maintaining the parallel execution levels defined in the execution plan while respecting resource constraints.</p>\n</li>\n<li><p><strong>Pipeline Progress Tracking</strong>: The Orchestrator continuously tracks overall pipeline progress by monitoring the completion ratio of tasks at each execution level. It publishes pipeline progress events that external systems can consume for dashboards and notifications.</p>\n</li>\n<li><p><strong>Failure Impact Analysis</strong>: If any task fails and exhausts its retry attempts, the Orchestrator analyzes the impact on downstream tasks. Depending on the pipeline&#39;s failure policy, it may cancel dependent tasks, mark them as skipped, or continue execution of independent task branches.</p>\n</li>\n<li><p><strong>Pipeline Completion</strong>: The pipeline completes when all tasks have reached terminal states (SUCCESS, FAILED, or SKIPPED). The Orchestrator calculates final pipeline status, aggregates metrics from all tasks, and publishes a pipeline completion event.</p>\n</li>\n</ol>\n<h4 id=\"phase-5-cleanup-and-lineage-recording\">Phase 5: Cleanup and Lineage Recording</h4>\n<p>The final phase ensures proper resource cleanup and captures complete data lineage information for audit and debugging purposes.</p>\n<ol start=\"22\">\n<li><p><strong>Resource Cleanup</strong>: Task Executors perform final cleanup of any resources allocated for the pipeline run, including temporary files, database connections, and allocated memory. They also publish resource deallocation events for capacity planning.</p>\n</li>\n<li><p><strong>Lineage Consolidation</strong>: The Lineage Tracker aggregates all lineage events published during the pipeline run to create a complete data provenance record. This includes source data locations, transformation operations applied, and destination data locations.</p>\n</li>\n<li><p><strong>Metrics Aggregation</strong>: The Monitoring system calculates pipeline-level metrics by aggregating task-level metrics. This includes total execution time, data volume processed, resource utilization, and error rates.</p>\n</li>\n<li><p><strong>Audit Log Creation</strong>: The system generates comprehensive audit logs containing the complete execution history, all state transitions, error messages, and performance metrics. These logs are stored for compliance and debugging purposes.</p>\n</li>\n<li><p><strong>Schedule Update</strong>: The Scheduler updates the pipeline&#39;s next execution time based on its cron schedule and records the completion of the current run. This ensures proper spacing of future executions according to the schedule configuration.</p>\n</li>\n</ol>\n<h3 id=\"data-lineage-tracking\">Data Lineage Tracking</h3>\n<p><strong>Data lineage</strong> provides a complete audit trail of how data flows through the ETL pipeline, enabling data governance, impact analysis, and debugging of data quality issues. The lineage system captures not just what data was processed, but how it was transformed and where it ended up.</p>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Fmonitoring-data-flow.svg\" alt=\"Monitoring and Lineage Data Flow\"></p>\n<h4 id=\"mental-model-evidence-chain-in-investigation\">Mental Model: Evidence Chain in Investigation</h4>\n<p>Think of data lineage like the chain of evidence in a criminal investigation. Every piece of evidence must be tracked from its original location through every person who handled it, every test performed on it, and every conclusion drawn from it. If a piece of evidence becomes contaminated or questions arise about its authenticity, investigators can trace back through the complete chain to identify where problems occurred. Similarly, when data quality issues arise in production reports, data lineage allows you to trace back through every transformation, join, and aggregation to identify the root cause.</p>\n<p>Just as evidence must be handled by authorized personnel following documented procedures, data in the ETL pipeline should only be modified by authorized transformation steps following defined business rules. The lineage system acts like the evidence log book, recording every hand-off and every operation performed.</p>\n<h4 id=\"lineage-data-model\">Lineage Data Model</h4>\n<p>The lineage system tracks relationships between <strong>data assets</strong> (databases, tables, files), <strong>transformations</strong> (ETL tasks), and <strong>pipeline runs</strong> through a graph-based model that captures both schema-level and instance-level lineage.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>lineage_id</code></td>\n<td>str</td>\n<td>Unique identifier for lineage record</td>\n<td>&quot;lineage_123e4567-e89b&quot;</td>\n</tr>\n<tr>\n<td><code>pipeline_run_id</code></td>\n<td>str</td>\n<td>Associated pipeline execution</td>\n<td>&quot;run_20240115_143000&quot;</td>\n</tr>\n<tr>\n<td><code>task_id</code></td>\n<td>str</td>\n<td>Task that performed transformation</td>\n<td>&quot;transform_customer_data&quot;</td>\n</tr>\n<tr>\n<td><code>source_assets</code></td>\n<td>List[AssetReference]</td>\n<td>Input data assets consumed</td>\n<td>[{&quot;type&quot;: &quot;table&quot;, &quot;name&quot;: &quot;raw.customers&quot;}]</td>\n</tr>\n<tr>\n<td><code>target_assets</code></td>\n<td>List[AssetReference]</td>\n<td>Output data assets produced</td>\n<td>[{&quot;type&quot;: &quot;table&quot;, &quot;name&quot;: &quot;clean.customers&quot;}]</td>\n</tr>\n<tr>\n<td><code>transformation_type</code></td>\n<td>str</td>\n<td>Type of operation performed</td>\n<td>&quot;COLUMN_MAPPING&quot;, &quot;AGGREGATION&quot;, &quot;JOIN&quot;</td>\n</tr>\n<tr>\n<td><code>transformation_logic</code></td>\n<td>str</td>\n<td>SQL query or transformation code</td>\n<td>&quot;SELECT customer_id, UPPER(name) FROM raw.customers&quot;</td>\n</tr>\n<tr>\n<td><code>schema_changes</code></td>\n<td>List[SchemaChange]</td>\n<td>Column additions, deletions, renames</td>\n<td>[{&quot;type&quot;: &quot;RENAME&quot;, &quot;from&quot;: &quot;cust_name&quot;, &quot;to&quot;: &quot;customer_name&quot;}]</td>\n</tr>\n<tr>\n<td><code>data_profile</code></td>\n<td>dict</td>\n<td>Statistical summary of processed data</td>\n<td>{&quot;row_count&quot;: 1000000, &quot;null_percentage&quot;: 0.02}</td>\n</tr>\n<tr>\n<td><code>execution_timestamp</code></td>\n<td>datetime</td>\n<td>When transformation occurred</td>\n<td>&quot;2024-01-15T14:35:22Z&quot;</td>\n</tr>\n<tr>\n<td><code>lineage_metadata</code></td>\n<td>dict</td>\n<td>Additional context and annotations</td>\n<td>{&quot;business_purpose&quot;: &quot;data cleanup&quot;, &quot;data_steward&quot;: &quot;alice&quot;}</td>\n</tr>\n</tbody></table>\n<p><strong>AssetReference Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>asset_type</code></td>\n<td>str</td>\n<td>Type of data asset</td>\n<td>&quot;database_table&quot;, &quot;file&quot;, &quot;api_endpoint&quot;</td>\n</tr>\n<tr>\n<td><code>asset_name</code></td>\n<td>str</td>\n<td>Fully qualified asset name</td>\n<td>&quot;warehouse.clean.customers&quot;</td>\n</tr>\n<tr>\n<td><code>asset_schema</code></td>\n<td>str</td>\n<td>Schema or namespace</td>\n<td>&quot;clean&quot;</td>\n</tr>\n<tr>\n<td><code>location_uri</code></td>\n<td>str</td>\n<td>Physical location or connection string</td>\n<td>&quot;postgresql://warehouse:5432/clean/customers&quot;</td>\n</tr>\n<tr>\n<td><code>partition_info</code></td>\n<td>dict</td>\n<td>Partition or file path details</td>\n<td>{&quot;date_partition&quot;: &quot;2024-01-15&quot;, &quot;file_path&quot;: &quot;/data/customers/2024/01/15/&quot;}</td>\n</tr>\n<tr>\n<td><code>column_lineage</code></td>\n<td>List[ColumnLineage]</td>\n<td>Field-level transformation details</td>\n<td>[{&quot;source_column&quot;: &quot;cust_name&quot;, &quot;target_column&quot;: &quot;customer_name&quot;, &quot;transformation&quot;: &quot;UPPER()&quot;}]</td>\n</tr>\n</tbody></table>\n<h4 id=\"lineage-collection-strategy\">Lineage Collection Strategy</h4>\n<p>The ETL system employs a <strong>multi-level lineage collection</strong> strategy that captures lineage information at different granularities depending on the transformation complexity and business requirements.</p>\n<blockquote>\n<p><strong>Decision: Multi-Level Lineage Collection</strong></p>\n<ul>\n<li><strong>Context</strong>: Different stakeholders need lineage at different levels of detail</li>\n<li><strong>Options Considered</strong>: Table-level only, column-level only, configurable multi-level</li>\n<li><strong>Decision</strong>: Implement configurable multi-level collection with table, column, and value-level tracking</li>\n<li><strong>Rationale</strong>: Business users need table-level for impact analysis, data stewards need column-level for compliance, developers need value-level for debugging</li>\n<li><strong>Consequences</strong>: Enables flexible lineage reporting but increases storage and processing overhead</li>\n</ul>\n</blockquote>\n<p><strong>Lineage Collection Levels:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Level</th>\n<th>Granularity</th>\n<th>Collection Method</th>\n<th>Use Cases</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dataset</td>\n<td>Table/file level</td>\n<td>Connector metadata</td>\n<td>Impact analysis, data discovery</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>Schema</td>\n<td>Column level</td>\n<td>SQL parsing, schema inference</td>\n<td>Compliance reporting, data mapping</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Instance</td>\n<td>Row/record level</td>\n<td>Data sampling, checksums</td>\n<td>Data quality debugging, auditing</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>Field transformation level</td>\n<td>Complete data capture</td>\n<td>Regulatory compliance, forensics</td>\n<td>Very High</td>\n</tr>\n</tbody></table>\n<h4 id=\"automatic-lineage-capture\">Automatic Lineage Capture</h4>\n<p>The lineage system automatically captures lineage information during pipeline execution without requiring explicit instrumentation from pipeline developers. This automatic collection uses multiple techniques depending on the transformation type.</p>\n<p><strong>SQL-Based Transformation Lineage:</strong></p>\n<p>For SQL transformations, the system uses <strong>SQL parsing and analysis</strong> to extract lineage relationships automatically. The SQL parser analyzes SELECT statements to identify source tables, join relationships, filter conditions, and column transformations.</p>\n<ol>\n<li><p><strong>Query Parsing</strong>: The transformation engine intercepts SQL queries before execution and parses them using a SQL abstract syntax tree (AST) parser that understands the specific SQL dialect being used.</p>\n</li>\n<li><p><strong>Dependency Extraction</strong>: The parser identifies all referenced tables, views, and columns in FROM, JOIN, and WHERE clauses, creating source asset references with exact column mappings.</p>\n</li>\n<li><p><strong>Transformation Logic Capture</strong>: The parser extracts column expressions from the SELECT clause, including function calls, arithmetic operations, and case statements, preserving the exact transformation logic applied.</p>\n</li>\n<li><p><strong>Output Schema Inference</strong>: The parser predicts output column names and types based on the SELECT clause analysis, creating target asset references that match the actual query results.</p>\n</li>\n</ol>\n<p><strong>Python UDF Lineage:</strong></p>\n<p>For Python user-defined functions, the system uses <strong>code instrumentation and runtime inspection</strong> to capture lineage relationships that cannot be determined through static analysis.</p>\n<ol>\n<li><p><strong>Function Registration</strong>: When UDFs are registered using <code>register_function(name, func)</code>, the system wraps the function with lineage collection decorators that intercept input and output data.</p>\n</li>\n<li><p><strong>Input Tracking</strong>: The wrapper captures metadata about input data streams, including schema information, data source references, and statistical profiles of the input data.</p>\n</li>\n<li><p><strong>Output Analysis</strong>: The wrapper analyzes function outputs to determine output schema, data transformations applied, and relationships between input and output fields.</p>\n</li>\n<li><p><strong>Manual Annotations</strong>: The system provides decorators that allow UDF developers to explicitly declare lineage relationships when automatic detection is insufficient, such as complex business logic or external API calls.</p>\n</li>\n</ol>\n<h4 id=\"lineage-query-and-analysis\">Lineage Query and Analysis</h4>\n<p>The lineage system provides both programmatic APIs and query interfaces for analyzing data provenance and impact relationships. These capabilities support both interactive exploration and automated governance workflows.</p>\n<p><strong>Forward Impact Analysis:</strong></p>\n<p>Forward impact analysis answers the question: &quot;If this source data changes, what downstream systems will be affected?&quot; This analysis is crucial for change management and data governance.</p>\n<table>\n<thead>\n<tr>\n<th>Query Type</th>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Direct Dependencies</td>\n<td><code>get_direct_dependencies(asset)</code></td>\n<td>Asset reference</td>\n<td>List of immediate downstream assets</td>\n<td>Change impact assessment</td>\n</tr>\n<tr>\n<td>Transitive Dependencies</td>\n<td><code>get_all_dependencies(asset, max_depth)</code></td>\n<td>Asset reference, traversal depth</td>\n<td>Complete dependency graph</td>\n<td>Full impact analysis</td>\n</tr>\n<tr>\n<td>Pipeline Impact</td>\n<td><code>get_affected_pipelines(asset)</code></td>\n<td>Asset reference</td>\n<td>List of pipeline IDs</td>\n<td>Pipeline scheduling decisions</td>\n</tr>\n<tr>\n<td>Schema Impact</td>\n<td><code>get_schema_dependencies(asset, column)</code></td>\n<td>Asset and column references</td>\n<td>Column-level dependency graph</td>\n<td>Schema evolution planning</td>\n</tr>\n</tbody></table>\n<p><strong>Backward Provenance Analysis:</strong></p>\n<p>Backward provenance analysis answers: &quot;Where did this data come from and how was it transformed?&quot; This analysis supports data quality investigations and audit requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Query Type</th>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Sources</td>\n<td><code>get_data_sources(asset)</code></td>\n<td>Asset reference</td>\n<td>List of source assets and transformations</td>\n<td>Root cause analysis</td>\n</tr>\n<tr>\n<td>Transformation History</td>\n<td><code>get_transformation_path(source, target)</code></td>\n<td>Source and target assets</td>\n<td>Complete transformation sequence</td>\n<td>Data quality debugging</td>\n</tr>\n<tr>\n<td>Business Logic Trace</td>\n<td><code>get_business_rules(asset, column)</code></td>\n<td>Asset and column references</td>\n<td>Applied business rules and transformations</td>\n<td>Compliance auditing</td>\n</tr>\n<tr>\n<td>Temporal Lineage</td>\n<td><code>get_lineage_at_time(asset, timestamp)</code></td>\n<td>Asset reference and time</td>\n<td>Historical lineage relationships</td>\n<td>Point-in-time analysis</td>\n</tr>\n</tbody></table>\n<h4 id=\"common-pitfalls\">Common Pitfalls</h4>\n<p> <strong>Pitfall: Lineage Collection Overhead</strong>\nMany implementations collect too much lineage information by default, severely impacting pipeline performance. Collecting row-level lineage for high-volume data streams can slow down pipelines by 50% or more. <strong>Solution</strong>: Use tiered collection levels and sample-based collection for high-volume streams. Configure collection levels based on data sensitivity and regulatory requirements.</p>\n<p> <strong>Pitfall: Incomplete Schema Lineage</strong>\nSQL parsing often misses complex transformations like UDF calls, dynamic SQL, or external API enrichments, creating gaps in lineage graphs. <strong>Solution</strong>: Implement hybrid collection that combines automatic SQL analysis with manual lineage annotations for complex transformations. Provide clear documentation on when manual annotations are required.</p>\n<p> <strong>Pitfall: Lineage Storage Explosion</strong>\nStoring complete lineage for every pipeline run can quickly consume massive storage, especially for high-frequency pipelines processing large datasets. <strong>Solution</strong>: Implement lineage retention policies that keep detailed lineage for recent runs but aggregate older lineage to schema-level relationships. Consider compressing lineage for archived pipeline runs.</p>\n<p> <strong>Pitfall: Cross-System Lineage Gaps</strong>\nLineage often breaks when data moves between different systems (ETL  data warehouse  BI tools) because each system maintains its own lineage model. <strong>Solution</strong>: Standardize on lineage metadata formats and implement lineage bridges that can import/export lineage information between systems. Use standard formats like OpenLineage for interoperability.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Message Broker</td>\n<td>Redis Streams with Python redis-py</td>\n<td>Apache Kafka with confluent-kafka-python</td>\n</tr>\n<tr>\n<td>API Framework</td>\n<td>FastAPI with Pydantic validation</td>\n<td>FastAPI + Celery for async processing</td>\n</tr>\n<tr>\n<td>State Storage</td>\n<td>SQLite with SQLAlchemy ORM</td>\n<td>PostgreSQL with asyncpg for performance</td>\n</tr>\n<tr>\n<td>Lineage Storage</td>\n<td>JSON documents in primary database</td>\n<td>Graph database (Neo4j) with py2neo</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Python logging + Prometheus client</td>\n<td>OpenTelemetry with Jaeger tracing</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl_pipeline/\n  core/\n    communication/\n      __init__.py\n      message_broker.py           Message publishing and subscription\n      api_server.py              REST API endpoints\n      event_handlers.py          Async event processing\n    orchestration/\n      __init__.py\n      pipeline_executor.py       Main execution flow coordination\n      dependency_tracker.py      Task dependency resolution\n      state_manager.py           Pipeline and task state management\n    lineage/\n      __init__.py\n      lineage_collector.py       Automatic lineage capture\n      lineage_analyzer.py        Forward/backward analysis\n      sql_parser.py             SQL lineage extraction\n  examples/\n    sample_pipeline.py           Complete end-to-end example\n    lineage_queries.py          Example lineage analysis queries\n  tests/\n    integration/\n      test_pipeline_execution.py  Full pipeline execution tests\n      test_lineage_tracking.py    End-to-end lineage tests</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Message Broker Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Message broker implementation for component communication.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides both synchronous API calls and asynchronous event publishing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Callable, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MessageEvent</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Standard message format for all inter-component communication.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    event_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_component: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    event_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MessageBroker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Handles all inter-component messaging for the ETL system.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports both pub/sub patterns for events and point-to-point for control messages.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6379\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis.from_url(redis_url, </span><span style=\"color:#FFAB70\">decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.subscribers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Callable]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> publish_event</span><span style=\"color:#E1E4E8\">(self, topic: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, event: MessageEvent) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Publish event to specified topic for subscriber consumption.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if published successfully, False otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            message_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.dumps(asdict(event), </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis_client.xadd(topic, {</span><span style=\"color:#9ECBFF\">\"event\"</span><span style=\"color:#E1E4E8\">: message_data})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.debug(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Published event </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.event_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> to topic </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">topic</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to publish event </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.event_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe_to_topic</span><span style=\"color:#E1E4E8\">(self, topic: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: Callable[[MessageEvent], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register event handler for specified topic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> topic </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.subscribers:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.subscribers[topic] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.subscribers[topic].append(handler)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Registered handler for topic </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">topic</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> start_consumer</span><span style=\"color:#E1E4E8\">(self, consumer_group: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, consumer_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start consuming messages from subscribed topics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> topic </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.subscribers.keys():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Create consumer group if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                        self</span><span style=\"color:#E1E4E8\">.redis_client.xgroup_create(topic, consumer_group, </span><span style=\"color:#FFAB70\">id</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'0'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">mkstream</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    except</span><span style=\"color:#E1E4E8\"> redis.ResponseError:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        pass</span><span style=\"color:#6A737D\">  # Group already exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Read messages from the stream</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    messages </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis_client.xreadgroup(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        consumer_group, consumer_name, {topic: </span><span style=\"color:#9ECBFF\">'>'</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">block</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    for</span><span style=\"color:#E1E4E8\"> stream, msgs </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> messages:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        for</span><span style=\"color:#E1E4E8\"> msg_id, fields </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> msgs:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                event_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(fields[</span><span style=\"color:#9ECBFF\">'event'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MessageEvent(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">event_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                                # Call all registered handlers for this topic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                                for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.subscribers.get(topic, []):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    handler(event)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                                # Acknowledge message processing</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                                self</span><span style=\"color:#E1E4E8\">.redis_client.xack(topic, consumer_group, msg_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                                self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to process message </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">msg_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Consumer error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>State Management Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Centralized state management for pipeline and task execution tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles state transitions, persistence, and consistency across components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sqlite3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"PENDING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WAITING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"WAITING\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    QUEUED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"QUEUED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RUNNING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SUCCESS\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"FAILED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRYING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RETRYING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CANCELLED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SKIPPED\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskEvent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEPENDENCIES_MET</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"DEPENDENCIES_MET\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_STARTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"EXECUTION_STARTED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"EXECUTION_COMPLETED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    EXECUTION_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"EXECUTION_FAILED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RETRY_SCHEDULED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RETRY_SCHEDULED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MAX_RETRIES_EXCEEDED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"MAX_RETRIES_EXCEEDED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED_BY_USER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CANCELLED_BY_USER\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UPSTREAM_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"UPSTREAM_FAILED\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># State transition mapping - defines valid state changes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">TRANSITIONS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">PENDING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">DEPENDENCIES_MET</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">WAITING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">WAITING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">UPSTREAM_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SKIPPED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_COMPLETED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">SUCCESS</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_FAILED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">MAX_RETRIES_EXCEEDED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TaskState.</span><span style=\"color:#79B8FF\">RETRYING</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">EXECUTION_STARTED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">MAX_RETRIES_EXCEEDED</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TaskEvent.</span><span style=\"color:#79B8FF\">CANCELLED_BY_USER</span><span style=\"color:#E1E4E8\">: TaskState.</span><span style=\"color:#79B8FF\">CANCELLED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TaskExecution</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single task execution within a pipeline run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state: TaskState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logs: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StateManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Thread-safe state management for pipeline and task executions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides atomic state transitions and persistent storage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, db_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"etl_state.db\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._init_database()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _init_database</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize SQLite database with required tables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sqlite3.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conn.cursor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            CREATE TABLE IF NOT EXISTS task_executions (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                task_id TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                pipeline_run_id TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                state TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                attempt_count INTEGER,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                started_at TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                completed_at TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                error_message TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                logs TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                metrics TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                PRIMARY KEY (task_id, pipeline_run_id)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            CREATE TABLE IF NOT EXISTS state_transitions (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                id INTEGER PRIMARY KEY AUTOINCREMENT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                task_id TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                pipeline_run_id TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                from_state TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                to_state TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                event TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                timestamp TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                error_message TEXT</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn.commit()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conn.close()</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Pipeline Execution Coordinator:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Main pipeline execution coordination logic.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Students implement the core orchestration algorithms here.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Set, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PipelineExecutor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Coordinates execution of complete pipeline runs including dependency resolution,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    task scheduling, and failure handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message_broker, state_manager, dag_engine):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message_broker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message_broker</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> state_manager  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dag_engine </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dag_engine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> execute_pipeline</span><span style=\"color:#E1E4E8\">(self, pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute complete pipeline run with dependency management and monitoring.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if pipeline completed successfully, False otherwise.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve pipeline definition using get_pipeline(pipeline_id)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate pipeline definition using validate_pipeline(pipeline)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create execution plan using create_execution_plan(pipeline, task_durations)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize all task executions in PENDING state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Start dependency resolution loop for task scheduling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Monitor task completion events and update dependency tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Handle task failures according to pipeline failure policy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Determine pipeline completion status when all tasks finish</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Publish pipeline completion event with final metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Perform cleanup of temporary resources and state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _schedule_eligible_tasks</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, execution_plan) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Schedule tasks that have all dependencies satisfied.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns list of scheduled task IDs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Query current task states for this pipeline run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find tasks in WAITING state (dependencies met but not yet scheduled)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check resource availability for each eligible task</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Assign tasks to available executors using load balancing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update task states to QUEUED and publish scheduling events</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of successfully scheduled task IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _update_dependencies</span><span style=\"color:#E1E4E8\">(self, completed_task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Update dependency tracking when a task completes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns set of task IDs that became eligible for execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve execution plan dependency matrix for this run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find all tasks that depend on the completed task</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if all dependencies are now satisfied for each dependent task</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Transition eligible tasks from PENDING to WAITING state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return set of task IDs that became eligible</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_task_failure</span><span style=\"color:#E1E4E8\">(self, failed_task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle task failure according to pipeline failure policy.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if pipeline should continue, False if it should abort.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check task retry policy and current attempt count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If retries available, schedule retry with exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If no retries left, determine impact on downstream tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply pipeline failure policy (fail-fast, continue-on-error, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Cancel or skip dependent tasks based on failure policy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return whether pipeline execution should continue</span></span></code></pre></div>\n\n<p><strong>Lineage Collection Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Automatic lineage collection during pipeline execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Students implement lineage capture and analysis logic here.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LineageCollector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Captures data lineage information during pipeline execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports both automatic collection and manual annotation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, storage_backend):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage_backend</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> collect_sql_lineage</span><span style=\"color:#E1E4E8\">(self, sql_query: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, input_tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], output_table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Extract lineage from SQL transformations using query analysis.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if lineage was successfully captured.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse SQL query using SQL AST parser to extract structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify all referenced tables and columns in FROM/JOIN clauses</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract column expressions from SELECT clause with transformations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Map input columns to output columns through SELECT expressions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create AssetReference objects for input and output tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Build ColumnLineage mappings for each transformed column</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Store complete lineage record with transformation logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Publish lineage event for downstream consumers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> collect_udf_lineage</span><span style=\"color:#E1E4E8\">(self, function_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, input_data_schema: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           output_data_schema: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">, pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Capture lineage for Python UDF transformations through runtime inspection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if lineage was successfully captured.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve UDF metadata and manual lineage annotations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare input and output schemas to infer column mappings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Sample input and output data to detect transformation patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Extract business logic annotations from function docstrings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Build lineage record with available transformation information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Mark uncertain lineage relationships for manual review</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Store lineage record and publish collection event</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_data_lineage</span><span style=\"color:#E1E4E8\">(self, asset_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, direction: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"both\"</span><span style=\"color:#E1E4E8\">, max_depth: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Query lineage relationships for specified data asset.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Direction can be 'upstream', 'downstream', or 'both'.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns lineage graph with transformation details.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate asset name and query parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Build graph traversal query based on direction and depth</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute lineage query against storage backend</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Construct lineage graph with nodes (assets) and edges (transformations)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Include transformation logic and schema evolution details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply access controls to filter visible lineage information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return structured lineage graph for visualization/analysis</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1 - Component Communication (End of Milestone 1):</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/integration/test_messaging.py -v</code></li>\n<li>Expected: All message publishing and subscription tests pass</li>\n<li>Manual verification: Start message broker, publish test event, verify subscriber receives it</li>\n<li>Signs of issues: Message delivery failures, serialization errors, subscription timeouts</li>\n</ul>\n<p><strong>Checkpoint 2 - Pipeline Execution Flow (End of Milestone 2):</strong></p>\n<ul>\n<li>Run: <code>python scripts/test_pipeline_execution.py sample_pipeline.yaml</code></li>\n<li>Expected: Complete pipeline execution with task dependency resolution</li>\n<li>Manual verification: Monitor task state transitions through dashboard/logs</li>\n<li>Signs of issues: Dependency deadlocks, resource allocation failures, state inconsistencies</li>\n</ul>\n<p><strong>Checkpoint 3 - Lineage Tracking (End of Milestone 3):</strong></p>\n<ul>\n<li>Run: <code>python scripts/test_lineage_collection.py</code></li>\n<li>Expected: Automatic lineage capture for SQL and Python transformations</li>\n<li>Manual verification: Query lineage API to verify forward/backward relationships</li>\n<li>Signs of issues: Missing lineage records, incorrect column mappings, SQL parsing failures</li>\n</ul>\n<p><strong>Checkpoint 4 - Complete Integration (End of Milestone 4):</strong></p>\n<ul>\n<li>Run: <code>python scripts/run_full_pipeline_test.py</code></li>\n<li>Expected: End-to-end pipeline with monitoring, alerting, and lineage</li>\n<li>Manual verification: Complete pipeline run with full observability</li>\n<li>Signs of issues: Missing metrics, alert failures, incomplete lineage graphs</li>\n</ul>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - comprehensive error handling affects pipeline definition validation (Milestone 1), data processing resilience (Milestones 2-3), and orchestration reliability (Milestone 4). This section establishes the production-grade error handling patterns required across all system components.</p>\n</blockquote>\n<h3 id=\"mental-model-hospital-emergency-response-system\">Mental Model: Hospital Emergency Response System</h3>\n<p>Think of our error handling system like a hospital&#39;s emergency response infrastructure. When patients arrive, they don&#39;t just randomly hope for the best - there&#39;s a systematic approach to handling different types of medical emergencies. <strong>Triage</strong> classifies problems by severity (minor cut vs. heart attack), <strong>treatment protocols</strong> define standardized responses to common conditions (broken bone  X-ray  cast  recovery), and <strong>escalation procedures</strong> know when to call specialists or transfer to higher-level care facilities.</p>\n<p>Similarly, our ETL system must systematically classify failures, apply appropriate treatment protocols (retry strategies), and escalate to human operators when automated recovery fails. Just as hospitals have backup generators and redundant life support systems, our pipeline infrastructure requires multiple layers of fault tolerance to ensure critical data flows never completely fail.</p>\n<p>The key insight is that <strong>not all failures are created equal</strong>. A temporary network blip should trigger automatic retry with exponential backoff, while data corruption requires immediate human attention and potentially rolling back processed data. Like medical triage, proper classification at the moment of failure determines whether the patient (pipeline) recovers quickly or suffers permanent damage.</p>\n<h3 id=\"common-failure-modes\">Common Failure Modes</h3>\n<p>Production ETL systems face numerous failure scenarios that can disrupt data processing, corrupt results, or leave pipelines in inconsistent states. Understanding these failure modes enables us to design appropriate detection, classification, and recovery mechanisms.</p>\n<h4 id=\"network-and-connectivity-failures\">Network and Connectivity Failures</h4>\n<p>Network-related failures represent the most common category of transient errors in distributed ETL systems. These failures manifest as connection timeouts, DNS resolution failures, SSL handshake errors, and intermittent packet loss that can cause partial data corruption during transmission.</p>\n<p><strong>Connection timeouts</strong> occur when source systems become temporarily overloaded or network congestion prevents timely responses. Database connections may time out during long-running extraction queries, while API endpoints may become unresponsive during peak usage periods. The challenge lies in distinguishing between temporary overload (retry appropriate) and permanent service degradation (escalation required).</p>\n<p><strong>DNS resolution failures</strong> can prevent pipeline tasks from reaching their target systems entirely. These failures often indicate infrastructure-level problems that affect multiple pipelines simultaneously. Unlike connection timeouts, DNS failures typically require immediate escalation since they suggest broader network infrastructure issues.</p>\n<p><strong>SSL certificate issues</strong> create authentication failures that prevent secure connections to external systems. Certificate expiration, hostname mismatches, and certificate authority changes can break previously working connections. These failures require immediate attention since they often indicate security-related configuration drift.</p>\n<p><strong>Partial network failures</strong> represent the most insidious category, where connections succeed initially but encounter intermittent packet loss or bandwidth restrictions. Large data transfers may partially complete before failing, leaving destination systems in inconsistent states that require careful cleanup.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Symptoms</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connection Timeout</td>\n<td>HTTP 408, socket timeout exceptions</td>\n<td>Connection monitoring, response time alerts</td>\n<td>Exponential backoff retry, connection pool cycling</td>\n</tr>\n<tr>\n<td>DNS Resolution Failure</td>\n<td>Name resolution errors, DNS lookup timeouts</td>\n<td>DNS health checks, resolution time monitoring</td>\n<td>Immediate escalation, fallback DNS servers</td>\n</tr>\n<tr>\n<td>SSL Certificate Issues</td>\n<td>Certificate validation errors, handshake failures</td>\n<td>Certificate expiration monitoring, handshake alerts</td>\n<td>Immediate escalation, certificate renewal workflow</td>\n</tr>\n<tr>\n<td>Partial Network Failure</td>\n<td>Incomplete transfers, checksum mismatches</td>\n<td>Transfer verification, data integrity checks</td>\n<td>Resume from checkpoint, full transfer retry</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-quality-and-corruption-issues\">Data Quality and Corruption Issues</h4>\n<p>Data quality problems represent critical failures that can propagate incorrect information throughout downstream systems. Unlike network failures, data corruption issues require immediate human intervention and potentially complex rollback procedures.</p>\n<p><strong>Schema evolution conflicts</strong> occur when source systems change their data structures without coordinating with ETL pipelines. New required fields, removed columns, or data type changes can cause extraction or transformation tasks to fail catastrophically. The fundamental challenge is distinguishing between temporary schema access issues and permanent structural changes.</p>\n<p><strong>Data format corruption</strong> manifests when source systems produce malformed records that don&#39;t conform to expected schemas. JSON parsing errors, CSV files with inconsistent column counts, and binary data corruption represent common examples. These failures require sophisticated validation to prevent corrupt records from contaminating destination systems.</p>\n<p><strong>Referential integrity violations</strong> occur when extracted data references entities that don&#39;t exist in destination systems. Foreign key constraint violations, orphaned records, and circular references can prevent bulk loading operations from completing successfully.</p>\n<p><strong>Data volume anomalies</strong> indicate potential upstream system failures or data generation issues. Sudden spikes or drops in record counts, unusually large field values, or completely empty extractions suggest problems that require immediate investigation.</p>\n<table>\n<thead>\n<tr>\n<th>Data Issue Type</th>\n<th>Detection Method</th>\n<th>Impact Assessment</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Schema Evolution</td>\n<td>Field validation, type checking failures</td>\n<td>High - affects all downstream consumers</td>\n<td>Stop pipeline, schema compatibility analysis</td>\n</tr>\n<tr>\n<td>Format Corruption</td>\n<td>Parse errors, validation failures</td>\n<td>Medium - affects individual records</td>\n<td>Quarantine invalid records, continue processing</td>\n</tr>\n<tr>\n<td>Referential Integrity</td>\n<td>Foreign key violations, constraint errors</td>\n<td>High - data consistency compromised</td>\n<td>Rollback transaction, dependency resolution</td>\n</tr>\n<tr>\n<td>Volume Anomalies</td>\n<td>Record count monitoring, statistical analysis</td>\n<td>Variable - depends on business context</td>\n<td>Alert operators, apply business rule validation</td>\n</tr>\n</tbody></table>\n<h4 id=\"resource-exhaustion-and-performance-degradation\">Resource Exhaustion and Performance Degradation</h4>\n<p>Resource exhaustion represents a class of failures where system resources become insufficient to support normal pipeline operations. These failures often develop gradually and can affect multiple pipelines simultaneously.</p>\n<p><strong>Memory exhaustion</strong> occurs when transformation operations attempt to load datasets larger than available RAM. Aggregation operations, large joins, and bulk data processing can trigger out-of-memory errors that terminate pipeline tasks abruptly. The challenge lies in detecting memory pressure early enough to implement mitigation strategies.</p>\n<p><strong>Disk space exhaustion</strong> affects both temporary processing storage and permanent result storage. Staging tables, intermediate transformation results, and log files can consume available disk space, preventing pipeline completion. These failures require both immediate cleanup and longer-term capacity planning.</p>\n<p><strong>Connection pool exhaustion</strong> happens when concurrent pipeline tasks exceed the maximum number of available database connections. This creates resource starvation where new tasks cannot acquire necessary connections to proceed.</p>\n<p><strong>CPU and I/O bottlenecks</strong> manifest as severely degraded performance that can cause pipeline tasks to exceed their timeout limits. While not technically failures, extreme performance degradation often leads to timeout-based task cancellation.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Resource exhaustion failures often cascade across multiple pipelines. A single memory-intensive transformation can consume available RAM, causing unrelated pipelines to fail due to resource starvation. Detection and mitigation must consider system-wide resource allocation, not just individual pipeline requirements.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Exhaustion Symptoms</th>\n<th>Monitoring Approach</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory</td>\n<td>OutOfMemoryError, swap thrashing</td>\n<td>Memory usage monitoring, GC pressure alerts</td>\n<td>Streaming processing, data partitioning</td>\n</tr>\n<tr>\n<td>Disk Space</td>\n<td>Write failures, temp file creation errors</td>\n<td>Disk usage monitoring, growth rate analysis</td>\n<td>Cleanup temporary files, partition pruning</td>\n</tr>\n<tr>\n<td>Connection Pool</td>\n<td>Connection acquisition timeouts</td>\n<td>Active connection monitoring, pool saturation alerts</td>\n<td>Connection pool sizing, connection recycling</td>\n</tr>\n<tr>\n<td>CPU/I/O</td>\n<td>Extreme response times, timeout failures</td>\n<td>Performance monitoring, resource utilization tracking</td>\n<td>Task parallelization limits, resource quotas</td>\n</tr>\n</tbody></table>\n<h3 id=\"retry-and-backoff-strategies\">Retry and Backoff Strategies</h3>\n<p>Effective retry strategies distinguish between transient failures that will likely resolve themselves and permanent failures that require human intervention. The goal is to achieve maximum reliability without overwhelming already-stressed systems with aggressive retry attempts.</p>\n<h4 id=\"exponential-backoff-implementation\">Exponential Backoff Implementation</h4>\n<p><strong>Exponential backoff</strong> provides a mathematically sound approach to spacing retry attempts that reduces load on failing systems while maintaining reasonable recovery times for transient issues. The strategy involves doubling the delay between successive retry attempts, with optional jitter to prevent thundering herd effects.</p>\n<p>The basic exponential backoff formula calculates delay as <code>initial_delay * (2^attempt_count)</code>, with maximum delay caps to prevent indefinitely long wait times. However, naive implementation can create synchronized retry storms when multiple pipeline tasks fail simultaneously and begin retrying on identical schedules.</p>\n<p><strong>Jitter injection</strong> addresses synchronized retry problems by introducing randomness to retry timing. Full jitter randomizes the entire delay window, while decorrelated jitter uses the previous delay as input to random number generation. The choice between jitter strategies depends on the expected correlation between failure events and system recovery characteristics.</p>\n<p><strong>Retry budget management</strong> prevents retry attempts from consuming excessive execution time relative to useful work. Each task maintains a retry budget that limits total retry time or attempt counts, ensuring that persistently failing tasks don&#39;t block pipeline completion indefinitely.</p>\n<blockquote>\n<p><strong>Decision: Exponential Backoff with Decorrelated Jitter</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple pipeline tasks often fail simultaneously due to shared dependencies (database, network), creating retry storms that can overwhelm recovering systems</li>\n<li><strong>Options Considered</strong>: Fixed delay, exponential backoff, exponential backoff with full jitter, decorrelated jitter</li>\n<li><strong>Decision</strong>: Exponential backoff with decorrelated jitter and maximum delay caps</li>\n<li><strong>Rationale</strong>: Decorrelated jitter spreads retry attempts across time while maintaining reasonable mathematical properties. Maximum delay caps prevent indefinitely long waits that could block pipeline completion.</li>\n<li><strong>Consequences</strong>: Eliminates retry storms and reduces system load during recovery, at the cost of slightly more complex retry timing calculation</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Retry Strategy</th>\n<th>Formula</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Fixed Delay</td>\n<td><code>delay = constant</code></td>\n<td>Simple, predictable timing</td>\n<td>Can create retry storms, ignores system recovery</td>\n</tr>\n<tr>\n<td>Exponential Backoff</td>\n<td><code>delay = initial * (2^attempt)</code></td>\n<td>Reduces load on failing systems</td>\n<td>Can create synchronized retry storms</td>\n</tr>\n<tr>\n<td>Full Jitter</td>\n<td><code>delay = random(0, exponential_delay)</code></td>\n<td>Eliminates synchronization</td>\n<td>Can retry too quickly or too slowly</td>\n</tr>\n<tr>\n<td>Decorrelated Jitter</td>\n<td><code>delay = random(initial, previous_delay * 3)</code></td>\n<td>Balances timing with load spreading</td>\n<td>More complex calculation</td>\n</tr>\n</tbody></table>\n<p>The <code>RetryPolicy</code> configuration allows per-task customization of retry behavior based on the expected failure characteristics of different operation types:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Default Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>max_attempts</td>\n<td>int</td>\n<td>Maximum number of retry attempts before giving up</td>\n<td>3</td>\n</tr>\n<tr>\n<td>backoff_seconds</td>\n<td>int</td>\n<td>Initial delay in seconds before first retry</td>\n<td>1</td>\n</tr>\n<tr>\n<td>exponential_backoff</td>\n<td>bool</td>\n<td>Whether to use exponential backoff or fixed delay</td>\n<td>true</td>\n</tr>\n<tr>\n<td>retry_on_error_types</td>\n<td>List[str]</td>\n<td>List of error types that should trigger retries</td>\n<td>[&quot;NetworkError&quot;, &quot;TimeoutError&quot;]</td>\n</tr>\n</tbody></table>\n<h4 id=\"circuit-breaker-pattern\">Circuit Breaker Pattern</h4>\n<p><strong>Circuit breakers</strong> protect downstream systems from cascading failures by temporarily suspending requests to systems that are experiencing problems. Like electrical circuit breakers, they &quot;trip&quot; when failure rates exceed acceptable thresholds and automatically &quot;reset&quot; when systems recover.</p>\n<p>The circuit breaker maintains three states: <strong>Closed</strong> (normal operation), <strong>Open</strong> (failures detected, requests blocked), and <strong>Half-Open</strong> (testing recovery). State transitions depend on failure rate monitoring and recovery detection logic.</p>\n<p><strong>Failure rate calculation</strong> requires sliding window analysis to distinguish between temporary failure spikes and sustained system degradation. Count-based windows track failures over a fixed number of recent requests, while time-based windows monitor failure rates over rolling time periods.</p>\n<p><strong>Recovery detection</strong> in the Half-Open state allows a limited number of test requests to determine if the downstream system has recovered. Successful test requests transition the circuit back to Closed state, while continued failures return to Open state with potentially extended timeout periods.</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Request Handling</th>\n<th>Transition Trigger</th>\n<th>Typical Duration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>Forward all requests</td>\n<td>Failure rate exceeds threshold</td>\n<td>N/A - normal operation</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>Reject requests immediately</td>\n<td>Timeout period expires</td>\n<td>60-300 seconds</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Allow limited test requests</td>\n<td>Successful/failed test requests</td>\n<td>5-30 seconds</td>\n</tr>\n</tbody></table>\n<h4 id=\"dead-letter-queue-management\">Dead Letter Queue Management</h4>\n<p><strong>Dead letter queues</strong> provide a mechanism for handling messages or tasks that cannot be processed successfully after exhausting all retry attempts. Rather than losing failed operations entirely, dead letter queues preserve them for later analysis and potential reprocessing.</p>\n<p><strong>Message preservation</strong> ensures that failed operations retain all necessary context for debugging and recovery. This includes original task parameters, error messages from all retry attempts, execution timing information, and environmental context like pipeline run identifiers.</p>\n<p><strong>Retry exhaustion criteria</strong> determine when tasks should be moved to dead letter queues. Simple count-based criteria move tasks after a fixed number of attempts, while time-based criteria consider total retry duration. Sophisticated approaches consider error type classification, with permanent errors moving immediately to dead letter queues.</p>\n<p><strong>Dead letter processing</strong> enables batch reprocessing of failed operations after resolving underlying issues. Manual reprocessing allows operators to modify task parameters or execution context, while automatic reprocessing can retry operations when system health metrics indicate recovery.</p>\n<blockquote>\n<p><strong>Architecture Decision: Separate Dead Letter Queues by Failure Type</strong></p>\n<ul>\n<li><strong>Context</strong>: Different failure types require different reprocessing approaches and have different urgency levels for human intervention</li>\n<li><strong>Options Considered</strong>: Single dead letter queue, separate queues by pipeline, separate queues by failure type</li>\n<li><strong>Decision</strong>: Separate dead letter queues categorized by failure type (transient, data quality, system error)</li>\n<li><strong>Rationale</strong>: Enables targeted reprocessing strategies and appropriate alert prioritization. Data quality issues need immediate attention while transient failures can be batch processed.</li>\n<li><strong>Consequences</strong>: Requires failure classification logic and multiple queue management, but provides better operational control and reduced alert noise.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Dead Letter Queue Type</th>\n<th>Failure Categories</th>\n<th>Reprocessing Strategy</th>\n<th>Alert Priority</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient Failures</td>\n<td>Network timeouts, temporary resource exhaustion</td>\n<td>Automatic batch retry during off-peak hours</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Data Quality Issues</td>\n<td>Schema validation, referential integrity</td>\n<td>Manual review and correction required</td>\n<td>High</td>\n</tr>\n<tr>\n<td>System Errors</td>\n<td>Configuration issues, permission problems</td>\n<td>Requires system administrator intervention</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Permanent Failures</td>\n<td>Invalid configurations, missing resources</td>\n<td>Manual task modification or removal</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<h3 id=\"partial-failure-recovery\">Partial Failure Recovery</h3>\n<p>Partial failures represent some of the most challenging scenarios in distributed ETL systems. Unlike complete failures where the entire operation fails cleanly, partial failures leave systems in intermediate states that require careful analysis and recovery procedures.</p>\n<h4 id=\"checkpointing-and-state-persistence\">Checkpointing and State Persistence</h4>\n<p><strong>Checkpointing</strong> enables pipelines to resume processing from intermediate points rather than restarting from the beginning after failures. Effective checkpointing requires identifying appropriate checkpoint boundaries, persisting sufficient state information, and implementing recovery logic that can resume from any checkpoint.</p>\n<p><strong>Checkpoint boundary selection</strong> depends on the natural granularity of data processing operations. Bulk loading operations might checkpoint after each batch of records, while transformation operations might checkpoint after processing each input partition. The goal is to balance checkpoint frequency (more frequent checkpoints reduce rework) against checkpoint overhead (state persistence costs).</p>\n<p><strong>State persistence requirements</strong> extend beyond simple progress tracking to include all information necessary to resume processing. This includes input data positions (file offsets, database cursors), intermediate calculation results, configuration parameters, and dependency state. The challenge lies in ensuring checkpoint state remains consistent even if the checkpointing process itself is interrupted.</p>\n<p><strong>Recovery logic complexity</strong> grows significantly with checkpoint granularity. Coarse-grained checkpoints require simpler recovery logic but result in more rework after failures. Fine-grained checkpoints minimize rework but require sophisticated logic to handle partially completed operations and state inconsistencies.</p>\n<blockquote>\n<p><strong>Critical Design Principle</strong>: Checkpoints must be <strong>atomic</strong> and <strong>consistent</strong>. A checkpoint is only valid if all related state can be persisted atomically. Partial checkpoints can leave the system in an unrecoverable state that&#39;s worse than no checkpoint at all.</p>\n</blockquote>\n<p>The <code>TaskExecution</code> model supports checkpointing through the metrics field, which can store arbitrary checkpoint data:</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint Information</th>\n<th>Storage Location</th>\n<th>Recovery Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input data position</td>\n<td>metrics[&quot;input_position&quot;]</td>\n<td>Resume reading from exact location</td>\n</tr>\n<tr>\n<td>Processed record count</td>\n<td>metrics[&quot;records_processed&quot;]</td>\n<td>Validate recovery completeness</td>\n</tr>\n<tr>\n<td>Intermediate results</td>\n<td>metrics[&quot;intermediate_state&quot;]</td>\n<td>Avoid recalculating expensive operations</td>\n</tr>\n<tr>\n<td>Error recovery state</td>\n<td>metrics[&quot;error_context&quot;]</td>\n<td>Understand failure context for recovery</td>\n</tr>\n</tbody></table>\n<h4 id=\"transaction-management-and-rollback\">Transaction Management and Rollback</h4>\n<p><strong>Transaction management</strong> ensures that data operations either complete entirely or leave no traces in destination systems. ETL operations often involve multiple systems (source extraction, staging tables, destination loading) that must be coordinated to maintain consistency.</p>\n<p><strong>Two-phase commit protocols</strong> can coordinate transactions across multiple systems, but introduce significant complexity and performance overhead. The first phase involves sending prepare messages to all participating systems, while the second phase sends commit or abort decisions based on unanimous agreement.</p>\n<p><strong>Compensation-based transactions</strong> provide an alternative approach where each operation includes a corresponding compensation action that undoes its effects. Rather than preventing inconsistent states, compensation transactions restore consistency after detecting failures.</p>\n<p><strong>Saga pattern implementation</strong> breaks long-running transactions into sequences of smaller transactions, each with its own compensation action. If any step fails, the saga executes compensation actions for all completed steps in reverse order.</p>\n<table>\n<thead>\n<tr>\n<th>Transaction Approach</th>\n<th>Coordination Method</th>\n<th>Consistency Guarantee</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Two-Phase Commit</td>\n<td>Distributed consensus</td>\n<td>Strong consistency</td>\n<td>High latency, blocking</td>\n</tr>\n<tr>\n<td>Compensation Transactions</td>\n<td>Reverse operation execution</td>\n<td>Eventual consistency</td>\n<td>Lower latency, non-blocking</td>\n</tr>\n<tr>\n<td>Saga Pattern</td>\n<td>Sequential compensation</td>\n<td>Eventual consistency</td>\n<td>Moderate latency, complex logic</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-cleanup-and-consistency-repair\">Data Cleanup and Consistency Repair</h4>\n<p><strong>Data cleanup operations</strong> remove incomplete or corrupted data that results from partial failures. The challenge lies in identifying which data was affected by failures and determining safe cleanup boundaries that don&#39;t accidentally remove valid data.</p>\n<p><strong>Staging table cleanup</strong> involves removing partially loaded data from staging areas after extraction or transformation failures. Simple approaches truncate entire staging tables, while sophisticated approaches identify and remove only records from failed operations.</p>\n<p><strong>Referential integrity repair</strong> addresses situations where partial failures create orphaned records or broken foreign key relationships. Repair operations might involve removing orphaned records, restoring missing parent records, or updating references to point to valid entities.</p>\n<p><strong>Idempotent operation design</strong> enables safe retry of cleanup operations without risking data corruption. Cleanup operations should produce identical results regardless of how many times they&#39;re executed, even if the system state changes between executions.</p>\n<p> <strong>Pitfall: Cleanup Operations Without Transaction Boundaries</strong></p>\n<p>Many developers implement cleanup operations as simple DELETE statements without considering transaction boundaries. If the cleanup operation itself fails partway through, it can leave the system in a state that&#39;s more inconsistent than before cleanup began.</p>\n<p><strong>Why it&#39;s wrong</strong>: Partial cleanup can remove some corrupted data while leaving other corrupted data in place, making it harder to identify the scope of data quality issues.</p>\n<p><strong>How to fix</strong>: Wrap cleanup operations in transactions and implement cleanup checkpointing for operations that affect large data volumes. Consider using staging tables for cleanup operations so that incomplete cleanup doesn&#39;t affect production data.</p>\n<table>\n<thead>\n<tr>\n<th>Cleanup Operation Type</th>\n<th>Scope Identification</th>\n<th>Safety Mechanism</th>\n<th>Recovery Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Staging Table Cleanup</td>\n<td>Pipeline run ID, timestamp ranges</td>\n<td>Transaction boundaries</td>\n<td>Truncate and reload</td>\n</tr>\n<tr>\n<td>Partial Load Cleanup</td>\n<td>Batch identifiers, watermark ranges</td>\n<td>Backup before cleanup</td>\n<td>Restore from backup</td>\n</tr>\n<tr>\n<td>Referential Integrity Repair</td>\n<td>Foreign key violation detection</td>\n<td>Constraint validation</td>\n<td>Cascade repair operations</td>\n</tr>\n<tr>\n<td>Orphaned Record Cleanup</td>\n<td>Parent-child relationship analysis</td>\n<td>Soft delete before hard delete</td>\n<td>Restoration from soft delete</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Understanding error handling patterns requires implementing robust failure detection, classification, and recovery mechanisms. The following guidance provides concrete implementation approaches for production-grade error handling.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Retry Logic</td>\n<td>Simple exponential backoff with random jitter</td>\n<td>Redis-backed circuit breaker with sliding window</td>\n</tr>\n<tr>\n<td>Dead Letter Queue</td>\n<td>Database table with failed task records</td>\n<td>Message broker (RabbitMQ/Apache Kafka) with topic-based routing</td>\n</tr>\n<tr>\n<td>Circuit Breaker</td>\n<td>In-memory failure tracking with timeout-based reset</td>\n<td>Distributed circuit breaker with shared state</td>\n</tr>\n<tr>\n<td>Checkpointing</td>\n<td>JSON files with task state snapshots</td>\n<td>Database transactions with write-ahead logging</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Structured logging with failure categorization</td>\n<td>Metrics collection (Prometheus) with alerting rules</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>etl-system/\n  core/\n    error_handling/\n      __init__.py\n      retry_policy.py            RetryPolicy and exponential backoff\n      circuit_breaker.py         Circuit breaker implementation\n      dead_letter_queue.py       Dead letter queue management\n      checkpoint_manager.py      Checkpointing and state persistence\n      error_classifier.py        Failure type classification\n      recovery_engine.py         Automated recovery procedures\n  tests/\n    error_handling/\n      test_retry_policy.py\n      test_circuit_breaker.py\n      test_checkpoint_manager.py\n  monitoring/\n    error_metrics.py            Error metrics collection\n    alert_manager.py            Alert generation and suppression</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>RetryPolicy with Exponential Backoff:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NETWORK_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"NetworkError\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TIMEOUT_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"TimeoutError\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DATA_QUALITY_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"DataQualityError\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RESOURCE_EXHAUSTION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"ResourceExhaustion\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PERMISSION_ERROR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"PermissionError\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryPolicy</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    exponential_backoff: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_on_error_types: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_delay_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    jitter_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"decorrelated\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.retry_on_error_types </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.retry_on_error_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ErrorType.</span><span style=\"color:#79B8FF\">NETWORK_ERROR</span><span style=\"color:#E1E4E8\">.value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ErrorType.</span><span style=\"color:#79B8FF\">TIMEOUT_ERROR</span><span style=\"color:#E1E4E8\">.value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ErrorType.</span><span style=\"color:#79B8FF\">RESOURCE_EXHAUSTION</span><span style=\"color:#E1E4E8\">.value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, retry_policy: RetryPolicy):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.policy </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retry_policy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_retry</span><span style=\"color:#E1E4E8\">(self, error_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, attempt_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if operation should be retried based on policy and error type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> attempt_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.policy.max_attempts:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> error_type </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.policy.retry_on_error_types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_delay</span><span style=\"color:#E1E4E8\">(self, attempt_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, previous_delay: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate delay before next retry attempt.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.policy.exponential_backoff:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.policy.backoff_seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.policy.jitter_type </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"decorrelated\"</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> previous_delay:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Decorrelated jitter: random(initial_delay, previous_delay * 3)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            min_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.policy.backoff_seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            max_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(previous_delay </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.policy.max_delay_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> random.randint(min_delay, max_delay)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Standard exponential backoff with full jitter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.policy.backoff_seconds </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> **</span><span style=\"color:#E1E4E8\"> attempt_count)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            max_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(base_delay, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.policy.max_delay_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> random.randint(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, max_delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_with_retry</span><span style=\"color:#E1E4E8\">(self, operation, error_classifier, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute operation with retry logic based on policy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_exception </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        previous_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.policy.max_attempts):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> operation(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                error_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> error_classifier.classify_error(e)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                last_exception </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> e</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.should_retry(error_type, attempt </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.calculate_delay(attempt, previous_delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                previous_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                time.sleep(delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#E1E4E8\"> last_exception</span></span></code></pre></div>\n\n<p><strong>Circuit Breaker Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Callable, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CLOSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"closed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"open\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HALF_OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"half_open\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreakerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failure_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    success_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    window_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: CircuitBreakerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.request_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> call</span><span style=\"color:#E1E4E8\">(self, operation: Callable, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute operation through circuit breaker.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.timeout_seconds:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    raise</span><span style=\"color:#E1E4E8\"> CircuitBreakerOpenError(</span><span style=\"color:#9ECBFF\">\"Circuit breaker is open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operation(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._record_success()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._record_failure()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _record_success</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record successful operation and potentially close circuit.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.success_threshold:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _record_failure</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record failed operation and potentially open circuit.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.failure_threshold:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreakerOpenError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when circuit breaker is open and blocking requests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Error Classification System:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Type, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorClassifier</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.classification_rules </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._build_classification_rules()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> classify_error</span><span style=\"color:#E1E4E8\">(self, exception: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Classify exception into error type category for retry decision.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check exception type against known error type mappings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Examine exception message for pattern-based classification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consider exception context (network, database, file system)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return appropriate ErrorType enum value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Default to non-retryable error for unknown exception types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_retryable</span><span style=\"color:#E1E4E8\">(self, error_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if error type should trigger retry attempts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check error_type against list of retryable categories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Consider system state (circuit breaker status, resource availability)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return boolean indicating retry appropriateness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _build_classification_rules</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Build mapping of error patterns to classification categories.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Define regex patterns for network-related errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Define patterns for database connection and timeout errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Define patterns for resource exhaustion (memory, disk, connections)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Define patterns for data quality and validation errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive pattern mapping dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Checkpoint Manager:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CheckpointManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, checkpoint_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoint_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checkpoint_dir</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.makedirs(checkpoint_dir, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_checkpoint</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       checkpoint_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save task checkpoint data for recovery purposes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create checkpoint record with task_id, run_id, and timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Serialize checkpoint_data to JSON with error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Write to temporary file first, then atomic rename</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify checkpoint file integrity after writing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return success/failure status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use os.rename() for atomic file operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_checkpoint</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load most recent checkpoint data for task recovery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Construct checkpoint filename from task_id and run_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if checkpoint file exists and is readable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load and deserialize JSON data with error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate checkpoint data structure and completeness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return checkpoint data or None if not found/invalid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_old_checkpoints</span><span style=\"color:#E1E4E8\">(self, retention_hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove checkpoint files older than retention period.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Scan checkpoint directory for all checkpoint files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse timestamps from filenames or file modification times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify files older than retention_hours</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Remove old files with proper error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return count of files removed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Dead Letter Queue Manager:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DeadLetterMessage</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    original_payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failed_at: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DeadLetterQueueManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, storage_backend: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"database\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage_backend </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage_backend</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._initialize_storage()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> send_to_dead_letter_queue</span><span style=\"color:#E1E4E8\">(self, message: DeadLetterMessage) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send failed task to appropriate dead letter queue.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine dead letter queue category based on error_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Serialize message data for storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store message in appropriate dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update dead letter queue metrics and alerts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return success status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> retrieve_messages</span><span style=\"color:#E1E4E8\">(self, queue_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> List[DeadLetterMessage]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve messages from dead letter queue for reprocessing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Query storage backend for messages of specified queue_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Deserialize message data back to DeadLetterMessage objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply limit to prevent memory exhaustion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Mark retrieved messages as \"in_progress\" to prevent duplicate processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of messages ready for reprocessing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> requeue_message</span><span style=\"color:#E1E4E8\">(self, message_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, new_retry_policy: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Move message from dead letter queue back to normal processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve original message data from dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply new retry policy if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create new task execution record for retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Remove message from dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Submit task to normal execution queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing error handling components, verify the following behaviors:</p>\n<p><strong>Checkpoint 1 - Retry Logic Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run retry policy tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/error_handling/test_retry_policy.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output should show:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Exponential backoff calculations with jitter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Proper error type classification and retry decisions  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Maximum retry limits respected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Circuit breaker state transitions</span></span></code></pre></div>\n\n<p><strong>Checkpoint 2 - Dead Letter Queue Processing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Simulate pipeline failure and dead letter queue processing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/simulate_pipeline_failure.py</span><span style=\"color:#79B8FF\"> --task-id</span><span style=\"color:#9ECBFF\"> test-task</span><span style=\"color:#79B8FF\"> --error-type</span><span style=\"color:#9ECBFF\"> NetworkError</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Failed task appears in dead letter queue after max retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Error classification matches expected category</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Requeue functionality moves task back to processing</span></span></code></pre></div>\n\n<p><strong>Checkpoint 3 - Checkpoint Recovery:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test checkpoint save/restore functionality</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/test_checkpoint_recovery.py</span><span style=\"color:#79B8FF\"> --pipeline-id</span><span style=\"color:#9ECBFF\"> test-pipeline</span><span style=\"color:#79B8FF\"> --simulate-failure</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Pipeline creates checkpoints at regular intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - After simulated failure, pipeline resumes from last checkpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - No duplicate processing of checkpointed data</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tasks retry indefinitely</td>\n<td>Error type classified as retryable but actually permanent</td>\n<td>Check error classification logs and retry attempt patterns</td>\n<td>Update error classification rules to identify permanent errors</td>\n</tr>\n<tr>\n<td>Circuit breaker never opens</td>\n<td>Failure threshold too high or failures not being counted</td>\n<td>Monitor failure count metrics and circuit breaker state transitions</td>\n<td>Lower failure threshold or fix failure detection logic</td>\n</tr>\n<tr>\n<td>Checkpoints cause performance issues</td>\n<td>Checkpoint frequency too high or checkpoint data too large</td>\n<td>Profile checkpoint save times and storage usage</td>\n<td>Reduce checkpoint frequency or optimize checkpoint data size</td>\n</tr>\n<tr>\n<td>Dead letter queue grows unbounded</td>\n<td>No dead letter queue processing or requeue logic</td>\n<td>Monitor dead letter queue size and processing rate</td>\n<td>Implement automated dead letter queue processing</td>\n</tr>\n<tr>\n<td>Recovery produces duplicate data</td>\n<td>Checkpoint boundaries don&#39;t align with transaction boundaries</td>\n<td>Check for data duplication after recovery operations</td>\n<td>Align checkpoints with atomic operation boundaries</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - comprehensive testing approach validates pipeline definition (Milestone 1), data processing reliability (Milestones 2-3), and orchestration correctness (Milestone 4)</p>\n</blockquote>\n<h3 id=\"mental-model-quality-assurance-factory\">Mental Model: Quality Assurance Factory</h3>\n<p>Think of our testing strategy as a multi-stage quality assurance factory for complex machinery. Just as a car manufacturer tests individual components (engines, transmissions, brakes) in isolation before assembling them into complete vehicles, and then tests those vehicles under various road conditions, our ETL system requires testing at multiple levels. We test individual components like cycle detection algorithms and data connectors in isolation (unit testing), then test how they work together with real data sources (integration testing), and finally verify that each major assembly milestone produces working functionality (milestone checkpoints). Each testing stage catches different types of defects - unit tests catch logic errors, integration tests catch interface mismatches, and milestone checkpoints catch system-level failures.</p>\n<p>The key insight is that ETL systems have unique testing challenges compared to typical web applications. They process large volumes of data, interact with external systems that may be unreliable, and have complex dependency graphs that can fail in subtle ways. Our testing strategy must address data quality issues, handle non-deterministic external dependencies, and validate both correctness and performance characteristics under realistic load conditions.</p>\n<h3 id=\"unit-testing-approach\">Unit Testing Approach</h3>\n<p><strong>Component Isolation Strategy</strong></p>\n<p>Unit testing in an ETL system requires careful isolation of components that normally interact with external systems, maintain state across operations, and process large data volumes. The primary challenge is creating reliable, fast tests that validate core logic without depending on databases, file systems, or network services.</p>\n<p>Our unit testing approach focuses on testing individual algorithms, data transformations, and business logic in complete isolation. Each test should run in milliseconds, be deterministic regardless of execution order, and require no external dependencies. This means mocking or stubbing any interactions with databases, APIs, file systems, or message brokers.</p>\n<p><strong>Core Component Testing Areas</strong></p>\n<p>The following table outlines the major component categories and their specific testing requirements:</p>\n<table>\n<thead>\n<tr>\n<th>Component Type</th>\n<th>Primary Test Focus</th>\n<th>Mock Dependencies</th>\n<th>Key Test Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DAG Engine</td>\n<td>Cycle detection, topological sorting</td>\n<td>None (pure algorithms)</td>\n<td>Valid DAGs, circular dependencies, complex graphs</td>\n</tr>\n<tr>\n<td>Connectors</td>\n<td>Query generation, pagination logic</td>\n<td>Database connections, HTTP clients</td>\n<td>SQL templating, cursor handling, error responses</td>\n</tr>\n<tr>\n<td>Transformations</td>\n<td>Data type conversion, null handling</td>\n<td>External data sources</td>\n<td>Type coercion, schema validation, edge cases</td>\n</tr>\n<tr>\n<td>State Machine</td>\n<td>Task state transitions</td>\n<td>Persistence layer, message broker</td>\n<td>Valid transitions, invalid events, retry logic</td>\n</tr>\n<tr>\n<td>Scheduler</td>\n<td>Cron parsing, next run calculation</td>\n<td>System clock, database</td>\n<td>Schedule expressions, timezone handling, DST</td>\n</tr>\n<tr>\n<td>Orchestrator</td>\n<td>Execution order, resource allocation</td>\n<td>Task executors, monitoring</td>\n<td>Parallel execution, failure propagation, cleanup</td>\n</tr>\n</tbody></table>\n<p><strong>Data Structure Validation Tests</strong></p>\n<p>Every data structure defined in our system requires comprehensive validation testing. These tests verify that validation functions correctly identify invalid data and that serialization/deserialization preserves data integrity across system boundaries.</p>\n<p>For <code>PipelineDefinition</code> validation, we test scenarios including missing required fields, invalid cron expressions, circular task dependencies, invalid parameter types, and malformed task configurations. Each validation test should verify that appropriate error messages are generated with sufficient detail for debugging.</p>\n<p>For <code>TaskExecution</code> state management, we test state transitions under normal conditions and edge cases including invalid state combinations, concurrent state modifications, and recovery from corrupted state. These tests use mock storage backends to avoid dependency on persistent storage systems.</p>\n<p><strong>Algorithm Testing with Edge Cases</strong></p>\n<p>The DAG processing algorithms require particularly thorough testing because they handle complex graph structures that can fail in subtle ways. The <code>detect_cycles_dfs</code> function must be tested with various graph topologies including simple cycles, complex interconnected cycles, self-loops, disconnected components, and graphs with thousands of nodes to verify performance characteristics.</p>\n<p>The <code>topological_sort_kahns</code> algorithm requires testing with scenarios including linear chains (no parallelism), fully parallel tasks (no dependencies), diamond-shaped dependencies, and graphs with multiple valid topological orderings. We verify that the algorithm correctly identifies parallelizable task groups and handles edge cases like empty graphs and single-node graphs.</p>\n<p><strong>Mock Strategy for External Dependencies</strong></p>\n<p>External dependency mocking follows a consistent pattern across all components. Database connections are mocked using test doubles that simulate query execution, connection failures, and timeout scenarios. HTTP clients are mocked to return predefined responses, simulate network errors, and test pagination edge cases.</p>\n<p>The following table shows our mocking approach for each external system type:</p>\n<table>\n<thead>\n<tr>\n<th>External System</th>\n<th>Mock Implementation</th>\n<th>Test Scenarios</th>\n<th>Failure Simulation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database</td>\n<td>In-memory query executor</td>\n<td>Query results, schema info</td>\n<td>Connection timeout, query failure</td>\n</tr>\n<tr>\n<td>REST API</td>\n<td>HTTP response simulator</td>\n<td>JSON responses, pagination</td>\n<td>Network error, rate limiting</td>\n</tr>\n<tr>\n<td>File System</td>\n<td>In-memory file operations</td>\n<td>File read/write, directory listing</td>\n<td>Permission denied, disk full</td>\n</tr>\n<tr>\n<td>Message Broker</td>\n<td>Event queue simulator</td>\n<td>Message delivery, ordering</td>\n<td>Message loss, duplicate delivery</td>\n</tr>\n<tr>\n<td>Time/Clock</td>\n<td>Controllable time source</td>\n<td>Schedule calculation, timeouts</td>\n<td>Clock drift, timezone changes</td>\n</tr>\n</tbody></table>\n<p><strong>Data Transformation Testing</strong></p>\n<p>Data transformation testing requires careful handling of data type edge cases, null value semantics, and schema evolution scenarios. We create test datasets that include boundary values for each data type, various null representations, and malformed data that should trigger validation errors.</p>\n<p>Type conversion testing covers all supported <code>DataType</code> combinations, including lossy conversions that should generate warnings, invalid conversions that should fail, and precision preservation for numeric types. Each conversion test includes boundary values like maximum integers, special floating-point values (NaN, infinity), and edge cases like empty strings and whitespace-only values.</p>\n<p>Schema validation testing uses controlled test schemas to verify that validation correctly identifies required field violations, type mismatches, constraint violations, and unknown fields. We test both strict validation (reject any violations) and lenient validation (warn but continue processing) modes.</p>\n<p><strong>Performance and Memory Testing</strong></p>\n<p>Unit tests include performance benchmarks for critical algorithms to catch performance regressions during development. The DAG algorithms are benchmarked with graphs of various sizes to verify that performance scales appropriately with graph complexity.</p>\n<p>Memory usage testing is particularly important for data processing components that handle large datasets. We use controlled test datasets to verify that streaming operations maintain constant memory usage regardless of input size, and that batch operations properly release memory after processing.</p>\n<p><strong>Common Pitfalls in Unit Testing</strong></p>\n<p> <strong>Pitfall: Testing with Real External Systems</strong>\nMany developers write unit tests that connect to actual databases or APIs during development. This makes tests slow, non-deterministic, and dependent on external system availability. Instead, use mock implementations that simulate the exact interface behavior without external dependencies.</p>\n<p> <strong>Pitfall: Insufficient Edge Case Coverage</strong>\nETL systems handle diverse data types and formats, making edge case testing critical. Don&#39;t just test happy path scenarios - include boundary values, malformed data, and unusual but valid input combinations. Create comprehensive test data generators that cover the full range of possible inputs.</p>\n<p> <strong>Pitfall: Ignoring Concurrent Access Patterns</strong>\nMany ETL components will run concurrently in production. Unit tests should include scenarios with concurrent access to shared resources, using techniques like goroutines with channels or threading libraries to simulate race conditions and verify thread safety.</p>\n<h3 id=\"integration-testing\">Integration Testing</h3>\n<p><strong>End-to-End Pipeline Validation</strong></p>\n<p>Integration testing validates that our ETL components work correctly when combined with real external systems and realistic data volumes. Unlike unit tests that focus on individual component logic, integration tests verify that data flows correctly through the entire pipeline, that external system interactions handle real-world conditions, and that performance meets requirements under realistic load.</p>\n<p>The primary challenge in integration testing is creating realistic test environments that include representative data sources, network conditions, and system load without requiring full production infrastructure. We accomplish this through containerized test environments, synthetic data generation, and careful test data management.</p>\n<p><strong>Test Environment Architecture</strong></p>\n<p>Our integration test environment uses containerized services to simulate production dependencies while maintaining test isolation and repeatability. The test environment includes database containers with realistic schemas and data volumes, mock API services that simulate third-party systems, and message broker containers for testing event-driven components.</p>\n<p>Each test run starts with a clean environment state, loads controlled test data, executes the pipeline under test, and validates the results against expected outcomes. Test data is carefully crafted to include realistic data distributions, common data quality issues, and edge cases that occur in production systems.</p>\n<p>The following table outlines our test environment components:</p>\n<table>\n<thead>\n<tr>\n<th>Environment Component</th>\n<th>Implementation</th>\n<th>Purpose</th>\n<th>Data Volume</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Source Database</td>\n<td>PostgreSQL container</td>\n<td>Realistic SQL extraction testing</td>\n<td>10K-100K records</td>\n</tr>\n<tr>\n<td>API Mock Service</td>\n<td>HTTP server with predefined responses</td>\n<td>API connector testing</td>\n<td>Configurable pagination</td>\n</tr>\n<tr>\n<td>Target Database</td>\n<td>PostgreSQL container</td>\n<td>Bulk loading validation</td>\n<td>Variable based on test</td>\n</tr>\n<tr>\n<td>Message Broker</td>\n<td>Redis container</td>\n<td>Event-driven testing</td>\n<td>Message ordering validation</td>\n</tr>\n<tr>\n<td>File Storage</td>\n<td>Local filesystem mount</td>\n<td>File-based extraction</td>\n<td>Multiple file formats</td>\n</tr>\n<tr>\n<td>Monitoring Stack</td>\n<td>Prometheus/Grafana containers</td>\n<td>Metrics collection testing</td>\n<td>Full metric pipeline</td>\n</tr>\n</tbody></table>\n<p><strong>Data Source Integration Testing</strong></p>\n<p>Database connector testing uses real database instances with controlled test schemas and data. We test against multiple database types (PostgreSQL, MySQL, SQL Server) to verify that our SQL generation and result parsing work across different database dialects and driver implementations.</p>\n<p>Test scenarios include complex join queries, large result sets that require pagination, schema introspection for automatic mapping, and connection pooling under concurrent access. We simulate database failures including connection timeouts, query timeouts, and temporary network partitions to verify that error handling and retry logic work correctly.</p>\n<p>API connector testing uses both real external APIs (where available and appropriate) and sophisticated mock services that simulate real API behavior. Mock services implement realistic pagination patterns, rate limiting, authentication flows, and error responses that match actual API behavior.</p>\n<p>The following integration test scenarios validate API connector robustness:</p>\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Validation Focus</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Large dataset pagination</td>\n<td>Cursor stability across pages</td>\n<td>Complete data extraction without duplicates</td>\n</tr>\n<tr>\n<td>Rate limit handling</td>\n<td>Backoff and retry logic</td>\n<td>Automatic throttling with eventual success</td>\n</tr>\n<tr>\n<td>Authentication token refresh</td>\n<td>Token expiration handling</td>\n<td>Seamless authentication renewal</td>\n</tr>\n<tr>\n<td>Partial API failures</td>\n<td>Individual request failures</td>\n<td>Retry failed requests without re-fetching successful pages</td>\n</tr>\n<tr>\n<td>Network instability</td>\n<td>Connection interruption recovery</td>\n<td>Resume from last successful position</td>\n</tr>\n</tbody></table>\n<p><strong>Transformation Pipeline Integration</strong></p>\n<p>Transformation integration testing validates that SQL-based transformations execute correctly against real database engines and that Python UDF execution handles realistic data volumes and types. We test transformation chains where the output of one transformation becomes the input to subsequent transformations, verifying that data types are preserved correctly across the chain.</p>\n<p>Schema evolution testing uses versioned test schemas to verify that our transformation pipeline handles schema changes gracefully. We test scenarios including adding columns, changing column types, renaming columns, and removing columns, validating that appropriate warnings are generated and data processing continues where possible.</p>\n<p>UDF integration testing executes Python functions in isolated subprocess environments with realistic data volumes to verify that memory usage, execution time, and error handling meet production requirements. We test both row-by-row UDF execution and batch processing modes to validate performance characteristics.</p>\n<p><strong>End-to-End Pipeline Execution</strong></p>\n<p>Complete pipeline integration tests execute full ETL workflows from source extraction through transformation to target loading. These tests use realistic data volumes (thousands to tens of thousands of records) to validate performance characteristics and identify bottlenecks that only appear under load.</p>\n<p>We test various pipeline topologies including linear pipelines, diamond-shaped dependency graphs, and complex multi-source pipelines that combine data from multiple sources. Each test validates that data lineage tracking captures complete provenance information and that monitoring metrics accurately reflect pipeline performance.</p>\n<p><strong>Failure and Recovery Testing</strong></p>\n<p>Integration tests include comprehensive failure scenario testing to validate that our error handling and retry logic work correctly with real external systems. We use techniques like network manipulation, process termination, and resource exhaustion to simulate realistic failure conditions.</p>\n<p>Recovery testing validates that pipelines can resume correctly after various failure types. We test scenarios including mid-pipeline failures, external system outages, and resource exhaustion, verifying that checkpointing and watermarking allow pipelines to resume from appropriate points without data duplication or loss.</p>\n<p>The following table outlines key failure scenarios and their validation criteria:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Simulation Method</th>\n<th>Recovery Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database connection loss</td>\n<td>Network partition during extraction</td>\n<td>Resume from last successful batch</td>\n</tr>\n<tr>\n<td>API rate limit exceeded</td>\n<td>Mock service rate limiting</td>\n<td>Automatic backoff and retry</td>\n</tr>\n<tr>\n<td>Disk space exhaustion</td>\n<td>Filesystem quota limits</td>\n<td>Graceful failure with cleanup</td>\n</tr>\n<tr>\n<td>Memory exhaustion</td>\n<td>Large dataset processing</td>\n<td>Streaming fallback or batch reduction</td>\n</tr>\n<tr>\n<td>Process termination</td>\n<td>SIGTERM during transformation</td>\n<td>Checkpoint recovery on restart</td>\n</tr>\n</tbody></table>\n<p><strong>Performance and Scalability Testing</strong></p>\n<p>Integration tests include performance benchmarks that validate system behavior under realistic load conditions. We test data processing throughput, memory usage patterns, and concurrent pipeline execution to identify performance bottlenecks and validate that resource usage scales appropriately with data volume.</p>\n<p>Scalability testing validates that our system handles increasing data volumes gracefully. We test with datasets ranging from thousands to hundreds of thousands of records, measuring processing time, memory usage, and system resource consumption to identify scaling limitations.</p>\n<p><strong>Common Pitfalls in Integration Testing</strong></p>\n<p> <strong>Pitfall: Insufficient Test Data Realism</strong>\nUsing overly simplified test data fails to catch issues that occur with real-world data complexity. Create test datasets that include realistic data distributions, common data quality issues, and edge cases found in production systems. Include null values, special characters, and boundary values in test data.</p>\n<p> <strong>Pitfall: Ignoring External System Variability</strong>\nExternal systems behave differently under various conditions. Test with different response times, intermittent failures, and varying data volumes to ensure your pipeline handles real-world system variability. Don&#39;t assume external systems will always respond quickly or correctly.</p>\n<p> <strong>Pitfall: Inadequate Cleanup Between Tests</strong>\nETL integration tests can leave persistent state in databases, file systems, and external services. Implement thorough cleanup procedures that run before and after each test to ensure test isolation and prevent cascading failures between test runs.</p>\n<h3 id=\"milestone-checkpoints\">Milestone Checkpoints</h3>\n<p><strong>Milestone 1: DAG Definition and Validation Checkpoint</strong></p>\n<p>After implementing the DAG definition and validation engine, the system must demonstrate correct parsing, validation, and visualization of pipeline definitions. This checkpoint validates that the foundation for all subsequent pipeline operations is solid and handles complex dependency scenarios correctly.</p>\n<p><strong>Validation Criteria for Milestone 1:</strong></p>\n<p>The checkpoint validates DAG parsing from both YAML and Python configuration formats, ensuring that all pipeline metadata including task configurations, dependencies, and parameters are correctly extracted. We test with increasingly complex pipeline definitions to verify that the system scales to realistic production scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Scenarios</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>YAML Parsing</td>\n<td>Simple linear pipeline, complex multi-source DAG</td>\n<td>Correct task extraction and dependency mapping</td>\n</tr>\n<tr>\n<td>Python Definition</td>\n<td>Programmatic pipeline creation, dynamic task generation</td>\n<td>Proper object model population</td>\n</tr>\n<tr>\n<td>Cycle Detection</td>\n<td>Intentional cycles, complex indirect cycles</td>\n<td>Accurate cycle identification with path reporting</td>\n</tr>\n<tr>\n<td>Topological Sort</td>\n<td>Various DAG shapes, parallel execution opportunities</td>\n<td>Correct execution levels with maximum parallelism</td>\n</tr>\n<tr>\n<td>Parameter Substitution</td>\n<td>Runtime variables, nested parameter references</td>\n<td>Proper template resolution with type checking</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Malformed YAML, invalid dependencies, missing tasks</td>\n<td>Clear error messages with specific problem identification</td>\n</tr>\n</tbody></table>\n<p><strong>Manual Verification Steps:</strong></p>\n<p>Execute the checkpoint validation by running a comprehensive test suite that exercises all DAG processing functionality. Start with simple pipeline definitions and progressively test more complex scenarios including pipelines with dozens of tasks and complex dependency relationships.</p>\n<p>Verify DAG visualization by generating graph representations of test pipelines and confirming that the visual output accurately represents the dependency structure. Check that parallel execution opportunities are correctly identified and that critical path calculations provide accurate estimates.</p>\n<p>Test error handling by intentionally creating invalid pipeline definitions and verifying that error messages provide sufficient detail for developers to identify and fix configuration problems.</p>\n<p><strong>Milestone 2: Data Extraction and Loading Checkpoint</strong></p>\n<p>The data extraction and loading checkpoint validates that connectors can reliably extract data from various sources and load it to target destinations with proper error handling and incremental processing capabilities. This milestone is critical because data connectivity issues are among the most common ETL pipeline failures.</p>\n<p><strong>Source Connector Validation:</strong></p>\n<p>Database connector testing uses real database instances with controlled test schemas containing representative data types, null values, and edge cases. We validate that SQL query generation produces correct queries for various extraction patterns including simple SELECT statements, complex joins, and filtered extractions.</p>\n<p>Incremental extraction testing verifies that watermarking correctly identifies changed records and that pagination handles large result sets efficiently. We test with datasets large enough to require multiple pages and verify that no records are missed or duplicated during extraction.</p>\n<p>API connector testing validates HTTP client behavior including authentication, pagination, rate limiting, and error recovery. We test against both mock APIs and real external services where appropriate, ensuring that connector behavior matches API specifications and handles real-world API quirks.</p>\n<p>The following validation criteria ensure connector reliability:</p>\n<table>\n<thead>\n<tr>\n<th>Connector Type</th>\n<th>Validation Focus</th>\n<th>Test Data Volume</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database</td>\n<td>SQL generation, type mapping, pagination</td>\n<td>50K+ records</td>\n<td>Complete extraction without data loss</td>\n</tr>\n<tr>\n<td>REST API</td>\n<td>Authentication, pagination, rate limiting</td>\n<td>10K+ records across pages</td>\n<td>Proper cursor handling and retry logic</td>\n</tr>\n<tr>\n<td>File System</td>\n<td>Format detection, streaming, compression</td>\n<td>Multi-GB files</td>\n<td>Memory-efficient processing</td>\n</tr>\n</tbody></table>\n<p><strong>Destination Connector Validation:</strong></p>\n<p>Loading connector testing validates that data can be efficiently written to target systems with proper error handling and transactional guarantees. We test bulk loading scenarios with large datasets to verify that performance meets requirements and that partial failures are handled gracefully.</p>\n<p>Schema mapping testing ensures that data types are correctly converted between source and destination systems, with appropriate warnings for lossy conversions and errors for incompatible types. We test with diverse data type combinations to verify comprehensive type system coverage.</p>\n<p><strong>Milestone 3: Data Transformation Checkpoint</strong></p>\n<p>The transformation checkpoint validates that both SQL-based transformations and Python UDFs execute correctly with realistic data volumes and handle error conditions gracefully. This milestone ensures that data quality and transformation logic meet business requirements.</p>\n<p><strong>SQL Transformation Validation:</strong></p>\n<p>SQL transformation testing executes template-based transformations against real database engines to verify that SQL generation produces correct results and handles parameter substitution properly. We test with complex transformations including aggregations, window functions, and multi-table joins.</p>\n<p>Schema validation testing ensures that transformation outputs match expected schemas and that data quality rules correctly identify and handle invalid records. We test with datasets containing known data quality issues to verify that validation logic produces appropriate warnings and errors.</p>\n<p><strong>UDF Execution Validation:</strong></p>\n<p>Python UDF testing validates that user-defined functions execute correctly in isolated environments with proper resource management and error handling. We test both row-by-row processing and batch processing modes to verify performance characteristics meet requirements.</p>\n<p>The following table outlines UDF validation criteria:</p>\n<table>\n<thead>\n<tr>\n<th>UDF Type</th>\n<th>Execution Mode</th>\n<th>Test Data</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple transformations</td>\n<td>Row-by-row</td>\n<td>100K records</td>\n<td>&lt; 10ms per record average</td>\n</tr>\n<tr>\n<td>Complex analytics</td>\n<td>Batch processing</td>\n<td>1M+ records</td>\n<td>Linear scaling with data volume</td>\n</tr>\n<tr>\n<td>External API calls</td>\n<td>Rate-limited batch</td>\n<td>10K records</td>\n<td>Proper throttling and error handling</td>\n</tr>\n</tbody></table>\n<p><strong>Error Handling Validation:</strong></p>\n<p>Transformation error handling testing validates that invalid data is handled gracefully with appropriate logging and that pipeline execution continues where possible. We test scenarios including malformed input data, UDF exceptions, and resource exhaustion.</p>\n<p><strong>Milestone 4: Orchestration and Monitoring Checkpoint</strong></p>\n<p>The final checkpoint validates complete end-to-end pipeline execution with scheduling, monitoring, and alerting functionality. This milestone ensures that the system can reliably execute production workloads with appropriate observability and failure recovery.</p>\n<p><strong>Schedule Execution Validation:</strong></p>\n<p>Scheduler testing validates that pipelines execute according to configured schedules and that event-driven triggers work correctly. We test various cron expressions including edge cases like month boundaries and daylight saving time transitions.</p>\n<p>Concurrent execution testing validates that the system handles multiple simultaneous pipeline runs correctly with proper resource isolation and conflict detection.</p>\n<p><strong>Monitoring and Alerting Validation:</strong></p>\n<p>Monitoring checkpoint testing validates that metrics are correctly collected and reported, that alerting triggers appropriately for various failure conditions, and that data lineage tracking captures complete provenance information.</p>\n<p>We test monitoring under various load conditions to verify that metric collection doesn&#39;t significantly impact pipeline performance and that alerting provides actionable information for operators.</p>\n<p><strong>End-to-End Integration Validation:</strong></p>\n<p>The final validation executes complex multi-stage pipelines that exercise all system components together. These tests use realistic data volumes and complexity to validate that the complete system meets performance and reliability requirements.</p>\n<p>We execute failure recovery testing that simulates various production failure scenarios and validates that the system recovers gracefully with minimal data loss or processing delays.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test Framework</td>\n<td>pytest with fixtures</td>\n<td>pytest + testcontainers + factory-boy</td>\n</tr>\n<tr>\n<td>Mock Library</td>\n<td>unittest.mock (built-in)</td>\n<td>responses + freezegun + factory-boy</td>\n</tr>\n<tr>\n<td>Database Testing</td>\n<td>SQLite in-memory</td>\n<td>PostgreSQL testcontainers</td>\n</tr>\n<tr>\n<td>API Testing</td>\n<td>requests-mock</td>\n<td>WireMock or Prism mock server</td>\n</tr>\n<tr>\n<td>Performance Testing</td>\n<td>pytest-benchmark</td>\n<td>locust for load testing</td>\n</tr>\n<tr>\n<td>Test Data</td>\n<td>JSON fixtures</td>\n<td>factory-boy data factories</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Test Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\ntests/\n  unit/                           isolated component tests\n    test_dag_engine.py            DAG parsing and validation\n    test_connectors.py            individual connector logic\n    test_transformations.py       transformation functions\n    test_scheduler.py             scheduling logic\n    test_state_machine.py         state transition logic\n  integration/                    end-to-end component tests\n    test_pipeline_execution.py    complete pipeline runs\n    test_external_systems.py      real database/API tests\n    test_failure_recovery.py      error handling scenarios\n  milestones/                     milestone checkpoint tests\n    test_milestone_1.py           DAG definition checkpoint\n    test_milestone_2.py           extraction/loading checkpoint\n    test_milestone_3.py           transformation checkpoint\n    test_milestone_4.py           orchestration checkpoint\n  fixtures/                       test data and configuration\n    pipelines/                    sample pipeline definitions\n    data/                         test datasets\n    mocks/                        mock service configurations\n  conftest.py                     pytest configuration and shared fixtures</code></pre></div>\n\n<p><strong>Unit Test Infrastructure Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/conftest.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tempfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> shutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> unittest.mock </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MagicMock</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> mock_database_connection</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Mock database connection for unit tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MagicMock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.execute.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.fetchall.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.fetchone.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.commit.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.rollback.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn.close.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> conn</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> mock_http_client</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Mock HTTP client for API connector tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MagicMock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client.get.return_value.status_code </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 200</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client.get.return_value.json.return_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"data\"</span><span style=\"color:#E1E4E8\">: []}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client.post.return_value.status_code </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 201</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> client</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> sample_pipeline_definition</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Standard pipeline definition for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> PipelineDefinition(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        id</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"test-pipeline\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Test Pipeline\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Pipeline for unit testing\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        schedule</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"0 * * * *\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        tasks</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskDefinition(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                id</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"extract-task\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Extract Data\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"database_extract\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                config</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"query\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"SELECT * FROM users\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                dependencies</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                retry_policy</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RetryPolicy(</span><span style=\"color:#FFAB70\">max_attempts</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">backoff_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exponential_backoff</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">retry_on_error_types</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"NETWORK_ERROR\"</span><span style=\"color:#E1E4E8\">]),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timeout_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">300</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TaskDefinition(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                id</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"transform-task\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Transform Data\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"sql_transform\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                config</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"query\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"SELECT id, UPPER(name) as name FROM input\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                dependencies</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"extract-task\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                retry_policy</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RetryPolicy(</span><span style=\"color:#FFAB70\">max_attempts</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">backoff_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exponential_backoff</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">retry_on_error_types</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"DATA_QUALITY_ERROR\"</span><span style=\"color:#E1E4E8\">]),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timeout_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">600</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        parameters</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"source_table\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"users\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        created_at</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.now(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        version</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> temp_directory</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Temporary directory for file-based tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temp_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tempfile.mkdtemp()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    yield</span><span style=\"color:#E1E4E8\"> temp_dir</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shutil.rmtree(temp_dir)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MockTimeProvider</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Controllable time source for testing scheduling logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, start_time: datetime):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> now</span><span style=\"color:#E1E4E8\">(self) -> datetime:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance</span><span style=\"color:#E1E4E8\">(self, delta: timedelta):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_time </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> delta</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> mock_time</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Mock time provider for deterministic scheduling tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> MockTimeProvider(datetime(</span><span style=\"color:#79B8FF\">2024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span></code></pre></div>\n\n<p><strong>Unit Test Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/unit/test_dag_engine.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dag_engine </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DAGEngine, CycleDetectionResult, ExecutionPlan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineDefinition, TaskDefinition</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestDAGValidation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test suite for DAG parsing and validation logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_detect_cycles_simple_cycle</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test cycle detection with a simple two-node cycle.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create adjacency list representing A -> B -> A cycle</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call detect_cycles_dfs with the adjacency list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Assert that has_cycle is True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Assert that cycle_path contains the expected cycle nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify that cycle_path forms a valid cycle (first == last node)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_topological_sort_parallel_tasks</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test topological sort identifies parallel execution opportunities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create adjacency list with diamond dependency pattern (A -> B,C -> D)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call topological_sort_kahns with the adjacency list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Assert that result has correct number of execution levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Assert that B and C are in the same execution level (can run parallel)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Assert that A is in first level and D is in last level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_parse_yaml_invalid_syntax</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test YAML parsing with malformed syntax.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create YAML string with invalid syntax (missing colons, wrong indentation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call parse_yaml_file with the invalid YAML</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Assert that appropriate exception is raised</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify that error message contains useful debugging information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestExecutionPlanning</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test suite for DAG execution planning logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_calculate_critical_path</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test critical path calculation with task duration estimates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create adjacency list with known task dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create task_durations dict with estimated execution times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Call calculate_critical_path with adjacency list and durations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Assert that returned path represents the longest duration chain</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Assert that total duration matches sum of critical path task durations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Integration Test Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/integration/conftest.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> docker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psycopg2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> testcontainers.postgres </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PostgresContainer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> testcontainers.compose </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DockerCompose</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">scope</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"session\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> postgres_container</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"PostgreSQL container for integration testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> PostgresContainer(</span><span style=\"color:#9ECBFF\">\"postgres:13\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">driver</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"psycopg2\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> postgres:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Wait for container to be ready</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time.sleep(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create test schema and sample data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connection </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psycopg2.connect(postgres.get_connection_url())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> connection.cursor() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cursor:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE TABLE users (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    id SERIAL PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    name VARCHAR(100) NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    email VARCHAR(100) UNIQUE,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                );</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                INSERT INTO users (name, email) VALUES</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                ('Alice Johnson', 'alice@example.com'),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                ('Bob Smith', 'bob@example.com'),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                ('Charlie Brown', 'charlie@example.com');</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            connection.commit()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> postgres</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">scope</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"session\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_environment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete test environment with all external services.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#E1E4E8\"> DockerCompose(</span><span style=\"color:#9ECBFF\">\"tests/integration\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">compose_file_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"docker-compose.test.yml\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> compose:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Wait for all services to be ready</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time.sleep(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> compose</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> IntegrationTestHelper</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper utilities for integration testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> wait_for_pipeline_completion</span><span style=\"color:#E1E4E8\">(pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Wait for pipeline run to complete with timeout.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement polling logic to check pipeline run status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Raise TimeoutError if pipeline doesn't complete within timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_data_lineage</span><span style=\"color:#E1E4E8\">(source_table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, target_table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, expected_transformations: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that data lineage correctly captures transformations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Query lineage tracking system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify that all expected transformations are recorded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Assert that source and target tables are correctly linked</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/milestones/test_milestone_1.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dag_engine </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DAGEngine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PipelineDefinition</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestMilestone1Checkpoint</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Milestone 1: DAG Definition and Validation checkpoint tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_yaml_pipeline_parsing</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify YAML pipeline definitions parse correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load sample YAML pipeline from fixtures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse YAML using parse_yaml_file function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate all required fields are present in PipelineDefinition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify task dependencies are correctly mapped</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check that parameters are properly extracted</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_complex_dag_validation</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test validation with realistic complex pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create pipeline with 10+ tasks and complex dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that cycle detection completes successfully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify topological sort produces valid execution order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check that parallel execution opportunities are identified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate that critical path calculation is accurate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_error_reporting_quality</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify that validation errors provide actionable information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create pipeline with intentional errors (cycles, missing tasks)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run validation and capture error messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Assert that errors specify exact problem location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify that suggested fixes are included where possible</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Checkpoint validation command:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># pytest tests/milestones/test_milestone_1.py -v</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: All tests pass, demonstrating working DAG engine</span></span></code></pre></div>\n\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tests hang indefinitely</td>\n<td>External service dependency not mocked</td>\n<td>Check test for database/API calls</td>\n<td>Add appropriate mocks or use testcontainers</td>\n</tr>\n<tr>\n<td>Intermittent test failures</td>\n<td>Race conditions in concurrent code</td>\n<td>Run tests multiple times, check for shared state</td>\n<td>Add proper synchronization or test isolation</td>\n</tr>\n<tr>\n<td>Memory usage grows during tests</td>\n<td>Test data not cleaned between runs</td>\n<td>Monitor memory usage with memory_profiler</td>\n<td>Implement proper teardown in fixtures</td>\n</tr>\n<tr>\n<td>Integration tests fail in CI</td>\n<td>Different environment configuration</td>\n<td>Compare local vs CI environment settings</td>\n<td>Standardize environment using containers</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone Checkpoint Commands:</strong></p>\n<p>After completing each milestone, run these validation commands:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Milestone 1: DAG Definition and Validation</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/milestones/test_milestone_1.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> dag_engine.visualizer</span><span style=\"color:#79B8FF\"> --input</span><span style=\"color:#9ECBFF\"> tests/fixtures/pipelines/complex.yaml</span><span style=\"color:#79B8FF\"> --output</span><span style=\"color:#9ECBFF\"> /tmp/dag.svg</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 2: Data Extraction and Loading  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/milestones/test_milestone_2.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_connectors.py</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> --tb=short</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 3: Data Transformations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/milestones/test_milestone_3.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_transformations.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone 4: Orchestration and Monitoring</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/milestones/test_milestone_4.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_pipeline_execution.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Complete test suite</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">pytest</span><span style=\"color:#9ECBFF\"> tests/</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#79B8FF\"> --cov=src</span><span style=\"color:#79B8FF\"> --cov-report=html</span></span></code></pre></div>\n\n<p>Expected behavior after each milestone:</p>\n<ul>\n<li>All milestone-specific tests pass without errors</li>\n<li>Integration tests demonstrate working functionality with real data</li>\n<li>Performance benchmarks meet established criteria  </li>\n<li>Manual verification steps complete successfully</li>\n<li>System demonstrates graceful error handling and recovery</li>\n</ul>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - systematic debugging approaches are essential for pipeline definition troubleshooting (Milestone 1), data processing issue resolution (Milestones 2-3), and orchestration debugging (Milestone 4).</p>\n</blockquote>\n<h3 id=\"mental-model-medical-diagnosis\">Mental Model: Medical Diagnosis</h3>\n<p>Think of debugging ETL pipelines like being a doctor diagnosing a patient. You start by gathering symptoms (error messages, performance metrics, unusual behaviors), then form hypotheses about potential causes based on your understanding of how the system works. You run specific tests to confirm or rule out each hypothesis, narrowing down to the root cause. Just as a doctor has a systematic approach to diagnosis - checking vital signs first, then running targeted tests - debugging requires a methodical approach rather than random guessing.</p>\n<p>The key insight is that symptoms often manifest far from their root causes in distributed systems. A task timeout might be caused by network congestion, resource exhaustion, or a deadlock in a completely different component. Like referred pain in medicine, the symptom location doesn&#39;t always indicate the problem source.</p>\n<h3 id=\"common-symptoms-and-causes\">Common Symptoms and Causes</h3>\n<p>ETL pipeline debugging follows predictable patterns. Most issues fall into a few categories with characteristic symptoms that can guide diagnosis. Understanding these symptom-cause patterns allows developers to quickly narrow the search space and apply targeted fixes.</p>\n<p>The following symptom-cause mapping represents the most common issues encountered in production ETL systems, organized by the component where symptoms first appear:</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Immediate Diagnosis Steps</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline stuck in PENDING state</td>\n<td>DAG parsing errors or validation failures</td>\n<td>Check pipeline definition YAML/Python syntax; examine validation error logs</td>\n<td>Fix syntax errors, resolve dependency cycles, validate task configurations</td>\n</tr>\n<tr>\n<td>Tasks skip unexpectedly</td>\n<td>Upstream dependencies failed or wrong dependency configuration</td>\n<td>Trace upstream task states; verify dependency specifications match actual task IDs</td>\n<td>Correct dependency declarations, fix upstream failures, check for typos in task IDs</td>\n</tr>\n<tr>\n<td>&quot;Cycle detected&quot; error during parsing</td>\n<td>Circular dependencies in task definitions</td>\n<td>Run <code>detect_cycles_dfs()</code> manually on pipeline definition; examine dependency graph visualization</td>\n<td>Remove circular dependencies, consider splitting tasks, add conditional logic</td>\n</tr>\n<tr>\n<td>Tasks run in wrong order</td>\n<td>Topological sort error or missing dependencies</td>\n<td>Compare actual execution order with expected DAG visualization; check for missing dependency declarations</td>\n<td>Add missing dependencies, verify topological sort implementation, check for race conditions</td>\n</tr>\n<tr>\n<td>Database connection timeouts</td>\n<td>Connection pool exhaustion or network issues</td>\n<td>Monitor active connections, check network latency, examine connection pool metrics</td>\n<td>Increase pool size, implement connection retry logic, optimize query performance</td>\n</tr>\n<tr>\n<td>API extraction fails intermittently</td>\n<td>Rate limiting, authentication expiry, or pagination issues</td>\n<td>Check API response headers for rate limit info; examine authentication token validity; verify pagination cursor handling</td>\n<td>Implement exponential backoff, refresh tokens proactively, fix pagination logic</td>\n</tr>\n<tr>\n<td>Incremental loads miss data</td>\n<td>Watermark not updated atomically or clock skew issues</td>\n<td>Verify watermark update happens in same transaction as data load; check for time zone mismatches</td>\n<td>Use database transactions for atomic updates, add lookback window for clock skew</td>\n</tr>\n<tr>\n<td>Duplicate records in destination</td>\n<td>Upsert logic failures or non-atomic operations</td>\n<td>Check for unique constraint violations; examine upsert key definitions; verify transaction isolation</td>\n<td>Fix upsert key selection, use proper transaction boundaries, implement deduplication</td>\n</tr>\n<tr>\n<td>Schema validation errors</td>\n<td>Type coercion failures or schema evolution issues</td>\n<td>Compare source and destination schemas; check for null handling differences; examine type conversion logs</td>\n<td>Update schema mappings, implement graceful type coercion, handle null value differences</td>\n</tr>\n<tr>\n<td>Transformation timeouts</td>\n<td>Memory exhaustion from large datasets or inefficient SQL</td>\n<td>Monitor memory usage during transformation; examine SQL execution plans; check for full table scans</td>\n<td>Implement streaming processing, optimize SQL queries, add memory limits and checkpoints</td>\n</tr>\n<tr>\n<td>Python UDF crashes</td>\n<td>Unhandled exceptions or resource limits exceeded</td>\n<td>Examine UDF execution logs; check memory and CPU usage; verify error handling in UDF code</td>\n<td>Add try-catch blocks, implement resource monitoring, use subprocess isolation</td>\n</tr>\n<tr>\n<td>Data quality validation failures</td>\n<td>Source data corruption or business rule violations</td>\n<td>Sample failed records; examine validation rule logic; check for data drift in source systems</td>\n<td>Implement data profiling, adjust validation thresholds, add data quality monitoring</td>\n</tr>\n<tr>\n<td>Pipeline runs never start</td>\n<td>Scheduler configuration errors or resource unavailability</td>\n<td>Check cron expression syntax; verify scheduler service health; examine resource allocation logs</td>\n<td>Fix schedule syntax, restart scheduler service, increase resource limits</td>\n</tr>\n<tr>\n<td>Tasks hang in RUNNING state indefinitely</td>\n<td>Deadlocks, infinite loops, or resource starvation</td>\n<td>Check for database locks; examine task execution logs; monitor CPU and memory usage</td>\n<td>Implement task timeouts, add deadlock detection, optimize resource usage</td>\n</tr>\n<tr>\n<td>Pipeline fails but tasks show SUCCESS</td>\n<td>State management race conditions or incomplete failure propagation</td>\n<td>Compare task-level and pipeline-level state; check for concurrent state updates; examine event ordering</td>\n<td>Fix state transition logic, implement proper locking, ensure atomic state updates</td>\n</tr>\n<tr>\n<td>Memory usage grows continuously</td>\n<td>Memory leaks in transformation code or unclosed connections</td>\n<td>Profile memory usage over time; check for unclosed database connections; examine object lifecycle</td>\n<td>Fix connection management, implement proper cleanup, add garbage collection monitoring</td>\n</tr>\n<tr>\n<td>High CPU usage with no progress</td>\n<td>Infinite loops, busy waiting, or inefficient algorithms</td>\n<td>Profile CPU usage by component; examine algorithm complexity; check for blocking operations</td>\n<td>Optimize algorithms, add sleep to polling loops, implement proper backpressure</td>\n</tr>\n<tr>\n<td>Disk space fills rapidly</td>\n<td>Excessive logging, temporary file accumulation, or large intermediate results</td>\n<td>Check log file sizes; examine temporary directory usage; monitor intermediate data volumes</td>\n<td>Implement log rotation, clean up temporary files, use streaming for large datasets</td>\n</tr>\n<tr>\n<td>Network timeouts between components</td>\n<td>Network congestion, firewall issues, or component overload</td>\n<td>Check network latency and packet loss; verify firewall rules; examine component health metrics</td>\n<td>Increase timeout values, implement retry logic, optimize network usage</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: The vast majority of ETL pipeline issues stem from three root causes: resource management problems (memory, connections, disk), state consistency issues (race conditions, atomic operations), and data quality problems (schema changes, null handling). Understanding these patterns helps focus debugging efforts.</p>\n</blockquote>\n<h3 id=\"common-anti-patterns-and-debugging-traps\">Common Anti-Patterns and Debugging Traps</h3>\n<p> <strong>Pitfall: Log Flooding During Debugging</strong>\nDevelopers often enable verbose logging across all components when debugging, generating massive log volumes that obscure the actual problem. This makes debugging slower and can impact system performance.</p>\n<p><em>Why it&#39;s wrong</em>: Excessive logging creates noise that hides signal, fills disk space rapidly, and can cause performance degradation that masks the original issue.</p>\n<p><em>Fix</em>: Enable targeted logging only for the specific component and time window where the issue occurs. Use log levels strategically and implement log sampling for high-frequency events.</p>\n<p> <strong>Pitfall: Testing in Isolation Only</strong>\nTesting individual components in isolation without integration testing often misses issues that only appear when components interact under realistic conditions.</p>\n<p><em>Why it&#39;s wrong</em>: Many ETL issues emerge from component interactions, timing dependencies, and resource contention that don&#39;t appear in unit tests.</p>\n<p><em>Fix</em>: Implement comprehensive integration tests with realistic data volumes and concurrent execution patterns. Test failure scenarios and recovery paths.</p>\n<p> <strong>Pitfall: Ignoring Resource Limits</strong>\nDebugging in development environments without realistic resource constraints often fails to reveal issues that only appear under production load.</p>\n<p><em>Why it&#39;s wrong</em>: Memory leaks, connection pool exhaustion, and performance degradation only manifest under realistic load conditions.</p>\n<p><em>Fix</em>: Use production-like resource limits in testing environments. Implement resource monitoring and alerting to catch issues early.</p>\n<h3 id=\"debugging-techniques\">Debugging Techniques</h3>\n<p>Effective ETL debugging requires a systematic approach that combines multiple investigation techniques. The key is to gather evidence methodically rather than making assumptions about where problems might be.</p>\n<h4 id=\"log-based-investigation\">Log-Based Investigation</h4>\n<p>Pipeline debugging starts with understanding the log structure and using logs strategically to trace execution flow and identify anomalies.</p>\n<p><strong>Log Correlation Strategy</strong></p>\n<p>ETL systems generate logs from multiple components, making it challenging to trace a single pipeline run across the system. Implement correlation using the pipeline run ID and task execution ID to connect related log entries:</p>\n<ol>\n<li>Start with the pipeline-level logs to understand the overall execution state and timeline</li>\n<li>Identify which tasks failed or behaved unexpectedly based on pipeline logs</li>\n<li>Drill down to task-level logs using the task execution ID to examine detailed behavior</li>\n<li>Follow data lineage through transformation logs to understand data flow issues</li>\n<li>Correlate with system-level logs (database, message broker) using timestamps and correlation IDs</li>\n</ol>\n<p><strong>Log Analysis Patterns</strong></p>\n<table>\n<thead>\n<tr>\n<th>Log Pattern</th>\n<th>Indicates</th>\n<th>Investigation Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gaps in timestamp sequence</td>\n<td>Component hang or crash</td>\n<td>Check system logs for crash dumps; examine memory and CPU usage during gap period</td>\n</tr>\n<tr>\n<td>Repeated error messages with same context</td>\n<td>Retry loop without progress</td>\n<td>Examine retry policy configuration; check if underlying issue is being addressed</td>\n</tr>\n<tr>\n<td>Error messages without stack traces</td>\n<td>Swallowed exceptions</td>\n<td>Review error handling code; ensure proper exception logging and propagation</td>\n</tr>\n<tr>\n<td>Performance metrics showing degradation over time</td>\n<td>Resource leak or memory pressure</td>\n<td>Profile memory usage; check for unclosed connections or accumulating objects</td>\n</tr>\n<tr>\n<td>Inconsistent state between components</td>\n<td>Race condition or incomplete transaction</td>\n<td>Check transaction boundaries; examine concurrent access patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Structured Log Querying</strong></p>\n<p>Implement structured logging with consistent field names to enable efficient querying:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>timestamp=2024-01-15T10:30:45Z level=ERROR component=task-executor \npipeline_id=pipeline-123 run_id=run-456 task_id=extract-customers \nattempt=2 error_type=TIMEOUT_ERROR message=&quot;Database query timeout after 300s&quot;</code></pre></div>\n\n<p>Use log aggregation tools to query across multiple components and time ranges. Key queries for ETL debugging:</p>\n<ul>\n<li>Find all errors for a specific pipeline run: <code>run_id=run-456 AND level=ERROR</code></li>\n<li>Trace task execution timeline: <code>task_id=extract-customers ORDER BY timestamp</code></li>\n<li>Identify resource exhaustion patterns: <code>error_type=RESOURCE_EXHAUSTION last 24h</code></li>\n<li>Monitor retry patterns: <code>attempt&gt;1 GROUP BY error_type</code></li>\n</ul>\n<h4 id=\"state-inspection-and-monitoring\">State Inspection and Monitoring</h4>\n<p>Understanding current system state is crucial for diagnosing issues, especially for intermittent problems that don&#39;t leave clear log traces.</p>\n<p><strong>Database State Analysis</strong></p>\n<p>The pipeline metadata database contains the authoritative state for all executions. Use direct database queries to understand state inconsistencies:</p>\n<table>\n<thead>\n<tr>\n<th>State Query Purpose</th>\n<th>SQL Pattern</th>\n<th>Key Information</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Find stuck pipelines</td>\n<td><code>SELECT * FROM pipeline_runs WHERE state=&#39;RUNNING&#39; AND started_at &lt; NOW() - INTERVAL &#39;2 hours&#39;</code></td>\n<td>Long-running executions that may be stuck</td>\n</tr>\n<tr>\n<td>Identify retry patterns</td>\n<td><code>SELECT task_id, COUNT(*) as attempts FROM task_executions WHERE pipeline_run_id=&#39;run-456&#39; GROUP BY task_id</code></td>\n<td>Tasks requiring multiple attempts</td>\n</tr>\n<tr>\n<td>Check dependency resolution</td>\n<td><code>SELECT upstream_task, downstream_task FROM task_dependencies WHERE pipeline_id=&#39;pipeline-123&#39;</code></td>\n<td>Verify dependencies match expectations</td>\n</tr>\n<tr>\n<td>Monitor failure rates</td>\n<td><code>SELECT DATE(completed_at), state, COUNT(*) FROM pipeline_runs GROUP BY DATE(completed_at), state</code></td>\n<td>Track success/failure trends over time</td>\n</tr>\n</tbody></table>\n<p><strong>Resource Monitoring Integration</strong></p>\n<p>Correlate application metrics with system-level resource usage to identify bottlenecks:</p>\n<ul>\n<li><strong>Memory patterns</strong>: Track heap usage during transformation tasks to identify memory leaks</li>\n<li><strong>Connection pools</strong>: Monitor active/idle connections to detect pool exhaustion</li>\n<li><strong>Disk I/O</strong>: Watch for disk space consumption during large data loads</li>\n<li><strong>Network utilization</strong>: Monitor bandwidth usage during data extraction phases</li>\n<li><strong>CPU usage</strong>: Identify compute-intensive transformations that need optimization</li>\n</ul>\n<h4 id=\"interactive-debugging-techniques\">Interactive Debugging Techniques</h4>\n<p>For complex issues that require real-time investigation, interactive debugging provides deeper insights than static log analysis.</p>\n<p><strong>Pipeline Replay and Simulation</strong></p>\n<p>Implement pipeline replay functionality to reproduce issues in controlled environments:</p>\n<ol>\n<li>Capture pipeline state and input data from the failed execution</li>\n<li>Create isolated replay environment with same resource constraints</li>\n<li>Execute pipeline with additional debugging instrumentation enabled</li>\n<li>Step through execution phases to identify exact failure point</li>\n<li>Modify inputs systematically to isolate root cause</li>\n</ol>\n<p><strong>Live State Inspection</strong></p>\n<p>Provide debugging APIs that allow real-time inspection of running pipelines:</p>\n<table>\n<thead>\n<tr>\n<th>Debug API Endpoint</th>\n<th>Purpose</th>\n<th>Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>/debug/pipeline/{run_id}/state</code></td>\n<td>Current state of all tasks</td>\n<td>Check for unexpected task states or timing issues</td>\n</tr>\n<tr>\n<td><code>/debug/task/{execution_id}/metrics</code></td>\n<td>Real-time task metrics</td>\n<td>Monitor memory, CPU, and data throughput during execution</td>\n</tr>\n<tr>\n<td><code>/debug/connections/pools</code></td>\n<td>Connection pool status</td>\n<td>Identify connection leaks or pool exhaustion</td>\n</tr>\n<tr>\n<td><code>/debug/queues/depths</code></td>\n<td>Message queue depths</td>\n<td>Detect backpressure or component overload</td>\n</tr>\n</tbody></table>\n<p><strong>Component Health Checks</strong></p>\n<p>Implement comprehensive health checks that validate both basic connectivity and operational readiness:</p>\n<ul>\n<li><strong>Database connectivity</strong>: Verify read/write access with sample queries</li>\n<li><strong>External API availability</strong>: Test authentication and basic endpoint access</li>\n<li><strong>Message broker health</strong>: Confirm topic access and message flow</li>\n<li><strong>Resource availability</strong>: Check disk space, memory, and CPU capacity</li>\n<li><strong>Schema compatibility</strong>: Validate source/destination schema compatibility</li>\n</ul>\n<h3 id=\"performance-debugging\">Performance Debugging</h3>\n<p>Performance issues in ETL pipelines often manifest as gradually degrading throughput, increasing memory usage, or extended execution times. These issues require specialized debugging approaches that focus on resource utilization and algorithmic efficiency.</p>\n<h4 id=\"identifying-performance-bottlenecks\">Identifying Performance Bottlenecks</h4>\n<p>Performance debugging starts with establishing baseline metrics and identifying deviations from expected behavior patterns.</p>\n<p><strong>Throughput Analysis</strong></p>\n<p>ETL pipeline performance is primarily measured in terms of data throughput and execution time. Establish baseline metrics for comparison:</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Measurement Method</th>\n<th>Typical Issues</th>\n<th>Investigation Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Records per second processed</td>\n<td>Count input/output records divided by execution time</td>\n<td>Decreasing over time, varies between runs</td>\n<td>Check for data volume changes, schema complexity increases, resource contention</td>\n</tr>\n<tr>\n<td>Data volume throughput</td>\n<td>Bytes processed per minute during extraction/loading</td>\n<td>Network bandwidth limitations, serialization overhead</td>\n<td>Monitor network utilization, examine data compression, check serialization efficiency</td>\n</tr>\n<tr>\n<td>Task execution time</td>\n<td>Wall clock time from start to completion</td>\n<td>Individual tasks taking longer than expected</td>\n<td>Profile task internals, check for blocking operations, examine algorithm complexity</td>\n</tr>\n<tr>\n<td>End-to-end pipeline latency</td>\n<td>Time from trigger to completion</td>\n<td>Overall pipeline slowdown affecting SLAs</td>\n<td>Analyze critical path, identify parallelization opportunities, check for sequential bottlenecks</td>\n</tr>\n</tbody></table>\n<p><strong>Resource Utilization Patterns</strong></p>\n<p>Different types of performance issues create characteristic resource usage patterns:</p>\n<ul>\n<li><strong>Memory-bound operations</strong>: Steady memory growth during execution, potential out-of-memory errors</li>\n<li><strong>CPU-bound operations</strong>: High CPU utilization with low I/O, often in transformation phases</li>\n<li><strong>I/O-bound operations</strong>: High disk or network activity with lower CPU usage</li>\n<li><strong>Lock contention</strong>: Low resource usage but poor throughput due to blocking</li>\n</ul>\n<h4 id=\"memory-performance-issues\">Memory Performance Issues</h4>\n<p>Memory-related performance problems are common in ETL pipelines due to large dataset processing and transformation operations.</p>\n<p><strong>Memory Leak Detection</strong></p>\n<p>Memory leaks in ETL pipelines typically occur in transformation code, connection management, or intermediate result caching:</p>\n<ol>\n<li><strong>Monitor heap growth patterns</strong> over multiple pipeline executions to identify consistent memory increases</li>\n<li><strong>Profile object allocation</strong> during transformation phases to identify accumulating objects</li>\n<li><strong>Check connection lifecycle</strong> to ensure database connections are properly closed</li>\n<li><strong>Examine caching behavior</strong> for intermediate results that may not be evicted properly</li>\n</ol>\n<p><strong>Memory Optimization Strategies</strong></p>\n<table>\n<thead>\n<tr>\n<th>Memory Issue</th>\n<th>Symptoms</th>\n<th>Solution Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Large dataset transformations</td>\n<td>Out-of-memory during SQL operations</td>\n<td>Implement streaming processing, use temp tables for intermediate results</td>\n</tr>\n<tr>\n<td>Python UDF memory accumulation</td>\n<td>Memory usage grows within single task</td>\n<td>Add explicit garbage collection, use generator functions for large datasets</td>\n</tr>\n<tr>\n<td>Connection object accumulation</td>\n<td>Memory growth proportional to connection usage</td>\n<td>Implement proper connection pooling, ensure connections are returned to pool</td>\n</tr>\n<tr>\n<td>Intermediate result caching</td>\n<td>Memory usage doesn&#39;t decrease between pipeline runs</td>\n<td>Implement cache eviction policies, use disk-based caching for large results</td>\n</tr>\n</tbody></table>\n<h4 id=\"database-performance-debugging\">Database Performance Debugging</h4>\n<p>Database operations often become bottlenecks in ETL pipelines, especially during bulk loading and complex transformations.</p>\n<p><strong>Query Performance Analysis</strong></p>\n<p>Database performance issues require examining both query structure and execution patterns:</p>\n<ol>\n<li><strong>Capture query execution plans</strong> for all SQL operations to identify inefficient operations</li>\n<li><strong>Monitor query execution times</strong> and correlate with data volume changes</li>\n<li><strong>Check for missing indexes</strong> on frequently queried columns, especially join and filter columns</li>\n<li><strong>Analyze transaction isolation levels</strong> to prevent lock contention issues</li>\n</ol>\n<p><strong>Bulk Operation Optimization</strong></p>\n<p>ETL pipelines perform many bulk operations that require specific optimization approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Performance Consideration</th>\n<th>Optimization Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bulk inserts</td>\n<td>Insert speed vs. transaction safety</td>\n<td>Use batch inserts with optimal batch size, disable non-essential indexes during load</td>\n</tr>\n<tr>\n<td>Upsert operations</td>\n<td>Conflict detection overhead</td>\n<td>Implement efficient merge strategies, use staging tables for large upserts</td>\n</tr>\n<tr>\n<td>Large result sets</td>\n<td>Memory usage and network transfer</td>\n<td>Stream results using cursor-based pagination, implement result set compression</td>\n</tr>\n<tr>\n<td>Complex transformations</td>\n<td>CPU usage and temp space</td>\n<td>Break complex operations into smaller steps, use appropriate join algorithms</td>\n</tr>\n</tbody></table>\n<h4 id=\"network-and-io-performance\">Network and I/O Performance</h4>\n<p>ETL pipelines often move large amounts of data across network connections, making network and I/O performance critical.</p>\n<p><strong>Network Bottleneck Identification</strong></p>\n<p>Network performance issues manifest in several ways:</p>\n<ul>\n<li><strong>High latency</strong>: Individual operations take longer but overall bandwidth is acceptable</li>\n<li><strong>Low throughput</strong>: Network bandwidth is saturated, affecting overall pipeline performance</li>\n<li><strong>Intermittent failures</strong>: Network instability causing connection drops and retries</li>\n</ul>\n<p><strong>I/O Optimization Techniques</strong></p>\n<p>File and database I/O optimization focuses on minimizing the number of operations and maximizing transfer efficiency:</p>\n<ol>\n<li><strong>Use connection pooling</strong> to avoid connection setup overhead for database operations</li>\n<li><strong>Implement read-ahead buffering</strong> for sequential file operations</li>\n<li><strong>Use compression</strong> for network transfers when CPU resources allow</li>\n<li><strong>Batch operations</strong> to reduce the overhead of individual I/O calls</li>\n<li><strong>Implement parallel I/O</strong> for independent operations that can run concurrently</li>\n</ol>\n<blockquote>\n<p><strong>Performance Debugging Principle</strong>: Always measure before optimizing. Many performance &quot;improvements&quot; actually make things worse by optimizing the wrong bottleneck or introducing complexity that reduces maintainability without providing measurable benefits.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The debugging infrastructure should be built into the system from the beginning rather than added as an afterthought. This section provides concrete tools and techniques for implementing effective debugging capabilities.</p>\n<h4 id=\"technology-recommendations-for-debugging\">Technology Recommendations for Debugging</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log Aggregation</td>\n<td>File-based logging with log rotation</td>\n<td>ELK Stack (Elasticsearch, Logstash, Kibana) or cloud logging services</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Prometheus with Grafana</td>\n<td>DataDog, New Relic, or comprehensive APM solutions</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Correlation IDs in logs</td>\n<td>OpenTelemetry with Jaeger or Zipkin</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Built-in Python profiler</td>\n<td>Continuous profiling with Pyflame or py-spy</td>\n</tr>\n<tr>\n<td>Database Monitoring</td>\n<td>Query logging and pg_stat_statements</td>\n<td>Dedicated database monitoring tools</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure-for-debugging-tools\">Recommended File Structure for Debugging Tools</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  debugging/\n    profiling/\n      memory_profiler.py       Memory usage analysis\n      cpu_profiler.py          CPU performance profiling\n    monitoring/\n      health_checks.py         Component health verification\n      metrics_collector.py     Performance metrics gathering\n    log_analysis/\n      log_parser.py           Structured log parsing\n      correlation.py          Cross-component log correlation\n  tests/\n    debugging/\n      test_profiling.py       Profiling tool tests\n      test_monitoring.py      Monitoring functionality tests\n  scripts/\n    debug_pipeline.py         Interactive pipeline debugging script\n    replay_pipeline.py        Pipeline execution replay utility</code></pre></div>\n\n<h4 id=\"essential-debugging-infrastructure\">Essential Debugging Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debugging/monitoring/health_checks.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psycopg2</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheckResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SystemHealthChecker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive health checking for ETL pipeline components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, db_connection_string: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, message_broker_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db_connection_string </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db_connection_string</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message_broker_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message_broker_url</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_system_resources</span><span style=\"color:#E1E4E8\">(self) -> HealthCheckResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check basic system resource availability.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            disk </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.disk_usage(</span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cpu_percent </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.cpu_percent(</span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'memory_usage_percent'</span><span style=\"color:#E1E4E8\">: memory.percent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'disk_usage_percent'</span><span style=\"color:#E1E4E8\">: (disk.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> disk.total) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'cpu_usage_percent'</span><span style=\"color:#E1E4E8\">: cpu_percent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'available_memory_mb'</span><span style=\"color:#E1E4E8\">: memory.available </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Determine status based on thresholds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> memory.percent </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 90</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> disk.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> disk.total </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.95</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Critical resource usage: Memory </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">memory.percent</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">%, Disk </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">(disk.used</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">disk.total)</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\">:.1f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">%\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> memory.percent </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 80</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> disk.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> disk.total </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.85</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"High resource usage: Memory </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">memory.percent</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">%, Disk </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">(disk.used</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">disk.total)</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\">:.1f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">%\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"System resources normal\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheckResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                component</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"system_resources\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">status,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">message,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                metrics</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">metrics,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheckResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                component</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"system_resources\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to check system resources: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                metrics</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_database_connectivity</span><span style=\"color:#E1E4E8\">(self) -> HealthCheckResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify database connection and basic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.db_connection_string)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> conn.cursor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test basic read operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor.execute(</span><span style=\"color:#9ECBFF\">\"SELECT 1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cursor.fetchone()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test write operation to a health check table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE TABLE IF NOT EXISTS health_check_temp (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    check_time TIMESTAMP DEFAULT NOW()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor.execute(</span><span style=\"color:#9ECBFF\">\"INSERT INTO health_check_temp DEFAULT VALUES\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Clean up</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor.execute(</span><span style=\"color:#9ECBFF\">\"DELETE FROM health_check_temp WHERE check_time &#x3C; NOW() - INTERVAL '1 hour'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.commit()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            response_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#6A737D\">  # Convert to milliseconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor.close()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conn.close()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'response_time_ms'</span><span style=\"color:#E1E4E8\">: response_time}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> response_time </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 5000</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># 5 seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Database responding slowly: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">response_time</span><span style=\"color:#F97583\">:.1f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"Database healthy: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">response_time</span><span style=\"color:#F97583\">:.1f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms response\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheckResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                component</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"database\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">status,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">message,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                metrics</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">metrics,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheckResult(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                component</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"database\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Database connection failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                metrics</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'response_time_ms'</span><span style=\"color:#E1E4E8\">: (time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># debugging/log_analysis/correlation.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LogEntry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    level: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    raw_line: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LogCorrelator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Correlate logs across components for pipeline debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Regex pattern for structured log format</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.log_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.compile(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">timestamp=</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#85E89D\">?P&#x3C;timestamp></span><span style=\"color:#79B8FF\">\\S</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">level=</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#85E89D\">?P&#x3C;level></span><span style=\"color:#79B8FF\">\\S</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">component=</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#85E89D\">?P&#x3C;component></span><span style=\"color:#79B8FF\">\\S</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">(?:\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#DBEDFF\">pipeline_id=</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#85E89D\">?P&#x3C;pipeline_id></span><span style=\"color:#79B8FF\">\\S</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">))</span><span style=\"color:#F97583\">?</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">(?:\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#DBEDFF\">run_id=</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#85E89D\">?P&#x3C;run_id></span><span style=\"color:#79B8FF\">\\S</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">))</span><span style=\"color:#F97583\">?</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">(?:\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#DBEDFF\">task_id=</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#85E89D\">?P&#x3C;task_id></span><span style=\"color:#79B8FF\">\\S</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">))</span><span style=\"color:#F97583\">?</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">message=\"</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#85E89D\">?P&#x3C;message></span><span style=\"color:#79B8FF\">[</span><span style=\"color:#F97583\">^</span><span style=\"color:#79B8FF\">\"]</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#DBEDFF\">\"</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_log_entry</span><span style=\"color:#E1E4E8\">(self, log_line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[LogEntry]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse structured log entry from log line.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        match </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.log_pattern.match(log_line.strip())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> match:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.fromisoformat(match.group(</span><span style=\"color:#9ECBFF\">'timestamp'</span><span style=\"color:#E1E4E8\">).replace(</span><span style=\"color:#9ECBFF\">'Z'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'+00:00'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> LogEntry(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            level</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">match.group(</span><span style=\"color:#9ECBFF\">'level'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            component</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">match.group(</span><span style=\"color:#9ECBFF\">'component'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            pipeline_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">match.group(</span><span style=\"color:#9ECBFF\">'pipeline_id'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            run_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">match.group(</span><span style=\"color:#9ECBFF\">'run_id'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            task_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">match.group(</span><span style=\"color:#9ECBFF\">'task_id'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            message</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">match.group(</span><span style=\"color:#9ECBFF\">'message'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            raw_line</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">log_line</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> correlate_pipeline_logs</span><span style=\"color:#E1E4E8\">(self, log_entries: List[LogEntry], run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[LogEntry]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Group log entries by component for a specific pipeline run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        correlated_logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> entry </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> log_entries:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> entry.run_id </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> run_id:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entry.component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> component </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> correlated_logs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    correlated_logs[component] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                correlated_logs[component].append(entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort each component's logs by timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> component </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> correlated_logs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            correlated_logs[component].sort(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x.timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> correlated_logs</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> find_error_context</span><span style=\"color:#E1E4E8\">(self, log_entries: List[LogEntry], error_entry: LogEntry, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          context_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">) -> List[LogEntry]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find log entries around an error for context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> error_entry.timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        context_logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> entry </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> log_entries:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Same run and within time window</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> (entry.run_id </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> error_entry.run_id </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                abs</span><span style=\"color:#E1E4E8\">((entry.timestamp </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> error_time).total_seconds()) </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> context_seconds):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                context_logs.append(entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(context_logs, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x.timestamp)</span></span></code></pre></div>\n\n<h4 id=\"core-debugging-tools-implementation\">Core Debugging Tools Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debugging/profiling/performance_analyzer.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict, deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceAnalyzer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Real-time performance monitoring and analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, sample_interval_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sample_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sample_interval_seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(deque)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_monitoring</span><span style=\"color:#E1E4E8\">(self, pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Begin performance monitoring for a pipeline run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_run_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pipeline_run_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._monitoring_loop,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"perf-monitor-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">pipeline_run_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor_thread.daemon </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitor_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop_monitoring</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop monitoring and return collected samples.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.monitor_thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.monitor_thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert deques to lists for JSON serialization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> metric_name, samples </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.samples.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            results[metric_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(samples)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Clear samples for next run</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.samples.clear()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _monitoring_loop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Background thread that collects performance samples.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.monitoring:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Collect system metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cpu_percent </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.cpu_percent()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Store samples with timestamp</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.samples[</span><span style=\"color:#9ECBFF\">'timestamp'</span><span style=\"color:#E1E4E8\">].append(timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.samples[</span><span style=\"color:#9ECBFF\">'memory_usage_mb'</span><span style=\"color:#E1E4E8\">].append(memory.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.samples[</span><span style=\"color:#9ECBFF\">'memory_percent'</span><span style=\"color:#E1E4E8\">].append(memory.percent)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.samples[</span><span style=\"color:#9ECBFF\">'cpu_percent'</span><span style=\"color:#E1E4E8\">].append(cpu_percent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Collect process-specific metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                process </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                process_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> process.memory_info()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.samples[</span><span style=\"color:#9ECBFF\">'process_memory_mb'</span><span style=\"color:#E1E4E8\">].append(process_memory.rss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.samples[</span><span style=\"color:#9ECBFF\">'process_cpu_percent'</span><span style=\"color:#E1E4E8\">].append(process.cpu_percent())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> psutil.Error:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.sample_interval)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># scripts/debug_pipeline.py</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">#!/usr/bin/env python3</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Interactive pipeline debugging utility.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> argparse</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> debugging.monitoring.health_checks </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> SystemHealthChecker, HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> debugging.log_analysis.correlation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> LogCorrelator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> debugging.profiling.performance_analyzer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PerformanceAnalyzer</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> debug_pipeline_execution</span><span style=\"color:#E1E4E8\">(pipeline_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, log_file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Debug a specific pipeline execution using multiple analysis techniques.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Debugging pipeline </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">pipeline_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, run </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">run_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. System health check</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">1. SYSTEM HEALTH CHECK\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"-\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    health_checker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SystemHealthChecker(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        db_connection_string</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"postgresql://user:pass@localhost/etl_db\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        message_broker_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"amqp://localhost:5672\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_health </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> health_checker.check_system_resources()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db_health </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> health_checker.check_database_connectivity()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"System Resources: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">resource_health.status.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">resource_health.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Database: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">db_health.status.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">db_health.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resource_health.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">  CRITICAL: System resources are unhealthy. This may be causing pipeline issues.\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. Log analysis</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">2. LOG ANALYSIS\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"-\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LogCorrelator()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_entries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Parse log file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(log_file_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> line </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> correlator.parse_log_entry(line)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> entry:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    log_entries.append(entry)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Parsed </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(log_entries)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> log entries\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Find logs for this pipeline run</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        run_logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> correlator.correlate_pipeline_logs(log_entries, run_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Found logs from </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(run_logs)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> components for run </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">run_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Look for errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [entry </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> entry </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> log_entries </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                     if</span><span style=\"color:#E1E4E8\"> entry.run_id </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> run_id </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> entry.level </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'ERROR'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> error_logs:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">  Found </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(error_logs)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> error entries:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> error </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> error_logs:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error.timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> [</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error.component</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">error.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Get context around each error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> correlator.find_error_context(log_entries, error, </span><span style=\"color:#FFAB70\">context_seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"    Context (</span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(context)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> entries around error):\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> ctx_entry </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> context[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">:]:  </span><span style=\"color:#6A737D\"># Show last 5 context entries</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"      </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">ctx_entry.timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> [</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">ctx_entry.component</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">ctx_entry.level</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">ctx_entry.message</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\" No errors found in logs\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> FileNotFoundError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Log file not found: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">log_file_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"  Error parsing logs: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. Performance analysis suggestions</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">3. PERFORMANCE ANALYSIS SUGGESTIONS\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"-\"</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"To analyze performance issues:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"1. Enable performance monitoring for the next pipeline run:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"   analyzer = PerformanceAnalyzer()\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"   analyzer.start_monitoring('</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">run_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">')\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"   # Run pipeline\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"   results = analyzer.stop_monitoring()\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">2. Check database query performance:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"   - Enable query logging in database configuration\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"   - Look for slow queries in database logs\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"   - Check for missing indexes on large tables\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">3. Monitor resource usage patterns:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"   - Memory growth during transformation phases\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"   - CPU spikes during data processing\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"   - Network usage during extraction/loading\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> argparse.ArgumentParser(</span><span style=\"color:#FFAB70\">description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Debug ETL pipeline execution\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"--pipeline-id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">required</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Pipeline ID to debug\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"--run-id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">required</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Pipeline run ID to debug\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"--log-file\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">required</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Path to log file\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse_args()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug_pipeline_execution(args.pipeline_id, args.run_id, args.log_file)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    main()</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints-for-debugging-implementation\">Milestone Checkpoints for Debugging Implementation</h4>\n<p><strong>After implementing health checking:</strong></p>\n<ul>\n<li>Run: <code>python -m debugging.monitoring.health_checks</code></li>\n<li>Expected: Health status reports for system resources and database connectivity</li>\n<li>Verify: Health checks correctly identify resource pressure and connection issues</li>\n</ul>\n<p><strong>After implementing log correlation:</strong></p>\n<ul>\n<li>Run: <code>python scripts/debug_pipeline.py --pipeline-id test-pipeline --run-id run-123 --log-file pipeline.log</code></li>\n<li>Expected: Parsed log entries grouped by component with error context</li>\n<li>Verify: Error messages are correctly correlated with surrounding context</li>\n</ul>\n<p><strong>After implementing performance monitoring:</strong></p>\n<ul>\n<li>Run a pipeline with performance monitoring enabled</li>\n<li>Expected: Real-time metrics collection during execution</li>\n<li>Verify: Memory and CPU usage patterns are captured and can identify bottlenecks</li>\n</ul>\n<h4 id=\"common-debugging-issues-and-solutions\">Common Debugging Issues and Solutions</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Command</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Debug script fails to connect to database</td>\n<td>Connection string incorrect or database unavailable</td>\n<td><code>psql -d &quot;postgresql://user:pass@localhost/etl_db&quot; -c &quot;SELECT 1&quot;</code></td>\n<td>Update connection string, verify database is running</td>\n</tr>\n<tr>\n<td>Log parsing returns empty results</td>\n<td>Log format doesn&#39;t match expected structure</td>\n<td>Check log file manually, verify timestamp format matches regex</td>\n<td>Update log pattern regex or fix log format</td>\n</tr>\n<tr>\n<td>Performance monitoring shows no data</td>\n<td>Background thread not starting or crashing</td>\n<td>Add exception handling and logging to monitoring loop</td>\n<td>Fix threading issues, handle psutil exceptions</td>\n</tr>\n<tr>\n<td>Health checks always report unhealthy</td>\n<td>Thresholds too strict for environment</td>\n<td>Adjust resource usage thresholds in health check logic</td>\n<td>Tune thresholds based on system capacity</td>\n</tr>\n</tbody></table>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - potential enhancements that build upon pipeline definition (Milestone 1), data processing capabilities (Milestones 2-3), and orchestration infrastructure (Milestone 4) to enable advanced use cases and cloud-scale deployment.</p>\n</blockquote>\n<h3 id=\"mental-model-growing-city-infrastructure\">Mental Model: Growing City Infrastructure</h3>\n<p>Think of our ETL system like a small city that&#39;s planning for growth. Right now we&#39;ve built the essential infrastructure - roads (data pipelines), traffic lights (orchestration), utilities (data processing), and a city hall (monitoring). But as the city grows, we&#39;ll need highways for high-speed traffic (distributed processing), airports for long-distance connections (cloud integration), smart traffic systems (real-time processing), and specialized districts (machine learning workflows). The key is designing today&#39;s infrastructure so it can evolve into tomorrow&#39;s metropolis without requiring a complete rebuild.</p>\n<p>The current ETL system provides a solid foundation with its DAG-based pipeline definition, pluggable connector architecture, transformation engine, and orchestration framework. However, production environments often demand capabilities beyond traditional batch processing - from handling massive data volumes that require distributed computing, to processing streaming data in real-time, to supporting machine learning workflows with specialized requirements. This section explores how the existing architecture can be extended to support these advanced scenarios while maintaining the system&#39;s core principles of reliability, observability, and ease of use.</p>\n<p>The extensions fall into two main categories: scalability improvements that help the system handle larger workloads and more complex deployment scenarios, and advanced pipeline features that enable new types of data processing workflows. Each extension is designed to build incrementally on the existing foundation, allowing teams to adopt only the capabilities they need while maintaining backward compatibility with existing pipelines.</p>\n<h3 id=\"scalability-extensions\">Scalability Extensions</h3>\n<h4 id=\"distributed-task-execution\">Distributed Task Execution</h4>\n<p>The current system executes tasks on a single machine using thread-based parallelization within each execution level of the DAG. While this approach works well for moderate workloads, large-scale data processing often requires distributing task execution across multiple machines to handle datasets that exceed single-machine memory and CPU capacity, or to achieve processing speeds that require parallel computation.</p>\n<blockquote>\n<p><strong>Decision: Distributed Execution Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Single-machine execution limits throughput and dataset size, preventing the system from handling enterprise-scale workloads that may involve terabytes of data or complex transformations requiring substantial computational resources.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Container-based task execution with Kubernetes orchestration</li>\n<li>Spark-based distributed processing integration</li>\n<li>Custom distributed worker pool with message queues</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach using containerized task execution for general tasks with optional Spark integration for data-intensive transformations</li>\n<li><strong>Rationale</strong>: Containers provide isolation and resource control for arbitrary tasks, while Spark integration leverages existing distributed processing expertise for heavy data workloads</li>\n<li><strong>Consequences</strong>: Enables horizontal scaling and large dataset processing, but requires container orchestration infrastructure and adds deployment complexity</li>\n</ul>\n</blockquote>\n<p>The distributed execution model extends the existing <code>TaskExecution</code> framework by introducing execution targets that can be either local threads or remote execution environments. The <code>ExecutionPlan</code> component gains new capabilities to consider resource requirements and execution target availability when scheduling tasks across the cluster.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Distribution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Distributed Task Scheduler</strong></td>\n<td>Assigns tasks to available execution targets based on resource requirements</td>\n<td>Round-robin with resource consideration</td>\n</tr>\n<tr>\n<td><strong>Container Task Executor</strong></td>\n<td>Executes tasks in isolated containers with configurable resource limits</td>\n<td>One container per task execution</td>\n</tr>\n<tr>\n<td><strong>Spark Integration Layer</strong></td>\n<td>Submits data-intensive transformations to Spark clusters</td>\n<td>Spark job per transformation task</td>\n</tr>\n<tr>\n<td><strong>Resource Manager</strong></td>\n<td>Tracks available compute resources across execution targets</td>\n<td>Periodic heartbeat with capacity reporting</td>\n</tr>\n<tr>\n<td><strong>Result Collector</strong></td>\n<td>Aggregates task results from distributed executions</td>\n<td>Streaming collection with partial results</td>\n</tr>\n</tbody></table>\n<p>The distributed execution workflow follows these steps:</p>\n<ol>\n<li>The <code>ExecutionPlan</code> analyzer examines each task&#39;s resource requirements and estimated data volume to determine optimal execution target</li>\n<li>Tasks requiring large memory or CPU are marked for container execution, while data transformations exceeding size thresholds are routed to Spark</li>\n<li>The Distributed Task Scheduler maintains a registry of available execution targets with their current resource utilization</li>\n<li>When a task becomes ready for execution, the scheduler selects an appropriate target and packages the task with its dependencies</li>\n<li>Container-based tasks are submitted to Kubernetes with resource limits and environment variables containing connection credentials</li>\n<li>Spark-based tasks are converted to Spark jobs with optimized partitioning strategies based on input data characteristics</li>\n<li>The Result Collector streams task outputs back to the central orchestrator and updates task state in the shared metadata store</li>\n<li>Failed tasks can be rescheduled on different execution targets, with automatic retry logic adapted for distributed failure scenarios</li>\n</ol>\n<p>Container task execution requires packaging each task&#39;s execution environment, including the necessary connector libraries, transformation functions, and configuration data. The system creates lightweight container images containing the ETL runtime and mounts task-specific configuration and credentials at execution time.</p>\n<table>\n<thead>\n<tr>\n<th>Container Component</th>\n<th>Purpose</th>\n<th>Configuration Source</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>ETL Runtime Base Image</strong></td>\n<td>Contains Python environment and core ETL libraries</td>\n<td>Pre-built and versioned container registry</td>\n</tr>\n<tr>\n<td><strong>Task Configuration Mount</strong></td>\n<td>Task definition, parameters, and connection configs</td>\n<td>ConfigMap generated from TaskDefinition</td>\n</tr>\n<tr>\n<td><strong>Credential Mount</strong></td>\n<td>Database passwords, API keys, and certificates</td>\n<td>Kubernetes secrets with rotation support</td>\n</tr>\n<tr>\n<td><strong>Shared Storage Mount</strong></td>\n<td>Large intermediate datasets and checkpoint files</td>\n<td>Persistent volume or object storage mount</td>\n</tr>\n<tr>\n<td><strong>Log Collection Sidecar</strong></td>\n<td>Streams task logs back to central monitoring</td>\n<td>Fluent Bit or similar log shipping agent</td>\n</tr>\n</tbody></table>\n<p>Spark integration focuses on data transformations that benefit from distributed processing, particularly large joins, aggregations, and complex analytical workloads. The integration layer translates SQL-based transformations into Spark SQL jobs and provides a framework for registering Python UDFs as Spark user-defined functions.</p>\n<h4 id=\"horizontal-scaling-and-auto-scaling\">Horizontal Scaling and Auto-scaling</h4>\n<p>The current orchestration system runs as a single process managing pipeline scheduling and execution coordination. Production deployments require horizontal scaling to handle increased pipeline throughput, support high availability during component failures, and automatically adjust capacity based on workload demand.</p>\n<p>The horizontal scaling model introduces multiple orchestrator instances that coordinate through a shared metadata store and message broker. Each instance can handle pipeline scheduling, task execution coordination, and monitoring responsibilities, with automatic failover when instances become unavailable.</p>\n<table>\n<thead>\n<tr>\n<th>Scaling Component</th>\n<th>Single Instance Role</th>\n<th>Multi-Instance Coordination</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Pipeline Scheduler</strong></td>\n<td>Triggers pipelines based on schedules</td>\n<td>Leader election with schedule ownership</td>\n</tr>\n<tr>\n<td><strong>Task Execution Coordinator</strong></td>\n<td>Manages task state and dependencies</td>\n<td>Distributed coordination via message queue</td>\n</tr>\n<tr>\n<td><strong>Monitoring Collector</strong></td>\n<td>Aggregates metrics and logs</td>\n<td>Partition-based collection with merge</td>\n</tr>\n<tr>\n<td><strong>Metadata Store Access</strong></td>\n<td>Direct database connections</td>\n<td>Connection pooling with read replicas</td>\n</tr>\n<tr>\n<td><strong>Message Broker Client</strong></td>\n<td>Simple pub/sub for events</td>\n<td>Consumer groups with partition assignment</td>\n</tr>\n</tbody></table>\n<p>Leader election ensures that only one orchestrator instance schedules each pipeline to prevent duplicate executions, while multiple instances can coordinate task execution within the same pipeline run. The system uses etcd or a similar consensus system for leader election, with lease renewal and automatic failover when the current leader becomes unavailable.</p>\n<p>Auto-scaling responds to increased workload by monitoring key metrics and adjusting the number of orchestrator instances and execution targets. The auto-scaling controller tracks pipeline queue length, average task execution time, and resource utilization to make scaling decisions.</p>\n<table>\n<thead>\n<tr>\n<th>Scaling Trigger</th>\n<th>Scale Up Condition</th>\n<th>Scale Down Condition</th>\n<th>Scaling Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Pipeline Queue Length</strong></td>\n<td>&gt;50 queued pipelines</td>\n<td>&lt;10 queued pipelines</td>\n<td>Add/remove orchestrator instances</td>\n</tr>\n<tr>\n<td><strong>Task Execution Wait Time</strong></td>\n<td>&gt;5 minutes average wait</td>\n<td>&lt;1 minute average wait</td>\n<td>Add/remove execution workers</td>\n</tr>\n<tr>\n<td><strong>Resource Utilization</strong></td>\n<td>&gt;80% CPU/memory usage</td>\n<td>&lt;30% CPU/memory usage</td>\n<td>Adjust container resource limits</td>\n</tr>\n<tr>\n<td><strong>Error Rate Spike</strong></td>\n<td>&gt;10% task failure rate</td>\n<td>Normal failure rates</td>\n<td>Temporarily reduce concurrency</td>\n</tr>\n</tbody></table>\n<p>The auto-scaling system includes safeguards to prevent rapid scaling oscillation and considers the cost implications of adding resources. Scale-down decisions include grace periods to allow in-progress tasks to complete and evaluate recent scaling actions to avoid immediate reversals.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: Auto-scaling decisions must account for the stateful nature of ETL pipelines, where scaling down may interrupt long-running transformations or cause data inconsistency if not coordinated properly with task execution state.</p>\n</blockquote>\n<h4 id=\"cloud-native-integration\">Cloud-Native Integration</h4>\n<p>Modern ETL deployments increasingly leverage cloud services for storage, compute, and managed infrastructure services. The cloud-native extensions integrate with cloud provider APIs to dynamically provision resources, leverage managed services for data storage and processing, and implement cloud-specific optimization patterns.</p>\n<p>The cloud integration layer provides abstractions over common cloud services while maintaining the ability to deploy in on-premises or hybrid environments. The system detects its deployment environment and automatically configures appropriate service integrations.</p>\n<table>\n<thead>\n<tr>\n<th>Cloud Service Category</th>\n<th>AWS Integration</th>\n<th>GCP Integration</th>\n<th>Azure Integration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Object Storage</strong></td>\n<td>S3 with IAM roles</td>\n<td>Cloud Storage with service accounts</td>\n<td>Blob Storage with managed identity</td>\n</tr>\n<tr>\n<td><strong>Managed Databases</strong></td>\n<td>RDS, Redshift connections</td>\n<td>Cloud SQL, BigQuery connectors</td>\n<td>SQL Database, Synapse Analytics</td>\n</tr>\n<tr>\n<td><strong>Container Orchestration</strong></td>\n<td>EKS with Fargate support</td>\n<td>GKE with Autopilot</td>\n<td>AKS with virtual nodes</td>\n</tr>\n<tr>\n<td><strong>Serverless Compute</strong></td>\n<td>Lambda for lightweight tasks</td>\n<td>Cloud Functions integration</td>\n<td>Azure Functions support</td>\n</tr>\n<tr>\n<td><strong>Message Queues</strong></td>\n<td>SQS/SNS for coordination</td>\n<td>Pub/Sub for event streaming</td>\n<td>Service Bus for messaging</td>\n</tr>\n<tr>\n<td><strong>Monitoring Integration</strong></td>\n<td>CloudWatch metrics/logs</td>\n<td>Cloud Monitoring/Logging</td>\n<td>Azure Monitor integration</td>\n</tr>\n</tbody></table>\n<p>Cloud-native deployment leverages Infrastructure as Code (IaC) tools to provision and configure the required cloud resources. The system includes Terraform modules and Kubernetes Helm charts that can deploy the complete ETL infrastructure with appropriate security configurations, network policies, and monitoring integrations.</p>\n<p>Serverless integration allows lightweight tasks to run in cloud functions rather than requiring persistent compute resources. Tasks suitable for serverless execution include data validation, simple transformations, and notification delivery. The system automatically identifies serverless-compatible tasks based on resource requirements and execution patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Serverless Criteria</th>\n<th>Eligible Task Types</th>\n<th>Execution Environment</th>\n<th>Limitations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Memory &lt; 1GB</strong></td>\n<td>Validation, notification tasks</td>\n<td>Lambda, Cloud Functions</td>\n<td>15-minute maximum execution time</td>\n</tr>\n<tr>\n<td><strong>No persistent state</strong></td>\n<td>Stateless transformations</td>\n<td>Function runtime with mounted storage</td>\n<td>No local file system persistence</td>\n</tr>\n<tr>\n<td><strong>Predictable runtime</strong></td>\n<td>Simple data operations</td>\n<td>Auto-scaling function instances</td>\n<td>Cold start latency considerations</td>\n</tr>\n<tr>\n<td><strong>Standard dependencies</strong></td>\n<td>Tasks using built-in libraries</td>\n<td>Pre-packaged runtime environment</td>\n<td>Limited custom library support</td>\n</tr>\n</tbody></table>\n<p>Cloud storage integration optimizes data transfer by leveraging cloud-native features like transfer acceleration, regional storage, and intelligent tiering. Large datasets are processed using cloud-specific optimization patterns, such as S3 Transfer Acceleration for fast uploads and BigQuery&#39;s columnar storage for analytical workloads.</p>\n<p> <strong>Pitfall: Cloud Vendor Lock-in</strong>\nDirectly using cloud-specific APIs throughout the codebase creates tight coupling that makes migration difficult. The abstraction layer must provide cloud-agnostic interfaces while still allowing access to cloud-specific optimizations when needed. This requires careful interface design that balances portability with performance.</p>\n<h3 id=\"advanced-pipeline-features\">Advanced Pipeline Features</h3>\n<h4 id=\"stream-processing-integration\">Stream Processing Integration</h4>\n<p>Traditional ETL systems focus on batch processing, where data is processed in discrete chunks at scheduled intervals. However, many modern use cases require real-time or near-real-time processing of continuously arriving data streams. Stream processing integration extends the DAG-based pipeline model to support continuous data flows while maintaining the existing batch processing capabilities.</p>\n<blockquote>\n<p><strong>Decision: Hybrid Stream-Batch Processing Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Organizations need both batch processing for historical data and stream processing for real-time analytics, requiring a unified system that can handle both paradigms without forcing users to maintain separate infrastructures.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Pure stream processing with micro-batching for historical data</li>\n<li>Separate stream and batch systems with shared metadata</li>\n<li>Unified DAG model supporting both stream and batch tasks</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Extended DAG model with stream-aware task types and unified orchestration</li>\n<li><strong>Rationale</strong>: Maintains familiar DAG abstraction while enabling stream processing, allows gradual migration from batch to stream, and provides unified monitoring and debugging experience</li>\n<li><strong>Consequences</strong>: Enables real-time use cases with consistent tooling, but adds complexity to scheduling and state management for continuous processing</li>\n</ul>\n</blockquote>\n<p>The stream processing model introduces new task types that operate on continuous data streams rather than discrete datasets. Stream tasks maintain persistent execution state and process data as it arrives, while batch tasks continue to operate on complete datasets at scheduled intervals.</p>\n<table>\n<thead>\n<tr>\n<th>Task Type</th>\n<th>Data Model</th>\n<th>Execution Pattern</th>\n<th>State Management</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Stream Source</strong></td>\n<td>Continuous event stream</td>\n<td>Always running with checkpoint recovery</td>\n<td>Maintains consumer offset/position</td>\n</tr>\n<tr>\n<td><strong>Stream Transform</strong></td>\n<td>Input stream  Output stream</td>\n<td>Stateful processing with windowing</td>\n<td>Checkpoint-based state persistence</td>\n</tr>\n<tr>\n<td><strong>Stream Sink</strong></td>\n<td>Stream  External system</td>\n<td>Micro-batch writes with exactly-once semantics</td>\n<td>Idempotent write tracking</td>\n</tr>\n<tr>\n<td><strong>Batch Task</strong></td>\n<td>Complete dataset</td>\n<td>Scheduled execution</td>\n<td>Traditional task state machine</td>\n</tr>\n<tr>\n<td><strong>Hybrid Task</strong></td>\n<td>Stream + Batch inputs</td>\n<td>Triggered on schedule or stream events</td>\n<td>Mixed state with clear boundaries</td>\n</tr>\n</tbody></table>\n<p>Stream processing requires modified dependency semantics where stream tasks can have dependencies on both other stream tasks (for streaming transformations) and batch tasks (for enrichment data). The DAG validation extends to verify that stream dependencies form valid topologies and that batch dependencies provide appropriate data freshness guarantees.</p>\n<p>The stream processing workflow follows these principles:</p>\n<ol>\n<li>Stream source tasks connect to message queues, event streams, or change data capture systems and continuously consume new events</li>\n<li>Each consumed event or micro-batch triggers downstream stream transformations following the DAG dependency structure</li>\n<li>Stream transforms maintain processing state (windows, aggregations, join state) with regular checkpointing to enable failure recovery</li>\n<li>Stream sinks buffer output events and write to destinations using configurable batching strategies for efficiency</li>\n<li>Hybrid tasks combine stream data with batch-processed reference data, triggering reprocessing when either input changes</li>\n<li>The orchestrator monitors stream task health and restarts failed stream tasks from the last successful checkpoint</li>\n<li>Stream processing metrics track throughput, latency, and backlog to enable monitoring and auto-scaling decisions</li>\n</ol>\n<p>Stream task checkpointing ensures exactly-once processing semantics by atomically saving processing state along with output records. The checkpoint mechanism coordinates with downstream tasks to maintain consistency across the entire stream processing pipeline.</p>\n<table>\n<thead>\n<tr>\n<th>Checkpoint Component</th>\n<th>Purpose</th>\n<th>Persistence Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Consumer Offset</strong></td>\n<td>Tracks position in input stream</td>\n<td>Stored in message broker or external store</td>\n</tr>\n<tr>\n<td><strong>Processing State</strong></td>\n<td>Window contents, aggregation values</td>\n<td>Serialized to persistent storage with versioning</td>\n</tr>\n<tr>\n<td><strong>Output Tracking</strong></td>\n<td>Records successfully written downstream</td>\n<td>Deduplicated output log with retention policy</td>\n</tr>\n<tr>\n<td><strong>Watermark Position</strong></td>\n<td>Event time progress for windowed operations</td>\n<td>Coordinated across parallel processing instances</td>\n</tr>\n</tbody></table>\n<h4 id=\"machine-learning-pipeline-integration\">Machine Learning Pipeline Integration</h4>\n<p>Data pipelines increasingly serve machine learning workloads that have specialized requirements around model training, feature engineering, model deployment, and inference serving. The ML integration extends the transformation engine to support ML-specific operations while leveraging existing orchestration and monitoring infrastructure.</p>\n<p>Machine learning workflows typically involve feature extraction from raw data, model training on prepared datasets, model validation and testing, and deployment to serving infrastructure. These workflows have unique characteristics including long-running training jobs, iterative experimentation, model versioning, and performance monitoring that differ from traditional ETL operations.</p>\n<p>The ML pipeline extension introduces specialized task types that integrate with popular ML frameworks and provide abstractions for common ML operations. These tasks can be combined with traditional ETL tasks in the same DAG to create end-to-end ML pipelines.</p>\n<table>\n<thead>\n<tr>\n<th>ML Task Type</th>\n<th>Purpose</th>\n<th>Framework Integration</th>\n<th>Output Artifacts</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Feature Engineering</strong></td>\n<td>Transform raw data to ML features</td>\n<td>Pandas, Spark MLlib</td>\n<td>Feature datasets with schema</td>\n</tr>\n<tr>\n<td><strong>Model Training</strong></td>\n<td>Train ML models on prepared data</td>\n<td>Scikit-learn, TensorFlow, PyTorch</td>\n<td>Versioned model artifacts</td>\n</tr>\n<tr>\n<td><strong>Model Evaluation</strong></td>\n<td>Validate model performance</td>\n<td>MLflow, Weights &amp; Biases</td>\n<td>Performance metrics and reports</td>\n</tr>\n<tr>\n<td><strong>Model Deployment</strong></td>\n<td>Deploy models to serving infrastructure</td>\n<td>KubeFlow, SageMaker, MLflow</td>\n<td>Deployed model endpoints</td>\n</tr>\n<tr>\n<td><strong>Batch Inference</strong></td>\n<td>Score large datasets with trained models</td>\n<td>Spark, Dask for distributed scoring</td>\n<td>Scored datasets with predictions</td>\n</tr>\n<tr>\n<td><strong>Model Monitoring</strong></td>\n<td>Track model performance in production</td>\n<td>Custom monitoring with alerting</td>\n<td>Drift detection and performance metrics</td>\n</tr>\n</tbody></table>\n<p>Feature engineering tasks extend the transformation engine with ML-specific operations like feature scaling, encoding categorical variables, handling missing values, and creating time-based features. These transformations maintain lineage tracking to enable feature attribution and debugging model performance issues.</p>\n<p>Model training tasks handle the specialized requirements of ML training including experiment tracking, hyperparameter optimization, cross-validation, and distributed training for large models. The training integration provides abstractions that work across different ML frameworks while maintaining flexibility for framework-specific optimizations.</p>\n<table>\n<thead>\n<tr>\n<th>Training Feature</th>\n<th>Implementation</th>\n<th>Framework Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Experiment Tracking</strong></td>\n<td>MLflow integration for logging</td>\n<td>All major frameworks via MLflow</td>\n</tr>\n<tr>\n<td><strong>Hyperparameter Tuning</strong></td>\n<td>Optuna-based optimization</td>\n<td>Framework-agnostic optimization</td>\n</tr>\n<tr>\n<td><strong>Distributed Training</strong></td>\n<td>Multi-GPU and multi-node support</td>\n<td>TensorFlow distributed, PyTorch DDP</td>\n</tr>\n<tr>\n<td><strong>Model Versioning</strong></td>\n<td>Git-like versioning for model artifacts</td>\n<td>MLflow Model Registry integration</td>\n</tr>\n<tr>\n<td><strong>Resource Management</strong></td>\n<td>GPU allocation and scheduling</td>\n<td>Kubernetes resource quotas</td>\n</tr>\n</tbody></table>\n<p>Model deployment tasks automate the process of taking trained models and making them available for inference, whether through batch scoring jobs or real-time serving endpoints. The deployment integration handles model packaging, dependency management, and infrastructure provisioning.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: ML pipelines often require iterative development where data scientists experiment with different feature engineering and modeling approaches. The pipeline system must support branching and merging of experimental workflows while maintaining reproducibility and version control of successful experiments.</p>\n</blockquote>\n<p>The ML integration includes specialized monitoring for deployed models that tracks prediction accuracy, data drift, and performance degradation over time. This monitoring can trigger automatic retraining pipelines when model performance drops below acceptable thresholds.</p>\n<h4 id=\"real-time-processing-and-event-driven-orchestration\">Real-time Processing and Event-driven Orchestration</h4>\n<p>Beyond stream processing for continuous data flows, real-time processing enables sub-second response times for critical business processes and event-driven orchestration that responds immediately to external events rather than relying solely on schedule-based triggers.</p>\n<p>Real-time processing requirements include low-latency data transformation, immediate alerting on anomaly detection, real-time feature serving for ML models, and rapid response to business events. These use cases require processing architectures optimized for latency rather than throughput, with careful attention to resource allocation and prioritization.</p>\n<p>Event-driven orchestration extends the scheduling system to trigger pipelines based on external events such as file arrivals, database changes, API calls, or message queue events. This enables reactive data processing that responds to business events as they occur rather than waiting for the next scheduled batch window.</p>\n<table>\n<thead>\n<tr>\n<th>Event Source</th>\n<th>Trigger Mechanism</th>\n<th>Latency Target</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>File System Events</strong></td>\n<td>inotify, S3 events</td>\n<td>&lt;1 second</td>\n<td>Process files immediately upon arrival</td>\n</tr>\n<tr>\n<td><strong>Database Changes</strong></td>\n<td>CDC, triggers</td>\n<td>&lt;5 seconds</td>\n<td>Sync data changes across systems</td>\n</tr>\n<tr>\n<td><strong>API Webhooks</strong></td>\n<td>HTTP endpoints</td>\n<td>&lt;500ms</td>\n<td>React to external system notifications</td>\n</tr>\n<tr>\n<td><strong>Message Queues</strong></td>\n<td>Real-time consumers</td>\n<td>&lt;100ms</td>\n<td>Process high-priority business events</td>\n</tr>\n<tr>\n<td><strong>Monitoring Alerts</strong></td>\n<td>Alert manager integration</td>\n<td>&lt;30 seconds</td>\n<td>Trigger remediation pipelines</td>\n</tr>\n</tbody></table>\n<p>Real-time processing tasks use in-memory data structures and optimized execution paths to minimize latency. The system provides priority queues for real-time tasks and resource reservation to ensure adequate capacity for time-sensitive processing.</p>\n<p>The event-driven architecture includes event routing and filtering capabilities that allow pipelines to subscribe to specific types of events and apply filters to process only relevant events. This prevents overwhelming the system with low-priority events while ensuring critical events receive immediate attention.</p>\n<table>\n<thead>\n<tr>\n<th>Event Processing Stage</th>\n<th>Purpose</th>\n<th>Performance Optimization</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Event Ingestion</strong></td>\n<td>Receive and queue incoming events</td>\n<td>High-throughput async I/O</td>\n</tr>\n<tr>\n<td><strong>Event Filtering</strong></td>\n<td>Apply subscription and filter rules</td>\n<td>In-memory rule evaluation</td>\n</tr>\n<tr>\n<td><strong>Event Routing</strong></td>\n<td>Direct events to appropriate pipelines</td>\n<td>Hash-based partition assignment</td>\n</tr>\n<tr>\n<td><strong>Priority Scheduling</strong></td>\n<td>Schedule real-time tasks with priority</td>\n<td>Dedicated resource pools</td>\n</tr>\n<tr>\n<td><strong>Low-latency Execution</strong></td>\n<td>Execute time-sensitive transformations</td>\n<td>Pre-warmed execution environments</td>\n</tr>\n</tbody></table>\n<p>Real-time pipeline orchestration includes circuit breakers and bulkhead patterns to prevent cascading failures when external systems become slow or unavailable. The system can automatically route processing to backup systems or degrade functionality gracefully when real-time processing targets cannot be met.</p>\n<p> <strong>Pitfall: Real-time Complexity Trade-offs</strong>\nOptimizing for real-time processing often conflicts with other system qualities like reliability, consistency, and cost-effectiveness. Real-time capabilities should be applied selectively to use cases that truly require low latency, while maintaining robust batch processing for operations where eventual consistency is acceptable. Over-engineering for real-time requirements can significantly increase system complexity and operational overhead.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Container Orchestration</strong></td>\n<td>Docker Compose for development</td>\n<td>Kubernetes with Helm charts</td>\n</tr>\n<tr>\n<td><strong>Distributed Computing</strong></td>\n<td>Celery with Redis backend</td>\n<td>Apache Spark on Kubernetes</td>\n</tr>\n<tr>\n<td><strong>Message Broker</strong></td>\n<td>Redis Pub/Sub</td>\n<td>Apache Kafka with Schema Registry</td>\n</tr>\n<tr>\n<td><strong>Stream Processing</strong></td>\n<td>Python asyncio with Redis Streams</td>\n<td>Apache Kafka Streams or Apache Flink</td>\n</tr>\n<tr>\n<td><strong>Cloud Integration</strong></td>\n<td>boto3 for AWS, basic cloud APIs</td>\n<td>Terraform + cloud SDKs with IAM roles</td>\n</tr>\n<tr>\n<td><strong>ML Framework</strong></td>\n<td>Scikit-learn with joblib</td>\n<td>MLflow + Kubeflow for enterprise ML</td>\n</tr>\n<tr>\n<td><strong>Real-time Processing</strong></td>\n<td>WebSockets + in-memory queues</td>\n<td>Apache Pulsar with function computing</td>\n</tr>\n<tr>\n<td><strong>Auto-scaling</strong></td>\n<td>Simple threshold-based scaling</td>\n<td>Kubernetes HPA with custom metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure-extension\">Recommended File Structure Extension</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  etl_system/\n    extensions/\n      distributed/\n        __init__.py\n        container_executor.py       Container-based task execution\n        spark_integration.py        Spark job submission and monitoring\n        resource_manager.py         Cluster resource tracking\n        distributed_scheduler.py    Multi-target task scheduling\n      \n      scaling/\n        __init__.py\n        auto_scaler.py             Auto-scaling controller\n        leader_election.py         Multi-instance coordination\n        load_balancer.py           Request distribution\n      \n      cloud/\n        __init__.py\n        aws_integration.py         AWS service connectors\n        gcp_integration.py         Google Cloud service connectors\n        azure_integration.py       Azure service connectors\n        cloud_storage.py           Multi-cloud storage abstraction\n      \n      streaming/\n        __init__.py\n        stream_tasks.py            Stream processing task types\n        checkpoint_manager.py      Stream state checkpointing\n        event_router.py           Event-driven pipeline triggers\n      \n      ml/\n        __init__.py\n        feature_engineering.py     ML feature transformation tasks\n        model_training.py          ML training task types\n        model_serving.py           Model deployment and serving\n        experiment_tracking.py     MLflow integration\n      \n      realtime/\n        __init__.py\n        priority_scheduler.py      Low-latency task scheduling\n        event_processor.py         Real-time event processing\n        circuit_breaker.py         Failure protection patterns\n    \n    config/\n      extensions/\n        distributed.yaml           Container and Spark configuration\n        cloud.yaml                Cloud provider settings\n        streaming.yaml            Stream processing configuration\n        ml.yaml                   ML framework settings</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Container Task Executor Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> docker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> kubernetes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> kubernetes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> client, config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ContainerTaskSpec</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Specification for containerized task execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    image: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    command: </span><span style=\"color:#79B8FF\">list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    environment: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_limits: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    volumes: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ContainerTaskExecutor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Executes ETL tasks in isolated containers using Kubernetes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, namespace: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"etl-system\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.namespace </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> namespace</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.k8s_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            config.load_incluster_config()  </span><span style=\"color:#6A737D\"># Running in cluster</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> kubernetes.config.ConfigException:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            config.load_kube_config()  </span><span style=\"color:#6A737D\"># Local development</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.k8s_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client.BatchV1Api()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.core_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client.CoreV1Api()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> execute_task</span><span style=\"color:#E1E4E8\">(self, task_spec: ContainerTaskSpec) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute a task in a Kubernetes job container.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dict containing execution results, logs, and metadata</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"etl-task-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">task_spec.task_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Create Kubernetes job specification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_spec </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._create_job_spec(job_name, task_spec)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Submit job to Kubernetes</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.k8s_client.create_namespaced_job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.namespace,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                body</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_spec</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait for completion and collect results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._wait_for_completion(job_name, task_spec.timeout_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Cleanup completed job</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cleanup_job(job_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Container task execution failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cleanup_job(job_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_job_spec</span><span style=\"color:#E1E4E8\">(self, job_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, task_spec: ContainerTaskSpec):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create Kubernetes Job specification for ETL task.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> client.V1Job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client.V1ObjectMeta(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_name),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            spec</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client.V1JobSpec(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                template</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client.V1PodTemplateSpec(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client.V1ObjectMeta(</span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"app\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"etl-task\"</span><span style=\"color:#E1E4E8\">}),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    spec</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client.V1PodSpec(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        restart_policy</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Never\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        containers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            client.V1Container(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"etl-task\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                image</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">task_spec.image,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                command</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">task_spec.command,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                env</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    client.V1EnvVar(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">k, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">v)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                                    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> task_spec.environment.items()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                ],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                resources</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client.V1ResourceRequirements(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                    limits</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">task_spec.resource_limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                ),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                volume_mounts</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    client.V1VolumeMount(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                        name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                        mount_path</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                                    for</span><span style=\"color:#E1E4E8\"> name, path </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> task_spec.volumes.items()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        ],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        volumes</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            client.V1Volume(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                persistent_volume_claim</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client.V1PersistentVolumeClaimVolumeSource(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                                    claim_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">-pvc\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                            for</span><span style=\"color:#E1E4E8\"> name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> task_spec.volumes.keys()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _wait_for_completion</span><span style=\"color:#E1E4E8\">(self, job_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Wait for job completion and return results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asyncio.get_event_loop().time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#E1E4E8\"> (asyncio.get_event_loop().time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> timeout_seconds:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check job status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.k8s_client.read_namespaced_job_status(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.namespace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> job.status.succeeded:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Collect logs and return success result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._collect_logs(job_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"success\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"logs\"</span><span style=\"color:#E1E4E8\">: logs,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"duration\"</span><span style=\"color:#E1E4E8\">: asyncio.get_event_loop().time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> job.status.failed:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Collect logs and return failure result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._collect_logs(job_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"failed\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"logs\"</span><span style=\"color:#E1E4E8\">: logs,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Container job failed\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait before checking again</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Timeout occurred</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"timeout\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Task exceeded </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">timeout_seconds</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> second timeout\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _collect_logs</span><span style=\"color:#E1E4E8\">(self, job_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Collect logs from completed job pods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Find pods created by the job</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pods </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.core_client.list_namespaced_pod(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.namespace,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                label_selector</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job-name=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> pod </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> pods.items:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pod_logs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.core_client.read_namespaced_pod_log(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pod.metadata.name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.namespace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logs.append(pod_logs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> logs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to collect logs: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Log collection failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _cleanup_job</span><span style=\"color:#E1E4E8\">(self, job_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clean up completed Kubernetes job and associated pods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Delete the job (this also deletes associated pods)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.k8s_client.delete_namespaced_job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.namespace,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                propagation_policy</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Foreground\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to cleanup job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Stream Processing Task Framework:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, AsyncIterator, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StreamCheckpoint</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Stream processing checkpoint for recovery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    task_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stream_position: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processing_state: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StreamTask</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract base class for stream processing tasks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.task_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> task_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{__name__}</span><span style=\"color:#9ECBFF\">.</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">task_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checkpoint_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> process_event</span><span style=\"color:#E1E4E8\">(self, event: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Process a single event from the stream.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            event: Input event data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Transformed event or None to filter out</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> create_checkpoint</span><span style=\"color:#E1E4E8\">(self) -> StreamCheckpoint:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create checkpoint for current processing state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> restore_from_checkpoint</span><span style=\"color:#E1E4E8\">(self, checkpoint: StreamCheckpoint):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Restore processing state from checkpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> run</span><span style=\"color:#E1E4E8\">(self, input_stream: AsyncIterator[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  output_callback: </span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main execution loop for stream processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkpoint_counter </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            async</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> event </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> input_stream:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Process the event</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.process_event(event)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    await</span><span style=\"color:#E1E4E8\"> output_callback(result)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Checkpoint periodically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                checkpoint_counter </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> checkpoint_counter </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.get(</span><span style=\"color:#9ECBFF\">\"checkpoint_interval\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._create_checkpoint()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    checkpoint_counter </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Stream processing failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.is_running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Gracefully stop stream processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _create_checkpoint</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Internal checkpoint creation with error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoint_manager:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                checkpoint </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.create_checkpoint()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checkpoint_manager.save_checkpoint(checkpoint)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Checkpoint creation failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WindowedAggregationTask</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">StreamTask</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Stream task that performs windowed aggregations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(task_id, config)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.window_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">\"window_size_seconds\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.aggregation_func </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">\"aggregation\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"sum\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.group_by_field </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">\"group_by\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"key\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Processing state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_window </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.window_start </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> process_event</span><span style=\"color:#E1E4E8\">(self, event: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Aggregate events within time windows.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        event_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.fromisoformat(event.get(</span><span style=\"color:#9ECBFF\">\"timestamp\"</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Initialize window if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.window_start </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.window_start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check if event belongs to current window</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (event_time </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.window_start).total_seconds() </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.window_size:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Add to current window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            group_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event.get(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.group_by_field, </span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_window.get(group_key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.aggregation_func </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"sum\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.current_window[group_key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_value </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> event.get(</span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.aggregation_func </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"count\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.current_window[group_key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_value </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # No output until window closes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Window is complete, emit results and start new window</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"window_start\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.window_start.isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"window_end\"</span><span style=\"color:#E1E4E8\">: (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.window_start </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> timedelta(</span><span style=\"color:#FFAB70\">seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.window_size)).isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"aggregations\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_window.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Start new window</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.current_window </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {event.get(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.group_by_field, </span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">): event.get(</span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.window_start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> create_checkpoint</span><span style=\"color:#E1E4E8\">(self) -> StreamCheckpoint:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Save current window state for recovery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> StreamCheckpoint(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            task_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.task_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            stream_position</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Would be set by stream consumer</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            processing_state</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"current_window\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_window,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"window_start\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.window_start.isoformat() </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.window_start </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> restore_from_checkpoint</span><span style=\"color:#E1E4E8\">(self, checkpoint: StreamCheckpoint):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Restore window state from checkpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checkpoint.processing_state</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_window </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> state.get(</span><span style=\"color:#9ECBFF\">\"current_window\"</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        window_start_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> state.get(</span><span style=\"color:#9ECBFF\">\"window_start\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.window_start </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.fromisoformat(window_start_str) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> window_start_str </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Distributed Execution Coordinator:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionTarget</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOCAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"local\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CONTAINER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"container\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SPARK</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"spark\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SERVERLESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"serverless\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ResourceRequirements</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cores: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_gb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionTargetInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_type: ExecutionTarget</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    available_resources: ResourceRequirements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_utilization: ResourceRequirements</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DistributedExecutionCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates task execution across multiple execution targets.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.execution_targets: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ExecutionTargetInfo] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.task_assignments: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># task_id -> target_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_execution_target</span><span style=\"color:#E1E4E8\">(self, target_info: ExecutionTargetInfo):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register new execution target with available resources.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add target to execution_targets registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate target configuration and connectivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start health monitoring for the target</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log target registration for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> select_execution_target</span><span style=\"color:#E1E4E8\">(self, task_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, requirements: ResourceRequirements) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Select optimal execution target for task based on resource requirements.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns target_id of selected target, or None if no suitable target available.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Filter targets that have sufficient available resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate suitability score based on current utilization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Consider target type preferences (container vs spark vs serverless)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply load balancing to distribute tasks evenly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update target utilization after assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Score targets based on available_resources - current_utilization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_task_resources</span><span style=\"color:#E1E4E8\">(self, task_definition: </span><span style=\"color:#9ECBFF\">'TaskDefinition'</span><span style=\"color:#E1E4E8\">) -> ResourceRequirements:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate resource requirements based on task configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check task type and configuration for resource hints</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Look up historical resource usage for similar tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply default resource requirements based on task type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Consider data volume estimates for transformation tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add safety margins to prevent resource exhaustion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> determine_execution_target_type</span><span style=\"color:#E1E4E8\">(self, task_definition: </span><span style=\"color:#9ECBFF\">'TaskDefinition'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                      requirements: ResourceRequirements) -> ExecutionTarget:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine appropriate execution target type for task.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if task is suitable for serverless (stateless, short duration)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For large data transformations, prefer Spark execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For I/O intensive tasks, prefer container execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Fall back to local execution for simple tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Consider cost implications of different execution types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Auto-scaling Controller:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ScalingMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_queue_length: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    average_task_wait_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_utilization: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ScalingDecision</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    action: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # \"scale_up\", \"scale_down\", \"no_action\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_instances: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    reason: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AutoScalingController</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Automatically scales ETL system resources based on workload metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaling_history: List[ScalingDecision] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_instances </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_scaling_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> collect_metrics</span><span style=\"color:#E1E4E8\">(self) -> ScalingMetrics:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Collect current system metrics for scaling decisions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Query pipeline scheduler for queue length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate average task wait time from recent executions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Get resource utilization from all execution targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate error rate from recent task executions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return ScalingMetrics with current timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_scale_up</span><span style=\"color:#E1E4E8\">(self, metrics: ScalingMetrics) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if system should scale up based on metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if queue length exceeds scale-up threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if task wait time exceeds acceptable limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if resource utilization is too high</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure cooldown period has elapsed since last scaling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify maximum instance limit not exceeded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_scale_down</span><span style=\"color:#E1E4E8\">(self, metrics: ScalingMetrics) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if system should scale down based on metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if queue length is below scale-down threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if resource utilization is low for sustained period</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure minimum instance count maintained</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify no recent scaling actions (prevent oscillation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check that error rate is normal (don't scale down during issues)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> execute_scaling_decision</span><span style=\"color:#E1E4E8\">(self, decision: ScalingDecision):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute the scaling decision by adjusting system resources.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Log scaling decision with detailed reasoning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update container orchestration (Kubernetes HPA or similar)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Wait for new instances to become ready</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify scaling completed successfully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update internal state and record scaling history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Send notification about scaling action</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Stream Processing Milestone:</strong>\nAfter implementing basic stream processing capabilities, verify:</p>\n<ol>\n<li><p><strong>Stream Task Execution</strong>: Create a simple windowed aggregation task that sums events over 30-second windows</p>\n<ul>\n<li>Expected: Task should accumulate events and emit window results</li>\n<li>Command: <code>python -m etl_system.extensions.streaming.test_stream_task</code></li>\n<li>Verify: Check that window boundaries are respected and aggregations are correct</li>\n</ul>\n</li>\n<li><p><strong>Checkpoint Recovery</strong>: Stop and restart a stream task to verify checkpoint recovery</p>\n<ul>\n<li>Expected: Task resumes from last checkpoint without data loss</li>\n<li>Manual test: Send events, stop task mid-window, restart, verify window completion</li>\n<li>Signs of issues: Duplicate events, lost aggregation state, incorrect window boundaries</li>\n</ul>\n</li>\n<li><p><strong>Event-driven Pipeline Triggers</strong>: Set up file arrival event that triggers a pipeline</p>\n<ul>\n<li>Expected: Pipeline starts within 1 second of file creation</li>\n<li>Test: <code>touch /tmp/test_file.csv</code> should trigger pipeline execution</li>\n<li>Verify: Check pipeline run logs show event-triggered execution</li>\n</ul>\n</li>\n</ol>\n<p><strong>Distributed Execution Milestone:</strong>\nAfter implementing container-based execution:</p>\n<ol>\n<li><p><strong>Container Task Execution</strong>: Submit a simple data extraction task to Kubernetes</p>\n<ul>\n<li>Expected: Task runs in container and returns results to orchestrator</li>\n<li>Command: <code>kubectl logs -l app=etl-task</code> should show task execution logs</li>\n<li>Verify: Task completion reported in pipeline run status</li>\n</ul>\n</li>\n<li><p><strong>Resource-based Target Selection</strong>: Run tasks with different resource requirements</p>\n<ul>\n<li>Expected: High-memory tasks assigned to appropriate execution targets</li>\n<li>Test: Submit both light and heavy tasks, verify assignment logic</li>\n<li>Signs of issues: Tasks assigned to under-resourced targets, execution failures</li>\n</ul>\n</li>\n<li><p><strong>Auto-scaling Response</strong>: Generate high pipeline load to trigger scaling</p>\n<ul>\n<li>Expected: Additional orchestrator instances start automatically</li>\n<li>Monitor: <code>kubectl get pods</code> should show new instances after sustained load</li>\n<li>Verify: Load distributes across instances, no duplicate pipeline executions</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Container tasks never complete</strong></td>\n<td>Resource limits too restrictive</td>\n<td>Check <code>kubectl describe pod</code> for resource constraints</td>\n<td>Increase memory/CPU limits in container spec</td>\n</tr>\n<tr>\n<td><strong>Stream processing falls behind</strong></td>\n<td>Processing slower than event arrival rate</td>\n<td>Monitor event queue length and processing latency</td>\n<td>Add parallel stream processors or optimize processing logic</td>\n</tr>\n<tr>\n<td><strong>Auto-scaling oscillation</strong></td>\n<td>Thresholds too sensitive or cooldown too short</td>\n<td>Review scaling history and metric patterns</td>\n<td>Increase cooldown period and add hysteresis to thresholds</td>\n</tr>\n<tr>\n<td><strong>Distributed task failures</strong></td>\n<td>Network connectivity or authentication issues</td>\n<td>Test connectivity from containers to data sources</td>\n<td>Update network policies and credential mounting</td>\n</tr>\n<tr>\n<td><strong>ML pipeline memory errors</strong></td>\n<td>Model training exceeds available memory</td>\n<td>Monitor memory usage during training</td>\n<td>Use distributed training or gradient accumulation</td>\n</tr>\n<tr>\n<td><strong>Real-time processing timeouts</strong></td>\n<td>Processing chain too complex for latency target</td>\n<td>Profile each processing step latency</td>\n<td>Simplify processing or use faster execution targets</td>\n</tr>\n<tr>\n<td><strong>Cloud integration auth failures</strong></td>\n<td>IAM roles or service account misconfig</td>\n<td>Check cloud provider logs and permissions</td>\n<td>Update role policies and credential configuration</td>\n</tr>\n<tr>\n<td><strong>Stream checkpoint corruption</strong></td>\n<td>Concurrent checkpoint writes or storage failures</td>\n<td>Check checkpoint storage logs and file integrity</td>\n<td>Implement checkpoint locking and validation</td>\n</tr>\n</tbody></table>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones - comprehensive vocabulary reference that supports understanding across pipeline definition (Milestone 1), data processing (Milestones 2-3), orchestration and monitoring (Milestone 4), and system operations.</p>\n</blockquote>\n<p>This glossary provides comprehensive definitions of key technical terms and domain-specific vocabulary used throughout the ETL system design document. The terms are organized to support both newcomers learning ETL concepts and experienced developers working with the system&#39;s specific implementation details.</p>\n<h3 id=\"mental-model-technical-dictionary-with-context\">Mental Model: Technical Dictionary with Context</h3>\n<p>Think of this glossary as a specialized technical dictionary that goes beyond simple definitions. Like a good dictionary for a foreign language, each entry provides not just the meaning but also context about when and how the term is used in ETL systems. Just as language dictionaries show pronunciation, etymology, and usage examples, this technical glossary explains relationships between concepts, common usage patterns, and potential pitfalls that arise when working with these terms.</p>\n<p>The glossary serves as both a reference during development and a learning tool for understanding the broader ETL ecosystem. Terms build upon each other - understanding <strong>watermarking</strong> requires knowledge of <strong>incremental loading</strong>, which relates to <strong>change data capture</strong>, which connects to <strong>idempotent</strong> operations.</p>\n<h3 id=\"core-etl-and-data-processing-terms\">Core ETL and Data Processing Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>ETL</strong></td>\n<td>Extract Transform Load - the three-phase process of moving data from sources to destinations with transformations applied</td>\n<td>The fundamental pattern for data pipeline systems. Extract pulls data from sources, Transform applies business logic and data quality rules, Load writes results to destinations</td>\n</tr>\n<tr>\n<td><strong>DAG</strong></td>\n<td>Directed Acyclic Graph - a graph structure with directed edges and no cycles, used to represent task dependencies</td>\n<td>Core data structure for pipeline definition. Tasks are nodes, dependencies are edges. Acyclic property ensures execution order can be determined through topological sorting</td>\n</tr>\n<tr>\n<td><strong>idempotent</strong></td>\n<td>Operations that produce the same result when executed multiple times, regardless of how many times they run</td>\n<td>Critical property for ETL reliability. Enables safe retries after failures without data corruption or duplication. Example: <code>INSERT ... ON CONFLICT DO NOTHING</code></td>\n</tr>\n<tr>\n<td><strong>topological sort</strong></td>\n<td>Algorithm for ordering nodes in a DAG such that all dependencies come before their dependents</td>\n<td>Used to determine task execution order. Kahn&#39;s algorithm produces parallel execution levels, allowing multiple independent tasks to run simultaneously</td>\n</tr>\n<tr>\n<td><strong>checkpoint</strong></td>\n<td>Saving intermediate processing state to enable resumption after failures</td>\n<td>Enables fault tolerance by avoiding complete restart after partial completion. Includes task state, processed record counts, and watermark positions</td>\n</tr>\n<tr>\n<td><strong>lineage</strong></td>\n<td>Tracking data provenance and transformation history through pipeline execution</td>\n<td>Essential for compliance, debugging, and impact analysis. Records which source data contributed to each output record and what transformations were applied</td>\n</tr>\n<tr>\n<td><strong>watermark</strong></td>\n<td>High-water mark indicating the latest data that has been successfully processed</td>\n<td>Key mechanism for incremental loading. Typically a timestamp, sequence number, or other monotonically increasing value that tracks processing progress</td>\n</tr>\n<tr>\n<td><strong>adjacency list</strong></td>\n<td>Graph representation that maps each node to a list of its direct neighbors</td>\n<td>Standard representation for DAGs in pipeline systems. Enables efficient cycle detection and topological sorting algorithms</td>\n</tr>\n<tr>\n<td><strong>cycle detection</strong></td>\n<td>Algorithm to identify circular dependencies in directed graphs</td>\n<td>Critical validation step before pipeline execution. Uses depth-first search with three-color marking (white/gray/black) to detect back edges</td>\n</tr>\n<tr>\n<td><strong>critical path</strong></td>\n<td>Longest dependency chain in a DAG, determining minimum possible execution time</td>\n<td>Used for execution planning and performance optimization. Tasks on the critical path cannot be delayed without extending total pipeline runtime</td>\n</tr>\n<tr>\n<td><strong>execution levels</strong></td>\n<td>Groups of tasks that have no dependencies between them and can execute in parallel</td>\n<td>Result of topological sorting. Level 0 contains tasks with no dependencies, Level N contains tasks whose dependencies are all in levels 0 through N-1</td>\n</tr>\n<tr>\n<td><strong>in-degree</strong></td>\n<td>Number of incoming dependencies for a task in the dependency graph</td>\n<td>Used in Kahn&#39;s algorithm for topological sorting. Tasks with in-degree 0 are ready to execute</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-extraction-and-loading-terms\">Data Extraction and Loading Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>watermarking</strong></td>\n<td>Process of tracking and updating high-water marks for incremental data extraction</td>\n<td>Prevents duplicate processing and enables efficient incremental loads. Watermarks must be updated atomically with successful data loading</td>\n</tr>\n<tr>\n<td><strong>cursor-based pagination</strong></td>\n<td>Using opaque tokens or identifiers to track position in paginated API results</td>\n<td>More reliable than offset-based pagination for changing datasets. Cursors remain stable even when underlying data is modified during extraction</td>\n</tr>\n<tr>\n<td><strong>change data capture</strong></td>\n<td>Real-time stream of database changes (inserts, updates, deletes) for incremental loading</td>\n<td>Enables near real-time data synchronization. Common implementations include database transaction logs, triggers, or timestamp-based change tracking</td>\n</tr>\n<tr>\n<td><strong>bulk loading</strong></td>\n<td>Optimized batch insertion technique designed for high throughput data transfer</td>\n<td>Significantly faster than row-by-row inserts. Uses techniques like <code>COPY</code> statements, batch APIs, or staging files for maximum performance</td>\n</tr>\n<tr>\n<td><strong>upsert</strong></td>\n<td>Combined insert-or-update operation that handles conflicts when loading data</td>\n<td>Essential for idempotent loading. Syntax varies by database: PostgreSQL <code>ON CONFLICT</code>, MySQL <code>ON DUPLICATE KEY</code>, SQL Server <code>MERGE</code></td>\n</tr>\n<tr>\n<td><strong>schema mapping</strong></td>\n<td>Translation rules between source and destination data structures and types</td>\n<td>Handles differences in column names, data types, and structural organization. May include type conversion rules and default value assignments</td>\n</tr>\n<tr>\n<td><strong>connection pooling</strong></td>\n<td>Reusing database connections across multiple operations for improved performance</td>\n<td>Reduces connection overhead and manages concurrent access. Pools maintain minimum/maximum connection counts with timeout handling</td>\n</tr>\n<tr>\n<td><strong>incremental loading</strong></td>\n<td>Extracting and processing only data that has changed since the last pipeline execution</td>\n<td>Core technique for efficient ETL at scale. Relies on watermarking, timestamps, or change data capture to identify new/modified records</td>\n</tr>\n<tr>\n<td><strong>lookback window</strong></td>\n<td>Small time buffer added to watermark queries to handle clock skew and late-arriving data</td>\n<td>Prevents data loss from timing issues. Typically 1-5 minutes depending on system characteristics and consistency requirements</td>\n</tr>\n<tr>\n<td><strong>staging table</strong></td>\n<td>Temporary storage area for atomic bulk loading operations</td>\n<td>Enables transactional loading patterns. Data is loaded to staging first, then atomically moved to final destination, allowing rollback on failure</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-transformation-terms\">Data Transformation Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>schema evolution</strong></td>\n<td>Systematic management of data structure changes over time while maintaining compatibility</td>\n<td>Critical for production systems. Includes adding columns, changing types, and handling backward compatibility with existing consumers</td>\n</tr>\n<tr>\n<td><strong>type coercion</strong></td>\n<td>Automatic conversion between compatible data types during transformation</td>\n<td>Can cause precision loss (float to int) or truncation. Requires careful handling with validation and error reporting for failed conversions</td>\n</tr>\n<tr>\n<td><strong>UDF</strong></td>\n<td>User-Defined Function - custom transformation logic written in Python or other languages</td>\n<td>Enables complex business logic beyond SQL capabilities. Runs in isolated processes with proper error handling and resource limits</td>\n</tr>\n<tr>\n<td><strong>template rendering</strong></td>\n<td>Process of applying runtime parameters to SQL or configuration templates</td>\n<td>Uses template engines like Jinja2 to generate final SQL queries. Enables parameterized pipelines with dynamic behavior based on runtime context</td>\n</tr>\n<tr>\n<td><strong>subprocess isolation</strong></td>\n<td>Running transformation code in separate processes for safety and resource control</td>\n<td>Prevents memory leaks and crashes in one transformation from affecting others. Enables resource limits and timeout enforcement</td>\n</tr>\n<tr>\n<td><strong>schema registry</strong></td>\n<td>Centralized catalog of data structure definitions and their version history</td>\n<td>Enables schema validation, evolution tracking, and compatibility checking. Supports multiple schema formats (JSON Schema, Avro, etc.)</td>\n</tr>\n<tr>\n<td><strong>validation pipeline</strong></td>\n<td>Series of data quality checks that records pass through during transformation</td>\n<td>Includes type checking, constraint validation, and business rule verification. Failed records can be rejected, flagged, or sent to dead letter queues</td>\n</tr>\n<tr>\n<td><strong>null semantics</strong></td>\n<td>Rules for handling null/missing values across different systems and transformations</td>\n<td>Varies significantly between databases and languages. Requires explicit handling in transformations to prevent unexpected behavior</td>\n</tr>\n<tr>\n<td><strong>compatibility checking</strong></td>\n<td>Verification that schema changes don&#39;t break existing pipeline consumers</td>\n<td>Includes forward compatibility (new schemas work with old consumers) and backward compatibility (old schemas work with new consumers)</td>\n</tr>\n</tbody></table>\n<h3 id=\"pipeline-orchestration-terms\">Pipeline Orchestration Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>orchestration</strong></td>\n<td>Coordination of pipeline execution including scheduling, monitoring, and resource management</td>\n<td>Encompasses the entire pipeline lifecycle from trigger through completion. Includes dependency resolution, parallel execution, and failure handling</td>\n</tr>\n<tr>\n<td><strong>scheduler</strong></td>\n<td>Component responsible for triggering pipeline execution based on time schedules or external events</td>\n<td>Supports cron expressions for time-based triggers and event-driven execution. Manages schedule state and handles execution policies</td>\n</tr>\n<tr>\n<td><strong>execution engine</strong></td>\n<td>Component that runs pipeline tasks with parallelization, resource allocation, and state management</td>\n<td>Core runtime system that executes tasks according to DAG dependencies. Manages task state transitions and resource allocation</td>\n</tr>\n<tr>\n<td><strong>state machine</strong></td>\n<td>Formal model defining valid task states and the events that trigger transitions between them</td>\n<td>Ensures consistent task lifecycle management. States include PENDING, RUNNING, SUCCESS, FAILED with defined transition rules</td>\n</tr>\n<tr>\n<td><strong>resource allocation</strong></td>\n<td>Assignment of compute resources (CPU, memory, storage) to executing tasks</td>\n<td>Prevents resource exhaustion and enables performance optimization. Includes resource estimation, reservation, and cleanup</td>\n</tr>\n<tr>\n<td><strong>metrics collection</strong></td>\n<td>Systematic gathering of performance and business metrics during pipeline execution</td>\n<td>Enables monitoring, alerting, and performance optimization. Includes execution times, record counts, error rates, and resource usage</td>\n</tr>\n<tr>\n<td><strong>data lineage</strong></td>\n<td>Comprehensive tracking of data provenance and transformation history through pipelines</td>\n<td>Records complete data flow from sources through transformations to destinations. Essential for compliance, debugging, and impact analysis</td>\n</tr>\n<tr>\n<td><strong>alert suppression</strong></td>\n<td>Preventing duplicate or cascading alerts during system outages or widespread issues</td>\n<td>Reduces alert noise and prevents overwhelming operations teams. Uses correlation rules and time windows to group related alerts</td>\n</tr>\n<tr>\n<td><strong>level-based parallelization</strong></td>\n<td>Executing all tasks at the same DAG level simultaneously while respecting dependencies</td>\n<td>Maximizes pipeline throughput by running independent tasks in parallel. Each level waits for the previous level to complete</td>\n</tr>\n<tr>\n<td><strong>message broker</strong></td>\n<td>Asynchronous communication system enabling loose coupling between pipeline components</td>\n<td>Enables event-driven architecture and fault-tolerant communication. Common implementations include Apache Kafka, RabbitMQ, or cloud messaging services</td>\n</tr>\n<tr>\n<td><strong>dependency resolution</strong></td>\n<td>Process of determining task execution order based on declared prerequisites and constraints</td>\n<td>Combines topological sorting with runtime conditions. Handles complex scenarios like conditional dependencies and dynamic task generation</td>\n</tr>\n</tbody></table>\n<h3 id=\"error-handling-and-recovery-terms\">Error Handling and Recovery Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>exponential backoff</strong></td>\n<td>Retry strategy with exponentially increasing delays between attempts, often with randomization</td>\n<td>Prevents overwhelming failing systems while providing reasonable retry behavior. Typical pattern: 1s, 2s, 4s, 8s with jitter</td>\n</tr>\n<tr>\n<td><strong>circuit breaker</strong></td>\n<td>Protection mechanism that stops calling a failing service to prevent cascading failures</td>\n<td>Opens circuit after consecutive failures, enters half-open state for testing, closes when service recovers. Prevents thundering herd problems</td>\n</tr>\n<tr>\n<td><strong>dead letter queue</strong></td>\n<td>Storage system for messages that cannot be processed after maximum retry attempts</td>\n<td>Enables manual inspection and reprocessing of failed items. Prevents data loss while avoiding infinite retry loops</td>\n</tr>\n<tr>\n<td><strong>jitter</strong></td>\n<td>Random variation added to retry timing to prevent synchronized load spikes</td>\n<td>Prevents thundering herd when many clients retry simultaneously. Typically 10-50% random variation in delay timing</td>\n</tr>\n<tr>\n<td><strong>saga pattern</strong></td>\n<td>Breaking long-running transactions into smaller, compensatable steps with rollback capability</td>\n<td>Enables fault tolerance in distributed systems. Each step has a corresponding compensation operation for rollback</td>\n</tr>\n<tr>\n<td><strong>two-phase commit</strong></td>\n<td>Distributed transaction protocol ensuring atomicity across multiple systems</td>\n<td>Provides strong consistency guarantees at the cost of performance and availability. Requires all participants to vote before committing</td>\n</tr>\n<tr>\n<td><strong>compensation transaction</strong></td>\n<td>Reverse operation designed to undo the effects of a completed transaction step</td>\n<td>Key component of saga pattern. Must be idempotent and handle partial completion scenarios</td>\n</tr>\n</tbody></table>\n<p><img src=\"/api/project/etl-pipeline/architecture-doc/asset?path=diagrams%2Ferror-handling-flow.svg\" alt=\"Error Handling and Recovery Flow\"></p>\n<h3 id=\"monitoring-and-operations-terms\">Monitoring and Operations Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>health check</strong></td>\n<td>Automated verification of component operational status and readiness to handle requests</td>\n<td>Includes connectivity tests, resource availability checks, and functional validation. Used by load balancers and monitoring systems</td>\n</tr>\n<tr>\n<td><strong>log correlation</strong></td>\n<td>Process of connecting related log entries across different components and time periods</td>\n<td>Essential for distributed system debugging. Uses correlation IDs, timestamps, and context propagation to trace request flows</td>\n</tr>\n<tr>\n<td><strong>performance profiling</strong></td>\n<td>Systematic analysis of resource usage, bottlenecks, and optimization opportunities</td>\n<td>Identifies CPU, memory, and I/O hotspots. Includes both real-time monitoring and historical analysis for capacity planning</td>\n</tr>\n<tr>\n<td><strong>symptom-cause mapping</strong></td>\n<td>Structured diagnostic approach that maps observable symptoms to underlying root causes</td>\n<td>Systematic troubleshooting methodology. Documents known failure patterns and their resolution steps for faster incident response</td>\n</tr>\n<tr>\n<td><strong>interactive debugging</strong></td>\n<td>Real-time investigation of system behavior using debugging tools and techniques</td>\n<td>Includes breakpoints, variable inspection, and step-through execution. Challenging in distributed systems due to timing dependencies</td>\n</tr>\n<tr>\n<td><strong>resource monitoring</strong></td>\n<td>Continuous tracking of system resource utilization including CPU, memory, disk, and network</td>\n<td>Enables capacity planning, performance optimization, and early warning of resource exhaustion. Includes both host-level and application-level metrics</td>\n</tr>\n<tr>\n<td><strong>error context</strong></td>\n<td>Log entries and system state surrounding an error event to provide diagnostic information</td>\n<td>Critical for effective troubleshooting. Includes events leading up to the error, concurrent activities, and system state at failure time</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-architecture-terms\">Advanced Architecture Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>distributed execution</strong></td>\n<td>Running pipeline tasks across multiple machines or compute environments</td>\n<td>Enables horizontal scaling and resource optimization. Requires coordination, state management, and fault tolerance across network boundaries</td>\n</tr>\n<tr>\n<td><strong>horizontal scaling</strong></td>\n<td>Adding more compute instances to handle increased load, as opposed to upgrading existing hardware</td>\n<td>Preferred scaling approach for cloud systems. Requires stateless task design and effective load distribution mechanisms</td>\n</tr>\n<tr>\n<td><strong>auto-scaling</strong></td>\n<td>Automatically adjusting resource allocation based on current demand and performance metrics</td>\n<td>Balances cost optimization with performance requirements. Includes scale-up triggers, scale-down policies, and resource estimation</td>\n</tr>\n<tr>\n<td><strong>cloud-native</strong></td>\n<td>Architecture designed specifically for cloud deployment patterns and services</td>\n<td>Emphasizes containerization, microservices, and managed cloud services. Designed for elasticity, fault tolerance, and operational simplicity</td>\n</tr>\n<tr>\n<td><strong>stream processing</strong></td>\n<td>Continuous processing of data streams as they arrive, rather than batch processing</td>\n<td>Enables real-time analytics and low-latency data pipelines. Requires different architectural patterns than traditional batch ETL</td>\n</tr>\n<tr>\n<td><strong>machine learning pipeline</strong></td>\n<td>Specialized workflow for ML model training, validation, and deployment with unique requirements</td>\n<td>Includes data preprocessing, feature engineering, model training, validation, and deployment stages. Requires versioning and experiment tracking</td>\n</tr>\n<tr>\n<td><strong>real-time processing</strong></td>\n<td>Data processing with sub-second response time requirements</td>\n<td>More demanding than stream processing, requiring specialized architectures and technologies. Often uses in-memory processing and optimized data structures</td>\n</tr>\n<tr>\n<td><strong>event-driven orchestration</strong></td>\n<td>Triggering pipeline execution based on external events rather than time schedules</td>\n<td>Enables reactive data processing and just-in-time pipeline execution. Requires reliable event delivery and proper ordering</td>\n</tr>\n<tr>\n<td><strong>container orchestration</strong></td>\n<td>Managing containerized applications across clusters with scheduling, scaling, and service discovery</td>\n<td>Enables portable deployment and efficient resource utilization. Common platforms include Kubernetes, Docker Swarm, and cloud container services</td>\n</tr>\n<tr>\n<td><strong>serverless integration</strong></td>\n<td>Using cloud functions and managed services for lightweight, event-driven tasks</td>\n<td>Eliminates infrastructure management while providing automatic scaling. Suitable for simple transformations and integration tasks</td>\n</tr>\n<tr>\n<td><strong>checkpoint recovery</strong></td>\n<td>Resuming pipeline execution from saved processing state after failures</td>\n<td>Enables fault tolerance without complete restart. Requires careful state management and atomic checkpoint operations</td>\n</tr>\n<tr>\n<td><strong>leader election</strong></td>\n<td>Choosing a primary instance in a distributed system to coordinate shared operations</td>\n<td>Prevents split-brain scenarios and ensures single point of control. Common in distributed schedulers and coordination services</td>\n</tr>\n<tr>\n<td><strong>priority scheduling</strong></td>\n<td>Executing higher-priority tasks before lower-priority ones, subject to resource availability</td>\n<td>Enables SLA management and critical path optimization. Requires careful balance to prevent starvation of lower-priority tasks</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-types-and-validation-terms\">Data Types and Validation Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>STRING</strong></td>\n<td>Variable-length text data type supporting Unicode characters</td>\n<td>Most flexible data type but requires careful handling of encoding, length limits, and special characters during transformations</td>\n</tr>\n<tr>\n<td><strong>INTEGER</strong></td>\n<td>Whole number data type with defined precision and range limits</td>\n<td>Common source of overflow errors during transformations. Different systems have different integer sizes and signedness</td>\n</tr>\n<tr>\n<td><strong>FLOAT</strong></td>\n<td>Floating-point numeric data type with inherent precision limitations</td>\n<td>Can cause precision loss during type conversion. Requires careful handling of NaN, infinity, and rounding behaviors</td>\n</tr>\n<tr>\n<td><strong>DECIMAL</strong></td>\n<td>Fixed-precision numeric data type for exact decimal calculations</td>\n<td>Preferred for financial calculations. Precision and scale must be preserved during transformations to prevent data loss</td>\n</tr>\n<tr>\n<td><strong>BOOLEAN</strong></td>\n<td>Binary true/false data type with varying representations across systems</td>\n<td>Representations vary: true/false, 1/0, Y/N, T/F. Requires normalization during cross-system data movement</td>\n</tr>\n<tr>\n<td><strong>DATE</strong></td>\n<td>Calendar date without time component, with varying precision and timezone handling</td>\n<td>Timezone-naive type that can cause issues in global systems. Requires careful handling of date arithmetic and comparisons</td>\n</tr>\n<tr>\n<td><strong>TIMESTAMP</strong></td>\n<td>Date and time data type with optional timezone information</td>\n<td>Complex type requiring timezone handling, precision management, and careful comparison logic across systems</td>\n</tr>\n<tr>\n<td><strong>JSON</strong></td>\n<td>Semi-structured data type for nested objects and arrays</td>\n<td>Requires schema validation and careful handling of type coercion for nested fields. Not all systems support JSON natively</td>\n</tr>\n<tr>\n<td><strong>BINARY</strong></td>\n<td>Raw binary data type for storing files, images, or encoded content</td>\n<td>Requires base64 encoding for text-based transport. Large binary fields can impact pipeline performance</td>\n</tr>\n</tbody></table>\n<h3 id=\"task-and-execution-state-terms\">Task and Execution State Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context and Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>PENDING</strong></td>\n<td>Initial task state before dependencies are evaluated and resources allocated</td>\n<td>Task is defined but not yet ready for execution. Waiting for dependency resolution and resource availability</td>\n</tr>\n<tr>\n<td><strong>WAITING</strong></td>\n<td>Task state when dependencies are not yet satisfied</td>\n<td>Task is ready to execute but blocked by upstream dependencies. Moves to QUEUED when dependencies complete successfully</td>\n</tr>\n<tr>\n<td><strong>QUEUED</strong></td>\n<td>Task state when ready for execution but waiting for available resources</td>\n<td>Dependencies are satisfied but execution slot or resources not yet available. Managed by scheduler priority queues</td>\n</tr>\n<tr>\n<td><strong>RUNNING</strong></td>\n<td>Task state during active execution</td>\n<td>Task is consuming resources and performing work. Requires monitoring for progress, timeouts, and resource usage</td>\n</tr>\n<tr>\n<td><strong>SUCCESS</strong></td>\n<td>Task completed successfully with expected outputs</td>\n<td>Terminal state indicating successful completion. Enables downstream dependencies to begin execution</td>\n</tr>\n<tr>\n<td><strong>FAILED</strong></td>\n<td>Task completed unsuccessfully due to errors or exceptions</td>\n<td>Terminal state unless retry policy applies. May trigger failure handling, alerts, and pipeline-level error responses</td>\n</tr>\n<tr>\n<td><strong>RETRYING</strong></td>\n<td>Task is scheduled for retry after a failure</td>\n<td>Temporary state between failure and retry attempt. Managed by retry policy with exponential backoff timing</td>\n</tr>\n<tr>\n<td><strong>CANCELLED</strong></td>\n<td>Task execution was cancelled by user or system action</td>\n<td>Terminal state for tasks that were stopped before completion. Requires cleanup of partial work and resources</td>\n</tr>\n<tr>\n<td><strong>SKIPPED</strong></td>\n<td>Task was intentionally bypassed due to conditional logic or upstream failures</td>\n<td>Terminal state for tasks that didn&#39;t need to execute. Common in conditional pipelines and failure scenarios</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This glossary serves as both a learning resource and a practical reference during development. The terms are carefully chosen to align with industry standards while providing specific context for this ETL system implementation.</p>\n<h4 id=\"technology-recommendations-for-glossary-management\">Technology Recommendations for Glossary Management</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Glossary Storage</td>\n<td>Static markdown files in repository</td>\n<td>Searchable documentation platform (GitBook, Notion)</td>\n</tr>\n<tr>\n<td>Term Cross-References</td>\n<td>Manual links between sections</td>\n<td>Automated link detection and validation</td>\n</tr>\n<tr>\n<td>Definition Validation</td>\n<td>Manual review process</td>\n<td>Automated consistency checking against codebase</td>\n</tr>\n<tr>\n<td>Usage Examples</td>\n<td>Inline code snippets</td>\n<td>Live examples from test cases</td>\n</tr>\n</tbody></table>\n<h4 id=\"glossary-maintenance-structure\">Glossary Maintenance Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  docs/\n    design/\n      glossary.md                     this document\n      term-usage-examples/            code examples for key terms\n        dag-operations.py             DAG manipulation examples  \n        watermarking-patterns.py      incremental loading examples\n        retry-policies.py             error handling examples\n    api/\n      terminology.json                machine-readable term definitions\n      cross-references.json           term relationship mappings\n  src/\n    common/\n      constants.py                    canonical constant definitions\n      types.py                        type definitions matching glossary</code></pre></div>\n\n<h4 id=\"term-consistency-validation\">Term Consistency Validation</h4>\n<p>The glossary terms should be validated against the actual codebase to ensure consistency. Key validation points include:</p>\n<ul>\n<li>Type names match exactly between glossary definitions and code declarations</li>\n<li>Method signatures align with interface descriptions  </li>\n<li>Constants are defined with correct values and naming conventions</li>\n<li>Enum values match exactly across documentation and implementation</li>\n<li>State machine transitions are accurately reflected in both glossary and code</li>\n</ul>\n<h4 id=\"common-glossary-usage-patterns\">Common Glossary Usage Patterns</h4>\n<p> <strong>Pitfall: Terminology Drift</strong>\nTerms defined in early design phases often evolve during implementation, leading to inconsistencies between documentation and code. Establish a single source of truth (preferably the code) and update documentation accordingly.</p>\n<p> <strong>Pitfall: Overloaded Terms</strong><br>Some terms like &quot;pipeline&quot; or &quot;state&quot; have multiple meanings in different contexts. Always provide sufficient context to disambiguate, and consider using compound terms like &quot;pipeline definition&quot; vs &quot;pipeline execution&quot; when clarity is important.</p>\n<p> <strong>Pitfall: Missing Domain Context</strong>\nTechnical terms often have different meanings in different domains. ETL-specific definitions may differ from general software engineering or database administration usage. Always provide ETL-specific context and usage examples.</p>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint: Terminology Consistency</strong></p>\n<ul>\n<li>Verify all type names in code match glossary definitions exactly</li>\n<li>Confirm method signatures align with interface descriptions</li>\n<li>Validate that state machine implementations match documented transitions</li>\n<li>Check that constant values match their glossary descriptions</li>\n</ul>\n<p><strong>Checkpoint: Documentation Completeness</strong>  </p>\n<ul>\n<li>Ensure every public type has a corresponding glossary entry</li>\n<li>Verify all domain-specific terms are defined with appropriate context</li>\n<li>Confirm cross-references between related terms are accurate and helpful</li>\n<li>Validate that examples provided are current and functional</li>\n</ul>\n<p>The glossary should be treated as a living document that evolves with the system while maintaining accuracy and usefulness as a reference tool for both learning and development activities.</p>\n","toc":[{"level":1,"text":"Data Pipeline / ETL System: Design Document","id":"data-pipeline-etl-system-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: Factory Assembly Line","id":"mental-model-factory-assembly-line"},{"level":3,"text":"Existing ETL Solutions","id":"existing-etl-solutions"},{"level":4,"text":"Apache Airflow: The Enterprise Standard","id":"apache-airflow-the-enterprise-standard"},{"level":4,"text":"Dagster: The Developer Experience Focus","id":"dagster-the-developer-experience-focus"},{"level":4,"text":"Custom Solutions: The Build vs. Buy Decision","id":"custom-solutions-the-build-vs-buy-decision"},{"level":3,"text":"Core Technical Challenges","id":"core-technical-challenges"},{"level":4,"text":"Dependency Management: The Coordination Problem","id":"dependency-management-the-coordination-problem"},{"level":4,"text":"Failure Recovery: Building Resilient Data Workflows","id":"failure-recovery-building-resilient-data-workflows"},{"level":4,"text":"Data Consistency: Managing State Across Distributed Operations","id":"data-consistency-managing-state-across-distributed-operations"},{"level":4,"text":"Scalability Challenges: Performance and Resource Management","id":"scalability-challenges-performance-and-resource-management"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Core Architecture Patterns","id":"core-architecture-patterns"},{"level":4,"text":"Development Environment Setup","id":"development-environment-setup"},{"level":4,"text":"Language-Specific Implementation Notes","id":"language-specific-implementation-notes"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Mental Model: Project Charter","id":"mental-model-project-charter"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":3,"text":"Non-Functional Goals","id":"non-functional-goals"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Mental Model: Orchestra Conductor System","id":"mental-model-orchestra-conductor-system"},{"level":3,"text":"System Components","id":"system-components"},{"level":4,"text":"DAG Definition Engine","id":"dag-definition-engine"},{"level":4,"text":"Pipeline Scheduler","id":"pipeline-scheduler"},{"level":4,"text":"Task Executor","id":"task-executor"},{"level":4,"text":"Data Connectors","id":"data-connectors"},{"level":4,"text":"Monitoring and Observability System","id":"monitoring-and-observability-system"},{"level":3,"text":"Component Interactions","id":"component-interactions"},{"level":4,"text":"Scheduler to DAG Engine Communication","id":"scheduler-to-dag-engine-communication"},{"level":4,"text":"Scheduler to Task Executor Coordination","id":"scheduler-to-task-executor-coordination"},{"level":4,"text":"Task Executor to Connector Interactions","id":"task-executor-to-connector-interactions"},{"level":4,"text":"Monitoring System Event Collection","id":"monitoring-system-event-collection"},{"level":4,"text":"Error Propagation and Recovery Coordination","id":"error-propagation-and-recovery-coordination"},{"level":3,"text":"Deployment Topology","id":"deployment-topology"},{"level":4,"text":"Development Environment Structure","id":"development-environment-structure"},{"level":4,"text":"Production Deployment Architecture","id":"production-deployment-architecture"},{"level":4,"text":"Network Communication Patterns","id":"network-communication-patterns"},{"level":4,"text":"Persistent State Management","id":"persistent-state-management"},{"level":4,"text":"Security and Access Control Deployment","id":"security-and-access-control-deployment"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Mental Model: Digital Blueprint and Construction Log","id":"mental-model-digital-blueprint-and-construction-log"},{"level":3,"text":"Pipeline and Task Definitions","id":"pipeline-and-task-definitions"},{"level":4,"text":"PipelineDefinition Structure","id":"pipelinedefinition-structure"},{"level":4,"text":"TaskDefinition Structure","id":"taskdefinition-structure"},{"level":4,"text":"RetryPolicy Configuration","id":"retrypolicy-configuration"},{"level":4,"text":"Dependency Resolution and Validation","id":"dependency-resolution-and-validation"},{"level":3,"text":"Runtime State Model","id":"runtime-state-model"},{"level":4,"text":"Pipeline Run Lifecycle","id":"pipeline-run-lifecycle"},{"level":4,"text":"Task Execution State Management","id":"task-execution-state-management"},{"level":4,"text":"State Transition Logic","id":"state-transition-logic"},{"level":4,"text":"Execution Metrics and Observability","id":"execution-metrics-and-observability"},{"level":3,"text":"Metadata and Lineage","id":"metadata-and-lineage"},{"level":4,"text":"Data Lineage Tracking","id":"data-lineage-tracking"},{"level":4,"text":"Schema Evolution and Versioning","id":"schema-evolution-and-versioning"},{"level":4,"text":"Audit Trail and Compliance","id":"audit-trail-and-compliance"},{"level":4,"text":"Data Quality and Validation History","id":"data-quality-and-validation-history"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"DAG Definition and Validation Engine","id":"dag-definition-and-validation-engine"},{"level":3,"text":"Mental Model: Recipe Dependencies","id":"mental-model-recipe-dependencies"},{"level":3,"text":"DAG Parsing and Validation","id":"dag-parsing-and-validation"},{"level":3,"text":"Execution Order Resolution","id":"execution-order-resolution"},{"level":3,"text":"DAG Visualization","id":"dag-visualization"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Extraction and Loading","id":"data-extraction-and-loading"},{"level":3,"text":"Mental Model: Universal Adapters","id":"mental-model-universal-adapters"},{"level":3,"text":"Source Connectors","id":"source-connectors"},{"level":3,"text":"Destination Connectors","id":"destination-connectors"},{"level":3,"text":"Incremental Loading Strategies","id":"incremental-loading-strategies"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Transformation Engine","id":"data-transformation-engine"},{"level":3,"text":"Mental Model: Data Refinery","id":"mental-model-data-refinery"},{"level":3,"text":"SQL-Based Transformations","id":"sql-based-transformations"},{"level":3,"text":"Python User-Defined Functions","id":"python-user-defined-functions"},{"level":3,"text":"Schema Validation and Evolution","id":"schema-validation-and-evolution"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Pipeline Orchestration and Monitoring","id":"pipeline-orchestration-and-monitoring"},{"level":3,"text":"Mental Model: Air Traffic Control","id":"mental-model-air-traffic-control"},{"level":3,"text":"Scheduler Integration","id":"scheduler-integration"},{"level":4,"text":"Cron-Based Scheduling","id":"cron-based-scheduling"},{"level":4,"text":"Event-Driven Triggers","id":"event-driven-triggers"},{"level":4,"text":"Schedule Management Interface","id":"schedule-management-interface"},{"level":3,"text":"Task Execution Engine","id":"task-execution-engine"},{"level":4,"text":"Parallel Execution Management","id":"parallel-execution-management"},{"level":4,"text":"State Management and Persistence","id":"state-management-and-persistence"},{"level":4,"text":"Resource Allocation and Cleanup","id":"resource-allocation-and-cleanup"},{"level":3,"text":"Monitoring and Alerting","id":"monitoring-and-alerting"},{"level":4,"text":"Metrics Collection and Aggregation","id":"metrics-collection-and-aggregation"},{"level":4,"text":"Real-Time Dashboard Integration","id":"real-time-dashboard-integration"},{"level":4,"text":"Failure Notification and Escalation","id":"failure-notification-and-escalation"},{"level":4,"text":"Data Lineage and Audit Trail","id":"data-lineage-and-audit-trail"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Mental Model: Orchestra Conductor","id":"mental-model-orchestra-conductor"},{"level":3,"text":"Component Communication","id":"component-communication"},{"level":4,"text":"Control Plane APIs","id":"control-plane-apis"},{"level":4,"text":"Data Plane Messaging","id":"data-plane-messaging"},{"level":4,"text":"Inter-Component State Synchronization","id":"inter-component-state-synchronization"},{"level":3,"text":"Pipeline Execution Flow","id":"pipeline-execution-flow"},{"level":4,"text":"Phase 1: Pipeline Trigger and Validation","id":"phase-1-pipeline-trigger-and-validation"},{"level":4,"text":"Phase 2: Task Scheduling and Dependency Resolution","id":"phase-2-task-scheduling-and-dependency-resolution"},{"level":4,"text":"Phase 3: Task Execution and State Management","id":"phase-3-task-execution-and-state-management"},{"level":4,"text":"Phase 4: Dependency Propagation and Pipeline Completion","id":"phase-4-dependency-propagation-and-pipeline-completion"},{"level":4,"text":"Phase 5: Cleanup and Lineage Recording","id":"phase-5-cleanup-and-lineage-recording"},{"level":3,"text":"Data Lineage Tracking","id":"data-lineage-tracking"},{"level":4,"text":"Mental Model: Evidence Chain in Investigation","id":"mental-model-evidence-chain-in-investigation"},{"level":4,"text":"Lineage Data Model","id":"lineage-data-model"},{"level":4,"text":"Lineage Collection Strategy","id":"lineage-collection-strategy"},{"level":4,"text":"Automatic Lineage Capture","id":"automatic-lineage-capture"},{"level":4,"text":"Lineage Query and Analysis","id":"lineage-query-and-analysis"},{"level":4,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: Hospital Emergency Response System","id":"mental-model-hospital-emergency-response-system"},{"level":3,"text":"Common Failure Modes","id":"common-failure-modes"},{"level":4,"text":"Network and Connectivity Failures","id":"network-and-connectivity-failures"},{"level":4,"text":"Data Quality and Corruption Issues","id":"data-quality-and-corruption-issues"},{"level":4,"text":"Resource Exhaustion and Performance Degradation","id":"resource-exhaustion-and-performance-degradation"},{"level":3,"text":"Retry and Backoff Strategies","id":"retry-and-backoff-strategies"},{"level":4,"text":"Exponential Backoff Implementation","id":"exponential-backoff-implementation"},{"level":4,"text":"Circuit Breaker Pattern","id":"circuit-breaker-pattern"},{"level":4,"text":"Dead Letter Queue Management","id":"dead-letter-queue-management"},{"level":3,"text":"Partial Failure Recovery","id":"partial-failure-recovery"},{"level":4,"text":"Checkpointing and State Persistence","id":"checkpointing-and-state-persistence"},{"level":4,"text":"Transaction Management and Rollback","id":"transaction-management-and-rollback"},{"level":4,"text":"Data Cleanup and Consistency Repair","id":"data-cleanup-and-consistency-repair"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: Quality Assurance Factory","id":"mental-model-quality-assurance-factory"},{"level":3,"text":"Unit Testing Approach","id":"unit-testing-approach"},{"level":3,"text":"Integration Testing","id":"integration-testing"},{"level":3,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: Medical Diagnosis","id":"mental-model-medical-diagnosis"},{"level":3,"text":"Common Symptoms and Causes","id":"common-symptoms-and-causes"},{"level":3,"text":"Common Anti-Patterns and Debugging Traps","id":"common-anti-patterns-and-debugging-traps"},{"level":3,"text":"Debugging Techniques","id":"debugging-techniques"},{"level":4,"text":"Log-Based Investigation","id":"log-based-investigation"},{"level":4,"text":"State Inspection and Monitoring","id":"state-inspection-and-monitoring"},{"level":4,"text":"Interactive Debugging Techniques","id":"interactive-debugging-techniques"},{"level":3,"text":"Performance Debugging","id":"performance-debugging"},{"level":4,"text":"Identifying Performance Bottlenecks","id":"identifying-performance-bottlenecks"},{"level":4,"text":"Memory Performance Issues","id":"memory-performance-issues"},{"level":4,"text":"Database Performance Debugging","id":"database-performance-debugging"},{"level":4,"text":"Network and I/O Performance","id":"network-and-io-performance"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations for Debugging","id":"technology-recommendations-for-debugging"},{"level":4,"text":"Recommended File Structure for Debugging Tools","id":"recommended-file-structure-for-debugging-tools"},{"level":4,"text":"Essential Debugging Infrastructure","id":"essential-debugging-infrastructure"},{"level":4,"text":"Core Debugging Tools Implementation","id":"core-debugging-tools-implementation"},{"level":4,"text":"Milestone Checkpoints for Debugging Implementation","id":"milestone-checkpoints-for-debugging-implementation"},{"level":4,"text":"Common Debugging Issues and Solutions","id":"common-debugging-issues-and-solutions"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Mental Model: Growing City Infrastructure","id":"mental-model-growing-city-infrastructure"},{"level":3,"text":"Scalability Extensions","id":"scalability-extensions"},{"level":4,"text":"Distributed Task Execution","id":"distributed-task-execution"},{"level":4,"text":"Horizontal Scaling and Auto-scaling","id":"horizontal-scaling-and-auto-scaling"},{"level":4,"text":"Cloud-Native Integration","id":"cloud-native-integration"},{"level":3,"text":"Advanced Pipeline Features","id":"advanced-pipeline-features"},{"level":4,"text":"Stream Processing Integration","id":"stream-processing-integration"},{"level":4,"text":"Machine Learning Pipeline Integration","id":"machine-learning-pipeline-integration"},{"level":4,"text":"Real-time Processing and Event-driven Orchestration","id":"real-time-processing-and-event-driven-orchestration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure Extension","id":"recommended-file-structure-extension"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: Technical Dictionary with Context","id":"mental-model-technical-dictionary-with-context"},{"level":3,"text":"Core ETL and Data Processing Terms","id":"core-etl-and-data-processing-terms"},{"level":3,"text":"Data Extraction and Loading Terms","id":"data-extraction-and-loading-terms"},{"level":3,"text":"Data Transformation Terms","id":"data-transformation-terms"},{"level":3,"text":"Pipeline Orchestration Terms","id":"pipeline-orchestration-terms"},{"level":3,"text":"Error Handling and Recovery Terms","id":"error-handling-and-recovery-terms"},{"level":3,"text":"Monitoring and Operations Terms","id":"monitoring-and-operations-terms"},{"level":3,"text":"Advanced Architecture Terms","id":"advanced-architecture-terms"},{"level":3,"text":"Data Types and Validation Terms","id":"data-types-and-validation-terms"},{"level":3,"text":"Task and Execution State Terms","id":"task-and-execution-state-terms"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations for Glossary Management","id":"technology-recommendations-for-glossary-management"},{"level":4,"text":"Glossary Maintenance Structure","id":"glossary-maintenance-structure"},{"level":4,"text":"Term Consistency Validation","id":"term-consistency-validation"},{"level":4,"text":"Common Glossary Usage Patterns","id":"common-glossary-usage-patterns"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"}],"title":"Data Pipeline / ETL System: Design Document","markdown":"# Data Pipeline / ETL System: Design Document\n\n\n## Overview\n\nThis system builds a scalable ETL pipeline framework that orchestrates data movement, transformation, and validation across heterogeneous sources. The key architectural challenge is managing complex task dependencies, handling failures gracefully, and ensuring data consistency while maintaining high throughput and observability.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** Foundation for all milestones - establishes the architectural context and requirements that guide the entire system design.\n\n### Mental Model: Factory Assembly Line\n\nThink of an ETL pipeline system as orchestrating a massive **factory assembly line** where raw materials flow through multiple processing stations to create finished products. Just as Henry Ford revolutionized manufacturing by breaking down car assembly into discrete, coordinated steps, modern data processing requires breaking down complex data workflows into manageable, interdependent tasks.\n\nIn our factory analogy, **raw data sources** are like suppliers delivering different types of materials - some arrive by truck (batch files), others through conveyor belts (streaming APIs), and still others from warehouse inventory (databases). Each **transformation step** is a specialized workstation with specific tools and workers trained for particular operations - cutting, welding, painting, or quality inspection. The **final destinations** are shipping docks where finished products are loaded onto trucks bound for different customers.\n\nThe critical insight from manufacturing applies directly to data processing: **dependencies matter immensely**. You cannot paint a car door before it's been cut and shaped. You cannot install wheels before the chassis is assembled. Similarly, you cannot aggregate sales data before cleaning customer records, and you cannot generate reports before all source systems have been synchronized. This dependency management becomes the central orchestration challenge.\n\nJust as a factory needs a **production control system** to track work orders, monitor station capacity, handle equipment failures, and ensure quality standards, our ETL system needs sophisticated orchestration to manage task scheduling, resource allocation, failure recovery, and data validation. The complexity multiplies when you consider that our \"factory\" operates 24/7 across distributed systems, with \"materials\" arriving from dozens of sources on different schedules, and \"customers\" expecting deliveries with strict SLA requirements.\n\nThe assembly line analogy also reveals why simple script-based approaches fail at scale. A small workshop can operate with informal coordination - the craftsperson handles each order from start to finish. But when you need to process terabytes of data daily with sub-hour latency requirements, you need the equivalent of Toyota's production system: standardized processes, quality gates, just-in-time delivery, and sophisticated error recovery mechanisms.\n\n### Existing ETL Solutions\n\nThe ETL landscape today offers several mature platforms, each optimizing for different aspects of the assembly line metaphor. Understanding their design philosophies and trade-offs illuminates the architectural decisions our system must make.\n\n![System Component Overview](./diagrams/system-overview.svg)\n\n#### Apache Airflow: The Enterprise Standard\n\nApache Airflow dominates enterprise ETL orchestration by treating pipelines as **Directed Acyclic Graphs (DAGs) defined in Python code**. Airflow's strength lies in its comprehensive ecosystem and battle-tested scalability - it orchestrates workflows for companies processing petabytes daily across thousands of tasks.\n\n| Aspect | Strength | Limitation | Impact on Design |\n|--------|----------|------------|------------------|\n| DAG Definition | Python-native, familiar to data engineers | Code-heavy configuration, version control complexity | Need balance between code flexibility and declarative simplicity |\n| Task Execution | Robust parallelization, resource management | Heavy infrastructure requirements (Redis, Postgres) | Must consider deployment complexity vs. feature richness |\n| Failure Handling | Comprehensive retry policies, alerting integration | Complex debugging when failures cascade through dependencies | Error handling must be intuitive and provide clear diagnosis |\n| Extensibility | Rich plugin ecosystem, custom operators | Steep learning curve, opinionated architecture | Plugin system should be simple but powerful |\n\nAirflow's **scheduler-executor-webserver architecture** separates concerns cleanly but requires significant operational overhead. The scheduler parses DAGs, determines task readiness, and queues work. Executors (LocalExecutor, CeleryExecutor, KubernetesExecutor) handle the actual task execution with different scalability and isolation characteristics. The webserver provides monitoring and manual intervention capabilities.\n\nA critical Airflow insight is that **task definitions must be idempotent** because the scheduler may retry tasks multiple times due to infrastructure failures. This idempotency requirement permeates every aspect of pipeline design and significantly influences our transformation engine architecture.\n\n#### Dagster: The Developer Experience Focus\n\nDagster represents a newer generation of orchestration tools prioritizing **developer experience and data quality**. While Airflow focuses on workflow orchestration, Dagster emphasizes the data itself - assets, lineage, and validation.\n\n| Design Philosophy | Dagster Approach | Traditional ETL Approach | Architectural Implication |\n|-------------------|------------------|--------------------------|---------------------------|\n| Mental Model | Data assets with materialization logic | Tasks that happen to process data | Our system should be data-centric, not just task-centric |\n| Testing | Built-in unit testing for individual ops | Integration testing in staging environments | Need comprehensive testing framework from the start |\n| Type System | Strong typing with runtime validation | Schema-on-read with runtime failures | Type safety should be configurable, not mandatory |\n| Observability | Asset lineage and data quality metrics built-in | Monitoring added as afterthought | Lineage tracking should be first-class, not bolted on |\n\nDagster's **software-defined assets** concept treats data tables, files, and ML models as first-class citizens with explicit dependencies. Instead of thinking \"run this SQL query after that API call completes,\" you think \"materialize the customer_summary table when clean_customer_data and recent_orders assets are available.\" This abstraction level significantly improves reasoning about complex pipelines.\n\nThe Dagster **op (operation) and job** model separates reusable logic (ops) from specific pipeline definitions (jobs). This separation enables better testing - you can unit test individual ops with mock data, then compose them into jobs for integration testing. Our system should adopt similar composability principles.\n\n#### Custom Solutions: The Build vs. Buy Decision\n\nMany organizations build custom ETL solutions when existing tools don't match their specific requirements or constraints. Common drivers for custom development include:\n\n| Requirement | Why Existing Tools Fall Short | Custom Solution Advantage | Trade-off |\n|-------------|-------------------------------|---------------------------|-----------|\n| Extreme Performance | General-purpose tools have abstraction overhead | Optimize for specific data patterns and volumes | Development and maintenance cost vs. performance gain |\n| Legacy Integration | Existing tools don't support proprietary protocols | Direct integration with legacy systems | Technical debt vs. operational efficiency |\n| Regulatory Compliance | Standard tools may not meet audit requirements | Full control over security and logging | Compliance assurance vs. feature velocity |\n| Cost Optimization | Commercial tools expensive at scale | No licensing fees, cloud-optimized architecture | Development investment vs. operational savings |\n\nCustom solutions often excel in narrow domains but struggle with the **feature breadth** that mature platforms provide. A custom solution might perfectly handle your specific data transformation needs but lack comprehensive monitoring, alerting, user management, and operational tooling that enterprises require.\n\n> **The Build vs. Buy Decision Framework**: Choose custom development when the performance, integration, or cost advantages outweigh the opportunity cost of not investing in business logic development. The hidden costs of custom platforms include ongoing maintenance, feature development, security updates, and knowledge transfer as teams change.\n\n### Core Technical Challenges\n\nBuilding a production-grade ETL system requires solving fundamental distributed systems challenges while maintaining the developer experience that makes complex data workflows manageable. These challenges are not independent - decisions in one area create constraints and opportunities in others.\n\n#### Dependency Management: The Coordination Problem\n\nManaging task dependencies in distributed systems involves more complexity than simple precedence constraints. In our assembly line analogy, this is equivalent to coordinating multiple production lines with shared resources, variable processing times, and occasional equipment failures.\n\n**The DAG Parsing Challenge**: Unlike traditional job schedulers that work with simple precedence lists, ETL systems must parse potentially complex dependency graphs from various sources (YAML files, Python code, database configurations) and validate them for correctness before execution begins.\n\n| Parsing Requirement | Technical Challenge | Solution Approach | Failure Mode |\n|---------------------|--------------------|--------------------|--------------|\n| Cycle Detection | Must identify circular dependencies in potentially large graphs | Implement depth-first search with back-edge detection | False positives when parsing incremental updates |\n| Dynamic Dependencies | Task dependencies may depend on runtime data or external conditions | Support conditional dependencies with runtime resolution | Deadlocks when conditions create cycles |\n| Cross-Pipeline Dependencies | Tasks in different pipelines may depend on each other | Global dependency resolution with pipeline isolation | Cascading failures across pipeline boundaries |\n| Parameterized Dependencies | Template-based task definitions with variable substitution | Parse dependencies after parameter resolution | Invalid graphs when parameters create cycles |\n\n**The Topological Ordering Problem**: Once dependencies are validated, the system must determine execution order while maximizing parallelism. This is a classic topological sort problem, but ETL systems add complications:\n\n1. **Resource Constraints**: Not all ready tasks can execute simultaneously due to memory, CPU, or external system limits\n2. **Priority Scheduling**: Critical path tasks should execute before non-critical tasks when resources are limited  \n3. **Dynamic Rescheduling**: Task failures require recomputing execution order for remaining tasks\n4. **Partial Pipeline Execution**: Users often want to run subsets of pipelines, requiring dependency subgraph extraction\n\n> **Design Insight**: The dependency resolution system is the architectural foundation that determines system scalability. A naive implementation that recomputes the entire topological order after each task completion will become the bottleneck as pipeline complexity grows. The system must maintain incremental data structures that support efficient updates.\n\n**Distributed State Consistency**: In a multi-node deployment, task scheduling decisions must be coordinated across schedulers to prevent duplicate execution while maintaining high availability.\n\n| Consistency Model | Coordination Mechanism | Pros | Cons |\n|-------------------|------------------------|------|------|\n| Single Leader | One scheduler node makes all decisions | Simple, consistent, no conflicts | Single point of failure, scalability bottleneck |\n| Consensus-Based | Raft/Paxos for scheduling decisions | High availability, consistency guarantees | Complex implementation, network partition sensitivity |\n| Eventually Consistent | Distributed schedulers with conflict resolution | High availability, partition tolerance | Potential duplicate execution, complex recovery |\n| External Coordination | Use existing systems (etcd, Zookeeper) | Proven reliability, operational familiarity | External dependency, network latency overhead |\n\n#### Failure Recovery: Building Resilient Data Workflows\n\nData processing failures occur at multiple levels - infrastructure, network, application logic, and data quality. Unlike web applications where failed requests can be retried immediately, ETL failures often require sophisticated recovery strategies because of data consistency requirements and resource costs.\n\n**Failure Classification and Response Strategies**: Different failure types require different recovery approaches, and the system must automatically classify failures to apply appropriate remediation.\n\n| Failure Type | Detection Method | Recovery Strategy | Prevention Approach |\n|--------------|------------------|-------------------|---------------------|\n| Transient Network | Connection timeout, DNS resolution failure | Exponential backoff retry | Circuit breaker pattern, connection pooling |\n| Resource Exhaustion | Out of memory, disk full, CPU throttling | Queue task for later execution | Resource monitoring, preemptive scaling |\n| Data Quality Issues | Schema validation failure, constraint violations | Skip bad records with logging | Data profiling, upstream validation |\n| Logic Errors | Application exceptions, assertion failures | Human intervention required | Comprehensive testing, gradual rollouts |\n| Upstream System Unavailable | API rate limiting, database maintenance | Defer execution, alert operators | SLA monitoring, alternative data sources |\n\n**The Partial Failure Problem**: ETL tasks often process large datasets where partial completion is valuable. A task that successfully processes 80% of records before encountering corrupt data should preserve its progress rather than starting from scratch.\n\nConsider a task that extracts customer records from a REST API with pagination. The task successfully processes pages 1-8 but encounters a malformed response on page 9. The recovery options are:\n\n1. **Full Restart**: Discard all progress and restart from page 1 - simple but wasteful\n2. **Checkpoint Resume**: Save pagination state after each page and resume from page 9 - complex but efficient  \n3. **Partial Success**: Mark pages 1-8 as complete, fail page 9, and let dependent tasks proceed with available data - requires sophisticated dependency management\n\n> **Architectural Decision**: The system must support **checkpoint-based recovery** as a first-class concept. Tasks should be able to save intermediate state that enables resumption after failures. This requires careful API design to make checkpointing efficient without overwhelming task implementations with persistence concerns.\n\n**Cascading Failure Prevention**: When upstream tasks fail, downstream tasks must decide whether to skip execution, execute with partial data, or wait for upstream recovery. This decision depends on business requirements and data freshness constraints.\n\n| Downstream Strategy | When to Use | Implementation Complexity | Business Impact |\n|--------------------|--------------|-----------------------------|-----------------|\n| Fail Fast | Critical data dependencies, regulatory requirements | Low - propagate failure status | High - entire pipeline stops |\n| Partial Execution | Analytics workloads, non-critical reporting | Medium - handle missing data gracefully | Medium - reduced data quality |\n| Wait and Retry | Near real-time requirements, SLA commitments | High - timeout management, resource allocation | Low - maintains data quality |\n| Use Cached Data | Dashboard updates, periodic reports | Medium - cache invalidation logic | Variable - depends on data staleness tolerance |\n\n#### Data Consistency: Managing State Across Distributed Operations\n\nETL operations must maintain data consistency across multiple systems while providing reasonable performance. Unlike OLTP databases that handle small, frequent transactions, ETL systems deal with large bulk operations that may run for hours and interact with systems that don't support transactions.\n\n**The Two-Phase Load Problem**: Many ETL workflows follow an \"extract-transform-load\" pattern where the final load step must be atomic despite potentially loading data to multiple destination systems. Consider a pipeline that loads both a data warehouse and a search index - both destinations must be updated or neither should be updated.\n\nTraditional database transactions don't apply because:\n1. **Cross-System Boundaries**: Destinations may be different database types, file systems, or external APIs\n2. **Long-Running Operations**: Bulk loads may take hours, exceeding reasonable transaction timeouts\n3. **Resource Constraints**: Holding locks during large operations blocks other concurrent workflows\n\n**Idempotency Requirements**: Since the system may retry failed operations, all transformations and loads must produce the same result when executed multiple times. This requirement significantly influences API design and data modeling.\n\n| Operation Type | Idempotency Challenge | Solution Pattern | Implementation Notes |\n|----------------|----------------------|-------------------|---------------------|\n| Incremental Loads | Duplicate records on retry | Upsert with deterministic keys | Requires unique key identification |\n| Aggregations | Double-counting on retry | Checkpoint aggregation state | Complex with streaming data |\n| File Operations | Partial writes on failure | Atomic rename operations | Requires staging area |\n| API Calls | Side effects on retry | Idempotency tokens | Requires upstream system support |\n\n**Schema Evolution Management**: Production ETL systems must handle schema changes in source systems without breaking downstream consumers. This is particularly challenging because schema changes often occur without coordination between teams.\n\nThe schema evolution problem manifests in several ways:\n- **Additive Changes**: New columns in source tables may break transformations that use `SELECT *`\n- **Breaking Changes**: Removed or renamed columns cause immediate pipeline failures  \n- **Type Changes**: Column type modifications may cause silent data truncation or conversion errors\n- **Constraint Changes**: New validation rules may reject previously acceptable data\n\n> **Design Principle**: The system should implement **schema versioning** with backward compatibility policies. Each data asset should declare its expected schema version, and the system should validate compatibility before execution and provide automatic migration paths for compatible changes.\n\n#### Scalability Challenges: Performance and Resource Management\n\nETL systems face unique scalability challenges because workloads are often **bursty, resource-intensive, and deadline-driven**. Unlike web applications with relatively predictable load patterns, ETL systems may need to scale from processing megabytes during quiet periods to terabytes during month-end batch jobs.\n\n**Resource Allocation and Queueing**: The system must efficiently allocate compute, memory, and I/O resources across concurrent tasks while respecting priority constraints and resource limits.\n\n| Resource Type | Constraint Pattern | Scaling Strategy | Monitoring Approach |\n|---------------|-------------------|-------------------|---------------------|\n| CPU | Compute-intensive transformations | Horizontal scaling, distributed processing | CPU utilization per task type |\n| Memory | Large dataset operations, caching | Memory limits per task, spill-to-disk | Memory high-water marks |\n| I/O | Database connections, file system throughput | Connection pooling, async operations | Queue depths, latency percentiles |\n| Network | Cross-region data movement | Compression, regional caching | Bandwidth utilization, transfer costs |\n\n**The Thundering Herd Problem**: When upstream data becomes available, multiple downstream tasks may become eligible for execution simultaneously. Without careful resource management, this can overwhelm the execution infrastructure and cause cascading failures.\n\nConsider a nightly batch job that loads customer transaction data. When the load completes, dozens of downstream analytics tasks become ready - customer segmentation, fraud detection, reporting aggregations, and ML feature generation. If all these tasks start immediately, they may:\n- Overwhelm the source database with concurrent queries\n- Exhaust memory by loading overlapping datasets\n- Saturate network bandwidth and slow each other down\n- Cause resource contention that degrades overall throughput\n\n**Data Locality and Transfer Costs**: Large-scale ETL systems must consider data gravity - the tendency for computation to move toward data rather than vice versa. Moving terabytes across network boundaries is expensive in both time and money.\n\n| Data Movement Pattern | Cost Factor | Optimization Strategy | Trade-off |\n|-----------------------|-------------|----------------------|-----------|\n| Cross-Region Transfer | Network egress charges, latency | Regional data replication | Storage cost vs. transfer cost |\n| Cross-Cloud Transfer | Bandwidth costs, vendor lock-in | Multi-cloud data mesh | Complexity vs. flexibility |\n| On-Premise to Cloud | Internet bandwidth limits | Incremental sync, compression | Migration time vs. operational cost |\n| Database to Processing | Query overhead, connection limits | Result caching, read replicas | Freshness vs. performance |\n\n> **Scalability Design Principle**: The system architecture must support **elastic scaling** that automatically adjusts resource allocation based on pipeline demand and data volume patterns. This requires sophisticated workload prediction and resource provisioning that considers both current demand and scheduled pipeline execution.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\nThe architecture described above can be implemented using various technology stacks. The choice depends on organizational constraints, performance requirements, and operational preferences.\n\n| Component | Simple Option | Advanced Option | Enterprise Option |\n|-----------|---------------|------------------|-------------------|\n| Pipeline Definition | YAML configuration files | Python-based DSL (Airflow-style) | Visual pipeline builder with code generation |\n| Task Execution | Process-based execution | Container orchestration (Docker) | Kubernetes jobs with resource isolation |\n| State Management | SQLite with WAL mode | PostgreSQL with connection pooling | Distributed database (CockroachDB, Spanner) |\n| Message Queue | In-memory queues | Redis with persistence | Apache Kafka or AWS SQS |\n| Monitoring | File-based logging | Structured logging (ELK stack) | APM integration (Datadog, New Relic) |\n| Failure Recovery | Simple retry with backoff | Dead letter queues | Circuit breakers with health checks |\n\n#### Core Architecture Patterns\n\n**Repository Pattern for Pipeline Definitions**: Abstracts pipeline storage and versioning to support multiple configuration sources.\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass PipelineDefinition:\n    \"\"\"Core pipeline definition with metadata and task specifications.\"\"\"\n    id: str\n    name: str\n    description: str\n    schedule: str  # cron expression\n    tasks: List['TaskDefinition']\n    parameters: dict\n    created_at: datetime\n    version: int\n\n@dataclass  \nclass TaskDefinition:\n    \"\"\"Individual task within a pipeline with dependencies and configuration.\"\"\"\n    id: str\n    name: str\n    type: str  # extract, transform, load\n    config: dict\n    dependencies: List[str]  # upstream task IDs\n    retry_policy: 'RetryPolicy'\n    timeout_seconds: int\n\n@dataclass\nclass RetryPolicy:\n    \"\"\"Configurable retry behavior for task failures.\"\"\"\n    max_attempts: int\n    backoff_seconds: int\n    exponential_backoff: bool\n    retry_on_error_types: List[str]\n\nclass PipelineRepository(ABC):\n    \"\"\"Abstract interface for pipeline definition storage and retrieval.\"\"\"\n    \n    @abstractmethod\n    def get_pipeline(self, pipeline_id: str) -> Optional[PipelineDefinition]:\n        \"\"\"Retrieve pipeline definition by ID.\"\"\"\n        pass\n    \n    @abstractmethod\n    def list_pipelines(self) -> List[PipelineDefinition]:\n        \"\"\"List all available pipeline definitions.\"\"\"\n        pass\n    \n    @abstractmethod\n    def save_pipeline(self, pipeline: PipelineDefinition) -> None:\n        \"\"\"Persist pipeline definition with version increment.\"\"\"\n        pass\n    \n    @abstractmethod\n    def validate_pipeline(self, pipeline: PipelineDefinition) -> List[str]:\n        \"\"\"Validate pipeline definition and return error messages.\"\"\"\n        pass\n\nclass YamlPipelineRepository(PipelineRepository):\n    \"\"\"File-based pipeline repository using YAML configuration files.\"\"\"\n    \n    def __init__(self, config_directory: str):\n        self.config_directory = config_directory\n        # TODO: Initialize YAML parser and file system monitoring\n        # TODO: Implement configuration file validation schema\n        # TODO: Set up file watcher for automatic reload on changes\n    \n    def get_pipeline(self, pipeline_id: str) -> Optional[PipelineDefinition]:\n        # TODO: Load YAML file for specified pipeline ID\n        # TODO: Parse YAML content into PipelineDefinition object\n        # TODO: Validate task dependency references within pipeline\n        # TODO: Return None if file not found or parsing fails\n        pass\n    \n    def validate_pipeline(self, pipeline: PipelineDefinition) -> List[str]:\n        # TODO: Check for circular dependencies in task graph\n        # TODO: Validate task configuration schemas by type\n        # TODO: Ensure all dependency references point to valid tasks\n        # TODO: Validate cron schedule expression syntax\n        # TODO: Check for duplicate task IDs within pipeline\n        pass\n```\n\n**State Machine Pattern for Task Execution**: Manages task lifecycle transitions with clear state boundaries and event handling.\n\n```python\nfrom enum import Enum\nfrom typing import Dict, Callable, Optional\nimport logging\n\nclass TaskState(Enum):\n    \"\"\"Enumeration of all possible task execution states.\"\"\"\n    PENDING = \"pending\"\n    WAITING = \"waiting\"      # waiting for dependencies\n    QUEUED = \"queued\"        # ready to execute\n    RUNNING = \"running\"\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    RETRYING = \"retrying\"\n    CANCELLED = \"cancelled\"\n    SKIPPED = \"skipped\"      # skipped due to upstream failure\n\nclass TaskEvent(Enum):\n    \"\"\"Events that trigger task state transitions.\"\"\"\n    DEPENDENCIES_MET = \"dependencies_met\"\n    EXECUTION_STARTED = \"execution_started\"\n    EXECUTION_COMPLETED = \"execution_completed\"\n    EXECUTION_FAILED = \"execution_failed\"\n    RETRY_SCHEDULED = \"retry_scheduled\"\n    MAX_RETRIES_EXCEEDED = \"max_retries_exceeded\"\n    CANCELLED_BY_USER = \"cancelled_by_user\"\n    UPSTREAM_FAILED = \"upstream_failed\"\n\n@dataclass\nclass TaskExecution:\n    \"\"\"Runtime state for individual task execution.\"\"\"\n    task_id: str\n    pipeline_run_id: str\n    state: TaskState\n    attempt_count: int\n    started_at: Optional[datetime]\n    completed_at: Optional[datetime]\n    error_message: Optional[str]\n    logs: List[str]\n    metrics: Dict[str, float]\n\nclass TaskStateMachine:\n    \"\"\"Manages task state transitions and enforces valid state changes.\"\"\"\n    \n    # Valid state transitions - current_state -> {event -> next_state}\n    TRANSITIONS: Dict[TaskState, Dict[TaskEvent, TaskState]] = {\n        TaskState.PENDING: {\n            TaskEvent.DEPENDENCIES_MET: TaskState.QUEUED,\n            TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED,\n            TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED\n        },\n        TaskState.QUEUED: {\n            TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n            TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED\n        },\n        TaskState.RUNNING: {\n            TaskEvent.EXECUTION_COMPLETED: TaskState.SUCCESS,\n            TaskEvent.EXECUTION_FAILED: TaskState.FAILED,\n            TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED\n        },\n        TaskState.FAILED: {\n            TaskEvent.RETRY_SCHEDULED: TaskState.RETRYING,\n            TaskEvent.MAX_RETRIES_EXCEEDED: TaskState.FAILED\n        },\n        TaskState.RETRYING: {\n            TaskEvent.EXECUTION_STARTED: TaskState.RUNNING\n        }\n    }\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        # TODO: Initialize state transition hooks for monitoring\n        # TODO: Set up metrics collection for state change events\n        # TODO: Configure audit logging for compliance requirements\n    \n    def transition(self, execution: TaskExecution, event: TaskEvent, \n                  error_message: Optional[str] = None) -> bool:\n        \"\"\"\n        Attempt to transition task to new state based on event.\n        Returns True if transition was successful, False otherwise.\n        \"\"\"\n        # TODO: Validate current state allows this event transition\n        # TODO: Update execution object with new state and metadata\n        # TODO: Log state transition with timestamp and context\n        # TODO: Trigger any registered state change hooks\n        # TODO: Update metrics counters for monitoring dashboard\n        # TODO: Return False if transition is invalid for current state\n        pass\n    \n    def get_valid_events(self, current_state: TaskState) -> List[TaskEvent]:\n        \"\"\"Return list of events that can be applied to current state.\"\"\"\n        # TODO: Look up valid transitions from TRANSITIONS mapping\n        # TODO: Return empty list if state has no valid transitions\n        pass\n```\n\n**Plugin System for Extensible Connectors**: Enables adding new data sources and destinations without modifying core system.\n\n```python\nclass DataConnector(ABC):\n    \"\"\"Base class for all data source and destination connectors.\"\"\"\n    \n    @abstractmethod\n    def connect(self, config: dict) -> 'Connection':\n        \"\"\"Establish connection to data source using provided configuration.\"\"\"\n        pass\n    \n    @abstractmethod\n    def extract(self, connection: 'Connection', query: dict) -> 'DataStream':\n        \"\"\"Extract data from source system based on query parameters.\"\"\"\n        pass\n    \n    @abstractmethod\n    def load(self, connection: 'Connection', data: 'DataStream', \n             target: str) -> 'LoadResult':\n        \"\"\"Load data stream to target destination.\"\"\"\n        pass\n    \n    @abstractmethod\n    def validate_config(self, config: dict) -> List[str]:\n        \"\"\"Validate connector configuration and return error messages.\"\"\"\n        pass\n\nclass DatabaseConnector(DataConnector):\n    \"\"\"Generic database connector supporting SQL-based sources.\"\"\"\n    \n    def __init__(self, driver_name: str):\n        self.driver_name = driver_name\n        # TODO: Initialize database driver and connection pooling\n        # TODO: Set up query timeout and retry configuration\n        # TODO: Configure connection health checking\n    \n    def extract(self, connection: 'Connection', query: dict) -> 'DataStream':\n        # TODO: Build SQL query from query parameters\n        # TODO: Execute query with cursor-based pagination for large results\n        # TODO: Handle connection failures with automatic retry\n        # TODO: Stream results to avoid memory exhaustion\n        # TODO: Track extraction metrics (rows processed, query time)\n        pass\n    \n    def load(self, connection: 'Connection', data: 'DataStream', \n             target: str) -> 'LoadResult':\n        # TODO: Begin database transaction for atomicity\n        # TODO: Batch insert records for optimal performance\n        # TODO: Handle constraint violations and data type errors\n        # TODO: Commit transaction only after all batches succeed\n        # TODO: Record load statistics and performance metrics\n        pass\n\nclass ConnectorRegistry:\n    \"\"\"Registry for discovering and instantiating data connectors.\"\"\"\n    \n    def __init__(self):\n        self._connectors: Dict[str, type] = {}\n        # TODO: Scan for connector implementations in plugin directories\n        # TODO: Validate connector implementations match interface\n        # TODO: Set up hot-reload capability for development environments\n    \n    def register_connector(self, name: str, connector_class: type) -> None:\n        # TODO: Validate connector_class implements DataConnector interface\n        # TODO: Store connector in registry with name as key\n        # TODO: Log connector registration for debugging\n        pass\n    \n    def get_connector(self, name: str, config: dict) -> DataConnector:\n        # TODO: Look up connector class by name in registry\n        # TODO: Instantiate connector with provided configuration  \n        # TODO: Validate configuration before returning instance\n        # TODO: Raise descriptive error if connector not found\n        pass\n```\n\n#### Development Environment Setup\n\n**Recommended Project Structure:**\n```\netl-pipeline-system/\n src/\n    core/\n       __init__.py\n       pipeline.py          # PipelineDefinition, TaskDefinition models\n       repository.py        # PipelineRepository interface\n       state_machine.py     # TaskStateMachine implementation\n       executor.py          # Task execution engine\n    connectors/\n       __init__.py\n       base.py              # DataConnector abstract base class\n       database.py          # DatabaseConnector implementation\n       file_system.py       # File-based extraction/loading\n       rest_api.py          # HTTP REST API connector\n    scheduling/\n       __init__.py\n       scheduler.py         # Cron-based pipeline scheduling\n       dag_parser.py        # DAG validation and topological sorting\n       dependency_resolver.py  # Task dependency management\n    monitoring/\n       __init__.py\n       metrics.py           # Performance metrics collection\n       logging.py           # Structured logging configuration\n       alerting.py          # Failure notification system\n    web/\n        __init__.py\n        api.py               # REST API for pipeline management\n        dashboard.py         # Web dashboard for monitoring\n        templates/           # HTML templates for UI\n tests/\n    unit/                    # Component-specific unit tests\n    integration/             # End-to-end pipeline tests\n    fixtures/                # Test data and mock configurations\n config/\n    pipelines/               # Pipeline definition YAML files\n    connectors.yaml          # Data source connection configs\n    scheduler.yaml           # Scheduling and retry policies\n docs/\n    api/                     # API documentation\n    connectors/              # Connector development guides\n    deployment/              # Operational deployment guides\n requirements.txt             # Python dependencies\n setup.py                     # Package installation configuration\n README.md                    # Quick start and architecture overview\n```\n\n#### Language-Specific Implementation Notes\n\n**Python Dependency Management**: Use `requirements.txt` for development and `setup.py` for distribution. Pin exact versions for reproducible builds.\n\n**Concurrency Patterns**: Leverage `asyncio` for I/O-bound operations (database queries, API calls) and `multiprocessing` for CPU-bound transformations. Use `concurrent.futures.ThreadPoolExecutor` for mixed workloads.\n\n**Configuration Management**: Use `pydantic` for configuration validation with automatic type conversion and detailed error messages. Support both environment variables and configuration files.\n\n**Logging Best Practices**: Configure structured logging with `python-json-logger` for machine-readable logs. Include correlation IDs to trace requests across distributed components.\n\n**Error Handling Patterns**: Create custom exception hierarchies for different failure types (transient vs. permanent). Use `tenacity` library for declarative retry policies with exponential backoff.\n\n#### Milestone Checkpoints\n\n**Context Validation Checkpoint**: After reading this section, you should be able to:\n\n1. **Explain the Assembly Line Analogy**: Describe how ETL pipelines resemble manufacturing processes and why dependency management is the central challenge.\n\n2. **Compare ETL Platforms**: Create a comparison table showing when to choose Airflow vs. Dagster vs. custom development based on specific requirements.\n\n3. **Identify Technical Challenges**: Given a data processing scenario, identify which of the four core challenges (dependency management, failure recovery, data consistency, scalability) apply and why.\n\n4. **Design Trade-off Analysis**: For any architectural decision, articulate the options considered, decision made, rationale, and consequences using the ADR format.\n\n**Verification Steps**:\n- Draw a simple ETL pipeline as a DAG and identify potential failure points\n- Write a brief architectural decision record for choosing between different state management approaches\n- List the key differences between task-centric (Airflow) and data-centric (Dagster) pipeline models\n- Explain why idempotency is crucial for ETL operations with specific examples\n\n**Common Issues at This Stage**:\n- **Symptom**: Overwhelmed by the complexity of existing ETL tools\n- **Cause**: Trying to understand implementation details before architectural concepts\n- **Fix**: Focus on the mental models and analogies first, then gradually add technical detail\n\n- **Symptom**: Unclear about build vs. buy decision criteria\n- **Cause**: Not considering total cost of ownership including maintenance and feature development\n- **Fix**: Create a decision matrix weighing development cost, operational overhead, and strategic flexibility\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** Foundation for all milestones - defines system boundaries and requirements that guide implementation decisions across pipeline definition, data processing, and orchestration features.\n\n### Mental Model: Project Charter\n\nThink of this goals section as a **project charter for a construction project**. Before breaking ground on a skyscraper, architects and stakeholders must agree on fundamental questions: How many floors? What's the budget? Will it have a parking garage? What about a helicopter pad? The charter explicitly states \"we're building a 40-story office building with underground parking\" and equally importantly \"we're NOT building a shopping mall or residential units.\" This prevents scope creep and ensures everyone builds toward the same vision.\n\nSimilarly, our ETL system goals define both what we're building and what we're deliberately not building. Without clear boundaries, an ETL system could expand infinitely - real-time streaming, machine learning pipelines, data lakes, visualization tools, user management systems. The goals act as guardrails that keep the architecture focused and implementable within reasonable complexity bounds.\n\n### Functional Goals\n\nThe functional goals define the core capabilities our ETL system must deliver to users. These represent the essential features that justify the system's existence and differentiate it from simple scripts or manual processes.\n\n**Pipeline Definition and Management**\n\nOur system must enable users to define complex data workflows as directed acyclic graphs (DAGs) where tasks have dependencies on other tasks. Users should be able to express these dependencies declaratively using configuration files rather than writing procedural code. The system must support both YAML and Python configuration formats to accommodate different user preferences and complexity levels.\n\nThe pipeline definition capability includes parameter substitution, allowing users to create reusable pipeline templates that can be instantiated with different configuration values. For example, a data ingestion pipeline template might accept database connection parameters and table names as runtime variables, enabling the same pipeline logic to process different data sources.\n\n> **Decision: Declarative Pipeline Definition**\n> - **Context**: Users need to define complex data workflows with task dependencies. Options include procedural code, visual designers, or declarative configuration.\n> - **Options Considered**: Pure Python code, GUI-based visual editor, YAML/JSON configuration files\n> - **Decision**: Declarative YAML/Python configuration with programmatic task definition support\n> - **Rationale**: Declarative formats enable version control, code review, and automated validation. Visual editors create vendor lock-in and are difficult to version control. Pure code provides flexibility but requires programming expertise.\n> - **Consequences**: Enables infrastructure-as-code practices and makes pipelines reviewable by non-programmers, but requires building a custom DSL and validation layer.\n\n| Feature | Description | User Benefit |\n|---------|-------------|--------------|\n| DAG Definition | Define task dependencies as directed acyclic graphs | Ensures proper execution order and enables parallel processing |\n| Parameter Substitution | Runtime variable replacement in configurations | Creates reusable pipeline templates for different environments |\n| Multi-format Support | Both YAML and Python configuration options | Accommodates different user skill levels and complexity needs |\n| Dependency Validation | Automatic cycle detection and dependency verification | Prevents invalid pipeline definitions before execution |\n\n**Data Extraction and Loading**\n\nThe system must provide robust connectors for extracting data from common enterprise data sources including relational databases, REST APIs, and file systems. These connectors must handle authentication, connection pooling, and error recovery automatically while exposing simple configuration interfaces to users.\n\nFor databases, the extraction capability must support both full table dumps and incremental loading strategies. Incremental loading uses techniques like watermarking (tracking the maximum timestamp or ID processed) and change data capture to extract only new or modified records since the last pipeline run. This dramatically reduces processing time and resource usage for large datasets.\n\nLoading capabilities must support bulk insertion strategies that optimize throughput for target systems. The system should automatically batch records, use appropriate bulk loading APIs (like PostgreSQL's COPY command), and handle schema mapping between source and destination systems when column names or types differ.\n\n| Connector Type | Extraction Features | Loading Features | Incremental Support |\n|----------------|-------------------|------------------|-------------------|\n| Database | SQL query execution, connection pooling, authentication | Bulk inserts, upserts, schema mapping | Watermark-based, CDC integration |\n| REST API | Pagination, rate limiting, retry logic, authentication | HTTP POST/PUT with batching | Cursor-based pagination, timestamp filtering |\n| File System | Multiple formats (CSV, JSON, Parquet), compression | Atomic writes, partitioning, compression | File modification time, filename patterns |\n\n**Data Transformation and Validation**\n\nThe transformation engine must support both SQL-based transformations for users familiar with declarative data manipulation and Python user-defined functions (UDFs) for custom business logic that cannot be expressed in SQL. SQL transformations should support templating to inject runtime parameters and enable reusable transformation logic.\n\nData validation capabilities must enforce schema constraints, business rules, and data quality checks during the transformation process. The system should support configurable validation strategies - some pipelines may reject invalid records entirely, while others may flag invalid records for manual review but continue processing valid data.\n\nSchema evolution handling is critical for production systems where source data structures change over time. The transformation engine must detect schema changes, apply configurable evolution strategies (like adding default values for new columns), and maintain backward compatibility with existing pipeline definitions.\n\n> **Decision: Hybrid SQL and Python Transformation Support**\n> - **Context**: Users have varying technical backgrounds and transformation complexity requirements. Some transformations are simple aggregations while others require complex business logic.\n> - **Options Considered**: SQL-only, Python-only, or hybrid approach supporting both\n> - **Decision**: Support both SQL and Python transformations with seamless integration\n> - **Rationale**: SQL handles 80% of common transformations efficiently and is familiar to analysts. Python provides flexibility for complex logic and integration with external libraries.\n> - **Consequences**: Increases implementation complexity but maximizes user adoption by supporting different skill sets and use cases.\n\n**Pipeline Orchestration and Scheduling**\n\nThe orchestration engine must execute pipelines according to user-defined schedules using standard cron expressions for time-based triggers. Additionally, the system must support event-driven execution where pipelines trigger automatically when upstream data sources change or external events occur.\n\nTask execution must handle parallelism intelligently, running independent tasks concurrently while respecting dependency constraints. The system should provide configurable resource limits to prevent any single pipeline from overwhelming the execution environment.\n\nFailure handling capabilities must include configurable retry policies with exponential backoff, dead letter queues for persistently failing tasks, and alerting integration to notify operators when manual intervention is required. The system must maintain detailed execution history and provide log aggregation for debugging failed pipeline runs.\n\n### Non-Functional Goals\n\nNon-functional goals define the quality attributes and operational characteristics the system must exhibit in production environments. These requirements significantly influence architectural decisions and implementation approaches.\n\n**Performance and Scalability**\n\nThe system must process datasets ranging from thousands to millions of records efficiently. Pipeline execution overhead should remain minimal - a simple two-task pipeline should complete within 30 seconds of trigger time, with most overhead attributed to actual data processing rather than orchestration.\n\nTask parallelism must scale to utilize available CPU cores effectively. On a 4-core development machine, four independent tasks should execute concurrently. The architecture must support horizontal scaling where additional worker machines can be added to increase overall processing capacity.\n\nMemory usage must remain bounded even for large datasets. The system should use streaming and batching strategies to process datasets larger than available RAM. A pipeline processing a 1GB dataset should not require more than 256MB of heap memory at peak usage.\n\n| Performance Metric | Target | Measurement Method |\n|-------------------|--------|-------------------|\n| Pipeline Startup Overhead | < 30 seconds for simple pipelines | Time from trigger to first task execution |\n| Task Parallelism | Utilize all available CPU cores | Concurrent task execution count |\n| Memory Efficiency | Process 4x data size in available RAM | Peak memory usage vs dataset size |\n| Throughput | 10,000 records/second for simple transformations | Records processed per unit time |\n\n**Reliability and Availability**\n\nThe system must handle infrastructure failures gracefully without losing data or leaving pipelines in inconsistent states. Task execution must be idempotent - running the same pipeline multiple times should produce identical results without negative side effects. This enables safe retries and recovery from partial failures.\n\nState persistence must survive process restarts and machine failures. Pipeline execution state, task progress, and metadata must be stored in durable storage with appropriate backup and recovery procedures. The system should resume interrupted pipelines from the last successful checkpoint rather than restarting from the beginning.\n\nError recovery must be automatic for transient failures like network timeouts or temporary resource unavailability. Only persistent errors that require human intervention should halt pipeline execution and trigger alerts.\n\n> **Decision: Checkpoint-Based Recovery Strategy**\n> - **Context**: Pipelines may fail partway through execution due to infrastructure issues, and restarting from the beginning wastes computational resources.\n> - **Options Considered**: No recovery (restart from beginning), task-level checkpointing, operation-level checkpointing\n> - **Decision**: Task-level checkpointing with idempotent operation design\n> - **Rationale**: Task-level granularity provides good balance between implementation complexity and recovery efficiency. Operation-level checkpointing adds significant complexity for marginal benefit.\n> - **Consequences**: Failed pipelines can resume from the last completed task, but individual tasks must be designed to be idempotent and handle partial state.\n\n**Monitoring and Observability**\n\nThe system must provide comprehensive visibility into pipeline execution through metrics, logs, and tracing. Users should be able to answer questions like \"Why did my pipeline fail?\" and \"Which task is the bottleneck?\" without accessing raw log files or debugging tools.\n\nReal-time monitoring must track pipeline run status, task execution progress, and system resource utilization. Historical metrics should enable trend analysis to identify performance degradation or capacity planning needs. The monitoring system must integrate with standard observability tools like Prometheus, Grafana, or cloud-native monitoring services.\n\nData lineage tracking must capture the provenance of every dataset - which source systems contributed data, what transformations were applied, and when the processing occurred. This lineage information supports compliance requirements, debugging, and impact analysis when upstream data sources change.\n\n| Observability Feature | Information Provided | Use Case |\n|----------------------|---------------------|----------|\n| Real-time Status | Current pipeline and task execution state | Operations monitoring and alerting |\n| Performance Metrics | Execution time, throughput, resource usage | Performance optimization and capacity planning |\n| Data Lineage | Source-to-destination data flow tracking | Compliance, debugging, impact analysis |\n| Audit Logs | User actions and system changes | Security auditing and change tracking |\n\n### Explicit Non-Goals\n\nExplicitly defining what the system will not do is equally important as defining what it will do. These non-goals prevent scope creep and help users understand the system's boundaries and integration requirements.\n\n**Real-Time Stream Processing**\n\nThis ETL system focuses on batch processing workflows and does not attempt to provide real-time stream processing capabilities. Users requiring sub-second data processing latency should integrate with dedicated streaming platforms like Apache Kafka, Apache Flink, or cloud streaming services.\n\nThe system's scheduling granularity targets minute-level intervals rather than millisecond-level responsiveness. While the system supports event-driven pipeline triggers, these events are expected to be coarse-grained notifications (like \"new file arrived\" or \"database sync completed\") rather than individual record-level events.\n\nThis architectural decision significantly simplifies the system design by avoiding the complexity of stream processing semantics, windowing, watermarking, and exactly-once processing guarantees that streaming systems require.\n\n**Data Storage and Serving**\n\nThe ETL system acts as a data processing orchestrator rather than a data storage platform. Users must provide their own source and destination systems - the ETL system does not include embedded databases, data lakes, or serving layers.\n\nThis boundary means the system does not handle data modeling, query optimization, or end-user data access patterns. Users requiring these capabilities must integrate with appropriate storage solutions like data warehouses, operational databases, or analytical platforms.\n\nThe system focuses on the \"T\" (Transform) and orchestration aspects of ETL while delegating the \"E\" (Extract) and \"L\" (Load) to specialized connectors that interface with existing storage systems.\n\n**Visual Pipeline Designer**\n\nWhile the system provides pipeline visualization for monitoring and debugging purposes, it does not include a drag-and-drop visual pipeline designer or GUI-based pipeline authoring tools. Pipeline definitions must be created using configuration files or programmatic APIs.\n\nThis decision prioritizes version control, code review, and infrastructure-as-code practices over visual ease-of-use. Visual pipeline designers often create vendor lock-in and make it difficult to apply software engineering best practices to pipeline definitions.\n\nUsers requiring visual authoring capabilities should use external tools that can generate compatible pipeline configurations or integrate with the system's programmatic APIs.\n\n**Machine Learning and Advanced Analytics**\n\nThe system provides general-purpose data transformation capabilities but does not include specialized machine learning features like model training, inference, or ML-specific data preprocessing operations. Users requiring these capabilities should integrate with dedicated ML platforms or data science tools.\n\nThis boundary prevents the system from becoming an overly complex platform that tries to solve every data-related problem. By focusing on general ETL orchestration, the system can integrate effectively with specialized tools rather than competing with them.\n\n> **Decision: Integration-First Architecture Over Platform Expansion**\n> - **Context**: Data ecosystems require many specialized tools (streaming, ML, storage, visualization). The system could try to include these capabilities or focus on integration.\n> - **Options Considered**: Build an all-in-one data platform, focus purely on ETL orchestration, hybrid approach with some built-in capabilities\n> - **Decision**: Focus on ETL orchestration with excellent integration capabilities\n> - **Rationale**: All-in-one platforms become bloated and cannot compete with specialized tools. Integration-first design enables users to choose best-of-breed tools for each use case.\n> - **Consequences**: Users must manage multiple tools but can optimize each component. System remains focused and maintainable.\n\n**Multi-Tenancy and User Management**\n\nThe system does not provide built-in user authentication, authorization, or multi-tenant isolation capabilities. Deployment security and access control must be handled by the surrounding infrastructure using standard practices like network isolation, service authentication, and infrastructure-level access controls.\n\nThis decision reduces system complexity and allows organizations to integrate with their existing identity and access management solutions rather than learning yet another authentication system. The system assumes it operates in a trusted environment where access control is enforced at the infrastructure layer.\n\n**High-Frequency Trading or Financial Data**\n\nWhile the system can process financial datasets, it does not provide specialized features for high-frequency trading, market data processing, or real-time risk management that require microsecond-level latency and specialized financial protocols.\n\nThe system's reliability and consistency guarantees target business intelligence and analytical use cases where eventual consistency and minute-level latency are acceptable, rather than trading systems where milliseconds matter and regulatory compliance requires specialized audit trails.\n\n| Non-Goal Category | What We Don't Build | Recommended Alternatives |\n|-------------------|-------------------|------------------------|\n| Stream Processing | Real-time event processing, windowing, exactly-once semantics | Apache Kafka, Flink, cloud streaming services |\n| Data Storage | Embedded databases, data lakes, query engines | PostgreSQL, Snowflake, BigQuery, S3/HDFS |\n| Visual Authoring | Drag-and-drop pipeline designer, GUI configuration | External tools that generate configurations |\n| Machine Learning | Model training, inference, ML-specific preprocessing | MLflow, Kubeflow, cloud ML platforms |\n| User Management | Authentication, authorization, multi-tenancy | Infrastructure-level access controls, identity providers |\n| Financial Systems | High-frequency trading, microsecond latency, specialized compliance | Dedicated financial data platforms |\n\n### Implementation Guidance\n\nThis implementation guidance helps translate the functional and non-functional goals into concrete technical decisions and development practices.\n\n**Technology Recommendations**\n\nThe following technology choices support the goals defined above while balancing simplicity for development with production-readiness for deployment:\n\n| Component | Simple Option | Advanced Option | Goal Alignment |\n|-----------|---------------|-----------------|----------------|\n| Configuration Format | YAML with JSON Schema validation | Python-based DSL with type checking | Supports declarative pipeline definition goal |\n| Task Execution | Thread pool with concurrent.futures | Distributed task queue (Celery, RQ) | Enables parallelism and scalability goals |\n| State Storage | SQLite for development | PostgreSQL for production | Provides reliability and state persistence |\n| Logging | Python logging with file handlers | Structured logging with centralized collection | Supports observability and monitoring goals |\n| Scheduling | APScheduler for cron support | Integration with external schedulers (Airflow, etc.) | Enables time-based and event-driven execution |\n| Monitoring | Simple metrics collection | Prometheus + Grafana integration | Provides production-ready observability |\n\n**Recommended Project Structure**\n\nOrganize the codebase to support the modular architecture implied by our functional goals:\n\n```\netl-pipeline/\n src/\n    pipeline/\n       __init__.py\n       definition.py          # PipelineDefinition, TaskDefinition classes\n       validation.py          # Pipeline validation logic\n       serialization.py       # YAML/JSON config parsing\n    execution/\n       __init__.py\n       scheduler.py           # Schedule management and triggering\n       executor.py            # Task execution engine\n       state_machine.py       # TaskState transitions\n    connectors/\n       __init__.py\n       base.py               # Abstract connector interfaces\n       database.py           # Database extraction/loading\n       api.py                # REST API connectors\n       filesystem.py         # File-based connectors\n    transforms/\n       __init__.py\n       sql_transform.py      # SQL-based transformations\n       python_transform.py   # Python UDF support\n    monitoring/\n        __init__.py\n        metrics.py            # Performance and execution metrics\n        logging.py            # Log aggregation and formatting\n        lineage.py            # Data lineage tracking\n config/\n    pipeline_schemas/         # JSON schemas for pipeline validation\n    examples/                 # Example pipeline definitions\n tests/\n    unit/                    # Component-level unit tests\n    integration/             # End-to-end pipeline tests\n    fixtures/                # Test data and mock configurations\n docs/\n     user_guide.md            # Pipeline authoring documentation\n     api_reference.md         # Programmatic API documentation\n```\n\n**Core Data Structures Starter Code**\n\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Optional, Any\nfrom enum import Enum\n\n@dataclass\nclass RetryPolicy:\n    \"\"\"Defines retry behavior for failed task executions.\"\"\"\n    max_attempts: int\n    backoff_seconds: int\n    exponential_backoff: bool\n    retry_on_error_types: List[str]\n\n@dataclass\nclass TaskDefinition:\n    \"\"\"Defines a single task within a pipeline DAG.\"\"\"\n    id: str\n    name: str\n    type: str  # 'extract', 'transform', 'load'\n    config: Dict[str, Any]\n    dependencies: List[str]  # Task IDs this task depends on\n    retry_policy: RetryPolicy\n    timeout_seconds: int\n\n@dataclass\nclass PipelineDefinition:\n    \"\"\"Complete pipeline specification with metadata and task definitions.\"\"\"\n    id: str\n    name: str\n    description: str\n    schedule: str  # Cron expression or event trigger\n    tasks: List[TaskDefinition]\n    parameters: Dict[str, Any]  # Runtime parameters\n    created_at: datetime\n    version: int\n\nclass TaskState(Enum):\n    \"\"\"Enumeration of possible task execution states.\"\"\"\n    PENDING = \"pending\"\n    WAITING = \"waiting\" \n    QUEUED = \"queued\"\n    RUNNING = \"running\"\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    RETRYING = \"retrying\"\n    CANCELLED = \"cancelled\"\n    SKIPPED = \"skipped\"\n\nclass TaskEvent(Enum):\n    \"\"\"Events that trigger task state transitions.\"\"\"\n    DEPENDENCIES_MET = \"dependencies_met\"\n    EXECUTION_STARTED = \"execution_started\"\n    EXECUTION_COMPLETED = \"execution_completed\"\n    EXECUTION_FAILED = \"execution_failed\"\n    RETRY_SCHEDULED = \"retry_scheduled\"\n    MAX_RETRIES_EXCEEDED = \"max_retries_exceeded\"\n    CANCELLED_BY_USER = \"cancelled_by_user\"\n    UPSTREAM_FAILED = \"upstream_failed\"\n\n@dataclass\nclass TaskExecution:\n    \"\"\"Runtime state for a specific task execution instance.\"\"\"\n    task_id: str\n    pipeline_run_id: str\n    state: TaskState\n    attempt_count: int\n    started_at: Optional[datetime]\n    completed_at: Optional[datetime]\n    error_message: Optional[str]\n    logs: List[str]\n    metrics: Dict[str, float]\n```\n\n**Configuration Validation Infrastructure**\n\n```python\nimport yaml\nimport jsonschema\nfrom pathlib import Path\nfrom typing import Optional, List\n\ndef get_pipeline(pipeline_id: str) -> Optional[PipelineDefinition]:\n    \"\"\"Retrieve pipeline definition by ID from configuration storage.\n    \n    Args:\n        pipeline_id: Unique identifier for the pipeline\n        \n    Returns:\n        PipelineDefinition if found, None otherwise\n        \n    TODO: Implement configuration storage backend (file, database, etc.)\n    TODO: Add caching layer for frequently accessed pipelines\n    TODO: Handle configuration versioning and schema migration\n    \"\"\"\n    # Implementation will load from config storage\n    pass\n\ndef validate_pipeline(pipeline: PipelineDefinition) -> List[str]:\n    \"\"\"Validate pipeline definition and return list of error messages.\n    \n    Args:\n        pipeline: Pipeline definition to validate\n        \n    Returns:\n        List of validation error messages (empty if valid)\n        \n    This function performs multiple validation checks:\n    - Schema validation against JSON schema\n    - DAG cycle detection\n    - Task dependency reference validation\n    - Resource and timeout constraint validation\n    \"\"\"\n    errors = []\n    \n    # TODO 1: Validate against JSON schema for structural correctness\n    # TODO 2: Check for cycles in task dependency graph\n    # TODO 3: Verify all task dependencies reference existing tasks\n    # TODO 4: Validate cron schedule expression syntax\n    # TODO 5: Check resource constraints and timeout values\n    # TODO 6: Validate connector configurations for each task type\n    \n    return errors\n\ndef transition(execution: TaskExecution, event: TaskEvent, error_message: Optional[str] = None) -> bool:\n    \"\"\"Attempt state transition based on event and current state.\n    \n    Args:\n        execution: Current task execution instance\n        event: Event triggering the transition\n        error_message: Optional error details for failure events\n        \n    Returns:\n        True if transition was valid and applied, False otherwise\n        \n    TODO: Implement state machine transition logic based on TRANSITIONS mapping\n    TODO: Update execution timestamps and attempt counts\n    TODO: Handle retry logic and backoff scheduling\n    TODO: Log state changes for audit and debugging\n    \"\"\"\n    # Implementation will check TRANSITIONS table and update execution state\n    pass\n\n# State transition mapping - defines valid state changes\nTRANSITIONS = {\n    TaskState.PENDING: {\n        TaskEvent.DEPENDENCIES_MET: TaskState.QUEUED,\n        TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.QUEUED: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.RUNNING: {\n        TaskEvent.EXECUTION_COMPLETED: TaskState.SUCCESS,\n        TaskEvent.EXECUTION_FAILED: TaskState.FAILED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.FAILED: {\n        TaskEvent.RETRY_SCHEDULED: TaskState.RETRYING,\n        TaskEvent.MAX_RETRIES_EXCEEDED: TaskState.FAILED,\n    },\n    TaskState.RETRYING: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    # Success, cancelled, and skipped are terminal states\n}\n```\n\n**Milestone Checkpoint Guidelines**\n\nAfter implementing the goals and boundaries defined in this section, verify the following checkpoint behaviors:\n\n1. **Goal Validation Checkpoint**: Create a simple pipeline definition and verify it can be loaded and validated without errors. The validation should catch basic issues like missing required fields or invalid cron expressions.\n\n2. **Non-Goal Boundary Checkpoint**: Attempt to use features that are explicitly non-goals (like real-time streaming or visual authoring) and verify they are clearly unsupported with helpful error messages directing users to appropriate alternatives.\n\n3. **Configuration Structure Checkpoint**: The project structure should accommodate the modular architecture implied by the functional goals. Each major component (pipeline definition, execution, connectors, transforms, monitoring) should have its own module with clear interfaces.\n\n**Common Implementation Pitfalls**\n\n **Pitfall: Over-Engineering Goals**\nEarly implementations often try to build every possible feature rather than focusing on the core goals. This leads to complex, unmaintainable systems that don't excel at their primary purpose. Stick to the defined functional goals and resist feature creep.\n\n **Pitfall: Ignoring Non-Functional Requirements**\nFunctional goals are visible to users, but non-functional goals (performance, reliability, observability) often determine production success. Design data structures and interfaces with performance and monitoring in mind from the beginning rather than retrofitting these concerns later.\n\n **Pitfall: Unclear Goal Boundaries**\nVague goal definitions lead to scope creep and architectural confusion. Each goal should be measurable and testable. If you can't write a test that verifies a goal is met, the goal needs to be more specific.\n\n **Pitfall: Non-Goals Become Goals**\nTeams often rationalize why a non-goal is actually essential and should be included. This dilutes the system's focus and increases complexity. When users request non-goal features, provide clear integration paths with external systems rather than expanding the system's scope.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** Foundation for all milestones - provides the architectural blueprint that guides implementation across pipeline definition (Milestone 1), data processing (Milestones 2-3), and orchestration/monitoring (Milestone 4).\n\n### Mental Model: Orchestra Conductor System\n\nThink of our ETL system like a symphony orchestra with multiple specialized sections working in harmony. The **Conductor** (Scheduler) reads the musical score (pipeline definition) and coordinates when each section plays. The **Section Leaders** (Task Executors) manage their musicians and ensure they play their parts correctly. The **Stage Manager** (DAG Engine) ensures all the sheet music is correct and tells the conductor the proper sequence. The **Audio Engineers** (Monitoring System) record the performance, adjust levels, and alert everyone if something goes wrong. Finally, the **Musicians** themselves (Connectors) are the specialists who actually produce the music by extracting, transforming, and loading data.\n\nJust as a conductor can't start the string section before the woodwinds finish their passage, our system ensures data dependencies are respected. When a musician makes a mistake, the section leader (executor) can have them retry their part without stopping the entire orchestra. The audio engineers continuously monitor the performance quality and can quickly identify which section needs attention.\n\nThis orchestration analogy helps us understand why we need separate, specialized components rather than a monolithic system - each component has a distinct responsibility, and the coordination between them creates the beautiful symphony of data processing.\n\n### System Components\n\nOur ETL system consists of five core components, each with distinct responsibilities and clear boundaries. Understanding these components and their relationships is crucial for implementing a maintainable and scalable data pipeline system.\n\n#### DAG Definition Engine\n\nThe **DAG Definition Engine** serves as the foundation of our pipeline system, responsible for parsing, validating, and maintaining pipeline definitions. This component acts as the \"compiler\" for our ETL pipelines, taking human-readable pipeline configurations and transforming them into executable task graphs.\n\n| Component Responsibility | Description | Key Data Structures |\n|-------------------------|-------------|-------------------|\n| Pipeline Parsing | Reads YAML/Python configuration files and converts them into `PipelineDefinition` objects | `PipelineDefinition`, `TaskDefinition` |\n| Dependency Validation | Performs cycle detection and validates task dependency relationships | DAG validation results, dependency graphs |\n| Execution Graph Building | Creates optimized task execution graphs using topological sorting | Execution order lists, dependency matrices |\n| Schema Validation | Ensures pipeline definitions conform to required schemas and constraints | Validation error collections |\n\nThe DAG Engine maintains a registry of all pipeline definitions and provides versioning capabilities. When a pipeline definition changes, it validates the new version against existing data schemas and dependency constraints before allowing the update.\n\n> **Design Insight**: The DAG Engine is intentionally stateless regarding pipeline executions. It only deals with pipeline *definitions*, not runtime state. This separation allows multiple scheduler instances to share the same pipeline definitions while maintaining independent execution state.\n\n#### Pipeline Scheduler\n\nThe **Pipeline Scheduler** orchestrates when pipelines execute based on time-based schedules, external triggers, or data availability conditions. Think of it as the \"air traffic control\" system that manages the flow of pipeline executions across the entire system.\n\n| Scheduling Capability | Implementation Approach | Configuration Options |\n|---------------------|------------------------|----------------------|\n| Cron-based Scheduling | Uses cron expressions parsed into trigger events | Standard cron syntax with seconds precision |\n| Event-driven Triggers | Listens for external events (file arrivals, API calls) | HTTP webhooks, message queue listeners |\n| Data Dependency Scheduling | Monitors upstream data sources for freshness | Watermark-based triggers, file modification monitoring |\n| Manual Triggering | Provides API endpoints for on-demand execution | REST API with parameter overrides |\n\nThe Scheduler maintains a priority queue of pending pipeline runs and coordinates with the Task Executor to ensure proper resource allocation. It implements backpressure mechanisms to prevent system overload when multiple pipelines are ready for execution.\n\n#### Task Executor\n\nThe **Task Executor** is responsible for the actual execution of individual tasks within pipeline runs. It manages task lifecycle, resource allocation, failure handling, and state transitions. This component bridges the gap between high-level pipeline orchestration and low-level data processing operations.\n\n| Execution Capability | Implementation Details | Failure Handling |\n|---------------------|----------------------|------------------|\n| Parallel Task Execution | Manages thread pools with configurable concurrency limits | Task isolation prevents failures from affecting siblings |\n| State Management | Tracks `TaskExecution` objects through `TaskState` transitions | Atomic state updates with event-driven notifications |\n| Resource Allocation | Coordinates memory, CPU, and I/O resources across concurrent tasks | Resource limits with graceful degradation |\n| Retry Logic | Implements exponential backoff based on `RetryPolicy` configurations | Configurable retry conditions and maximum attempt limits |\n\nThe Task Executor maintains execution context for each task, including runtime parameters, connection pools, and intermediate results. It provides isolation between tasks to prevent resource conflicts and implements comprehensive logging for debugging purposes.\n\n#### Data Connectors\n\n**Data Connectors** are specialized components responsible for interfacing with external data systems. They provide a pluggable architecture that abstracts the complexity of different data sources and destinations behind a unified interface.\n\n| Connector Type | Supported Systems | Key Capabilities |\n|----------------|------------------|------------------|\n| Source Connectors | Databases (PostgreSQL, MySQL), APIs (REST, GraphQL), Files (S3, HDFS) | Incremental extraction, schema discovery, pagination handling |\n| Destination Connectors | Data warehouses (Snowflake, BigQuery), Databases, File systems | Bulk loading, upsert operations, schema evolution |\n| Transform Connectors | SQL engines (Spark, DuckDB), Python execution environments | Custom logic execution, data validation, aggregations |\n\nEach connector implements standardized interfaces for connection management, data streaming, and error handling. The connector architecture supports plugin-style extensibility, allowing new data sources to be added without modifying core system components.\n\n#### Monitoring and Observability System\n\nThe **Monitoring and Observability System** provides comprehensive visibility into pipeline execution, performance metrics, and system health. It serves as the \"control tower\" that gives operators the information needed to maintain and optimize the ETL system.\n\n| Monitoring Aspect | Data Collected | Alerting Capabilities |\n|------------------|----------------|----------------------|\n| Pipeline Execution | Run duration, success/failure rates, task timing | SLA violation alerts, failure notifications |\n| Resource Utilization | Memory usage, CPU consumption, I/O throughput | Resource exhaustion warnings, capacity planning metrics |\n| Data Quality | Record counts, schema validation results, data profiling | Data anomaly detection, quality threshold alerts |\n| System Health | Component availability, error rates, dependency status | Service degradation alerts, dependency failure notifications |\n\nThe monitoring system implements configurable alerting rules that can escalate issues based on severity and duration. It maintains historical metrics for trend analysis and capacity planning.\n\n### Component Interactions\n\nUnderstanding how these components communicate and coordinate is essential for implementing a robust ETL system. Each interaction follows specific protocols and handles various failure scenarios gracefully.\n\n#### Scheduler to DAG Engine Communication\n\nThe Scheduler queries the DAG Engine to retrieve pipeline definitions and validate execution requests. This interaction is read-only from the Scheduler's perspective, ensuring clear separation of concerns.\n\n| Interaction Type | Protocol | Error Handling |\n|------------------|----------|----------------|\n| Pipeline Retrieval | Synchronous API call using `get_pipeline(pipeline_id)` | Returns `None` for non-existent pipelines, triggering schedule cleanup |\n| Definition Validation | Calls `validate_pipeline(pipeline)` before scheduling runs | Validation errors prevent scheduling and generate operator alerts |\n| Dependency Resolution | Requests task dependency graphs for execution planning | Invalid dependencies cause immediate run failure with detailed error messages |\n\n> **Architecture Decision: Pull-based Pipeline Retrieval**\n> - **Context**: Scheduler needs access to current pipeline definitions for execution planning\n> - **Options Considered**: \n>   1. Push-based updates from DAG Engine to Scheduler\n>   2. Pull-based retrieval with caching\n>   3. Shared database access\n> - **Decision**: Pull-based retrieval with local caching\n> - **Rationale**: Provides better fault isolation, simpler failure scenarios, and allows independent scaling of components\n> - **Consequences**: Slight latency for pipeline definition updates, but improved system reliability and reduced coupling\n\n#### Scheduler to Task Executor Coordination\n\nThe Scheduler coordinates with Task Executors to manage pipeline run lifecycle and resource allocation. This relationship involves both command-style interactions and event-driven updates.\n\n| Coordination Activity | Message Format | Failure Recovery |\n|----------------------|----------------|------------------|\n| Run Initialization | `PipelineRun` object with execution context and parameters | Executor startup failures trigger run cancellation and cleanup |\n| Task Assignment | Individual `TaskExecution` assignments with dependency information | Task assignment failures cause graceful task skipping with downstream impact analysis |\n| Progress Updates | Periodic state updates using `TaskState` transitions | Missing heartbeats trigger task timeout and potential retry scheduling |\n| Resource Management | Resource allocation requests and availability notifications | Resource exhaustion causes task queuing with backpressure to Scheduler |\n\nThe Scheduler maintains a registry of available Task Executor instances and implements load balancing across them. When an Executor becomes unavailable, the Scheduler redistributes pending tasks to healthy instances.\n\n#### Task Executor to Connector Interactions\n\nTask Executors invoke Data Connectors to perform actual data processing operations. These interactions involve streaming data transfers and require careful resource management.\n\n| Operation Type | Interface Method | Resource Considerations |\n|----------------|------------------|------------------------|\n| Data Extraction | `extract(connection, query) -> DataStream` | Memory usage for result buffering, connection pool management |\n| Data Loading | `load(connection, data, target) -> LoadResult` | Batch size optimization, transaction management |\n| Schema Operations | `discover_schema(connection) -> SchemaInfo` | Connection timeout handling, metadata caching |\n| Connection Management | `create_connection(config) -> Connection` | Connection pooling, credential management, network retry logic |\n\nConnectors implement streaming interfaces to handle large datasets without exhausting system memory. They provide detailed error information that the Task Executor can use for intelligent retry decisions.\n\n#### Monitoring System Event Collection\n\nThe Monitoring System collects events and metrics from all other components through a combination of push-based events and pull-based metric collection.\n\n| Event Source | Event Types | Collection Method |\n|--------------|-------------|-------------------|\n| Scheduler | Pipeline started, completed, failed, cancelled | Event streaming to monitoring message queue |\n| Task Executor | Task state transitions, resource usage, performance metrics | Periodic metric export via HTTP endpoints |\n| DAG Engine | Pipeline definition changes, validation errors | Audit log streaming with structured event format |\n| Connectors | Data transfer volumes, connection health, schema changes | Embedded metric collection within execution context |\n\nThe monitoring system implements event correlation to connect related activities across components. For example, it can trace a pipeline failure back through task failures to specific connector errors.\n\n> **Design Insight**: Event correlation requires each pipeline run to carry a unique correlation ID that flows through all component interactions. This enables distributed tracing and comprehensive failure analysis.\n\n#### Error Propagation and Recovery Coordination\n\nWhen failures occur, components coordinate recovery efforts while maintaining system consistency and preventing cascading failures.\n\n| Failure Scenario | Detection Method | Recovery Coordination |\n|------------------|------------------|----------------------|\n| Task Execution Failure | Executor reports failure via state transition | Scheduler evaluates retry policy and coordinates with fresh Executor instance |\n| Connector Communication Loss | Connection timeout or error response | Executor implements circuit breaker pattern and reports degraded capability to Scheduler |\n| Scheduler Instance Failure | Monitoring system detects missing heartbeats | Standby Scheduler instances take over using persistent state from shared storage |\n| DAG Engine Unavailability | Pipeline retrieval failures | Scheduler operates with cached definitions and queues definition refresh requests |\n\nThe system implements the **Circuit Breaker Pattern** for external dependencies. When a data source becomes unreliable, connectors temporarily stop attempting connections and return predictable errors rather than causing timeout delays across the system.\n\n### Deployment Topology\n\nUnderstanding the recommended deployment architecture helps teams plan their infrastructure and avoid common deployment pitfalls. Our system supports both single-node development deployments and distributed production architectures.\n\n#### Development Environment Structure\n\nFor development and testing, all components can run within a single process or as separate processes on a single machine. This configuration simplifies debugging while maintaining the same interfaces used in production.\n\n```\nproject-root/\n cmd/\n    scheduler/           Scheduler service entry point\n    executor/            Task Executor service entry point  \n    single-node/         All-in-one development deployment\n internal/\n    dag/                 DAG Definition Engine\n       parser.go\n       validator.go\n       graph.go\n    scheduler/           Pipeline Scheduler\n       cron_scheduler.go\n       event_scheduler.go\n       run_manager.go\n    executor/            Task Executor\n       task_runner.go\n       state_machine.go\n       resource_manager.go\n    connectors/          Data Connectors\n       database/\n       api/\n       filesystem/\n    monitoring/          Monitoring System\n        metrics.go\n        events.go\n        alerting.go\n configs/\n    development.yaml     Single-node configuration\n    staging.yaml         Multi-node staging environment\n    production.yaml      Production deployment configuration\n pipelines/\n     examples/            Sample pipeline definitions\n     schemas/             Pipeline definition JSON schemas\n```\n\nThe development configuration runs all components in a single process with shared in-memory state. This enables rapid iteration and simplified debugging with standard development tools.\n\n#### Production Deployment Architecture\n\nProduction deployments distribute components across multiple nodes for scalability, fault tolerance, and resource isolation. Each component type can scale independently based on workload characteristics.\n\n| Component Type | Scaling Strategy | Resource Requirements | High Availability |\n|----------------|------------------|----------------------|-------------------|\n| Scheduler | Active/Standby with shared persistent state | Low CPU, moderate memory for pipeline metadata | Shared state store (PostgreSQL/etcd) enables quick failover |\n| Task Executor | Horizontal scaling with auto-scaling based on queue depth | High CPU/memory, varies by workload type | Stateless design allows dynamic scaling without data loss |\n| DAG Engine | Load balanced read replicas with single writer | Low CPU, memory proportional to pipeline count | Git-based pipeline storage provides natural backup and versioning |\n| Connectors | Embedded within Task Executors | Resource requirements vary by connector type | Connection pooling and retry logic handle transient failures |\n| Monitoring | Clustered deployment with data replication | Moderate CPU, high storage for metrics retention | Time-series database clustering provides data durability |\n\n#### Network Communication Patterns\n\nComponents communicate using well-defined protocols that support both local and distributed deployments. The communication patterns prioritize reliability and observability over raw performance.\n\n| Communication Path | Protocol | Failure Handling | Security Considerations |\n|--------------------|----------|------------------|------------------------|\n| Scheduler  DAG Engine | HTTP REST with JSON | Retry with exponential backoff, circuit breaker | TLS encryption, API key authentication |\n| Scheduler  Task Executor | Message queue (Redis/RabbitMQ) | Message acknowledgment, dead letter queue | Message-level encryption, queue access controls |\n| Task Executor  Connectors | In-process function calls or HTTP | Exception handling, connection pooling | Credential injection, secret management |\n| All Components  Monitoring | Event streaming + HTTP metrics | Best-effort delivery with local buffering | Metric anonymization, access logging |\n\n> **Architecture Decision: Message Queue for Task Assignment**\n> - **Context**: Scheduler needs reliable way to assign tasks to available executors\n> - **Options Considered**:\n>   1. Direct HTTP calls to executor instances\n>   2. Shared database with polling\n>   3. Message queue with work distribution\n> - **Decision**: Message queue with work distribution pattern\n> - **Rationale**: Provides natural load balancing, handles executor failures gracefully, enables backpressure management\n> - **Consequences**: Introduces message queue as additional infrastructure dependency, but significantly improves system resilience\n\n#### Persistent State Management\n\nEach component manages different types of persistent state with appropriate storage technologies and consistency requirements.\n\n| Component | State Type | Storage Technology | Consistency Requirements |\n|-----------|------------|-------------------|-------------------------|\n| Scheduler | Run schedules, execution history | PostgreSQL with ACID transactions | Strong consistency for schedule integrity |\n| Task Executor | Execution checkpoints, task logs | Object storage (S3) + local caching | Eventually consistent, focus on durability |\n| DAG Engine | Pipeline definitions, validation cache | Git repository + local file cache | Strong consistency for definitions, eventual for cache |\n| Monitoring | Metrics, events, alert history | Time-series database (Prometheus/InfluxDB) | Eventually consistent, optimized for write throughput |\n\nThe system implements **checkpoint-based recovery** for long-running tasks. Task Executors periodically save execution state to persistent storage, enabling recovery from partial failures without reprocessing all data.\n\n#### Security and Access Control Deployment\n\nProduction deployments implement defense-in-depth security with multiple layers of access control and data protection.\n\n| Security Layer | Implementation | Components Affected |\n|----------------|----------------|-------------------|\n| Network Security | VPC isolation, security groups, TLS encryption | All inter-component communication |\n| Authentication | Service-to-service authentication using certificates or tokens | Scheduler, DAG Engine, Monitoring APIs |\n| Authorization | Role-based access control for pipeline operations | Pipeline definition access, execution permissions |\n| Secret Management | External secret store integration (HashiCorp Vault, AWS Secrets Manager) | Connector configurations, database credentials |\n| Audit Logging | Comprehensive logging of all system actions with immutable audit trail | All components with centralized log aggregation |\n\nConnector credentials are injected at runtime from the secret management system and never stored in pipeline definitions or system configuration files.\n\n### Common Pitfalls\n\n **Pitfall: Shared State Between Components**\n\nMany developers initially try to share state directly between components (like using shared variables or files for coordination). This creates tight coupling and makes the system fragile to failures.\n\n**Why it's wrong**: Shared state creates hidden dependencies that break when components are deployed separately or when failures occur. It also makes testing and development much more difficult.\n\n**How to fix it**: Use message passing and well-defined APIs between components. Each component should own its state completely, and coordination should happen through explicit communication protocols.\n\n **Pitfall: Synchronous Communication Everywhere**\n\nUsing synchronous HTTP calls for all inter-component communication seems simple but creates cascading failure scenarios and tight coupling between component lifecycles.\n\n**Why it's wrong**: When one component becomes slow or unavailable, synchronous calls cause the entire call chain to block or fail. This eliminates the benefits of component isolation.\n\n**How to fix it**: Use asynchronous message queues for work distribution and events. Reserve synchronous calls only for queries where immediate responses are required (like pipeline definition retrieval).\n\n **Pitfall: Insufficient Error Context**\n\nDevelopers often propagate generic errors between components without preserving context about what operation failed and why.\n\n**Why it's wrong**: When debugging production issues, generic errors make it impossible to determine root causes. Operators need to understand which pipeline, task, and data source caused problems.\n\n**How to fix it**: Implement structured error types that preserve operation context, correlation IDs, and relevant metadata. Each component should add its own context when propagating errors.\n\n **Pitfall: No Resource Boundaries**\n\nRunning all components with unlimited resource access causes resource contention and unpredictable performance as the system scales.\n\n**Why it's wrong**: Without resource boundaries, one pipeline can consume all system memory or CPU, causing other pipelines to fail or perform poorly.\n\n**How to fix it**: Implement resource limits at the Task Executor level, use connection pooling for external resources, and monitor resource usage to detect anomalies early.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Inter-Component Communication | HTTP REST with `requests` library | Message queues (Redis/RabbitMQ) with `celery` or `rq` |\n| State Storage | SQLite for development, PostgreSQL for production | Distributed databases (PostgreSQL + Redis cluster) |\n| Configuration Management | YAML files with `pyyaml` library | Configuration service (Consul, etcd) with hot reloading |\n| Monitoring | Built-in Python `logging` + simple metrics | Prometheus + Grafana with custom metrics |\n| Process Management | Single Python process with threading | Container orchestration (Docker + Kubernetes) |\n\n#### Recommended File Structure\n\n```\netl-system/\n src/\n    etl/\n       __init__.py\n       core/                     Shared data structures and interfaces\n          __init__.py\n          models.py             PipelineDefinition, TaskDefinition, TaskExecution\n          interfaces.py         Abstract base classes for connectors\n          events.py             TaskEvent, TaskState enums\n       dag/                      DAG Definition Engine (Milestone 1)\n          __init__.py\n          parser.py             YAML/Python pipeline parsing\n          validator.py          Cycle detection and validation\n          graph.py              Topological sorting and execution graphs\n       scheduler/                Pipeline Scheduler (Milestone 4)\n          __init__.py\n          cron_scheduler.py     Time-based scheduling\n          event_scheduler.py    Event-driven triggers\n          run_manager.py        Pipeline run lifecycle\n       executor/                 Task Executor (Milestone 4)\n          __init__.py\n          task_runner.py        Task execution engine\n          state_machine.py      TaskState transitions\n          resource_manager.py   Resource allocation and limits\n       connectors/               Data Connectors (Milestones 2-3)\n          __init__.py\n          base.py               Abstract connector interfaces\n          database/             Database source/destination connectors\n          api/                  REST API connectors\n          filesystem/           File-based connectors\n          transforms/           SQL and Python transformation engines\n       monitoring/               Monitoring System (Milestone 4)\n           __init__.py\n           metrics.py            Metrics collection and export\n           events.py             Event collection and correlation\n           alerting.py           Alert generation and notification\n    scripts/\n       start_scheduler.py        Scheduler service entry point\n       start_executor.py         Executor service entry point\n       single_node.py            Development single-process runner\n    tests/\n        unit/                     Component unit tests\n        integration/              Cross-component integration tests\n        e2e/                      End-to-end pipeline tests\n configs/\n    development.yaml              Development environment configuration\n    staging.yaml                  Staging environment configuration\n    production.yaml               Production deployment configuration\n pipelines/\n    examples/                     Sample pipeline definitions\n       simple_etl.yaml\n       complex_analytics.yaml\n    schemas/\n        pipeline_schema.json      JSON Schema for pipeline validation\n docker/\n    Dockerfile.scheduler          Container image for scheduler\n    Dockerfile.executor           Container image for executor\n    docker-compose.yml            Local development setup\n docs/\n     architecture.md               This design document\n     api/                          Component API documentation\n     examples/                     Usage examples and tutorials\n```\n\n#### Infrastructure Starter Code\n\n**Core Data Models (complete implementation):**\n\n```python\n# src/etl/core/models.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import List, Dict, Optional, Any\nimport uuid\n\n@dataclass\nclass RetryPolicy:\n    \"\"\"Configuration for task retry behavior.\"\"\"\n    max_attempts: int = 3\n    backoff_seconds: int = 60\n    exponential_backoff: bool = True\n    retry_on_error_types: List[str] = field(default_factory=lambda: [\"ConnectionError\", \"TimeoutError\"])\n\nclass TaskState(Enum):\n    \"\"\"All possible states for a task execution.\"\"\"\n    PENDING = \"pending\"\n    WAITING = \"waiting\"\n    QUEUED = \"queued\"\n    RUNNING = \"running\"\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n    RETRYING = \"retrying\"\n    CANCELLED = \"cancelled\"\n    SKIPPED = \"skipped\"\n\nclass TaskEvent(Enum):\n    \"\"\"Events that trigger task state transitions.\"\"\"\n    DEPENDENCIES_MET = \"dependencies_met\"\n    EXECUTION_STARTED = \"execution_started\"\n    EXECUTION_COMPLETED = \"execution_completed\"\n    EXECUTION_FAILED = \"execution_failed\"\n    RETRY_SCHEDULED = \"retry_scheduled\"\n    MAX_RETRIES_EXCEEDED = \"max_retries_exceeded\"\n    CANCELLED_BY_USER = \"cancelled_by_user\"\n    UPSTREAM_FAILED = \"upstream_failed\"\n\n@dataclass\nclass TaskDefinition:\n    \"\"\"Definition of a single task within a pipeline.\"\"\"\n    id: str\n    name: str\n    type: str  # \"extract\", \"transform\", \"load\"\n    config: Dict[str, Any]\n    dependencies: List[str] = field(default_factory=list)\n    retry_policy: RetryPolicy = field(default_factory=RetryPolicy)\n    timeout_seconds: int = 3600\n\n@dataclass  \nclass PipelineDefinition:\n    \"\"\"Complete definition of an ETL pipeline.\"\"\"\n    id: str\n    name: str\n    description: str\n    schedule: str  # cron expression\n    tasks: List[TaskDefinition]\n    parameters: Dict[str, Any] = field(default_factory=dict)\n    created_at: datetime = field(default_factory=datetime.now)\n    version: int = 1\n\n@dataclass\nclass TaskExecution:\n    \"\"\"Runtime execution instance of a task.\"\"\"\n    task_id: str\n    pipeline_run_id: str\n    state: TaskState = TaskState.PENDING\n    attempt_count: int = 0\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    logs: List[str] = field(default_factory=list)\n    metrics: Dict[str, float] = field(default_factory=dict)\n    \n    def __post_init__(self):\n        if not hasattr(self, 'execution_id'):\n            self.execution_id = str(uuid.uuid4())\n```\n\n**State Machine Implementation (complete):**\n\n```python\n# src/etl/core/state_machine.py\nfrom typing import Dict, Set, Optional, Tuple\nfrom .models import TaskState, TaskEvent, TaskExecution\n\n# Valid state transitions - defines the complete state machine\nTRANSITIONS: Dict[TaskState, Dict[TaskEvent, TaskState]] = {\n    TaskState.PENDING: {\n        TaskEvent.DEPENDENCIES_MET: TaskState.QUEUED,\n        TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.WAITING: {\n        TaskEvent.DEPENDENCIES_MET: TaskState.QUEUED,\n        TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.QUEUED: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.RUNNING: {\n        TaskEvent.EXECUTION_COMPLETED: TaskState.SUCCESS,\n        TaskEvent.EXECUTION_FAILED: TaskState.FAILED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.FAILED: {\n        TaskEvent.RETRY_SCHEDULED: TaskState.RETRYING,\n        TaskEvent.MAX_RETRIES_EXCEEDED: TaskState.FAILED,  # terminal state\n    },\n    TaskState.RETRYING: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    # Terminal states have no outgoing transitions\n    TaskState.SUCCESS: {},\n    TaskState.CANCELLED: {},\n    TaskState.SKIPPED: {},\n}\n\ndef transition(execution: TaskExecution, event: TaskEvent, error_message: Optional[str] = None) -> bool:\n    \"\"\"\n    Attempt to transition task execution to new state based on event.\n    Returns True if transition was valid and applied, False otherwise.\n    \"\"\"\n    current_state = execution.state\n    \n    if current_state not in TRANSITIONS:\n        return False\n        \n    valid_events = TRANSITIONS[current_state]\n    if event not in valid_events:\n        return False\n    \n    # Apply the transition\n    new_state = valid_events[event]\n    execution.state = new_state\n    \n    # Update execution metadata based on transition\n    if event == TaskEvent.EXECUTION_STARTED:\n        execution.started_at = datetime.now()\n        execution.attempt_count += 1\n    elif event == TaskEvent.EXECUTION_COMPLETED:\n        execution.completed_at = datetime.now()\n    elif event == TaskEvent.EXECUTION_FAILED:\n        execution.completed_at = datetime.now()\n        if error_message:\n            execution.error_message = error_message\n    \n    return True\n\ndef get_terminal_states() -> Set[TaskState]:\n    \"\"\"Return set of states that have no outgoing transitions.\"\"\"\n    return {state for state, transitions in TRANSITIONS.items() if not transitions}\n\ndef is_terminal_state(state: TaskState) -> bool:\n    \"\"\"Check if a state is terminal (has no outgoing transitions).\"\"\"\n    return state in get_terminal_states()\n```\n\n#### Core Logic Skeleton\n\n**DAG Engine Interface (skeleton for student implementation):**\n\n```python\n# src/etl/dag/engine.py\nfrom typing import Optional, List, Dict, Set\nfrom ..core.models import PipelineDefinition, TaskDefinition\n\nclass DAGEngine:\n    \"\"\"Core engine for pipeline definition management and validation.\"\"\"\n    \n    def __init__(self, pipeline_storage_path: str):\n        \"\"\"Initialize DAG engine with pipeline storage location.\"\"\"\n        self.storage_path = pipeline_storage_path\n        self._pipeline_cache: Dict[str, PipelineDefinition] = {}\n        \n    def get_pipeline(self, pipeline_id: str) -> Optional[PipelineDefinition]:\n        \"\"\"\n        Retrieve pipeline definition by ID.\n        Returns None if pipeline doesn't exist.\n        \"\"\"\n        # TODO 1: Check local cache first - return cached version if available\n        # TODO 2: Load pipeline from storage (YAML file) if not in cache  \n        # TODO 3: Parse YAML content into PipelineDefinition object\n        # TODO 4: Store in cache for future requests\n        # TODO 5: Return PipelineDefinition or None if file doesn't exist\n        # Hint: Use self.storage_path + pipeline_id + \".yaml\" as file path\n        pass\n        \n    def validate_pipeline(self, pipeline: PipelineDefinition) -> List[str]:\n        \"\"\"\n        Validate pipeline definition and return list of error messages.\n        Returns empty list if pipeline is valid.\n        \"\"\"\n        errors = []\n        # TODO 1: Check that all task IDs are unique within pipeline\n        # TODO 2: Validate that all task dependencies reference existing tasks\n        # TODO 3: Check for circular dependencies using cycle detection\n        # TODO 4: Validate that each task has required config fields for its type\n        # TODO 5: Check that cron schedule expression is valid\n        # Hint: Use self._detect_cycles() helper method for dependency validation\n        return errors\n        \n    def _detect_cycles(self, tasks: List[TaskDefinition]) -> bool:\n        \"\"\"\n        Detect if task dependencies contain cycles using depth-first search.\n        Returns True if cycles are found, False otherwise.\n        \"\"\"\n        # TODO 1: Build adjacency list from task dependencies\n        # TODO 2: Track visited nodes and recursion stack\n        # TODO 3: For each unvisited task, start DFS traversal\n        # TODO 4: If we encounter a node already in recursion stack, cycle detected\n        # TODO 5: Return True if any cycle found, False otherwise\n        # Hint: Use three colors (white=unvisited, gray=visiting, black=done)\n        pass\n        \n    def get_execution_order(self, pipeline: PipelineDefinition) -> List[List[str]]:\n        \"\"\"\n        Return task IDs grouped by execution level using topological sort.\n        Tasks in same inner list can execute in parallel.\n        \"\"\"\n        # TODO 1: Build dependency graph with in-degree counting\n        # TODO 2: Start with tasks that have zero dependencies (in-degree = 0)\n        # TODO 3: Process tasks level by level, removing edges as we go\n        # TODO 4: Add tasks to next level when their in-degree reaches 0\n        # TODO 5: Return list of execution levels for parallel scheduling\n        # Hint: Use queue for breadth-first processing of each level\n        pass\n```\n\n#### Language-Specific Hints\n\n**Python Specific Recommendations:**\n\n- Use `dataclasses` with `frozen=True` for immutable data structures like `PipelineDefinition`\n- Implement `__post_init__` in dataclasses for validation and computed fields\n- Use `typing.Protocol` for defining connector interfaces that can be implemented by third parties\n- Use `concurrent.futures.ThreadPoolExecutor` for parallel task execution with configurable pool size\n- Use `pathlib.Path` for all file system operations instead of string manipulation\n- Use `logging` module with structured logging (JSON format) for production deployments\n- Use `yaml.safe_load()` for parsing pipeline definitions to prevent code injection\n- Implement connection pooling using `queue.Queue` for database connectors\n\n**Error Handling Patterns:**\n\n```python\n# Custom exception hierarchy for better error handling\nclass ETLError(Exception):\n    \"\"\"Base exception for all ETL system errors.\"\"\"\n    pass\n\nclass PipelineValidationError(ETLError):\n    \"\"\"Raised when pipeline definition is invalid.\"\"\"\n    def __init__(self, pipeline_id: str, errors: List[str]):\n        self.pipeline_id = pipeline_id\n        self.errors = errors\n        super().__init__(f\"Pipeline {pipeline_id} validation failed: {errors}\")\n\nclass TaskExecutionError(ETLError):\n    \"\"\"Raised when task execution fails.\"\"\"\n    def __init__(self, task_id: str, attempt: int, cause: Exception):\n        self.task_id = task_id\n        self.attempt = attempt\n        self.cause = cause\n        super().__init__(f\"Task {task_id} failed on attempt {attempt}: {cause}\")\n```\n\n#### Milestone Checkpoints\n\n**After implementing core data models and state machine:**\n\n1. Run unit tests: `python -m pytest tests/unit/test_models.py -v`\n2. Expected output: All state transition tests pass, invalid transitions properly rejected\n3. Manual verification: Create a `TaskExecution` instance and call `transition()` with various events\n4. Check that terminal states (SUCCESS, FAILED, CANCELLED) don't accept any transitions\n5. Verify that attempt_count increments only on EXECUTION_STARTED events\n\n**After implementing DAG Engine basic functionality:**\n\n1. Create a sample pipeline YAML file with 3-4 tasks and dependencies\n2. Run: `python -c \"from etl.dag.engine import DAGEngine; engine = DAGEngine('pipelines/'); print(engine.get_pipeline('test'))\"`\n3. Expected behavior: Pipeline loads successfully and prints PipelineDefinition object\n4. Test validation: Create pipeline with circular dependency, verify `validate_pipeline()` catches it\n5. Test execution order: Verify `get_execution_order()` returns correct parallel execution groups\n\n**After implementing basic component communication:**\n\n1. Start scheduler in one terminal: `python scripts/start_scheduler.py`\n2. Start executor in another terminal: `python scripts/start_executor.py`\n3. Submit a simple pipeline through REST API: `curl -X POST localhost:8080/pipelines/test/run`\n4. Check that both components log the pipeline execution flow\n5. Verify that task state transitions are properly communicated between components\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Pipeline fails to load | YAML syntax error or missing file | Check file exists, validate YAML syntax | Fix YAML formatting, verify file permissions |\n| Circular dependency error | Invalid task dependencies in pipeline definition | Draw dependency graph on paper | Remove circular references, add proper task ordering |\n| Tasks stuck in PENDING state | Dependency resolution failure | Check task dependency configuration | Verify all dependencies reference existing tasks |\n| High memory usage during execution | Large dataset loading without streaming | Monitor memory usage during connector operations | Implement streaming in connectors, reduce batch sizes |\n| Components can't communicate | Network configuration or service discovery issues | Check port binding and firewall rules | Verify component startup order and configuration |\n| State transitions fail | Invalid state machine transitions | Enable debug logging for state transitions | Check that events match current state in TRANSITIONS table |\n\n\n## Data Model\n\n> **Milestone(s):** Foundation for all milestones - provides the data structures and state management that underpin pipeline definition (Milestone 1), data processing (Milestones 2-3), and orchestration (Milestone 4).\n\n### Mental Model: Digital Blueprint and Construction Log\n\nThink of the data model as the combination of **architectural blueprints** and a **construction project log**. The pipeline and task definitions are like detailed blueprints that specify exactly what needs to be built, in what order, and with what materials. The runtime state model is like the foreman's log that tracks which workers are doing what, when each task started and finished, and any problems encountered. The metadata and lineage system is like a comprehensive project history that documents every decision, every change order, and the complete chain of how raw materials became the finished building.\n\nJust as you wouldn't start construction without blueprints, and you wouldn't manage a complex construction project without tracking progress and maintaining detailed records, an ETL system needs these three layers of data organization to function reliably at scale.\n\n![Data Model Relationships](./diagrams/data-model.svg)\n\nThe data model serves as the foundation for all system operations, providing the structured representation of pipeline configurations, execution state, and historical metadata. This model must support concurrent access patterns, handle state transitions safely, and maintain data integrity across distributed components.\n\n### Pipeline and Task Definitions\n\nThe pipeline definition schema establishes the static blueprint for ETL workflows, capturing the declarative specification of what should happen without concern for runtime execution details. This separation between definition and execution enables versioning, validation, and reuse of pipeline logic across different environments.\n\n#### PipelineDefinition Structure\n\nThe `PipelineDefinition` serves as the top-level container for all pipeline metadata and task specifications. Each pipeline represents a complete ETL workflow with its own scheduling, parameters, and dependency graph.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| id | str | Unique identifier for the pipeline, used for referencing and scheduling |\n| name | str | Human-readable display name for the pipeline |\n| description | str | Detailed explanation of pipeline purpose and business logic |\n| schedule | str | Cron expression or event trigger specification for pipeline execution |\n| tasks | List[TaskDefinition] | Complete list of all tasks that comprise this pipeline |\n| parameters | dict | Default parameter values that can be overridden at runtime |\n| created_at | datetime | Timestamp when this pipeline version was first created |\n| version | int | Monotonically increasing version number for schema evolution |\n\nThe pipeline identifier must be globally unique within the system and should follow a hierarchical naming convention that reflects organizational structure and purpose. The version field enables schema evolution and rollback capabilities, while the parameters dictionary provides a mechanism for runtime customization without modifying the core pipeline definition.\n\n> **Critical Design Insight**: Pipeline definitions are immutable once created. Any modification creates a new version, ensuring that running pipelines complete with their original logic while new runs use updated definitions.\n\n#### TaskDefinition Structure\n\nIndividual tasks represent atomic units of work within the pipeline, each encapsulating a specific transformation, extraction, or loading operation. Tasks must be designed to be idempotent and independently executable to support retry and recovery scenarios.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| id | str | Unique task identifier within the pipeline scope |\n| name | str | Human-readable task name for monitoring and debugging |\n| type | str | Task type identifier that determines execution behavior |\n| config | dict | Task-specific configuration parameters and connection details |\n| dependencies | List[str] | List of upstream task IDs that must complete before this task |\n| retry_policy | RetryPolicy | Configuration for automatic retry behavior on failure |\n| timeout_seconds | int | Maximum execution time before task is considered failed |\n\nThe task type field determines which executor will handle the task and defines the expected structure of the config dictionary. Common task types include `sql_transform`, `api_extract`, `file_load`, and `python_udf`. The dependencies list establishes the DAG structure and must be validated to ensure no cycles exist.\n\n> **Architecture Decision: Task Configuration Flexibility**\n> - **Context**: Tasks need varying configuration parameters based on their type, but we want type safety and validation\n> - **Options Considered**: \n>   1. Strongly typed task subclasses with specific fields\n>   2. Generic config dictionary with runtime validation\n>   3. JSON schema validation for each task type\n> - **Decision**: Generic config dictionary with pluggable validation per task type\n> - **Rationale**: Provides maximum flexibility for custom task types while maintaining validation. Easier to extend than rigid inheritance hierarchies.\n> - **Consequences**: Requires runtime validation and clear documentation of expected config schemas per task type\n\n| Option | Pros | Cons |\n|--------|------|------|\n| Strongly typed subclasses | Compile-time safety, clear API | Rigid, hard to extend, complex inheritance |\n| Generic config dict | Flexible, easy custom tasks | Runtime validation only, potential config errors |\n| JSON schema validation | Good balance, clear contracts | Additional schema maintenance overhead |\n\n#### RetryPolicy Configuration\n\nRetry policies define how the system should respond to task failures, balancing between automatic recovery and avoiding infinite retry loops. The policy configuration must be expressive enough to handle different failure types with appropriate retry strategies.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| max_attempts | int | Total number of execution attempts including the initial run |\n| backoff_seconds | int | Base delay between retry attempts |\n| exponential_backoff | bool | Whether to double the delay after each failure |\n| retry_on_error_types | List[str] | Specific error categories that should trigger retries |\n\nThe retry policy enables sophisticated failure handling by distinguishing between transient errors (network timeouts, temporary resource unavailability) and permanent errors (authentication failures, malformed queries). The `retry_on_error_types` field allows fine-grained control over which failures justify retry attempts.\n\n#### Dependency Resolution and Validation\n\nPipeline definitions undergo comprehensive validation during registration to catch configuration errors early and ensure reliable execution. The validation process examines both individual task configurations and the overall pipeline structure.\n\n**Pipeline Validation Steps:**\n\n1. **Task ID Uniqueness**: Verify that all task IDs within a pipeline are unique and follow naming conventions\n2. **Dependency Reference Validation**: Confirm that all task dependencies refer to valid task IDs within the same pipeline\n3. **Cycle Detection**: Perform topological analysis to ensure the dependency graph forms a valid DAG with no cycles\n4. **Task Configuration Validation**: Validate each task's config dictionary against its type-specific schema\n5. **Resource Requirement Analysis**: Check that resource requirements don't exceed system capacity limits\n6. **Parameter Substitution Verification**: Ensure all parameter references in task configs have corresponding defaults or required parameters\n\nThe `validate_pipeline(pipeline) -> List[str]` function performs this comprehensive validation and returns a list of human-readable error messages. An empty list indicates a valid pipeline ready for execution.\n\n> **Common Pitfall**: Circular Dependencies\n> \n>  **Pitfall: Hidden Circular Dependencies**\n> Developers often create circular dependencies through indirect chains (ABCA) that aren't obvious in large pipelines. The validation must detect these transitively, not just check immediate dependencies. Always run full cycle detection using depth-first search with a visited set to catch these subtle cycles.\n\n### Runtime State Model\n\nThe runtime state model tracks the dynamic execution of pipeline definitions, maintaining detailed state information for active runs, completed executions, and failed attempts. This model must support concurrent access from multiple system components while ensuring consistency and auditability.\n\n#### Pipeline Run Lifecycle\n\nEach pipeline execution creates a `PipelineRun` instance that tracks the overall execution progress and coordinates individual task executions. Pipeline runs serve as the primary unit for monitoring, alerting, and historical analysis.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| run_id | str | Unique identifier for this specific pipeline execution |\n| pipeline_id | str | Reference to the pipeline definition being executed |\n| pipeline_version | int | Version of pipeline definition used for this run |\n| triggered_by | str | Identifier of user, scheduler, or event that initiated the run |\n| triggered_at | datetime | Timestamp when the run was initiated |\n| started_at | datetime | Timestamp when first task began execution |\n| completed_at | datetime | Timestamp when all tasks finished (success or failure) |\n| state | PipelineRunState | Overall run state (PENDING, RUNNING, SUCCESS, FAILED, CANCELLED) |\n| parameters | dict | Runtime parameters used for this execution |\n| task_executions | List[TaskExecution] | All task execution instances for this run |\n\nThe pipeline run maintains the execution context and provides a consistent view of progress across all constituent tasks. The `pipeline_version` field ensures that runs can be analyzed against the correct pipeline definition even after newer versions are deployed.\n\n#### Task Execution State Management\n\nIndividual task executions track the detailed progress of each task within a pipeline run. The state model must handle complex scenarios including retries, cancellations, and dependency failures while maintaining a clear audit trail.\n\n![Task State Transitions](./diagrams/task-state-machine.svg)\n\n| Field | Type | Description |\n|-------|------|-------------|\n| task_id | str | Reference to task definition within the pipeline |\n| pipeline_run_id | str | Reference to parent pipeline run |\n| state | TaskState | Current execution state of this task |\n| attempt_count | int | Number of execution attempts including current attempt |\n| started_at | datetime | Timestamp when task execution began |\n| completed_at | datetime | Timestamp when task finished (success or failure) |\n| error_message | str | Detailed error description for failed executions |\n| logs | List[str] | Chronological log entries from task execution |\n| metrics | Dict[str,float] | Performance metrics (duration, records processed, etc.) |\n\nThe `TaskExecution` model captures both the current state and the complete execution history for each task attempt. This granular tracking enables detailed debugging and performance analysis while supporting retry logic and failure recovery.\n\n#### State Transition Logic\n\nTask state transitions follow a deterministic state machine that ensures consistent behavior across different execution scenarios. The transition function `transition(execution, event, error_message) -> bool` attempts to move a task execution to the appropriate next state based on the triggering event.\n\n| Current State | Event | Next State | Action Taken |\n|---------------|-------|------------|--------------|\n| PENDING | DEPENDENCIES_MET | QUEUED | Add to execution queue |\n| QUEUED | EXECUTION_STARTED | RUNNING | Update started_at timestamp |\n| RUNNING | EXECUTION_COMPLETED | SUCCESS | Update completed_at, record metrics |\n| RUNNING | EXECUTION_FAILED | FAILED or RETRYING | Check retry policy, increment attempt_count |\n| RETRYING | EXECUTION_STARTED | RUNNING | Begin new execution attempt |\n| RUNNING | CANCELLED_BY_USER | CANCELLED | Terminate execution, cleanup resources |\n| WAITING | UPSTREAM_FAILED | SKIPPED | Mark as skipped due to dependency failure |\n| FAILED | MAX_RETRIES_EXCEEDED | FAILED | Final failure state, no more retries |\n\nThe state machine enforces valid transitions and prevents invalid state changes that could compromise system integrity. Each transition is atomic and logged for audit purposes.\n\n> **Architecture Decision: State Transition Atomicity**\n> - **Context**: State transitions must be atomic to prevent race conditions in concurrent execution environments\n> - **Options Considered**: \n>   1. Database transactions for each state change\n>   2. In-memory locks with periodic persistence\n>   3. Event sourcing with append-only log\n> - **Decision**: Database transactions with optimistic locking for state changes\n> - **Rationale**: Provides ACID guarantees while supporting distributed execution. Optimistic locking prevents contention in normal cases.\n> - **Consequences**: Requires retry logic for concurrent updates, but ensures consistency and supports system recovery\n\n#### Execution Metrics and Observability\n\nThe runtime state model captures comprehensive metrics to enable monitoring, alerting, and performance optimization. Metrics are collected at both the task and pipeline level to provide different granularities of visibility.\n\n**Task-Level Metrics:**\n\n| Metric Name | Type | Description |\n|-------------|------|-------------|\n| execution_duration_seconds | float | Wall-clock time from start to completion |\n| cpu_usage_percent | float | Average CPU utilization during execution |\n| memory_usage_mb | float | Peak memory consumption during execution |\n| records_processed | float | Number of data records processed by the task |\n| bytes_processed | float | Total bytes read and written by the task |\n| error_count | float | Number of recoverable errors encountered during execution |\n\n**Pipeline-Level Metrics:**\n\n| Metric Name | Type | Description |\n|-------------|------|-------------|\n| total_duration_seconds | float | End-to-end pipeline execution time |\n| task_success_count | float | Number of tasks that completed successfully |\n| task_failure_count | float | Number of tasks that failed after all retries |\n| critical_path_duration | float | Duration of longest task dependency chain |\n| parallelism_achieved | float | Average number of tasks running concurrently |\n| data_freshness_hours | float | Age of the oldest input data processed |\n\nThese metrics enable both real-time monitoring and historical analysis for capacity planning and performance optimization.\n\n### Metadata and Lineage\n\nThe metadata and lineage system maintains comprehensive provenance information that tracks how data flows through the system, enabling compliance, debugging, and impact analysis. This system captures both technical lineage (what transformations were applied) and business lineage (what business processes were affected).\n\n#### Data Lineage Tracking\n\nData lineage captures the complete transformation history of data as it moves through ETL pipelines, creating an auditable trail that can answer questions like \"where did this data come from?\" and \"what processes will be affected if this source changes?\"\n\n**Dataset Registration:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| dataset_id | str | Unique identifier for the logical dataset |\n| name | str | Human-readable dataset name |\n| schema_version | int | Current schema version number |\n| location | str | Physical location (table name, file path, etc.) |\n| owner | str | Business owner responsible for dataset quality |\n| tags | List[str] | Metadata tags for discovery and classification |\n| created_at | datetime | When this dataset was first registered |\n| last_updated | datetime | Most recent data refresh timestamp |\n\n**Lineage Edge Tracking:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| source_dataset_id | str | Dataset that provides input data |\n| target_dataset_id | str | Dataset that receives transformed data |\n| pipeline_id | str | Pipeline that performs the transformation |\n| task_id | str | Specific task within pipeline that creates the relationship |\n| transformation_type | str | Type of transformation applied (JOIN, AGGREGATE, FILTER, etc.) |\n| column_mappings | Dict[str,str] | Mapping from source columns to target columns |\n| created_at | datetime | When this lineage relationship was established |\n\nThe lineage system builds a directed graph of data dependencies that can be traversed in both directions to understand upstream sources and downstream consumers. This graph supports impact analysis when schema changes or data quality issues are detected.\n\n#### Schema Evolution and Versioning\n\nSchema evolution tracking ensures that changes to data structures are managed safely without breaking downstream consumers. The system maintains a complete history of schema changes and provides compatibility checking for pipeline modifications.\n\n**Schema Version History:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| dataset_id | str | Reference to the dataset being versioned |\n| version | int | Monotonically increasing version number |\n| schema_definition | dict | Complete schema specification (columns, types, constraints) |\n| change_type | str | Type of change (ADD_COLUMN, DROP_COLUMN, CHANGE_TYPE, etc.) |\n| change_description | str | Human-readable description of what changed |\n| compatibility | str | Backward compatibility assessment (COMPATIBLE, BREAKING, UNKNOWN) |\n| applied_by | str | User or system that applied this schema change |\n| applied_at | datetime | Timestamp when change was applied |\n\n**Schema Compatibility Rules:**\n\n1. **Backward Compatible Changes**: Adding optional columns, relaxing constraints, adding enum values\n2. **Breaking Changes**: Removing columns, changing data types, adding required columns, tightening constraints\n3. **Forward Compatible Changes**: Changes that older readers can safely ignore\n\nThe schema evolution system automatically analyzes proposed changes and identifies potentially affected downstream consumers, enabling proactive communication and migration planning.\n\n> **Architecture Decision: Schema Evolution Strategy**\n> - **Context**: Need to support schema changes without breaking existing pipelines while maintaining data quality\n> - **Options Considered**: \n>   1. Strict immutability - never change schemas\n>   2. Automatic migration with compatibility checking\n>   3. Explicit versioning with parallel schema support\n> - **Decision**: Explicit versioning with automated compatibility analysis and migration assistance\n> - **Rationale**: Balances safety with flexibility. Provides clear upgrade paths while catching breaking changes early.\n> - **Consequences**: Requires schema registry maintenance and coordination between teams for breaking changes\n\n#### Audit Trail and Compliance\n\nThe audit trail maintains a complete record of all system activities for compliance, debugging, and security analysis. This includes user actions, system events, and data access patterns that might be required for regulatory compliance.\n\n**Audit Event Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| event_id | str | Unique identifier for this audit event |\n| event_type | str | Category of event (USER_ACTION, SYSTEM_EVENT, DATA_ACCESS) |\n| actor | str | User, service, or system component that initiated the action |\n| resource_type | str | Type of resource affected (PIPELINE, TASK, DATASET) |\n| resource_id | str | Identifier of specific resource instance |\n| action | str | Specific action performed (CREATE, UPDATE, DELETE, EXECUTE, READ) |\n| timestamp | datetime | Precise timestamp when event occurred |\n| ip_address | str | Network address of request origin |\n| session_id | str | Session identifier for grouping related actions |\n| details | dict | Action-specific details and parameters |\n| result | str | Outcome of the action (SUCCESS, FAILURE, PARTIAL) |\n\n**Common Audit Event Types:**\n\n| Event Type | Actor | Action | Details Captured |\n|------------|-------|---------|------------------|\n| Pipeline Execution | System | EXECUTE | Run parameters, duration, success/failure |\n| Schema Change | User | UPDATE | Old/new schemas, compatibility assessment |\n| Data Access | Pipeline/User | READ | Query executed, rows returned, data volume |\n| Configuration Change | User | UPDATE | Before/after configuration, change reason |\n| User Authentication | User | LOGIN/LOGOUT | Authentication method, success/failure reason |\n\nThe audit trail supports compliance with regulations like GDPR, SOX, and industry-specific requirements by maintaining detailed records of who accessed what data when, and what changes were made to processing logic.\n\n> **Critical Design Insight**: Audit events are write-only and immutable once created. They use append-only storage with cryptographic integrity checking to prevent tampering and ensure compliance requirements are met.\n\n#### Data Quality and Validation History\n\nThe system tracks data quality metrics and validation results over time to identify trends, detect anomalies, and ensure data reliability for downstream consumers. This historical view enables proactive data quality management and root cause analysis.\n\n**Data Quality Metrics:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| dataset_id | str | Dataset being measured |\n| pipeline_run_id | str | Pipeline run that generated these metrics |\n| metric_name | str | Name of quality metric (COMPLETENESS, ACCURACY, CONSISTENCY) |\n| metric_value | float | Measured value for this metric |\n| threshold_min | float | Minimum acceptable value for this metric |\n| threshold_max | float | Maximum acceptable value for this metric |\n| status | str | Quality status (PASS, WARN, FAIL) based on thresholds |\n| measured_at | datetime | When this measurement was taken |\n| sample_size | int | Number of records included in measurement |\n\n**Validation Rule Results:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| rule_id | str | Identifier for the validation rule |\n| dataset_id | str | Dataset being validated |\n| rule_type | str | Type of validation (NOT_NULL, RANGE_CHECK, FORMAT_VALIDATION) |\n| rule_expression | str | Actual validation logic or SQL expression |\n| records_checked | int | Total number of records evaluated |\n| records_passed | int | Number of records that satisfied the rule |\n| records_failed | int | Number of records that violated the rule |\n| failure_examples | List[dict] | Sample records that failed validation |\n| execution_time_ms | float | Time taken to execute this validation rule |\n\nThis quality tracking enables automated alerting when data quality degrades and provides historical context for understanding data reliability trends over time.\n\n **Pitfall: Metadata Storage Performance**\n\nLineage and audit systems generate high-volume write workloads that can impact operational system performance. Design these systems with separate storage infrastructure and asynchronous processing to avoid slowing down pipeline execution. Consider using time-series databases or append-only storage optimized for write-heavy workloads.\n\n### Implementation Guidance\n\nThe data model implementation requires careful consideration of storage technologies, access patterns, and consistency requirements. The following guidance provides practical recommendations for implementing a production-ready system.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Pipeline Definitions | JSON files + file system | PostgreSQL with versioning |\n| Runtime State | SQLite with WAL mode | PostgreSQL with connection pooling |\n| Metrics Storage | In-memory + periodic dumps | InfluxDB or TimescaleDB |\n| Lineage Graph | PostgreSQL with recursive queries | Neo4j graph database |\n| Audit Log | Structured log files | Elasticsearch with retention policies |\n| Schema Registry | JSON Schema + file storage | Confluent Schema Registry |\n\n#### Recommended File Structure\n\n```python\netl_system/\n models/\n    __init__.py\n    pipeline.py          # PipelineDefinition, TaskDefinition\n    execution.py         # TaskExecution, PipelineRun\n    lineage.py          # Dataset, LineageEdge\n    audit.py            # AuditEvent, QualityMetric\n storage/\n    __init__.py\n    pipeline_store.py    # Pipeline CRUD operations\n    execution_store.py   # Runtime state management\n    metadata_store.py    # Lineage and audit storage\n validation/\n    __init__.py\n    pipeline_validator.py # Pipeline validation logic\n    schema_validator.py  # Schema compatibility checking\n migrations/\n     001_initial_schema.sql\n     002_add_lineage.sql\n     003_audit_indices.sql\n```\n\n#### Infrastructure Starter Code\n\n**Database Schema Setup (Complete):**\n\n```python\n# storage/schema.py\nfrom sqlalchemy import create_engine, Column, String, Integer, DateTime, Float, JSON, Text, Boolean\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, relationship\nfrom sqlalchemy.dialects.postgresql import UUID\nimport uuid\nfrom datetime import datetime\n\nBase = declarative_base()\n\nclass PipelineDefinitionModel(Base):\n    __tablename__ = 'pipeline_definitions'\n    \n    id = Column(String, primary_key=True)\n    name = Column(String, nullable=False)\n    description = Column(Text)\n    schedule = Column(String)\n    tasks = Column(JSON, nullable=False)  # Serialized TaskDefinition list\n    parameters = Column(JSON, default=dict)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    version = Column(Integer, nullable=False, default=1)\n    \n    # Relationships\n    pipeline_runs = relationship(\"PipelineRunModel\", back_populates=\"pipeline\")\n\nclass PipelineRunModel(Base):\n    __tablename__ = 'pipeline_runs'\n    \n    run_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n    pipeline_id = Column(String, nullable=False)\n    pipeline_version = Column(Integer, nullable=False)\n    triggered_by = Column(String, nullable=False)\n    triggered_at = Column(DateTime, default=datetime.utcnow)\n    started_at = Column(DateTime)\n    completed_at = Column(DateTime)\n    state = Column(String, nullable=False, default='PENDING')\n    parameters = Column(JSON, default=dict)\n    \n    # Relationships\n    pipeline = relationship(\"PipelineDefinitionModel\", back_populates=\"pipeline_runs\")\n    task_executions = relationship(\"TaskExecutionModel\", back_populates=\"pipeline_run\")\n\nclass TaskExecutionModel(Base):\n    __tablename__ = 'task_executions'\n    \n    execution_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n    task_id = Column(String, nullable=False)\n    pipeline_run_id = Column(String, nullable=False)\n    state = Column(String, nullable=False, default='PENDING')\n    attempt_count = Column(Integer, default=1)\n    started_at = Column(DateTime)\n    completed_at = Column(DateTime)\n    error_message = Column(Text)\n    logs = Column(JSON, default=list)  # List of log entries\n    metrics = Column(JSON, default=dict)\n    \n    # Relationships\n    pipeline_run = relationship(\"PipelineRunModel\", back_populates=\"task_executions\")\n\n# Database connection and session management\ndef create_database_engine(connection_string: str):\n    \"\"\"Create SQLAlchemy engine with connection pooling.\"\"\"\n    engine = create_engine(\n        connection_string,\n        pool_size=20,\n        max_overflow=30,\n        pool_pre_ping=True,  # Verify connections before use\n        echo=False  # Set to True for SQL debugging\n    )\n    return engine\n\ndef create_session_factory(engine):\n    \"\"\"Create session factory for database operations.\"\"\"\n    return sessionmaker(bind=engine, expire_on_commit=False)\n\ndef initialize_database(engine):\n    \"\"\"Create all tables and initial data.\"\"\"\n    Base.metadata.create_all(engine)\n```\n\n**State Management Utilities (Complete):**\n\n```python\n# models/state.py\nfrom enum import Enum\nfrom typing import Dict, Set, Optional\nfrom dataclasses import dataclass\n\nclass TaskState(Enum):\n    PENDING = \"PENDING\"\n    WAITING = \"WAITING\"\n    QUEUED = \"QUEUED\"\n    RUNNING = \"RUNNING\"\n    SUCCESS = \"SUCCESS\"\n    FAILED = \"FAILED\"\n    RETRYING = \"RETRYING\"\n    CANCELLED = \"CANCELLED\"\n    SKIPPED = \"SKIPPED\"\n\nclass TaskEvent(Enum):\n    DEPENDENCIES_MET = \"DEPENDENCIES_MET\"\n    EXECUTION_STARTED = \"EXECUTION_STARTED\"\n    EXECUTION_COMPLETED = \"EXECUTION_COMPLETED\"\n    EXECUTION_FAILED = \"EXECUTION_FAILED\"\n    RETRY_SCHEDULED = \"RETRY_SCHEDULED\"\n    MAX_RETRIES_EXCEEDED = \"MAX_RETRIES_EXCEEDED\"\n    CANCELLED_BY_USER = \"CANCELLED_BY_USER\"\n    UPSTREAM_FAILED = \"UPSTREAM_FAILED\"\n\n# State transition mapping - defines valid transitions\nTRANSITIONS: Dict[TaskState, Dict[TaskEvent, TaskState]] = {\n    TaskState.PENDING: {\n        TaskEvent.DEPENDENCIES_MET: TaskState.QUEUED,\n        TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.WAITING: {\n        TaskEvent.DEPENDENCIES_MET: TaskState.QUEUED,\n        TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.QUEUED: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.RUNNING: {\n        TaskEvent.EXECUTION_COMPLETED: TaskState.SUCCESS,\n        TaskEvent.EXECUTION_FAILED: TaskState.FAILED,  # Will be modified by retry logic\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    TaskState.RETRYING: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.MAX_RETRIES_EXCEEDED: TaskState.FAILED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n    },\n    # Terminal states have no outgoing transitions\n    TaskState.SUCCESS: {},\n    TaskState.FAILED: {},\n    TaskState.CANCELLED: {},\n    TaskState.SKIPPED: {},\n}\n\ndef get_terminal_states() -> Set[TaskState]:\n    \"\"\"Return states that represent completed execution.\"\"\"\n    return {TaskState.SUCCESS, TaskState.FAILED, TaskState.CANCELLED, TaskState.SKIPPED}\n\ndef is_valid_transition(current_state: TaskState, event: TaskEvent) -> bool:\n    \"\"\"Check if a state transition is valid.\"\"\"\n    return event in TRANSITIONS.get(current_state, {})\n\ndef get_next_state(current_state: TaskState, event: TaskEvent) -> Optional[TaskState]:\n    \"\"\"Get the next state for a given current state and event.\"\"\"\n    return TRANSITIONS.get(current_state, {}).get(event)\n```\n\n#### Core Logic Skeleton Code\n\n**Pipeline Validation (Skeleton with TODOs):**\n\n```python\n# validation/pipeline_validator.py\nfrom typing import List, Set, Dict\nfrom models.pipeline import PipelineDefinition, TaskDefinition\n\ndef validate_pipeline(pipeline: PipelineDefinition) -> List[str]:\n    \"\"\"\n    Validate pipeline definition and return list of error messages.\n    Empty list indicates valid pipeline.\n    \"\"\"\n    errors = []\n    \n    # TODO 1: Validate pipeline-level fields (id, name, schedule format)\n    # Hint: Use croniter library to validate cron expressions\n    \n    # TODO 2: Extract all task IDs and check for duplicates\n    # Hint: Use set() to detect duplicates in task ID list\n    \n    # TODO 3: Validate each task definition individually\n    # Hint: Call validate_task_definition for each task\n    \n    # TODO 4: Build dependency graph and check for cycles\n    # Hint: Use depth-first search with visited/visiting sets\n    \n    # TODO 5: Verify all dependency references point to valid tasks\n    # Hint: Check that every dependency ID exists in task_ids set\n    \n    # TODO 6: Check for unreachable tasks (tasks with no path from roots)\n    # Hint: Find tasks with no dependencies, then traverse reachable tasks\n    \n    return errors\n\ndef validate_task_definition(task: TaskDefinition, valid_task_ids: Set[str]) -> List[str]:\n    \"\"\"Validate individual task definition.\"\"\"\n    errors = []\n    \n    # TODO 1: Check required fields are present and valid format\n    # TODO 2: Validate task type is supported\n    # TODO 3: Validate retry policy parameters are reasonable\n    # TODO 4: Check timeout_seconds is positive\n    # TODO 5: Validate task-specific configuration based on task type\n    \n    return errors\n\ndef detect_cycles(tasks: List[TaskDefinition]) -> List[str]:\n    \"\"\"Detect cycles in task dependency graph using DFS.\"\"\"\n    # TODO 1: Build adjacency list from task dependencies\n    # TODO 2: Initialize visited and visiting sets for DFS\n    # TODO 3: For each unvisited task, start DFS traversal\n    # TODO 4: Mark nodes as visiting when entering, visited when exiting\n    # TODO 5: If we encounter a visiting node, we found a cycle\n    # TODO 6: Return detailed cycle information for debugging\n    \n    pass  # Implementation goes here\n\ndef find_unreachable_tasks(tasks: List[TaskDefinition]) -> List[str]:\n    \"\"\"Find tasks that cannot be reached from root tasks.\"\"\"\n    # TODO 1: Find root tasks (tasks with no dependencies)\n    # TODO 2: Build adjacency list for forward traversal\n    # TODO 3: Perform BFS/DFS from all root tasks\n    # TODO 4: Mark all reachable tasks\n    # TODO 5: Return list of unreachable task IDs\n    \n    pass  # Implementation goes here\n```\n\n**State Transition Logic (Skeleton with TODOs):**\n\n```python\n# models/execution.py\nfrom typing import Optional\nfrom datetime import datetime\nfrom models.state import TaskState, TaskEvent, TRANSITIONS\n\ndef transition(execution: 'TaskExecution', event: TaskEvent, error_message: Optional[str] = None) -> bool:\n    \"\"\"\n    Attempt state transition based on event. Returns True if transition succeeded.\n    Updates execution object in-place if successful.\n    \"\"\"\n    current_state = TaskState(execution.state)\n    \n    # TODO 1: Check if transition is valid using TRANSITIONS mapping\n    # Hint: Use is_valid_transition helper function\n    \n    # TODO 2: Handle retry logic for EXECUTION_FAILED event\n    # Check if task has remaining retry attempts and error type is retryable\n    # Modify target state to RETRYING instead of FAILED if retry is appropriate\n    \n    # TODO 3: Update execution object with new state\n    # Set appropriate timestamps (started_at, completed_at)\n    # Update error_message if provided\n    # Increment attempt_count for retries\n    \n    # TODO 4: Log the state transition for audit trail\n    # Include old state, new state, event, and timestamp\n    \n    # TODO 5: Return success/failure based on whether transition was applied\n    \n    pass  # Implementation goes here\n\ndef should_retry(execution: 'TaskExecution', error_message: str) -> bool:\n    \"\"\"Determine if task execution should be retried based on retry policy.\"\"\"\n    # TODO 1: Check if attempt_count < retry_policy.max_attempts\n    # TODO 2: Check if error type matches retry_on_error_types\n    # TODO 3: Return boolean indicating if retry should happen\n    \n    pass  # Implementation goes here\n\ndef calculate_retry_delay(execution: 'TaskExecution') -> int:\n    \"\"\"Calculate delay in seconds before next retry attempt.\"\"\"\n    # TODO 1: Get base delay from retry_policy.backoff_seconds\n    # TODO 2: Apply exponential backoff if retry_policy.exponential_backoff is True\n    # TODO 3: Add jitter to prevent thundering herd (random 20%)\n    # TODO 4: Return delay in seconds\n    \n    pass  # Implementation goes here\n```\n\n#### Language-Specific Implementation Hints\n\n**Python-Specific Recommendations:**\n\n- Use `SQLAlchemy` with `alembic` for database schema migrations and ORM mapping\n- Use `pydantic` for data validation and serialization with automatic type checking\n- Use `enum.Enum` for state definitions to ensure type safety and IDE support\n- Use `dataclasses` or `pydantic.BaseModel` for data transfer objects between components\n- Use `asyncio` and `asyncpg` for high-performance async database operations if needed\n- Use `structlog` for structured logging with consistent field names and JSON output\n- Use `pytest` with `pytest-asyncio` for comprehensive testing including async code paths\n\n**Database Optimization Tips:**\n\n- Create indexes on frequently queried fields: `pipeline_run_id`, `state`, `started_at`\n- Use database constraints to enforce data integrity (foreign keys, check constraints)\n- Consider partitioning large tables by time (`completed_at`) for better query performance\n- Use connection pooling to handle concurrent database access efficiently\n- Implement database health checks and circuit breakers for resilience\n\n**Concurrency Considerations:**\n\n- Use optimistic locking for state transitions to handle concurrent updates safely\n- Implement database transactions for operations that must be atomic\n- Consider using database-level advisory locks for critical sections\n- Design for idempotency - operations should be safe to retry\n\n#### Milestone Checkpoints\n\n**Checkpoint 1: Basic Data Model (After Pipeline Definition Implementation)**\n- **Test Command**: `python -m pytest tests/test_models.py -v`\n- **Expected Output**: All basic model creation and validation tests pass\n- **Manual Verification**: \n  1. Create a simple pipeline definition with 3 tasks\n  2. Verify cycle detection catches circular dependencies\n  3. Confirm validation rejects invalid cron expressions\n- **Success Criteria**: Pipeline definitions can be created, validated, and stored without errors\n\n**Checkpoint 2: State Management (After Runtime State Implementation)**\n- **Test Command**: `python -m pytest tests/test_state_machine.py -v`\n- **Expected Output**: All state transition tests pass, invalid transitions are rejected\n- **Manual Verification**:\n  1. Create task execution and verify initial PENDING state\n  2. Trigger valid transitions and confirm state changes\n  3. Attempt invalid transitions and verify they're rejected\n- **Success Criteria**: State machine enforces valid transitions and tracks execution history\n\n**Checkpoint 3: Metadata Integration (After Lineage Implementation)**\n- **Test Command**: `python -m pytest tests/test_lineage.py -v`\n- **Expected Output**: Lineage tracking and schema evolution tests pass\n- **Manual Verification**:\n  1. Execute pipeline and verify lineage relationships are created\n  2. Query upstream and downstream datasets for a given dataset\n  3. Test schema evolution with compatible and breaking changes\n- **Success Criteria**: Complete data provenance tracking with schema evolution support\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Pipeline validation hangs | Circular dependency in large DAG | Add logging to cycle detection algorithm, check task dependency chains | Implement cycle detection with proper visited tracking |\n| State transitions fail silently | Missing error handling in transition logic | Add logging to transition function, check TRANSITIONS mapping | Ensure all valid transitions are defined in state machine |\n| Database deadlocks during concurrent runs | Multiple transactions updating same records | Enable database deadlock logging, identify conflicting queries | Use consistent lock ordering, shorter transactions |\n| Memory usage grows over time | Task execution objects not garbage collected | Profile memory usage, check for circular references | Implement proper cleanup after task completion |\n| Lineage queries are slow | Missing database indexes on relationship tables | Use EXPLAIN ANALYZE on slow queries | Add indexes on source_dataset_id, target_dataset_id |\n| Schema validation errors unclear | Generic error messages in validation code | Add detailed error context with field names and values | Include specific field validation errors with suggested fixes |\n\n\n## DAG Definition and Validation Engine\n\n> **Milestone(s):** Milestone 1 - Pipeline DAG Definition: Implements pipeline definition with dependencies as directed acyclic graph, including DAG parsing, validation, cycle detection, and visualization capabilities.\n\n### Mental Model: Recipe Dependencies\n\nThink of an ETL pipeline like preparing a complex multi-course dinner where dishes have strict preparation dependencies. Just as you cannot plate the main course before cooking it, or start cooking before chopping ingredients, ETL tasks must execute in a precise order based on their dependencies.\n\nConsider making a Thanksgiving dinner: you must thaw the turkey before seasoning it, season it before cooking it, cook it before carving it, and carve it before serving. Meanwhile, side dishes like mashed potatoes can be prepared in parallel with the turkey cooking, but they depend on earlier steps like peeling and boiling potatoes. The cranberry sauce is completely independent and can be made at any time.\n\nThis cooking analogy maps perfectly to ETL pipelines. Each recipe step is a **task** with specific inputs (dependencies) and outputs (results). The overall meal plan is the **pipeline definition**. Some tasks can run in parallel (like cooking turkey and preparing vegetables simultaneously), while others must wait for their dependencies (you cannot make gravy until the turkey drippings are ready). The critical insight is that we need to validate the entire recipe before we start cooking to ensure we never encounter impossible situations like circular dependencies (needing the gravy to cook the turkey, but needing the turkey to make the gravy).\n\nThe **DAG Definition and Validation Engine** serves as both the head chef who designs the meal plan and the kitchen manager who ensures the recipe is logically sound before any cooking begins. It parses recipe definitions (YAML/Python configs), validates that all dependencies make sense (cycle detection), determines the optimal preparation order (topological sorting), and creates visual cooking schedules (DAG visualization) that kitchen staff can follow.\n\n### DAG Parsing and Validation\n\nThe DAG parsing and validation system transforms human-readable pipeline definitions into validated execution graphs that the orchestration engine can safely execute. This process involves multiple stages of parsing, validation, and graph construction, each with specific responsibilities and error handling requirements.\n\n**Pipeline Definition Parsing**\n\nThe system supports two primary formats for pipeline definitions: YAML configuration files and Python-based definitions. YAML provides a declarative approach suitable for data engineers who prefer configuration-over-code, while Python definitions offer programmatic flexibility for complex dynamic pipelines.\n\nThe parsing engine first loads the raw configuration and performs structural validation to ensure all required fields are present and properly typed. The parser constructs `PipelineDefinition` objects that contain all necessary metadata and task specifications.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| id | str | Unique identifier for the pipeline across the entire system |\n| name | str | Human-readable display name for UI and logging purposes |\n| description | str | Detailed explanation of pipeline purpose and data flow |\n| schedule | str | Cron expression or event trigger specification for execution timing |\n| tasks | List[TaskDefinition] | Complete list of all tasks in the pipeline with their configurations |\n| parameters | dict | Global pipeline parameters that can be referenced by tasks at runtime |\n| created_at | datetime | Pipeline creation timestamp for versioning and audit purposes |\n| version | int | Pipeline version number for schema evolution and rollback support |\n\nEach `TaskDefinition` within the pipeline contains comprehensive metadata about individual task execution requirements and dependencies:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| id | str | Unique task identifier within the pipeline scope |\n| name | str | Human-readable task name for monitoring and debugging |\n| type | str | Task type indicating which executor should handle this task (extract, transform, load) |\n| config | dict | Task-specific configuration parameters including connection details and operations |\n| dependencies | List[str] | List of upstream task IDs that must complete successfully before this task runs |\n| retry_policy | RetryPolicy | Configuration for failure handling and automatic retry behavior |\n| timeout_seconds | int | Maximum execution time before the task is considered failed and terminated |\n\nThe retry policy configuration provides fine-grained control over failure recovery behavior:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| max_attempts | int | Maximum number of execution attempts including the initial attempt |\n| backoff_seconds | int | Base delay between retry attempts in seconds |\n| exponential_backoff | bool | Whether to apply exponential backoff increasing delay between retries |\n| retry_on_error_types | List[str] | Specific error types that should trigger retries vs immediate failure |\n\n> **Decision: Support Both YAML and Python Pipeline Definitions**\n> - **Context**: Data engineers have varying preferences for configuration management - some prefer declarative YAML while others need programmatic Python flexibility\n> - **Options Considered**: YAML-only (simple but limited), Python-only (flexible but complex), Both formats (flexible but more maintenance)\n> - **Decision**: Support both YAML and Python definitions with a common internal representation\n> - **Rationale**: YAML handles 80% of use cases with better readability, while Python enables complex dynamic pipelines and parameterization that configuration files cannot express\n> - **Consequences**: Increases parser complexity but maximizes adoption across different user preferences and use cases\n\n**Dependency Graph Construction**\n\nAfter parsing individual task definitions, the engine constructs a directed graph representation where nodes represent tasks and edges represent dependencies. This graph construction process validates referential integrity by ensuring all declared dependencies reference existing tasks within the pipeline.\n\nThe graph construction algorithm follows these steps:\n\n1. Create a node for each task definition using the task ID as the unique identifier\n2. Iterate through each task's dependency list and verify that each referenced task ID exists in the pipeline\n3. Create directed edges from dependency tasks to the current task (upstream  downstream)\n4. Build adjacency lists for efficient graph traversal during validation and execution planning\n5. Create reverse adjacency lists to support upstream impact analysis during failures\n\nThe resulting graph structure supports multiple query patterns required by the orchestration engine:\n\n- **Forward traversal**: Finding all downstream tasks affected by a failure\n- **Backward traversal**: Identifying all upstream dependencies that must complete before a task can run\n- **Parallel task identification**: Finding tasks with satisfied dependencies that can execute simultaneously\n- **Critical path analysis**: Determining the longest dependency chain that affects overall pipeline completion time\n\n**Cycle Detection Algorithm**\n\nCycle detection is the most critical validation step, as cyclical dependencies would cause the pipeline to deadlock indefinitely. The engine implements depth-first search (DFS) with coloring to detect cycles efficiently in O(V + E) time complexity where V is tasks and E is dependencies.\n\nThe three-color DFS algorithm works as follows:\n\n1. Initialize all tasks as WHITE (unvisited)\n2. For each WHITE task, start a DFS traversal\n3. Mark the current task as GRAY (currently being processed) when entering its DFS branch\n4. Recursively visit all downstream tasks (dependencies of current task)\n5. If we encounter a GRAY task during traversal, we have found a cycle\n6. Mark the task as BLACK (completely processed) when finishing its DFS branch\n7. Report any cycles found with the specific tasks involved for debugging\n\nThe coloring approach distinguishes between back edges (which indicate cycles) and forward/cross edges (which are harmless). This precision prevents false positives that simpler visited/unvisited algorithms might produce in complex graphs.\n\n> The critical insight is that GRAY nodes represent the current DFS path stack. Finding a GRAY node means we have encountered a task that depends on itself through a chain of intermediate dependencies.\n\n**Validation Rule Engine**\n\nBeyond structural graph validation, the engine enforces semantic business rules that ensure pipeline correctness:\n\n| Validation Rule | Description | Error Handling |\n|----------------|-------------|----------------|\n| Task ID Uniqueness | No duplicate task IDs within a pipeline | Reject pipeline with specific duplicate IDs listed |\n| Dependency Existence | All referenced dependencies must exist as tasks | Report missing task IDs and suggest similar names |\n| Self-Dependency Prevention | Tasks cannot depend on themselves directly | Identify self-referential tasks and remove invalid dependencies |\n| Orphaned Task Detection | All tasks except roots must have at least one path from a root | Warn about unreachable tasks that will never execute |\n| Dangling Dependency Detection | Dependencies must reference valid task IDs | List invalid references with suggestions for correction |\n| Resource Constraint Validation | Task resource requirements must not exceed system limits | Warn about tasks that may fail due to resource constraints |\n\nThe validation engine returns a structured list of errors, warnings, and suggestions rather than failing fast on the first issue. This comprehensive feedback helps pipeline developers fix multiple issues simultaneously rather than discovering them one at a time through trial and error.\n\n**Parameter Substitution and Templating**\n\nPipeline definitions support parameterization through template variables that are resolved at parse time or runtime. The templating system enables environment-specific configurations and dynamic pipeline behavior without duplicating pipeline definitions.\n\nThe parameter resolution process handles multiple scopes with clear precedence rules:\n\n1. **Global pipeline parameters**: Defined at the pipeline level and available to all tasks\n2. **Task-specific parameters**: Override global parameters for individual tasks\n3. **Runtime parameters**: Provided when triggering pipeline execution to customize behavior\n4. **Environment variables**: System environment variables available for configuration injection\n\nTemplate syntax follows standard conventions using double curly braces: `{{parameter_name}}` with support for default values: `{{parameter_name|default_value}}`. The templating engine validates that all referenced parameters are defined and substitutes values before final validation occurs.\n\n**Configuration Schema Validation**\n\nThe parsing engine enforces strict schema validation to catch configuration errors early in the development cycle. JSON Schema definitions specify the exact structure, types, and constraints for pipeline and task configurations.\n\nSchema validation covers multiple levels:\n\n- **Structural validation**: Required fields, correct types, valid enumerations\n- **Cross-field validation**: Constraints that span multiple configuration fields\n- **Business rule validation**: Domain-specific rules like valid cron expressions and connection string formats\n- **Version compatibility**: Ensuring configurations are compatible with the current engine version\n\nThe validation system provides detailed error messages with field paths, expected vs. actual values, and suggested corrections. This comprehensive feedback reduces the debugging time for pipeline developers and prevents runtime failures due to misconfiguration.\n\n### Execution Order Resolution\n\nOnce the DAG passes validation, the execution order resolution system determines the optimal sequence for task execution while maximizing parallelism and respecting all dependency constraints. This process involves sophisticated graph algorithms that balance execution efficiency with resource utilization.\n\n**Topological Sorting Algorithm**\n\nThe foundation of execution order resolution is topological sorting, which produces a linear ordering of tasks such that for every dependency relationship, the upstream task appears before the downstream task in the sequence. The engine implements Kahn's algorithm for its stability and intuitive behavior.\n\nKahn's algorithm operates through the following steps:\n\n1. Calculate the **in-degree** (number of incoming dependencies) for each task\n2. Initialize a queue with all tasks that have zero in-degree (no dependencies)\n3. While the queue is not empty, remove a task from the queue and add it to the result sequence\n4. For each downstream task dependent on the current task, decrement its in-degree\n5. If decrementing causes a downstream task's in-degree to reach zero, add it to the queue\n6. Continue until all tasks are processed or the queue becomes empty with tasks remaining\n\nThe algorithm naturally handles parallel execution opportunities by identifying tasks with zero in-degree at each step. Tasks removed from the queue simultaneously can execute in parallel since they have no mutual dependencies.\n\n**Parallel Execution Planning**\n\nBeyond basic topological ordering, the execution planner groups tasks into **execution levels** that maximize parallelism while respecting resource constraints. Each execution level contains tasks that can run simultaneously without violating dependencies.\n\nThe level-based execution planning algorithm works as follows:\n\n1. Start with level 0 containing all tasks with no dependencies\n2. For each subsequent level, include tasks whose dependencies are all satisfied by previous levels\n3. Continue creating levels until all tasks are assigned\n4. Within each level, apply resource-based scheduling to avoid oversubscription\n5. Generate execution plan with explicit parallelism annotations\n\nThis approach provides several advantages over simple topological sorting:\n\n- **Resource optimization**: Tasks within a level can be scheduled based on available CPU, memory, and I/O capacity\n- **Failure isolation**: If a task fails, only subsequent levels containing dependent tasks need to be cancelled\n- **Progress visualization**: Users can see pipeline progress as completion percentages within each level\n- **Dynamic rescheduling**: Failed tasks can be retried without recalculating the entire execution plan\n\n**Critical Path Analysis**\n\nThe execution planner performs critical path analysis to identify the longest dependency chain that determines minimum pipeline completion time. This analysis helps with resource allocation decisions and provides realistic completion time estimates.\n\nCritical path calculation involves:\n\n1. Calculate the **longest path** from each task to any terminal task (task with no downstream dependencies)\n2. Identify tasks that lie on the critical path - any delay in these tasks delays the entire pipeline\n3. Prioritize critical path tasks for resource allocation and monitoring\n4. Provide completion time estimates based on critical path length and historical task durations\n\nTasks not on the critical path have **slack time** - the amount they can be delayed without affecting overall pipeline completion. The scheduler uses slack time information to optimize resource utilization and handle transient failures gracefully.\n\n**Dynamic Dependency Resolution**\n\nSome pipelines require dynamic dependency resolution where task dependencies are determined at runtime based on data characteristics or external conditions. The execution planner supports conditional dependencies through predicate evaluation.\n\nDynamic dependency types include:\n\n| Dependency Type | Description | Resolution Strategy |\n|----------------|-------------|-------------------|\n| Conditional Dependencies | Dependencies that apply only when specific conditions are met | Evaluate predicates before adding edges to execution graph |\n| Data-Driven Dependencies | Dependencies determined by upstream task output characteristics | Re-evaluate dependency graph after each task completion |\n| External Dependencies | Dependencies on external systems or scheduled events | Poll external conditions before marking dependencies as satisfied |\n| Parameterized Dependencies | Dependencies that vary based on pipeline parameters | Resolve parameter values before constructing dependency graph |\n\nThe dynamic resolution system maintains the execution graph as a mutable structure that can be safely modified during pipeline execution while preserving consistency and preventing cycles.\n\n**Resource-Aware Scheduling**\n\nThe execution planner incorporates resource constraints to prevent system overload and ensure stable pipeline execution. Resource-aware scheduling considers multiple constraint types:\n\n| Resource Type | Constraint | Scheduling Strategy |\n|---------------|------------|-------------------|\n| CPU Cores | Maximum concurrent CPU-intensive tasks | Queue CPU-bound tasks when core limit reached |\n| Memory | Total memory allocation across running tasks | Delay memory-intensive tasks until sufficient memory available |\n| Database Connections | Connection pool limits per database | Serialize tasks accessing the same database when pool exhausted |\n| API Rate Limits | External API call quotas and rate limits | Space API-dependent tasks to respect rate limiting windows |\n| File System I/O | Concurrent file operations and disk bandwidth | Balance file-intensive tasks across available storage devices |\n\nThe scheduler maintains resource allocation tracking and updates availability as tasks start and complete. This real-time resource management prevents cascading failures due to resource exhaustion while maximizing system utilization.\n\n**Execution Plan Optimization**\n\nThe final execution plan undergoes optimization to improve overall pipeline performance and resource efficiency. Optimization techniques include:\n\n**Task Batching**: Small tasks with similar resource requirements are batched together to reduce orchestration overhead and improve resource locality.\n\n**Prefetching**: The planner identifies opportunities to prefetch data or establish connections before tasks need them, reducing task startup latency.\n\n**Load Balancing**: Tasks are distributed across available execution nodes to balance resource utilization and prevent hotspots.\n\n**Checkpoint Placement**: Strategic checkpoints are inserted to enable efficient failure recovery without recomputing the entire execution plan.\n\nThe optimized execution plan includes detailed scheduling metadata that the orchestration engine uses for efficient task execution and resource management.\n\n### DAG Visualization\n\nDAG visualization transforms the abstract dependency graph into intuitive visual representations that help pipeline developers understand, debug, and communicate about complex data workflows. The visualization system provides multiple rendering modes optimized for different use cases and audiences.\n\n**Graph Rendering Engine**\n\nThe visualization engine converts the internal DAG representation into multiple output formats suitable for different consumption scenarios. The core rendering pipeline processes the validated DAG structure and applies layout algorithms to create readable visual representations.\n\nThe rendering process involves several stages:\n\n1. **Node positioning**: Apply graph layout algorithms (hierarchical, force-directed, or circular) to determine optimal task placement\n2. **Edge routing**: Calculate connection paths between tasks that minimize visual clutter and overlapping\n3. **Visual styling**: Apply consistent colors, shapes, and annotations based on task types and states\n4. **Interactive elements**: Add hover tooltips, click handlers, and zoom/pan capabilities for exploration\n5. **Export generation**: Produce static images, interactive HTML, or embeddable SVG formats\n\n**Layout Algorithms**\n\nDifferent graph layout algorithms serve different visualization needs and user preferences:\n\n| Algorithm | Use Case | Advantages | Limitations |\n|-----------|----------|------------|-------------|\n| Hierarchical (Layered) | Sequential pipelines with clear stages | Shows execution order clearly, minimal edge crossings | Requires acyclic graphs, can be wide with many parallel tasks |\n| Force-Directed | Complex interconnected pipelines | Handles arbitrary graph structures, visually appealing | May not show execution order clearly, can be unstable |\n| Circular | Pipelines with hub-and-spoke patterns | Compact representation, good for radial dependencies | Difficult to read with many levels, edge crossings |\n| Grid-Based | Pipelines with regular structure | Predictable layout, easy to align with external systems | Rigid, may waste space, doesn't adapt to graph structure |\n\nThe visualization engine automatically selects the most appropriate algorithm based on graph characteristics but allows manual override for specific presentation needs.\n\n> **Decision: Support Multiple Layout Algorithms with Automatic Selection**\n> - **Context**: Different pipeline structures benefit from different visualization approaches, and users have varying preferences for graph layout\n> - **Options Considered**: Single algorithm (simple but limited), User choice only (overwhelming for new users), Automatic with override (complexity but optimal UX)\n> - **Decision**: Implement automatic algorithm selection with manual override capability\n> - **Rationale**: Automatic selection provides good defaults for 90% of cases while expert users can choose specific algorithms for presentation or debugging needs\n> - **Consequences**: Increases implementation complexity but dramatically improves user experience across different pipeline types\n\n**Interactive Features**\n\nModern DAG visualization requires interactive capabilities that enable pipeline exploration and debugging workflows. The visualization system implements rich interactivity through web-based rendering with JavaScript integration.\n\nCore interactive features include:\n\n**Node Inspection**: Clicking on task nodes displays detailed information including configuration parameters, execution history, dependency relationships, and current status. The inspection panel shows both design-time configuration and runtime execution metrics.\n\n**Dependency Tracing**: Users can highlight dependency chains by selecting a task and visualizing all upstream dependencies (what must complete before this task) or downstream impacts (what depends on this task). This tracing capability is essential for impact analysis during debugging.\n\n**Execution Replay**: The visualization can animate historical pipeline executions by showing task state changes over time. This temporal visualization helps understand execution bottlenecks, failure propagation, and resource utilization patterns.\n\n**Real-time Updates**: During live pipeline execution, the visualization updates task states in real-time through WebSocket connections to the monitoring system. Color coding and progress indicators provide immediate feedback on pipeline health.\n\n**Filtering and Search**: Large pipelines benefit from filtering capabilities that hide or highlight specific task types, execution states, or dependency patterns. Search functionality enables quick navigation to specific tasks by name or configuration attributes.\n\n**Graph Navigation**: Zoom, pan, and minimap features enable exploration of large complex pipelines that don't fit entirely on screen. The navigation system maintains context and provides orientation cues for user spatial awareness.\n\n**Visual Design System**\n\nConsistent visual design creates intuitive understanding across different pipelines and reduces cognitive load for users switching between projects. The design system defines standardized visual elements and their meanings.\n\n**Node Styling Convention**:\n\n| Task Type | Shape | Color | Icon |\n|-----------|-------|--------|------|\n| Extract | Rectangle | Blue | Database/API symbol |\n| Transform | Diamond | Green | Gear/Function symbol |\n| Load | Rectangle | Orange | Target/Destination symbol |\n| Validation | Hexagon | Purple | Check/Warning symbol |\n| Notification | Circle | Yellow | Bell/Message symbol |\n\n**Edge Styling Convention**:\n\n| Dependency Type | Line Style | Color | Arrow Style |\n|----------------|------------|-------|-------------|\n| Data Dependency | Solid | Black | Standard arrow |\n| Control Dependency | Dashed | Gray | Hollow arrow |\n| Conditional Dependency | Dotted | Blue | Diamond arrow |\n| External Dependency | Dash-dot | Red | Square arrow |\n\n**State Indication System**:\n\n| Task State | Border Color | Fill Pattern | Animation |\n|------------|--------------|---------------|-----------|\n| PENDING | Gray | None | None |\n| RUNNING | Blue | Diagonal stripes | Pulsing |\n| SUCCESS | Green | Solid | None |\n| FAILED | Red | Cross-hatch | None |\n| RETRYING | Orange | Dots | Spinning |\n| SKIPPED | Gray | Faded | None |\n\n**Export and Integration Capabilities**\n\nThe visualization system supports multiple export formats to integrate with documentation systems, monitoring dashboards, and presentation tools.\n\n**Static Export Formats**:\n\n- **SVG**: Vector graphics suitable for documentation and high-quality printing\n- **PNG/JPEG**: Raster images for embedding in presentations and reports\n- **PDF**: Multi-page layouts for large pipelines with detailed annotations\n\n**Interactive Export Formats**:\n\n- **HTML**: Self-contained interactive visualizations that work without server connectivity\n- **Embedding codes**: JavaScript widgets for integration into custom dashboards and monitoring systems\n- **API endpoints**: Real-time data feeds for custom visualization tools\n\n**Integration with External Tools**:\n\nThe visualization engine provides REST API endpoints that external tools can query to retrieve graph data in standardized formats (GraphML, DOT, JSON). This integration capability enables pipeline visualization within existing workflow management tools and custom monitoring dashboards.\n\n**Performance Optimization for Large Pipelines**\n\nLarge enterprise pipelines with hundreds or thousands of tasks require specialized rendering optimizations to maintain interactive performance. The visualization system implements several performance strategies:\n\n**Level-of-Detail Rendering**: Distant or zoomed-out nodes are rendered with simplified graphics while detailed rendering applies only to visible nodes. This technique maintains smooth interaction even with complex graphs.\n\n**Virtual Scrolling**: Only visible portions of large graphs are rendered in the DOM while off-screen elements exist as lightweight data structures. This approach prevents browser performance degradation with massive pipelines.\n\n**Clustering and Summarization**: Related tasks can be visually grouped into clusters with expand/collapse functionality. This hierarchical approach enables users to understand high-level pipeline structure while drilling into specific areas of interest.\n\n**Incremental Updates**: Real-time state updates use efficient differential rendering that updates only changed elements rather than re-rendering the entire graph. This optimization maintains responsiveness during active pipeline execution.\n\n![End-to-End Pipeline Processing](./diagrams/pipeline-processing-flow.svg)\n\n**Common Pitfalls in DAG Visualization**\n\n **Pitfall: Over-complex Layout for Simple Pipelines**\nMany visualization systems apply sophisticated force-directed algorithms to simple sequential pipelines, resulting in unnecessarily complex layouts that obscure the straightforward execution flow. Simple pipelines benefit from hierarchical left-to-right layouts that clearly show the execution sequence. The solution is to analyze graph structure first and select layout algorithms appropriate for the specific pipeline topology.\n\n **Pitfall: Insufficient Visual Distinction Between Task Types**\nUsing similar colors or shapes for different task types creates confusion and makes it difficult to understand pipeline structure at a glance. Users should be able to identify extract, transform, and load tasks immediately through visual cues. The solution is to establish clear visual conventions and apply them consistently across all pipeline visualizations.\n\n **Pitfall: Missing Real-time State Updates**\nStatic visualizations that don't reflect current execution state force users to switch between monitoring tools and pipeline diagrams, reducing debugging efficiency. The visualization should integrate with the monitoring system to show current task states, progress indicators, and error conditions directly on the graph. This integration requires WebSocket connections or polling mechanisms to fetch real-time state updates.\n\n **Pitfall: Poor Performance with Large Graphs**\nNaive rendering implementations become unusably slow with pipelines containing hundreds of tasks, forcing users to avoid the visualization tool when they need it most. The solution involves implementing level-of-detail rendering, virtual scrolling, and clustering techniques to maintain interactive performance regardless of pipeline size.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Graph Processing | NetworkX (Python graphs) | igraph (high-performance graph algorithms) |\n| YAML Parsing | PyYAML (standard library) | ruamel.yaml (preserves comments and formatting) |\n| Visualization Backend | Matplotlib + NetworkX | Graphviz + PyGraphviz |\n| Web Visualization | D3.js + SVG | Cytoscape.js (interactive graph library) |\n| Schema Validation | jsonschema (Python) | Cerberus (more expressive validation rules) |\n| Template Engine | Jinja2 (widely adopted) | Chevron (logic-less mustache templates) |\n\n**B. Recommended File/Module Structure:**\n\n```\netl-pipeline/\n  pipeline_engine/\n    dag/\n      __init__.py                     public API exports\n      parser.py                       YAML/Python config parsing\n      validator.py                    cycle detection and validation\n      executor_planner.py             topological sort and execution planning\n      visualizer.py                   graph rendering and layout\n      models.py                       PipelineDefinition, TaskDefinition classes\n      exceptions.py                   DAG-specific exception classes\n    schemas/\n      pipeline_schema.json            JSON schema for pipeline validation\n      task_schemas/                   task type-specific schemas\n        extract_schema.json\n        transform_schema.json\n        load_schema.json\n    templates/\n      visualization_template.html     HTML template for interactive graphs\n    tests/\n      test_parser.py\n      test_validator.py\n      test_execution_planner.py\n      fixtures/                       sample pipeline definitions\n        simple_pipeline.yaml\n        complex_pipeline.yaml\n        invalid_pipeline.yaml\n```\n\n**C. Infrastructure Starter Code:**\n\n**Pipeline Schema Validation (`schemas/pipeline_schema.json`)**:\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"required\": [\"id\", \"name\", \"tasks\"],\n  \"properties\": {\n    \"id\": {\n      \"type\": \"string\",\n      \"pattern\": \"^[a-zA-Z0-9_-]+$\",\n      \"description\": \"Unique pipeline identifier\"\n    },\n    \"name\": {\n      \"type\": \"string\",\n      \"minLength\": 1,\n      \"description\": \"Human-readable pipeline name\"\n    },\n    \"description\": {\n      \"type\": \"string\",\n      \"description\": \"Pipeline purpose and data flow description\"\n    },\n    \"schedule\": {\n      \"type\": \"string\",\n      \"description\": \"Cron expression or trigger specification\"\n    },\n    \"tasks\": {\n      \"type\": \"array\",\n      \"minItems\": 1,\n      \"items\": {\"$ref\": \"#/definitions/task\"}\n    },\n    \"parameters\": {\n      \"type\": \"object\",\n      \"description\": \"Global pipeline parameters\"\n    }\n  },\n  \"definitions\": {\n    \"task\": {\n      \"type\": \"object\",\n      \"required\": [\"id\", \"name\", \"type\", \"config\"],\n      \"properties\": {\n        \"id\": {\n          \"type\": \"string\",\n          \"pattern\": \"^[a-zA-Z0-9_-]+$\"\n        },\n        \"name\": {\"type\": \"string\", \"minLength\": 1},\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\"extract\", \"transform\", \"load\", \"validate\", \"notify\"]\n        },\n        \"config\": {\"type\": \"object\"},\n        \"dependencies\": {\n          \"type\": \"array\",\n          \"items\": {\"type\": \"string\"}\n        },\n        \"timeout_seconds\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"maximum\": 86400\n        }\n      }\n    }\n  }\n}\n```\n\n**Graph Utility Functions (`dag/graph_utils.py`)**:\n```python\nfrom typing import Dict, List, Set, Tuple, Optional\nfrom collections import defaultdict, deque\nfrom enum import Enum\n\nclass NodeColor(Enum):\n    WHITE = \"white\"  # Unvisited\n    GRAY = \"gray\"    # Currently processing\n    BLACK = \"black\"  # Completely processed\n\nclass CycleDetectionResult:\n    def __init__(self, has_cycle: bool, cycle_path: List[str] = None):\n        self.has_cycle = has_cycle\n        self.cycle_path = cycle_path or []\n\ndef detect_cycles_dfs(adjacency_list: Dict[str, List[str]]) -> CycleDetectionResult:\n    \"\"\"\n    Detect cycles in directed graph using DFS with three-color algorithm.\n    Returns CycleDetectionResult with cycle information if found.\n    \"\"\"\n    colors = {node: NodeColor.WHITE for node in adjacency_list}\n    parent = {node: None for node in adjacency_list}\n    \n    def dfs_visit(node: str, path: List[str]) -> Optional[List[str]]:\n        colors[node] = NodeColor.GRAY\n        current_path = path + [node]\n        \n        for neighbor in adjacency_list.get(node, []):\n            if colors[neighbor] == NodeColor.GRAY:\n                # Found back edge - cycle detected\n                cycle_start = current_path.index(neighbor)\n                return current_path[cycle_start:] + [neighbor]\n            elif colors[neighbor] == NodeColor.WHITE:\n                cycle = dfs_visit(neighbor, current_path)\n                if cycle:\n                    return cycle\n        \n        colors[node] = NodeColor.BLACK\n        return None\n    \n    for node in adjacency_list:\n        if colors[node] == NodeColor.WHITE:\n            cycle = dfs_visit(node, [])\n            if cycle:\n                return CycleDetectionResult(True, cycle)\n    \n    return CycleDetectionResult(False)\n\ndef topological_sort_kahns(adjacency_list: Dict[str, List[str]]) -> List[List[str]]:\n    \"\"\"\n    Perform topological sort using Kahn's algorithm.\n    Returns list of lists where each inner list contains tasks that can run in parallel.\n    \"\"\"\n    # Calculate in-degrees\n    in_degree = defaultdict(int)\n    all_nodes = set(adjacency_list.keys())\n    for node in adjacency_list:\n        for neighbor in adjacency_list[node]:\n            all_nodes.add(neighbor)\n            in_degree[neighbor] += 1\n    \n    # Initialize queue with nodes having zero in-degree\n    queue = deque([node for node in all_nodes if in_degree[node] == 0])\n    result_levels = []\n    \n    while queue:\n        # All nodes in current queue can run in parallel\n        current_level = list(queue)\n        result_levels.append(current_level)\n        queue.clear()\n        \n        # Process current level nodes\n        for node in current_level:\n            for neighbor in adjacency_list.get(node, []):\n                in_degree[neighbor] -= 1\n                if in_degree[neighbor] == 0:\n                    queue.append(neighbor)\n    \n    return result_levels\n\ndef calculate_critical_path(adjacency_list: Dict[str, List[str]], \n                          task_durations: Dict[str, int]) -> Tuple[List[str], int]:\n    \"\"\"\n    Calculate the critical path (longest path) through the DAG.\n    Returns tuple of (critical_path_nodes, total_duration).\n    \"\"\"\n    # Build reverse adjacency list for backward traversal\n    reverse_adj = defaultdict(list)\n    all_nodes = set(adjacency_list.keys())\n    \n    for node in adjacency_list:\n        for neighbor in adjacency_list[node]:\n            reverse_adj[neighbor].append(node)\n            all_nodes.add(neighbor)\n    \n    # Calculate longest path to each node using dynamic programming\n    longest_path = {}\n    path_predecessor = {}\n    \n    def calculate_longest_path_to_node(node: str) -> int:\n        if node in longest_path:\n            return longest_path[node]\n        \n        if not reverse_adj[node]:  # No predecessors - starting node\n            longest_path[node] = task_durations.get(node, 0)\n            return longest_path[node]\n        \n        max_path = 0\n        best_predecessor = None\n        \n        for predecessor in reverse_adj[node]:\n            pred_path = calculate_longest_path_to_node(predecessor)\n            if pred_path > max_path:\n                max_path = pred_path\n                best_predecessor = predecessor\n        \n        longest_path[node] = max_path + task_durations.get(node, 0)\n        path_predecessor[node] = best_predecessor\n        return longest_path[node]\n    \n    # Find the node with maximum longest path (end of critical path)\n    max_duration = 0\n    critical_end = None\n    \n    for node in all_nodes:\n        duration = calculate_longest_path_to_node(node)\n        if duration > max_duration:\n            max_duration = duration\n            critical_end = node\n    \n    # Reconstruct critical path\n    critical_path = []\n    current = critical_end\n    while current is not None:\n        critical_path.append(current)\n        current = path_predecessor.get(current)\n    \n    critical_path.reverse()\n    return critical_path, max_duration\n```\n\n**D. Core Logic Skeleton Code:**\n\n**Pipeline Parser (`dag/parser.py`)**:\n```python\nimport yaml\nimport json\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\nfrom .models import PipelineDefinition, TaskDefinition, RetryPolicy\nfrom .exceptions import PipelineParsingError\nimport jsonschema\n\nclass PipelineParser:\n    def __init__(self, schema_path: str):\n        \"\"\"Initialize parser with JSON schema for validation.\"\"\"\n        with open(schema_path, 'r') as f:\n            self.schema = json.load(f)\n    \n    def parse_yaml_file(self, file_path: str) -> PipelineDefinition:\n        \"\"\"\n        Parse pipeline definition from YAML file.\n        Validates schema and constructs PipelineDefinition object.\n        \"\"\"\n        # TODO 1: Load YAML file and handle file reading errors gracefully\n        # TODO 2: Validate loaded YAML against JSON schema using jsonschema.validate()\n        # TODO 3: Extract pipeline metadata (id, name, description, schedule, parameters)\n        # TODO 4: Parse tasks list and create TaskDefinition objects for each task\n        # TODO 5: Handle template parameter substitution if parameters are provided\n        # TODO 6: Create and return PipelineDefinition object with all parsed data\n        # TODO 7: Wrap any parsing errors in PipelineParsingError with helpful messages\n        \n        # Hint: Use yaml.safe_load() to avoid security issues with arbitrary Python execution\n        # Hint: jsonschema.validate(data, schema) raises ValidationError with detailed messages\n        pass\n    \n    def parse_python_definition(self, definition_func) -> PipelineDefinition:\n        \"\"\"\n        Parse pipeline definition from Python function that returns pipeline dict.\n        Supports dynamic pipeline generation with parameters.\n        \"\"\"\n        # TODO 1: Call definition_func() to get pipeline dictionary\n        # TODO 2: Handle any exceptions from dynamic pipeline generation\n        # TODO 3: Validate returned dictionary against JSON schema\n        # TODO 4: Convert dictionary to PipelineDefinition object (reuse logic from YAML parser)\n        # TODO 5: Return constructed PipelineDefinition\n        \n        # Hint: Python definitions enable dynamic task generation based on runtime conditions\n        pass\n    \n    def _create_task_definition(self, task_dict: Dict[str, Any]) -> TaskDefinition:\n        \"\"\"Convert task dictionary to TaskDefinition object with full validation.\"\"\"\n        # TODO 1: Extract required fields (id, name, type, config) with validation\n        # TODO 2: Parse optional dependencies list, defaulting to empty list\n        # TODO 3: Parse retry_policy dict and create RetryPolicy object, using defaults if not provided\n        # TODO 4: Parse timeout_seconds with reasonable default (e.g., 3600 seconds)\n        # TODO 5: Validate that task type is one of supported types (extract, transform, load, validate, notify)\n        # TODO 6: Create and return TaskDefinition object\n        \n        # Hint: Use .get() method with defaults for optional fields\n        # Hint: Validate task_dict keys against expected schema before accessing\n        pass\n\n    def _substitute_parameters(self, template_str: str, parameters: Dict[str, Any]) -> str:\n        \"\"\"\n        Substitute template variables in configuration strings.\n        Supports {{variable}} and {{variable|default}} syntax.\n        \"\"\"\n        # TODO 1: Find all template variables using regex pattern {{variable_name}} or {{variable_name|default}}\n        # TODO 2: For each template variable, check if parameter exists in parameters dict\n        # TODO 3: If parameter exists, substitute with actual value\n        # TODO 4: If parameter doesn't exist but default is provided, use default value\n        # TODO 5: If parameter doesn't exist and no default, raise PipelineParsingError\n        # TODO 6: Return string with all substitutions completed\n        \n        # Hint: Use re.findall() to find template patterns, then re.sub() for substitution\n        pass\n```\n\n**DAG Validator (`dag/validator.py`)**:\n```python\nfrom typing import List, Dict, Set\nfrom .models import PipelineDefinition\nfrom .graph_utils import detect_cycles_dfs, CycleDetectionResult\nfrom .exceptions import PipelineValidationError\n\nclass DAGValidator:\n    def validate_pipeline(self, pipeline: PipelineDefinition) -> List[str]:\n        \"\"\"\n        Comprehensive pipeline validation returning list of error messages.\n        Empty list indicates valid pipeline.\n        \"\"\"\n        errors = []\n        \n        # TODO 1: Validate pipeline-level constraints (unique ID, valid schedule format)\n        # TODO 2: Build task ID set and check for duplicates within pipeline\n        # TODO 3: Validate each task individually (call _validate_task for each)\n        # TODO 4: Build dependency graph from task definitions\n        # TODO 5: Validate dependency references (all dependencies must exist as tasks)\n        # TODO 6: Perform cycle detection on dependency graph\n        # TODO 7: Check for orphaned tasks (tasks with no path from root tasks)\n        # TODO 8: Validate resource constraints if specified\n        # TODO 9: Return accumulated error list\n        \n        # Hint: Build adjacency list: {task_id: [list_of_dependent_task_ids]}\n        # Hint: Use set operations to find missing dependency references efficiently\n        return errors\n    \n    def _validate_task(self, task: TaskDefinition, all_task_ids: Set[str]) -> List[str]:\n        \"\"\"Validate individual task definition and return any errors found.\"\"\"\n        errors = []\n        \n        # TODO 1: Validate task ID format (alphanumeric, underscore, hyphen only)\n        # TODO 2: Validate task name is non-empty\n        # TODO 3: Validate task type is supported (extract, transform, load, validate, notify)\n        # TODO 4: Check that dependencies list contains only valid task IDs from all_task_ids\n        # TODO 5: Validate retry policy values (max_attempts > 0, backoff_seconds >= 0)\n        # TODO 6: Validate timeout_seconds is reasonable (> 0, < 86400)\n        # TODO 7: Validate task-specific config based on task type\n        \n        # Hint: Use regular expressions for ID format validation\n        # Hint: Task-specific config validation can be extended based on task types\n        return errors\n    \n    def _build_adjacency_list(self, pipeline: PipelineDefinition) -> Dict[str, List[str]]:\n        \"\"\"Build adjacency list representation of task dependency graph.\"\"\"\n        # TODO 1: Initialize adjacency list dictionary with all task IDs as keys\n        # TODO 2: For each task, add its dependencies as outgoing edges\n        # TODO 3: Handle tasks with no dependencies (empty adjacency list)\n        # TODO 4: Return complete adjacency list for graph algorithms\n        \n        # Hint: Adjacency list maps each task to list of tasks that depend on it (downstream tasks)\n        pass\n    \n    def _validate_dependency_references(self, pipeline: PipelineDefinition) -> List[str]:\n        \"\"\"Check that all dependency references point to existing tasks.\"\"\"\n        # TODO 1: Build set of all valid task IDs in pipeline\n        # TODO 2: For each task, check that all its dependencies exist in task ID set\n        # TODO 3: Collect any invalid dependency references\n        # TODO 4: Return list of error messages for invalid references\n        # TODO 5: Suggest similar task names for typos using string similarity\n        \n        # Hint: Use set difference to find invalid references efficiently\n        pass\n    \n    def _detect_orphaned_tasks(self, adjacency_list: Dict[str, List[str]]) -> List[str]:\n        \"\"\"Find tasks that have no path from any root task (tasks with no dependencies).\"\"\"\n        # TODO 1: Identify root tasks (tasks with no incoming dependencies)\n        # TODO 2: Perform BFS/DFS from all root tasks to find reachable tasks\n        # TODO 3: Compare reachable tasks with all tasks to find orphaned ones\n        # TODO 4: Return list of orphaned task IDs\n        \n        # Hint: Build reverse adjacency list to find tasks with no incoming edges\n        pass\n```\n\n**Execution Planner (`dag/execution_planner.py`)**:\n```python\nfrom typing import List, Dict, Optional, Tuple\nfrom .models import PipelineDefinition, TaskDefinition\nfrom .graph_utils import topological_sort_kahns, calculate_critical_path\n\nclass ExecutionPlan:\n    def __init__(self):\n        self.execution_levels: List[List[str]] = []  # Tasks grouped by execution level\n        self.critical_path: List[str] = []           # Tasks on critical path\n        self.estimated_duration: int = 0             # Total estimated runtime\n        self.resource_requirements: Dict[str, int] = {}  # Resource needs per level\n\nclass ExecutionPlanner:\n    def __init__(self, max_parallel_tasks: int = 10):\n        self.max_parallel_tasks = max_parallel_tasks\n    \n    def create_execution_plan(self, pipeline: PipelineDefinition, \n                            task_durations: Optional[Dict[str, int]] = None) -> ExecutionPlan:\n        \"\"\"\n        Create optimized execution plan with parallelization and resource awareness.\n        Returns ExecutionPlan with detailed scheduling information.\n        \"\"\"\n        # TODO 1: Build adjacency list from pipeline task dependencies\n        # TODO 2: Perform topological sort to get execution levels\n        # TODO 3: Apply resource constraints to limit parallelism within each level\n        # TODO 4: Calculate critical path if task durations provided\n        # TODO 5: Estimate total execution time based on critical path and parallelism\n        # TODO 6: Generate resource requirement estimates for each execution level\n        # TODO 7: Create and return ExecutionPlan object with all scheduling data\n        \n        # Hint: Use topological_sort_kahns() from graph_utils for level-based execution\n        # Hint: Resource constraints may split execution levels into smaller batches\n        pass\n    \n    def _apply_resource_constraints(self, execution_levels: List[List[str]], \n                                  pipeline: PipelineDefinition) -> List[List[str]]:\n        \"\"\"Split execution levels that exceed resource constraints into smaller batches.\"\"\"\n        # TODO 1: For each execution level, check if task count exceeds max_parallel_tasks\n        # TODO 2: If level is too large, split into multiple sub-levels\n        # TODO 3: Consider task resource requirements (CPU, memory) when splitting\n        # TODO 4: Maintain dependency ordering when creating sub-levels\n        # TODO 5: Return modified execution levels with resource constraints applied\n        \n        # Hint: Simple approach is to split levels by max_parallel_tasks\n        # Advanced: Consider actual resource requirements from task configs\n        pass\n    \n    def _estimate_execution_time(self, execution_levels: List[List[str]], \n                               task_durations: Dict[str, int]) -> int:\n        \"\"\"Calculate estimated total execution time based on critical path and parallelism.\"\"\"\n        # TODO 1: Sum the maximum task duration within each execution level\n        # TODO 2: Add estimated overhead for task startup and coordination\n        # TODO 3: Consider resource contention delays for large parallel levels\n        # TODO 4: Return total estimated duration in seconds\n        \n        # Hint: Each level's duration is the maximum duration of tasks in that level\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- **NetworkX Integration**: Use `nx.DiGraph()` for graph representation and `nx.is_directed_acyclic_graph()` for quick cycle checking, but implement custom algorithms for better error reporting\n- **YAML Parsing**: Use `yaml.safe_load()` instead of `yaml.load()` to prevent arbitrary code execution security vulnerabilities\n- **JSON Schema Validation**: Install `jsonschema` package and use detailed error messages from `ValidationError.message` for user-friendly feedback\n- **Template Substitution**: Use `re.findall(r'\\{\\{([^}]+)\\}\\}', template_str)` to find template variables, then `str.replace()` for substitution\n- **File Path Handling**: Use `pathlib.Path` for cross-platform file operations and `Path.exists()` for file validation\n- **Error Aggregation**: Collect all validation errors before returning to provide comprehensive feedback rather than failing on first error\n\n**F. Milestone Checkpoint:**\n\nAfter implementing DAG parsing and validation:\n\n1. **Basic Functionality Test**:\n```bash\npython -m pytest pipeline_engine/dag/tests/test_parser.py -v\npython -m pytest pipeline_engine/dag/tests/test_validator.py -v\n```\n\n2. **Integration Test**:\nCreate a test pipeline YAML file and verify parsing:\n```python\nfrom pipeline_engine.dag.parser import PipelineParser\nfrom pipeline_engine.dag.validator import DAGValidator\n\nparser = PipelineParser('schemas/pipeline_schema.json')\nvalidator = DAGValidator()\n\n# Should parse successfully\npipeline = parser.parse_yaml_file('tests/fixtures/simple_pipeline.yaml')\nerrors = validator.validate_pipeline(pipeline)\nassert len(errors) == 0, f\"Validation errors: {errors}\"\n\n# Should detect cycle in invalid pipeline\ninvalid_pipeline = parser.parse_yaml_file('tests/fixtures/invalid_pipeline.yaml')\nerrors = validator.validate_pipeline(invalid_pipeline)\nassert len(errors) > 0, \"Should have detected validation errors\"\n```\n\n3. **Expected Output**: \n- Valid pipelines parse without errors and produce `PipelineDefinition` objects\n- Invalid pipelines return specific error messages mentioning cycle detection, missing dependencies, or schema violations\n- Visualization generates SVG or HTML files showing task nodes and dependency edges\n\n4. **Signs of Issues**:\n- **Parsing fails silently**: Check schema validation is working and exceptions are properly handled\n- **Cycle detection gives false positives**: Verify adjacency list construction - dependencies should point FROM upstream TO downstream\n- **Topological sort produces wrong order**: Check that in-degree calculation counts incoming dependencies correctly\n\n\n## Data Extraction and Loading\n\n> **Milestone(s):** Milestone 2 - Data Extraction & Loading: Implements extractors and loaders for common data sources with incremental loading, schema mapping, and bulk transfer capabilities.\n\n### Mental Model: Universal Adapters\n\nThink of data connectors like universal power adapters when traveling internationally. Just as a universal adapter has a standard plug interface on one side and swappable country-specific plugs on the other, our ETL connectors have a standard internal data interface and pluggable source/destination-specific implementations.\n\nThe universal adapter abstracts away the complexity of different electrical systems (voltage, frequency, plug shape) and provides a consistent interface to your device. Similarly, our data connectors abstract away the complexity of different data systems (authentication, pagination, data formats, network protocols) and provide a consistent `DataStream` interface to the transformation engine.\n\nWhen you plug your laptop into a universal adapter, you don't need to know whether you're connected to 120V or 240V power - the adapter handles that complexity. When your ETL pipeline calls `extract(connection, query)`, it doesn't need to know whether it's reading from PostgreSQL, a REST API, or a CSV file - the connector handles that complexity and returns a standardized data stream.\n\nJust as a good universal adapter is reliable (won't fry your electronics), efficient (minimal power loss), and resumable (maintains connection through brief outages), our data connectors must be reliable (handle network failures gracefully), efficient (stream data without loading everything into memory), and resumable (support incremental loading and checkpointing).\n\n### Source Connectors\n\nSource connectors implement the extraction logic for reading data from various systems. Each connector abstracts the complexity of its specific data source while providing a uniform interface to the ETL pipeline. The connector architecture uses a plugin pattern where each source type implements a common `SourceConnector` interface.\n\n**Database Connectors** handle relational databases like PostgreSQL, MySQL, and SQL Server. These connectors manage database connections, execute SQL queries, and stream results efficiently. The database connector must handle connection pooling to avoid overwhelming the source system with too many concurrent connections. For large result sets, the connector implements cursor-based pagination to avoid loading millions of rows into memory at once.\n\nThe database connector supports both full extraction (reading all data) and incremental extraction (reading only changed data since the last run). For incremental extraction, the connector uses watermarking strategies based on timestamp columns, auto-incrementing IDs, or change data capture (CDC) logs. The connector automatically detects the optimal incremental strategy by analyzing the table schema for suitable watermark columns.\n\n**API Connectors** interface with REST APIs and handle the complexities of HTTP communication, authentication, rate limiting, and pagination. API responses often use different pagination patterns - some use offset/limit, others use cursor tokens, and some use page numbers. The API connector automatically detects the pagination pattern from the initial response and adapts its extraction strategy accordingly.\n\nAuthentication is handled through pluggable authentication providers that support API keys, OAuth 2.0, JWT tokens, and basic authentication. The connector includes automatic token refresh logic for OAuth flows and respects rate limits by implementing exponential backoff when receiving 429 (Too Many Requests) responses.\n\n**File System Connectors** read data from various file formats including CSV, JSON, Parquet, and Avro files. These connectors can read from local file systems, network shares, or cloud storage systems like S3, GCS, or Azure Blob Storage. The file connector automatically detects file formats based on extensions and content sniffing, then selects the appropriate parser.\n\nFor large files, the connector implements streaming readers that process data in chunks rather than loading entire files into memory. This is particularly important for multi-gigabyte CSV files or large JSON arrays. The connector supports file globbing patterns to read multiple related files as a single logical dataset.\n\n> **Decision: Pluggable Connector Architecture**\n> - **Context**: Different data sources have vastly different connection patterns, authentication schemes, and data formats, making a one-size-fits-all approach impractical\n> - **Options Considered**: Monolithic connector with conditional logic, pluggable architecture with common interface, separate tools for each source type\n> - **Decision**: Pluggable architecture with abstract base class and concrete implementations\n> - **Rationale**: Enables independent development of connectors, easier testing and maintenance, and allows third-party connector development without modifying core system\n> - **Consequences**: Requires well-designed interface abstraction but provides maximum flexibility and extensibility\n\n| Connector Type | Authentication Methods | Pagination Support | Incremental Loading | Schema Detection |\n|----------------|----------------------|-------------------|-------------------|------------------|\n| Database | Username/password, connection strings, SSL certificates | Cursor-based, offset/limit | Timestamp, ID-based, CDC | Information schema queries |\n| REST API | API keys, OAuth 2.0, JWT, basic auth | Cursor tokens, offset/limit, page numbers | Modified-since headers, cursor bookmarks | Content-Type headers, response introspection |\n| File System | Access keys, service accounts, network credentials | File chunking, directory scanning | File modification time, manifest files | File extension, content sniffing |\n\nThe source connector interface defines a standard contract that all implementations must follow:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `connect()` | connection_config: Dict | Connection | Establishes connection to source system with retry logic |\n| `extract()` | query: Query, options: ExtractOptions | DataStream | Extracts data matching query parameters as streaming iterator |\n| `get_schema()` | table_name: str | Schema | Retrieves schema information including column names, types, constraints |\n| `supports_incremental()` | table_name: str | bool | Indicates whether source supports incremental extraction |\n| `get_watermark()` | table_name: str | Optional[Any] | Returns current high-water mark for incremental extraction |\n| `test_connection()` | connection_config: Dict | ConnectionTestResult | Validates connection configuration without establishing full connection |\n\n**Change Data Capture (CDC) Integration** represents an advanced incremental loading strategy where the source system publishes a stream of data changes (inserts, updates, deletes) rather than requiring the ETL system to poll for changes. Many modern databases support CDC through features like PostgreSQL's logical replication, MySQL's binlog, or SQL Server's Change Tracking.\n\nCDC connectors subscribe to these change streams and convert database-specific change events into standardized change records. Each change record includes the operation type (INSERT, UPDATE, DELETE), the affected row data, and metadata like transaction IDs and timestamps. This approach provides near real-time data synchronization with minimal impact on the source system.\n\nHowever, CDC comes with operational complexity. The connector must handle stream interruptions gracefully, maintain proper offset tracking to avoid missing or duplicating changes, and deal with schema evolution when source tables are altered. The CDC connector implements checkpointing to periodically save its position in the change stream, enabling recovery from exactly where it left off after failures.\n\n### Destination Connectors\n\nDestination connectors implement the loading logic for writing data to target systems. Like source connectors, they follow a pluggable architecture but focus on optimizing write performance, handling schema mapping, and ensuring data consistency during loads.\n\n**Database Destination Connectors** write data to relational databases using bulk loading techniques optimized for each database system. PostgreSQL destinations use `COPY` commands for maximum throughput, while MySQL destinations use `LOAD DATA INFILE` or batch `INSERT` statements. The connector automatically selects the optimal loading strategy based on the destination database type and data volume.\n\nSchema mapping is a critical function where the connector translates between source and destination data types. For example, a source API might return timestamps as ISO 8601 strings, but the destination PostgreSQL table expects `TIMESTAMP WITH TIME ZONE` columns. The connector maintains mapping rules that define how to convert between different data type systems.\n\nUpsert operations (insert or update) require careful handling of conflict resolution. The connector must identify which columns constitute the primary key or unique constraint, then generate appropriate `ON CONFLICT` (PostgreSQL) or `ON DUPLICATE KEY UPDATE` (MySQL) statements. For databases that don't support native upsert syntax, the connector implements upsert logic using separate insert and update operations with proper transaction boundaries.\n\n**Data Warehouse Connectors** are specialized for analytical systems like Snowflake, BigQuery, or Redshift. These systems optimize for different usage patterns than transactional databases - they favor bulk loading over row-by-row operations and often require specific file formats for optimal performance.\n\nThe data warehouse connector implements staging-based loading where data is first written to temporary staging tables, then merged into final destination tables using SQL `MERGE` statements. This pattern enables atomic updates where either the entire batch succeeds or fails together, preventing partial loads that could corrupt analytical queries.\n\nColumn-oriented data warehouses often perform better with specific file formats. The connector can export data to Parquet files for systems like BigQuery, or generate CSV files with appropriate delimiters and escaping for Snowflake's `COPY INTO` command. The connector handles the complexity of file generation, upload to cloud storage, and triggering the warehouse's native bulk loading commands.\n\n**Stream Processing Connectors** write data to real-time systems like Apache Kafka, Amazon Kinesis, or Google Pub/Sub. These connectors must handle message ordering, partitioning strategies, and delivery guarantees. Unlike batch-oriented database connectors, stream connectors must consider message size limits, serialization formats, and consumer scaling patterns.\n\nThe stream connector implements configurable partitioning strategies to ensure related records are delivered to the same partition for ordered processing. For example, all changes for a specific customer might be routed to the same Kafka partition using a hash of the customer ID. This ensures downstream consumers can process changes in the correct order.\n\n> **Decision: Staging-Based Loading for Data Warehouses**\n> - **Context**: Data warehouses optimize for analytical queries and bulk operations, while row-by-row loading creates small files and poor query performance\n> - **Options Considered**: Direct row-by-row inserts, streaming API calls, staging table with bulk merge operations\n> - **Decision**: Staging-based loading with temporary tables and bulk merge operations\n> - **Rationale**: Provides atomicity (all-or-nothing loading), optimal performance through bulk operations, and enables data validation before final commit\n> - **Consequences**: Requires additional storage for staging tables but significantly improves loading performance and data consistency\n\n| Destination Type | Loading Strategy | Conflict Resolution | Transaction Support | Optimal Batch Size |\n|------------------|------------------|-------------------|-------------------|-------------------|\n| PostgreSQL | COPY commands, batch INSERT | ON CONFLICT clauses | Full ACID transactions | 10,000-50,000 rows |\n| MySQL | LOAD DATA, batch INSERT | ON DUPLICATE KEY UPDATE | Full ACID transactions | 5,000-25,000 rows |\n| Snowflake | COPY INTO from staged files | MERGE statements | Warehouse-level consistency | 1M+ rows per file |\n| BigQuery | Streaming inserts, load jobs | Table decorators, DML MERGE | Eventually consistent | 10MB-1GB per load job |\n| Kafka | Producer API with batching | Idempotent producers | At-least-once delivery | 100-1000 messages |\n\nThe destination connector interface mirrors the source connector pattern with methods optimized for write operations:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `connect()` | connection_config: Dict | Connection | Establishes connection to destination system with write permissions |\n| `load()` | data_stream: DataStream, target: str, options: LoadOptions | LoadResult | Writes data stream to target table/topic with specified options |\n| `create_target()` | target: str, schema: Schema | bool | Creates target table/topic if it doesn't exist |\n| `get_target_schema()` | target: str | Schema | Retrieves current schema of target table for validation |\n| `supports_upsert()` | target: str | bool | Indicates whether destination supports upsert operations |\n| `begin_transaction()` | None | Transaction | Starts transaction for atomic multi-table loading |\n| `commit_transaction()` | transaction: Transaction | bool | Commits transaction and makes changes permanent |\n\n**Bulk Loading Optimization** is crucial for achieving high throughput in ETL pipelines. The destination connector implements several optimization techniques based on the target system's capabilities. For databases, this includes disabling indexes during loading, using unlogged tables for temporary data, and parallelizing writes across multiple connections.\n\nThe connector monitors loading performance and dynamically adjusts batch sizes based on throughput metrics. If small batches are causing excessive overhead, the connector increases batch size up to memory limits. If large batches are causing timeouts or memory issues, the connector reduces batch size to maintain stability.\n\nConnection pooling is essential for parallel loading scenarios where multiple tasks might write to the same destination simultaneously. The connector maintains a pool of database connections that can be shared across concurrent loading operations while ensuring proper isolation and transaction boundaries.\n\n![Connector Architecture](./diagrams/connector-architecture.svg)\n\n### Incremental Loading Strategies\n\nIncremental loading is the practice of extracting only data that has changed since the last successful pipeline run, rather than reprocessing the entire dataset. This dramatically improves pipeline performance and reduces load on source systems, but requires careful state management and change detection strategies.\n\n**Watermarking** is the most common incremental loading pattern, where the pipeline tracks a \"high-water mark\" representing the latest processed data point. The watermark is typically based on a monotonically increasing column like a timestamp, auto-increment ID, or version number. On each pipeline run, the system extracts only records where the watermark column is greater than the stored high-water mark.\n\nTimestamp-based watermarking uses columns like `created_at` or `modified_at` to identify new or changed records. The pipeline stores the maximum timestamp from the previous run and extracts records with timestamps greater than this value. However, timestamp watermarking has subtleties around clock skew, transaction timing, and null values that must be handled carefully.\n\nConsider a scenario where records are inserted with `created_at = '2024-01-15 14:30:00'` but the previous pipeline run completed at `14:30:05`. If the pipeline uses `WHERE created_at > '2024-01-15 14:30:05'` for the next extraction, it will miss records that were inserted during the brief overlap period. To handle this, the watermarking strategy typically includes a small lookback window (e.g., 5 minutes) to ensure no records are missed due to timing issues.\n\n**Cursor-Based Pagination** provides a more robust alternative to timestamp watermarking by using opaque cursor tokens that represent positions in the data stream. Many APIs provide cursor tokens that encode the exact position of the last returned record, allowing the next request to continue from that exact point without gaps or duplicates.\n\nThe cursor approach handles several edge cases that timestamp watermarking struggles with. If multiple records have identical timestamps, cursor-based pagination can still distinguish between them. If records are updated rather than inserted, cursors can track the update sequence. If the source system experiences clock changes or time zone transitions, cursors remain unaffected.\n\nHowever, cursor-based pagination requires the source system to maintain stable cursors across API calls. Some systems generate time-limited cursors that expire after a certain period, requiring the pipeline to fall back to full extraction if too much time passes between runs. The incremental loading logic must handle cursor expiration gracefully and maintain fallback strategies.\n\n**Change Data Capture (CDC) Watermarking** represents the most sophisticated incremental loading strategy, where the source system publishes a stream of change events that the pipeline can consume incrementally. Each change event contains metadata like log sequence numbers (LSNs) or transaction IDs that serve as watermarks for tracking progress through the change stream.\n\nCDC watermarking provides several advantages over other strategies. It captures all types of changes (inserts, updates, deletes) rather than just new records. It provides near real-time latency since changes are available immediately rather than waiting for batch extraction windows. It eliminates the need to query the source system for changes since changes are pushed rather than pulled.\n\nThe complexity of CDC watermarking lies in handling stream failures and recovery. If the pipeline crashes or loses connection to the change stream, it must be able to resume from exactly where it left off without missing or duplicating changes. This requires persistent storage of watermark positions and careful handling of duplicate detection during recovery periods.\n\n> **Decision: Multi-Strategy Incremental Loading**\n> - **Context**: Different source systems provide different mechanisms for change detection, and no single strategy works optimally for all sources\n> - **Options Considered**: Single timestamp-based strategy, single cursor-based strategy, multi-strategy approach with automatic selection\n> - **Decision**: Multi-strategy approach where connectors automatically select the best available strategy\n> - **Rationale**: Maximizes compatibility with diverse source systems while optimizing performance for each system's capabilities\n> - **Consequences**: Increases connector complexity but provides optimal incremental loading for each source type\n\n| Strategy | Best Use Cases | Advantages | Limitations | Recovery Complexity |\n|----------|---------------|------------|-------------|-------------------|\n| Timestamp Watermarking | Tables with reliable created_at/modified_at columns | Simple implementation, works with any SQL database | Clock skew issues, may miss concurrent inserts | Low - just store last timestamp |\n| ID-Based Watermarking | Tables with auto-increment primary keys | No clock dependencies, handles concurrent inserts | Only captures new records, not updates | Low - store last processed ID |\n| Cursor-Based Pagination | APIs with stable cursor support | No gaps or duplicates, handles complex ordering | Requires API cursor support, cursors may expire | Medium - handle cursor expiration |\n| Change Data Capture | Systems with CDC capabilities | Real-time, captures all change types including deletes | Complex setup, requires CDC infrastructure | High - manage stream offsets and recovery |\n\n**Watermark Storage and Management** requires careful consideration of where and how to store watermark values. The watermark must be stored transactionally with the loaded data to ensure consistency. If the data load succeeds but the watermark update fails, the next pipeline run will reprocess the same data, potentially causing duplicates. If the watermark update succeeds but the data load fails, the next pipeline run will skip data that was never actually loaded.\n\nThe watermarking system stores watermarks in a dedicated metadata table with columns for pipeline ID, source table, watermark column, and watermark value. Each watermark entry includes timestamps for when it was created and last updated, enabling audit trails and debugging of incremental loading issues.\n\nFor complex pipelines that extract from multiple source tables, the watermarking system maintains separate watermarks for each source table. This allows different tables to have different incremental strategies and prevents failures in one table from affecting watermark management for other tables.\n\n**Backfill and Historical Loading** represents a special case of incremental loading where the pipeline needs to process historical data that predates the current watermark. This might be necessary when adding new data sources, recovering from extended outages, or reprocessing data after bug fixes.\n\nThe backfill process creates temporary watermarks that start from a specified historical point and incrementally process data in chunks up to the current watermark. This allows historical processing to use the same incremental loading logic as regular pipeline runs while maintaining proper progress tracking.\n\nBackfill operations must coordinate with regular pipeline runs to avoid conflicts. The system implements backfill scheduling that ensures historical processing doesn't interfere with current data extraction, typically by running backfill jobs during low-traffic periods or using separate resource pools.\n\n**Schema Evolution Handling** becomes critical in incremental loading scenarios where the source or destination schema changes between pipeline runs. The incremental loading system must detect schema changes and handle them gracefully without breaking the watermarking logic.\n\nWhen new columns are added to source tables, the incremental loading logic continues to work with existing watermark columns. However, the pipeline must handle cases where watermark columns themselves are modified or removed. The system maintains schema fingerprints alongside watermarks to detect when schema changes might affect incremental loading strategies.\n\nFor schema changes that break existing watermarks (such as changing the watermark column data type), the system provides schema migration tools that can reset watermarks and optionally trigger backfill operations to ensure data consistency.\n\n **Pitfall: Watermark Clock Skew Issues**\n\nA common mistake is using server timestamps as watermarks without accounting for clock differences between the source system and the ETL pipeline. If the source database server's clock is 5 minutes ahead of the ETL server's clock, timestamp-based watermarking may miss records that appear to be \"in the future\" from the ETL system's perspective. Always use the source system's clock for timestamp comparisons and include appropriate lookback windows to handle minor clock skew.\n\n **Pitfall: Non-Atomic Watermark Updates**\n\nAnother frequent error is updating the watermark in a separate transaction from the data loading operation. This creates a race condition where the system might crash after loading data but before updating the watermark, causing data duplication on the next run. Alternatively, updating the watermark before confirming successful data loading can cause data loss if the load fails. Always update watermarks atomically with data loading operations using database transactions or equivalent consistency mechanisms.\n\n **Pitfall: Ignoring Null Watermark Values**\n\nMany developers forget to handle null values in watermark columns, which can cause incremental loading queries to behave unexpectedly. In SQL, comparisons with null values return unknown rather than true or false, potentially excluding records with null timestamps from incremental loads. Design incremental loading queries to explicitly handle null values in watermark columns and decide whether to include or exclude them based on business requirements.\n\n### Implementation Guidance\n\nThe data extraction and loading system requires careful balance between flexibility and performance. This section provides practical guidance for implementing connectors that can handle diverse data sources while maintaining high throughput and reliability.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Database Connectivity | `psycopg2` for PostgreSQL, `mysql-connector-python` for MySQL | `SQLAlchemy` with connection pooling and dialect abstraction |\n| HTTP Client | `requests` library with session reuse | `aiohttp` for async operations or `httpx` for HTTP/2 support |\n| File Processing | `csv` module, `json` module for basic formats | `pandas` for complex transformations, `pyarrow` for Parquet |\n| Cloud Storage | `boto3` for AWS S3, individual cloud SDKs | `fsspec` for unified interface across cloud providers |\n| Streaming | `itertools` for basic iteration patterns | `apache-beam` or `dask` for distributed stream processing |\n\n**Recommended File Structure:**\n```\netl-system/\n  src/connectors/\n    __init__.py                     connector registry and factory functions\n    base.py                        abstract base classes for source/destination connectors\n    source/\n      __init__.py                  source connector imports and registry\n      database.py                  database source connector implementation\n      api.py                       REST API source connector implementation  \n      filesystem.py                file system source connector implementation\n    destination/\n      __init__.py                  destination connector imports and registry\n      database.py                  database destination connector implementation\n      warehouse.py                 data warehouse connector implementation\n      streaming.py                 stream processing connector implementation\n    utils/\n      __init__.py\n      pagination.py                pagination strategy implementations\n      watermark.py                 watermark management utilities\n      schema.py                    schema mapping and validation utilities\n  tests/connectors/\n    test_source_database.py        database connector tests with test database\n    test_destination_warehouse.py  warehouse connector tests with mock services\n    integration/                   integration tests with real services\n  config/\n    connector-examples/            example connector configurations\n```\n\n**Infrastructure Starter Code:**\n\nComplete watermark management utility that handles persistent storage and atomic updates:\n\n```python\nfrom datetime import datetime, timezone\nfrom typing import Optional, Any, Dict\nfrom dataclasses import dataclass\nimport sqlite3\nimport json\n\n@dataclass\nclass WatermarkEntry:\n    pipeline_id: str\n    source_table: str\n    watermark_column: str\n    watermark_value: Any\n    watermark_type: str\n    created_at: datetime\n    updated_at: datetime\n\nclass WatermarkManager:\n    \"\"\"Manages watermark persistence and atomic updates for incremental loading.\"\"\"\n    \n    def __init__(self, db_path: str = \"watermarks.db\"):\n        self.db_path = db_path\n        self._init_database()\n    \n    def _init_database(self):\n        \"\"\"Initialize watermark storage database with proper schema.\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS watermarks (\n                    pipeline_id TEXT NOT NULL,\n                    source_table TEXT NOT NULL,\n                    watermark_column TEXT NOT NULL,\n                    watermark_value TEXT NOT NULL,\n                    watermark_type TEXT NOT NULL,\n                    created_at TEXT NOT NULL,\n                    updated_at TEXT NOT NULL,\n                    PRIMARY KEY (pipeline_id, source_table, watermark_column)\n                )\n            \"\"\")\n            conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_pipeline_table \n                ON watermarks(pipeline_id, source_table)\n            \"\"\")\n    \n    def get_watermark(self, pipeline_id: str, source_table: str, \n                     watermark_column: str) -> Optional[Any]:\n        \"\"\"Retrieve current watermark value for specified source.\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.execute(\n                \"SELECT watermark_value, watermark_type FROM watermarks \"\n                \"WHERE pipeline_id = ? AND source_table = ? AND watermark_column = ?\",\n                (pipeline_id, source_table, watermark_column)\n            )\n            row = cursor.fetchone()\n            if row:\n                value_str, value_type = row\n                return self._deserialize_watermark(value_str, value_type)\n            return None\n    \n    def update_watermark(self, pipeline_id: str, source_table: str,\n                        watermark_column: str, watermark_value: Any) -> bool:\n        \"\"\"Atomically update watermark value.\"\"\"\n        now = datetime.now(timezone.utc).isoformat()\n        value_str = self._serialize_watermark(watermark_value)\n        value_type = type(watermark_value).__name__\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO watermarks \n                (pipeline_id, source_table, watermark_column, watermark_value, \n                 watermark_type, created_at, updated_at)\n                VALUES (?, ?, ?, ?, ?, \n                        COALESCE((SELECT created_at FROM watermarks \n                                 WHERE pipeline_id = ? AND source_table = ? \n                                 AND watermark_column = ?), ?), ?)\n            \"\"\", (pipeline_id, source_table, watermark_column, value_str, value_type,\n                  pipeline_id, source_table, watermark_column, now, now))\n            return True\n    \n    def _serialize_watermark(self, value: Any) -> str:\n        \"\"\"Convert watermark value to storable string format.\"\"\"\n        if isinstance(value, datetime):\n            return value.isoformat()\n        elif isinstance(value, (int, float, str)):\n            return str(value)\n        else:\n            return json.dumps(value)\n    \n    def _deserialize_watermark(self, value_str: str, value_type: str) -> Any:\n        \"\"\"Convert stored string back to original watermark type.\"\"\"\n        if value_type == \"datetime\":\n            return datetime.fromisoformat(value_str)\n        elif value_type == \"int\":\n            return int(value_str)\n        elif value_type == \"float\":\n            return float(value_str)\n        elif value_type == \"str\":\n            return value_str\n        else:\n            return json.loads(value_str)\n```\n\nComplete connection pooling utility for database connectors:\n\n```python\nimport threading\nfrom contextlib import contextmanager\nfrom typing import Dict, Any, Generator\nfrom queue import Queue, Empty\nimport psycopg2\nfrom psycopg2 import pool\n\nclass ConnectionPoolManager:\n    \"\"\"Thread-safe connection pool manager for database connectors.\"\"\"\n    \n    def __init__(self, connection_config: Dict[str, Any], \n                 min_connections: int = 1, max_connections: int = 10):\n        self.connection_config = connection_config\n        self.min_connections = min_connections\n        self.max_connections = max_connections\n        self._pool = None\n        self._lock = threading.Lock()\n        self._initialize_pool()\n    \n    def _initialize_pool(self):\n        \"\"\"Create database connection pool with specified parameters.\"\"\"\n        try:\n            self._pool = psycopg2.pool.ThreadedConnectionPool(\n                minconn=self.min_connections,\n                maxconn=self.max_connections,\n                host=self.connection_config['host'],\n                port=self.connection_config.get('port', 5432),\n                database=self.connection_config['database'],\n                user=self.connection_config['user'],\n                password=self.connection_config['password']\n            )\n        except Exception as e:\n            raise ConnectionError(f\"Failed to initialize connection pool: {e}\")\n    \n    @contextmanager\n    def get_connection(self) -> Generator[Any, None, None]:\n        \"\"\"Get connection from pool with automatic cleanup.\"\"\"\n        conn = None\n        try:\n            with self._lock:\n                conn = self._pool.getconn()\n            yield conn\n        except Exception as e:\n            if conn:\n                conn.rollback()\n            raise e\n        finally:\n            if conn:\n                with self._lock:\n                    self._pool.putconn(conn)\n    \n    def close_all_connections(self):\n        \"\"\"Close all connections in the pool.\"\"\"\n        if self._pool:\n            self._pool.closeall()\n```\n\n**Core Logic Skeleton Code:**\n\nDatabase source connector implementation with detailed TODOs for learners:\n\n```python\nfrom typing import Iterator, Dict, Any, Optional\nfrom abc import ABC, abstractmethod\nimport pandas as pd\n\nclass SourceConnector(ABC):\n    \"\"\"Abstract base class for all source connectors.\"\"\"\n    \n    @abstractmethod\n    def extract(self, query: str, options: Dict[str, Any]) -> Iterator[Dict[str, Any]]:\n        \"\"\"Extract data from source system as iterator of records.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_schema(self, table_name: str) -> Dict[str, str]:\n        \"\"\"Get schema information for specified table.\"\"\"\n        pass\n\nclass DatabaseSourceConnector(SourceConnector):\n    \"\"\"Source connector for relational databases with incremental loading support.\"\"\"\n    \n    def __init__(self, connection_config: Dict[str, Any]):\n        self.connection_config = connection_config\n        self.pool_manager = None\n        self.watermark_manager = WatermarkManager()\n    \n    def extract(self, query: str, options: Dict[str, Any]) -> Iterator[Dict[str, Any]]:\n        \"\"\"Extract data from database with optional incremental loading.\"\"\"\n        # TODO 1: Initialize connection pool if not already created\n        # TODO 2: Check if incremental loading is requested in options\n        # TODO 3: If incremental, modify query to include watermark WHERE clause\n        # TODO 4: Execute query using connection from pool\n        # TODO 5: Fetch results in batches to avoid memory issues (use fetchmany)\n        # TODO 6: Yield each row as dictionary with column names as keys\n        # TODO 7: Track maximum watermark value seen during extraction\n        # TODO 8: After successful extraction, update stored watermark\n        # Hint: Use pandas.read_sql with chunksize parameter for large result sets\n        # Hint: Convert each row tuple to dict using column names from cursor.description\n        pass\n    \n    def get_schema(self, table_name: str) -> Dict[str, str]:\n        \"\"\"Retrieve column information from database system tables.\"\"\"\n        # TODO 1: Connect to database using connection pool\n        # TODO 2: Query information_schema.columns for table structure\n        # TODO 3: Handle different database types (PostgreSQL, MySQL, etc.)\n        # TODO 4: Map database-specific types to standard type names\n        # TODO 5: Return dictionary mapping column names to standardized types\n        # Hint: PostgreSQL uses information_schema.columns, MySQL uses DESCRIBE\n        # Hint: Common type mapping: VARCHAR->str, INTEGER->int, TIMESTAMP->datetime\n        pass\n    \n    def _build_incremental_query(self, base_query: str, watermark_info: Dict[str, Any]) -> str:\n        \"\"\"Modify base query to include watermark filtering for incremental extraction.\"\"\"\n        # TODO 1: Parse base query to identify WHERE clause location\n        # TODO 2: Add watermark condition (e.g., modified_at > last_watermark)\n        # TODO 3: Handle case where base query already has WHERE clause\n        # TODO 4: Include small lookback window to handle clock skew\n        # TODO 5: Return modified query string\n        # Hint: Use SQL parsing library or simple string manipulation\n        # Hint: Always use parameterized queries to prevent SQL injection\n        pass\n```\n\nAPI source connector with pagination and rate limiting:\n\n```python\nimport requests\nimport time\nfrom typing import Iterator, Dict, Any, Optional\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\nclass APISourceConnector(SourceConnector):\n    \"\"\"Source connector for REST APIs with automatic pagination and rate limiting.\"\"\"\n    \n    def __init__(self, connection_config: Dict[str, Any]):\n        self.base_url = connection_config['base_url']\n        self.auth_config = connection_config.get('auth', {})\n        self.session = self._create_session()\n    \n    def extract(self, endpoint: str, options: Dict[str, Any]) -> Iterator[Dict[str, Any]]:\n        \"\"\"Extract data from API endpoint with automatic pagination handling.\"\"\"\n        # TODO 1: Authenticate with API using configured auth method\n        # TODO 2: Make initial request to endpoint with base parameters\n        # TODO 3: Detect pagination pattern from response (cursor, offset, page-based)\n        # TODO 4: Extract records from response data based on configured data_path\n        # TODO 5: Yield individual records from current page\n        # TODO 6: Check for next page indicator and prepare next request\n        # TODO 7: Implement rate limiting with exponential backoff on 429 errors\n        # TODO 8: Continue pagination until no more pages available\n        # Hint: Common pagination patterns - look for 'next', 'cursor', 'has_more' fields\n        # Hint: Use time.sleep() between requests to respect rate limits\n        pass\n    \n    def _create_session(self) -> requests.Session:\n        \"\"\"Create HTTP session with retry strategy and timeout configuration.\"\"\"\n        # TODO 1: Create requests.Session object\n        # TODO 2: Configure retry strategy for transient failures\n        # TODO 3: Set appropriate timeouts for connect and read operations  \n        # TODO 4: Add authentication headers/parameters based on auth_config\n        # TODO 5: Mount HTTPAdapter with retry configuration\n        # Hint: Retry on 500, 502, 503, 504 status codes but not 4xx errors\n        # Hint: Use exponential backoff starting from 1 second\n        pass\n    \n    def _handle_pagination(self, response: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"Determine next page parameters based on response pagination metadata.\"\"\"\n        # TODO 1: Check response for pagination indicators (next_cursor, next_page, etc.)\n        # TODO 2: Extract pagination parameters for next request\n        # TODO 3: Return None if no more pages available\n        # TODO 4: Handle different pagination patterns (offset/limit, cursor-based, page numbers)\n        # TODO 5: Validate pagination parameters to avoid infinite loops\n        # Hint: Store pagination state to detect when you've seen the same cursor twice\n        pass\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the source and destination connectors, verify functionality with these tests:\n\n1. **Database Connector Test**: Create a test PostgreSQL database with sample data. Run `python -m pytest tests/connectors/test_source_database.py` and verify:\n   - Full extraction returns all records\n   - Incremental extraction with timestamp watermark returns only new records\n   - Schema detection correctly identifies column types\n   - Connection pooling handles concurrent extractions\n\n2. **API Connector Test**: Use a public API like JSONPlaceholder (https://jsonplaceholder.typicode.com/). Run extraction and verify:\n   - Pagination automatically follows all pages\n   - Rate limiting respects API limits without errors\n   - Authentication headers are properly included\n   - Cursor-based pagination maintains position across requests\n\n3. **Integration Test**: Set up end-to-end pipeline from database source to database destination:\n   ```bash\n   python scripts/test_pipeline.py --source postgres://test_db/users --dest postgres://target_db/users_copy\n   ```\n   Expected output shows successful extraction, transformation, and loading with record counts.\n\n**Common Debugging Issues:**\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|-------------|-----------|-----|\n| Incremental extraction misses records | Clock skew between systems | Check timestamps on both source and ETL servers | Add 5-10 minute lookback window to watermark queries |\n| API extraction fails with 429 errors | Rate limiting too aggressive | Check API rate limit headers in response | Implement exponential backoff with longer delays |\n| Database connection timeout | Connection pool exhausted | Monitor active connection count | Increase pool size or reduce query timeout |\n| Partial data loads in destination | Non-atomic watermark updates | Check if watermark is updated before load completion | Use database transactions to update watermark with data |\n| Memory errors during large extractions | Loading entire result set into memory | Check if using streaming vs batch loading | Implement cursor-based iteration with smaller batch sizes |\n\n\n## Data Transformation Engine\n\n> **Milestone(s):** Milestone 3 - Data Transformations: Implements transformation operations with schema validation, supporting SQL-based transformations, Python UDFs, null handling, and data validation rules.\n\n### Mental Model: Data Refinery\n\nThink of the data transformation engine as an **oil refinery** that processes raw crude oil into various refined products. Just as a refinery has multiple processing units (distillation towers, crackers, reformers) that each perform specific transformations on the petroleum, our data transformation engine has multiple transformation stages that clean, reshape, and enrich raw data.\n\nIn an oil refinery, crude oil enters the facility and flows through a series of specialized processing units. Each unit has specific operating parameters (temperature, pressure, catalysts) and produces intermediate products that feed into downstream units. Quality control labs continuously test the products at each stage, rejecting batches that don't meet specifications and sending them back for reprocessing.\n\nSimilarly, our data transformation engine receives raw extracted data and passes it through a pipeline of transformation stages. Each stage has configurable parameters (SQL queries, Python functions, validation rules) and produces intermediate datasets. Schema validation acts like quality control, checking data types, null constraints, and business rules at each stage, flagging or rejecting records that don't meet specifications.\n\nThe key insight from this analogy is that **transformations are composable and order-dependent**. Just as you can't crack heavy oil before distilling it, you often need to clean and standardize data before applying complex business logic transformations. The refinery analogy also highlights that **quality control must be continuous** - checking only the final output misses problems that compound through multiple stages.\n\nThis mental model guides our design decisions: transformation stages should be pluggable and reusable, intermediate results should be inspectable for debugging, and validation should happen at stage boundaries to catch problems early rather than at the end of a long transformation pipeline.\n\n### SQL-Based Transformations\n\nSQL-based transformations form the backbone of most ETL operations because SQL provides a declarative, widely-understood language for data manipulation. The transformation engine treats SQL as a **templating language** where queries can contain parameter substitutions, enabling reusable transformation logic across different pipeline runs and datasets.\n\n> **Decision: Templated SQL with Runtime Parameter Substitution**\n> - **Context**: ETL pipelines need to apply similar transformations across different time periods, datasets, or environments, requiring dynamic query generation\n> - **Options Considered**: Static SQL files, string concatenation, template engine (Jinja2), prepared statement parameters\n> - **Decision**: Jinja2 template engine for SQL with runtime parameter substitution\n> - **Rationale**: Template engines provide safe parameter substitution preventing SQL injection, support complex logic (loops, conditionals), and are familiar to data engineers. Prepared statements only handle value parameters, not structural changes like table names or column lists.\n> - **Consequences**: Enables parameterized queries but requires template validation and introduces dependency on Jinja2. Complex templates can become hard to debug.\n\n| Template Feature | Purpose | Example Usage | Validation Required |\n|------------------|---------|---------------|-------------------|\n| Variable Substitution | Dynamic table/column names | `SELECT * FROM {{ source_table }}` | Table existence check |\n| Date Range Filters | Incremental processing | `WHERE created_at >= '{{ start_date }}'` | Date format validation |\n| Conditional Logic | Environment-specific queries | `{% if env == 'prod' %} AND status = 'active' {% endif %}` | Branch coverage testing |\n| Loop Constructs | Dynamic column generation | `{% for col in numeric_cols %} SUM({{ col }}) {% endfor %}` | Column existence validation |\n| Macro Functions | Reusable query fragments | `{{ standardize_phone_number('phone_col') }}` | Macro parameter validation |\n\nThe SQL transformation executor follows a multi-phase execution model that separates template rendering from SQL execution, enabling better error handling and debugging:\n\n1. **Template Validation Phase**: Parse the SQL template using Jinja2 parser to check for syntax errors, undefined variables, and template structure issues before runtime\n2. **Parameter Injection Phase**: Merge runtime parameters from pipeline configuration, upstream task outputs, and system variables into template context\n3. **Template Rendering Phase**: Generate the final SQL query by applying parameters to template, producing executable SQL with all variables resolved\n4. **Query Validation Phase**: Validate the rendered SQL for syntax correctness, check referenced tables/columns exist, and verify the user has required permissions\n5. **Execution Phase**: Execute the SQL against the target database, handling connection pooling, transaction management, and result set streaming\n6. **Result Processing Phase**: Convert database result sets into standardized data structures, apply any post-processing transformations, and pass results to downstream tasks\n\n> The critical insight here is that template rendering and SQL execution are separate phases - this allows us to generate and inspect the final SQL before execution, enabling better debugging and dry-run capabilities.\n\n**Parameterization Strategy**\n\nThe transformation engine supports multiple parameter sources with a defined precedence order to handle conflicts:\n\n| Parameter Source | Precedence | Example Usage | When to Use |\n|------------------|------------|---------------|-------------|\n| Task Configuration | 1 (highest) | Task-specific table names, custom business logic | Task-specific overrides |\n| Pipeline Parameters | 2 | Environment settings, global date ranges | Pipeline-wide configuration |\n| System Variables | 3 | Current timestamp, pipeline run ID, execution date | Built-in system context |\n| Environment Variables | 4 (lowest) | Database connection strings, API keys | Infrastructure configuration |\n\nThe parameter resolution engine merges these sources into a single context dictionary, with higher precedence sources overriding lower ones. This enables flexible configuration inheritance where global settings provide defaults that specific tasks can override as needed.\n\n**Query Result Handling**\n\nSQL transformations produce tabular results that must be converted into the standard `DataStream` format for pipeline interoperability. The transformation engine handles this conversion while managing memory efficiently for large result sets:\n\n| Result Processing Strategy | Memory Usage | Throughput | When to Use |\n|----------------------------|--------------|------------|-------------|\n| Streaming Row-by-Row | Low | Medium | Large result sets, memory constraints |\n| Batched Processing | Medium | High | Moderate result sets, balanced performance |\n| Full Materialization | High | Highest | Small result sets, need random access |\n\nThe executor automatically chooses the appropriate strategy based on result set size estimates from query plan analysis. For result sets under 10MB, it uses full materialization for maximum performance. For larger sets, it switches to batched processing with configurable batch sizes, typically 1000-10000 rows per batch depending on row width.\n\n**Error Handling and Rollback**\n\nSQL transformations run within database transactions to ensure consistency and enable rollback on failures:\n\n1. **Connection Acquisition**: Obtain database connection from connection pool, handling pool exhaustion with exponential backoff retry\n2. **Transaction Begin**: Start explicit transaction to ensure atomicity of all SQL operations within the transformation\n3. **Query Execution**: Execute the transformed SQL, capturing both result data and execution metadata (row counts, execution time)\n4. **Validation Checks**: Verify result data meets expected schema and business rule constraints before committing\n5. **Transaction Commit**: Commit the transaction if all validations pass, making changes permanent\n6. **Connection Release**: Return connection to pool for reuse, ensuring proper cleanup regardless of success or failure\n\nIf any step fails, the transaction automatically rolls back, leaving the database in its original state. This transactional approach is crucial for data consistency, especially when transformations modify multiple tables or when downstream tasks depend on complete, consistent datasets.\n\n**Common Pitfalls in SQL Transformations**\n\n **Pitfall: Template Injection Vulnerabilities**\nUsing string concatenation or unsafe templating can create SQL injection vulnerabilities. For example, `SELECT * FROM users WHERE name = '{{ user_input }}'` allows malicious input to break out of the string context. Always use Jinja2's auto-escaping features and validate parameter values against expected patterns before template rendering.\n\n **Pitfall: Resource Exhaustion from Large Result Sets**\nMaterializing large query results in memory can exhaust available RAM and crash the pipeline executor. Monitor query result size estimates and automatically switch to streaming processing for results over configurable thresholds. Implement query result limits as a safety mechanism.\n\n **Pitfall: Transaction Timeout in Long-Running Queries**\nDatabase transactions held open for extended periods can block other operations and may be automatically rolled back by database timeout settings. For transformations that take more than a few minutes, consider breaking them into smaller chunks or using separate transactions for independent operations.\n\n### Python User-Defined Functions\n\nPython User-Defined Functions (UDFs) enable custom transformation logic that goes beyond SQL capabilities, such as complex string processing, machine learning inference, external API calls, or specialized business logic. The UDF execution engine provides a safe, performant environment for running Python code within the transformation pipeline while maintaining data type safety and error isolation.\n\n> **Decision: Isolated Python Execution with Resource Limits**\n> - **Context**: Python UDFs need to execute arbitrary user code safely without compromising pipeline stability or security\n> - **Options Considered**: Same-process execution, subprocess isolation, Docker containers, serverless functions\n> - **Decision**: Subprocess isolation with resource limits and timeout enforcement\n> - **Rationale**: Subprocess isolation prevents memory leaks and crashes from affecting the main pipeline process, while resource limits prevent runaway UDFs from consuming excessive CPU/memory. Simpler than containers but provides adequate isolation.\n> - **Consequences**: Enables safe execution of arbitrary Python code but adds process overhead and inter-process communication complexity. Subprocess startup time affects performance for small datasets.\n\n**UDF Definition and Registration**\n\nPython UDFs are defined as regular Python functions with type annotations that specify input and output schemas. The transformation engine uses these annotations for automatic data type conversion and validation:\n\n| UDF Component | Purpose | Example | Validation Applied |\n|---------------|---------|---------|-------------------|\n| Function Signature | Define input parameters and types | `def clean_phone(phone: str) -> str:` | Parameter count and types |\n| Type Annotations | Specify expected data types | `phone: Optional[str]` for nullable columns | Null handling requirements |\n| Docstring Schema | Document expected behavior | `\"\"\"Standardizes phone numbers to E.164 format\"\"\"` | Human-readable documentation |\n| Return Type | Specify output data type | `-> Optional[str]` | Output type validation |\n| Error Handling | Manage exceptions | `try/except` blocks with specific error types | Exception classification |\n\nThe UDF registry maintains a catalog of available functions with their signatures, enabling the pipeline definition parser to validate UDF usage at pipeline definition time rather than runtime. This early validation prevents many common errors like type mismatches or missing function definitions.\n\n**Data Type Mapping and Conversion**\n\nThe transformation engine automatically converts between the pipeline's internal data representation and Python native types, handling null values, type coercion, and precision requirements:\n\n| Pipeline Type | Python Type | Null Handling | Precision Notes |\n|---------------|-------------|---------------|-----------------|\n| `String` | `str` | `None` for null | Unicode encoding preserved |\n| `Integer` | `int` | `None` for null | Unlimited precision in Python |\n| `Float` | `float` | `None` for null, `NaN` for invalid | IEEE 754 double precision |\n| `Decimal` | `decimal.Decimal` | `None` for null | Arbitrary precision maintained |\n| `Boolean` | `bool` | `None` for null | Strict true/false, no truthy conversion |\n| `Date` | `datetime.date` | `None` for null | ISO date format |\n| `Timestamp` | `datetime.datetime` | `None` for null | Timezone-aware UTC |\n| `JSON` | `dict` or `list` | `None` for null | Nested structures supported |\n\nThe type conversion engine handles edge cases like integer overflow, floating-point precision loss, and timezone conversion automatically. For cases where automatic conversion might lose information (like converting `Decimal` to `float`), it logs warnings and provides configuration options to make the conversion explicit or reject the operation.\n\n**UDF Execution Modes**\n\nThe transformation engine supports different execution modes for Python UDFs based on performance requirements and data characteristics:\n\n| Execution Mode | Use Case | Performance Characteristics | Memory Usage |\n|----------------|----------|----------------------------|--------------|\n| Row-by-Row | Complex logic per record | Low throughput, high latency | Constant |\n| Vectorized | Pandas/NumPy operations | High throughput, moderate latency | Proportional to batch size |\n| Streaming | Large datasets | Moderate throughput, low latency | Constant |\n| Batch | Bulk operations | Highest throughput, high latency | High |\n\n**Row-by-Row Mode** executes the UDF once per input record, suitable for complex transformations that require full context of each record. This mode has the lowest performance but provides the most flexibility and is easiest to debug.\n\n**Vectorized Mode** passes entire columns as pandas Series or NumPy arrays to the UDF, enabling efficient operations on numerical data. UDFs in this mode must be written to handle array inputs and produce array outputs with the same length.\n\n**Streaming Mode** processes data in small batches (typically 100-1000 records) while maintaining constant memory usage. This balances performance with resource consumption for medium-sized datasets.\n\n**Batch Mode** materializes the entire input dataset before passing it to the UDF, enabling operations that require full dataset context like statistical analysis or machine learning training.\n\n**Resource Management and Safety**\n\nPython UDF execution includes comprehensive resource management to prevent runaway processes and ensure predictable performance:\n\n1. **Memory Limits**: Each UDF subprocess has a configurable memory limit (default 1GB) enforced through OS process limits, preventing individual functions from exhausting system memory\n2. **CPU Limits**: CPU time limits prevent infinite loops or computationally intensive operations from blocking pipeline progress indefinitely\n3. **Timeout Enforcement**: Wall-clock timeout ensures UDFs complete within reasonable time bounds, even if waiting on I/O operations\n4. **Subprocess Isolation**: Each UDF runs in a separate Python subprocess, preventing crashes or memory leaks from affecting the main pipeline process\n5. **Import Restrictions**: UDF execution environments restrict imports to approved libraries, preventing access to dangerous modules like `os`, `subprocess`, or `sys`\n6. **Network Isolation**: By default, UDF processes cannot make outbound network connections, though this can be configured for specific use cases\n\nThe resource management system monitors UDF execution continuously and terminates processes that exceed limits, logging detailed information about resource usage for performance tuning.\n\n**Error Handling and Debugging**\n\nUDF error handling follows a structured approach that maximizes debugging information while maintaining pipeline reliability:\n\n| Error Type | Handling Strategy | Information Captured | Recovery Action |\n|------------|-------------------|---------------------|-----------------|\n| Import Error | Fail fast at registration | Missing module, Python version | Pipeline validation failure |\n| Type Error | Record-level handling | Expected vs actual types, record ID | Skip record or fail task |\n| Value Error | Configurable handling | Invalid input value, validation rules | Apply default or fail record |\n| Runtime Exception | Isolation and logging | Full stack trace, input data | Fail record, continue batch |\n| Resource Exhaustion | Process termination | Resource usage stats, partial results | Retry with smaller batch |\n| Timeout | Graceful shutdown | Execution time, processed records | Retry or skip based on config |\n\nThe error handling system provides multiple recovery strategies:\n\n- **Fail Fast**: Stop pipeline execution immediately on first error, suitable for critical transformations where partial results are unacceptable\n- **Skip Invalid Records**: Continue processing valid records while logging invalid ones for later analysis\n- **Apply Default Values**: Replace failed transformations with configured default values, useful for optional enrichment operations\n- **Retry with Backoff**: Retry failed UDF calls with exponential backoff for transient errors like temporary resource constraints\n\n**Common Pitfalls in Python UDFs**\n\n **Pitfall: Memory Leaks from Global Variables**\nPython UDFs that use global variables or class-level state can accumulate memory across multiple invocations since subprocess environments are reused. Always use function-local variables and explicitly clean up any resources like file handles or database connections within the UDF.\n\n **Pitfall: Inconsistent Null Handling**\nDifferent Python libraries handle `None` values differently - pandas uses `NaN`, NumPy has multiple null representations, and standard library functions may raise exceptions. Always explicitly check for `None` in UDF inputs and decide how to handle null values consistently with your data model.\n\n **Pitfall: Side Effects in Pure Transformation Functions**\nUDFs should be pure functions that don't modify external state, write files, or make network calls unless explicitly designed for those purposes. Side effects make UDFs non-idempotent and can cause inconsistent results when pipelines retry or run in parallel.\n\n### Schema Validation and Evolution\n\nSchema validation ensures data consistency throughout the transformation pipeline by checking data types, constraints, and business rules at transformation boundaries. The validation engine must balance thoroughness with performance while handling the inevitable evolution of data schemas over time.\n\nSchema evolution presents one of the most challenging aspects of ETL system design because it requires maintaining backward compatibility while allowing data models to grow and change. The transformation engine treats schema as a **first-class citizen** with explicit versioning, migration strategies, and compatibility checking.\n\n> **Decision: Schema Registry with Backward Compatibility Checking**\n> - **Context**: Data schemas evolve over time as business requirements change, requiring systematic management of schema versions and compatibility\n> - **Options Considered**: No schema management, inline schema definitions, external schema registry, database-driven schema evolution\n> - **Decision**: Centralized schema registry with version management and compatibility checking\n> - **Rationale**: Centralized registry enables schema reuse across pipelines, version management tracks evolution over time, and compatibility checking prevents breaking changes from propagating. External systems like Confluent Schema Registry provide proven solutions.\n> - **Consequences**: Enables systematic schema evolution but adds operational complexity and external dependency. Schema registry becomes critical infrastructure component.\n\n**Schema Definition and Versioning**\n\nThe schema registry maintains versioned schema definitions that specify data structure, constraints, and evolution rules:\n\n| Schema Component | Purpose | Example | Validation Applied |\n|------------------|---------|---------|-------------------|\n| Field Definitions | Column names, types, nullability | `{\"name\": \"user_id\", \"type\": \"integer\", \"nullable\": false}` | Type compatibility, null constraints |\n| Primary Keys | Unique identification | `{\"primary_key\": [\"user_id\", \"timestamp\"]}` | Uniqueness validation |\n| Foreign Keys | Referential integrity | `{\"foreign_key\": {\"field\": \"user_id\", \"references\": \"users.id\"}}` | Reference validation |\n| Check Constraints | Business rules | `{\"check\": \"age >= 0 AND age <= 150\"}` | Value range validation |\n| Default Values | Missing data handling | `{\"field\": \"status\", \"default\": \"active\"}` | Type-compatible defaults |\n| Evolution Rules | Schema change policies | `{\"allow_new_fields\": true, \"allow_field_deletion\": false}` | Compatibility enforcement |\n\nEach schema version includes a compatibility level that determines what changes are allowed:\n\n- **Full Compatibility**: New schema can read data written with any previous schema version, and previous versions can read data written with new schema\n- **Backward Compatibility**: New schema can read data written with previous schema versions, but not vice versa\n- **Forward Compatibility**: Previous schema versions can read data written with new schema, but new schema may not read old data\n- **No Compatibility**: Breaking changes allowed, requiring explicit data migration\n\n**Validation Pipeline Architecture**\n\nThe validation engine operates as a series of validation stages that data passes through during transformation:\n\n1. **Input Validation Stage**: Validate data entering transformation against source schema, checking basic type compatibility and required field presence\n2. **Constraint Validation Stage**: Apply business rule constraints like range checks, format validation, and cross-field dependencies\n3. **Type Coercion Stage**: Attempt automatic type conversions for compatible types, logging all coercions for audit purposes\n4. **Output Validation Stage**: Validate transformation results against target schema before passing to next pipeline stage\n5. **Schema Evolution Check**: Compare current data schema against registry to detect schema drift and compatibility issues\n\nEach stage can be configured with different error handling policies:\n\n| Validation Policy | Behavior | Use Cases | Performance Impact |\n|-------------------|----------|-----------|-------------------|\n| Strict Validation | Fail pipeline on first validation error | Critical financial data, regulatory compliance | Lowest throughput |\n| Best Effort | Log errors but continue processing valid records | Data exploration, non-critical analytics | Balanced performance |\n| Schema Inference | Automatically adapt schema based on observed data | Prototype pipelines, exploratory analysis | Highest throughput |\n| Sampling Validation | Validate random sample of records | Large datasets with consistent structure | Minimal impact |\n\n**Type System and Coercion Rules**\n\nThe transformation engine uses a rich type system that captures both logical data types and physical representation requirements:\n\n| Logical Type | Physical Types | Coercion Rules | Precision Handling |\n|--------------|----------------|----------------|--------------------|\n| Numeric | `int32`, `int64`, `float32`, `float64`, `decimal` | Widening conversions only | Maintain maximum precision |\n| Text | `varchar(n)`, `text`, `char(n)` | Truncate with warning | Preserve UTF-8 encoding |\n| Temporal | `date`, `timestamp`, `timestamptz` | Parse standard formats | Maintain timezone info |\n| Boolean | `bool`, `bit`, `tinyint` | Standard truthy conversion | Map to true/false |\n| JSON | `json`, `jsonb`, `text` | Parse and validate JSON syntax | Preserve nested structure |\n| Binary | `blob`, `bytea`, `varbinary` | Base64 encoding for text transport | Preserve exact bytes |\n\nAutomatic type coercion follows safe conversion rules that never lose information without explicit user configuration. For example, `int32` to `int64` conversion is automatic, but `int64` to `int32` requires explicit truncation handling since it may lose data.\n\n**Null Value Semantics**\n\nDifferent data systems have varying concepts of null values, requiring careful handling during transformations:\n\n| System | Null Representation | Semantics | Transformation Behavior |\n|--------|---------------------|-----------|------------------------|\n| SQL Databases | `NULL` | Unknown value, not equal to anything including itself | Preserve SQL null semantics |\n| JSON | `null` literal | Explicit null value | Map to SQL NULL |\n| CSV Files | Empty string or missing field | Ambiguous - could be null or empty | Configurable interpretation |\n| Python | `None` object | Absence of value | Direct mapping to SQL NULL |\n| Pandas | `NaN` (Not a Number) | Missing numerical value | Convert to SQL NULL |\n| Apache Parquet | Null bitmap | Efficient null encoding | Direct null preservation |\n\nThe transformation engine provides configurable null handling policies to manage these semantic differences:\n\n- **Strict Null Preservation**: Maintain exact null semantics from source system\n- **Null Unification**: Convert all null representations to standard SQL NULL\n- **Null Replacement**: Replace nulls with configurable default values based on data type\n- **Null Rejection**: Treat null values as validation errors for non-nullable fields\n\n**Schema Evolution Strategies**\n\nThe schema evolution engine supports multiple strategies for handling schema changes over time:\n\n| Evolution Strategy | Approach | Benefits | Limitations |\n|-------------------|----------|----------|-------------|\n| Append-Only | Only add new optional fields | Maintains full compatibility | Cannot remove obsolete fields |\n| Versioned Schemas | Maintain multiple schema versions simultaneously | Supports breaking changes | Complexity increases with versions |\n| Schema Migration | Transform data during schema updates | Clean schema evolution | Requires migration downtime |\n| Schema Union | Merge schemas using union types | Handles diverse data sources | Complex type system |\n\n**Append-Only Evolution** is the simplest approach where new fields can be added but existing fields cannot be removed or have their types changed. This maintains backward compatibility but can lead to schema bloat over time.\n\n**Versioned Schemas** allow breaking changes by maintaining multiple schema versions and routing data through appropriate transformation pipelines. Each pipeline version handles a specific schema version, enabling gradual migration.\n\n**Schema Migration** performs bulk transformation of existing data when schemas change incompatibly. This requires coordinated downtime but results in clean, consistent schemas.\n\n**Schema Union** approaches treat schemas as unions of possible field sets, enabling pipelines to handle multiple schema versions simultaneously. This works well for diverse data sources but requires complex type resolution logic.\n\n**Performance Optimization for Validation**\n\nSchema validation can become a performance bottleneck for high-throughput pipelines, requiring optimization strategies:\n\n1. **Validation Caching**: Cache validation results for identical records to avoid repeated validation work\n2. **Parallel Validation**: Distribute validation across multiple threads or processes for CPU-bound operations\n3. **Lazy Validation**: Defer expensive validation operations until data is actually accessed downstream\n4. **Sampling Strategies**: Validate statistical samples rather than every record for consistent data sources\n5. **Early Exit Optimization**: Stop validation on first error for fail-fast policies\n6. **Columnar Validation**: Validate entire columns at once using vectorized operations when possible\n\nThe validation engine monitors its own performance and can automatically adjust validation strategies based on observed data patterns and performance requirements.\n\n**Common Pitfalls in Schema Validation**\n\n **Pitfall: Over-Strict Validation Causing Brittleness**\nExtremely strict validation rules make pipelines brittle to minor schema changes that don't affect downstream consumers. For example, rejecting records because a rarely-used optional field changes type, even though most consumers ignore that field. Design validation rules to focus on fields actually used by downstream systems.\n\n **Pitfall: Validation Performance Degradation**\nValidating every field of every record can become extremely expensive for wide tables with complex constraints. Profile validation performance and use sampling or lazy validation for non-critical checks. Monitor validation overhead as a percentage of total pipeline runtime.\n\n **Pitfall: Inconsistent Error Handling Across Schema Changes**\nDifferent types of schema validation failures may require different handling strategies, but implementing inconsistent policies makes debugging difficult. Establish clear error handling hierarchies: syntax errors always fail, type errors configurable, constraint violations logged but allowed to pass.\n\n### Implementation Guidance\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| SQL Templating | Python string formatting | Jinja2 template engine |\n| Python UDF Execution | Same-process function calls | Subprocess isolation with resource limits |\n| Schema Registry | JSON files in version control | Confluent Schema Registry or custom REST API |\n| Type Validation | Manual isinstance() checks | Pydantic models with automatic validation |\n| Database Connectivity | Direct database drivers | SQLAlchemy with connection pooling |\n\n**Recommended File Structure**\n\n```\netl-pipeline/\n  src/transformation/\n    __init__.py\n    engine.py                     Main transformation engine\n    sql_transformer.py            SQL template processing\n    python_udf.py                Python UDF execution\n    schema_validator.py          Schema validation logic\n    type_system.py               Type definitions and coercion\n    tests/\n      test_sql_transformer.py    SQL transformation tests\n      test_python_udf.py         UDF execution tests  \n      test_schema_validator.py   Schema validation tests\n  schemas/\n    registry/\n      user_events_v1.json        Schema definitions\n      user_events_v2.json\n    migrations/\n      v1_to_v2_migration.sql     Schema evolution scripts\n  transformations/\n    sql/\n      user_cleanup.sql           SQL transformation templates\n      daily_aggregation.sql\n    udfs/\n      phone_standardization.py   Python UDF definitions\n      address_geocoding.py\n```\n\n**Infrastructure Starter Code**\n\nHere's a complete type system foundation for the transformation engine:\n\n```python\n# type_system.py - Complete type system implementation\nfrom typing import Any, Dict, List, Optional, Union, Type\nfrom enum import Enum\nfrom datetime import datetime, date\nfrom decimal import Decimal\nimport json\n\nclass DataType(Enum):\n    \"\"\"Supported data types in the transformation pipeline.\"\"\"\n    STRING = \"string\"\n    INTEGER = \"integer\"\n    FLOAT = \"float\"\n    DECIMAL = \"decimal\"\n    BOOLEAN = \"boolean\"\n    DATE = \"date\"\n    TIMESTAMP = \"timestamp\"\n    JSON = \"json\"\n    BINARY = \"binary\"\n\nclass ValidationResult:\n    \"\"\"Result of data validation operation.\"\"\"\n    def __init__(self, is_valid: bool, errors: List[str] = None, warnings: List[str] = None):\n        self.is_valid = is_valid\n        self.errors = errors or []\n        self.warnings = warnings or []\n        \n    def add_error(self, error: str):\n        self.errors.append(error)\n        self.is_valid = False\n        \n    def add_warning(self, warning: str):\n        self.warnings.append(warning)\n\nclass TypeConverter:\n    \"\"\"Handles type conversion between different representations.\"\"\"\n    \n    def __init__(self):\n        self.conversion_rules = {\n            (DataType.INTEGER, DataType.FLOAT): self._int_to_float,\n            (DataType.INTEGER, DataType.STRING): self._int_to_string,\n            (DataType.FLOAT, DataType.STRING): self._float_to_string,\n            (DataType.STRING, DataType.INTEGER): self._string_to_int,\n            (DataType.STRING, DataType.FLOAT): self._string_to_float,\n            (DataType.STRING, DataType.BOOLEAN): self._string_to_bool,\n            (DataType.STRING, DataType.DATE): self._string_to_date,\n            (DataType.STRING, DataType.TIMESTAMP): self._string_to_timestamp,\n        }\n    \n    def convert(self, value: Any, from_type: DataType, to_type: DataType) -> tuple[Any, ValidationResult]:\n        \"\"\"Convert value from one type to another.\"\"\"\n        if value is None:\n            return None, ValidationResult(True)\n            \n        if from_type == to_type:\n            return value, ValidationResult(True)\n            \n        conversion_key = (from_type, to_type)\n        if conversion_key not in self.conversion_rules:\n            result = ValidationResult(False)\n            result.add_error(f\"No conversion rule from {from_type.value} to {to_type.value}\")\n            return value, result\n            \n        try:\n            converted_value = self.conversion_rules[conversion_key](value)\n            return converted_value, ValidationResult(True)\n        except Exception as e:\n            result = ValidationResult(False)\n            result.add_error(f\"Conversion failed: {str(e)}\")\n            return value, result\n    \n    def _int_to_float(self, value: int) -> float:\n        return float(value)\n    \n    def _int_to_string(self, value: int) -> str:\n        return str(value)\n    \n    def _float_to_string(self, value: float) -> str:\n        return str(value)\n    \n    def _string_to_int(self, value: str) -> int:\n        return int(value.strip())\n    \n    def _string_to_float(self, value: str) -> float:\n        return float(value.strip())\n    \n    def _string_to_bool(self, value: str) -> bool:\n        value_lower = value.strip().lower()\n        if value_lower in ('true', '1', 'yes', 'on'):\n            return True\n        elif value_lower in ('false', '0', 'no', 'off'):\n            return False\n        else:\n            raise ValueError(f\"Cannot convert '{value}' to boolean\")\n    \n    def _string_to_date(self, value: str) -> date:\n        # Support common date formats\n        for fmt in ('%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y'):\n            try:\n                return datetime.strptime(value.strip(), fmt).date()\n            except ValueError:\n                continue\n        raise ValueError(f\"Cannot parse date from '{value}'\")\n    \n    def _string_to_timestamp(self, value: str) -> datetime:\n        # Support common timestamp formats\n        for fmt in ('%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d %H:%M:%S.%f'):\n            try:\n                return datetime.strptime(value.strip(), fmt)\n            except ValueError:\n                continue\n        raise ValueError(f\"Cannot parse timestamp from '{value}'\")\n\n# Schema management utilities\nclass FieldDefinition:\n    \"\"\"Definition of a single field in a schema.\"\"\"\n    def __init__(self, name: str, data_type: DataType, nullable: bool = True, \n                 default_value: Any = None, constraints: List[str] = None):\n        self.name = name\n        self.data_type = data_type\n        self.nullable = nullable\n        self.default_value = default_value\n        self.constraints = constraints or []\n\nclass SchemaDefinition:\n    \"\"\"Complete schema definition for a dataset.\"\"\"\n    def __init__(self, name: str, version: int, fields: List[FieldDefinition]):\n        self.name = name\n        self.version = version\n        self.fields = {field.name: field for field in fields}\n        \n    def get_field(self, name: str) -> Optional[FieldDefinition]:\n        return self.fields.get(name)\n        \n    def validate_record(self, record: Dict[str, Any]) -> ValidationResult:\n        \"\"\"Validate a single record against this schema.\"\"\"\n        result = ValidationResult(True)\n        \n        # Check for required fields\n        for field_name, field_def in self.fields.items():\n            if not field_def.nullable and field_name not in record:\n                result.add_error(f\"Required field '{field_name}' is missing\")\n            elif field_name not in record:\n                record[field_name] = field_def.default_value\n                \n        # Check for unexpected fields\n        for field_name in record:\n            if field_name not in self.fields:\n                result.add_warning(f\"Unexpected field '{field_name}' found\")\n                \n        return result\n```\n\n**Core Logic Skeleton Code**\n\n```python\n# sql_transformer.py - SQL transformation engine skeleton\nfrom typing import Dict, Any, Iterator, Optional\nfrom jinja2 import Environment, FileSystemLoader, TemplateSyntaxError\nimport logging\n\nclass SQLTransformer:\n    \"\"\"Executes SQL-based transformations with parameter templating.\"\"\"\n    \n    def __init__(self, template_dir: str, connection_manager):\n        self.template_env = Environment(\n            loader=FileSystemLoader(template_dir),\n            autoescape=False  # SQL doesn't need HTML escaping\n        )\n        self.connection_manager = connection_manager\n        self.logger = logging.getLogger(__name__)\n        \n    def execute_transformation(self, template_name: str, parameters: Dict[str, Any], \n                             connection_name: str) -> Iterator[Dict[str, Any]]:\n        \"\"\"Execute a SQL transformation using a template with parameters.\"\"\"\n        # TODO 1: Load and validate the SQL template\n        #   - Use self.template_env.get_template(template_name)\n        #   - Handle TemplateNotFound exception\n        #   - Return empty iterator on template errors\n        \n        # TODO 2: Render the template with parameters\n        #   - Call template.render(**parameters)\n        #   - Handle TemplateSyntaxError and UndefinedError\n        #   - Log the rendered SQL for debugging\n        \n        # TODO 3: Acquire database connection\n        #   - Use self.connection_manager.get_connection(connection_name)\n        #   - Handle connection failures with appropriate retries\n        #   - Ensure connection is returned to pool on completion\n        \n        # TODO 4: Execute the rendered SQL query\n        #   - Begin transaction for consistency\n        #   - Execute query and get cursor\n        #   - Handle SQL syntax errors and database exceptions\n        \n        # TODO 5: Stream results back to caller\n        #   - Fetch results in batches to manage memory\n        #   - Convert database rows to dictionaries\n        #   - Yield each record to maintain streaming behavior\n        #   - Commit transaction on success, rollback on failure\n        \n        # Hint: Use try/finally to ensure connection cleanup\n        # Hint: Log execution metrics (row count, duration)\n        pass\n        \n    def validate_template(self, template_name: str, parameters: Dict[str, Any]) -> ValidationResult:\n        \"\"\"Validate template syntax and parameter completeness.\"\"\"\n        # TODO 1: Attempt to load template\n        # TODO 2: Try rendering with provided parameters  \n        # TODO 3: Check for undefined variables\n        # TODO 4: Return validation result with specific errors\n        pass\n\n# python_udf.py - Python UDF execution engine skeleton  \nimport subprocess\nimport pickle\nimport json\nfrom typing import Callable, List, Any, Dict\nfrom concurrent.futures import ProcessPoolExecutor\nimport resource\n\nclass PythonUDFExecutor:\n    \"\"\"Executes Python User-Defined Functions with isolation and resource limits.\"\"\"\n    \n    def __init__(self, max_workers: int = 4, memory_limit_mb: int = 1024):\n        self.max_workers = max_workers\n        self.memory_limit_mb = memory_limit_mb\n        self.function_registry: Dict[str, Callable] = {}\n        \n    def register_function(self, name: str, func: Callable):\n        \"\"\"Register a UDF for execution.\"\"\"\n        # TODO 1: Validate function signature has type annotations\n        # TODO 2: Store function in registry with metadata\n        # TODO 3: Validate function doesn't use restricted imports\n        pass\n        \n    def execute_function(self, function_name: str, data: List[Dict[str, Any]], \n                        execution_mode: str = \"row\") -> Iterator[Dict[str, Any]]:\n        \"\"\"Execute a registered UDF on input data.\"\"\"\n        # TODO 1: Look up function in registry\n        #   - Validate function exists\n        #   - Get function metadata and signature\n        \n        # TODO 2: Choose execution strategy based on mode\n        #   - \"row\": execute once per record\n        #   - \"batch\": execute on entire dataset\n        #   - \"stream\": execute on chunks\n        \n        # TODO 3: Prepare subprocess execution environment\n        #   - Set memory limits using resource module\n        #   - Prepare data serialization (pickle or json)\n        #   - Set up timeout and CPU limits\n        \n        # TODO 4: Execute UDF in isolated subprocess\n        #   - Use ProcessPoolExecutor for isolation\n        #   - Handle subprocess timeouts and errors\n        #   - Capture both results and any error messages\n        \n        # TODO 5: Process and validate results\n        #   - Deserialize results from subprocess\n        #   - Validate output types match function signature\n        #   - Yield results maintaining input order\n        \n        # Hint: Use pickle for complex Python objects, JSON for simple data\n        # Hint: Monitor subprocess resource usage and terminate if exceeded\n        pass\n        \n    def _execute_in_subprocess(self, func_code: str, data: Any, limits: Dict[str, Any]) -> Any:\n        \"\"\"Execute function code in isolated subprocess with resource limits.\"\"\"\n        # TODO 1: Set resource limits (memory, CPU time)\n        # TODO 2: Execute function code safely\n        # TODO 3: Return results or error information\n        pass\n\n# schema_validator.py - Schema validation engine skeleton\nclass SchemaValidator:\n    \"\"\"Validates data against schema definitions with evolution support.\"\"\"\n    \n    def __init__(self, schema_registry):\n        self.schema_registry = schema_registry\n        self.type_converter = TypeConverter()\n        \n    def validate_data_stream(self, data_stream: Iterator[Dict[str, Any]], \n                           schema_name: str, schema_version: Optional[int] = None) -> Iterator[Dict[str, Any]]:\n        \"\"\"Validate and optionally transform data stream against schema.\"\"\"\n        # TODO 1: Resolve schema from registry\n        #   - Get schema by name and version (latest if version is None)\n        #   - Handle schema not found errors\n        #   - Log schema information for debugging\n        \n        # TODO 2: Set up validation statistics tracking\n        #   - Count total records processed\n        #   - Count validation errors and warnings\n        #   - Track performance metrics\n        \n        # TODO 3: Process each record in the stream\n        #   - Validate record structure against schema\n        #   - Apply type conversions where needed\n        #   - Handle validation errors based on policy\n        \n        # TODO 4: Apply business rule validation\n        #   - Check constraint rules (range checks, format validation)\n        #   - Validate cross-field dependencies\n        #   - Apply custom validation functions\n        \n        # TODO 5: Yield validated/transformed records\n        #   - Apply any schema transformations\n        #   - Include validation metadata if requested\n        #   - Log summary statistics periodically\n        \n        # Hint: Use yield to maintain streaming behavior\n        # Hint: Batch validation operations for better performance\n        pass\n        \n    def check_schema_compatibility(self, old_schema: SchemaDefinition, \n                                 new_schema: SchemaDefinition) -> ValidationResult:\n        \"\"\"Check compatibility between two schema versions.\"\"\"\n        # TODO 1: Compare field definitions between schemas\n        # TODO 2: Check for breaking changes (removed fields, type changes)\n        # TODO 3: Identify safe changes (new optional fields)\n        # TODO 4: Return compatibility result with specific issues\n        pass\n```\n\n**Language-Specific Hints**\n\n- **Template Security**: Use Jinja2's `select_autoescape()` with `autoescape=False` for SQL templates to avoid escaping SQL syntax, but validate all parameters before rendering\n- **Subprocess Communication**: Use `pickle` for serializing complex Python objects to subprocesses, but fallback to JSON for simple data types that need to be debuggable\n- **Resource Limits**: Use the `resource` module to set `RLIMIT_AS` (memory) and `RLIMIT_CPU` (CPU time) limits in subprocess before executing UDF code\n- **Connection Pooling**: Use SQLAlchemy's `pool_pre_ping=True` to validate connections before use, preventing stale connection errors\n- **Type Validation**: Use `typing.get_type_hints()` to extract type annotations from UDF functions for automatic validation\n\n**Milestone Checkpoint**\n\nAfter implementing the data transformation engine, verify functionality with these steps:\n\n1. **SQL Transformation Test**: Create a simple template `SELECT * FROM {{ table_name }} WHERE date >= '{{ start_date }}'` and verify it renders correctly with parameters\n2. **UDF Registration Test**: Register a simple function like `def upper_case(text: str) -> str: return text.upper()` and verify it appears in the registry\n3. **Schema Validation Test**: Define a simple schema with required and optional fields, then validate both compliant and non-compliant records\n4. **Integration Test**: Run a complete transformation pipeline that extracts data, applies SQL transformations, executes a Python UDF, and validates against output schema\n\nExpected behavior:\n- Templates should render without errors and produce valid SQL\n- UDFs should execute in isolation and return expected results\n- Schema validation should catch type mismatches and constraint violations\n- The complete pipeline should process sample data end-to-end\n\nSigns something is wrong:\n- **Template rendering fails**: Check Jinja2 template syntax and parameter names\n- **UDF execution hangs**: Verify subprocess limits are set correctly and functions don't have infinite loops\n- **Schema validation is slow**: Profile validation logic and consider sampling or lazy validation\n- **Memory usage grows**: Check for memory leaks in UDF processes or connection pooling\n\n\n## Pipeline Orchestration and Monitoring\n\n> **Milestone(s):** Milestone 4 - Pipeline Orchestration & Monitoring: Implements pipeline execution with monitoring, alerting, and lineage tracking\n\n### Mental Model: Air Traffic Control\n\nThink of pipeline orchestration like an air traffic control (ATC) system at a busy airport. Just as ATC coordinates hundreds of flights with complex schedules, dependencies, and safety requirements, our orchestration system manages hundreds of data pipeline tasks with their own schedules, dependencies, and error handling needs.\n\nIn this analogy, each **pipeline** is like a flight route with multiple legs (tasks). The **scheduler** acts as the control tower, deciding when each flight can take off based on weather conditions (resource availability), runway capacity (system load), and air traffic patterns (other running pipelines). The **task execution engine** is like the ground crew and pilots executing each flight leg, reporting status back to the control tower.\n\nJust as ATC must handle flight delays, cancellations, and emergency landings gracefully while keeping passengers informed, our orchestration system must handle task failures, retries, and cascading dependencies while providing clear visibility into what's happening. The **monitoring and alerting system** functions like the airport's information displays and announcement system, keeping stakeholders informed of delays, gate changes, and arrivals.\n\nThe critical insight from this analogy is that orchestration is fundamentally about **coordinating resources, managing dependencies, and maintaining visibility** across a complex distributed system. Like ATC, our system must be reliable, observable, and capable of graceful degradation when things go wrong.\n\n### Scheduler Integration\n\nThe scheduler integration component serves as the entry point for pipeline execution, responsible for determining when pipelines should run based on time-based schedules or external events. This component bridges the gap between pipeline definitions and actual execution, translating abstract scheduling requirements into concrete execution triggers.\n\n#### Cron-Based Scheduling\n\nCron-based scheduling provides time-triggered pipeline execution using familiar cron expression syntax. The scheduler maintains an internal registry of active pipelines and their schedules, continuously evaluating upcoming execution windows.\n\nThe core scheduling algorithm operates on a polling model with configurable intervals (typically 30-60 seconds). During each poll cycle, the scheduler evaluates all registered pipelines to determine if any are due for execution. This evaluation considers the pipeline's cron expression, timezone settings, and any configured execution windows or blackout periods.\n\n> **Decision: Polling vs Event-Driven Scheduling**\n> - **Context**: Need to support both cron-based and event-driven pipeline triggers while maintaining simple operational semantics\n> - **Options Considered**: Pure event-driven system with cron events, hybrid polling for cron with events for external triggers, pure polling for all trigger types\n> - **Decision**: Hybrid approach with polling for cron-based schedules and event queues for external triggers\n> - **Rationale**: Polling provides predictable resource usage and simple failure recovery for time-based schedules, while event queues enable low-latency response to external triggers without the complexity of distributed cron\n> - **Consequences**: Slight delay (up to polling interval) for cron-triggered pipelines, but simplified operational model and better handling of system restarts\n\nThe scheduler maintains pipeline execution state to prevent duplicate runs and handle overlapping schedules. Each pipeline definition includes execution policies that control behavior when previous runs are still active or when schedules overlap.\n\n| Execution Policy | Behavior | Use Case |\n|------------------|----------|----------|\n| `ALLOW_CONCURRENT` | Start new run regardless of existing runs | Independent data processing |\n| `SKIP_ON_RUNNING` | Skip execution if previous run still active | Long-running ETL processes |\n| `CANCEL_PREVIOUS` | Terminate previous run and start new one | Real-time data updates |\n| `QUEUE_SEQUENTIAL` | Queue execution to start after current completes | Dependent processing chains |\n\nThe scheduler implements **catchup behavior** for pipelines that miss scheduled executions due to system downtime. When the scheduler restarts, it evaluates missed execution windows and can optionally trigger historical runs to maintain data consistency.\n\n#### Event-Driven Triggers\n\nEvent-driven pipeline triggers enable reactive execution based on external system events such as file arrivals, database changes, or API notifications. The scheduler maintains event subscription queues for different trigger types, processing incoming events asynchronously from the cron-based scheduling loop.\n\nEvent triggers support **payload-based parameterization**, allowing external events to provide runtime parameters for pipeline execution. For example, a file arrival event can specify the file path as a pipeline parameter, enabling the same pipeline definition to process different files dynamically.\n\nThe event processing system implements **deduplication and idempotency** to handle duplicate events gracefully. Each event includes a unique identifier and optional deduplication window, preventing redundant pipeline executions for repeated notifications.\n\n| Event Source Type | Configuration | Deduplication Strategy |\n|------------------|---------------|----------------------|\n| `FILE_ARRIVAL` | Directory path, file pattern, polling interval | File path + modification time |\n| `DATABASE_CHANGE` | Connection, table, CDC log position | Transaction ID + sequence number |\n| `API_WEBHOOK` | Endpoint URL, authentication, payload schema | Event ID from payload |\n| `MESSAGE_QUEUE` | Queue name, consumer group, message format | Message ID + partition offset |\n\n#### Schedule Management Interface\n\nThe scheduler exposes a management interface for registering, updating, and monitoring pipeline schedules. This interface supports dynamic schedule updates without requiring system restarts, enabling operational flexibility for changing business requirements.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `register_schedule` | `pipeline_id: str, schedule_config: ScheduleConfig` | `bool` | Register new pipeline schedule |\n| `update_schedule` | `pipeline_id: str, schedule_config: ScheduleConfig` | `bool` | Modify existing pipeline schedule |\n| `pause_schedule` | `pipeline_id: str, reason: str` | `bool` | Temporarily disable pipeline execution |\n| `resume_schedule` | `pipeline_id: str` | `bool` | Re-enable paused pipeline schedule |\n| `get_next_run_time` | `pipeline_id: str` | `Optional[datetime]` | Calculate next scheduled execution |\n| `get_schedule_status` | `pipeline_id: str` | `ScheduleStatus` | Retrieve current schedule state |\n\nThe scheduler maintains **schedule metadata** including creation timestamps, modification history, and execution statistics to support operational monitoring and debugging.\n\n### Task Execution Engine\n\nThe task execution engine orchestrates the actual execution of pipeline tasks, managing parallel execution, resource allocation, and state transitions. This component translates the abstract DAG structure into concrete task executions while maintaining dependency relationships and handling failures gracefully.\n\n#### Parallel Execution Management\n\nThe execution engine implements **level-based parallelization**, executing all tasks at the same DAG level simultaneously while respecting resource constraints. This approach maximizes throughput while ensuring dependency correctness.\n\nThe engine maintains an **execution queue** for each parallelization level, populated by the topological sort results from the DAG validation engine. As tasks complete successfully, the engine evaluates downstream tasks to determine when their dependencies are satisfied and they can be queued for execution.\n\n> **Decision: Thread Pool vs Process Pool for Task Execution**\n> - **Context**: Tasks may include CPU-intensive transformations, I/O-bound data loading, and potentially untrusted user-defined functions\n> - **Options Considered**: Single-threaded sequential execution, thread pool with shared memory, process pool with isolation, hybrid approach based on task type\n> - **Decision**: Configurable hybrid approach with thread pool for I/O-bound tasks and process pool for CPU-intensive or untrusted tasks\n> - **Rationale**: Thread pool provides efficient resource sharing for database connections and network I/O, while process pool ensures isolation for UDFs and prevents memory leaks from affecting other tasks\n> - **Consequences**: Requires task type classification in pipeline definitions, but provides optimal performance and safety characteristics\n\nThe execution engine implements **resource-aware scheduling** to prevent system overload. Each task definition includes resource requirements (CPU cores, memory, network connections), and the engine tracks available system resources to determine execution capacity.\n\n| Resource Type | Measurement Unit | Default Limit | Overflow Behavior |\n|---------------|------------------|---------------|------------------|\n| `CPU_CORES` | Number of cores | System CPU count | Queue task until resources available |\n| `MEMORY_MB` | Megabytes | 75% of system memory | Queue task with memory pressure warning |\n| `DB_CONNECTIONS` | Connection count | Connection pool size | Queue task until connection available |\n| `NETWORK_BANDWIDTH` | Mbps | Unlimited (monitoring only) | Log warning but continue execution |\n\n#### State Management and Persistence\n\nThe execution engine maintains comprehensive state information for all active task executions, persisting state changes to enable recovery after system failures. The state model supports complex execution patterns including retries, cancellations, and dependency failures.\n\nTask execution state follows a well-defined state machine with specific transition rules. The engine validates all state transitions to ensure consistency and prevent invalid operations.\n\n| Current State | Valid Events | Next State | Actions Taken |\n|---------------|--------------|------------|---------------|\n| `PENDING` | `DEPENDENCIES_MET` | `QUEUED` | Add to execution queue |\n| `QUEUED` | `EXECUTION_STARTED` | `RUNNING` | Allocate resources, start task |\n| `RUNNING` | `EXECUTION_COMPLETED` | `SUCCESS` | Release resources, update metrics |\n| `RUNNING` | `EXECUTION_FAILED` | `FAILED` or `RETRYING` | Check retry policy, possibly reschedule |\n| `FAILED` | `RETRY_SCHEDULED` | `QUEUED` | Increment attempt count, apply backoff |\n| `RETRYING` | `EXECUTION_STARTED` | `RUNNING` | Start retry attempt |\n| `SUCCESS` | `UPSTREAM_FAILED` | `SKIPPED` | Mark downstream tasks as skipped |\n\nThe engine implements **optimistic concurrency control** for state updates, using version numbers to detect concurrent modifications. This ensures state consistency when multiple components (scheduler, execution threads, monitoring) update task state simultaneously.\n\n#### Resource Allocation and Cleanup\n\nThe execution engine manages system resources including database connections, temporary files, and memory allocations. Resource management follows a **lease-based model** where each task execution receives a time-bounded lease on required resources.\n\nDatabase connections are managed through a **connection pooling strategy** that maintains separate pools for different database types and connection configurations. The engine pre-allocates connections based on upcoming task requirements and implements connection health checking to ensure reliability.\n\n| Connection Pool | Max Size | Idle Timeout | Health Check Interval |\n|----------------|----------|--------------|----------------------|\n| `PRIMARY_DB` | 20 connections | 300 seconds | 60 seconds |\n| `WAREHOUSE_DB` | 10 connections | 600 seconds | 120 seconds |\n| `STAGING_DB` | 15 connections | 180 seconds | 30 seconds |\n\nTemporary file management implements **automatic cleanup** with configurable retention policies. The engine creates isolated temporary directories for each task execution and schedules cleanup based on task completion status and retention requirements.\n\n **Pitfall: Resource Leak in Failed Tasks**\nMany implementations fail to properly clean up resources when tasks fail unexpectedly. This leads to connection pool exhaustion, temporary disk space consumption, and memory leaks. Always implement resource cleanup in finally blocks or using context managers, and include explicit resource leak detection in monitoring dashboards. Track resource allocation per task execution and alert when cleanup doesn't occur within expected timeframes.\n\n### Monitoring and Alerting\n\nThe monitoring and alerting system provides comprehensive observability into pipeline execution, collecting metrics, logs, and lineage information to support operational management and debugging. This system operates as a separate subsystem that observes execution without interfering with performance.\n\n#### Metrics Collection and Aggregation\n\nThe monitoring system collects **multi-dimensional metrics** across different aspects of pipeline execution including performance, reliability, and resource utilization. Metrics collection uses an asynchronous publishing model to minimize impact on task execution performance.\n\nThe system implements **hierarchical metric aggregation**, collecting detailed per-task metrics while rolling up summary statistics at the pipeline and system levels. This approach enables both detailed debugging and high-level operational dashboards.\n\n| Metric Category | Examples | Collection Frequency | Retention Period |\n|-----------------|----------|---------------------|------------------|\n| `PERFORMANCE` | Task duration, queue wait time, throughput | Per task execution | 90 days detailed, 1 year aggregated |\n| `RELIABILITY` | Success rate, failure count, retry attempts | Per task execution | 180 days detailed, 2 years aggregated |\n| `RESOURCE` | CPU usage, memory consumption, I/O wait | Every 30 seconds during execution | 30 days detailed, 6 months aggregated |\n| `BUSINESS` | Records processed, data quality scores | Per task execution | 1 year detailed, 5 years aggregated |\n\nThe metrics system supports **custom business metrics** defined in pipeline configurations, enabling domain-specific monitoring. Tasks can emit custom metrics using a simple API that automatically handles aggregation and persistence.\n\n#### Real-Time Dashboard Integration\n\nThe monitoring system provides **real-time dashboard integration** through standardized APIs that support popular monitoring platforms. The dashboard data model emphasizes actionable information over raw metrics, presenting derived insights that guide operational decisions.\n\nThe system implements **adaptive alerting thresholds** that automatically adjust based on historical performance patterns and seasonal variations. This reduces false positive alerts while maintaining sensitivity to genuine issues.\n\n| Dashboard View | Update Frequency | Key Metrics | Drill-Down Capability |\n|----------------|------------------|-------------|----------------------|\n| `SYSTEM_OVERVIEW` | 30 seconds | Active pipelines, success rate, resource utilization | Pipeline-level details |\n| `PIPELINE_DETAIL` | 15 seconds | Task status, execution timeline, dependency graph | Individual task logs |\n| `RESOURCE_MONITORING` | 10 seconds | CPU, memory, network, storage I/O | Historical trending |\n| `DATA_QUALITY` | Per pipeline run | Validation failures, schema changes, row counts | Quality rule details |\n\n#### Failure Notification and Escalation\n\nThe alerting system implements **intelligent failure classification** that categorizes failures by severity, impact, and required response. This classification drives different notification channels and escalation timelines.\n\nThe system maintains **alert routing rules** that direct notifications to appropriate teams based on failure characteristics, time of day, and team availability. Integration with on-call scheduling systems ensures critical failures reach available personnel quickly.\n\n> **Decision: Push vs Pull Alerting Model**\n> - **Context**: Need to balance timely notifications with alert fatigue while supporting multiple notification channels\n> - **Options Considered**: Pure push model with immediate notifications, pure pull model with polling-based alerts, hybrid model with intelligent routing\n> - **Decision**: Hybrid model with immediate push for critical failures and batched notifications for non-critical issues\n> - **Rationale**: Critical failures (data corruption, security issues) require immediate attention, while transient failures (temporary network issues) benefit from aggregation to reduce noise\n> - **Consequences**: Requires failure severity classification but significantly improves alert signal-to-noise ratio and reduces operational fatigue\n\nThe notification system supports **alert suppression and correlation** to prevent alert storms during widespread system issues. When multiple related failures occur simultaneously, the system identifies root cause relationships and suppresses downstream alerts.\n\n| Alert Severity | Notification Channel | Response Time SLA | Escalation Timeline |\n|----------------|---------------------|-------------------|-------------------|\n| `CRITICAL` | Phone, SMS, Slack | Immediate | 15 minutes to secondary on-call |\n| `HIGH` | Email, Slack | 5 minutes | 1 hour to team lead |\n| `MEDIUM` | Email | 15 minutes | 4 hours to product owner |\n| `LOW` | Dashboard only | Best effort | Weekly summary report |\n\n#### Data Lineage and Audit Trail\n\nThe monitoring system captures comprehensive **data lineage information** that tracks data flow through transformation steps and across pipeline boundaries. This lineage information supports impact analysis, debugging, and compliance requirements.\n\nLineage tracking operates at the **dataset and column level**, recording not just which tables were read and written, but which specific columns influenced which outputs. This granular tracking enables precise impact analysis when upstream data changes or issues occur.\n\nThe system maintains an **immutable audit trail** of all pipeline executions, configuration changes, and manual interventions. This audit trail supports compliance requirements and provides a complete historical record for debugging complex issues.\n\n| Lineage Event Type | Captured Information | Retention Policy |\n|-------------------|---------------------|------------------|\n| `DATA_READ` | Source table, query, row count, timestamp | 2 years |\n| `DATA_WRITE` | Target table, operation type, row count, schema | 5 years |\n| `TRANSFORMATION` | Function name, input columns, output columns, parameters | 2 years |\n| `SCHEMA_CHANGE` | Old schema, new schema, compatibility check results | 10 years |\n\n **Pitfall: Lineage Collection Performance Impact**\nDetailed lineage collection can significantly impact pipeline performance if not implemented carefully. Avoid synchronous lineage writes during task execution - instead, buffer lineage events in memory and flush asynchronously. Use sampling for high-throughput pipelines and implement circuit breakers to disable lineage collection if it begins affecting SLA compliance. Monitor lineage collection overhead and tune based on business requirements.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Task Queue | Redis with `rq` library | Apache Airflow or Celery with RabbitMQ |\n| Metrics Storage | SQLite with custom tables | InfluxDB or Prometheus |\n| Log Aggregation | File-based logging with logrotate | ELK stack (Elasticsearch, Logstash, Kibana) |\n| Alerting | Email via `smtplib` | PagerDuty API integration |\n| Dashboard | Flask web UI with simple HTML/CSS | Grafana with custom dashboards |\n| State Persistence | SQLite database | PostgreSQL with connection pooling |\n\n#### Recommended File Structure\n\n```\npipeline_orchestration/\n __init__.py\n scheduler/\n    __init__.py\n    cron_scheduler.py      # Time-based scheduling logic\n    event_scheduler.py     # Event-driven trigger handling\n    schedule_manager.py    # Schedule registration and management\n executor/\n    __init__.py\n    task_executor.py       # Core task execution engine\n    resource_manager.py    # Resource allocation and cleanup\n    state_manager.py       # Task state persistence and transitions\n monitoring/\n    __init__.py\n    metrics_collector.py   # Metrics collection and aggregation\n    alerting.py           # Alert routing and notification\n    lineage_tracker.py    # Data lineage capture and storage\n web/\n     __init__.py\n     dashboard.py          # Web dashboard for monitoring\n     api.py               # REST API for external integrations\n```\n\n#### Infrastructure Starter Code\n\n**Basic Metrics Collection System:**\n\n```python\nimport time\nimport threading\nfrom collections import defaultdict, deque\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Any\nimport json\nimport sqlite3\n\nclass MetricsCollector:\n    \"\"\"Thread-safe metrics collection with automatic aggregation.\"\"\"\n    \n    def __init__(self, db_path: str = \"metrics.db\"):\n        self.db_path = db_path\n        self.metrics_buffer = defaultdict(deque)\n        self.lock = threading.RLock()\n        self._init_db()\n        self._start_background_flush()\n    \n    def _init_db(self):\n        \"\"\"Initialize metrics database schema.\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS task_metrics (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    task_id TEXT NOT NULL,\n                    pipeline_run_id TEXT NOT NULL,\n                    metric_name TEXT NOT NULL,\n                    metric_value REAL NOT NULL,\n                    labels TEXT,  -- JSON string\n                    timestamp DATETIME NOT NULL\n                )\n            \"\"\")\n            conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_metrics_task_time \n                ON task_metrics(task_id, timestamp)\n            \"\"\")\n    \n    def record_metric(self, task_id: str, pipeline_run_id: str, \n                     metric_name: str, value: float, \n                     labels: Optional[Dict[str, str]] = None):\n        \"\"\"Record a metric value for a specific task.\"\"\"\n        with self.lock:\n            metric_record = {\n                'task_id': task_id,\n                'pipeline_run_id': pipeline_run_id,\n                'metric_name': metric_name,\n                'value': value,\n                'labels': json.dumps(labels or {}),\n                'timestamp': datetime.utcnow()\n            }\n            self.metrics_buffer[task_id].append(metric_record)\n    \n    def _flush_metrics(self):\n        \"\"\"Flush buffered metrics to database.\"\"\"\n        if not self.metrics_buffer:\n            return\n        \n        with self.lock:\n            all_metrics = []\n            for task_metrics in self.metrics_buffer.values():\n                all_metrics.extend(task_metrics)\n            self.metrics_buffer.clear()\n        \n        if all_metrics:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.executemany(\"\"\"\n                    INSERT INTO task_metrics \n                    (task_id, pipeline_run_id, metric_name, metric_value, labels, timestamp)\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\", [(m['task_id'], m['pipeline_run_id'], m['metric_name'], \n                      m['value'], m['labels'], m['timestamp']) for m in all_metrics])\n\nclass SimpleStateManager:\n    \"\"\"Thread-safe task state management with persistence.\"\"\"\n    \n    def __init__(self, db_path: str = \"task_state.db\"):\n        self.db_path = db_path\n        self.lock = threading.RLock()\n        self._init_db()\n    \n    def _init_db(self):\n        \"\"\"Initialize state database schema.\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS task_executions (\n                    task_id TEXT NOT NULL,\n                    pipeline_run_id TEXT NOT NULL,\n                    state TEXT NOT NULL,\n                    attempt_count INTEGER NOT NULL,\n                    started_at DATETIME,\n                    completed_at DATETIME,\n                    error_message TEXT,\n                    version INTEGER NOT NULL DEFAULT 1,\n                    PRIMARY KEY (task_id, pipeline_run_id)\n                )\n            \"\"\")\n    \n    def create_execution(self, task_id: str, pipeline_run_id: str) -> TaskExecution:\n        \"\"\"Create new task execution in PENDING state.\"\"\"\n        execution = TaskExecution(\n            task_id=task_id,\n            pipeline_run_id=pipeline_run_id,\n            state=TaskState.PENDING,\n            attempt_count=0,\n            started_at=None,\n            completed_at=None,\n            error_message=None,\n            logs=[],\n            metrics={}\n        )\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO task_executions \n                (task_id, pipeline_run_id, state, attempt_count)\n                VALUES (?, ?, ?, ?)\n            \"\"\", (task_id, pipeline_run_id, execution.state.value, execution.attempt_count))\n        \n        return execution\n    \n    def transition_state(self, task_id: str, pipeline_run_id: str, \n                        event: TaskEvent, error_message: str = None) -> bool:\n        \"\"\"Attempt state transition based on event.\"\"\"\n        # Implementation details in skeleton below\n        pass\n\nclass BasicAlertManager:\n    \"\"\"Simple alerting system with email and webhook support.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.alert_history = deque(maxlen=1000)  # Keep recent alerts for suppression\n    \n    def send_alert(self, severity: str, title: str, message: str, \n                  task_id: str = None, pipeline_id: str = None):\n        \"\"\"Send alert through configured channels based on severity.\"\"\"\n        alert = {\n            'severity': severity,\n            'title': title,\n            'message': message,\n            'task_id': task_id,\n            'pipeline_id': pipeline_id,\n            'timestamp': datetime.utcnow()\n        }\n        \n        # Check for suppression (simplified)\n        if self._should_suppress_alert(alert):\n            return\n        \n        self.alert_history.append(alert)\n        \n        # Route based on severity\n        channels = self.config.get('alert_channels', {}).get(severity, ['email'])\n        for channel in channels:\n            self._send_to_channel(channel, alert)\n```\n\n#### Core Logic Skeleton\n\n**Task Execution Engine Core:**\n\n```python\nclass TaskExecutionEngine:\n    \"\"\"Orchestrates parallel execution of pipeline tasks with resource management.\"\"\"\n    \n    def __init__(self, max_workers: int = 4, resource_limits: Dict[str, int] = None):\n        self.max_workers = max_workers\n        self.resource_limits = resource_limits or {}\n        self.active_executions: Dict[str, TaskExecution] = {}\n        self.execution_queue = queue.Queue()\n        self.metrics = MetricsCollector()\n        self.state_manager = SimpleStateManager()\n        \n    def execute_pipeline(self, pipeline: PipelineDefinition, \n                        run_id: str, parameters: Dict[str, Any] = None) -> bool:\n        \"\"\"Execute complete pipeline with dependency management and monitoring.\"\"\"\n        # TODO 1: Create execution plan using topological sort from DAG engine\n        # Hint: Use create_execution_plan(pipeline, estimated_durations)\n        \n        # TODO 2: Initialize task executions in PENDING state\n        # Hint: Create TaskExecution for each task using state_manager.create_execution()\n        \n        # TODO 3: Start execution levels in dependency order\n        # Hint: Process execution_plan.execution_levels sequentially, tasks within level in parallel\n        \n        # TODO 4: Monitor running tasks and handle state transitions\n        # Hint: Use worker threads to poll task status and call _handle_task_completion()\n        \n        # TODO 5: Handle failures and determine if pipeline should continue\n        # Hint: Check retry policies, update dependent task states, decide on pipeline failure\n        \n        # TODO 6: Clean up resources and persist final state\n        # Hint: Release all allocated resources, flush metrics, update pipeline run status\n        \n    def _execute_task_level(self, task_ids: List[str], run_id: str, \n                           parameters: Dict[str, Any]) -> Dict[str, bool]:\n        \"\"\"Execute all tasks in a dependency level in parallel.\"\"\"\n        # TODO 1: Check resource availability for all tasks in level\n        # Hint: Sum resource requirements and compare to limits\n        \n        # TODO 2: Allocate resources and transition tasks to QUEUED state\n        # Hint: Reserve DB connections, temp directories, update task state\n        \n        # TODO 3: Submit tasks to thread pool for parallel execution\n        # Hint: Use ThreadPoolExecutor or ProcessPoolExecutor based on task type\n        \n        # TODO 4: Wait for all tasks to complete with timeout\n        # Hint: Use concurrent.futures.wait() with appropriate timeout\n        \n        # TODO 5: Collect results and handle any exceptions\n        # Hint: Check Future.exception() for each completed task\n        \n        return {}  # task_id -> success mapping\n    \n    def _execute_single_task(self, task_def: TaskDefinition, run_id: str, \n                           parameters: Dict[str, Any]) -> bool:\n        \"\"\"Execute individual task with monitoring and error handling.\"\"\"\n        task_id = task_def.id\n        start_time = time.time()\n        \n        # TODO 1: Transition task to RUNNING state\n        # Hint: Use state_manager.transition_state() with EXECUTION_STARTED event\n        \n        # TODO 2: Load appropriate connector/transformer based on task type\n        # Hint: Use factory pattern to instantiate correct handler for task_def.type\n        \n        # TODO 3: Execute task with timeout and resource monitoring\n        # Hint: Wrap execution in try/except, measure resource usage, enforce timeout\n        \n        # TODO 4: Record execution metrics (duration, rows processed, etc.)\n        # Hint: Use metrics.record_metric() for performance and business metrics\n        \n        # TODO 5: Handle success/failure and update task state appropriately\n        # Hint: Call state_manager.transition_state() with appropriate event\n        \n        # TODO 6: Clean up any task-specific resources\n        # Hint: Close connections, delete temp files, release memory\n        \n        return False  # Return success/failure\n```\n\n**Scheduler Integration Core:**\n\n```python\nclass CronScheduler:\n    \"\"\"Handles time-based pipeline scheduling with catchup and overlap handling.\"\"\"\n    \n    def __init__(self, execution_engine: TaskExecutionEngine):\n        self.execution_engine = execution_engine\n        self.registered_pipelines: Dict[str, PipelineDefinition] = {}\n        self.active_runs: Dict[str, str] = {}  # pipeline_id -> run_id\n        self.running = False\n        \n    def start_scheduler(self):\n        \"\"\"Start the scheduler polling loop.\"\"\"\n        self.running = True\n        threading.Thread(target=self._scheduler_loop, daemon=True).start()\n    \n    def register_pipeline(self, pipeline: PipelineDefinition) -> bool:\n        \"\"\"Register pipeline for scheduled execution.\"\"\"\n        # TODO 1: Validate pipeline definition and schedule expression\n        # Hint: Use validate_pipeline() and parse cron expression\n        \n        # TODO 2: Check for schedule conflicts with existing pipelines\n        # Hint: Evaluate if overlapping schedules might cause resource conflicts\n        \n        # TODO 3: Store pipeline in registry with schedule metadata\n        # Hint: Include next run time calculation and execution policy\n        \n        # TODO 4: Log registration and trigger initial schedule calculation\n        # Hint: Calculate next execution time and log for monitoring\n        \n        return False\n    \n    def _scheduler_loop(self):\n        \"\"\"Main scheduler polling loop that evaluates and triggers pipelines.\"\"\"\n        while self.running:\n            # TODO 1: Calculate current time and evaluation window\n            # Hint: Use timezone-aware datetime, consider scheduling tolerance window\n            \n            # TODO 2: Evaluate each registered pipeline for execution readiness\n            # Hint: Check if current time matches cron expression, handle missed executions\n            \n            # TODO 3: Apply execution policies for ready pipelines\n            # Hint: Check ALLOW_CONCURRENT, SKIP_ON_RUNNING, etc. policies\n            \n            # TODO 4: Trigger pipeline executions and track active runs\n            # Hint: Generate run_id, call execution_engine.execute_pipeline(), update tracking\n            \n            # TODO 5: Clean up completed runs and update metrics\n            # Hint: Remove from active_runs, record scheduling metrics\n            \n            # TODO 6: Sleep until next evaluation cycle\n            # Hint: Use configurable poll interval, typically 30-60 seconds\n            \n            time.sleep(30)  # Default poll interval\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the orchestration and monitoring components, verify the system with these checkpoints:\n\n**Test Command:**\n```bash\npython -m pytest tests/test_orchestration.py -v\npython scripts/run_sample_pipeline.py --pipeline simple_etl --monitor true\n```\n\n**Expected Behavior:**\n1. **Scheduler Registration**: Register a simple pipeline with 5-minute cron schedule, verify next execution time calculation\n2. **Task Execution**: Execute pipeline manually, observe parallel task execution within dependency levels\n3. **State Transitions**: Monitor task state changes from PENDING  QUEUED  RUNNING  SUCCESS\n4. **Metrics Collection**: Verify metrics are collected for task duration, success rate, and resource usage\n5. **Alert Generation**: Trigger a task failure and verify alert is generated with appropriate severity\n6. **Resource Management**: Execute multiple pipelines simultaneously and verify resource limits are respected\n\n**Verification Steps:**\n```bash\n# Check scheduler registration\ncurl http://localhost:8080/api/schedules | jq .\n\n# Monitor pipeline execution\ncurl http://localhost:8080/api/runs/active | jq .\n\n# View collected metrics\nsqlite3 metrics.db \"SELECT * FROM task_metrics ORDER BY timestamp DESC LIMIT 10;\"\n\n# Test alerting\ncurl -X POST http://localhost:8080/api/test/trigger-failure\n```\n\n**Signs of Issues:**\n- Tasks stuck in QUEUED state: Check resource availability and thread pool configuration\n- Missing metrics: Verify metrics collector background flush is running\n- Alert flooding: Check alert suppression logic and severity classification\n- Memory leaks during long runs: Verify resource cleanup in finally blocks\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** All milestones - describes how components from pipeline definition (Milestone 1), data processing (Milestones 2-3), and orchestration (Milestone 4) interact through well-defined interfaces and protocols.\n\n### Mental Model: Orchestra Conductor\n\nThink of the ETL system as a symphony orchestra performing a complex musical piece. The **DAG Definition Engine** is like the sheet music - it defines what needs to be played, when, and in what order. The **Pipeline Orchestration Engine** acts as the conductor, coordinating timing, cueing different sections, and managing the overall flow. Individual **task executors** are like musicians - each with specialized skills (violinists, trumpeters, percussionists) who execute their parts when signaled. The **monitoring system** is like the audience and recording equipment - observing the performance, capturing what happened, and providing feedback. Just as musicians must follow precise timing and hand-off cues between sections, ETL components communicate through well-defined message protocols and data contracts.\n\nThe beauty of this orchestration lies in the coordination - when the conductor raises their baton to start the performance, a cascade of precisely-timed interactions begins. First movement starts with strings (extraction tasks), then woodwinds join in (transformation tasks), finally brass completes the harmony (loading tasks). Each musician knows exactly when to play their part based on visual cues from the conductor and audio cues from other sections. Similarly, each ETL component knows when to execute based on state transitions and dependency signals from other components.\n\n### Component Communication\n\nThe ETL system components communicate through a combination of **synchronous API calls** for control operations and **asynchronous message passing** for data processing events. This hybrid approach balances the need for immediate feedback on critical operations with the scalability requirements of high-throughput data processing.\n\n> **Decision: Hybrid Synchronous/Asynchronous Communication**\n> - **Context**: Components need both immediate control feedback and scalable event processing\n> - **Options Considered**: Pure synchronous APIs, pure message queues, hybrid approach\n> - **Decision**: Hybrid synchronous APIs for control, asynchronous messages for events\n> - **Rationale**: Control operations need immediate success/failure feedback, but data events need decoupling for scalability\n> - **Consequences**: Enables responsive control plane while maintaining scalable data plane, but requires managing two communication patterns\n\n| Communication Type | Use Cases | Pattern | Benefits | Trade-offs |\n|-------------------|-----------|---------|----------|------------|\n| Synchronous APIs | Pipeline registration, schedule updates, manual triggers | Request/Response | Immediate feedback, simple error handling | Blocking operations, limited scalability |\n| Asynchronous Messages | Task state changes, metrics collection, lineage events | Pub/Sub | High throughput, loose coupling | Eventual consistency, complex error handling |\n| Shared State | Task execution status, pipeline run metadata | Database/Cache | Persistent state, complex queries | Potential bottleneck, consistency complexity |\n\n#### Control Plane APIs\n\nThe **control plane** handles pipeline management operations through RESTful APIs. These operations require immediate feedback and typically have lower frequency but higher reliability requirements.\n\n| Method | Endpoint | Request Format | Response Format | Description |\n|--------|----------|---------------|-----------------|-------------|\n| `register_pipeline` | POST /api/v1/pipelines | `PipelineDefinition` | `{\"pipeline_id\": str, \"version\": int}` | Register new pipeline definition |\n| `update_pipeline` | PUT /api/v1/pipelines/{id} | `PipelineDefinition` | `{\"pipeline_id\": str, \"version\": int}` | Update existing pipeline |\n| `trigger_pipeline` | POST /api/v1/pipelines/{id}/runs | `{\"parameters\": dict, \"priority\": int}` | `{\"run_id\": str, \"status\": str}` | Manually trigger pipeline execution |\n| `get_pipeline_status` | GET /api/v1/pipelines/{id}/status | None | `{\"status\": str, \"last_run\": datetime, \"next_run\": datetime}` | Retrieve pipeline execution status |\n| `pause_pipeline` | POST /api/v1/pipelines/{id}/pause | `{\"reason\": str}` | `{\"success\": bool}` | Temporarily disable pipeline |\n| `get_task_logs` | GET /api/v1/runs/{run_id}/tasks/{task_id}/logs | None | `{\"logs\": List[str], \"metrics\": dict}` | Retrieve task execution logs |\n\nEach API endpoint follows a standard request/response pattern with consistent error handling. All endpoints return HTTP status codes that map directly to operation outcomes: 200 for success, 400 for validation errors, 404 for missing resources, 500 for system errors. Error responses include structured error objects with error codes, human-readable messages, and context information for debugging.\n\n#### Data Plane Messaging\n\nThe **data plane** handles high-frequency operational events through asynchronous messaging. This system uses a publish/subscribe pattern where components publish events to named topics and subscribe to events they need to process.\n\n| Event Type | Topic | Message Format | Publisher | Subscribers |\n|------------|-------|---------------|-----------|-------------|\n| Task State Change | `task.state.{pipeline_id}` | `TaskStateEvent` | Task Executor | Orchestrator, Monitoring |\n| Data Lineage | `lineage.{pipeline_id}` | `LineageEvent` | Connectors, Transformers | Lineage Tracker |\n| Metrics Collection | `metrics.{component}` | `MetricsEvent` | All Components | Monitoring Dashboard |\n| Pipeline Progress | `progress.{run_id}` | `ProgressEvent` | Orchestrator | UI, Alerting |\n| Resource Usage | `resources.{executor_id}` | `ResourceEvent` | Task Executors | Resource Manager |\n\n**Message Format Specifications:**\n\n| Field | Type | Description | Required | Example |\n|-------|------|-------------|----------|---------|\n| `event_id` | str | Unique event identifier | Yes | \"evt_123e4567-e89b-12d3\" |\n| `timestamp` | datetime | Event occurrence time | Yes | \"2024-01-15T14:30:00Z\" |\n| `source_component` | str | Component that generated event | Yes | \"task_executor_3\" |\n| `event_type` | str | Specific event classification | Yes | \"TASK_STATE_CHANGED\" |\n| `pipeline_id` | str | Associated pipeline identifier | Yes | \"customer_data_pipeline\" |\n| `run_id` | str | Associated pipeline run | Yes | \"run_20240115_143000\" |\n| `task_id` | str | Associated task identifier | No | \"extract_customer_data\" |\n| `payload` | dict | Event-specific data | Yes | {\"old_state\": \"RUNNING\", \"new_state\": \"SUCCESS\"} |\n| `correlation_id` | str | Request correlation identifier | No | \"req_987fcdeb-51a2-34b5\" |\n\n#### Inter-Component State Synchronization\n\nComponents maintain consistency through a combination of **authoritative state ownership** and **event-driven synchronization**. Each component owns specific state domains and publishes changes as events that other components can consume to maintain their derived views.\n\n> **Key Design Principle**: State ownership is clearly partitioned - the Orchestrator owns pipeline run state, Task Executors own task execution state, and the Monitoring system owns aggregated metrics. No component directly modifies another component's authoritative state.\n\n| Component | Owned State | Published Events | Subscribed Events |\n|-----------|-------------|------------------|-------------------|\n| DAG Engine | Pipeline definitions, validation results | `pipeline.registered`, `dag.validated` | None |\n| Orchestrator | Pipeline runs, execution plans | `run.started`, `run.completed`, `task.scheduled` | `task.state.*`, `executor.heartbeat` |\n| Task Executor | Task executions, resource usage | `task.state.*`, `metrics.executor` | `task.scheduled`, `run.cancelled` |\n| Scheduler | Schedule configurations, next run times | `schedule.triggered`, `schedule.updated` | `run.completed`, `pipeline.updated` |\n| Monitoring | Aggregated metrics, alerts | `alert.triggered`, `metrics.aggregated` | `task.state.*`, `metrics.*` |\n\n### Pipeline Execution Flow\n\nA complete pipeline execution involves a carefully orchestrated sequence of interactions between components. Understanding this flow is crucial for debugging issues and optimizing performance.\n\n![DAG Execution Sequence](./diagrams/dag-execution-flow.svg)\n\n#### Phase 1: Pipeline Trigger and Validation\n\nThe execution flow begins when the **Scheduler** determines that a pipeline should run based on its schedule configuration or when a manual trigger is received through the control API.\n\n1. **Schedule Evaluation**: The Scheduler evaluates all active pipeline schedules using their cron expressions and determines which pipelines are ready to execute. It checks the current time against the `get_next_run_time()` calculation for each pipeline.\n\n2. **Trigger Generation**: When a pipeline's execution time arrives, the Scheduler generates a trigger event containing the pipeline ID, scheduled execution time, and any default parameters defined in the schedule configuration.\n\n3. **Trigger Validation**: The Orchestrator receives the trigger event and performs initial validation checks. It verifies that the pipeline definition exists, is not paused, and that the execution policy allows a new run (checking concurrent execution rules).\n\n4. **Run Instance Creation**: The Orchestrator creates a new pipeline run instance with a unique `run_id`, initial state of `INITIALIZING`, and timestamps for tracking. It also resolves any runtime parameters by merging scheduled parameters with pipeline defaults.\n\n5. **DAG Retrieval and Validation**: The Orchestrator calls `get_pipeline(pipeline_id)` to retrieve the current pipeline definition, then invokes `validate_pipeline(pipeline)` to ensure the DAG is still valid (no cycles, all task definitions complete).\n\n6. **Execution Plan Generation**: The Orchestrator calls `create_execution_plan(pipeline, task_durations)` to determine the optimal execution order, parallel execution levels, and resource requirements. This plan guides the entire execution process.\n\n> **Critical Insight**: The execution plan is generated fresh for each run, allowing the system to incorporate updated task duration estimates and current resource availability. This dynamic planning enables better resource utilization than static execution orders.\n\n#### Phase 2: Task Scheduling and Dependency Resolution\n\nOnce the execution plan is ready, the Orchestrator begins scheduling individual tasks based on their dependency relationships and resource requirements.\n\n7. **Initial Task Identification**: The Orchestrator examines the execution plan's first level (tasks with no dependencies) and marks them as eligible for scheduling. These tasks transition from `PENDING` to `WAITING` state.\n\n8. **Resource Availability Check**: For each eligible task, the Orchestrator queries available Task Executors to find those with sufficient resources (CPU, memory, storage) to handle the task based on its resource requirements specification.\n\n9. **Task Assignment**: The Orchestrator assigns tasks to available executors using a load balancing algorithm that considers current executor load, task resource requirements, and data locality hints. Each assignment creates a `TaskExecution` record with initial state `QUEUED`.\n\n10. **Execution Message Dispatch**: The Orchestrator publishes task execution messages to the `task.scheduled` topic, containing task definitions, runtime parameters, and execution context. Task Executors subscribe to these messages and begin processing assigned tasks.\n\n11. **Dependency Tracking**: The Orchestrator maintains a dependency tracking matrix that monitors which tasks are running, completed, or failed. This matrix enables efficient determination of when downstream tasks become eligible for execution.\n\n#### Phase 3: Task Execution and State Management\n\nTask Executors receive task assignments and manage the actual execution of extraction, transformation, and loading operations.\n\n12. **Task Initialization**: When a Task Executor receives a task assignment, it transitions the task state to `RUNNING` and publishes a state change event. It also allocates local resources and establishes any required connections to data sources.\n\n13. **Data Processing Execution**: The executor invokes the appropriate connector or transformer based on the task type. For extraction tasks, it calls `extract(query, options)` to retrieve data. For transformation tasks, it calls transformation functions. For loading tasks, it calls `load(data_stream, target, options)`.\n\n14. **Progress Monitoring**: During execution, the Task Executor periodically publishes progress events containing metrics like records processed, bytes transferred, and execution time. These events enable real-time monitoring and early detection of performance issues.\n\n15. **Error Handling and Retries**: If a task encounters an error, the executor evaluates the task's `RetryPolicy` to determine whether to retry immediately, schedule a delayed retry with exponential backoff, or mark the task as failed. Each decision triggers appropriate state transitions.\n\n16. **Task Completion**: Upon successful completion, the executor publishes a task completion event with final metrics and output metadata. It also performs cleanup of local resources and temporary files.\n\n#### Phase 4: Dependency Propagation and Pipeline Completion\n\nAs tasks complete, the Orchestrator updates its dependency tracking and schedules downstream tasks that become eligible for execution.\n\n17. **Dependency Update**: When the Orchestrator receives a task completion event, it updates its dependency matrix and identifies downstream tasks whose dependencies are now satisfied. These tasks transition from `PENDING` to `WAITING` state.\n\n18. **Next Level Scheduling**: The Orchestrator repeats the task scheduling process (steps 8-11) for newly eligible tasks, maintaining the parallel execution levels defined in the execution plan while respecting resource constraints.\n\n19. **Pipeline Progress Tracking**: The Orchestrator continuously tracks overall pipeline progress by monitoring the completion ratio of tasks at each execution level. It publishes pipeline progress events that external systems can consume for dashboards and notifications.\n\n20. **Failure Impact Analysis**: If any task fails and exhausts its retry attempts, the Orchestrator analyzes the impact on downstream tasks. Depending on the pipeline's failure policy, it may cancel dependent tasks, mark them as skipped, or continue execution of independent task branches.\n\n21. **Pipeline Completion**: The pipeline completes when all tasks have reached terminal states (SUCCESS, FAILED, or SKIPPED). The Orchestrator calculates final pipeline status, aggregates metrics from all tasks, and publishes a pipeline completion event.\n\n#### Phase 5: Cleanup and Lineage Recording\n\nThe final phase ensures proper resource cleanup and captures complete data lineage information for audit and debugging purposes.\n\n22. **Resource Cleanup**: Task Executors perform final cleanup of any resources allocated for the pipeline run, including temporary files, database connections, and allocated memory. They also publish resource deallocation events for capacity planning.\n\n23. **Lineage Consolidation**: The Lineage Tracker aggregates all lineage events published during the pipeline run to create a complete data provenance record. This includes source data locations, transformation operations applied, and destination data locations.\n\n24. **Metrics Aggregation**: The Monitoring system calculates pipeline-level metrics by aggregating task-level metrics. This includes total execution time, data volume processed, resource utilization, and error rates.\n\n25. **Audit Log Creation**: The system generates comprehensive audit logs containing the complete execution history, all state transitions, error messages, and performance metrics. These logs are stored for compliance and debugging purposes.\n\n26. **Schedule Update**: The Scheduler updates the pipeline's next execution time based on its cron schedule and records the completion of the current run. This ensures proper spacing of future executions according to the schedule configuration.\n\n### Data Lineage Tracking\n\n**Data lineage** provides a complete audit trail of how data flows through the ETL pipeline, enabling data governance, impact analysis, and debugging of data quality issues. The lineage system captures not just what data was processed, but how it was transformed and where it ended up.\n\n![Monitoring and Lineage Data Flow](./diagrams/monitoring-data-flow.svg)\n\n#### Mental Model: Evidence Chain in Investigation\n\nThink of data lineage like the chain of evidence in a criminal investigation. Every piece of evidence must be tracked from its original location through every person who handled it, every test performed on it, and every conclusion drawn from it. If a piece of evidence becomes contaminated or questions arise about its authenticity, investigators can trace back through the complete chain to identify where problems occurred. Similarly, when data quality issues arise in production reports, data lineage allows you to trace back through every transformation, join, and aggregation to identify the root cause.\n\nJust as evidence must be handled by authorized personnel following documented procedures, data in the ETL pipeline should only be modified by authorized transformation steps following defined business rules. The lineage system acts like the evidence log book, recording every hand-off and every operation performed.\n\n#### Lineage Data Model\n\nThe lineage system tracks relationships between **data assets** (databases, tables, files), **transformations** (ETL tasks), and **pipeline runs** through a graph-based model that captures both schema-level and instance-level lineage.\n\n| Field | Type | Description | Example |\n|-------|------|-------------|---------|\n| `lineage_id` | str | Unique identifier for lineage record | \"lineage_123e4567-e89b\" |\n| `pipeline_run_id` | str | Associated pipeline execution | \"run_20240115_143000\" |\n| `task_id` | str | Task that performed transformation | \"transform_customer_data\" |\n| `source_assets` | List[AssetReference] | Input data assets consumed | [{\"type\": \"table\", \"name\": \"raw.customers\"}] |\n| `target_assets` | List[AssetReference] | Output data assets produced | [{\"type\": \"table\", \"name\": \"clean.customers\"}] |\n| `transformation_type` | str | Type of operation performed | \"COLUMN_MAPPING\", \"AGGREGATION\", \"JOIN\" |\n| `transformation_logic` | str | SQL query or transformation code | \"SELECT customer_id, UPPER(name) FROM raw.customers\" |\n| `schema_changes` | List[SchemaChange] | Column additions, deletions, renames | [{\"type\": \"RENAME\", \"from\": \"cust_name\", \"to\": \"customer_name\"}] |\n| `data_profile` | dict | Statistical summary of processed data | {\"row_count\": 1000000, \"null_percentage\": 0.02} |\n| `execution_timestamp` | datetime | When transformation occurred | \"2024-01-15T14:35:22Z\" |\n| `lineage_metadata` | dict | Additional context and annotations | {\"business_purpose\": \"data cleanup\", \"data_steward\": \"alice\"} |\n\n**AssetReference Structure:**\n\n| Field | Type | Description | Example |\n|-------|------|-------------|---------|\n| `asset_type` | str | Type of data asset | \"database_table\", \"file\", \"api_endpoint\" |\n| `asset_name` | str | Fully qualified asset name | \"warehouse.clean.customers\" |\n| `asset_schema` | str | Schema or namespace | \"clean\" |\n| `location_uri` | str | Physical location or connection string | \"postgresql://warehouse:5432/clean/customers\" |\n| `partition_info` | dict | Partition or file path details | {\"date_partition\": \"2024-01-15\", \"file_path\": \"/data/customers/2024/01/15/\"} |\n| `column_lineage` | List[ColumnLineage] | Field-level transformation details | [{\"source_column\": \"cust_name\", \"target_column\": \"customer_name\", \"transformation\": \"UPPER()\"}] |\n\n#### Lineage Collection Strategy\n\nThe ETL system employs a **multi-level lineage collection** strategy that captures lineage information at different granularities depending on the transformation complexity and business requirements.\n\n> **Decision: Multi-Level Lineage Collection**\n> - **Context**: Different stakeholders need lineage at different levels of detail\n> - **Options Considered**: Table-level only, column-level only, configurable multi-level\n> - **Decision**: Implement configurable multi-level collection with table, column, and value-level tracking\n> - **Rationale**: Business users need table-level for impact analysis, data stewards need column-level for compliance, developers need value-level for debugging\n> - **Consequences**: Enables flexible lineage reporting but increases storage and processing overhead\n\n**Lineage Collection Levels:**\n\n| Level | Granularity | Collection Method | Use Cases | Performance Impact |\n|-------|-------------|-------------------|-----------|-------------------|\n| Dataset | Table/file level | Connector metadata | Impact analysis, data discovery | Minimal |\n| Schema | Column level | SQL parsing, schema inference | Compliance reporting, data mapping | Low |\n| Instance | Row/record level | Data sampling, checksums | Data quality debugging, auditing | High |\n| Value | Field transformation level | Complete data capture | Regulatory compliance, forensics | Very High |\n\n#### Automatic Lineage Capture\n\nThe lineage system automatically captures lineage information during pipeline execution without requiring explicit instrumentation from pipeline developers. This automatic collection uses multiple techniques depending on the transformation type.\n\n**SQL-Based Transformation Lineage:**\n\nFor SQL transformations, the system uses **SQL parsing and analysis** to extract lineage relationships automatically. The SQL parser analyzes SELECT statements to identify source tables, join relationships, filter conditions, and column transformations.\n\n1. **Query Parsing**: The transformation engine intercepts SQL queries before execution and parses them using a SQL abstract syntax tree (AST) parser that understands the specific SQL dialect being used.\n\n2. **Dependency Extraction**: The parser identifies all referenced tables, views, and columns in FROM, JOIN, and WHERE clauses, creating source asset references with exact column mappings.\n\n3. **Transformation Logic Capture**: The parser extracts column expressions from the SELECT clause, including function calls, arithmetic operations, and case statements, preserving the exact transformation logic applied.\n\n4. **Output Schema Inference**: The parser predicts output column names and types based on the SELECT clause analysis, creating target asset references that match the actual query results.\n\n**Python UDF Lineage:**\n\nFor Python user-defined functions, the system uses **code instrumentation and runtime inspection** to capture lineage relationships that cannot be determined through static analysis.\n\n1. **Function Registration**: When UDFs are registered using `register_function(name, func)`, the system wraps the function with lineage collection decorators that intercept input and output data.\n\n2. **Input Tracking**: The wrapper captures metadata about input data streams, including schema information, data source references, and statistical profiles of the input data.\n\n3. **Output Analysis**: The wrapper analyzes function outputs to determine output schema, data transformations applied, and relationships between input and output fields.\n\n4. **Manual Annotations**: The system provides decorators that allow UDF developers to explicitly declare lineage relationships when automatic detection is insufficient, such as complex business logic or external API calls.\n\n#### Lineage Query and Analysis\n\nThe lineage system provides both programmatic APIs and query interfaces for analyzing data provenance and impact relationships. These capabilities support both interactive exploration and automated governance workflows.\n\n**Forward Impact Analysis:**\n\nForward impact analysis answers the question: \"If this source data changes, what downstream systems will be affected?\" This analysis is crucial for change management and data governance.\n\n| Query Type | Method | Parameters | Returns | Use Case |\n|------------|--------|------------|---------|----------|\n| Direct Dependencies | `get_direct_dependencies(asset)` | Asset reference | List of immediate downstream assets | Change impact assessment |\n| Transitive Dependencies | `get_all_dependencies(asset, max_depth)` | Asset reference, traversal depth | Complete dependency graph | Full impact analysis |\n| Pipeline Impact | `get_affected_pipelines(asset)` | Asset reference | List of pipeline IDs | Pipeline scheduling decisions |\n| Schema Impact | `get_schema_dependencies(asset, column)` | Asset and column references | Column-level dependency graph | Schema evolution planning |\n\n**Backward Provenance Analysis:**\n\nBackward provenance analysis answers: \"Where did this data come from and how was it transformed?\" This analysis supports data quality investigations and audit requirements.\n\n| Query Type | Method | Parameters | Returns | Use Case |\n|------------|--------|------------|---------|----------|\n| Data Sources | `get_data_sources(asset)` | Asset reference | List of source assets and transformations | Root cause analysis |\n| Transformation History | `get_transformation_path(source, target)` | Source and target assets | Complete transformation sequence | Data quality debugging |\n| Business Logic Trace | `get_business_rules(asset, column)` | Asset and column references | Applied business rules and transformations | Compliance auditing |\n| Temporal Lineage | `get_lineage_at_time(asset, timestamp)` | Asset reference and time | Historical lineage relationships | Point-in-time analysis |\n\n#### Common Pitfalls\n\n **Pitfall: Lineage Collection Overhead**\nMany implementations collect too much lineage information by default, severely impacting pipeline performance. Collecting row-level lineage for high-volume data streams can slow down pipelines by 50% or more. **Solution**: Use tiered collection levels and sample-based collection for high-volume streams. Configure collection levels based on data sensitivity and regulatory requirements.\n\n **Pitfall: Incomplete Schema Lineage**\nSQL parsing often misses complex transformations like UDF calls, dynamic SQL, or external API enrichments, creating gaps in lineage graphs. **Solution**: Implement hybrid collection that combines automatic SQL analysis with manual lineage annotations for complex transformations. Provide clear documentation on when manual annotations are required.\n\n **Pitfall: Lineage Storage Explosion**\nStoring complete lineage for every pipeline run can quickly consume massive storage, especially for high-frequency pipelines processing large datasets. **Solution**: Implement lineage retention policies that keep detailed lineage for recent runs but aggregate older lineage to schema-level relationships. Consider compressing lineage for archived pipeline runs.\n\n **Pitfall: Cross-System Lineage Gaps**\nLineage often breaks when data moves between different systems (ETL  data warehouse  BI tools) because each system maintains its own lineage model. **Solution**: Standardize on lineage metadata formats and implement lineage bridges that can import/export lineage information between systems. Use standard formats like OpenLineage for interoperability.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Message Broker | Redis Streams with Python redis-py | Apache Kafka with confluent-kafka-python |\n| API Framework | FastAPI with Pydantic validation | FastAPI + Celery for async processing |\n| State Storage | SQLite with SQLAlchemy ORM | PostgreSQL with asyncpg for performance |\n| Lineage Storage | JSON documents in primary database | Graph database (Neo4j) with py2neo |\n| Monitoring | Python logging + Prometheus client | OpenTelemetry with Jaeger tracing |\n\n#### Recommended File Structure\n\n```\netl_pipeline/\n  core/\n    communication/\n      __init__.py\n      message_broker.py           Message publishing and subscription\n      api_server.py              REST API endpoints\n      event_handlers.py          Async event processing\n    orchestration/\n      __init__.py\n      pipeline_executor.py       Main execution flow coordination\n      dependency_tracker.py      Task dependency resolution\n      state_manager.py           Pipeline and task state management\n    lineage/\n      __init__.py\n      lineage_collector.py       Automatic lineage capture\n      lineage_analyzer.py        Forward/backward analysis\n      sql_parser.py             SQL lineage extraction\n  examples/\n    sample_pipeline.py           Complete end-to-end example\n    lineage_queries.py          Example lineage analysis queries\n  tests/\n    integration/\n      test_pipeline_execution.py  Full pipeline execution tests\n      test_lineage_tracking.py    End-to-end lineage tests\n```\n\n#### Infrastructure Starter Code\n\n**Message Broker Implementation:**\n\n```python\n\"\"\"\nMessage broker implementation for component communication.\nProvides both synchronous API calls and asynchronous event publishing.\n\"\"\"\nimport json\nimport asyncio\nfrom typing import Dict, List, Callable, Any, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nimport redis\nimport logging\n\n@dataclass\nclass MessageEvent:\n    \"\"\"Standard message format for all inter-component communication.\"\"\"\n    event_id: str\n    timestamp: datetime\n    source_component: str\n    event_type: str\n    pipeline_id: str\n    run_id: str\n    task_id: Optional[str] = None\n    payload: Dict[str, Any] = None\n    correlation_id: Optional[str] = None\n\nclass MessageBroker:\n    \"\"\"\n    Handles all inter-component messaging for the ETL system.\n    Supports both pub/sub patterns for events and point-to-point for control messages.\n    \"\"\"\n    \n    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n        self.redis_client = redis.Redis.from_url(redis_url, decode_responses=True)\n        self.subscribers: Dict[str, List[Callable]] = {}\n        self.logger = logging.getLogger(__name__)\n        \n    async def publish_event(self, topic: str, event: MessageEvent) -> bool:\n        \"\"\"\n        Publish event to specified topic for subscriber consumption.\n        Returns True if published successfully, False otherwise.\n        \"\"\"\n        try:\n            message_data = json.dumps(asdict(event), default=str)\n            result = self.redis_client.xadd(topic, {\"event\": message_data})\n            self.logger.debug(f\"Published event {event.event_id} to topic {topic}\")\n            return result is not None\n        except Exception as e:\n            self.logger.error(f\"Failed to publish event {event.event_id}: {e}\")\n            return False\n            \n    def subscribe_to_topic(self, topic: str, handler: Callable[[MessageEvent], None]) -> None:\n        \"\"\"Register event handler for specified topic.\"\"\"\n        if topic not in self.subscribers:\n            self.subscribers[topic] = []\n        self.subscribers[topic].append(handler)\n        self.logger.info(f\"Registered handler for topic {topic}\")\n        \n    async def start_consumer(self, consumer_group: str, consumer_name: str) -> None:\n        \"\"\"Start consuming messages from subscribed topics.\"\"\"\n        while True:\n            try:\n                for topic in self.subscribers.keys():\n                    # Create consumer group if it doesn't exist\n                    try:\n                        self.redis_client.xgroup_create(topic, consumer_group, id='0', mkstream=True)\n                    except redis.ResponseError:\n                        pass  # Group already exists\n                    \n                    # Read messages from the stream\n                    messages = self.redis_client.xreadgroup(\n                        consumer_group, consumer_name, {topic: '>'}, count=10, block=1000\n                    )\n                    \n                    for stream, msgs in messages:\n                        for msg_id, fields in msgs:\n                            try:\n                                event_data = json.loads(fields['event'])\n                                event = MessageEvent(**event_data)\n                                \n                                # Call all registered handlers for this topic\n                                for handler in self.subscribers.get(topic, []):\n                                    handler(event)\n                                    \n                                # Acknowledge message processing\n                                self.redis_client.xack(topic, consumer_group, msg_id)\n                                \n                            except Exception as e:\n                                self.logger.error(f\"Failed to process message {msg_id}: {e}\")\n                                \n            except Exception as e:\n                self.logger.error(f\"Consumer error: {e}\")\n                await asyncio.sleep(1)\n```\n\n**State Management Implementation:**\n\n```python\n\"\"\"\nCentralized state management for pipeline and task execution tracking.\nHandles state transitions, persistence, and consistency across components.\n\"\"\"\nfrom typing import Dict, List, Optional, Set\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom enum import Enum\nimport threading\nimport sqlite3\nimport json\nimport logging\n\nclass TaskState(Enum):\n    PENDING = \"PENDING\"\n    WAITING = \"WAITING\" \n    QUEUED = \"QUEUED\"\n    RUNNING = \"RUNNING\"\n    SUCCESS = \"SUCCESS\"\n    FAILED = \"FAILED\"\n    RETRYING = \"RETRYING\"\n    CANCELLED = \"CANCELLED\"\n    SKIPPED = \"SKIPPED\"\n\nclass TaskEvent(Enum):\n    DEPENDENCIES_MET = \"DEPENDENCIES_MET\"\n    EXECUTION_STARTED = \"EXECUTION_STARTED\"\n    EXECUTION_COMPLETED = \"EXECUTION_COMPLETED\"\n    EXECUTION_FAILED = \"EXECUTION_FAILED\"\n    RETRY_SCHEDULED = \"RETRY_SCHEDULED\"\n    MAX_RETRIES_EXCEEDED = \"MAX_RETRIES_EXCEEDED\"\n    CANCELLED_BY_USER = \"CANCELLED_BY_USER\"\n    UPSTREAM_FAILED = \"UPSTREAM_FAILED\"\n\n# State transition mapping - defines valid state changes\nTRANSITIONS = {\n    TaskState.PENDING: {\n        TaskEvent.DEPENDENCIES_MET: TaskState.WAITING,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n        TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED\n    },\n    TaskState.WAITING: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED,\n        TaskEvent.UPSTREAM_FAILED: TaskState.SKIPPED\n    },\n    TaskState.RUNNING: {\n        TaskEvent.EXECUTION_COMPLETED: TaskState.SUCCESS,\n        TaskEvent.EXECUTION_FAILED: TaskState.RETRYING,\n        TaskEvent.MAX_RETRIES_EXCEEDED: TaskState.FAILED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED\n    },\n    TaskState.RETRYING: {\n        TaskEvent.EXECUTION_STARTED: TaskState.RUNNING,\n        TaskEvent.MAX_RETRIES_EXCEEDED: TaskState.FAILED,\n        TaskEvent.CANCELLED_BY_USER: TaskState.CANCELLED\n    }\n}\n\n@dataclass \nclass TaskExecution:\n    \"\"\"Represents a single task execution within a pipeline run.\"\"\"\n    task_id: str\n    pipeline_run_id: str\n    state: TaskState\n    attempt_count: int\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    logs: List[str] = None\n    metrics: Dict[str, float] = None\n\nclass StateManager:\n    \"\"\"\n    Thread-safe state management for pipeline and task executions.\n    Provides atomic state transitions and persistent storage.\n    \"\"\"\n    \n    def __init__(self, db_path: str = \"etl_state.db\"):\n        self.db_path = db_path\n        self.lock = threading.RLock()\n        self.logger = logging.getLogger(__name__)\n        self._init_database()\n        \n    def _init_database(self):\n        \"\"\"Initialize SQLite database with required tables.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS task_executions (\n                task_id TEXT,\n                pipeline_run_id TEXT,\n                state TEXT,\n                attempt_count INTEGER,\n                started_at TEXT,\n                completed_at TEXT,\n                error_message TEXT,\n                logs TEXT,\n                metrics TEXT,\n                PRIMARY KEY (task_id, pipeline_run_id)\n            )\n        \"\"\")\n        \n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS state_transitions (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                task_id TEXT,\n                pipeline_run_id TEXT,\n                from_state TEXT,\n                to_state TEXT,\n                event TEXT,\n                timestamp TEXT,\n                error_message TEXT\n            )\n        \"\"\")\n        \n        conn.commit()\n        conn.close()\n```\n\n#### Core Logic Skeleton\n\n**Pipeline Execution Coordinator:**\n\n```python\n\"\"\"\nMain pipeline execution coordination logic.\nStudents implement the core orchestration algorithms here.\n\"\"\"\nfrom typing import Dict, List, Set, Optional\nfrom datetime import datetime\nimport asyncio\n\nclass PipelineExecutor:\n    \"\"\"\n    Coordinates execution of complete pipeline runs including dependency resolution,\n    task scheduling, and failure handling.\n    \"\"\"\n    \n    def __init__(self, message_broker, state_manager, dag_engine):\n        self.message_broker = message_broker\n        self.state_manager = state_manager  \n        self.dag_engine = dag_engine\n        \n    async def execute_pipeline(self, pipeline_id: str, run_id: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"\n        Execute complete pipeline run with dependency management and monitoring.\n        Returns True if pipeline completed successfully, False otherwise.\n        \"\"\"\n        # TODO 1: Retrieve pipeline definition using get_pipeline(pipeline_id)\n        # TODO 2: Validate pipeline definition using validate_pipeline(pipeline)\n        # TODO 3: Create execution plan using create_execution_plan(pipeline, task_durations)\n        # TODO 4: Initialize all task executions in PENDING state\n        # TODO 5: Start dependency resolution loop for task scheduling\n        # TODO 6: Monitor task completion events and update dependency tracking\n        # TODO 7: Handle task failures according to pipeline failure policy\n        # TODO 8: Determine pipeline completion status when all tasks finish\n        # TODO 9: Publish pipeline completion event with final metrics\n        # TODO 10: Perform cleanup of temporary resources and state\n        \n    def _schedule_eligible_tasks(self, run_id: str, execution_plan) -> List[str]:\n        \"\"\"\n        Schedule tasks that have all dependencies satisfied.\n        Returns list of scheduled task IDs.\n        \"\"\"\n        # TODO 1: Query current task states for this pipeline run\n        # TODO 2: Find tasks in WAITING state (dependencies met but not yet scheduled)\n        # TODO 3: Check resource availability for each eligible task\n        # TODO 4: Assign tasks to available executors using load balancing\n        # TODO 5: Update task states to QUEUED and publish scheduling events\n        # TODO 6: Return list of successfully scheduled task IDs\n        \n    def _update_dependencies(self, completed_task_id: str, run_id: str) -> Set[str]:\n        \"\"\"\n        Update dependency tracking when a task completes.\n        Returns set of task IDs that became eligible for execution.\n        \"\"\"\n        # TODO 1: Retrieve execution plan dependency matrix for this run\n        # TODO 2: Find all tasks that depend on the completed task\n        # TODO 3: Check if all dependencies are now satisfied for each dependent task\n        # TODO 4: Transition eligible tasks from PENDING to WAITING state\n        # TODO 5: Return set of task IDs that became eligible\n        \n    def _handle_task_failure(self, failed_task_id: str, run_id: str, error_message: str) -> bool:\n        \"\"\"\n        Handle task failure according to pipeline failure policy.\n        Returns True if pipeline should continue, False if it should abort.\n        \"\"\"\n        # TODO 1: Check task retry policy and current attempt count\n        # TODO 2: If retries available, schedule retry with exponential backoff\n        # TODO 3: If no retries left, determine impact on downstream tasks\n        # TODO 4: Apply pipeline failure policy (fail-fast, continue-on-error, etc.)\n        # TODO 5: Cancel or skip dependent tasks based on failure policy\n        # TODO 6: Return whether pipeline execution should continue\n```\n\n**Lineage Collection Implementation:**\n\n```python\n\"\"\"\nAutomatic lineage collection during pipeline execution.\nStudents implement lineage capture and analysis logic here.\n\"\"\"\n\nclass LineageCollector:\n    \"\"\"\n    Captures data lineage information during pipeline execution.\n    Supports both automatic collection and manual annotation.\n    \"\"\"\n    \n    def __init__(self, storage_backend):\n        self.storage = storage_backend\n        \n    def collect_sql_lineage(self, sql_query: str, input_tables: List[str], output_table: str, \n                           pipeline_run_id: str, task_id: str) -> bool:\n        \"\"\"\n        Extract lineage from SQL transformations using query analysis.\n        Returns True if lineage was successfully captured.\n        \"\"\"\n        # TODO 1: Parse SQL query using SQL AST parser to extract structure\n        # TODO 2: Identify all referenced tables and columns in FROM/JOIN clauses\n        # TODO 3: Extract column expressions from SELECT clause with transformations\n        # TODO 4: Map input columns to output columns through SELECT expressions\n        # TODO 5: Create AssetReference objects for input and output tables\n        # TODO 6: Build ColumnLineage mappings for each transformed column\n        # TODO 7: Store complete lineage record with transformation logic\n        # TODO 8: Publish lineage event for downstream consumers\n        \n    def collect_udf_lineage(self, function_name: str, input_data_schema: dict, \n                           output_data_schema: dict, pipeline_run_id: str, task_id: str) -> bool:\n        \"\"\"\n        Capture lineage for Python UDF transformations through runtime inspection.\n        Returns True if lineage was successfully captured.\n        \"\"\"\n        # TODO 1: Retrieve UDF metadata and manual lineage annotations\n        # TODO 2: Compare input and output schemas to infer column mappings\n        # TODO 3: Sample input and output data to detect transformation patterns\n        # TODO 4: Extract business logic annotations from function docstrings\n        # TODO 5: Build lineage record with available transformation information\n        # TODO 6: Mark uncertain lineage relationships for manual review\n        # TODO 7: Store lineage record and publish collection event\n        \n    def get_data_lineage(self, asset_name: str, direction: str = \"both\", max_depth: int = 10) -> dict:\n        \"\"\"\n        Query lineage relationships for specified data asset.\n        Direction can be 'upstream', 'downstream', or 'both'.\n        Returns lineage graph with transformation details.\n        \"\"\"\n        # TODO 1: Validate asset name and query parameters\n        # TODO 2: Build graph traversal query based on direction and depth\n        # TODO 3: Execute lineage query against storage backend\n        # TODO 4: Construct lineage graph with nodes (assets) and edges (transformations)\n        # TODO 5: Include transformation logic and schema evolution details\n        # TODO 6: Apply access controls to filter visible lineage information\n        # TODO 7: Return structured lineage graph for visualization/analysis\n```\n\n#### Milestone Checkpoints\n\n**Checkpoint 1 - Component Communication (End of Milestone 1):**\n- Run: `python -m pytest tests/integration/test_messaging.py -v`\n- Expected: All message publishing and subscription tests pass\n- Manual verification: Start message broker, publish test event, verify subscriber receives it\n- Signs of issues: Message delivery failures, serialization errors, subscription timeouts\n\n**Checkpoint 2 - Pipeline Execution Flow (End of Milestone 2):**\n- Run: `python scripts/test_pipeline_execution.py sample_pipeline.yaml`\n- Expected: Complete pipeline execution with task dependency resolution\n- Manual verification: Monitor task state transitions through dashboard/logs\n- Signs of issues: Dependency deadlocks, resource allocation failures, state inconsistencies\n\n**Checkpoint 3 - Lineage Tracking (End of Milestone 3):**\n- Run: `python scripts/test_lineage_collection.py`\n- Expected: Automatic lineage capture for SQL and Python transformations\n- Manual verification: Query lineage API to verify forward/backward relationships\n- Signs of issues: Missing lineage records, incorrect column mappings, SQL parsing failures\n\n**Checkpoint 4 - Complete Integration (End of Milestone 4):**\n- Run: `python scripts/run_full_pipeline_test.py`\n- Expected: End-to-end pipeline with monitoring, alerting, and lineage\n- Manual verification: Complete pipeline run with full observability\n- Signs of issues: Missing metrics, alert failures, incomplete lineage graphs\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones - comprehensive error handling affects pipeline definition validation (Milestone 1), data processing resilience (Milestones 2-3), and orchestration reliability (Milestone 4). This section establishes the production-grade error handling patterns required across all system components.\n\n### Mental Model: Hospital Emergency Response System\n\nThink of our error handling system like a hospital's emergency response infrastructure. When patients arrive, they don't just randomly hope for the best - there's a systematic approach to handling different types of medical emergencies. **Triage** classifies problems by severity (minor cut vs. heart attack), **treatment protocols** define standardized responses to common conditions (broken bone  X-ray  cast  recovery), and **escalation procedures** know when to call specialists or transfer to higher-level care facilities.\n\nSimilarly, our ETL system must systematically classify failures, apply appropriate treatment protocols (retry strategies), and escalate to human operators when automated recovery fails. Just as hospitals have backup generators and redundant life support systems, our pipeline infrastructure requires multiple layers of fault tolerance to ensure critical data flows never completely fail.\n\nThe key insight is that **not all failures are created equal**. A temporary network blip should trigger automatic retry with exponential backoff, while data corruption requires immediate human attention and potentially rolling back processed data. Like medical triage, proper classification at the moment of failure determines whether the patient (pipeline) recovers quickly or suffers permanent damage.\n\n### Common Failure Modes\n\nProduction ETL systems face numerous failure scenarios that can disrupt data processing, corrupt results, or leave pipelines in inconsistent states. Understanding these failure modes enables us to design appropriate detection, classification, and recovery mechanisms.\n\n#### Network and Connectivity Failures\n\nNetwork-related failures represent the most common category of transient errors in distributed ETL systems. These failures manifest as connection timeouts, DNS resolution failures, SSL handshake errors, and intermittent packet loss that can cause partial data corruption during transmission.\n\n**Connection timeouts** occur when source systems become temporarily overloaded or network congestion prevents timely responses. Database connections may time out during long-running extraction queries, while API endpoints may become unresponsive during peak usage periods. The challenge lies in distinguishing between temporary overload (retry appropriate) and permanent service degradation (escalation required).\n\n**DNS resolution failures** can prevent pipeline tasks from reaching their target systems entirely. These failures often indicate infrastructure-level problems that affect multiple pipelines simultaneously. Unlike connection timeouts, DNS failures typically require immediate escalation since they suggest broader network infrastructure issues.\n\n**SSL certificate issues** create authentication failures that prevent secure connections to external systems. Certificate expiration, hostname mismatches, and certificate authority changes can break previously working connections. These failures require immediate attention since they often indicate security-related configuration drift.\n\n**Partial network failures** represent the most insidious category, where connections succeed initially but encounter intermittent packet loss or bandwidth restrictions. Large data transfers may partially complete before failing, leaving destination systems in inconsistent states that require careful cleanup.\n\n| Failure Type | Symptoms | Detection Method | Recovery Strategy |\n|--------------|----------|------------------|------------------|\n| Connection Timeout | HTTP 408, socket timeout exceptions | Connection monitoring, response time alerts | Exponential backoff retry, connection pool cycling |\n| DNS Resolution Failure | Name resolution errors, DNS lookup timeouts | DNS health checks, resolution time monitoring | Immediate escalation, fallback DNS servers |\n| SSL Certificate Issues | Certificate validation errors, handshake failures | Certificate expiration monitoring, handshake alerts | Immediate escalation, certificate renewal workflow |\n| Partial Network Failure | Incomplete transfers, checksum mismatches | Transfer verification, data integrity checks | Resume from checkpoint, full transfer retry |\n\n#### Data Quality and Corruption Issues\n\nData quality problems represent critical failures that can propagate incorrect information throughout downstream systems. Unlike network failures, data corruption issues require immediate human intervention and potentially complex rollback procedures.\n\n**Schema evolution conflicts** occur when source systems change their data structures without coordinating with ETL pipelines. New required fields, removed columns, or data type changes can cause extraction or transformation tasks to fail catastrophically. The fundamental challenge is distinguishing between temporary schema access issues and permanent structural changes.\n\n**Data format corruption** manifests when source systems produce malformed records that don't conform to expected schemas. JSON parsing errors, CSV files with inconsistent column counts, and binary data corruption represent common examples. These failures require sophisticated validation to prevent corrupt records from contaminating destination systems.\n\n**Referential integrity violations** occur when extracted data references entities that don't exist in destination systems. Foreign key constraint violations, orphaned records, and circular references can prevent bulk loading operations from completing successfully.\n\n**Data volume anomalies** indicate potential upstream system failures or data generation issues. Sudden spikes or drops in record counts, unusually large field values, or completely empty extractions suggest problems that require immediate investigation.\n\n| Data Issue Type | Detection Method | Impact Assessment | Recovery Action |\n|------------------|------------------|-------------------|-----------------|\n| Schema Evolution | Field validation, type checking failures | High - affects all downstream consumers | Stop pipeline, schema compatibility analysis |\n| Format Corruption | Parse errors, validation failures | Medium - affects individual records | Quarantine invalid records, continue processing |\n| Referential Integrity | Foreign key violations, constraint errors | High - data consistency compromised | Rollback transaction, dependency resolution |\n| Volume Anomalies | Record count monitoring, statistical analysis | Variable - depends on business context | Alert operators, apply business rule validation |\n\n#### Resource Exhaustion and Performance Degradation\n\nResource exhaustion represents a class of failures where system resources become insufficient to support normal pipeline operations. These failures often develop gradually and can affect multiple pipelines simultaneously.\n\n**Memory exhaustion** occurs when transformation operations attempt to load datasets larger than available RAM. Aggregation operations, large joins, and bulk data processing can trigger out-of-memory errors that terminate pipeline tasks abruptly. The challenge lies in detecting memory pressure early enough to implement mitigation strategies.\n\n**Disk space exhaustion** affects both temporary processing storage and permanent result storage. Staging tables, intermediate transformation results, and log files can consume available disk space, preventing pipeline completion. These failures require both immediate cleanup and longer-term capacity planning.\n\n**Connection pool exhaustion** happens when concurrent pipeline tasks exceed the maximum number of available database connections. This creates resource starvation where new tasks cannot acquire necessary connections to proceed.\n\n**CPU and I/O bottlenecks** manifest as severely degraded performance that can cause pipeline tasks to exceed their timeout limits. While not technically failures, extreme performance degradation often leads to timeout-based task cancellation.\n\n> **Critical Insight**: Resource exhaustion failures often cascade across multiple pipelines. A single memory-intensive transformation can consume available RAM, causing unrelated pipelines to fail due to resource starvation. Detection and mitigation must consider system-wide resource allocation, not just individual pipeline requirements.\n\n| Resource Type | Exhaustion Symptoms | Monitoring Approach | Mitigation Strategy |\n|---------------|-------------------|---------------------|-------------------|\n| Memory | OutOfMemoryError, swap thrashing | Memory usage monitoring, GC pressure alerts | Streaming processing, data partitioning |\n| Disk Space | Write failures, temp file creation errors | Disk usage monitoring, growth rate analysis | Cleanup temporary files, partition pruning |\n| Connection Pool | Connection acquisition timeouts | Active connection monitoring, pool saturation alerts | Connection pool sizing, connection recycling |\n| CPU/I/O | Extreme response times, timeout failures | Performance monitoring, resource utilization tracking | Task parallelization limits, resource quotas |\n\n### Retry and Backoff Strategies\n\nEffective retry strategies distinguish between transient failures that will likely resolve themselves and permanent failures that require human intervention. The goal is to achieve maximum reliability without overwhelming already-stressed systems with aggressive retry attempts.\n\n#### Exponential Backoff Implementation\n\n**Exponential backoff** provides a mathematically sound approach to spacing retry attempts that reduces load on failing systems while maintaining reasonable recovery times for transient issues. The strategy involves doubling the delay between successive retry attempts, with optional jitter to prevent thundering herd effects.\n\nThe basic exponential backoff formula calculates delay as `initial_delay * (2^attempt_count)`, with maximum delay caps to prevent indefinitely long wait times. However, naive implementation can create synchronized retry storms when multiple pipeline tasks fail simultaneously and begin retrying on identical schedules.\n\n**Jitter injection** addresses synchronized retry problems by introducing randomness to retry timing. Full jitter randomizes the entire delay window, while decorrelated jitter uses the previous delay as input to random number generation. The choice between jitter strategies depends on the expected correlation between failure events and system recovery characteristics.\n\n**Retry budget management** prevents retry attempts from consuming excessive execution time relative to useful work. Each task maintains a retry budget that limits total retry time or attempt counts, ensuring that persistently failing tasks don't block pipeline completion indefinitely.\n\n> **Decision: Exponential Backoff with Decorrelated Jitter**\n> - **Context**: Multiple pipeline tasks often fail simultaneously due to shared dependencies (database, network), creating retry storms that can overwhelm recovering systems\n> - **Options Considered**: Fixed delay, exponential backoff, exponential backoff with full jitter, decorrelated jitter\n> - **Decision**: Exponential backoff with decorrelated jitter and maximum delay caps\n> - **Rationale**: Decorrelated jitter spreads retry attempts across time while maintaining reasonable mathematical properties. Maximum delay caps prevent indefinitely long waits that could block pipeline completion.\n> - **Consequences**: Eliminates retry storms and reduces system load during recovery, at the cost of slightly more complex retry timing calculation\n\n| Retry Strategy | Formula | Pros | Cons |\n|----------------|---------|------|------|\n| Fixed Delay | `delay = constant` | Simple, predictable timing | Can create retry storms, ignores system recovery |\n| Exponential Backoff | `delay = initial * (2^attempt)` | Reduces load on failing systems | Can create synchronized retry storms |\n| Full Jitter | `delay = random(0, exponential_delay)` | Eliminates synchronization | Can retry too quickly or too slowly |\n| Decorrelated Jitter | `delay = random(initial, previous_delay * 3)` | Balances timing with load spreading | More complex calculation |\n\nThe `RetryPolicy` configuration allows per-task customization of retry behavior based on the expected failure characteristics of different operation types:\n\n| Field | Type | Description | Default Value |\n|-------|------|-------------|---------------|\n| max_attempts | int | Maximum number of retry attempts before giving up | 3 |\n| backoff_seconds | int | Initial delay in seconds before first retry | 1 |\n| exponential_backoff | bool | Whether to use exponential backoff or fixed delay | true |\n| retry_on_error_types | List[str] | List of error types that should trigger retries | [\"NetworkError\", \"TimeoutError\"] |\n\n#### Circuit Breaker Pattern\n\n**Circuit breakers** protect downstream systems from cascading failures by temporarily suspending requests to systems that are experiencing problems. Like electrical circuit breakers, they \"trip\" when failure rates exceed acceptable thresholds and automatically \"reset\" when systems recover.\n\nThe circuit breaker maintains three states: **Closed** (normal operation), **Open** (failures detected, requests blocked), and **Half-Open** (testing recovery). State transitions depend on failure rate monitoring and recovery detection logic.\n\n**Failure rate calculation** requires sliding window analysis to distinguish between temporary failure spikes and sustained system degradation. Count-based windows track failures over a fixed number of recent requests, while time-based windows monitor failure rates over rolling time periods.\n\n**Recovery detection** in the Half-Open state allows a limited number of test requests to determine if the downstream system has recovered. Successful test requests transition the circuit back to Closed state, while continued failures return to Open state with potentially extended timeout periods.\n\n| Circuit State | Request Handling | Transition Trigger | Typical Duration |\n|---------------|------------------|-------------------|------------------|\n| Closed | Forward all requests | Failure rate exceeds threshold | N/A - normal operation |\n| Open | Reject requests immediately | Timeout period expires | 60-300 seconds |\n| Half-Open | Allow limited test requests | Successful/failed test requests | 5-30 seconds |\n\n#### Dead Letter Queue Management\n\n**Dead letter queues** provide a mechanism for handling messages or tasks that cannot be processed successfully after exhausting all retry attempts. Rather than losing failed operations entirely, dead letter queues preserve them for later analysis and potential reprocessing.\n\n**Message preservation** ensures that failed operations retain all necessary context for debugging and recovery. This includes original task parameters, error messages from all retry attempts, execution timing information, and environmental context like pipeline run identifiers.\n\n**Retry exhaustion criteria** determine when tasks should be moved to dead letter queues. Simple count-based criteria move tasks after a fixed number of attempts, while time-based criteria consider total retry duration. Sophisticated approaches consider error type classification, with permanent errors moving immediately to dead letter queues.\n\n**Dead letter processing** enables batch reprocessing of failed operations after resolving underlying issues. Manual reprocessing allows operators to modify task parameters or execution context, while automatic reprocessing can retry operations when system health metrics indicate recovery.\n\n> **Architecture Decision: Separate Dead Letter Queues by Failure Type**\n> - **Context**: Different failure types require different reprocessing approaches and have different urgency levels for human intervention\n> - **Options Considered**: Single dead letter queue, separate queues by pipeline, separate queues by failure type\n> - **Decision**: Separate dead letter queues categorized by failure type (transient, data quality, system error)\n> - **Rationale**: Enables targeted reprocessing strategies and appropriate alert prioritization. Data quality issues need immediate attention while transient failures can be batch processed.\n> - **Consequences**: Requires failure classification logic and multiple queue management, but provides better operational control and reduced alert noise.\n\n| Dead Letter Queue Type | Failure Categories | Reprocessing Strategy | Alert Priority |\n|------------------------|-------------------|----------------------|----------------|\n| Transient Failures | Network timeouts, temporary resource exhaustion | Automatic batch retry during off-peak hours | Low |\n| Data Quality Issues | Schema validation, referential integrity | Manual review and correction required | High |\n| System Errors | Configuration issues, permission problems | Requires system administrator intervention | Medium |\n| Permanent Failures | Invalid configurations, missing resources | Manual task modification or removal | Low |\n\n### Partial Failure Recovery\n\nPartial failures represent some of the most challenging scenarios in distributed ETL systems. Unlike complete failures where the entire operation fails cleanly, partial failures leave systems in intermediate states that require careful analysis and recovery procedures.\n\n#### Checkpointing and State Persistence\n\n**Checkpointing** enables pipelines to resume processing from intermediate points rather than restarting from the beginning after failures. Effective checkpointing requires identifying appropriate checkpoint boundaries, persisting sufficient state information, and implementing recovery logic that can resume from any checkpoint.\n\n**Checkpoint boundary selection** depends on the natural granularity of data processing operations. Bulk loading operations might checkpoint after each batch of records, while transformation operations might checkpoint after processing each input partition. The goal is to balance checkpoint frequency (more frequent checkpoints reduce rework) against checkpoint overhead (state persistence costs).\n\n**State persistence requirements** extend beyond simple progress tracking to include all information necessary to resume processing. This includes input data positions (file offsets, database cursors), intermediate calculation results, configuration parameters, and dependency state. The challenge lies in ensuring checkpoint state remains consistent even if the checkpointing process itself is interrupted.\n\n**Recovery logic complexity** grows significantly with checkpoint granularity. Coarse-grained checkpoints require simpler recovery logic but result in more rework after failures. Fine-grained checkpoints minimize rework but require sophisticated logic to handle partially completed operations and state inconsistencies.\n\n> **Critical Design Principle**: Checkpoints must be **atomic** and **consistent**. A checkpoint is only valid if all related state can be persisted atomically. Partial checkpoints can leave the system in an unrecoverable state that's worse than no checkpoint at all.\n\nThe `TaskExecution` model supports checkpointing through the metrics field, which can store arbitrary checkpoint data:\n\n| Checkpoint Information | Storage Location | Recovery Usage |\n|----------------------|------------------|----------------|\n| Input data position | metrics[\"input_position\"] | Resume reading from exact location |\n| Processed record count | metrics[\"records_processed\"] | Validate recovery completeness |\n| Intermediate results | metrics[\"intermediate_state\"] | Avoid recalculating expensive operations |\n| Error recovery state | metrics[\"error_context\"] | Understand failure context for recovery |\n\n#### Transaction Management and Rollback\n\n**Transaction management** ensures that data operations either complete entirely or leave no traces in destination systems. ETL operations often involve multiple systems (source extraction, staging tables, destination loading) that must be coordinated to maintain consistency.\n\n**Two-phase commit protocols** can coordinate transactions across multiple systems, but introduce significant complexity and performance overhead. The first phase involves sending prepare messages to all participating systems, while the second phase sends commit or abort decisions based on unanimous agreement.\n\n**Compensation-based transactions** provide an alternative approach where each operation includes a corresponding compensation action that undoes its effects. Rather than preventing inconsistent states, compensation transactions restore consistency after detecting failures.\n\n**Saga pattern implementation** breaks long-running transactions into sequences of smaller transactions, each with its own compensation action. If any step fails, the saga executes compensation actions for all completed steps in reverse order.\n\n| Transaction Approach | Coordination Method | Consistency Guarantee | Performance Impact |\n|---------------------|-------------------|---------------------|-------------------|\n| Two-Phase Commit | Distributed consensus | Strong consistency | High latency, blocking |\n| Compensation Transactions | Reverse operation execution | Eventual consistency | Lower latency, non-blocking |\n| Saga Pattern | Sequential compensation | Eventual consistency | Moderate latency, complex logic |\n\n#### Data Cleanup and Consistency Repair\n\n**Data cleanup operations** remove incomplete or corrupted data that results from partial failures. The challenge lies in identifying which data was affected by failures and determining safe cleanup boundaries that don't accidentally remove valid data.\n\n**Staging table cleanup** involves removing partially loaded data from staging areas after extraction or transformation failures. Simple approaches truncate entire staging tables, while sophisticated approaches identify and remove only records from failed operations.\n\n**Referential integrity repair** addresses situations where partial failures create orphaned records or broken foreign key relationships. Repair operations might involve removing orphaned records, restoring missing parent records, or updating references to point to valid entities.\n\n**Idempotent operation design** enables safe retry of cleanup operations without risking data corruption. Cleanup operations should produce identical results regardless of how many times they're executed, even if the system state changes between executions.\n\n **Pitfall: Cleanup Operations Without Transaction Boundaries**\n\nMany developers implement cleanup operations as simple DELETE statements without considering transaction boundaries. If the cleanup operation itself fails partway through, it can leave the system in a state that's more inconsistent than before cleanup began.\n\n**Why it's wrong**: Partial cleanup can remove some corrupted data while leaving other corrupted data in place, making it harder to identify the scope of data quality issues.\n\n**How to fix**: Wrap cleanup operations in transactions and implement cleanup checkpointing for operations that affect large data volumes. Consider using staging tables for cleanup operations so that incomplete cleanup doesn't affect production data.\n\n| Cleanup Operation Type | Scope Identification | Safety Mechanism | Recovery Approach |\n|----------------------|-------------------|------------------|-------------------|\n| Staging Table Cleanup | Pipeline run ID, timestamp ranges | Transaction boundaries | Truncate and reload |\n| Partial Load Cleanup | Batch identifiers, watermark ranges | Backup before cleanup | Restore from backup |\n| Referential Integrity Repair | Foreign key violation detection | Constraint validation | Cascade repair operations |\n| Orphaned Record Cleanup | Parent-child relationship analysis | Soft delete before hard delete | Restoration from soft delete |\n\n### Implementation Guidance\n\nUnderstanding error handling patterns requires implementing robust failure detection, classification, and recovery mechanisms. The following guidance provides concrete implementation approaches for production-grade error handling.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Retry Logic | Simple exponential backoff with random jitter | Redis-backed circuit breaker with sliding window |\n| Dead Letter Queue | Database table with failed task records | Message broker (RabbitMQ/Apache Kafka) with topic-based routing |\n| Circuit Breaker | In-memory failure tracking with timeout-based reset | Distributed circuit breaker with shared state |\n| Checkpointing | JSON files with task state snapshots | Database transactions with write-ahead logging |\n| Monitoring | Structured logging with failure categorization | Metrics collection (Prometheus) with alerting rules |\n\n#### Recommended File Structure\n\n```\netl-system/\n  core/\n    error_handling/\n      __init__.py\n      retry_policy.py            RetryPolicy and exponential backoff\n      circuit_breaker.py         Circuit breaker implementation\n      dead_letter_queue.py       Dead letter queue management\n      checkpoint_manager.py      Checkpointing and state persistence\n      error_classifier.py        Failure type classification\n      recovery_engine.py         Automated recovery procedures\n  tests/\n    error_handling/\n      test_retry_policy.py\n      test_circuit_breaker.py\n      test_checkpoint_manager.py\n  monitoring/\n    error_metrics.py            Error metrics collection\n    alert_manager.py            Alert generation and suppression\n```\n\n#### Infrastructure Starter Code\n\n**RetryPolicy with Exponential Backoff:**\n\n```python\nimport random\nimport time\nfrom typing import List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ErrorType(Enum):\n    NETWORK_ERROR = \"NetworkError\"\n    TIMEOUT_ERROR = \"TimeoutError\"\n    DATA_QUALITY_ERROR = \"DataQualityError\"\n    RESOURCE_EXHAUSTION = \"ResourceExhaustion\"\n    PERMISSION_ERROR = \"PermissionError\"\n\n@dataclass\nclass RetryPolicy:\n    max_attempts: int = 3\n    backoff_seconds: int = 1\n    exponential_backoff: bool = True\n    retry_on_error_types: List[str] = None\n    max_delay_seconds: int = 300\n    jitter_type: str = \"decorrelated\"\n    \n    def __post_init__(self):\n        if self.retry_on_error_types is None:\n            self.retry_on_error_types = [\n                ErrorType.NETWORK_ERROR.value,\n                ErrorType.TIMEOUT_ERROR.value,\n                ErrorType.RESOURCE_EXHAUSTION.value\n            ]\n\nclass RetryManager:\n    def __init__(self, retry_policy: RetryPolicy):\n        self.policy = retry_policy\n        \n    def should_retry(self, error_type: str, attempt_count: int) -> bool:\n        \"\"\"Determine if operation should be retried based on policy and error type.\"\"\"\n        if attempt_count >= self.policy.max_attempts:\n            return False\n        return error_type in self.policy.retry_on_error_types\n    \n    def calculate_delay(self, attempt_count: int, previous_delay: Optional[int] = None) -> int:\n        \"\"\"Calculate delay before next retry attempt.\"\"\"\n        if not self.policy.exponential_backoff:\n            return self.policy.backoff_seconds\n            \n        if self.policy.jitter_type == \"decorrelated\" and previous_delay:\n            # Decorrelated jitter: random(initial_delay, previous_delay * 3)\n            min_delay = self.policy.backoff_seconds\n            max_delay = min(previous_delay * 3, self.policy.max_delay_seconds)\n            return random.randint(min_delay, max_delay)\n        else:\n            # Standard exponential backoff with full jitter\n            base_delay = self.policy.backoff_seconds * (2 ** attempt_count)\n            max_delay = min(base_delay, self.policy.max_delay_seconds)\n            return random.randint(0, max_delay)\n    \n    def execute_with_retry(self, operation, error_classifier, *args, **kwargs):\n        \"\"\"Execute operation with retry logic based on policy.\"\"\"\n        last_exception = None\n        previous_delay = None\n        \n        for attempt in range(self.policy.max_attempts):\n            try:\n                return operation(*args, **kwargs)\n            except Exception as e:\n                error_type = error_classifier.classify_error(e)\n                last_exception = e\n                \n                if not self.should_retry(error_type, attempt + 1):\n                    break\n                \n                delay = self.calculate_delay(attempt, previous_delay)\n                previous_delay = delay\n                time.sleep(delay)\n        \n        raise last_exception\n```\n\n**Circuit Breaker Implementation:**\n\n```python\nimport time\nimport threading\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Callable, Any\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"\n    OPEN = \"open\"  \n    HALF_OPEN = \"half_open\"\n\n@dataclass\nclass CircuitBreakerConfig:\n    failure_threshold: int = 5\n    success_threshold: int = 3\n    timeout_seconds: int = 60\n    window_size: int = 100\n\nclass CircuitBreaker:\n    def __init__(self, config: CircuitBreakerConfig):\n        self.config = config\n        self.state = CircuitState.CLOSED\n        self.failure_count = 0\n        self.success_count = 0\n        self.last_failure_time = 0\n        self.request_count = 0\n        self.lock = threading.Lock()\n    \n    def call(self, operation: Callable, *args, **kwargs) -> Any:\n        \"\"\"Execute operation through circuit breaker.\"\"\"\n        with self.lock:\n            if self.state == CircuitState.OPEN:\n                if time.time() - self.last_failure_time < self.config.timeout_seconds:\n                    raise CircuitBreakerOpenError(\"Circuit breaker is open\")\n                else:\n                    self.state = CircuitState.HALF_OPEN\n                    self.success_count = 0\n        \n        try:\n            result = operation(*args, **kwargs)\n            self._record_success()\n            return result\n        except Exception as e:\n            self._record_failure()\n            raise\n    \n    def _record_success(self):\n        \"\"\"Record successful operation and potentially close circuit.\"\"\"\n        with self.lock:\n            if self.state == CircuitState.HALF_OPEN:\n                self.success_count += 1\n                if self.success_count >= self.config.success_threshold:\n                    self.state = CircuitState.CLOSED\n                    self.failure_count = 0\n            elif self.state == CircuitState.CLOSED:\n                self.failure_count = max(0, self.failure_count - 1)\n    \n    def _record_failure(self):\n        \"\"\"Record failed operation and potentially open circuit.\"\"\"\n        with self.lock:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            if self.failure_count >= self.config.failure_threshold:\n                self.state = CircuitState.OPEN\n            elif self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.OPEN\n\nclass CircuitBreakerOpenError(Exception):\n    \"\"\"Raised when circuit breaker is open and blocking requests.\"\"\"\n    pass\n```\n\n#### Core Logic Skeleton Code\n\n**Error Classification System:**\n\n```python\nfrom typing import Dict, Type, List\nfrom enum import Enum\nimport re\n\nclass ErrorClassifier:\n    def __init__(self):\n        self.classification_rules = self._build_classification_rules()\n    \n    def classify_error(self, exception: Exception) -> str:\n        \"\"\"Classify exception into error type category for retry decision.\"\"\"\n        # TODO 1: Check exception type against known error type mappings\n        # TODO 2: Examine exception message for pattern-based classification\n        # TODO 3: Consider exception context (network, database, file system)\n        # TODO 4: Return appropriate ErrorType enum value\n        # TODO 5: Default to non-retryable error for unknown exception types\n        pass\n    \n    def is_retryable(self, error_type: str) -> bool:\n        \"\"\"Determine if error type should trigger retry attempts.\"\"\"\n        # TODO 1: Check error_type against list of retryable categories\n        # TODO 2: Consider system state (circuit breaker status, resource availability)\n        # TODO 3: Return boolean indicating retry appropriateness\n        pass\n    \n    def _build_classification_rules(self) -> Dict[str, List[str]]:\n        \"\"\"Build mapping of error patterns to classification categories.\"\"\"\n        # TODO 1: Define regex patterns for network-related errors\n        # TODO 2: Define patterns for database connection and timeout errors\n        # TODO 3: Define patterns for resource exhaustion (memory, disk, connections)\n        # TODO 4: Define patterns for data quality and validation errors\n        # TODO 5: Return comprehensive pattern mapping dictionary\n        pass\n```\n\n**Checkpoint Manager:**\n\n```python\nfrom typing import Dict, Any, Optional\nimport json\nimport os\nfrom datetime import datetime\n\nclass CheckpointManager:\n    def __init__(self, checkpoint_dir: str):\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    def save_checkpoint(self, task_id: str, pipeline_run_id: str, \n                       checkpoint_data: Dict[str, Any]) -> bool:\n        \"\"\"Save task checkpoint data for recovery purposes.\"\"\"\n        # TODO 1: Create checkpoint record with task_id, run_id, and timestamp\n        # TODO 2: Serialize checkpoint_data to JSON with error handling\n        # TODO 3: Write to temporary file first, then atomic rename\n        # TODO 4: Verify checkpoint file integrity after writing\n        # TODO 5: Return success/failure status\n        # Hint: Use os.rename() for atomic file operations\n        pass\n    \n    def load_checkpoint(self, task_id: str, pipeline_run_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Load most recent checkpoint data for task recovery.\"\"\"\n        # TODO 1: Construct checkpoint filename from task_id and run_id\n        # TODO 2: Check if checkpoint file exists and is readable\n        # TODO 3: Load and deserialize JSON data with error handling\n        # TODO 4: Validate checkpoint data structure and completeness\n        # TODO 5: Return checkpoint data or None if not found/invalid\n        pass\n    \n    def cleanup_old_checkpoints(self, retention_hours: int = 24) -> int:\n        \"\"\"Remove checkpoint files older than retention period.\"\"\"\n        # TODO 1: Scan checkpoint directory for all checkpoint files\n        # TODO 2: Parse timestamps from filenames or file modification times\n        # TODO 3: Identify files older than retention_hours\n        # TODO 4: Remove old files with proper error handling\n        # TODO 5: Return count of files removed\n        pass\n```\n\n**Dead Letter Queue Manager:**\n\n```python\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport json\n\n@dataclass\nclass DeadLetterMessage:\n    task_id: str\n    pipeline_run_id: str\n    error_type: str\n    error_message: str\n    retry_count: int\n    original_payload: Dict[str, Any]\n    failed_at: datetime\n    \nclass DeadLetterQueueManager:\n    def __init__(self, storage_backend: str = \"database\"):\n        self.storage_backend = storage_backend\n        self._initialize_storage()\n    \n    def send_to_dead_letter_queue(self, message: DeadLetterMessage) -> bool:\n        \"\"\"Send failed task to appropriate dead letter queue.\"\"\"\n        # TODO 1: Determine dead letter queue category based on error_type\n        # TODO 2: Serialize message data for storage\n        # TODO 3: Store message in appropriate dead letter queue\n        # TODO 4: Update dead letter queue metrics and alerts\n        # TODO 5: Return success status\n        pass\n    \n    def retrieve_messages(self, queue_type: str, limit: int = 100) -> List[DeadLetterMessage]:\n        \"\"\"Retrieve messages from dead letter queue for reprocessing.\"\"\"\n        # TODO 1: Query storage backend for messages of specified queue_type\n        # TODO 2: Deserialize message data back to DeadLetterMessage objects\n        # TODO 3: Apply limit to prevent memory exhaustion\n        # TODO 4: Mark retrieved messages as \"in_progress\" to prevent duplicate processing\n        # TODO 5: Return list of messages ready for reprocessing\n        pass\n    \n    def requeue_message(self, message_id: str, new_retry_policy: Optional[Dict] = None) -> bool:\n        \"\"\"Move message from dead letter queue back to normal processing.\"\"\"\n        # TODO 1: Retrieve original message data from dead letter queue\n        # TODO 2: Apply new retry policy if provided\n        # TODO 3: Create new task execution record for retry\n        # TODO 4: Remove message from dead letter queue\n        # TODO 5: Submit task to normal execution queue\n        pass\n```\n\n#### Milestone Checkpoints\n\nAfter implementing error handling components, verify the following behaviors:\n\n**Checkpoint 1 - Retry Logic Verification:**\n```bash\n# Run retry policy tests\npython -m pytest tests/error_handling/test_retry_policy.py -v\n\n# Expected output should show:\n# - Exponential backoff calculations with jitter\n# - Proper error type classification and retry decisions  \n# - Maximum retry limits respected\n# - Circuit breaker state transitions\n```\n\n**Checkpoint 2 - Dead Letter Queue Processing:**\n```bash  \n# Simulate pipeline failure and dead letter queue processing\npython scripts/simulate_pipeline_failure.py --task-id test-task --error-type NetworkError\n\n# Expected behavior:\n# - Failed task appears in dead letter queue after max retries\n# - Error classification matches expected category\n# - Requeue functionality moves task back to processing\n```\n\n**Checkpoint 3 - Checkpoint Recovery:**\n```bash\n# Test checkpoint save/restore functionality\npython scripts/test_checkpoint_recovery.py --pipeline-id test-pipeline --simulate-failure\n\n# Expected behavior:\n# - Pipeline creates checkpoints at regular intervals\n# - After simulated failure, pipeline resumes from last checkpoint\n# - No duplicate processing of checkpointed data\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|-----------------|-----|\n| Tasks retry indefinitely | Error type classified as retryable but actually permanent | Check error classification logs and retry attempt patterns | Update error classification rules to identify permanent errors |\n| Circuit breaker never opens | Failure threshold too high or failures not being counted | Monitor failure count metrics and circuit breaker state transitions | Lower failure threshold or fix failure detection logic |\n| Checkpoints cause performance issues | Checkpoint frequency too high or checkpoint data too large | Profile checkpoint save times and storage usage | Reduce checkpoint frequency or optimize checkpoint data size |\n| Dead letter queue grows unbounded | No dead letter queue processing or requeue logic | Monitor dead letter queue size and processing rate | Implement automated dead letter queue processing |\n| Recovery produces duplicate data | Checkpoint boundaries don't align with transaction boundaries | Check for data duplication after recovery operations | Align checkpoints with atomic operation boundaries |\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones - comprehensive testing approach validates pipeline definition (Milestone 1), data processing reliability (Milestones 2-3), and orchestration correctness (Milestone 4)\n\n### Mental Model: Quality Assurance Factory\n\nThink of our testing strategy as a multi-stage quality assurance factory for complex machinery. Just as a car manufacturer tests individual components (engines, transmissions, brakes) in isolation before assembling them into complete vehicles, and then tests those vehicles under various road conditions, our ETL system requires testing at multiple levels. We test individual components like cycle detection algorithms and data connectors in isolation (unit testing), then test how they work together with real data sources (integration testing), and finally verify that each major assembly milestone produces working functionality (milestone checkpoints). Each testing stage catches different types of defects - unit tests catch logic errors, integration tests catch interface mismatches, and milestone checkpoints catch system-level failures.\n\nThe key insight is that ETL systems have unique testing challenges compared to typical web applications. They process large volumes of data, interact with external systems that may be unreliable, and have complex dependency graphs that can fail in subtle ways. Our testing strategy must address data quality issues, handle non-deterministic external dependencies, and validate both correctness and performance characteristics under realistic load conditions.\n\n### Unit Testing Approach\n\n**Component Isolation Strategy**\n\nUnit testing in an ETL system requires careful isolation of components that normally interact with external systems, maintain state across operations, and process large data volumes. The primary challenge is creating reliable, fast tests that validate core logic without depending on databases, file systems, or network services.\n\nOur unit testing approach focuses on testing individual algorithms, data transformations, and business logic in complete isolation. Each test should run in milliseconds, be deterministic regardless of execution order, and require no external dependencies. This means mocking or stubbing any interactions with databases, APIs, file systems, or message brokers.\n\n**Core Component Testing Areas**\n\nThe following table outlines the major component categories and their specific testing requirements:\n\n| Component Type | Primary Test Focus | Mock Dependencies | Key Test Scenarios |\n|----------------|-------------------|-------------------|-------------------|\n| DAG Engine | Cycle detection, topological sorting | None (pure algorithms) | Valid DAGs, circular dependencies, complex graphs |\n| Connectors | Query generation, pagination logic | Database connections, HTTP clients | SQL templating, cursor handling, error responses |\n| Transformations | Data type conversion, null handling | External data sources | Type coercion, schema validation, edge cases |\n| State Machine | Task state transitions | Persistence layer, message broker | Valid transitions, invalid events, retry logic |\n| Scheduler | Cron parsing, next run calculation | System clock, database | Schedule expressions, timezone handling, DST |\n| Orchestrator | Execution order, resource allocation | Task executors, monitoring | Parallel execution, failure propagation, cleanup |\n\n**Data Structure Validation Tests**\n\nEvery data structure defined in our system requires comprehensive validation testing. These tests verify that validation functions correctly identify invalid data and that serialization/deserialization preserves data integrity across system boundaries.\n\nFor `PipelineDefinition` validation, we test scenarios including missing required fields, invalid cron expressions, circular task dependencies, invalid parameter types, and malformed task configurations. Each validation test should verify that appropriate error messages are generated with sufficient detail for debugging.\n\nFor `TaskExecution` state management, we test state transitions under normal conditions and edge cases including invalid state combinations, concurrent state modifications, and recovery from corrupted state. These tests use mock storage backends to avoid dependency on persistent storage systems.\n\n**Algorithm Testing with Edge Cases**\n\nThe DAG processing algorithms require particularly thorough testing because they handle complex graph structures that can fail in subtle ways. The `detect_cycles_dfs` function must be tested with various graph topologies including simple cycles, complex interconnected cycles, self-loops, disconnected components, and graphs with thousands of nodes to verify performance characteristics.\n\nThe `topological_sort_kahns` algorithm requires testing with scenarios including linear chains (no parallelism), fully parallel tasks (no dependencies), diamond-shaped dependencies, and graphs with multiple valid topological orderings. We verify that the algorithm correctly identifies parallelizable task groups and handles edge cases like empty graphs and single-node graphs.\n\n**Mock Strategy for External Dependencies**\n\nExternal dependency mocking follows a consistent pattern across all components. Database connections are mocked using test doubles that simulate query execution, connection failures, and timeout scenarios. HTTP clients are mocked to return predefined responses, simulate network errors, and test pagination edge cases.\n\nThe following table shows our mocking approach for each external system type:\n\n| External System | Mock Implementation | Test Scenarios | Failure Simulation |\n|----------------|-------------------|-----------------|-------------------|\n| Database | In-memory query executor | Query results, schema info | Connection timeout, query failure |\n| REST API | HTTP response simulator | JSON responses, pagination | Network error, rate limiting |\n| File System | In-memory file operations | File read/write, directory listing | Permission denied, disk full |\n| Message Broker | Event queue simulator | Message delivery, ordering | Message loss, duplicate delivery |\n| Time/Clock | Controllable time source | Schedule calculation, timeouts | Clock drift, timezone changes |\n\n**Data Transformation Testing**\n\nData transformation testing requires careful handling of data type edge cases, null value semantics, and schema evolution scenarios. We create test datasets that include boundary values for each data type, various null representations, and malformed data that should trigger validation errors.\n\nType conversion testing covers all supported `DataType` combinations, including lossy conversions that should generate warnings, invalid conversions that should fail, and precision preservation for numeric types. Each conversion test includes boundary values like maximum integers, special floating-point values (NaN, infinity), and edge cases like empty strings and whitespace-only values.\n\nSchema validation testing uses controlled test schemas to verify that validation correctly identifies required field violations, type mismatches, constraint violations, and unknown fields. We test both strict validation (reject any violations) and lenient validation (warn but continue processing) modes.\n\n**Performance and Memory Testing**\n\nUnit tests include performance benchmarks for critical algorithms to catch performance regressions during development. The DAG algorithms are benchmarked with graphs of various sizes to verify that performance scales appropriately with graph complexity.\n\nMemory usage testing is particularly important for data processing components that handle large datasets. We use controlled test datasets to verify that streaming operations maintain constant memory usage regardless of input size, and that batch operations properly release memory after processing.\n\n**Common Pitfalls in Unit Testing**\n\n **Pitfall: Testing with Real External Systems**\nMany developers write unit tests that connect to actual databases or APIs during development. This makes tests slow, non-deterministic, and dependent on external system availability. Instead, use mock implementations that simulate the exact interface behavior without external dependencies.\n\n **Pitfall: Insufficient Edge Case Coverage**\nETL systems handle diverse data types and formats, making edge case testing critical. Don't just test happy path scenarios - include boundary values, malformed data, and unusual but valid input combinations. Create comprehensive test data generators that cover the full range of possible inputs.\n\n **Pitfall: Ignoring Concurrent Access Patterns**\nMany ETL components will run concurrently in production. Unit tests should include scenarios with concurrent access to shared resources, using techniques like goroutines with channels or threading libraries to simulate race conditions and verify thread safety.\n\n### Integration Testing\n\n**End-to-End Pipeline Validation**\n\nIntegration testing validates that our ETL components work correctly when combined with real external systems and realistic data volumes. Unlike unit tests that focus on individual component logic, integration tests verify that data flows correctly through the entire pipeline, that external system interactions handle real-world conditions, and that performance meets requirements under realistic load.\n\nThe primary challenge in integration testing is creating realistic test environments that include representative data sources, network conditions, and system load without requiring full production infrastructure. We accomplish this through containerized test environments, synthetic data generation, and careful test data management.\n\n**Test Environment Architecture**\n\nOur integration test environment uses containerized services to simulate production dependencies while maintaining test isolation and repeatability. The test environment includes database containers with realistic schemas and data volumes, mock API services that simulate third-party systems, and message broker containers for testing event-driven components.\n\nEach test run starts with a clean environment state, loads controlled test data, executes the pipeline under test, and validates the results against expected outcomes. Test data is carefully crafted to include realistic data distributions, common data quality issues, and edge cases that occur in production systems.\n\nThe following table outlines our test environment components:\n\n| Environment Component | Implementation | Purpose | Data Volume |\n|--------------------- |---------------|---------|-------------|\n| Source Database | PostgreSQL container | Realistic SQL extraction testing | 10K-100K records |\n| API Mock Service | HTTP server with predefined responses | API connector testing | Configurable pagination |\n| Target Database | PostgreSQL container | Bulk loading validation | Variable based on test |\n| Message Broker | Redis container | Event-driven testing | Message ordering validation |\n| File Storage | Local filesystem mount | File-based extraction | Multiple file formats |\n| Monitoring Stack | Prometheus/Grafana containers | Metrics collection testing | Full metric pipeline |\n\n**Data Source Integration Testing**\n\nDatabase connector testing uses real database instances with controlled test schemas and data. We test against multiple database types (PostgreSQL, MySQL, SQL Server) to verify that our SQL generation and result parsing work across different database dialects and driver implementations.\n\nTest scenarios include complex join queries, large result sets that require pagination, schema introspection for automatic mapping, and connection pooling under concurrent access. We simulate database failures including connection timeouts, query timeouts, and temporary network partitions to verify that error handling and retry logic work correctly.\n\nAPI connector testing uses both real external APIs (where available and appropriate) and sophisticated mock services that simulate real API behavior. Mock services implement realistic pagination patterns, rate limiting, authentication flows, and error responses that match actual API behavior.\n\nThe following integration test scenarios validate API connector robustness:\n\n| Test Scenario | Validation Focus | Expected Behavior |\n|--------------|-----------------|-------------------|\n| Large dataset pagination | Cursor stability across pages | Complete data extraction without duplicates |\n| Rate limit handling | Backoff and retry logic | Automatic throttling with eventual success |\n| Authentication token refresh | Token expiration handling | Seamless authentication renewal |\n| Partial API failures | Individual request failures | Retry failed requests without re-fetching successful pages |\n| Network instability | Connection interruption recovery | Resume from last successful position |\n\n**Transformation Pipeline Integration**\n\nTransformation integration testing validates that SQL-based transformations execute correctly against real database engines and that Python UDF execution handles realistic data volumes and types. We test transformation chains where the output of one transformation becomes the input to subsequent transformations, verifying that data types are preserved correctly across the chain.\n\nSchema evolution testing uses versioned test schemas to verify that our transformation pipeline handles schema changes gracefully. We test scenarios including adding columns, changing column types, renaming columns, and removing columns, validating that appropriate warnings are generated and data processing continues where possible.\n\nUDF integration testing executes Python functions in isolated subprocess environments with realistic data volumes to verify that memory usage, execution time, and error handling meet production requirements. We test both row-by-row UDF execution and batch processing modes to validate performance characteristics.\n\n**End-to-End Pipeline Execution**\n\nComplete pipeline integration tests execute full ETL workflows from source extraction through transformation to target loading. These tests use realistic data volumes (thousands to tens of thousands of records) to validate performance characteristics and identify bottlenecks that only appear under load.\n\nWe test various pipeline topologies including linear pipelines, diamond-shaped dependency graphs, and complex multi-source pipelines that combine data from multiple sources. Each test validates that data lineage tracking captures complete provenance information and that monitoring metrics accurately reflect pipeline performance.\n\n**Failure and Recovery Testing**\n\nIntegration tests include comprehensive failure scenario testing to validate that our error handling and retry logic work correctly with real external systems. We use techniques like network manipulation, process termination, and resource exhaustion to simulate realistic failure conditions.\n\nRecovery testing validates that pipelines can resume correctly after various failure types. We test scenarios including mid-pipeline failures, external system outages, and resource exhaustion, verifying that checkpointing and watermarking allow pipelines to resume from appropriate points without data duplication or loss.\n\nThe following table outlines key failure scenarios and their validation criteria:\n\n| Failure Type | Simulation Method | Recovery Validation |\n|-------------|------------------|-------------------|\n| Database connection loss | Network partition during extraction | Resume from last successful batch |\n| API rate limit exceeded | Mock service rate limiting | Automatic backoff and retry |\n| Disk space exhaustion | Filesystem quota limits | Graceful failure with cleanup |\n| Memory exhaustion | Large dataset processing | Streaming fallback or batch reduction |\n| Process termination | SIGTERM during transformation | Checkpoint recovery on restart |\n\n**Performance and Scalability Testing**\n\nIntegration tests include performance benchmarks that validate system behavior under realistic load conditions. We test data processing throughput, memory usage patterns, and concurrent pipeline execution to identify performance bottlenecks and validate that resource usage scales appropriately with data volume.\n\nScalability testing validates that our system handles increasing data volumes gracefully. We test with datasets ranging from thousands to hundreds of thousands of records, measuring processing time, memory usage, and system resource consumption to identify scaling limitations.\n\n**Common Pitfalls in Integration Testing**\n\n **Pitfall: Insufficient Test Data Realism**\nUsing overly simplified test data fails to catch issues that occur with real-world data complexity. Create test datasets that include realistic data distributions, common data quality issues, and edge cases found in production systems. Include null values, special characters, and boundary values in test data.\n\n **Pitfall: Ignoring External System Variability**\nExternal systems behave differently under various conditions. Test with different response times, intermittent failures, and varying data volumes to ensure your pipeline handles real-world system variability. Don't assume external systems will always respond quickly or correctly.\n\n **Pitfall: Inadequate Cleanup Between Tests**\nETL integration tests can leave persistent state in databases, file systems, and external services. Implement thorough cleanup procedures that run before and after each test to ensure test isolation and prevent cascading failures between test runs.\n\n### Milestone Checkpoints\n\n**Milestone 1: DAG Definition and Validation Checkpoint**\n\nAfter implementing the DAG definition and validation engine, the system must demonstrate correct parsing, validation, and visualization of pipeline definitions. This checkpoint validates that the foundation for all subsequent pipeline operations is solid and handles complex dependency scenarios correctly.\n\n**Validation Criteria for Milestone 1:**\n\nThe checkpoint validates DAG parsing from both YAML and Python configuration formats, ensuring that all pipeline metadata including task configurations, dependencies, and parameters are correctly extracted. We test with increasingly complex pipeline definitions to verify that the system scales to realistic production scenarios.\n\n| Validation Area | Test Scenarios | Success Criteria |\n|----------------|-----------------|-----------------|\n| YAML Parsing | Simple linear pipeline, complex multi-source DAG | Correct task extraction and dependency mapping |\n| Python Definition | Programmatic pipeline creation, dynamic task generation | Proper object model population |\n| Cycle Detection | Intentional cycles, complex indirect cycles | Accurate cycle identification with path reporting |\n| Topological Sort | Various DAG shapes, parallel execution opportunities | Correct execution levels with maximum parallelism |\n| Parameter Substitution | Runtime variables, nested parameter references | Proper template resolution with type checking |\n| Error Handling | Malformed YAML, invalid dependencies, missing tasks | Clear error messages with specific problem identification |\n\n**Manual Verification Steps:**\n\nExecute the checkpoint validation by running a comprehensive test suite that exercises all DAG processing functionality. Start with simple pipeline definitions and progressively test more complex scenarios including pipelines with dozens of tasks and complex dependency relationships.\n\nVerify DAG visualization by generating graph representations of test pipelines and confirming that the visual output accurately represents the dependency structure. Check that parallel execution opportunities are correctly identified and that critical path calculations provide accurate estimates.\n\nTest error handling by intentionally creating invalid pipeline definitions and verifying that error messages provide sufficient detail for developers to identify and fix configuration problems.\n\n**Milestone 2: Data Extraction and Loading Checkpoint**\n\nThe data extraction and loading checkpoint validates that connectors can reliably extract data from various sources and load it to target destinations with proper error handling and incremental processing capabilities. This milestone is critical because data connectivity issues are among the most common ETL pipeline failures.\n\n**Source Connector Validation:**\n\nDatabase connector testing uses real database instances with controlled test schemas containing representative data types, null values, and edge cases. We validate that SQL query generation produces correct queries for various extraction patterns including simple SELECT statements, complex joins, and filtered extractions.\n\nIncremental extraction testing verifies that watermarking correctly identifies changed records and that pagination handles large result sets efficiently. We test with datasets large enough to require multiple pages and verify that no records are missed or duplicated during extraction.\n\nAPI connector testing validates HTTP client behavior including authentication, pagination, rate limiting, and error recovery. We test against both mock APIs and real external services where appropriate, ensuring that connector behavior matches API specifications and handles real-world API quirks.\n\nThe following validation criteria ensure connector reliability:\n\n| Connector Type | Validation Focus | Test Data Volume | Success Criteria |\n|---------------|------------------|------------------|------------------|\n| Database | SQL generation, type mapping, pagination | 50K+ records | Complete extraction without data loss |\n| REST API | Authentication, pagination, rate limiting | 10K+ records across pages | Proper cursor handling and retry logic |\n| File System | Format detection, streaming, compression | Multi-GB files | Memory-efficient processing |\n\n**Destination Connector Validation:**\n\nLoading connector testing validates that data can be efficiently written to target systems with proper error handling and transactional guarantees. We test bulk loading scenarios with large datasets to verify that performance meets requirements and that partial failures are handled gracefully.\n\nSchema mapping testing ensures that data types are correctly converted between source and destination systems, with appropriate warnings for lossy conversions and errors for incompatible types. We test with diverse data type combinations to verify comprehensive type system coverage.\n\n**Milestone 3: Data Transformation Checkpoint**\n\nThe transformation checkpoint validates that both SQL-based transformations and Python UDFs execute correctly with realistic data volumes and handle error conditions gracefully. This milestone ensures that data quality and transformation logic meet business requirements.\n\n**SQL Transformation Validation:**\n\nSQL transformation testing executes template-based transformations against real database engines to verify that SQL generation produces correct results and handles parameter substitution properly. We test with complex transformations including aggregations, window functions, and multi-table joins.\n\nSchema validation testing ensures that transformation outputs match expected schemas and that data quality rules correctly identify and handle invalid records. We test with datasets containing known data quality issues to verify that validation logic produces appropriate warnings and errors.\n\n**UDF Execution Validation:**\n\nPython UDF testing validates that user-defined functions execute correctly in isolated environments with proper resource management and error handling. We test both row-by-row processing and batch processing modes to verify performance characteristics meet requirements.\n\nThe following table outlines UDF validation criteria:\n\n| UDF Type | Execution Mode | Test Data | Success Criteria |\n|----------|---------------|-----------|------------------|\n| Simple transformations | Row-by-row | 100K records | < 10ms per record average |\n| Complex analytics | Batch processing | 1M+ records | Linear scaling with data volume |\n| External API calls | Rate-limited batch | 10K records | Proper throttling and error handling |\n\n**Error Handling Validation:**\n\nTransformation error handling testing validates that invalid data is handled gracefully with appropriate logging and that pipeline execution continues where possible. We test scenarios including malformed input data, UDF exceptions, and resource exhaustion.\n\n**Milestone 4: Orchestration and Monitoring Checkpoint**\n\nThe final checkpoint validates complete end-to-end pipeline execution with scheduling, monitoring, and alerting functionality. This milestone ensures that the system can reliably execute production workloads with appropriate observability and failure recovery.\n\n**Schedule Execution Validation:**\n\nScheduler testing validates that pipelines execute according to configured schedules and that event-driven triggers work correctly. We test various cron expressions including edge cases like month boundaries and daylight saving time transitions.\n\nConcurrent execution testing validates that the system handles multiple simultaneous pipeline runs correctly with proper resource isolation and conflict detection.\n\n**Monitoring and Alerting Validation:**\n\nMonitoring checkpoint testing validates that metrics are correctly collected and reported, that alerting triggers appropriately for various failure conditions, and that data lineage tracking captures complete provenance information.\n\nWe test monitoring under various load conditions to verify that metric collection doesn't significantly impact pipeline performance and that alerting provides actionable information for operators.\n\n**End-to-End Integration Validation:**\n\nThe final validation executes complex multi-stage pipelines that exercise all system components together. These tests use realistic data volumes and complexity to validate that the complete system meets performance and reliability requirements.\n\nWe execute failure recovery testing that simulates various production failure scenarios and validates that the system recovers gracefully with minimal data loss or processing delays.\n\n### Implementation Guidance\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Test Framework | pytest with fixtures | pytest + testcontainers + factory-boy |\n| Mock Library | unittest.mock (built-in) | responses + freezegun + factory-boy |\n| Database Testing | SQLite in-memory | PostgreSQL testcontainers |\n| API Testing | requests-mock | WireMock or Prism mock server |\n| Performance Testing | pytest-benchmark | locust for load testing |\n| Test Data | JSON fixtures | factory-boy data factories |\n\n**Recommended Test Structure:**\n\n```\nproject-root/\ntests/\n  unit/                           isolated component tests\n    test_dag_engine.py            DAG parsing and validation\n    test_connectors.py            individual connector logic\n    test_transformations.py       transformation functions\n    test_scheduler.py             scheduling logic\n    test_state_machine.py         state transition logic\n  integration/                    end-to-end component tests\n    test_pipeline_execution.py    complete pipeline runs\n    test_external_systems.py      real database/API tests\n    test_failure_recovery.py      error handling scenarios\n  milestones/                     milestone checkpoint tests\n    test_milestone_1.py           DAG definition checkpoint\n    test_milestone_2.py           extraction/loading checkpoint\n    test_milestone_3.py           transformation checkpoint\n    test_milestone_4.py           orchestration checkpoint\n  fixtures/                       test data and configuration\n    pipelines/                    sample pipeline definitions\n    data/                         test datasets\n    mocks/                        mock service configurations\n  conftest.py                     pytest configuration and shared fixtures\n```\n\n**Unit Test Infrastructure Code:**\n\n```python\n# tests/conftest.py\nimport pytest\nimport tempfile\nimport shutil\nfrom unittest.mock import MagicMock\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, List\n\n@pytest.fixture\ndef mock_database_connection():\n    \"\"\"Mock database connection for unit tests.\"\"\"\n    conn = MagicMock()\n    conn.execute.return_value = []\n    conn.fetchall.return_value = []\n    conn.fetchone.return_value = None\n    conn.commit.return_value = None\n    conn.rollback.return_value = None\n    conn.close.return_value = None\n    return conn\n\n@pytest.fixture\ndef mock_http_client():\n    \"\"\"Mock HTTP client for API connector tests.\"\"\"\n    client = MagicMock()\n    client.get.return_value.status_code = 200\n    client.get.return_value.json.return_value = {\"data\": []}\n    client.post.return_value.status_code = 201\n    return client\n\n@pytest.fixture\ndef sample_pipeline_definition():\n    \"\"\"Standard pipeline definition for testing.\"\"\"\n    return PipelineDefinition(\n        id=\"test-pipeline\",\n        name=\"Test Pipeline\",\n        description=\"Pipeline for unit testing\",\n        schedule=\"0 * * * *\",\n        tasks=[\n            TaskDefinition(\n                id=\"extract-task\",\n                name=\"Extract Data\",\n                type=\"database_extract\",\n                config={\"query\": \"SELECT * FROM users\"},\n                dependencies=[],\n                retry_policy=RetryPolicy(max_attempts=3, backoff_seconds=60, exponential_backoff=True, retry_on_error_types=[\"NETWORK_ERROR\"]),\n                timeout_seconds=300\n            ),\n            TaskDefinition(\n                id=\"transform-task\",\n                name=\"Transform Data\",\n                type=\"sql_transform\",\n                config={\"query\": \"SELECT id, UPPER(name) as name FROM input\"},\n                dependencies=[\"extract-task\"],\n                retry_policy=RetryPolicy(max_attempts=2, backoff_seconds=30, exponential_backoff=False, retry_on_error_types=[\"DATA_QUALITY_ERROR\"]),\n                timeout_seconds=600\n            )\n        ],\n        parameters={\"source_table\": \"users\"},\n        created_at=datetime.now(),\n        version=1\n    )\n\n@pytest.fixture\ndef temp_directory():\n    \"\"\"Temporary directory for file-based tests.\"\"\"\n    temp_dir = tempfile.mkdtemp()\n    yield temp_dir\n    shutil.rmtree(temp_dir)\n\nclass MockTimeProvider:\n    \"\"\"Controllable time source for testing scheduling logic.\"\"\"\n    \n    def __init__(self, start_time: datetime):\n        self.current_time = start_time\n    \n    def now(self) -> datetime:\n        return self.current_time\n    \n    def advance(self, delta: timedelta):\n        self.current_time += delta\n\n@pytest.fixture\ndef mock_time():\n    \"\"\"Mock time provider for deterministic scheduling tests.\"\"\"\n    return MockTimeProvider(datetime(2024, 1, 1, 12, 0, 0))\n```\n\n**Unit Test Skeleton Code:**\n\n```python\n# tests/unit/test_dag_engine.py\nimport pytest\nfrom dag_engine import DAGEngine, CycleDetectionResult, ExecutionPlan\nfrom data_model import PipelineDefinition, TaskDefinition\n\nclass TestDAGValidation:\n    \"\"\"Test suite for DAG parsing and validation logic.\"\"\"\n    \n    def test_detect_cycles_simple_cycle(self):\n        \"\"\"Test cycle detection with a simple two-node cycle.\"\"\"\n        # TODO 1: Create adjacency list representing A -> B -> A cycle\n        # TODO 2: Call detect_cycles_dfs with the adjacency list\n        # TODO 3: Assert that has_cycle is True\n        # TODO 4: Assert that cycle_path contains the expected cycle nodes\n        # TODO 5: Verify that cycle_path forms a valid cycle (first == last node)\n        pass\n    \n    def test_topological_sort_parallel_tasks(self):\n        \"\"\"Test topological sort identifies parallel execution opportunities.\"\"\"\n        # TODO 1: Create adjacency list with diamond dependency pattern (A -> B,C -> D)\n        # TODO 2: Call topological_sort_kahns with the adjacency list\n        # TODO 3: Assert that result has correct number of execution levels\n        # TODO 4: Assert that B and C are in the same execution level (can run parallel)\n        # TODO 5: Assert that A is in first level and D is in last level\n        pass\n    \n    def test_parse_yaml_invalid_syntax(self):\n        \"\"\"Test YAML parsing with malformed syntax.\"\"\"\n        # TODO 1: Create YAML string with invalid syntax (missing colons, wrong indentation)\n        # TODO 2: Call parse_yaml_file with the invalid YAML\n        # TODO 3: Assert that appropriate exception is raised\n        # TODO 4: Verify that error message contains useful debugging information\n        pass\n\nclass TestExecutionPlanning:\n    \"\"\"Test suite for DAG execution planning logic.\"\"\"\n    \n    def test_calculate_critical_path(self):\n        \"\"\"Test critical path calculation with task duration estimates.\"\"\"\n        # TODO 1: Create adjacency list with known task dependencies\n        # TODO 2: Create task_durations dict with estimated execution times\n        # TODO 3: Call calculate_critical_path with adjacency list and durations\n        # TODO 4: Assert that returned path represents the longest duration chain\n        # TODO 5: Assert that total duration matches sum of critical path task durations\n        pass\n```\n\n**Integration Test Infrastructure:**\n\n```python\n# tests/integration/conftest.py\nimport pytest\nimport docker\nimport time\nimport psycopg2\nfrom testcontainers.postgres import PostgresContainer\nfrom testcontainers.compose import DockerCompose\n\n@pytest.fixture(scope=\"session\")\ndef postgres_container():\n    \"\"\"PostgreSQL container for integration testing.\"\"\"\n    with PostgresContainer(\"postgres:13\", driver=\"psycopg2\") as postgres:\n        # Wait for container to be ready\n        time.sleep(5)\n        \n        # Create test schema and sample data\n        connection = psycopg2.connect(postgres.get_connection_url())\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                CREATE TABLE users (\n                    id SERIAL PRIMARY KEY,\n                    name VARCHAR(100) NOT NULL,\n                    email VARCHAR(100) UNIQUE,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                );\n                \n                INSERT INTO users (name, email) VALUES\n                ('Alice Johnson', 'alice@example.com'),\n                ('Bob Smith', 'bob@example.com'),\n                ('Charlie Brown', 'charlie@example.com');\n            \"\"\")\n            connection.commit()\n        \n        yield postgres\n\n@pytest.fixture(scope=\"session\")\ndef test_environment():\n    \"\"\"Complete test environment with all external services.\"\"\"\n    with DockerCompose(\"tests/integration\", compose_file_name=\"docker-compose.test.yml\") as compose:\n        # Wait for all services to be ready\n        time.sleep(10)\n        yield compose\n\nclass IntegrationTestHelper:\n    \"\"\"Helper utilities for integration testing.\"\"\"\n    \n    @staticmethod\n    def wait_for_pipeline_completion(pipeline_run_id: str, timeout_seconds: int = 300):\n        \"\"\"Wait for pipeline run to complete with timeout.\"\"\"\n        # TODO: Implement polling logic to check pipeline run status\n        # TODO: Raise TimeoutError if pipeline doesn't complete within timeout\n        pass\n    \n    @staticmethod\n    def validate_data_lineage(source_table: str, target_table: str, expected_transformations: List[str]):\n        \"\"\"Validate that data lineage correctly captures transformations.\"\"\"\n        # TODO: Query lineage tracking system\n        # TODO: Verify that all expected transformations are recorded\n        # TODO: Assert that source and target tables are correctly linked\n        pass\n```\n\n**Milestone Checkpoint Implementation:**\n\n```python\n# tests/milestones/test_milestone_1.py\nimport pytest\nimport yaml\nfrom pathlib import Path\nfrom dag_engine import DAGEngine\nfrom data_model import PipelineDefinition\n\nclass TestMilestone1Checkpoint:\n    \"\"\"Milestone 1: DAG Definition and Validation checkpoint tests.\"\"\"\n    \n    def test_yaml_pipeline_parsing(self):\n        \"\"\"Verify YAML pipeline definitions parse correctly.\"\"\"\n        # TODO 1: Load sample YAML pipeline from fixtures\n        # TODO 2: Parse YAML using parse_yaml_file function\n        # TODO 3: Validate all required fields are present in PipelineDefinition\n        # TODO 4: Verify task dependencies are correctly mapped\n        # TODO 5: Check that parameters are properly extracted\n        pass\n    \n    def test_complex_dag_validation(self):\n        \"\"\"Test validation with realistic complex pipeline.\"\"\"\n        # TODO 1: Create pipeline with 10+ tasks and complex dependencies\n        # TODO 2: Validate that cycle detection completes successfully\n        # TODO 3: Verify topological sort produces valid execution order\n        # TODO 4: Check that parallel execution opportunities are identified\n        # TODO 5: Validate that critical path calculation is accurate\n        pass\n    \n    def test_error_reporting_quality(self):\n        \"\"\"Verify that validation errors provide actionable information.\"\"\"\n        # TODO 1: Create pipeline with intentional errors (cycles, missing tasks)\n        # TODO 2: Run validation and capture error messages\n        # TODO 3: Assert that errors specify exact problem location\n        # TODO 4: Verify that suggested fixes are included where possible\n        pass\n\n# Checkpoint validation command:\n# pytest tests/milestones/test_milestone_1.py -v\n# Expected: All tests pass, demonstrating working DAG engine\n```\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | Diagnosis Steps | Fix |\n|---------|--------------|-----------------|-----|\n| Tests hang indefinitely | External service dependency not mocked | Check test for database/API calls | Add appropriate mocks or use testcontainers |\n| Intermittent test failures | Race conditions in concurrent code | Run tests multiple times, check for shared state | Add proper synchronization or test isolation |\n| Memory usage grows during tests | Test data not cleaned between runs | Monitor memory usage with memory_profiler | Implement proper teardown in fixtures |\n| Integration tests fail in CI | Different environment configuration | Compare local vs CI environment settings | Standardize environment using containers |\n\n**Milestone Checkpoint Commands:**\n\nAfter completing each milestone, run these validation commands:\n\n```bash\n# Milestone 1: DAG Definition and Validation\npytest tests/milestones/test_milestone_1.py -v\npython -m dag_engine.visualizer --input tests/fixtures/pipelines/complex.yaml --output /tmp/dag.svg\n\n# Milestone 2: Data Extraction and Loading  \npytest tests/milestones/test_milestone_2.py -v\npytest tests/integration/test_connectors.py -v --tb=short\n\n# Milestone 3: Data Transformations\npytest tests/milestones/test_milestone_3.py -v\npytest tests/integration/test_transformations.py -v\n\n# Milestone 4: Orchestration and Monitoring\npytest tests/milestones/test_milestone_4.py -v\npytest tests/integration/test_pipeline_execution.py -v\n\n# Complete test suite\npytest tests/ -v --cov=src --cov-report=html\n```\n\nExpected behavior after each milestone:\n- All milestone-specific tests pass without errors\n- Integration tests demonstrate working functionality with real data\n- Performance benchmarks meet established criteria  \n- Manual verification steps complete successfully\n- System demonstrates graceful error handling and recovery\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones - systematic debugging approaches are essential for pipeline definition troubleshooting (Milestone 1), data processing issue resolution (Milestones 2-3), and orchestration debugging (Milestone 4).\n\n### Mental Model: Medical Diagnosis\n\nThink of debugging ETL pipelines like being a doctor diagnosing a patient. You start by gathering symptoms (error messages, performance metrics, unusual behaviors), then form hypotheses about potential causes based on your understanding of how the system works. You run specific tests to confirm or rule out each hypothesis, narrowing down to the root cause. Just as a doctor has a systematic approach to diagnosis - checking vital signs first, then running targeted tests - debugging requires a methodical approach rather than random guessing.\n\nThe key insight is that symptoms often manifest far from their root causes in distributed systems. A task timeout might be caused by network congestion, resource exhaustion, or a deadlock in a completely different component. Like referred pain in medicine, the symptom location doesn't always indicate the problem source.\n\n### Common Symptoms and Causes\n\nETL pipeline debugging follows predictable patterns. Most issues fall into a few categories with characteristic symptoms that can guide diagnosis. Understanding these symptom-cause patterns allows developers to quickly narrow the search space and apply targeted fixes.\n\nThe following symptom-cause mapping represents the most common issues encountered in production ETL systems, organized by the component where symptoms first appear:\n\n| Symptom | Likely Cause | Immediate Diagnosis Steps | Fix Strategy |\n|---------|--------------|--------------------------|-------------|\n| Pipeline stuck in PENDING state | DAG parsing errors or validation failures | Check pipeline definition YAML/Python syntax; examine validation error logs | Fix syntax errors, resolve dependency cycles, validate task configurations |\n| Tasks skip unexpectedly | Upstream dependencies failed or wrong dependency configuration | Trace upstream task states; verify dependency specifications match actual task IDs | Correct dependency declarations, fix upstream failures, check for typos in task IDs |\n| \"Cycle detected\" error during parsing | Circular dependencies in task definitions | Run `detect_cycles_dfs()` manually on pipeline definition; examine dependency graph visualization | Remove circular dependencies, consider splitting tasks, add conditional logic |\n| Tasks run in wrong order | Topological sort error or missing dependencies | Compare actual execution order with expected DAG visualization; check for missing dependency declarations | Add missing dependencies, verify topological sort implementation, check for race conditions |\n| Database connection timeouts | Connection pool exhaustion or network issues | Monitor active connections, check network latency, examine connection pool metrics | Increase pool size, implement connection retry logic, optimize query performance |\n| API extraction fails intermittently | Rate limiting, authentication expiry, or pagination issues | Check API response headers for rate limit info; examine authentication token validity; verify pagination cursor handling | Implement exponential backoff, refresh tokens proactively, fix pagination logic |\n| Incremental loads miss data | Watermark not updated atomically or clock skew issues | Verify watermark update happens in same transaction as data load; check for time zone mismatches | Use database transactions for atomic updates, add lookback window for clock skew |\n| Duplicate records in destination | Upsert logic failures or non-atomic operations | Check for unique constraint violations; examine upsert key definitions; verify transaction isolation | Fix upsert key selection, use proper transaction boundaries, implement deduplication |\n| Schema validation errors | Type coercion failures or schema evolution issues | Compare source and destination schemas; check for null handling differences; examine type conversion logs | Update schema mappings, implement graceful type coercion, handle null value differences |\n| Transformation timeouts | Memory exhaustion from large datasets or inefficient SQL | Monitor memory usage during transformation; examine SQL execution plans; check for full table scans | Implement streaming processing, optimize SQL queries, add memory limits and checkpoints |\n| Python UDF crashes | Unhandled exceptions or resource limits exceeded | Examine UDF execution logs; check memory and CPU usage; verify error handling in UDF code | Add try-catch blocks, implement resource monitoring, use subprocess isolation |\n| Data quality validation failures | Source data corruption or business rule violations | Sample failed records; examine validation rule logic; check for data drift in source systems | Implement data profiling, adjust validation thresholds, add data quality monitoring |\n| Pipeline runs never start | Scheduler configuration errors or resource unavailability | Check cron expression syntax; verify scheduler service health; examine resource allocation logs | Fix schedule syntax, restart scheduler service, increase resource limits |\n| Tasks hang in RUNNING state indefinitely | Deadlocks, infinite loops, or resource starvation | Check for database locks; examine task execution logs; monitor CPU and memory usage | Implement task timeouts, add deadlock detection, optimize resource usage |\n| Pipeline fails but tasks show SUCCESS | State management race conditions or incomplete failure propagation | Compare task-level and pipeline-level state; check for concurrent state updates; examine event ordering | Fix state transition logic, implement proper locking, ensure atomic state updates |\n| Memory usage grows continuously | Memory leaks in transformation code or unclosed connections | Profile memory usage over time; check for unclosed database connections; examine object lifecycle | Fix connection management, implement proper cleanup, add garbage collection monitoring |\n| High CPU usage with no progress | Infinite loops, busy waiting, or inefficient algorithms | Profile CPU usage by component; examine algorithm complexity; check for blocking operations | Optimize algorithms, add sleep to polling loops, implement proper backpressure |\n| Disk space fills rapidly | Excessive logging, temporary file accumulation, or large intermediate results | Check log file sizes; examine temporary directory usage; monitor intermediate data volumes | Implement log rotation, clean up temporary files, use streaming for large datasets |\n| Network timeouts between components | Network congestion, firewall issues, or component overload | Check network latency and packet loss; verify firewall rules; examine component health metrics | Increase timeout values, implement retry logic, optimize network usage |\n\n> **Key Insight**: The vast majority of ETL pipeline issues stem from three root causes: resource management problems (memory, connections, disk), state consistency issues (race conditions, atomic operations), and data quality problems (schema changes, null handling). Understanding these patterns helps focus debugging efforts.\n\n### Common Anti-Patterns and Debugging Traps\n\n **Pitfall: Log Flooding During Debugging**\nDevelopers often enable verbose logging across all components when debugging, generating massive log volumes that obscure the actual problem. This makes debugging slower and can impact system performance.\n\n*Why it's wrong*: Excessive logging creates noise that hides signal, fills disk space rapidly, and can cause performance degradation that masks the original issue.\n\n*Fix*: Enable targeted logging only for the specific component and time window where the issue occurs. Use log levels strategically and implement log sampling for high-frequency events.\n\n **Pitfall: Testing in Isolation Only**\nTesting individual components in isolation without integration testing often misses issues that only appear when components interact under realistic conditions.\n\n*Why it's wrong*: Many ETL issues emerge from component interactions, timing dependencies, and resource contention that don't appear in unit tests.\n\n*Fix*: Implement comprehensive integration tests with realistic data volumes and concurrent execution patterns. Test failure scenarios and recovery paths.\n\n **Pitfall: Ignoring Resource Limits**\nDebugging in development environments without realistic resource constraints often fails to reveal issues that only appear under production load.\n\n*Why it's wrong*: Memory leaks, connection pool exhaustion, and performance degradation only manifest under realistic load conditions.\n\n*Fix*: Use production-like resource limits in testing environments. Implement resource monitoring and alerting to catch issues early.\n\n### Debugging Techniques\n\nEffective ETL debugging requires a systematic approach that combines multiple investigation techniques. The key is to gather evidence methodically rather than making assumptions about where problems might be.\n\n#### Log-Based Investigation\n\nPipeline debugging starts with understanding the log structure and using logs strategically to trace execution flow and identify anomalies.\n\n**Log Correlation Strategy**\n\nETL systems generate logs from multiple components, making it challenging to trace a single pipeline run across the system. Implement correlation using the pipeline run ID and task execution ID to connect related log entries:\n\n1. Start with the pipeline-level logs to understand the overall execution state and timeline\n2. Identify which tasks failed or behaved unexpectedly based on pipeline logs\n3. Drill down to task-level logs using the task execution ID to examine detailed behavior\n4. Follow data lineage through transformation logs to understand data flow issues\n5. Correlate with system-level logs (database, message broker) using timestamps and correlation IDs\n\n**Log Analysis Patterns**\n\n| Log Pattern | Indicates | Investigation Steps |\n|-------------|-----------|---------------------|\n| Gaps in timestamp sequence | Component hang or crash | Check system logs for crash dumps; examine memory and CPU usage during gap period |\n| Repeated error messages with same context | Retry loop without progress | Examine retry policy configuration; check if underlying issue is being addressed |\n| Error messages without stack traces | Swallowed exceptions | Review error handling code; ensure proper exception logging and propagation |\n| Performance metrics showing degradation over time | Resource leak or memory pressure | Profile memory usage; check for unclosed connections or accumulating objects |\n| Inconsistent state between components | Race condition or incomplete transaction | Check transaction boundaries; examine concurrent access patterns |\n\n**Structured Log Querying**\n\nImplement structured logging with consistent field names to enable efficient querying:\n\n```\ntimestamp=2024-01-15T10:30:45Z level=ERROR component=task-executor \npipeline_id=pipeline-123 run_id=run-456 task_id=extract-customers \nattempt=2 error_type=TIMEOUT_ERROR message=\"Database query timeout after 300s\"\n```\n\nUse log aggregation tools to query across multiple components and time ranges. Key queries for ETL debugging:\n\n- Find all errors for a specific pipeline run: `run_id=run-456 AND level=ERROR`\n- Trace task execution timeline: `task_id=extract-customers ORDER BY timestamp`\n- Identify resource exhaustion patterns: `error_type=RESOURCE_EXHAUSTION last 24h`\n- Monitor retry patterns: `attempt>1 GROUP BY error_type`\n\n#### State Inspection and Monitoring\n\nUnderstanding current system state is crucial for diagnosing issues, especially for intermittent problems that don't leave clear log traces.\n\n**Database State Analysis**\n\nThe pipeline metadata database contains the authoritative state for all executions. Use direct database queries to understand state inconsistencies:\n\n| State Query Purpose | SQL Pattern | Key Information |\n|-------------------|-------------|-----------------|\n| Find stuck pipelines | `SELECT * FROM pipeline_runs WHERE state='RUNNING' AND started_at < NOW() - INTERVAL '2 hours'` | Long-running executions that may be stuck |\n| Identify retry patterns | `SELECT task_id, COUNT(*) as attempts FROM task_executions WHERE pipeline_run_id='run-456' GROUP BY task_id` | Tasks requiring multiple attempts |\n| Check dependency resolution | `SELECT upstream_task, downstream_task FROM task_dependencies WHERE pipeline_id='pipeline-123'` | Verify dependencies match expectations |\n| Monitor failure rates | `SELECT DATE(completed_at), state, COUNT(*) FROM pipeline_runs GROUP BY DATE(completed_at), state` | Track success/failure trends over time |\n\n**Resource Monitoring Integration**\n\nCorrelate application metrics with system-level resource usage to identify bottlenecks:\n\n- **Memory patterns**: Track heap usage during transformation tasks to identify memory leaks\n- **Connection pools**: Monitor active/idle connections to detect pool exhaustion\n- **Disk I/O**: Watch for disk space consumption during large data loads\n- **Network utilization**: Monitor bandwidth usage during data extraction phases\n- **CPU usage**: Identify compute-intensive transformations that need optimization\n\n#### Interactive Debugging Techniques\n\nFor complex issues that require real-time investigation, interactive debugging provides deeper insights than static log analysis.\n\n**Pipeline Replay and Simulation**\n\nImplement pipeline replay functionality to reproduce issues in controlled environments:\n\n1. Capture pipeline state and input data from the failed execution\n2. Create isolated replay environment with same resource constraints\n3. Execute pipeline with additional debugging instrumentation enabled\n4. Step through execution phases to identify exact failure point\n5. Modify inputs systematically to isolate root cause\n\n**Live State Inspection**\n\nProvide debugging APIs that allow real-time inspection of running pipelines:\n\n| Debug API Endpoint | Purpose | Usage |\n|-------------------|---------|-------|\n| `/debug/pipeline/{run_id}/state` | Current state of all tasks | Check for unexpected task states or timing issues |\n| `/debug/task/{execution_id}/metrics` | Real-time task metrics | Monitor memory, CPU, and data throughput during execution |\n| `/debug/connections/pools` | Connection pool status | Identify connection leaks or pool exhaustion |\n| `/debug/queues/depths` | Message queue depths | Detect backpressure or component overload |\n\n**Component Health Checks**\n\nImplement comprehensive health checks that validate both basic connectivity and operational readiness:\n\n- **Database connectivity**: Verify read/write access with sample queries\n- **External API availability**: Test authentication and basic endpoint access\n- **Message broker health**: Confirm topic access and message flow\n- **Resource availability**: Check disk space, memory, and CPU capacity\n- **Schema compatibility**: Validate source/destination schema compatibility\n\n### Performance Debugging\n\nPerformance issues in ETL pipelines often manifest as gradually degrading throughput, increasing memory usage, or extended execution times. These issues require specialized debugging approaches that focus on resource utilization and algorithmic efficiency.\n\n#### Identifying Performance Bottlenecks\n\nPerformance debugging starts with establishing baseline metrics and identifying deviations from expected behavior patterns.\n\n**Throughput Analysis**\n\nETL pipeline performance is primarily measured in terms of data throughput and execution time. Establish baseline metrics for comparison:\n\n| Performance Metric | Measurement Method | Typical Issues | Investigation Steps |\n|-------------------|-------------------|----------------|-------------------|\n| Records per second processed | Count input/output records divided by execution time | Decreasing over time, varies between runs | Check for data volume changes, schema complexity increases, resource contention |\n| Data volume throughput | Bytes processed per minute during extraction/loading | Network bandwidth limitations, serialization overhead | Monitor network utilization, examine data compression, check serialization efficiency |\n| Task execution time | Wall clock time from start to completion | Individual tasks taking longer than expected | Profile task internals, check for blocking operations, examine algorithm complexity |\n| End-to-end pipeline latency | Time from trigger to completion | Overall pipeline slowdown affecting SLAs | Analyze critical path, identify parallelization opportunities, check for sequential bottlenecks |\n\n**Resource Utilization Patterns**\n\nDifferent types of performance issues create characteristic resource usage patterns:\n\n- **Memory-bound operations**: Steady memory growth during execution, potential out-of-memory errors\n- **CPU-bound operations**: High CPU utilization with low I/O, often in transformation phases\n- **I/O-bound operations**: High disk or network activity with lower CPU usage\n- **Lock contention**: Low resource usage but poor throughput due to blocking\n\n#### Memory Performance Issues\n\nMemory-related performance problems are common in ETL pipelines due to large dataset processing and transformation operations.\n\n**Memory Leak Detection**\n\nMemory leaks in ETL pipelines typically occur in transformation code, connection management, or intermediate result caching:\n\n1. **Monitor heap growth patterns** over multiple pipeline executions to identify consistent memory increases\n2. **Profile object allocation** during transformation phases to identify accumulating objects\n3. **Check connection lifecycle** to ensure database connections are properly closed\n4. **Examine caching behavior** for intermediate results that may not be evicted properly\n\n**Memory Optimization Strategies**\n\n| Memory Issue | Symptoms | Solution Approach |\n|--------------|----------|------------------|\n| Large dataset transformations | Out-of-memory during SQL operations | Implement streaming processing, use temp tables for intermediate results |\n| Python UDF memory accumulation | Memory usage grows within single task | Add explicit garbage collection, use generator functions for large datasets |\n| Connection object accumulation | Memory growth proportional to connection usage | Implement proper connection pooling, ensure connections are returned to pool |\n| Intermediate result caching | Memory usage doesn't decrease between pipeline runs | Implement cache eviction policies, use disk-based caching for large results |\n\n#### Database Performance Debugging\n\nDatabase operations often become bottlenecks in ETL pipelines, especially during bulk loading and complex transformations.\n\n**Query Performance Analysis**\n\nDatabase performance issues require examining both query structure and execution patterns:\n\n1. **Capture query execution plans** for all SQL operations to identify inefficient operations\n2. **Monitor query execution times** and correlate with data volume changes\n3. **Check for missing indexes** on frequently queried columns, especially join and filter columns\n4. **Analyze transaction isolation levels** to prevent lock contention issues\n\n**Bulk Operation Optimization**\n\nETL pipelines perform many bulk operations that require specific optimization approaches:\n\n| Operation Type | Performance Consideration | Optimization Strategy |\n|----------------|--------------------------|----------------------|\n| Bulk inserts | Insert speed vs. transaction safety | Use batch inserts with optimal batch size, disable non-essential indexes during load |\n| Upsert operations | Conflict detection overhead | Implement efficient merge strategies, use staging tables for large upserts |\n| Large result sets | Memory usage and network transfer | Stream results using cursor-based pagination, implement result set compression |\n| Complex transformations | CPU usage and temp space | Break complex operations into smaller steps, use appropriate join algorithms |\n\n#### Network and I/O Performance\n\nETL pipelines often move large amounts of data across network connections, making network and I/O performance critical.\n\n**Network Bottleneck Identification**\n\nNetwork performance issues manifest in several ways:\n\n- **High latency**: Individual operations take longer but overall bandwidth is acceptable\n- **Low throughput**: Network bandwidth is saturated, affecting overall pipeline performance\n- **Intermittent failures**: Network instability causing connection drops and retries\n\n**I/O Optimization Techniques**\n\nFile and database I/O optimization focuses on minimizing the number of operations and maximizing transfer efficiency:\n\n1. **Use connection pooling** to avoid connection setup overhead for database operations\n2. **Implement read-ahead buffering** for sequential file operations\n3. **Use compression** for network transfers when CPU resources allow\n4. **Batch operations** to reduce the overhead of individual I/O calls\n5. **Implement parallel I/O** for independent operations that can run concurrently\n\n> **Performance Debugging Principle**: Always measure before optimizing. Many performance \"improvements\" actually make things worse by optimizing the wrong bottleneck or introducing complexity that reduces maintainability without providing measurable benefits.\n\n### Implementation Guidance\n\nThe debugging infrastructure should be built into the system from the beginning rather than added as an afterthought. This section provides concrete tools and techniques for implementing effective debugging capabilities.\n\n#### Technology Recommendations for Debugging\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Log Aggregation | File-based logging with log rotation | ELK Stack (Elasticsearch, Logstash, Kibana) or cloud logging services |\n| Metrics Collection | Prometheus with Grafana | DataDog, New Relic, or comprehensive APM solutions |\n| Distributed Tracing | Correlation IDs in logs | OpenTelemetry with Jaeger or Zipkin |\n| Performance Profiling | Built-in Python profiler | Continuous profiling with Pyflame or py-spy |\n| Database Monitoring | Query logging and pg_stat_statements | Dedicated database monitoring tools |\n\n#### Recommended File Structure for Debugging Tools\n\n```\nproject-root/\n  debugging/\n    profiling/\n      memory_profiler.py       Memory usage analysis\n      cpu_profiler.py          CPU performance profiling\n    monitoring/\n      health_checks.py         Component health verification\n      metrics_collector.py     Performance metrics gathering\n    log_analysis/\n      log_parser.py           Structured log parsing\n      correlation.py          Cross-component log correlation\n  tests/\n    debugging/\n      test_profiling.py       Profiling tool tests\n      test_monitoring.py      Monitoring functionality tests\n  scripts/\n    debug_pipeline.py         Interactive pipeline debugging script\n    replay_pipeline.py        Pipeline execution replay utility\n```\n\n#### Essential Debugging Infrastructure\n\n```python\n# debugging/monitoring/health_checks.py\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\nimport psutil\nimport psycopg2\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\" \n    UNHEALTHY = \"unhealthy\"\n\n@dataclass\nclass HealthCheckResult:\n    component: str\n    status: HealthStatus\n    message: str\n    metrics: Dict[str, float]\n    timestamp: float\n\nclass SystemHealthChecker:\n    \"\"\"Comprehensive health checking for ETL pipeline components.\"\"\"\n    \n    def __init__(self, db_connection_string: str, message_broker_url: str):\n        self.db_connection_string = db_connection_string\n        self.message_broker_url = message_broker_url\n        \n    def check_system_resources(self) -> HealthCheckResult:\n        \"\"\"Check basic system resource availability.\"\"\"\n        try:\n            memory = psutil.virtual_memory()\n            disk = psutil.disk_usage('/')\n            cpu_percent = psutil.cpu_percent(interval=1)\n            \n            metrics = {\n                'memory_usage_percent': memory.percent,\n                'disk_usage_percent': (disk.used / disk.total) * 100,\n                'cpu_usage_percent': cpu_percent,\n                'available_memory_mb': memory.available / (1024 * 1024)\n            }\n            \n            # Determine status based on thresholds\n            if memory.percent > 90 or disk.used / disk.total > 0.95:\n                status = HealthStatus.UNHEALTHY\n                message = f\"Critical resource usage: Memory {memory.percent}%, Disk {(disk.used/disk.total)*100:.1f}%\"\n            elif memory.percent > 80 or disk.used / disk.total > 0.85:\n                status = HealthStatus.DEGRADED\n                message = f\"High resource usage: Memory {memory.percent}%, Disk {(disk.used/disk.total)*100:.1f}%\"\n            else:\n                status = HealthStatus.HEALTHY\n                message = \"System resources normal\"\n                \n            return HealthCheckResult(\n                component=\"system_resources\",\n                status=status,\n                message=message,\n                metrics=metrics,\n                timestamp=time.time()\n            )\n            \n        except Exception as e:\n            return HealthCheckResult(\n                component=\"system_resources\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"Failed to check system resources: {str(e)}\",\n                metrics={},\n                timestamp=time.time()\n            )\n    \n    def check_database_connectivity(self) -> HealthCheckResult:\n        \"\"\"Verify database connection and basic operations.\"\"\"\n        start_time = time.time()\n        try:\n            conn = psycopg2.connect(self.db_connection_string)\n            cursor = conn.cursor()\n            \n            # Test basic read operation\n            cursor.execute(\"SELECT 1\")\n            result = cursor.fetchone()\n            \n            # Test write operation to a health check table\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS health_check_temp (\n                    check_time TIMESTAMP DEFAULT NOW()\n                )\n            \"\"\")\n            cursor.execute(\"INSERT INTO health_check_temp DEFAULT VALUES\")\n            \n            # Clean up\n            cursor.execute(\"DELETE FROM health_check_temp WHERE check_time < NOW() - INTERVAL '1 hour'\")\n            conn.commit()\n            \n            response_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n            \n            cursor.close()\n            conn.close()\n            \n            metrics = {'response_time_ms': response_time}\n            \n            if response_time > 5000:  # 5 seconds\n                status = HealthStatus.DEGRADED\n                message = f\"Database responding slowly: {response_time:.1f}ms\"\n            else:\n                status = HealthStatus.HEALTHY\n                message = f\"Database healthy: {response_time:.1f}ms response\"\n                \n            return HealthCheckResult(\n                component=\"database\",\n                status=status,\n                message=message,\n                metrics=metrics,\n                timestamp=time.time()\n            )\n            \n        except Exception as e:\n            return HealthCheckResult(\n                component=\"database\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"Database connection failed: {str(e)}\",\n                metrics={'response_time_ms': (time.time() - start_time) * 1000},\n                timestamp=time.time()\n            )\n\n# debugging/log_analysis/correlation.py\nimport re\nimport json\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass LogEntry:\n    timestamp: datetime\n    level: str\n    component: str\n    pipeline_id: Optional[str]\n    run_id: Optional[str] \n    task_id: Optional[str]\n    message: str\n    raw_line: str\n\nclass LogCorrelator:\n    \"\"\"Correlate logs across components for pipeline debugging.\"\"\"\n    \n    def __init__(self):\n        # Regex pattern for structured log format\n        self.log_pattern = re.compile(\n            r'timestamp=(?P<timestamp>\\S+)\\s+'\n            r'level=(?P<level>\\S+)\\s+'\n            r'component=(?P<component>\\S+)'\n            r'(?:\\s+pipeline_id=(?P<pipeline_id>\\S+))?'\n            r'(?:\\s+run_id=(?P<run_id>\\S+))?'\n            r'(?:\\s+task_id=(?P<task_id>\\S+))?'\n            r'.*message=\"(?P<message>[^\"]*)\"'\n        )\n    \n    def parse_log_entry(self, log_line: str) -> Optional[LogEntry]:\n        \"\"\"Parse structured log entry from log line.\"\"\"\n        match = self.log_pattern.match(log_line.strip())\n        if not match:\n            return None\n            \n        try:\n            timestamp = datetime.fromisoformat(match.group('timestamp').replace('Z', '+00:00'))\n        except ValueError:\n            return None\n            \n        return LogEntry(\n            timestamp=timestamp,\n            level=match.group('level'),\n            component=match.group('component'),\n            pipeline_id=match.group('pipeline_id'),\n            run_id=match.group('run_id'),\n            task_id=match.group('task_id'),\n            message=match.group('message'),\n            raw_line=log_line\n        )\n    \n    def correlate_pipeline_logs(self, log_entries: List[LogEntry], run_id: str) -> Dict[str, List[LogEntry]]:\n        \"\"\"Group log entries by component for a specific pipeline run.\"\"\"\n        correlated_logs = {}\n        \n        for entry in log_entries:\n            if entry.run_id == run_id:\n                component = entry.component\n                if component not in correlated_logs:\n                    correlated_logs[component] = []\n                correlated_logs[component].append(entry)\n        \n        # Sort each component's logs by timestamp\n        for component in correlated_logs:\n            correlated_logs[component].sort(key=lambda x: x.timestamp)\n            \n        return correlated_logs\n\n    def find_error_context(self, log_entries: List[LogEntry], error_entry: LogEntry, \n                          context_seconds: int = 30) -> List[LogEntry]:\n        \"\"\"Find log entries around an error for context.\"\"\"\n        error_time = error_entry.timestamp\n        context_logs = []\n        \n        for entry in log_entries:\n            # Same run and within time window\n            if (entry.run_id == error_entry.run_id and \n                abs((entry.timestamp - error_time).total_seconds()) <= context_seconds):\n                context_logs.append(entry)\n        \n        return sorted(context_logs, key=lambda x: x.timestamp)\n```\n\n#### Core Debugging Tools Implementation\n\n```python\n# debugging/profiling/performance_analyzer.py\nimport time\nimport threading\nimport psutil\nfrom collections import defaultdict, deque\nfrom typing import Dict, List, Optional\n\nclass PerformanceAnalyzer:\n    \"\"\"Real-time performance monitoring and analysis.\"\"\"\n    \n    def __init__(self, sample_interval_seconds: float = 1.0):\n        self.sample_interval = sample_interval_seconds\n        self.monitoring = False\n        self.samples = defaultdict(deque)\n        self.monitor_thread = None\n        \n    def start_monitoring(self, pipeline_run_id: str):\n        \"\"\"Begin performance monitoring for a pipeline run.\"\"\"\n        self.monitoring = True\n        self.current_run_id = pipeline_run_id\n        self.monitor_thread = threading.Thread(\n            target=self._monitoring_loop,\n            name=f\"perf-monitor-{pipeline_run_id}\"\n        )\n        self.monitor_thread.daemon = True\n        self.monitor_thread.start()\n    \n    def stop_monitoring(self) -> Dict[str, List[float]]:\n        \"\"\"Stop monitoring and return collected samples.\"\"\"\n        self.monitoring = False\n        if self.monitor_thread:\n            self.monitor_thread.join(timeout=5.0)\n        \n        # Convert deques to lists for JSON serialization\n        results = {}\n        for metric_name, samples in self.samples.items():\n            results[metric_name] = list(samples)\n        \n        # Clear samples for next run\n        self.samples.clear()\n        return results\n    \n    def _monitoring_loop(self):\n        \"\"\"Background thread that collects performance samples.\"\"\"\n        while self.monitoring:\n            timestamp = time.time()\n            \n            # Collect system metrics\n            memory = psutil.virtual_memory()\n            cpu_percent = psutil.cpu_percent()\n            \n            # Store samples with timestamp\n            self.samples['timestamp'].append(timestamp)\n            self.samples['memory_usage_mb'].append(memory.used / (1024 * 1024))\n            self.samples['memory_percent'].append(memory.percent)\n            self.samples['cpu_percent'].append(cpu_percent)\n            \n            # Collect process-specific metrics\n            try:\n                process = psutil.Process()\n                process_memory = process.memory_info()\n                self.samples['process_memory_mb'].append(process_memory.rss / (1024 * 1024))\n                self.samples['process_cpu_percent'].append(process.cpu_percent())\n            except psutil.Error:\n                pass\n            \n            time.sleep(self.sample_interval)\n\n# scripts/debug_pipeline.py\n#!/usr/bin/env python3\n\"\"\"Interactive pipeline debugging utility.\"\"\"\n\nimport sys\nimport json\nimport argparse\nfrom typing import Dict, Any\nfrom debugging.monitoring.health_checks import SystemHealthChecker, HealthStatus\nfrom debugging.log_analysis.correlation import LogCorrelator\nfrom debugging.profiling.performance_analyzer import PerformanceAnalyzer\n\ndef debug_pipeline_execution(pipeline_id: str, run_id: str, log_file_path: str):\n    \"\"\"Debug a specific pipeline execution using multiple analysis techniques.\"\"\"\n    \n    print(f\"Debugging pipeline {pipeline_id}, run {run_id}\")\n    print(\"=\" * 60)\n    \n    # 1. System health check\n    print(\"\\n1. SYSTEM HEALTH CHECK\")\n    print(\"-\" * 30)\n    health_checker = SystemHealthChecker(\n        db_connection_string=\"postgresql://user:pass@localhost/etl_db\",\n        message_broker_url=\"amqp://localhost:5672\"\n    )\n    \n    resource_health = health_checker.check_system_resources()\n    db_health = health_checker.check_database_connectivity()\n    \n    print(f\"System Resources: {resource_health.status.value}\")\n    print(f\"  {resource_health.message}\")\n    print(f\"Database: {db_health.status.value}\")\n    print(f\"  {db_health.message}\")\n    \n    if resource_health.status == HealthStatus.UNHEALTHY:\n        print(\"\\n  CRITICAL: System resources are unhealthy. This may be causing pipeline issues.\")\n    \n    # 2. Log analysis\n    print(\"\\n2. LOG ANALYSIS\") \n    print(\"-\" * 30)\n    \n    correlator = LogCorrelator()\n    log_entries = []\n    \n    # Parse log file\n    try:\n        with open(log_file_path, 'r') as f:\n            for line in f:\n                entry = correlator.parse_log_entry(line)\n                if entry:\n                    log_entries.append(entry)\n        \n        print(f\"Parsed {len(log_entries)} log entries\")\n        \n        # Find logs for this pipeline run\n        run_logs = correlator.correlate_pipeline_logs(log_entries, run_id)\n        \n        print(f\"Found logs from {len(run_logs)} components for run {run_id}\")\n        \n        # Look for errors\n        error_logs = [entry for entry in log_entries \n                     if entry.run_id == run_id and entry.level == 'ERROR']\n        \n        if error_logs:\n            print(f\"\\n  Found {len(error_logs)} error entries:\")\n            for error in error_logs:\n                print(f\"  {error.timestamp} [{error.component}] {error.message}\")\n                \n                # Get context around each error\n                context = correlator.find_error_context(log_entries, error, context_seconds=30)\n                print(f\"    Context ({len(context)} entries around error):\")\n                for ctx_entry in context[-5:]:  # Show last 5 context entries\n                    print(f\"      {ctx_entry.timestamp} [{ctx_entry.component}] {ctx_entry.level}: {ctx_entry.message}\")\n        else:\n            print(\" No errors found in logs\")\n            \n    except FileNotFoundError:\n        print(f\"  Log file not found: {log_file_path}\")\n    except Exception as e:\n        print(f\"  Error parsing logs: {e}\")\n    \n    # 3. Performance analysis suggestions\n    print(\"\\n3. PERFORMANCE ANALYSIS SUGGESTIONS\")\n    print(\"-\" * 30)\n    \n    print(\"To analyze performance issues:\")\n    print(\"1. Enable performance monitoring for the next pipeline run:\")\n    print(f\"   analyzer = PerformanceAnalyzer()\")\n    print(f\"   analyzer.start_monitoring('{run_id}')\")\n    print(\"   # Run pipeline\")\n    print(f\"   results = analyzer.stop_monitoring()\")\n    print(\"\\n2. Check database query performance:\")\n    print(\"   - Enable query logging in database configuration\")\n    print(\"   - Look for slow queries in database logs\")\n    print(\"   - Check for missing indexes on large tables\")\n    print(\"\\n3. Monitor resource usage patterns:\")\n    print(\"   - Memory growth during transformation phases\")\n    print(\"   - CPU spikes during data processing\")\n    print(\"   - Network usage during extraction/loading\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Debug ETL pipeline execution\")\n    parser.add_argument(\"--pipeline-id\", required=True, help=\"Pipeline ID to debug\")\n    parser.add_argument(\"--run-id\", required=True, help=\"Pipeline run ID to debug\") \n    parser.add_argument(\"--log-file\", required=True, help=\"Path to log file\")\n    \n    args = parser.parse_args()\n    \n    debug_pipeline_execution(args.pipeline_id, args.run_id, args.log_file)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n#### Milestone Checkpoints for Debugging Implementation\n\n**After implementing health checking:**\n- Run: `python -m debugging.monitoring.health_checks`\n- Expected: Health status reports for system resources and database connectivity\n- Verify: Health checks correctly identify resource pressure and connection issues\n\n**After implementing log correlation:**\n- Run: `python scripts/debug_pipeline.py --pipeline-id test-pipeline --run-id run-123 --log-file pipeline.log`\n- Expected: Parsed log entries grouped by component with error context\n- Verify: Error messages are correctly correlated with surrounding context\n\n**After implementing performance monitoring:**\n- Run a pipeline with performance monitoring enabled\n- Expected: Real-time metrics collection during execution\n- Verify: Memory and CPU usage patterns are captured and can identify bottlenecks\n\n#### Common Debugging Issues and Solutions\n\n| Symptom | Likely Cause | Diagnosis Command | Fix |\n|---------|--------------|------------------|-----|\n| Debug script fails to connect to database | Connection string incorrect or database unavailable | `psql -d \"postgresql://user:pass@localhost/etl_db\" -c \"SELECT 1\"` | Update connection string, verify database is running |\n| Log parsing returns empty results | Log format doesn't match expected structure | Check log file manually, verify timestamp format matches regex | Update log pattern regex or fix log format |\n| Performance monitoring shows no data | Background thread not starting or crashing | Add exception handling and logging to monitoring loop | Fix threading issues, handle psutil exceptions |\n| Health checks always report unhealthy | Thresholds too strict for environment | Adjust resource usage thresholds in health check logic | Tune thresholds based on system capacity |\n\n\n## Future Extensions\n\n> **Milestone(s):** All milestones - potential enhancements that build upon pipeline definition (Milestone 1), data processing capabilities (Milestones 2-3), and orchestration infrastructure (Milestone 4) to enable advanced use cases and cloud-scale deployment.\n\n### Mental Model: Growing City Infrastructure\n\nThink of our ETL system like a small city that's planning for growth. Right now we've built the essential infrastructure - roads (data pipelines), traffic lights (orchestration), utilities (data processing), and a city hall (monitoring). But as the city grows, we'll need highways for high-speed traffic (distributed processing), airports for long-distance connections (cloud integration), smart traffic systems (real-time processing), and specialized districts (machine learning workflows). The key is designing today's infrastructure so it can evolve into tomorrow's metropolis without requiring a complete rebuild.\n\nThe current ETL system provides a solid foundation with its DAG-based pipeline definition, pluggable connector architecture, transformation engine, and orchestration framework. However, production environments often demand capabilities beyond traditional batch processing - from handling massive data volumes that require distributed computing, to processing streaming data in real-time, to supporting machine learning workflows with specialized requirements. This section explores how the existing architecture can be extended to support these advanced scenarios while maintaining the system's core principles of reliability, observability, and ease of use.\n\nThe extensions fall into two main categories: scalability improvements that help the system handle larger workloads and more complex deployment scenarios, and advanced pipeline features that enable new types of data processing workflows. Each extension is designed to build incrementally on the existing foundation, allowing teams to adopt only the capabilities they need while maintaining backward compatibility with existing pipelines.\n\n### Scalability Extensions\n\n#### Distributed Task Execution\n\nThe current system executes tasks on a single machine using thread-based parallelization within each execution level of the DAG. While this approach works well for moderate workloads, large-scale data processing often requires distributing task execution across multiple machines to handle datasets that exceed single-machine memory and CPU capacity, or to achieve processing speeds that require parallel computation.\n\n> **Decision: Distributed Execution Architecture**\n> - **Context**: Single-machine execution limits throughput and dataset size, preventing the system from handling enterprise-scale workloads that may involve terabytes of data or complex transformations requiring substantial computational resources.\n> - **Options Considered**: \n>   1. Container-based task execution with Kubernetes orchestration\n>   2. Spark-based distributed processing integration\n>   3. Custom distributed worker pool with message queues\n> - **Decision**: Hybrid approach using containerized task execution for general tasks with optional Spark integration for data-intensive transformations\n> - **Rationale**: Containers provide isolation and resource control for arbitrary tasks, while Spark integration leverages existing distributed processing expertise for heavy data workloads\n> - **Consequences**: Enables horizontal scaling and large dataset processing, but requires container orchestration infrastructure and adds deployment complexity\n\nThe distributed execution model extends the existing `TaskExecution` framework by introducing execution targets that can be either local threads or remote execution environments. The `ExecutionPlan` component gains new capabilities to consider resource requirements and execution target availability when scheduling tasks across the cluster.\n\n| Component | Responsibility | Distribution Strategy |\n|-----------|---------------|----------------------|\n| **Distributed Task Scheduler** | Assigns tasks to available execution targets based on resource requirements | Round-robin with resource consideration |\n| **Container Task Executor** | Executes tasks in isolated containers with configurable resource limits | One container per task execution |\n| **Spark Integration Layer** | Submits data-intensive transformations to Spark clusters | Spark job per transformation task |\n| **Resource Manager** | Tracks available compute resources across execution targets | Periodic heartbeat with capacity reporting |\n| **Result Collector** | Aggregates task results from distributed executions | Streaming collection with partial results |\n\nThe distributed execution workflow follows these steps:\n\n1. The `ExecutionPlan` analyzer examines each task's resource requirements and estimated data volume to determine optimal execution target\n2. Tasks requiring large memory or CPU are marked for container execution, while data transformations exceeding size thresholds are routed to Spark\n3. The Distributed Task Scheduler maintains a registry of available execution targets with their current resource utilization\n4. When a task becomes ready for execution, the scheduler selects an appropriate target and packages the task with its dependencies\n5. Container-based tasks are submitted to Kubernetes with resource limits and environment variables containing connection credentials\n6. Spark-based tasks are converted to Spark jobs with optimized partitioning strategies based on input data characteristics\n7. The Result Collector streams task outputs back to the central orchestrator and updates task state in the shared metadata store\n8. Failed tasks can be rescheduled on different execution targets, with automatic retry logic adapted for distributed failure scenarios\n\nContainer task execution requires packaging each task's execution environment, including the necessary connector libraries, transformation functions, and configuration data. The system creates lightweight container images containing the ETL runtime and mounts task-specific configuration and credentials at execution time.\n\n| Container Component | Purpose | Configuration Source |\n|-------------------|---------|---------------------|\n| **ETL Runtime Base Image** | Contains Python environment and core ETL libraries | Pre-built and versioned container registry |\n| **Task Configuration Mount** | Task definition, parameters, and connection configs | ConfigMap generated from TaskDefinition |\n| **Credential Mount** | Database passwords, API keys, and certificates | Kubernetes secrets with rotation support |\n| **Shared Storage Mount** | Large intermediate datasets and checkpoint files | Persistent volume or object storage mount |\n| **Log Collection Sidecar** | Streams task logs back to central monitoring | Fluent Bit or similar log shipping agent |\n\nSpark integration focuses on data transformations that benefit from distributed processing, particularly large joins, aggregations, and complex analytical workloads. The integration layer translates SQL-based transformations into Spark SQL jobs and provides a framework for registering Python UDFs as Spark user-defined functions.\n\n#### Horizontal Scaling and Auto-scaling\n\nThe current orchestration system runs as a single process managing pipeline scheduling and execution coordination. Production deployments require horizontal scaling to handle increased pipeline throughput, support high availability during component failures, and automatically adjust capacity based on workload demand.\n\nThe horizontal scaling model introduces multiple orchestrator instances that coordinate through a shared metadata store and message broker. Each instance can handle pipeline scheduling, task execution coordination, and monitoring responsibilities, with automatic failover when instances become unavailable.\n\n| Scaling Component | Single Instance Role | Multi-Instance Coordination |\n|------------------|---------------------|---------------------------|\n| **Pipeline Scheduler** | Triggers pipelines based on schedules | Leader election with schedule ownership |\n| **Task Execution Coordinator** | Manages task state and dependencies | Distributed coordination via message queue |\n| **Monitoring Collector** | Aggregates metrics and logs | Partition-based collection with merge |\n| **Metadata Store Access** | Direct database connections | Connection pooling with read replicas |\n| **Message Broker Client** | Simple pub/sub for events | Consumer groups with partition assignment |\n\nLeader election ensures that only one orchestrator instance schedules each pipeline to prevent duplicate executions, while multiple instances can coordinate task execution within the same pipeline run. The system uses etcd or a similar consensus system for leader election, with lease renewal and automatic failover when the current leader becomes unavailable.\n\nAuto-scaling responds to increased workload by monitoring key metrics and adjusting the number of orchestrator instances and execution targets. The auto-scaling controller tracks pipeline queue length, average task execution time, and resource utilization to make scaling decisions.\n\n| Scaling Trigger | Scale Up Condition | Scale Down Condition | Scaling Action |\n|----------------|-------------------|---------------------|----------------|\n| **Pipeline Queue Length** | >50 queued pipelines | <10 queued pipelines | Add/remove orchestrator instances |\n| **Task Execution Wait Time** | >5 minutes average wait | <1 minute average wait | Add/remove execution workers |\n| **Resource Utilization** | >80% CPU/memory usage | <30% CPU/memory usage | Adjust container resource limits |\n| **Error Rate Spike** | >10% task failure rate | Normal failure rates | Temporarily reduce concurrency |\n\nThe auto-scaling system includes safeguards to prevent rapid scaling oscillation and considers the cost implications of adding resources. Scale-down decisions include grace periods to allow in-progress tasks to complete and evaluate recent scaling actions to avoid immediate reversals.\n\n> **Critical Design Insight**: Auto-scaling decisions must account for the stateful nature of ETL pipelines, where scaling down may interrupt long-running transformations or cause data inconsistency if not coordinated properly with task execution state.\n\n#### Cloud-Native Integration\n\nModern ETL deployments increasingly leverage cloud services for storage, compute, and managed infrastructure services. The cloud-native extensions integrate with cloud provider APIs to dynamically provision resources, leverage managed services for data storage and processing, and implement cloud-specific optimization patterns.\n\nThe cloud integration layer provides abstractions over common cloud services while maintaining the ability to deploy in on-premises or hybrid environments. The system detects its deployment environment and automatically configures appropriate service integrations.\n\n| Cloud Service Category | AWS Integration | GCP Integration | Azure Integration |\n|------------------------|-----------------|-----------------|------------------|\n| **Object Storage** | S3 with IAM roles | Cloud Storage with service accounts | Blob Storage with managed identity |\n| **Managed Databases** | RDS, Redshift connections | Cloud SQL, BigQuery connectors | SQL Database, Synapse Analytics |\n| **Container Orchestration** | EKS with Fargate support | GKE with Autopilot | AKS with virtual nodes |\n| **Serverless Compute** | Lambda for lightweight tasks | Cloud Functions integration | Azure Functions support |\n| **Message Queues** | SQS/SNS for coordination | Pub/Sub for event streaming | Service Bus for messaging |\n| **Monitoring Integration** | CloudWatch metrics/logs | Cloud Monitoring/Logging | Azure Monitor integration |\n\nCloud-native deployment leverages Infrastructure as Code (IaC) tools to provision and configure the required cloud resources. The system includes Terraform modules and Kubernetes Helm charts that can deploy the complete ETL infrastructure with appropriate security configurations, network policies, and monitoring integrations.\n\nServerless integration allows lightweight tasks to run in cloud functions rather than requiring persistent compute resources. Tasks suitable for serverless execution include data validation, simple transformations, and notification delivery. The system automatically identifies serverless-compatible tasks based on resource requirements and execution patterns.\n\n| Serverless Criteria | Eligible Task Types | Execution Environment | Limitations |\n|---------------------|--------------------|--------------------|-------------|\n| **Memory < 1GB** | Validation, notification tasks | Lambda, Cloud Functions | 15-minute maximum execution time |\n| **No persistent state** | Stateless transformations | Function runtime with mounted storage | No local file system persistence |\n| **Predictable runtime** | Simple data operations | Auto-scaling function instances | Cold start latency considerations |\n| **Standard dependencies** | Tasks using built-in libraries | Pre-packaged runtime environment | Limited custom library support |\n\nCloud storage integration optimizes data transfer by leveraging cloud-native features like transfer acceleration, regional storage, and intelligent tiering. Large datasets are processed using cloud-specific optimization patterns, such as S3 Transfer Acceleration for fast uploads and BigQuery's columnar storage for analytical workloads.\n\n **Pitfall: Cloud Vendor Lock-in**\nDirectly using cloud-specific APIs throughout the codebase creates tight coupling that makes migration difficult. The abstraction layer must provide cloud-agnostic interfaces while still allowing access to cloud-specific optimizations when needed. This requires careful interface design that balances portability with performance.\n\n### Advanced Pipeline Features\n\n#### Stream Processing Integration\n\nTraditional ETL systems focus on batch processing, where data is processed in discrete chunks at scheduled intervals. However, many modern use cases require real-time or near-real-time processing of continuously arriving data streams. Stream processing integration extends the DAG-based pipeline model to support continuous data flows while maintaining the existing batch processing capabilities.\n\n> **Decision: Hybrid Stream-Batch Processing Model**\n> - **Context**: Organizations need both batch processing for historical data and stream processing for real-time analytics, requiring a unified system that can handle both paradigms without forcing users to maintain separate infrastructures.\n> - **Options Considered**: \n>   1. Pure stream processing with micro-batching for historical data\n>   2. Separate stream and batch systems with shared metadata\n>   3. Unified DAG model supporting both stream and batch tasks\n> - **Decision**: Extended DAG model with stream-aware task types and unified orchestration\n> - **Rationale**: Maintains familiar DAG abstraction while enabling stream processing, allows gradual migration from batch to stream, and provides unified monitoring and debugging experience\n> - **Consequences**: Enables real-time use cases with consistent tooling, but adds complexity to scheduling and state management for continuous processing\n\nThe stream processing model introduces new task types that operate on continuous data streams rather than discrete datasets. Stream tasks maintain persistent execution state and process data as it arrives, while batch tasks continue to operate on complete datasets at scheduled intervals.\n\n| Task Type | Data Model | Execution Pattern | State Management |\n|-----------|------------|------------------|------------------|\n| **Stream Source** | Continuous event stream | Always running with checkpoint recovery | Maintains consumer offset/position |\n| **Stream Transform** | Input stream  Output stream | Stateful processing with windowing | Checkpoint-based state persistence |\n| **Stream Sink** | Stream  External system | Micro-batch writes with exactly-once semantics | Idempotent write tracking |\n| **Batch Task** | Complete dataset | Scheduled execution | Traditional task state machine |\n| **Hybrid Task** | Stream + Batch inputs | Triggered on schedule or stream events | Mixed state with clear boundaries |\n\nStream processing requires modified dependency semantics where stream tasks can have dependencies on both other stream tasks (for streaming transformations) and batch tasks (for enrichment data). The DAG validation extends to verify that stream dependencies form valid topologies and that batch dependencies provide appropriate data freshness guarantees.\n\nThe stream processing workflow follows these principles:\n\n1. Stream source tasks connect to message queues, event streams, or change data capture systems and continuously consume new events\n2. Each consumed event or micro-batch triggers downstream stream transformations following the DAG dependency structure\n3. Stream transforms maintain processing state (windows, aggregations, join state) with regular checkpointing to enable failure recovery\n4. Stream sinks buffer output events and write to destinations using configurable batching strategies for efficiency\n5. Hybrid tasks combine stream data with batch-processed reference data, triggering reprocessing when either input changes\n6. The orchestrator monitors stream task health and restarts failed stream tasks from the last successful checkpoint\n7. Stream processing metrics track throughput, latency, and backlog to enable monitoring and auto-scaling decisions\n\nStream task checkpointing ensures exactly-once processing semantics by atomically saving processing state along with output records. The checkpoint mechanism coordinates with downstream tasks to maintain consistency across the entire stream processing pipeline.\n\n| Checkpoint Component | Purpose | Persistence Strategy |\n|---------------------|---------|---------------------|\n| **Consumer Offset** | Tracks position in input stream | Stored in message broker or external store |\n| **Processing State** | Window contents, aggregation values | Serialized to persistent storage with versioning |\n| **Output Tracking** | Records successfully written downstream | Deduplicated output log with retention policy |\n| **Watermark Position** | Event time progress for windowed operations | Coordinated across parallel processing instances |\n\n#### Machine Learning Pipeline Integration\n\nData pipelines increasingly serve machine learning workloads that have specialized requirements around model training, feature engineering, model deployment, and inference serving. The ML integration extends the transformation engine to support ML-specific operations while leveraging existing orchestration and monitoring infrastructure.\n\nMachine learning workflows typically involve feature extraction from raw data, model training on prepared datasets, model validation and testing, and deployment to serving infrastructure. These workflows have unique characteristics including long-running training jobs, iterative experimentation, model versioning, and performance monitoring that differ from traditional ETL operations.\n\nThe ML pipeline extension introduces specialized task types that integrate with popular ML frameworks and provide abstractions for common ML operations. These tasks can be combined with traditional ETL tasks in the same DAG to create end-to-end ML pipelines.\n\n| ML Task Type | Purpose | Framework Integration | Output Artifacts |\n|-------------|---------|---------------------|------------------|\n| **Feature Engineering** | Transform raw data to ML features | Pandas, Spark MLlib | Feature datasets with schema |\n| **Model Training** | Train ML models on prepared data | Scikit-learn, TensorFlow, PyTorch | Versioned model artifacts |\n| **Model Evaluation** | Validate model performance | MLflow, Weights & Biases | Performance metrics and reports |\n| **Model Deployment** | Deploy models to serving infrastructure | KubeFlow, SageMaker, MLflow | Deployed model endpoints |\n| **Batch Inference** | Score large datasets with trained models | Spark, Dask for distributed scoring | Scored datasets with predictions |\n| **Model Monitoring** | Track model performance in production | Custom monitoring with alerting | Drift detection and performance metrics |\n\nFeature engineering tasks extend the transformation engine with ML-specific operations like feature scaling, encoding categorical variables, handling missing values, and creating time-based features. These transformations maintain lineage tracking to enable feature attribution and debugging model performance issues.\n\nModel training tasks handle the specialized requirements of ML training including experiment tracking, hyperparameter optimization, cross-validation, and distributed training for large models. The training integration provides abstractions that work across different ML frameworks while maintaining flexibility for framework-specific optimizations.\n\n| Training Feature | Implementation | Framework Support |\n|-----------------|---------------|------------------|\n| **Experiment Tracking** | MLflow integration for logging | All major frameworks via MLflow |\n| **Hyperparameter Tuning** | Optuna-based optimization | Framework-agnostic optimization |\n| **Distributed Training** | Multi-GPU and multi-node support | TensorFlow distributed, PyTorch DDP |\n| **Model Versioning** | Git-like versioning for model artifacts | MLflow Model Registry integration |\n| **Resource Management** | GPU allocation and scheduling | Kubernetes resource quotas |\n\nModel deployment tasks automate the process of taking trained models and making them available for inference, whether through batch scoring jobs or real-time serving endpoints. The deployment integration handles model packaging, dependency management, and infrastructure provisioning.\n\n> **Critical Design Insight**: ML pipelines often require iterative development where data scientists experiment with different feature engineering and modeling approaches. The pipeline system must support branching and merging of experimental workflows while maintaining reproducibility and version control of successful experiments.\n\nThe ML integration includes specialized monitoring for deployed models that tracks prediction accuracy, data drift, and performance degradation over time. This monitoring can trigger automatic retraining pipelines when model performance drops below acceptable thresholds.\n\n#### Real-time Processing and Event-driven Orchestration\n\nBeyond stream processing for continuous data flows, real-time processing enables sub-second response times for critical business processes and event-driven orchestration that responds immediately to external events rather than relying solely on schedule-based triggers.\n\nReal-time processing requirements include low-latency data transformation, immediate alerting on anomaly detection, real-time feature serving for ML models, and rapid response to business events. These use cases require processing architectures optimized for latency rather than throughput, with careful attention to resource allocation and prioritization.\n\nEvent-driven orchestration extends the scheduling system to trigger pipelines based on external events such as file arrivals, database changes, API calls, or message queue events. This enables reactive data processing that responds to business events as they occur rather than waiting for the next scheduled batch window.\n\n| Event Source | Trigger Mechanism | Latency Target | Use Cases |\n|-------------|------------------|---------------|-----------|\n| **File System Events** | inotify, S3 events | <1 second | Process files immediately upon arrival |\n| **Database Changes** | CDC, triggers | <5 seconds | Sync data changes across systems |\n| **API Webhooks** | HTTP endpoints | <500ms | React to external system notifications |\n| **Message Queues** | Real-time consumers | <100ms | Process high-priority business events |\n| **Monitoring Alerts** | Alert manager integration | <30 seconds | Trigger remediation pipelines |\n\nReal-time processing tasks use in-memory data structures and optimized execution paths to minimize latency. The system provides priority queues for real-time tasks and resource reservation to ensure adequate capacity for time-sensitive processing.\n\nThe event-driven architecture includes event routing and filtering capabilities that allow pipelines to subscribe to specific types of events and apply filters to process only relevant events. This prevents overwhelming the system with low-priority events while ensuring critical events receive immediate attention.\n\n| Event Processing Stage | Purpose | Performance Optimization |\n|-----------------------|---------|--------------------------|\n| **Event Ingestion** | Receive and queue incoming events | High-throughput async I/O |\n| **Event Filtering** | Apply subscription and filter rules | In-memory rule evaluation |\n| **Event Routing** | Direct events to appropriate pipelines | Hash-based partition assignment |\n| **Priority Scheduling** | Schedule real-time tasks with priority | Dedicated resource pools |\n| **Low-latency Execution** | Execute time-sensitive transformations | Pre-warmed execution environments |\n\nReal-time pipeline orchestration includes circuit breakers and bulkhead patterns to prevent cascading failures when external systems become slow or unavailable. The system can automatically route processing to backup systems or degrade functionality gracefully when real-time processing targets cannot be met.\n\n **Pitfall: Real-time Complexity Trade-offs**\nOptimizing for real-time processing often conflicts with other system qualities like reliability, consistency, and cost-effectiveness. Real-time capabilities should be applied selectively to use cases that truly require low latency, while maintaining robust batch processing for operations where eventual consistency is acceptable. Over-engineering for real-time requirements can significantly increase system complexity and operational overhead.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Extension Category | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| **Container Orchestration** | Docker Compose for development | Kubernetes with Helm charts |\n| **Distributed Computing** | Celery with Redis backend | Apache Spark on Kubernetes |\n| **Message Broker** | Redis Pub/Sub | Apache Kafka with Schema Registry |\n| **Stream Processing** | Python asyncio with Redis Streams | Apache Kafka Streams or Apache Flink |\n| **Cloud Integration** | boto3 for AWS, basic cloud APIs | Terraform + cloud SDKs with IAM roles |\n| **ML Framework** | Scikit-learn with joblib | MLflow + Kubeflow for enterprise ML |\n| **Real-time Processing** | WebSockets + in-memory queues | Apache Pulsar with function computing |\n| **Auto-scaling** | Simple threshold-based scaling | Kubernetes HPA with custom metrics |\n\n#### Recommended File Structure Extension\n\n```\nproject-root/\n  etl_system/\n    extensions/\n      distributed/\n        __init__.py\n        container_executor.py       Container-based task execution\n        spark_integration.py        Spark job submission and monitoring\n        resource_manager.py         Cluster resource tracking\n        distributed_scheduler.py    Multi-target task scheduling\n      \n      scaling/\n        __init__.py\n        auto_scaler.py             Auto-scaling controller\n        leader_election.py         Multi-instance coordination\n        load_balancer.py           Request distribution\n      \n      cloud/\n        __init__.py\n        aws_integration.py         AWS service connectors\n        gcp_integration.py         Google Cloud service connectors\n        azure_integration.py       Azure service connectors\n        cloud_storage.py           Multi-cloud storage abstraction\n      \n      streaming/\n        __init__.py\n        stream_tasks.py            Stream processing task types\n        checkpoint_manager.py      Stream state checkpointing\n        event_router.py           Event-driven pipeline triggers\n      \n      ml/\n        __init__.py\n        feature_engineering.py     ML feature transformation tasks\n        model_training.py          ML training task types\n        model_serving.py           Model deployment and serving\n        experiment_tracking.py     MLflow integration\n      \n      realtime/\n        __init__.py\n        priority_scheduler.py      Low-latency task scheduling\n        event_processor.py         Real-time event processing\n        circuit_breaker.py         Failure protection patterns\n    \n    config/\n      extensions/\n        distributed.yaml           Container and Spark configuration\n        cloud.yaml                Cloud provider settings\n        streaming.yaml            Stream processing configuration\n        ml.yaml                   ML framework settings\n```\n\n#### Infrastructure Starter Code\n\n**Container Task Executor Implementation:**\n\n```python\nimport asyncio\nimport logging\nfrom typing import Dict, Any, Optional\nimport docker\nimport kubernetes\nfrom kubernetes import client, config\nfrom dataclasses import dataclass\n\n@dataclass\nclass ContainerTaskSpec:\n    \"\"\"Specification for containerized task execution.\"\"\"\n    task_id: str\n    image: str\n    command: list\n    environment: Dict[str, str]\n    resource_limits: Dict[str, str]\n    volumes: Dict[str, str]\n    timeout_seconds: int\n\nclass ContainerTaskExecutor:\n    \"\"\"Executes ETL tasks in isolated containers using Kubernetes.\"\"\"\n    \n    def __init__(self, namespace: str = \"etl-system\"):\n        self.namespace = namespace\n        self.k8s_client = None\n        self.logger = logging.getLogger(__name__)\n        \n        try:\n            config.load_incluster_config()  # Running in cluster\n        except kubernetes.config.ConfigException:\n            config.load_kube_config()  # Local development\n            \n        self.k8s_client = client.BatchV1Api()\n        self.core_client = client.CoreV1Api()\n    \n    async def execute_task(self, task_spec: ContainerTaskSpec) -> Dict[str, Any]:\n        \"\"\"\n        Execute a task in a Kubernetes job container.\n        \n        Returns:\n            Dict containing execution results, logs, and metadata\n        \"\"\"\n        job_name = f\"etl-task-{task_spec.task_id}\"\n        \n        try:\n            # Create Kubernetes job specification\n            job_spec = self._create_job_spec(job_name, task_spec)\n            \n            # Submit job to Kubernetes\n            self.k8s_client.create_namespaced_job(\n                namespace=self.namespace,\n                body=job_spec\n            )\n            \n            # Wait for completion and collect results\n            result = await self._wait_for_completion(job_name, task_spec.timeout_seconds)\n            \n            # Cleanup completed job\n            await self._cleanup_job(job_name)\n            \n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Container task execution failed: {e}\")\n            await self._cleanup_job(job_name)\n            raise\n    \n    def _create_job_spec(self, job_name: str, task_spec: ContainerTaskSpec):\n        \"\"\"Create Kubernetes Job specification for ETL task.\"\"\"\n        return client.V1Job(\n            metadata=client.V1ObjectMeta(name=job_name),\n            spec=client.V1JobSpec(\n                template=client.V1PodTemplateSpec(\n                    metadata=client.V1ObjectMeta(labels={\"app\": \"etl-task\"}),\n                    spec=client.V1PodSpec(\n                        restart_policy=\"Never\",\n                        containers=[\n                            client.V1Container(\n                                name=\"etl-task\",\n                                image=task_spec.image,\n                                command=task_spec.command,\n                                env=[\n                                    client.V1EnvVar(name=k, value=v)\n                                    for k, v in task_spec.environment.items()\n                                ],\n                                resources=client.V1ResourceRequirements(\n                                    limits=task_spec.resource_limits\n                                ),\n                                volume_mounts=[\n                                    client.V1VolumeMount(\n                                        name=name,\n                                        mount_path=path\n                                    )\n                                    for name, path in task_spec.volumes.items()\n                                ]\n                            )\n                        ],\n                        volumes=[\n                            client.V1Volume(\n                                name=name,\n                                persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(\n                                    claim_name=f\"{name}-pvc\"\n                                )\n                            )\n                            for name in task_spec.volumes.keys()\n                        ]\n                    )\n                )\n            )\n        )\n    \n    async def _wait_for_completion(self, job_name: str, timeout_seconds: int) -> Dict[str, Any]:\n        \"\"\"Wait for job completion and return results.\"\"\"\n        start_time = asyncio.get_event_loop().time()\n        \n        while (asyncio.get_event_loop().time() - start_time) < timeout_seconds:\n            # Check job status\n            job = self.k8s_client.read_namespaced_job_status(\n                name=job_name,\n                namespace=self.namespace\n            )\n            \n            if job.status.succeeded:\n                # Collect logs and return success result\n                logs = await self._collect_logs(job_name)\n                return {\n                    \"status\": \"success\",\n                    \"logs\": logs,\n                    \"duration\": asyncio.get_event_loop().time() - start_time\n                }\n            \n            elif job.status.failed:\n                # Collect logs and return failure result\n                logs = await self._collect_logs(job_name)\n                return {\n                    \"status\": \"failed\",\n                    \"logs\": logs,\n                    \"error\": \"Container job failed\"\n                }\n            \n            # Wait before checking again\n            await asyncio.sleep(5)\n        \n        # Timeout occurred\n        return {\n            \"status\": \"timeout\",\n            \"error\": f\"Task exceeded {timeout_seconds} second timeout\"\n        }\n    \n    async def _collect_logs(self, job_name: str) -> list:\n        \"\"\"Collect logs from completed job pods.\"\"\"\n        try:\n            # Find pods created by the job\n            pods = self.core_client.list_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=f\"job-name={job_name}\"\n            )\n            \n            logs = []\n            for pod in pods.items:\n                pod_logs = self.core_client.read_namespaced_pod_log(\n                    name=pod.metadata.name,\n                    namespace=self.namespace\n                )\n                logs.append(pod_logs)\n            \n            return logs\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to collect logs: {e}\")\n            return [f\"Log collection failed: {e}\"]\n    \n    async def _cleanup_job(self, job_name: str):\n        \"\"\"Clean up completed Kubernetes job and associated pods.\"\"\"\n        try:\n            # Delete the job (this also deletes associated pods)\n            self.k8s_client.delete_namespaced_job(\n                name=job_name,\n                namespace=self.namespace,\n                propagation_policy=\"Foreground\"\n            )\n        except Exception as e:\n            self.logger.error(f\"Failed to cleanup job {job_name}: {e}\")\n```\n\n**Stream Processing Task Framework:**\n\n```python\nimport asyncio\nimport json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, AsyncIterator, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass StreamCheckpoint:\n    \"\"\"Stream processing checkpoint for recovery.\"\"\"\n    task_id: str\n    stream_position: str\n    processing_state: Dict[str, Any]\n    timestamp: datetime\n\nclass StreamTask(ABC):\n    \"\"\"Abstract base class for stream processing tasks.\"\"\"\n    \n    def __init__(self, task_id: str, config: Dict[str, Any]):\n        self.task_id = task_id\n        self.config = config\n        self.logger = logging.getLogger(f\"{__name__}.{task_id}\")\n        self.checkpoint_manager = None\n        self.is_running = False\n    \n    @abstractmethod\n    async def process_event(self, event: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Process a single event from the stream.\n        \n        Args:\n            event: Input event data\n            \n        Returns:\n            Transformed event or None to filter out\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def create_checkpoint(self) -> StreamCheckpoint:\n        \"\"\"Create checkpoint for current processing state.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def restore_from_checkpoint(self, checkpoint: StreamCheckpoint):\n        \"\"\"Restore processing state from checkpoint.\"\"\"\n        pass\n    \n    async def run(self, input_stream: AsyncIterator[Dict[str, Any]], \n                  output_callback: callable):\n        \"\"\"Main execution loop for stream processing.\"\"\"\n        self.is_running = True\n        checkpoint_counter = 0\n        \n        try:\n            async for event in input_stream:\n                if not self.is_running:\n                    break\n                \n                # Process the event\n                result = await self.process_event(event)\n                \n                if result is not None:\n                    await output_callback(result)\n                \n                # Checkpoint periodically\n                checkpoint_counter += 1\n                if checkpoint_counter >= self.config.get(\"checkpoint_interval\", 1000):\n                    await self._create_checkpoint()\n                    checkpoint_counter = 0\n                    \n        except Exception as e:\n            self.logger.error(f\"Stream processing failed: {e}\")\n            raise\n        finally:\n            self.is_running = False\n    \n    async def stop(self):\n        \"\"\"Gracefully stop stream processing.\"\"\"\n        self.is_running = False\n    \n    async def _create_checkpoint(self):\n        \"\"\"Internal checkpoint creation with error handling.\"\"\"\n        try:\n            if self.checkpoint_manager:\n                checkpoint = await self.create_checkpoint()\n                await self.checkpoint_manager.save_checkpoint(checkpoint)\n        except Exception as e:\n            self.logger.error(f\"Checkpoint creation failed: {e}\")\n\nclass WindowedAggregationTask(StreamTask):\n    \"\"\"Stream task that performs windowed aggregations.\"\"\"\n    \n    def __init__(self, task_id: str, config: Dict[str, Any]):\n        super().__init__(task_id, config)\n        self.window_size = config.get(\"window_size_seconds\", 60)\n        self.aggregation_func = config.get(\"aggregation\", \"sum\")\n        self.group_by_field = config.get(\"group_by\", \"key\")\n        \n        # Processing state\n        self.current_window = {}\n        self.window_start = None\n    \n    async def process_event(self, event: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"Aggregate events within time windows.\"\"\"\n        event_time = datetime.fromisoformat(event.get(\"timestamp\"))\n        \n        # Initialize window if needed\n        if self.window_start is None:\n            self.window_start = event_time\n        \n        # Check if event belongs to current window\n        if (event_time - self.window_start).total_seconds() <= self.window_size:\n            # Add to current window\n            group_key = event.get(self.group_by_field, \"default\")\n            current_value = self.current_window.get(group_key, 0)\n            \n            if self.aggregation_func == \"sum\":\n                self.current_window[group_key] = current_value + event.get(\"value\", 0)\n            elif self.aggregation_func == \"count\":\n                self.current_window[group_key] = current_value + 1\n            \n            return None  # No output until window closes\n        else:\n            # Window is complete, emit results and start new window\n            result = {\n                \"window_start\": self.window_start.isoformat(),\n                \"window_end\": (self.window_start + timedelta(seconds=self.window_size)).isoformat(),\n                \"aggregations\": self.current_window.copy()\n            }\n            \n            # Start new window\n            self.current_window = {event.get(self.group_by_field, \"default\"): event.get(\"value\", 0)}\n            self.window_start = event_time\n            \n            return result\n    \n    async def create_checkpoint(self) -> StreamCheckpoint:\n        \"\"\"Save current window state for recovery.\"\"\"\n        return StreamCheckpoint(\n            task_id=self.task_id,\n            stream_position=\"\",  # Would be set by stream consumer\n            processing_state={\n                \"current_window\": self.current_window,\n                \"window_start\": self.window_start.isoformat() if self.window_start else None\n            },\n            timestamp=datetime.now()\n        )\n    \n    async def restore_from_checkpoint(self, checkpoint: StreamCheckpoint):\n        \"\"\"Restore window state from checkpoint.\"\"\"\n        state = checkpoint.processing_state\n        self.current_window = state.get(\"current_window\", {})\n        window_start_str = state.get(\"window_start\")\n        self.window_start = datetime.fromisoformat(window_start_str) if window_start_str else None\n```\n\n#### Core Logic Skeleton Code\n\n**Distributed Execution Coordinator:**\n\n```python\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ExecutionTarget(Enum):\n    LOCAL = \"local\"\n    CONTAINER = \"container\"\n    SPARK = \"spark\"\n    SERVERLESS = \"serverless\"\n\n@dataclass\nclass ResourceRequirements:\n    cpu_cores: float\n    memory_gb: float\n    gpu_count: int = 0\n    storage_gb: float = 0\n    \n@dataclass\nclass ExecutionTargetInfo:\n    target_id: str\n    target_type: ExecutionTarget\n    available_resources: ResourceRequirements\n    current_utilization: ResourceRequirements\n\nclass DistributedExecutionCoordinator:\n    \"\"\"Coordinates task execution across multiple execution targets.\"\"\"\n    \n    def __init__(self):\n        self.execution_targets: Dict[str, ExecutionTargetInfo] = {}\n        self.task_assignments: Dict[str, str] = {}  # task_id -> target_id\n    \n    def register_execution_target(self, target_info: ExecutionTargetInfo):\n        \"\"\"Register new execution target with available resources.\"\"\"\n        # TODO 1: Add target to execution_targets registry\n        # TODO 2: Validate target configuration and connectivity\n        # TODO 3: Start health monitoring for the target\n        # TODO 4: Log target registration for debugging\n        pass\n    \n    def select_execution_target(self, task_id: str, requirements: ResourceRequirements) -> Optional[str]:\n        \"\"\"\n        Select optimal execution target for task based on resource requirements.\n        \n        Returns target_id of selected target, or None if no suitable target available.\n        \"\"\"\n        # TODO 1: Filter targets that have sufficient available resources\n        # TODO 2: Calculate suitability score based on current utilization\n        # TODO 3: Consider target type preferences (container vs spark vs serverless)\n        # TODO 4: Apply load balancing to distribute tasks evenly\n        # TODO 5: Update target utilization after assignment\n        # Hint: Score targets based on available_resources - current_utilization\n        pass\n    \n    def estimate_task_resources(self, task_definition: 'TaskDefinition') -> ResourceRequirements:\n        \"\"\"Estimate resource requirements based on task configuration.\"\"\"\n        # TODO 1: Check task type and configuration for resource hints\n        # TODO 2: Look up historical resource usage for similar tasks\n        # TODO 3: Apply default resource requirements based on task type\n        # TODO 4: Consider data volume estimates for transformation tasks\n        # TODO 5: Add safety margins to prevent resource exhaustion\n        pass\n    \n    def determine_execution_target_type(self, task_definition: 'TaskDefinition', \n                                      requirements: ResourceRequirements) -> ExecutionTarget:\n        \"\"\"Determine appropriate execution target type for task.\"\"\"\n        # TODO 1: Check if task is suitable for serverless (stateless, short duration)\n        # TODO 2: For large data transformations, prefer Spark execution\n        # TODO 3: For I/O intensive tasks, prefer container execution\n        # TODO 4: Fall back to local execution for simple tasks\n        # TODO 5: Consider cost implications of different execution types\n        pass\n```\n\n**Auto-scaling Controller:**\n\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport asyncio\n\n@dataclass\nclass ScalingMetrics:\n    pipeline_queue_length: int\n    average_task_wait_time: float\n    resource_utilization: float\n    error_rate: float\n    timestamp: datetime\n\n@dataclass \nclass ScalingDecision:\n    action: str  # \"scale_up\", \"scale_down\", \"no_action\"\n    target_instances: int\n    reason: str\n    \nclass AutoScalingController:\n    \"\"\"Automatically scales ETL system resources based on workload metrics.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.scaling_history: List[ScalingDecision] = []\n        self.current_instances = 1\n        self.last_scaling_time = None\n        \n    async def collect_metrics(self) -> ScalingMetrics:\n        \"\"\"Collect current system metrics for scaling decisions.\"\"\"\n        # TODO 1: Query pipeline scheduler for queue length\n        # TODO 2: Calculate average task wait time from recent executions\n        # TODO 3: Get resource utilization from all execution targets\n        # TODO 4: Calculate error rate from recent task executions\n        # TODO 5: Return ScalingMetrics with current timestamp\n        pass\n    \n    def should_scale_up(self, metrics: ScalingMetrics) -> bool:\n        \"\"\"Determine if system should scale up based on metrics.\"\"\"\n        # TODO 1: Check if queue length exceeds scale-up threshold\n        # TODO 2: Check if task wait time exceeds acceptable limits\n        # TODO 3: Check if resource utilization is too high\n        # TODO 4: Ensure cooldown period has elapsed since last scaling\n        # TODO 5: Verify maximum instance limit not exceeded\n        pass\n    \n    def should_scale_down(self, metrics: ScalingMetrics) -> bool:\n        \"\"\"Determine if system should scale down based on metrics.\"\"\"\n        # TODO 1: Check if queue length is below scale-down threshold\n        # TODO 2: Check if resource utilization is low for sustained period\n        # TODO 3: Ensure minimum instance count maintained\n        # TODO 4: Verify no recent scaling actions (prevent oscillation)\n        # TODO 5: Check that error rate is normal (don't scale down during issues)\n        pass\n    \n    async def execute_scaling_decision(self, decision: ScalingDecision):\n        \"\"\"Execute the scaling decision by adjusting system resources.\"\"\"\n        # TODO 1: Log scaling decision with detailed reasoning\n        # TODO 2: Update container orchestration (Kubernetes HPA or similar)\n        # TODO 3: Wait for new instances to become ready\n        # TODO 4: Verify scaling completed successfully\n        # TODO 5: Update internal state and record scaling history\n        # TODO 6: Send notification about scaling action\n        pass\n```\n\n#### Milestone Checkpoints\n\n**Stream Processing Milestone:**\nAfter implementing basic stream processing capabilities, verify:\n\n1. **Stream Task Execution**: Create a simple windowed aggregation task that sums events over 30-second windows\n   - Expected: Task should accumulate events and emit window results\n   - Command: `python -m etl_system.extensions.streaming.test_stream_task`\n   - Verify: Check that window boundaries are respected and aggregations are correct\n\n2. **Checkpoint Recovery**: Stop and restart a stream task to verify checkpoint recovery\n   - Expected: Task resumes from last checkpoint without data loss\n   - Manual test: Send events, stop task mid-window, restart, verify window completion\n   - Signs of issues: Duplicate events, lost aggregation state, incorrect window boundaries\n\n3. **Event-driven Pipeline Triggers**: Set up file arrival event that triggers a pipeline\n   - Expected: Pipeline starts within 1 second of file creation\n   - Test: `touch /tmp/test_file.csv` should trigger pipeline execution\n   - Verify: Check pipeline run logs show event-triggered execution\n\n**Distributed Execution Milestone:**\nAfter implementing container-based execution:\n\n1. **Container Task Execution**: Submit a simple data extraction task to Kubernetes\n   - Expected: Task runs in container and returns results to orchestrator\n   - Command: `kubectl logs -l app=etl-task` should show task execution logs\n   - Verify: Task completion reported in pipeline run status\n\n2. **Resource-based Target Selection**: Run tasks with different resource requirements\n   - Expected: High-memory tasks assigned to appropriate execution targets\n   - Test: Submit both light and heavy tasks, verify assignment logic\n   - Signs of issues: Tasks assigned to under-resourced targets, execution failures\n\n3. **Auto-scaling Response**: Generate high pipeline load to trigger scaling\n   - Expected: Additional orchestrator instances start automatically\n   - Monitor: `kubectl get pods` should show new instances after sustained load\n   - Verify: Load distributes across instances, no duplicate pipeline executions\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis Steps | Fix |\n|---------|--------------|----------------|-----|\n| **Container tasks never complete** | Resource limits too restrictive | Check `kubectl describe pod` for resource constraints | Increase memory/CPU limits in container spec |\n| **Stream processing falls behind** | Processing slower than event arrival rate | Monitor event queue length and processing latency | Add parallel stream processors or optimize processing logic |\n| **Auto-scaling oscillation** | Thresholds too sensitive or cooldown too short | Review scaling history and metric patterns | Increase cooldown period and add hysteresis to thresholds |\n| **Distributed task failures** | Network connectivity or authentication issues | Test connectivity from containers to data sources | Update network policies and credential mounting |\n| **ML pipeline memory errors** | Model training exceeds available memory | Monitor memory usage during training | Use distributed training or gradient accumulation |\n| **Real-time processing timeouts** | Processing chain too complex for latency target | Profile each processing step latency | Simplify processing or use faster execution targets |\n| **Cloud integration auth failures** | IAM roles or service account misconfig | Check cloud provider logs and permissions | Update role policies and credential configuration |\n| **Stream checkpoint corruption** | Concurrent checkpoint writes or storage failures | Check checkpoint storage logs and file integrity | Implement checkpoint locking and validation |\n\n\n## Glossary\n\n> **Milestone(s):** All milestones - comprehensive vocabulary reference that supports understanding across pipeline definition (Milestone 1), data processing (Milestones 2-3), orchestration and monitoring (Milestone 4), and system operations.\n\nThis glossary provides comprehensive definitions of key technical terms and domain-specific vocabulary used throughout the ETL system design document. The terms are organized to support both newcomers learning ETL concepts and experienced developers working with the system's specific implementation details.\n\n### Mental Model: Technical Dictionary with Context\n\nThink of this glossary as a specialized technical dictionary that goes beyond simple definitions. Like a good dictionary for a foreign language, each entry provides not just the meaning but also context about when and how the term is used in ETL systems. Just as language dictionaries show pronunciation, etymology, and usage examples, this technical glossary explains relationships between concepts, common usage patterns, and potential pitfalls that arise when working with these terms.\n\nThe glossary serves as both a reference during development and a learning tool for understanding the broader ETL ecosystem. Terms build upon each other - understanding **watermarking** requires knowledge of **incremental loading**, which relates to **change data capture**, which connects to **idempotent** operations.\n\n### Core ETL and Data Processing Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **ETL** | Extract Transform Load - the three-phase process of moving data from sources to destinations with transformations applied | The fundamental pattern for data pipeline systems. Extract pulls data from sources, Transform applies business logic and data quality rules, Load writes results to destinations |\n| **DAG** | Directed Acyclic Graph - a graph structure with directed edges and no cycles, used to represent task dependencies | Core data structure for pipeline definition. Tasks are nodes, dependencies are edges. Acyclic property ensures execution order can be determined through topological sorting |\n| **idempotent** | Operations that produce the same result when executed multiple times, regardless of how many times they run | Critical property for ETL reliability. Enables safe retries after failures without data corruption or duplication. Example: `INSERT ... ON CONFLICT DO NOTHING` |\n| **topological sort** | Algorithm for ordering nodes in a DAG such that all dependencies come before their dependents | Used to determine task execution order. Kahn's algorithm produces parallel execution levels, allowing multiple independent tasks to run simultaneously |\n| **checkpoint** | Saving intermediate processing state to enable resumption after failures | Enables fault tolerance by avoiding complete restart after partial completion. Includes task state, processed record counts, and watermark positions |\n| **lineage** | Tracking data provenance and transformation history through pipeline execution | Essential for compliance, debugging, and impact analysis. Records which source data contributed to each output record and what transformations were applied |\n| **watermark** | High-water mark indicating the latest data that has been successfully processed | Key mechanism for incremental loading. Typically a timestamp, sequence number, or other monotonically increasing value that tracks processing progress |\n| **adjacency list** | Graph representation that maps each node to a list of its direct neighbors | Standard representation for DAGs in pipeline systems. Enables efficient cycle detection and topological sorting algorithms |\n| **cycle detection** | Algorithm to identify circular dependencies in directed graphs | Critical validation step before pipeline execution. Uses depth-first search with three-color marking (white/gray/black) to detect back edges |\n| **critical path** | Longest dependency chain in a DAG, determining minimum possible execution time | Used for execution planning and performance optimization. Tasks on the critical path cannot be delayed without extending total pipeline runtime |\n| **execution levels** | Groups of tasks that have no dependencies between them and can execute in parallel | Result of topological sorting. Level 0 contains tasks with no dependencies, Level N contains tasks whose dependencies are all in levels 0 through N-1 |\n| **in-degree** | Number of incoming dependencies for a task in the dependency graph | Used in Kahn's algorithm for topological sorting. Tasks with in-degree 0 are ready to execute |\n\n### Data Extraction and Loading Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **watermarking** | Process of tracking and updating high-water marks for incremental data extraction | Prevents duplicate processing and enables efficient incremental loads. Watermarks must be updated atomically with successful data loading |\n| **cursor-based pagination** | Using opaque tokens or identifiers to track position in paginated API results | More reliable than offset-based pagination for changing datasets. Cursors remain stable even when underlying data is modified during extraction |\n| **change data capture** | Real-time stream of database changes (inserts, updates, deletes) for incremental loading | Enables near real-time data synchronization. Common implementations include database transaction logs, triggers, or timestamp-based change tracking |\n| **bulk loading** | Optimized batch insertion technique designed for high throughput data transfer | Significantly faster than row-by-row inserts. Uses techniques like `COPY` statements, batch APIs, or staging files for maximum performance |\n| **upsert** | Combined insert-or-update operation that handles conflicts when loading data | Essential for idempotent loading. Syntax varies by database: PostgreSQL `ON CONFLICT`, MySQL `ON DUPLICATE KEY`, SQL Server `MERGE` |\n| **schema mapping** | Translation rules between source and destination data structures and types | Handles differences in column names, data types, and structural organization. May include type conversion rules and default value assignments |\n| **connection pooling** | Reusing database connections across multiple operations for improved performance | Reduces connection overhead and manages concurrent access. Pools maintain minimum/maximum connection counts with timeout handling |\n| **incremental loading** | Extracting and processing only data that has changed since the last pipeline execution | Core technique for efficient ETL at scale. Relies on watermarking, timestamps, or change data capture to identify new/modified records |\n| **lookback window** | Small time buffer added to watermark queries to handle clock skew and late-arriving data | Prevents data loss from timing issues. Typically 1-5 minutes depending on system characteristics and consistency requirements |\n| **staging table** | Temporary storage area for atomic bulk loading operations | Enables transactional loading patterns. Data is loaded to staging first, then atomically moved to final destination, allowing rollback on failure |\n\n### Data Transformation Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **schema evolution** | Systematic management of data structure changes over time while maintaining compatibility | Critical for production systems. Includes adding columns, changing types, and handling backward compatibility with existing consumers |\n| **type coercion** | Automatic conversion between compatible data types during transformation | Can cause precision loss (float to int) or truncation. Requires careful handling with validation and error reporting for failed conversions |\n| **UDF** | User-Defined Function - custom transformation logic written in Python or other languages | Enables complex business logic beyond SQL capabilities. Runs in isolated processes with proper error handling and resource limits |\n| **template rendering** | Process of applying runtime parameters to SQL or configuration templates | Uses template engines like Jinja2 to generate final SQL queries. Enables parameterized pipelines with dynamic behavior based on runtime context |\n| **subprocess isolation** | Running transformation code in separate processes for safety and resource control | Prevents memory leaks and crashes in one transformation from affecting others. Enables resource limits and timeout enforcement |\n| **schema registry** | Centralized catalog of data structure definitions and their version history | Enables schema validation, evolution tracking, and compatibility checking. Supports multiple schema formats (JSON Schema, Avro, etc.) |\n| **validation pipeline** | Series of data quality checks that records pass through during transformation | Includes type checking, constraint validation, and business rule verification. Failed records can be rejected, flagged, or sent to dead letter queues |\n| **null semantics** | Rules for handling null/missing values across different systems and transformations | Varies significantly between databases and languages. Requires explicit handling in transformations to prevent unexpected behavior |\n| **compatibility checking** | Verification that schema changes don't break existing pipeline consumers | Includes forward compatibility (new schemas work with old consumers) and backward compatibility (old schemas work with new consumers) |\n\n### Pipeline Orchestration Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **orchestration** | Coordination of pipeline execution including scheduling, monitoring, and resource management | Encompasses the entire pipeline lifecycle from trigger through completion. Includes dependency resolution, parallel execution, and failure handling |\n| **scheduler** | Component responsible for triggering pipeline execution based on time schedules or external events | Supports cron expressions for time-based triggers and event-driven execution. Manages schedule state and handles execution policies |\n| **execution engine** | Component that runs pipeline tasks with parallelization, resource allocation, and state management | Core runtime system that executes tasks according to DAG dependencies. Manages task state transitions and resource allocation |\n| **state machine** | Formal model defining valid task states and the events that trigger transitions between them | Ensures consistent task lifecycle management. States include PENDING, RUNNING, SUCCESS, FAILED with defined transition rules |\n| **resource allocation** | Assignment of compute resources (CPU, memory, storage) to executing tasks | Prevents resource exhaustion and enables performance optimization. Includes resource estimation, reservation, and cleanup |\n| **metrics collection** | Systematic gathering of performance and business metrics during pipeline execution | Enables monitoring, alerting, and performance optimization. Includes execution times, record counts, error rates, and resource usage |\n| **data lineage** | Comprehensive tracking of data provenance and transformation history through pipelines | Records complete data flow from sources through transformations to destinations. Essential for compliance, debugging, and impact analysis |\n| **alert suppression** | Preventing duplicate or cascading alerts during system outages or widespread issues | Reduces alert noise and prevents overwhelming operations teams. Uses correlation rules and time windows to group related alerts |\n| **level-based parallelization** | Executing all tasks at the same DAG level simultaneously while respecting dependencies | Maximizes pipeline throughput by running independent tasks in parallel. Each level waits for the previous level to complete |\n| **message broker** | Asynchronous communication system enabling loose coupling between pipeline components | Enables event-driven architecture and fault-tolerant communication. Common implementations include Apache Kafka, RabbitMQ, or cloud messaging services |\n| **dependency resolution** | Process of determining task execution order based on declared prerequisites and constraints | Combines topological sorting with runtime conditions. Handles complex scenarios like conditional dependencies and dynamic task generation |\n\n### Error Handling and Recovery Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **exponential backoff** | Retry strategy with exponentially increasing delays between attempts, often with randomization | Prevents overwhelming failing systems while providing reasonable retry behavior. Typical pattern: 1s, 2s, 4s, 8s with jitter |\n| **circuit breaker** | Protection mechanism that stops calling a failing service to prevent cascading failures | Opens circuit after consecutive failures, enters half-open state for testing, closes when service recovers. Prevents thundering herd problems |\n| **dead letter queue** | Storage system for messages that cannot be processed after maximum retry attempts | Enables manual inspection and reprocessing of failed items. Prevents data loss while avoiding infinite retry loops |\n| **jitter** | Random variation added to retry timing to prevent synchronized load spikes | Prevents thundering herd when many clients retry simultaneously. Typically 10-50% random variation in delay timing |\n| **saga pattern** | Breaking long-running transactions into smaller, compensatable steps with rollback capability | Enables fault tolerance in distributed systems. Each step has a corresponding compensation operation for rollback |\n| **two-phase commit** | Distributed transaction protocol ensuring atomicity across multiple systems | Provides strong consistency guarantees at the cost of performance and availability. Requires all participants to vote before committing |\n| **compensation transaction** | Reverse operation designed to undo the effects of a completed transaction step | Key component of saga pattern. Must be idempotent and handle partial completion scenarios |\n\n![Error Handling and Recovery Flow](./diagrams/error-handling-flow.svg)\n\n### Monitoring and Operations Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **health check** | Automated verification of component operational status and readiness to handle requests | Includes connectivity tests, resource availability checks, and functional validation. Used by load balancers and monitoring systems |\n| **log correlation** | Process of connecting related log entries across different components and time periods | Essential for distributed system debugging. Uses correlation IDs, timestamps, and context propagation to trace request flows |\n| **performance profiling** | Systematic analysis of resource usage, bottlenecks, and optimization opportunities | Identifies CPU, memory, and I/O hotspots. Includes both real-time monitoring and historical analysis for capacity planning |\n| **symptom-cause mapping** | Structured diagnostic approach that maps observable symptoms to underlying root causes | Systematic troubleshooting methodology. Documents known failure patterns and their resolution steps for faster incident response |\n| **interactive debugging** | Real-time investigation of system behavior using debugging tools and techniques | Includes breakpoints, variable inspection, and step-through execution. Challenging in distributed systems due to timing dependencies |\n| **resource monitoring** | Continuous tracking of system resource utilization including CPU, memory, disk, and network | Enables capacity planning, performance optimization, and early warning of resource exhaustion. Includes both host-level and application-level metrics |\n| **error context** | Log entries and system state surrounding an error event to provide diagnostic information | Critical for effective troubleshooting. Includes events leading up to the error, concurrent activities, and system state at failure time |\n\n### Advanced Architecture Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **distributed execution** | Running pipeline tasks across multiple machines or compute environments | Enables horizontal scaling and resource optimization. Requires coordination, state management, and fault tolerance across network boundaries |\n| **horizontal scaling** | Adding more compute instances to handle increased load, as opposed to upgrading existing hardware | Preferred scaling approach for cloud systems. Requires stateless task design and effective load distribution mechanisms |\n| **auto-scaling** | Automatically adjusting resource allocation based on current demand and performance metrics | Balances cost optimization with performance requirements. Includes scale-up triggers, scale-down policies, and resource estimation |\n| **cloud-native** | Architecture designed specifically for cloud deployment patterns and services | Emphasizes containerization, microservices, and managed cloud services. Designed for elasticity, fault tolerance, and operational simplicity |\n| **stream processing** | Continuous processing of data streams as they arrive, rather than batch processing | Enables real-time analytics and low-latency data pipelines. Requires different architectural patterns than traditional batch ETL |\n| **machine learning pipeline** | Specialized workflow for ML model training, validation, and deployment with unique requirements | Includes data preprocessing, feature engineering, model training, validation, and deployment stages. Requires versioning and experiment tracking |\n| **real-time processing** | Data processing with sub-second response time requirements | More demanding than stream processing, requiring specialized architectures and technologies. Often uses in-memory processing and optimized data structures |\n| **event-driven orchestration** | Triggering pipeline execution based on external events rather than time schedules | Enables reactive data processing and just-in-time pipeline execution. Requires reliable event delivery and proper ordering |\n| **container orchestration** | Managing containerized applications across clusters with scheduling, scaling, and service discovery | Enables portable deployment and efficient resource utilization. Common platforms include Kubernetes, Docker Swarm, and cloud container services |\n| **serverless integration** | Using cloud functions and managed services for lightweight, event-driven tasks | Eliminates infrastructure management while providing automatic scaling. Suitable for simple transformations and integration tasks |\n| **checkpoint recovery** | Resuming pipeline execution from saved processing state after failures | Enables fault tolerance without complete restart. Requires careful state management and atomic checkpoint operations |\n| **leader election** | Choosing a primary instance in a distributed system to coordinate shared operations | Prevents split-brain scenarios and ensures single point of control. Common in distributed schedulers and coordination services |\n| **priority scheduling** | Executing higher-priority tasks before lower-priority ones, subject to resource availability | Enables SLA management and critical path optimization. Requires careful balance to prevent starvation of lower-priority tasks |\n\n### Data Types and Validation Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **STRING** | Variable-length text data type supporting Unicode characters | Most flexible data type but requires careful handling of encoding, length limits, and special characters during transformations |\n| **INTEGER** | Whole number data type with defined precision and range limits | Common source of overflow errors during transformations. Different systems have different integer sizes and signedness |\n| **FLOAT** | Floating-point numeric data type with inherent precision limitations | Can cause precision loss during type conversion. Requires careful handling of NaN, infinity, and rounding behaviors |\n| **DECIMAL** | Fixed-precision numeric data type for exact decimal calculations | Preferred for financial calculations. Precision and scale must be preserved during transformations to prevent data loss |\n| **BOOLEAN** | Binary true/false data type with varying representations across systems | Representations vary: true/false, 1/0, Y/N, T/F. Requires normalization during cross-system data movement |\n| **DATE** | Calendar date without time component, with varying precision and timezone handling | Timezone-naive type that can cause issues in global systems. Requires careful handling of date arithmetic and comparisons |\n| **TIMESTAMP** | Date and time data type with optional timezone information | Complex type requiring timezone handling, precision management, and careful comparison logic across systems |\n| **JSON** | Semi-structured data type for nested objects and arrays | Requires schema validation and careful handling of type coercion for nested fields. Not all systems support JSON natively |\n| **BINARY** | Raw binary data type for storing files, images, or encoded content | Requires base64 encoding for text-based transport. Large binary fields can impact pipeline performance |\n\n### Task and Execution State Terms\n\n| Term | Definition | Context and Usage |\n|------|------------|-------------------|\n| **PENDING** | Initial task state before dependencies are evaluated and resources allocated | Task is defined but not yet ready for execution. Waiting for dependency resolution and resource availability |\n| **WAITING** | Task state when dependencies are not yet satisfied | Task is ready to execute but blocked by upstream dependencies. Moves to QUEUED when dependencies complete successfully |\n| **QUEUED** | Task state when ready for execution but waiting for available resources | Dependencies are satisfied but execution slot or resources not yet available. Managed by scheduler priority queues |\n| **RUNNING** | Task state during active execution | Task is consuming resources and performing work. Requires monitoring for progress, timeouts, and resource usage |\n| **SUCCESS** | Task completed successfully with expected outputs | Terminal state indicating successful completion. Enables downstream dependencies to begin execution |\n| **FAILED** | Task completed unsuccessfully due to errors or exceptions | Terminal state unless retry policy applies. May trigger failure handling, alerts, and pipeline-level error responses |\n| **RETRYING** | Task is scheduled for retry after a failure | Temporary state between failure and retry attempt. Managed by retry policy with exponential backoff timing |\n| **CANCELLED** | Task execution was cancelled by user or system action | Terminal state for tasks that were stopped before completion. Requires cleanup of partial work and resources |\n| **SKIPPED** | Task was intentionally bypassed due to conditional logic or upstream failures | Terminal state for tasks that didn't need to execute. Common in conditional pipelines and failure scenarios |\n\n### Implementation Guidance\n\nThis glossary serves as both a learning resource and a practical reference during development. The terms are carefully chosen to align with industry standards while providing specific context for this ETL system implementation.\n\n#### Technology Recommendations for Glossary Management\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Glossary Storage | Static markdown files in repository | Searchable documentation platform (GitBook, Notion) |\n| Term Cross-References | Manual links between sections | Automated link detection and validation |\n| Definition Validation | Manual review process | Automated consistency checking against codebase |\n| Usage Examples | Inline code snippets | Live examples from test cases |\n\n#### Glossary Maintenance Structure\n\n```\nproject-root/\n  docs/\n    design/\n      glossary.md                     this document\n      term-usage-examples/            code examples for key terms\n        dag-operations.py             DAG manipulation examples  \n        watermarking-patterns.py      incremental loading examples\n        retry-policies.py             error handling examples\n    api/\n      terminology.json                machine-readable term definitions\n      cross-references.json           term relationship mappings\n  src/\n    common/\n      constants.py                    canonical constant definitions\n      types.py                        type definitions matching glossary\n```\n\n#### Term Consistency Validation\n\nThe glossary terms should be validated against the actual codebase to ensure consistency. Key validation points include:\n\n- Type names match exactly between glossary definitions and code declarations\n- Method signatures align with interface descriptions  \n- Constants are defined with correct values and naming conventions\n- Enum values match exactly across documentation and implementation\n- State machine transitions are accurately reflected in both glossary and code\n\n#### Common Glossary Usage Patterns\n\n **Pitfall: Terminology Drift**\nTerms defined in early design phases often evolve during implementation, leading to inconsistencies between documentation and code. Establish a single source of truth (preferably the code) and update documentation accordingly.\n\n **Pitfall: Overloaded Terms**  \nSome terms like \"pipeline\" or \"state\" have multiple meanings in different contexts. Always provide sufficient context to disambiguate, and consider using compound terms like \"pipeline definition\" vs \"pipeline execution\" when clarity is important.\n\n **Pitfall: Missing Domain Context**\nTechnical terms often have different meanings in different domains. ETL-specific definitions may differ from general software engineering or database administration usage. Always provide ETL-specific context and usage examples.\n\n#### Milestone Checkpoints\n\n**Checkpoint: Terminology Consistency**\n- Verify all type names in code match glossary definitions exactly\n- Confirm method signatures align with interface descriptions\n- Validate that state machine implementations match documented transitions\n- Check that constant values match their glossary descriptions\n\n**Checkpoint: Documentation Completeness**  \n- Ensure every public type has a corresponding glossary entry\n- Verify all domain-specific terms are defined with appropriate context\n- Confirm cross-references between related terms are accurate and helpful\n- Validate that examples provided are current and functional\n\nThe glossary should be treated as a living document that evolves with the system while maintaining accuracy and usefulness as a reference tool for both learning and development activities.\n"}